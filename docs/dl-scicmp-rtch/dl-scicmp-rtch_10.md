# 7  模块

> 原文：[`skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/modules.html`](https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/modules.html)

在上一章中，我们构建了一个用于回归任务的神经网络。有两种不同的操作类型：线性和非线性。

在非线性类别中，我们有了 ReLU 激活函数，它被表示为一个简单的函数调用：`nnf_relu()`。激活函数是*函数*：给定输入 $\mathbf{x}$，它们每次都会返回输出 $\mathbf{y}$。换句话说，它们是确定的。然而，线性部分则不同。

回归网络中的线性部分是通过矩阵乘法——权重矩阵——和向量加法（偏置向量）实现的。这样的操作，结果不可避免地依赖于存储在相应张量中的实际值。换句话说，操作是*有状态的*。

每当涉及到状态时，将其封装在对象中是有帮助的，这样就可以让用户免于手动管理。这正是`torch`的*模块*所做的事情。

注意术语，*模块*。在`torch`中，一个模块可以是任何复杂度，从基本的*层*——比如我们稍后将要介绍的`nn_linear()`——到由许多此类层组成的完整*模型*。在代码上，“层”和“模型”之间没有区别。这就是为什么在某些文本中，你会看到“模块”一词被广泛使用。在这本书中，我将主要遵循层和模型的通用术语，因为它更接近于概念上的事物表现。

回到模块的*为什么*。除了封装之外，提供层对象还有另一个原因：并非所有常用的层都像`nn_linear()`那样轻量级。我们将在下一节的末尾简要提及几个其他层，并将完整的介绍留到本书的后续章节。

## 7.1 内置的`nn_module()`s

在`torch`中，线性层是通过`nn_linear()`创建的。`nn_linear()`期望（至少）两个参数：`in_features`和`out_features`。假设你的输入数据有五十个观测值，每个观测值有五个特征；也就是说，它的大小是 50 x 5。你想构建一个包含十六个单元的隐藏层。那么`in_features`是 5，`out_features`是 16。（如果你自己构建，5 和 16 将构成权重矩阵的行数/列数。）

```r
library(torch)
l <- nn_linear(in_features = 5, out_features = 16)
```

*一旦创建，模块就会立即通知你其参数：

```r
l
```

```r
An `nn_module` containing 96 parameters.
Parameters
 weight: Float [1:16, 1:5]
 bias: Float [1:16]
```

封装并不会阻止我们检查权重和偏置张量：

```r
l$weight
```

```r
torch_tensor
-0.2079 -0.1920  0.2926  0.0036 -0.0897
 0.3658  0.0076 -0.0671  0.3981 -0.4215
 0.2568  0.3648 -0.0374 -0.2778 -0.1662
 0.4444  0.3851 -0.1225  0.1678 -0.3443
-0.3998  0.0207 -0.0767  0.4323  0.1653
 0.3997  0.0647 -0.2823 -0.1639 -0.0225
 0.0479  0.0207 -0.3426 -0.1567  0.2830
 0.0925 -0.4324  0.0448 -0.0039  0.1531
-0.2924 -0.0009 -0.1841  0.2028  0.1586
-0.3064 -0.4006 -0.0553 -0.0067  0.2575
-0.0472  0.1238 -0.3583  0.4426 -0.0269
-0.0275 -0.0295 -0.2687  0.2236  0.3787
-0.2617 -0.2221  0.1503 -0.0627  0.1094
 0.0122  0.2041  0.4466  0.4112  0.4168
-0.4362 -0.3390  0.3679 -0.3045  0.1358
 0.2979  0.0023  0.0695 -0.1906 -0.1526
[ CPUFloatType{16,5} ]
```

```r
l$bias
```

```r
torch_tensor
-0.2314
 0.2942
 0.0567
-0.1728
-0.3220
-0.1553
-0.4149
-0.2103
-0.1769
 0.4219
-0.3368
 0.0689
 0.3625
-0.1391
-0.1411
-0.2014
[ CPUFloatType{16} ]
```

在这一点上，我需要请求你的宽容。你可能已经注意到 `torch` 报告的权重矩阵大小为 16 x 5，而不是我们所说的 5 x 16。这是由于从底层 C++ 实现 `libtorch` 继承的实现细节。出于性能原因，`libtorch` 的线性模块以 *转置* 形式存储权重和偏置张量。在 R 上，我们所能做的就是明确地指出它，并希望因此减轻混淆。

让我们继续。要将这个模块应用于输入数据，只需像函数一样“调用”它：

```r
x <- torch_randn(50, 5)
output <- l(x)
output$size()
```

```r
[1] 50 16
```

所以这就是前向传播。梯度计算呢？之前，当创建一个用作梯度计算“源”的张量时，我们必须明确地让 `torch` 知道，通过传递 `requires_grad = TRUE`。对于内置的 `nn_module()`，不需要这样做。我们可以立即检查 `output` 在 `backward()` 上知道该怎么做：

```r
output$grad_fn
```

```r
AddmmBackward0
```

为了确保，让我们基于 `output` 计算一些“虚拟”损失，并调用 `backward()`。我们看到现在，线性模块的 `weight` 张量已经填充了其 `grad` 字段：

```r
loss <- output$mean()
loss$backward()
l$weight$grad
```

```r
torch_tensor
0.01 *
-0.3064  2.4118 -0.6095  0.3419 -1.6131
 -0.3064  2.4118 -0.6095  0.3419 -1.6131
 -0.3064  2.4118 -0.6095  0.3419 -1.6131
 -0.3064  2.4118 -0.6095  0.3419 -1.6131
 -0.3064  2.4118 -0.6095  0.3419 -1.6131
 -0.3064  2.4118 -0.6095  0.3419 -1.6131
 -0.3064  2.4118 -0.6095  0.3419 -1.6131
 -0.3064  2.4118 -0.6095  0.3419 -1.6131
 -0.3064  2.4118 -0.6095  0.3419 -1.6131
 -0.3064  2.4118 -0.6095  0.3419 -1.6131
 -0.3064  2.4118 -0.6095  0.3419 -1.6131
 -0.3064  2.4118 -0.6095  0.3419 -1.6131
 -0.3064  2.4118 -0.6095  0.3419 -1.6131
 -0.3064  2.4118 -0.6095  0.3419 -1.6131
 -0.3064  2.4118 -0.6095  0.3419 -1.6131
 -0.3064  2.4118 -0.6095  0.3419 -1.6131
[ CPUFloatType{16,5} ]
```

因此，一旦你开始使用 `nn_module`s，`torch` 就会自动假设你想要计算梯度。

`nn_linear()`，尽管它可能很简单，但它是大多数模型架构中遇到的基本构建块。其他还包括：

+   `nn_conv1d()`, `nn_conv2d()`, 和 `nn_conv3d()`，所谓的 *卷积* 层，它们将滤波器应用于不同维度的输入数据，

+   `nn_lstm()` 和 `nn_gru()`，携带状态的 *循环* 层，

+   `nn_embedding()`，用于将分类数据嵌入到高维空间中，

+   and more.
  
## 7.2 构建模型

内置的 `nn_module()` 给我们 *层*，用通常的话说。我们如何将这些组合成 *模型*？使用“工厂函数” `nn_module()`，我们可以定义任意复杂性的模型。但并不总是需要这样做。

### 7.2.1 模型作为层的序列：`nn_sequential()`index{`nn_sequential()`}

如果我们的模型只需要在层之间直接传播，我们可以使用 `nn_sequential()` 来构建它。由所有线性层组成的模型被称为 *多层感知器*index{多层感知器 (MLP)} (MLPs)。这里有一个例子：

```r
mlp <- nn_sequential(
 nn_linear(10, 32),
 nn_relu(),
 nn_linear(32, 64),
 nn_relu(),
 nn_linear(64, 1)
)
```

*仔细看看涉及的层。我们已经看到了 `nnf_relu()`，实现 ReLU 激活的 *函数*。（`nnf_` 中的 `f` 代表 functional。）下面，`nn_relu`，就像 `nn_linear()` 一样，是一个模块，即一个对象。这是因为 `nn_sequential()` 预期所有其参数都是模块。

就像内置模块一样，你可以通过 *调用* 它来应用这个模型到数据上：

```r
mlp(torch_randn(5, 10))
```

```r
torch_tensor
0.01 *
-7.8097
 -9.0363
 -38.3282
  5.3959
 -16.4837
[ CPUFloatType{5,1} ][ grad_fn = <AddmmBackward0> ]
```

单次调用触发了整个网络的前向传播。类似地，调用 `backward()` 将会反向传播通过所有层。

如果你需要模型以非顺序方式链式执行步骤呢？**  **### 7.2.2 带有自定义逻辑的模型

正如之前所暗示的，这就是你使用`nn_module()`的地方。

`nn_module()`为自定义的 R6 对象创建构造函数。下面，`my_linear()`就是一个这样的构造函数。当被调用时，它将返回一个类似于内置的`nn_linear()`的线性模块。

在定义构造函数时应该实现两种方法：`initialize()`和`forward()`。`initialize()`创建模块对象的字段，即它“拥有”并可以从其任何方法内部访问的对象或值。`forward()`定义当模块被调用在输入上时应该发生什么：

```r
my_linear <- nn_module(
 initialize = function(in_features, out_features) {
 self$w <- nn_parameter(torch_randn(
 in_features, out_features
 ))
 self$b <- nn_parameter(torch_zeros(out_features))
 },
 forward = function(input) {
 input$mm(self$w) + self$b
 }
)
```

*注意`nn_parameter()`的使用。`nn_parameter()`确保传递的 tensor 被注册为模块*参数*，因此默认情况下将受到反向传播的影响。

要实例化新定义的模块，调用其构造函数：

```r
l <- my_linear(7, 1)
l
```

```r
An `nn_module` containing 8 parameters.

Parameters ────────────────────────────────────────────────────────────────────────────────────────────
● w: Float [1:7, 1:1]
● b: Float [1:1]
```

承认，在这个例子中，我们确实没有需要定义自己的模块的*自定义逻辑*。但在这里，你有一个适用于任何用例的模板。稍后，我们将看到更复杂的`initialize()`和`forward()`定义，并且会遇到在模块上定义的额外方法。但基本机制将保持不变。

在这个阶段，你可能觉得想要用模块重写上一章中的神经网络。请随意这样做！或者，也许你可以等到下一章，我们学习了*优化器*和内置损失函数之后。完成这些后，我们将回到我们的两个例子，函数最小化和回归网络。然后，我们将移除所有由`torch`生成的冗余的自定义部分。

