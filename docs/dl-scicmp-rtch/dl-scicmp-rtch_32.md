# 25  矩阵计算：卷积

> 原文：[`skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/matrix_computations_convolution.html`](https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/matrix_computations_convolution.html)

在深度学习中，我们谈论卷积、卷积层和卷积神经网络。然而，正如图像处理章节中解释的那样，当我们这样做时，我们真正指的是不同的事情：交叉相关。

形式上，差异很小：符号被翻转了。语义上，这些完全不同。正如我们所见，交叉相关使我们能够发现相似性：它充当一个*特征检测器*。卷积在抽象方式上更难以描述。关于它在信号处理中扮演的显赫角色以及它的数学意义，可以写整本书。在这里，我们必须暂时放下更深层次的基础。相反，我们希望对其操作有所了解——首先，通过思考和想象涉及的步骤，其次，通过在代码中实现它。正如前一章所做的那样，重点是理解，并为进一步的探索打下基础，如果你有这个意向的话。

## 25.1 为什么是卷积？

在信号处理中，*滤波器*被用来以某种期望的方式修改信号——例如，截断高频。想象一下，你有一个时间序列的傅里叶变换表示；这意味着一组具有相关幅度和相位的频率。你希望将所有高于某个阈值的频率设置为零。最简单的方法是将频率集乘以一个由一和零组成的序列。如果你这样做，滤波就是在频域中进行的，而且通常这是最方便的方法。

那么，如果在时域中达到相同的结果——也就是说，处理原始时间序列呢？在这种情况下，你将不得不找到滤波器在时域中的表示（通过*逆傅里叶变换*实现）。然后，这个表示必须与时间序列进行*卷积*。换句话说，时域中的卷积对应于频域中的乘法。这个基本事实经常被利用。

现在，让我们更好地理解卷积的作用以及它是如何实现的。我们从一维开始，然后探索一下二维情况中发生的事情。

## 25.2 一维卷积

我们首先创建一个简单的信号`x`和一个简单的滤波器`h`。这种变量名的选择并非一时兴起；在信号处理中，`h`是表示*脉冲响应*的常用符号，我们很快就会接触到这个术语。

```r
library(torch)

x <- torch_arange(start = 1, end = 4) 
h <- torch_tensor(c(-1, 0, 1))
```

*现在——既然我们确实有`torch_conv1d()`可用——为什么我们不调用它并看看会发生什么？根据卷积的定义，输出长度等于输入长度加滤波器长度减一。使用`torch_conv1d()`，为了获得长度为六的输出，给定长度为三的滤波器，我们需要在两侧填充两个。

在下面的代码中，不要让对`view()`的调用分散你的注意力——它们的存在只是因为`torch`期望三维输入，其中维度一和二与批处理项和通道相关，就像往常一样。

```r
torch_conv1d(
 x$view(c(1, 1, 4)),
 h$view(c(1, 1, 3)),
 padding = 2
)
```

```r
torch_tensor
(1,.,.) = 
  1  2  2  2 -3 -4
[ CPUFloatType{1,1,6} ]
```

但是，你可能正在想——我们不是说过`torch_conv1d()`计算的是交叉相关而不是卷积吗？好吧，R 有`convolve()`——让我们双重检查：¹

```r
x_ <- as.numeric(x)
h_ <- as.numeric(h)

convolve(x_, h_, type = "open")
```

```r
[1]  1  2  2  2 -3 -4
```

结果是相同的。然而，查看`convolve()`的文档，我们看到：

> 注意，两个序列`x`和`y`的卷积的通常定义是`convolve(x, rev(y), type = "o")`。

显然，我们需要反转滤波器中项的顺序：

```r
convolve(x_, rev(h_), type = "open")
```

```r
[1] -1 -2 -2 -2  3  4
```

事实上，结果现在不同了。让我们用`torch_conv1d()`做同样的事情：

```r
torch_conv1d(
 x$view(c(1, 1, 4)),
 h$flip(1)$view(c(1, 1, 3)),
 padding = 2
)
```

```r
torch_tensor
(1,.,.) = 
 -1 -2 -2 -2  3  4
[ CPUFloatType{1,1,6} ]
```

再次，`torch`和 R 的结果是相同的。所以：在`convolve()`文档的“详细信息”部分中找到的这个简洁短语，捕捉了交叉相关和卷积之间的完整差异：在卷积中，第二个参数是反转的。或者用信号处理的话说，是*翻转*（“翻转”实际上是一个更好的术语，因为它可以推广到更高维度。）

技术上，差异很小——只是符号的改变。但从数学上讲，这是至关重要的——在意义上，它直接来源于滤波器*是什么*，以及它*做什么*。我们很快就能对此有所了解。

卷积操作的基本原理可以用两种方式来理解。

### 25.2.1 思考卷积的两种方式

首先，我们可以观察单个输出值，并确定它是如何产生的。也就是说，我们询问哪些输入元素对其值有贡献，以及这些元素是如何被组合的。这可以称为“输出视图”，这是我们已经在交叉相关中熟悉的一种视图。

至于交叉相关，我们这样描述它。一个滤波器“滑动”过图像，并在每个图像位置（像素）处，我们将周围输入像素与相应的“覆盖”滤波器值的乘积相加。换句话说，每个输出像素是由计算匹配输入和滤波器值的*点积*得到的。

另一种看待事物的方式是从输入的角度（因此命名为“输入视图”）。它询问：每个输入值是如何影响输出的？这种视图比第一种更难适应；但也许这只是社会化的问题——在神经网络环境中通常介绍这个主题的方式。无论如何，输入视图非常有教育意义，因为它使我们能够了解卷积的数学*意义*。

我们将查看两者，从更熟悉的一个开始，即输出视图。

#### 25.2.1.1 输出视图

在输出视图中，我们首先在输入信号的两边填充，就像我们调用 `torch_conv2d()` 时使用 `padding = 2` 一样。按照要求，我们翻转冲激响应，将其变为 `1, 0, -1`。然后，我们想象“滑动”。

下面，你可以在表格形式中找到这个可视化（表 25.1）。最后一行持有从每个位置单独乘积的总和中得到的结果。

表 25.1：卷积：输出视图。

| 信号 | 翻转 IR |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- |
| `0` | `1` |  |  |  |  |  |
| `0` | `0` | `1` |  |  |  |  |
| `1` | `-1` | `0` | `1` |  |  |  |
| `2` |  | `-1` | `0` | `1` |  |  |
| `3` |  |  | `-1` | `0` | `1` |  |
| `4` |  |  |  | `-1` | `0` | `1` |
| `0` |  |  |  |  | `-1` | `0` |
| `0` |  |  |  |  |  | `-1` |
| **结果** | `-1` | `-2` | `-2` | `-2` | `3` | `4` |

在我们对该主题的所有讨论之后，这种表示应该不会带来太多惊喜。接下来是输入视图。

#### 25.2.1.2 输入视图

输入视图的关键在于我们如何概念化输入信号：每个单独的元素被视为一个 – *缩放* 和 *平移 – 冲激*。

冲激由单位样本（或：冲激）函数，delta ($\delta$) 给出。此函数在除零以外的所有地方为零，在零处其值为一：

$$ \delta [n]={\begin{cases}1\ \ \ if \ n=0\\0\ \ \ if \ n \ne 0\end{cases}} $$

这就像一个克罗内克δ，$\delta_{ij}$²，其中一个索引被固定在 0：

$$ \delta [n]= \delta _{n0}= \delta _{0n} $$

因此，仅使用该函数，$\delta[n]$ – 其中 $n$ 代表离散时间，例如 – 我们可以精确表示一个信号值，即在时间 $n = 0$ 的那个值³，并且它唯一的可能值是 `1`。现在我们添加 *缩放* 和 *平移* 操作。

+   通过缩放，我们可以在 $n = 0$ 处产生任何值；例如：$x_0 = 0 * \delta [n]$。

+   通过平移，我们可以影响其他时间点的值。例如，时间 $n = 3$ 可以表示为 $\delta [n - 3]= \delta _{n3}$，因为 $n - 3 = 0$。

+   结合两者，我们可以在任何时间点表示任何值。例如：$x_5 = 1.11 * \delta [n - 5]$。

到目前为止，我们只谈论了信号。那么滤波器呢？就像冲激对于表征信号是至关重要的，滤波器完全由其 *冲激响应*⁴ 描述。根据定义，冲激响应是当输入是冲激（即在时间 $n = 0$ 发生）时输出的内容。与信号使用的符号类似，用 $h$ 表示冲激响应，我们有：

$$ h[n] = h[n- 0] \equiv h(\delta[n- 0]) $$

在我们的例子中，这将是一个序列 `-1, 0, 1`。但就像信号需要表示在额外的时刻，而不仅仅是 $0$ 时刻一样，滤波器也必须适用于其他位置。为此，再次使用移位操作，并以类似的方式形式化：例如，$h[n - 1]$ 表示滤波器应用于时间 $1$，即 $n - 1$ 等于零的时间。这些移位对应于我们非正式所说的“滑动”。

现在，剩下要做的就是组合这些部分。在 $n = 0$ 时刻，我们取未移位的冲激响应，并 *缩放* 它以匹配信号的幅度。在我们的例子中，这个值是 $1$。因此：$1 * h[n - 0] = 1 * [-1, 0, 1] = [-1, 0, 1]$。对于其他时间，我们将冲激响应移位到相应的输入位置，并相乘。最后，一旦我们从所有输入位置获得了所有贡献，我们将它们相加，从而得到卷积输出。

下表旨在说明这一点（表 25.2）：

表 25.2：卷积：输入视图。

| 信号 | 冲激响应 | 积 |
| --- | --- | --- |
| `1` | `h[n - 0]` | `-1 0 1 0 0 0` |
| `2` | `h[n - 1]` | `0 -2 0 2 0 0` |
| `3` | `h[n - 2]` | `0 0 -3 0 3 0` |
| `4` | `h[n - 3]` | `0 0 0 -4 0 4` |
| **总和** |  | `-1 -2 -2 -2 3 4` |

个人而言，虽然我发现输出视图更容易理解，但我感觉我可以从输入视图中获得更多的见解。特别是，它回答了——不可避免的——问题：那么 *为什么* 我们要翻转冲激响应？

结果表明，这并非由于任何神秘的力量，负号仅仅是由于 *信号表示方式* 的机械结果：在 $n = 2$ 时刻测量的信号表示为 $\delta [n - 2]$（两个减去两个等于零）；相应地，应用于该信号的滤波器为 $h[n -2]$。

### 25.2.2 实现

从我描述的输出视图来看，你可能认为关于如何编码这个视图没有太多可说的。看起来很简单：遍历输入向量，并在每个预期的输出位置计算点积。但那样做意味着要计算许多向量乘积，输入序列越长，计算的量就越大。

幸运的是，有一种更好的方法。一维（线性）卷积是通过托普利茨矩阵来计算的，这些矩阵有一些常数对角线，其他地方都是零。一旦滤波器被表示为托普利茨矩阵，就只需要进行一次乘法运算：即托普利茨矩阵和输入的乘法。即使矩阵需要与输入值一样多的列（否则我们无法进行乘法），但由于矩阵“几乎为空”，计算成本很小。

这里是一个为我们的示例构建的托普利茨矩阵：

```r
h <-torch_tensor(
 rbind(c(-1, 0, 0, 0),
 c(0, -1, 0, 0),
 c(1, 0, -1, 0),
 c(0, 1, 0, -1),
 c(0, 0, 1, 0),
 c(0, 0, 0, 1)
 ))
h
```

```r
torch_tensor
-1  0  0  0
 0 -1  0  0
 1  0 -1  0
 0  1  0 -1
 0  0  1  0
 0  0  0  1
[ CPUFloatType{6,4} ]
```

让我们用我们的示例输入来检查乘法是否得到预期的结果：

```r
h$matmul(x)
```

```r
torch_tensor
-1
-2
-2
-2
 3
 4
[ CPUFloatType{6} ]
```

确实如此。现在，让我们继续到二维。在概念上没有区别，但实际的计算（无论是手工计算还是使用矩阵）要复杂得多。因此，我们将满足于展示手动计算的一部分（可推广的部分），在计算部分，我们不旨在阐明每一个细节。
  
## 25.3 二维卷积

为了说明一维和二维卷积在概念上的相似性，我们假设输出视图。

### 25.3.1 工作原理（输出视图）

这次，示例输入是二维的。它可能看起来像这样：

$$ \begin{bmatrix} 1 & 4 & 1\\ 2 & 5 & 3\\ \end{bmatrix} $$

对于滤波器也是如此。这里是一个可能的例子：

$$ \begin{bmatrix} 1 & 1\\ 1 & -1\\ \end{bmatrix} $$

我们取输出视图，即滤波器“滑动”在输入上的视图。但是，为了保持可读性，我只需挑选一个单个输出值（“像素”）进行演示。如果输入的大小是 `m1 x n1`，滤波器的大小是 `m2 x n2`，则输出的大小将是 `(m1 + m2 - 1) x (n1 + n2 - 1)`；因此，在我们的例子中将是 `3 x 4`。我将选择位置 `(0, 1)` 的值——像图像处理中一样从底部开始计数：

$$ \begin{bmatrix} . & . & . & .\\ . & . & . & .\\ . & y_{01} & . & .\\ \end{bmatrix} $$

这里是输入，以表格形式显示，这将允许我们想象非现有（负数）位置上的元素。

| 位置 (x/y) | -1 | 0 | 1 | 2 |
| --- | --- | --- | --- | --- |
| **1** |  | 1 | 4 | 1 |
| **0** |  | 2 | 5 | 3 |
| **-1** |  |  |  |  |

而在这里，滤波器的值按照相应的顺序排列：

| 位置 (x/y) | -1 | 0 | 1 | 2 |
| --- | --- | --- | --- | --- |
| **1** |  | 1 | 1 |  |
| **0** |  | 1 | -1 |  |
| **-1** |  |  |  |  |

与一维情况一样，首先要做的是翻转滤波器。翻转在这里意味着旋转一百八十度。

| 位置 (x/y) | -1 | 0 | 1 | 2 |
| --- | --- | --- | --- | --- |
| **1** |  |  |  |  |
| **0** | -1 | 1 |  |  |
| **-1** | 1 | 1 |  |  |

接下来，将滤波器移动到所需的输出位置。我们想要做的是向右移动一个单位，垂直位置保持不变。

| 位置 (x/y) | -1 | 0 | 1 | 2 |
| --- | --- | --- | --- | --- |
| **1** |  |  |  |  |
| **0** |  | -1 | 1 |  |
| **-1** |  | 1 | 1 |  |

现在我们已经准备好计算位置 `(0, 1)` 的输出值。它是所有重叠图像和滤波器值的点积：

| 位置 (x/y) | -1 | 0 | 1 | 2 |
| --- | --- | --- | --- | --- |
| **1** |  |  |  |  |
| **0** |  | -1*2=-2 | 1*5=5 |  |
| **-1** |  |  |  |  |

最终结果是 `-2 + 5 = 3`。

$$ \begin{bmatrix} . & . & . & .\\ . & . & . & .\\ . & 3 & . & .\\ \end{bmatrix} $$

所有缺失的值都可以用类似的方式计算。但我们将跳过这个练习，看看实际的计算过程。

### 25.3.2 实现方法

代码中实现二维卷积的方式再次涉及到 Toeplitz 矩阵。就像我之前说的，我们不会深入探讨为什么每一步都采取 *确切的形式* – 这里的目的是展示一个工作示例，如果你愿意，你可以在此基础上进行自己的探索。

#### 25.3.2.1 第一步：准备滤波矩阵

我们首先将滤波器填充到输出大小，`3 x 4`。

```r
0  0 0 0
1  1 0 0
1 -1 0 0
```

然后，我们为滤波器中的每一行创建一个 Toeplitz 矩阵，从底部开始。

```r
# H0
 1  0  0  
-1  1  0  
 0 -1  1  
 0  0 -1  

# H1
 1  0  0  
 1  1  0  
 0  1  1  
 0  0  1  

# H2
 0  0  0  
 0  0  0  
 0  0  0 
```

在代码中，我们有：

```r
H0 <- torch_tensor(
 cbind(
 c(1, -1, 0, 0),
 c(0, 1, -1, 0),
 c(0, 0, 1, -1)
 )
)

H1 <- torch_tensor(
 cbind(
 c(1, 1, 0, 0),
 c(0, 1, 1, 0),
 c(0, 0, 1, 1)
 )
)

H2 <- torch_tensor(0)$unsqueeze(1)
```

*接下来，这三个矩阵被组装起来，形成一个 *双重块 Toeplitz* 矩阵。如下所示：

```r
H0   0
H1  H0
H2  H1
```

编码这种方法的其中一种方式是（两次）使用 `torch_block_diag()` 来构建两个非零块，并将它们连接起来：

```r
H <- torch_cat(
 list(
 torch_block_diag(list(H0, H0)), torch_zeros(4, 6)
 )
) +
 torch_cat(
 list(
 torch_zeros(4, 6),
 torch_block_diag(list(H1, H1))
 )
 )

H
```

```r
torch_tensor
 1  0  0  0  0  0
-1  1  0  0  0  0
 0 -1  1  0  0  0
 0  0 -1  0  0  0
 1  0  0  1  0  0
 1  1  0 -1  1  0
 0  1  1  0 -1  1
 0  0  1  0  0 -1
 0  0  0  1  0  0
 0  0  0  1  1  0
 0  0  0  0  1  1
 0  0  0  0  0  1
[ CPUFloatType{12,6} ]
```

最终矩阵有两个非零的“带”，由两个全零对角线分隔。这是矩阵乘法所需的滤波器的最终形式。**  **#### 25.3.2.2 第二步：准备输入

为了与这个 `12 x 6` 矩阵相乘，输入需要被展平成一个向量。同样，我们也是逐行进行，从底部开始。

```r
x0 <- torch_tensor(c(2, 5, 3)) 
x1 <- torch_tensor(c(1, 4, 1))

x <- torch_cat(list(x0, x1))
x
```

```r
torch_tensor
 2
 5
 3
 1
 4
 1
[ CPUFloatType{6} ]
```*  *#### 25.3.2.3 第三步：相乘

到现在为止，卷积已经变成了直接的矩阵乘法：

```r
y <- H$matmul(x)
y
```

```r
torch_tensor
  2
  3
 -2
 -3
  3
 10
  5
  2
  1
  5
  5
  1
[ CPUFloatType{12} ]
```

剩下的只是将输出重塑为正确的二维结构。按照顺序构建行（再次，从底部开始）我们得到：

$$ \begin{bmatrix} 1 & 5 & 5 & 1\\ 3 & 10 & 5 & 2\\ 2 & 3 & -2 & -3\\ \end{bmatrix} $$

查看元素 `(0, 1)`，我们看到计算结果证实了我们的手动计算。

通过以上内容，我们使用 `torch` 完成了矩阵计算的话题。但是，当我们转向下一个话题，傅里叶变换时，我们实际上并不会走得太远。记得我们之前说过，时域卷积对应于频域乘法？

这种对应关系通常用于加速计算：输入数据被傅里叶变换，结果与滤波器相乘，然后过滤后的频域表示再次变换回来。只需查看 R 的 `convolve()` 文档。它直接开始声明：

> 使用快速傅里叶变换来计算两个序列的几种卷积。

接下来是傅里叶变换！
  
 * *

1.  将 `type = "open"` 作为参数传递，以请求线性而不是循环卷积。↩︎

1.  克罗内克δ，$\delta_{ij}$，当 $i$ 等于 $j$ 时评估为 1，否则为 0。↩︎

1.  我使用 $n$ 而不是 $t$ 来索引不同的位置，因为信号 – 就像任何数字化的信号一样 – 只在离散的时间（或空间）点上“存在”。在某些上下文中，这读起来有点不自然，但至少是一致的。↩︎

1.  就像本章的每个地方一样，当我提到滤波器时，我只考虑线性时不变系统。卷积操作中限制为时不变系统是固有的。↩︎

