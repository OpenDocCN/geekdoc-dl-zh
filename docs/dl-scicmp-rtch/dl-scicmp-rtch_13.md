# 10 函数最小化与 L-BFGS

> 原文：[`skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/optim_2.html`](https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/optim_2.html)

现在我们已经熟悉了`torch`模块和优化器，我们可以回到我们之前没有使用它们的两个任务：函数最小化和训练神经网络。再次，我们以最小化开始，将网络留到下一章。

回想一下我们在最小化 Rosenbrock 函数时所做的，本质上就是：

1.  定义一个张量来保存要优化的参数，即函数取得最小值时的$\mathbf{x}$-位置。

1.  逐步更新参数，减去当前梯度的部分。

虽然作为一个策略，这很简单，但仍然存在一个问题：我们应该减去梯度的一部分有多大？这正是优化器发挥作用的地方。

## 10.1 了解 L-BFGS

到目前为止，我们只讨论了在深度学习中常用的一些优化器——随机梯度下降（SGD）、带有动量的 SGD 以及来自**自适应
学习率**家族的一些经典算法：RMSProp、Adadelta、Adagrad、Adam。所有这些算法有一个共同点：它们只利用了**梯度**，即一阶导数的向量。因此，它们都是**一阶**算法。然而，这意味着它们错过了由**海森矩阵**，即二阶导数的矩阵提供的有用信息。

### 10.1.1 斜率变化

一阶导数告诉我们关于景观的**斜率**：它是上升还是下降？上升或下降了多少？进一步来说，二阶导数编码了斜率**变化**的程度。

为什么这很重要？

假设我们处于点$\mathbf{x}_n$，并且刚刚决定了一个合适的下降方向。我们迈出一步，步长由某个预先选择的学习率决定，所有这些都设定为到达点$\mathbf{x}_{n+1}$。我们不知道当我们到达那里时斜率会有多大的变化。也许它在中间变得非常平坦：在这种情况下，我们会走得太远，超出了目标区域，最终落在远离目标区域的地方，那里可能发生任何事情（包括斜率再次上升！）。

我们可以在一个单变量函数上说明这一点。例如，取一个抛物线，如

$$ y = 10x² $$

其导数是$\frac{dy}{dx} = 20x$。如果我们当前的$x$是，比如说，$3$，并且我们使用一个学习率$0.1$，我们将减去$20 * 3 * 0.1= 6$，最终到达$-3$。

但假设我们在$2$处放慢了速度并检查了当前的斜率。我们会看到那里的斜率不那么陡峭；事实上，当在那个点时，我们只需减去$20 * 2 * 0.1= 4$。

仅凭运气，这种“闭眼跳”策略仍然可以奏效——*前提是我们恰好使用了针对该函数的正确学习率*。（在所选的学习率下，例如对于不同的抛物线 $y = 5x²$，这将是这种情况。）但一开始就包含二阶导数在决策中不是更有意义吗？

执行此操作的算法构成了牛顿方法家族。首先，我们来看它们的“最纯粹”的样本，它最好地说明了原理，但在实践中很少可行。

### 10.1.2 精确牛顿法

在高维中，精确牛顿法将梯度乘以 Hessian 的逆，从而逐个坐标地缩放下降方向。我们当前的例子只有一个独立变量；这意味着对我们来说：取一阶导数，然后除以二阶导数。

我们现在有一个缩放后的梯度——但我们应该减去它的哪一部分？在原始版本中，精确牛顿法没有使用学习率，因此使我们免于熟悉的试错游戏。让我们看看：在我们的例子中，二阶导数是 $20$，这意味着在 $x=3$ 时，我们必须减去 $(20 * 3)/20=3$。哇，我们一步就到达了 $0$，即最小值的位置。

看到结果如此之好，我们为什么不总是这样做呢？一方面，它只与二次函数，比如我们选择的演示函数，完美地工作。在其他情况下，它通常也需要一些“调整”，例如，在这里也使用学习率。

但主要原因还是另一个。在更现实的应用中，以及在机器学习和深度学习领域，每一步都计算 Hessian 的逆是非常昂贵的。（实际上，可能甚至不可能做到。）这就是近似方法，也称为准牛顿方法，发挥作用的地方。

### 10.1.3 近似牛顿：BFGS 和 L-BFGS

在近似牛顿方法中，最常用的可能是 *Broyden-Goldfarb-Fletcher-Shanno* 算法，或称 *BFGS*。它不是持续计算 Hessian 的精确逆，而是保持该逆的迭代更新近似。BFGS 通常以更节省内存的版本实现，称为 *Limited-Memory BFGS* (*L-BFGS*)。这是作为核心 `torch` 优化器的一部分提供的。 

然而，在我们到达那里之前，还有一个最后的概念问题需要讨论。

### 10.1.4 梯度搜索

与它们的精确对应物一样，近似牛顿方法可以在没有学习率的情况下工作。在这种情况下，它们计算一个下降方向，并按照缩放后的梯度直接跟随。我们已经讨论了，根据所涉及函数的不同，这可以工作得更好或更差。当它不起作用时，可以采取两种措施：首先，采取小步，或者换句话说，引入学习率。其次，进行 *梯度搜索*。

使用线搜索，我们花费一些时间来评估跟随下降方向多远。有两种主要的方法来做这件事。

第一个，*精确*的线搜索涉及另一个优化问题：取当前点，计算下降方向，并将它们作为给定值硬编码在仅依赖于学习率的第二个函数中。然后，对函数进行微分以找到*它的*最小值。这个解将是优化步长的学习率。

另一种策略是进行近似搜索。到现在为止，你可能不会感到惊讶：正如近似牛顿法比精确牛顿法更现实可行一样，近似线搜索比精确线搜索更实用。

对于线搜索，近似最佳解决方案意味着遵循一系列经过验证的启发式方法。本质上，我们寻找的是*刚好**足够好*的东西。其中最成熟的启发式方法之一是*强沃尔夫条件*，这是 `torch` 的 `optim_lbfgs()` 中实现的策略。在下一节中，我们将看到如何使用 `optim_lbfgs()` 来最小化 Rosenbrock 函数，无论是带线搜索还是不带线搜索。

## 10.2 使用 `optim_lbfgs()` 最小化 Rosenbrock 函数

再次给出 Rosenbrock 函数：

```r
library(torch)

a <- 1
b <- 5

rosenbrock <- function(x) {
 x1 <- x[1]
 x2 <- x[2]
 (a - x1)² + b * (x2 - x1²)²
}
```

*在我们的手动最小化努力中，程序如下。一次性操作，我们首先定义了参数张量，该张量将用于存储当前的 $\mathbf{x}$：

```r
x <- torch_tensor(c(-1, 1), requires_grad = TRUE)
```

然后，我们迭代执行以下操作：

1.  计算当前 $\mathbf{x}$ 的函数值。

1.  计算该值在所问位置处的梯度。

1.  从当前 $\mathbf{x}$ 中减去梯度的一部分。

那么，这个蓝图是如何改变的？

第一步保持不变。我们仍然有

```r
value <- rosenbrock(x)
```

*第二步也保持不变。我们仍然直接在输出张量上调用 `backward()`：

```r
value$backward()
```

*这是因为优化器并不*计算*梯度；它在梯度计算出来后，*决定如何处理梯度*。

因此，改变的是第三步，也是最繁琐的一步。现在，更新操作由优化器执行。为了能够这样做，有一个先决条件：在开始循环之前，需要告诉优化器它应该处理哪个参数。实际上，这一点非常重要，以至于你甚至不能在没有传递该参数的情况下创建优化器：

```r
opt <- optim_lbfgs(x)
```

*在循环中，我们现在调用优化器对象的 `step()` 方法来更新参数。我们手动程序中只有一个部分需要转移到新的方式：我们仍然需要在每次迭代中清零梯度。只是这次，不是在参数张量 `x` 上，而是在优化器对象本身上。*原则上*，这将在每次迭代上执行以下操作：

```r
value <- rosenbrock(x)

opt$zero_grad()
value$backward()

opt$step()
```

*为什么是“原则上”？事实上，这就是我们为每个优化器*除了* `optim_lbfgs()` *之外会写的代码。

对于 `optim_lbfgs()`，需要传递一个匿名函数，一个闭包来调用 `step()`。前向梯度的归零、函数调用和梯度计算，所有这些都在闭包内部发生：

```r
calc_loss <- function() {
 optimizer$zero_grad()
 value <- rosenbrock(x_star)
 value$backward()
 value
}
```

*执行了这些操作后，闭包返回函数值。以下是 `step()` 如何调用它的：

```r
for (i in 1:num_iterations) {
 optimizer$step(calc_loss)
}
```

*现在我们将所有这些放在一起，添加一些日志输出，并比较使用和不使用线搜索会发生什么。

### 10.2.1 `optim_lbfgs()` 默认行为

作为基线，我们首先不使用线搜索运行。两次迭代就足够了。在下面的输出中，你可以看到在每次迭代中，闭包被评估多次。这就是我们最初必须创建它的技术原因。

```r
num_iterations <- 2

x <- torch_tensor(c(-1, 1), requires_grad = TRUE)

optimizer <- optim_lbfgs(x)

calc_loss <- function() {
 optimizer$zero_grad()

 value <- rosenbrock(x)
 cat("Value is: ", as.numeric(value), "\n")

 value$backward()
 value
}

for (i in 1:num_iterations) {
 cat("\nIteration: ", i, "\n")
 optimizer$step(calc_loss)
}
```

```r
Iteration:  1 
Value is:  4 
Value is:  6 
Value is:  318.0431 
Value is:  5.146369 
Value is:  4.443705 
Value is:  0.8787204 
Value is:  0.8543001 
Value is:  2.001667 
Value is:  0.5656172 
Value is:  0.400589 
Value is:  7.726219 
Value is:  0.3388008 
Value is:  0.2861604 
Value is:  1.951176 
Value is:  0.2071857 
Value is:  0.150776 
Value is:  0.411357 
Value is:  0.08056168 
Value is:  0.04880721 
Value is:  0.0302862 

Iteration:  2 
Value is:  0.01697086 
Value is:  0.01124081 
Value is:  0.0006622815 
Value is:  3.300996e-05 
Value is:  1.35731e-07 
Value is:  1.111701e-09 
Value is:  4.547474e-12 
```

为了确保我们真的找到了最小值，我们检查 `x`：

```r
x
```

```r
torch_tensor
 1.0000
 1.0000
[ CPUFloatType{2} ]
```

这还能进一步改进吗？**  **### 10.2.2 `optim_lbfgs()` 使用线搜索

让我们看看。下面，唯一改变的是我们构建优化器的那一行。

```r
num_iterations <- 2

x <- torch_tensor(c(-1, 1), requires_grad = TRUE)

optimizer <- optim_lbfgs(x, line_search_fn = "strong_wolfe")

calc_loss <- function() {
 optimizer$zero_grad()

 value <- rosenbrock(x)
 cat("Value is: ", as.numeric(value), "\n")

 value$backward()
 value
}

for (i in 1:num_iterations) {
 cat("\nIteration: ", i, "\n")
 optimizer$step(calc_loss)
}
```

```r
Iteration:  1 
Value is:  4 
Value is:  6 
Value is:  3.802412 
Value is:  3.680712 
Value is:  2.883048 
Value is:  2.5165 
Value is:  2.064779 
Value is:  1.38384 
Value is:  1.073063 
Value is:  0.8844351 
Value is:  0.5554555 
Value is:  0.2501077 
Value is:  0.8948895 
Value is:  0.1619074 
Value is:  0.06823064 
Value is:  0.01653575 
Value is:  0.004060207 
Value is:  0.00353789 
Value is:  0.000391416 
Value is:  4.303527e-06 
Value is:  2.036851e-08 
Value is:  6.870948e-12 

Iteration:  2 
Value is:  6.870948e-12 
```

使用线搜索，单次迭代就足以达到最小值。检查单个损失，我们还可以看到，算法几乎每次探测函数时都会减少损失，而如果没有线搜索，情况并非如此。

