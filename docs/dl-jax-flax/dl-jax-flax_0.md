# JAX & Flax æ·±åº¦å­¦ä¹ æ•™ç¨‹

![](img/00001.jpeg)

## ä¸‹è½½æ‰€æœ‰ç¬”è®°æœ¬

ä¸‹è½½æ‰€æœ‰ç¬”è®°æœ¬çš„é“¾æ¥ã€‚ZIP æ–‡ä»¶çš„å¯†ç æ˜¯ FDCPJx0D5A6SO#%Qsg

## JAXï¼ˆå®ƒæ˜¯ä»€ä¹ˆä»¥åŠå¦‚ä½•åœ¨ Python ä¸­ä½¿ç”¨å®ƒï¼‰

JAX æ˜¯ä¸€ä¸ª Python åº“ï¼Œåˆ©ç”¨ XLA å’Œå³æ—¶ï¼ˆJITï¼‰ç¼–è¯‘æä¾›é«˜æ€§èƒ½æœºå™¨å­¦ä¹ ã€‚å…¶ API ç±»ä¼¼äº NumPyï¼Œä½†æœ‰ä¸€äº›ä¸åŒä¹‹å¤„ã€‚JAX é…å¤‡äº†æ—¨åœ¨æ”¹å–„å’Œæé«˜æœºå™¨å­¦ä¹ ç ”ç©¶é€Ÿåº¦çš„åŠŸèƒ½ã€‚è¿™äº›åŠŸèƒ½åŒ…æ‹¬ï¼š

è‡ªåŠ¨å¾®åˆ†

å‘é‡åŒ–

JIT ç¼–è¯‘

æœ¬æ–‡å°†æ¶µç›–è¿™äº›åŠŸèƒ½å’Œå…¶ä»– JAX æ¦‚å¿µã€‚è®©æˆ‘ä»¬å¼€å§‹å§ã€‚

## ä»€ä¹ˆæ˜¯ XLAï¼Ÿ

XLAï¼ˆåŠ é€Ÿçº¿æ€§ä»£æ•°ï¼‰æ˜¯ç”¨äºåŠ é€Ÿæœºå™¨å­¦ä¹ æ¨¡å‹çš„çº¿æ€§ä»£æ•°ç¼–è¯‘å™¨ã€‚å®ƒå¯¼è‡´æ¨¡å‹æ‰§è¡Œé€Ÿåº¦çš„æé«˜å’Œå†…å­˜ä½¿ç”¨é‡çš„å‡å°‘ã€‚JAXã€PyTorchã€Julia å’Œ NX éƒ½å¯ä»¥ç”Ÿæˆ XLA ç¨‹åºã€‚

## å®‰è£… JAX

JAX å¯ä»¥é€šè¿‡ Python åŒ…ç´¢å¼•è¿›è¡Œå®‰è£…ï¼š `pip install jax` JAX å·²é¢„è£…åœ¨ Google Colab ä¸Šã€‚æŸ¥çœ‹ä¸‹é¢çš„é“¾æ¥è·å–å…¶ä»–å®‰è£…é€‰é¡¹ã€‚

## åœ¨ Google Colab ä¸Šè®¾ç½® TPUs

æ‚¨éœ€è¦è®¾ç½® JAX ä»¥åœ¨ Colab ä¸Šä½¿ç”¨ TPUsã€‚é€šè¿‡æ‰§è¡Œä»¥ä¸‹ä»£ç æ¥å®Œæˆã€‚ç¡®ä¿æ‚¨å·²ç»å°†è¿è¡Œæ—¶æ›´æ”¹ä¸º TPUï¼Œæ–¹æ³•æ˜¯è½¬åˆ°è¿è¡Œæ—¶->æ›´æ”¹è¿è¡Œæ—¶ç±»å‹ã€‚å¦‚æœæ²¡æœ‰å¯ç”¨çš„åŠ é€Ÿå™¨ï¼ŒJAX å°†ä½¿ç”¨ CPUã€‚

`import jax.tools.colab_tpu jax.tools.colab_tpu.setup_tpu() jax.devices()`

## JAX ä¸­çš„æ•°æ®ç±»å‹

NumPy ä¸­çš„æ•°æ®ç±»å‹ä¸ JAX æ•°ç»„ä¸­çš„ç±»ä¼¼ã€‚ä¾‹å¦‚ï¼Œä»¥ä¸‹æ˜¯å¦‚ä½•åœ¨ JAX ä¸­åˆ›å»ºæµ®ç‚¹æ•°å’Œæ•´æ•°æ•°æ®çš„æ–¹å¼ã€‚

`import jax.numpy as jnp x = jnp.float32(1.25844) x = jnp.int32(45.25844)`

å½“æ‚¨æ£€æŸ¥æ•°æ®ç±»å‹æ—¶ï¼Œæ‚¨ä¼šå‘ç°å®ƒæ˜¯

JAX ä¸­çš„ `DeviceArray.DeviceArray` ç›¸å½“äº NumPy ä¸­çš„ `numpy.ndarray`ã€‚

`jax.numpy` æä¾›äº†ä¸ NumPy ç±»ä¼¼çš„æ¥å£ã€‚ä½†æ˜¯ï¼ŒJAX è¿˜æä¾›äº† `jax.lax`ï¼Œè¿™æ˜¯ä¸€ä¸ªæ›´å¼ºå¤§å’Œæ›´ä¸¥æ ¼çš„ä½çº§ APIã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨ `[jax.numpy]` å¯ä»¥æ·»åŠ å…·æœ‰æ··åˆç±»å‹çš„æ•°å­—ï¼Œä½† `[jax.lax]` ä¸å…è®¸è¿™æ ·åšã€‚

## åˆ›å»º JAX æ•°ç»„çš„æ–¹æ³•

æ‚¨å¯ä»¥åƒåœ¨ NumPy ä¸­ä¸€æ ·åˆ›å»º JAX æ•°ç»„ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨ï¼šarange

linspacePython lists.ones.zeros.identity.

`jnp.arange(10)`

`jnp.arange(0,10)`

`scores = [50,60,70,30,25,70] scores_array = jnp.array(scores) jnp.zeros(5)`

`jnp.ones(5)`

`jnp.eye(5)`

`jnp.identity(5)`

`![](img/00002.jpeg)`

## ä½¿ç”¨ JAX ç”Ÿæˆéšæœºæ•°

éšæœºæ•°ç”Ÿæˆæ˜¯ JAX ä¸ NumPy çš„ä¸€ä¸ªä¸»è¦åŒºåˆ«ã€‚JAX æ—¨åœ¨ä¸åŠŸèƒ½ç¨‹åºä¸€èµ·ä½¿ç”¨ã€‚JAX æœŸæœ›è¿™äº›å‡½æ•°æ˜¯çº¯å‡½æ•°ã€‚**çº¯å‡½æ•°** æ²¡æœ‰å‰¯ä½œç”¨ï¼Œå¹¶æœŸæœ›è¾“å‡ºä»…æ¥è‡ªå…¶è¾“å…¥ã€‚JAX è½¬æ¢å‡½æ•°æœŸæœ›çº¯å‡½æ•°ã€‚

å› æ­¤ï¼Œåœ¨ä½¿ç”¨ JAX æ—¶ï¼Œæ‰€æœ‰è¾“å…¥éƒ½åº”é€šè¿‡å‡½æ•°å‚æ•°ä¼ é€’ï¼Œè€Œæ‰€æœ‰è¾“å‡ºéƒ½åº”æ¥è‡ªå‡½æ•°ç»“æœã€‚å› æ­¤ï¼Œç±»ä¼¼äº Python çš„æ‰“å°å‡½æ•°ä¸æ˜¯çº¯å‡½æ•°ã€‚

çº¯å‡½æ•°åœ¨ä½¿ç”¨ç›¸åŒçš„è¾“å…¥è°ƒç”¨æ—¶è¿”å›ç›¸åŒçš„ç»“æœã€‚è¿™å¯¹äº[np.random.random()]æ˜¯ä¸å¯èƒ½çš„ï¼Œå› ä¸ºå®ƒæ˜¯æœ‰çŠ¶æ€çš„ï¼Œåœ¨å¤šæ¬¡è°ƒç”¨æ—¶è¿”å›ä¸åŒçš„ç»“æœã€‚

`print(np.random.random()) print(np.random.random()) print(np.random.random())`

`![](img/00003.jpeg)`

JAX ä½¿ç”¨éšæœºçŠ¶æ€æ¥å®ç°éšæœºæ•°ç”Ÿæˆã€‚è¿™ä¸ªéšæœºçŠ¶æ€è¢«ç§°ä¸º`keyğŸ”‘`ã€‚JAX ä»ä¼ªéšæœºæ•°ç”Ÿæˆå™¨ï¼ˆPRNGsï¼‰çŠ¶æ€ä¸­ç”Ÿæˆä¼ªéšæœºæ•°ã€‚

`seed = 98`

`key = jax.random.PRNGKey(seed) jax.random.uniform(key)`

å› æ­¤ï¼Œä½ ä¸åº”è¯¥é‡ç”¨ç›¸åŒçš„çŠ¶æ€ã€‚ç›¸åï¼Œä½ åº”è¯¥åˆ†å‰² PRNG ä»¥è·å¾—æ‰€éœ€æ•°é‡çš„å­é”®ã€‚`key, subkey = jax.random.split(key) ä½¿ç”¨ç›¸åŒçš„é”®å°†å§‹ç»ˆç”Ÿæˆç›¸åŒçš„è¾“å‡ºã€‚![](img/00004.gif)`

## çº¯å‡½æ•°

æˆ‘ä»¬å·²ç»æåˆ°çº¯å‡½æ•°çš„è¾“å‡ºåº”è¯¥åªæ¥è‡ªå‡½æ•°çš„ç»“æœã€‚å› æ­¤ï¼Œåƒ Python çš„[print]å‡½æ•°è¿™æ ·çš„ä¸œè¥¿ä¼šå¼•å…¥ä¸çº¯æ€§ã€‚è¿™å¯ä»¥é€šè¿‡è¿™ä¸ªå‡½æ•°æ¥æ¼”ç¤ºã€‚

`def impure_print_side_effect(x):`

`print("Executing function")` # è¿™æ˜¯ä¸€ä¸ªå‰¯ä½œç”¨è¿”å› x

# å‰¯ä½œç”¨å‡ºç°åœ¨ç¬¬ä¸€æ¬¡è¿è¡Œæ—¶`print ("First call: ", jax.jit(impure_print_side_effect)(4.))`

# ç›¸åŒç±»å‹å’Œå½¢çŠ¶çš„å‚æ•°è¿›è¡Œåç»­è¿è¡Œå¯èƒ½ä¸ä¼šæ˜¾ç¤ºå‰¯ä½œç”¨ã€‚

# è¿™æ˜¯å› ä¸º JAX ç°åœ¨è°ƒç”¨äº†å‡½æ•°çš„ç¼“å­˜ç¼–è¯‘

`print ("Second call: ", jax.jit(impure_print_side_effect)(5.))`

# å½“å‚æ•°çš„ç±»å‹æˆ–å½¢çŠ¶å‘ç”Ÿå˜åŒ–æ—¶ï¼ŒJAX ä¼šé‡æ–°è¿è¡Œ Python å‡½æ•°

`print ("Third call, different type: ", jax.jit(impure_print_sid e_effect)(jnp.array([5.])))`

`![](img/00005.gif)`

ç¬¬ä¸€æ¬¡æ‰§è¡Œå‡½æ•°æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ‰“å°çš„è¯­å¥ã€‚ç„¶è€Œï¼Œåœ¨è¿ç»­è¿è¡Œä¸­æˆ‘ä»¬çœ‹ä¸åˆ°è¿™ä¸ªæ‰“å°è¯­å¥ï¼Œå› ä¸ºå®ƒè¢«ç¼“å­˜äº†ã€‚åªæœ‰åœ¨æ”¹å˜æ•°æ®å½¢çŠ¶åï¼Œå¼ºåˆ¶ JAX é‡æ–°ç¼–è¯‘å‡½æ•°æ—¶ï¼Œæˆ‘ä»¬æ‰ä¼šå†æ¬¡çœ‹åˆ°è¿™ä¸ªè¯­å¥ã€‚ç¨åæˆ‘ä»¬ä¼šè¯¦ç»†è®¨è®º`jax.jit`ã€‚

## JAX NumPy æ“ä½œ

å¯¹ JAX æ•°ç»„çš„æ“ä½œç±»ä¼¼äºå¯¹ NumPy æ•°ç»„çš„æ“ä½œã€‚ä¾‹å¦‚ï¼Œä½ å¯ä»¥åƒåœ¨ NumPy ä¸­ä¸€æ ·ä½¿ç”¨[max]ã€[argmax]å’Œ[sum]ã€‚

`matrix = matrix.reshape(4,4) jnp.max(matrix)`

`jnp.argmax(matrix)`

`jnp.min(matrix)`

`jnp.argmin(matrix)`

`jnp.sum(matrix)`

`jnp.sqrt(matrix)`

`matrix.transpose()`

`![](img/00006.jpeg)`ç„¶è€Œï¼ŒJAX ä¸å…è®¸å¯¹éæ•°ç»„è¾“å…¥ï¼ˆå¦‚ NumPy ä¸­çš„ Python åˆ—è¡¨æˆ–å…ƒç»„ï¼‰è¿›è¡Œæ“ä½œï¼Œè¿™ä¼šå¯¼è‡´é”™è¯¯ã€‚

`try:`

`jnp.sum([1, 2, 3])`

`except TypeError as e:`

`print(f"TypeError: {e}")`

# `TypeError: sum requires ndarray or scalar arguments, got <c`

`lass 'list'> at position 0.`

## JAX æ•°ç»„æ˜¯ä¸å¯å˜çš„

ä¸ NumPy ä¸åŒï¼ŒJAX æ•°ç»„ä¸èƒ½å°±åœ°ä¿®æ”¹ã€‚è¿™æ˜¯å› ä¸º JAX æœŸæœ›çº¯å‡½æ•°ã€‚

`scores = [50,60,70,30,25]`

`scores_array = jnp.array(scores)`

`scores_array[0:3] = [20,40,90]`

# `TypeError: '<class 'jaxlib.xla_extension.DeviceArray'>' objec t does not support item assignment.`

# JAX æ•°ç»„æ˜¯ä¸å¯å˜çš„ã€‚è€Œä¸æ˜¯``x[idx] = y``ï¼Œè¯·ä½¿ç”¨``x

`= x.at[idx].set(y)`` æˆ–è€…å¦ä¸€ä¸ª `.at[]`

# æ–¹æ³•ï¼š`https://jax.readthedocs.io/en/latest/_autosummary/ja x.numpy.ndarray.at.html`

JAX ä¸­çš„æ•°ç»„æ›´æ–°ä½¿ç”¨`[x.at[idx].set(y)]`è¿›è¡Œã€‚è¿™å°†è¿”å›ä¸€ä¸ªæ–°æ•°ç»„ï¼Œè€Œæ—§æ•°ç»„ä¿æŒä¸å˜ã€‚

`try:`

`jnp.sum([1, 2, 3])`

`except TypeError as e:`

`print(f"TypeError: {e}")`

# `TypeError: sum éœ€è¦ ndarray æˆ–æ ‡é‡å‚æ•°ï¼Œå¾—åˆ°<c`

`åœ¨ä½ç½® 0 å¤„'list'åˆ—è¡¨ã€‚`

## **è¶Šç•Œç´¢å¼•**

`NumPy`é€šå¸¸åœ¨æ‚¨å°è¯•è·å–æ•°ç»„ä¸­è¶…å‡ºè¾¹ç•Œçš„é¡¹æ—¶ä¼šæŠ›å‡ºé”™è¯¯ã€‚JAX ä¸ä¼šæŠ›å‡ºä»»ä½•é”™è¯¯ï¼Œè€Œæ˜¯è¿”å›æ•°ç»„ä¸­çš„æœ€åä¸€é¡¹ã€‚

`matrix = jnp.arange(1,17) matrix[20]`

# `DeviceArray(16, dtype=int32)`

JAX è®¾è®¡å¦‚æ­¤ï¼Œå› ä¸ºåœ¨åŠ é€Ÿå™¨ä¸­æŠ›å‡ºé”™è¯¯å¯èƒ½ä¼šå¾ˆå…·æœ‰æŒ‘æˆ˜æ€§ã€‚

## **JAX ä¸­çš„æ•°æ®æ”¾ç½®**

JAX æ•°ç»„è¢«æ”¾ç½®åœ¨ç¬¬ä¸€ä¸ªè®¾å¤‡ä¸Šï¼Œ`[jax.devices()[0]]`å³ GPUã€TPU æˆ– CPUã€‚æ•°æ®å¯ä»¥è¢«æ”¾ç½®åœ¨ç‰¹å®šçš„è®¾å¤‡ä¸Š

ä½¿ç”¨`jax.device_put()`ã€‚

`from jax import device_put`

`import numpy as np`

`size = 5000`

`x = np.random.normal(size=(size, size)).astype(np.float32) x = device_put(x)`

æ•°æ®å°†æäº¤åˆ°è¯¥è®¾å¤‡ï¼Œå¹¶ä¸”å¯¹å…¶è¿›è¡Œçš„æ“ä½œä¹Ÿå°†æäº¤åˆ°åŒä¸€è®¾å¤‡ã€‚

## **JAX çš„é€Ÿåº¦æœ‰å¤šå¿«ï¼Ÿ**

JAX ä½¿ç”¨`asynchronous`åˆ†å‘ï¼Œè¿™æ„å‘³ç€å®ƒä¸ä¼šç­‰å¾…è®¡ç®—å®Œæˆå°±å°†æ§åˆ¶æƒäº¤è¿˜ç»™`Python ç¨‹åº`ã€‚å› æ­¤ï¼Œå½“æ‚¨æ‰§è¡Œä¸€ä¸ªæ“ä½œæ—¶ï¼ŒJAX ä¼šè¿”å›ä¸€ä¸ª futureã€‚å½“æ‚¨æƒ³è¦æ‰“å°è¾“å‡ºæˆ–å°†ç»“æœè½¬æ¢ä¸º`NumPy æ•°ç»„`æ—¶ï¼ŒJAX ä¼šå¼ºåˆ¶ Python ç­‰å¾…æ‰§è¡Œã€‚

å› æ­¤ï¼Œå¦‚æœæ‚¨æƒ³è®¡ç®—ç¨‹åºçš„æ‰§è¡Œæ—¶é—´ï¼Œæ‚¨å°†ä¸å¾—ä¸å°†ç»“æœè½¬æ¢ä¸º`NumPy`æ•°ç»„

ä½¿ç”¨`[block_until_ready()]`ç­‰å¾…æ‰§è¡Œå®Œæˆã€‚ä¸€èˆ¬æ¥è¯´ï¼Œåœ¨ CPU ä¸Šï¼Œ`NumPy`çš„æ€§èƒ½ä¼˜äº JAXï¼Œä½†åœ¨åŠ é€Ÿå™¨å’Œä½¿ç”¨ jitted å‡½æ•°æ—¶ï¼ŒJAX çš„æ€§èƒ½ä¼˜äº`NumPy`ã€‚

## **ä½¿ç”¨ jit()åŠ é€Ÿå‡½æ•°**

`[jit]`ä½¿ç”¨ XLA è¿›è¡Œå³æ—¶ç¼–è¯‘ã€‚`[jax.jit]`æœŸæœ›ä¸€ä¸ªçº¯å‡½æ•°ã€‚å‡½æ•°ä¸­çš„ä»»ä½•å‰¯ä½œç”¨å°†åªæ‰§è¡Œä¸€æ¬¡ã€‚è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªçº¯å‡½æ•°ï¼Œå¹¶è®¡ç®—å…¶åœ¨æ²¡æœ‰`jit`çš„æƒ…å†µä¸‹çš„æ‰§è¡Œæ—¶é—´ã€‚

`def test_fn(sample_rate=3000,frequency=3):`

`x = jnp.arange(sample_rate)`

`y = np.sin(2*jnp.pi*frequency * (frequency/sample_rate)) return jnp.dot(x,y)`

`%timeit test_fn()`

# `æœ€ä½³ç»“æœä¸º 5 æ¬¡ï¼šæ¯æ¬¡å¾ªç¯ 76.1 Âµs`

ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨`jit`å¹¶è®¡ç®—ç›¸åŒå‡½æ•°çš„æ‰§è¡Œæ—¶é—´ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä½¿ç”¨`jit`ä½¿æ‰§è¡Œé€Ÿåº¦å‡ ä¹å¿«äº† 20 å€ã€‚

`test_fn_jit = jax.jit(test_fn)`

`%timeit test_fn_jit().block_until_ready()` # æœ€ä½³ç»“æœä¸º 5 æ¬¡ï¼šæ¯æ¬¡å¾ªç¯ 4.54 Âµs

åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œ`[test_fn_jit]`æ˜¯å‡½æ•°çš„ jit ç¼–è¯‘ç‰ˆæœ¬ã€‚ç„¶å JAX åˆ›å»ºäº†é’ˆå¯¹ GPU æˆ– TPU ä¼˜åŒ–çš„ä»£ç ã€‚ä¼˜åŒ–åçš„ä»£ç å°†åœ¨ä¸‹æ¬¡è°ƒç”¨æ­¤å‡½æ•°æ—¶ä½¿ç”¨ã€‚

## **JIT çš„å·¥ä½œåŸç†**

JAX é€šè¿‡å°† Python å‡½æ•°è½¬æ¢ä¸ºä¸€ç§ç§°ä¸º jaxprï¼ˆJAX è¡¨è¾¾å¼ï¼‰çš„ä¸­é—´è¯­è¨€æ¥å·¥ä½œã€‚`[jax.make_jaxpr]`å¯ç”¨äºæ˜¾ç¤º Python å‡½æ•°çš„ jaxpr è¡¨ç¤ºã€‚å¦‚æœå‡½æ•°æœ‰ä»»ä½•å‰¯ä½œç”¨ï¼Œå®ƒä»¬ä¸ä¼šè¢« jaxpr è®°å½•ã€‚æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°ï¼Œä¾‹å¦‚æ‰“å°çš„ä»»ä½•å‰¯ä½œç”¨åªä¼šåœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶æ˜¾ç¤ºå‡ºæ¥ã€‚

`def sum_logistic(x):`

`print("printed x:", x)`

`return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))`

`x_small = jnp.arange(6.)print(jax.make_jaxpr(sum_logistic)(x_small))`

JAX é€šè¿‡è¿½è¸ªåˆ›å»º jaxprã€‚å‡½æ•°ä¸­çš„æ¯ä¸ªå‚æ•°éƒ½ä¼šè¢«åŒ…è£…æˆè¿½è¸ªå™¨å¯¹è±¡ã€‚è¿™äº›è¿½è¸ªå™¨çš„ç›®çš„æ˜¯åœ¨è°ƒç”¨å‡½æ•°æ—¶è®°å½•å¯¹å®ƒä»¬æ‰§è¡Œçš„æ‰€æœ‰ JAX æ“ä½œã€‚JAX ä½¿ç”¨è¿½è¸ªå™¨è®°å½•é‡å»ºå‡½æ•°ï¼Œä»è€Œå¾—åˆ° jaxprã€‚Python çš„å‰¯ä½œç”¨ä¸ä¼šå‡ºç°åœ¨ jaxpr ä¸­ï¼Œå› ä¸ºè¿½è¸ªå™¨ä¸è®°å½•å®ƒä»¬ã€‚

JAX è¦æ±‚æ•°ç»„å½¢çŠ¶åœ¨ç¼–è¯‘æ—¶æ˜¯é™æ€å’Œå·²çŸ¥çš„ã€‚å°†å¸¦æœ‰å€¼æ¡ä»¶çš„å‡½æ•°ä½¿ç”¨ jit ä¿®é¥°ä¼šå¯¼è‡´é”™è¯¯ã€‚å› æ­¤ï¼Œå¹¶éæ‰€æœ‰ä»£ç éƒ½å¯ä»¥ jit ç¼–è¯‘ã€‚

`@jax.jitdef f(boolean, x):return -x if boolean else x`

`f(True, 1)`

# `ConcretizationTypeError`: é‡åˆ°æŠ½è±¡è¿½è¸ªå™¨å€¼ï¼Œä½†éœ€è¦å…·ä½“å€¼ï¼š`Traced<ShapedArray(bool[], weak _type=True)>with<DynamicJaxprTrace(level=0/1)>`

å¯¹äºè¿™ä¸ªé—®é¢˜æœ‰å‡ ä¸ªè§£å†³æ–¹æ¡ˆï¼š

ç§»é™¤å€¼æ¡ä»¶ã€‚ä½¿ç”¨ JAX æ§åˆ¶æµæ“ä½œç¬¦å¦‚`[jax.lax.cond]`ã€‚

ä»…å¯¹å‡½æ•°çš„ä¸€éƒ¨åˆ†ä½¿ç”¨ jitã€‚ä½¿å‚æ•°é™æ€åŒ–ã€‚

æˆ‘ä»¬å¯ä»¥å®ç°æœ€åä¸€ç§é€‰é¡¹å¹¶ä½¿å¸ƒå°”å‚æ•°é™æ€åŒ–ã€‚è¿™å¯ä»¥é€šè¿‡æŒ‡å®š`[static_argnums]`æˆ–`[static_argnames]`æ¥å®Œæˆã€‚å½“é™æ€å‚æ•°çš„å€¼å˜åŒ–æ—¶ï¼Œè¿™ä¼šå¼ºåˆ¶ JAX é‡æ–°ç¼–è¯‘å‡½æ•°ã€‚å¦‚æœå‡½æ•°ä¼šè·å¾—è®¸å¤šé™æ€å‚æ•°çš„å€¼ï¼Œè¿™ä¸æ˜¯ä¸€ä¸ªå¥½ç­–ç•¥ã€‚ä½ ä¸å¸Œæœ›å‡½æ•°é‡æ–°ç¼–è¯‘å¤ªå¤šæ¬¡ã€‚

ä½ å¯ä»¥ä½¿ç”¨ Python çš„`[functools.partial]`ä¼ é€’é™æ€å‚æ•°ã€‚`from functools import partial@partial(jax.jit, static_argnums=(0,)) def f(boolean, x):return -x if boolean else xf(True, 1)`

## **ä½¿ç”¨`grad()`è®¡ç®—å¯¼æ•°**

åœ¨ JAX ä¸­è®¡ç®—å¯¼æ•°æ˜¯é€šè¿‡`jax.grad.`å®Œæˆçš„ã€‚`@jax.jitdef sum_logistic(x):return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))`

`x_small = jnp.arange(6.)`

`derivative_fn = jax.grad(sum_logistic) print(derivative_fn(x_small))`

`[grad]`å‡½æ•°æœ‰ä¸€ä¸ª`[has_aux]`å‚æ•°ï¼Œå…è®¸ä½ è¿”å›è¾…åŠ©æ•°æ®ã€‚ä¾‹å¦‚ï¼Œåœ¨æ„å»ºæœºå™¨å­¦ä¹ æ¨¡å‹æ—¶ï¼Œä½ å¯ä»¥ç”¨å®ƒè¿”å›æŸå¤±å’Œæ¢¯åº¦ã€‚

`@jax.jitdef sum_logistic(x):return jnp.sum(1.0 / (1.0 + jnp.exp(-x))),(x + 1)`

`x_small = jnp.arange(6.)`

`derivative_fn = jax.grad(sum_logistic, has_aux=True) print(derivative_fn(x_small))`

# `(DeviceArray([0.25 , 0.19661194, 0.10499357, 0.04517666, 0.01766271,`

# `0.00664806], dtype=float32), DeviceArray([1., 2.,`

`3., 4., 5., 6.], dtype=float32))`ä½¿ç”¨ **`jax.vjp()`** å’Œ **`jax.jvp()`** å¯ä»¥æ‰§è¡Œé«˜çº§è‡ªåŠ¨å¾®åˆ†ã€‚ **## ä½¿ç”¨ vmap è¿›è¡Œè‡ªåŠ¨å‘é‡åŒ–**

vmapï¼ˆå‘é‡åŒ–æ˜ å°„ï¼‰å…è®¸ä½ ç¼–å†™ä¸€ä¸ªå¯ä»¥åº”ç”¨äºå•ä¸ªæ•°æ®çš„å‡½æ•°ï¼Œç„¶å vmap å°†å…¶æ˜ å°„åˆ°ä¸€æ‰¹æ•°æ®ä¸­ã€‚å¦‚æœæ²¡æœ‰ vmapï¼Œåˆ™è§£å†³æ–¹æ¡ˆå°†æ˜¯é€šè¿‡æ‰¹å¤„ç†å¾ªç¯åº”ç”¨å‡½æ•°ã€‚åœ¨ä½¿ç”¨ jit å’Œå¾ªç¯çš„æƒ…å†µä¸‹ï¼Œè§£å†³æ–¹æ¡ˆä¼šæ›´åŠ å¤æ‚ä¸”å¯èƒ½ä¼šæ›´æ…¢ã€‚

`mat = jax.random.normal(key, (150, 100)) batched_x = jax.random.normal(key, (10, 100)) def apply_matrix(v):`

`return jnp.dot(mat, v)`

`@jax.jit`

`def vmap_batched_apply_matrix(v_batched):`

`return jax.vmap(apply_matrix)(v_batched)`

`print('ä½¿ç”¨ vmap è¿›è¡Œè‡ªåŠ¨å‘é‡åŒ–')`

`%timeit vmap_batched_apply_matrix(batched_x).block_until_ready ()`

åœ¨ JAX ä¸­ï¼Œ`[jax.vmap]` è½¬æ¢è®¾è®¡ä¸ºè‡ªåŠ¨ç”Ÿæˆå‡½æ•°çš„å‘é‡åŒ–å®ç°ã€‚å®ƒé€šè¿‡ç±»ä¼¼äº `[jax.jit]` çš„è·Ÿè¸ªåŠŸèƒ½å®ç°ï¼Œè‡ªåŠ¨åœ¨æ¯ä¸ªè¾“å…¥çš„å¼€å¤´æ·»åŠ æ‰¹å¤„ç†è½´ã€‚å¦‚æœæ‰¹å¤„ç†ç»´åº¦ä¸æ˜¯ç¬¬ä¸€ä¸ªï¼Œå¯ä»¥ä½¿ç”¨ `[in_axes]` å’Œ `[out_axes]` å‚æ•°æ¥æŒ‡å®šè¾“å…¥å’Œè¾“å‡ºä¸­æ‰¹å¤„ç†ç»´åº¦çš„ä½ç½®ã€‚å¦‚æœæ‰€æœ‰è¾“å…¥å’Œè¾“å‡ºçš„æ‰¹å¤„ç†è½´ç›¸åŒï¼Œå¯ä»¥ä½¿ç”¨æ•´æ•°ï¼Œå¦åˆ™å¯ä»¥ä½¿ç”¨åˆ—è¡¨ã€‚`Matteo Hesselï¼ŒJAX ä½œè€…ã€‚`

## ä½¿ç”¨ `[pmap]` è¿›è¡Œå¹¶è¡ŒåŒ–

`jax.pmap` çš„å·¥ä½œæ–¹å¼ç±»ä¼¼äº `jax.vmap`ã€‚ä¸åŒä¹‹å¤„åœ¨äº `jax.pmap` ç”¨äºå¹¶è¡Œæ‰§è¡Œï¼Œå³åœ¨å¤šä¸ªè®¾å¤‡ä¸Šè¿›è¡Œè®¡ç®—ã€‚è¿™åœ¨å¯¹æ•°æ®æ‰¹æ¬¡è¿›è¡Œè®­ç»ƒæ—¶æ˜¯é€‚ç”¨çš„ã€‚

è®¡ç®—åœ¨ä¸åŒè®¾å¤‡ä¸Šçš„æ‰¹å¤„ç†å¯èƒ½ä¼šå‘ç”Ÿï¼Œç„¶åå°†ç»“æœèšåˆã€‚è¢« `[pmap]` çš„å‡½æ•°è¿”å›ä¸€ä¸ª `[ShardedDeviceArray]`ã€‚è¿™æ˜¯å› ä¸ºæ•°ç»„åˆ†å¸ƒåœ¨æ‰€æœ‰è®¾å¤‡ä¸Šã€‚ä¸éœ€è¦ç”¨ jit è£…é¥°å‡½æ•°ï¼Œå› ä¸ºä½¿ç”¨ `[pmap]` æ—¶å‡½æ•°é»˜è®¤ä¸º jit ç¼–è¯‘ã€‚

`x = np.arange(5)w = np.array([2., 3., 4.])`

`def convolve(x, w):`

`output = []`

`for i in range(1, len(x)-1):`

`output.append(jnp.dot(x[i-1:i+2], w)) return jnp.array(output)`

`convolve(x, w) n_devices = jax.local_device_count()`

`xs = np.arange(5 * n_devices).reshape(-1, 5)`

`ws = np.stack([w] * n_devices)`

`jax.pmap(convolve)(xs, ws)`

# `ShardedDeviceArray([[ 11., 20., 29.],`

# `.................`

# `[326., 335., 344.]], dtype=float32)`

ä½ å¯èƒ½éœ€è¦ä½¿ç”¨å…¶ä¸­ä¸€ä¸ª `[collective operators]` æ¥èšåˆæ•°æ®ï¼Œä¾‹å¦‚ï¼Œè®¡ç®—å‡†ç¡®åº¦çš„å¹³å‡å€¼æˆ– logits çš„å¹³å‡å€¼ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ éœ€è¦æŒ‡å®šä¸€ä¸ª `[axis_name]`ã€‚è¿™ä¸ªåç§°åœ¨è®¾å¤‡ä¹‹é—´çš„é€šä¿¡ä¸­å¾ˆé‡è¦ã€‚

## åœ¨ JAX ä¸­è°ƒè¯• NANs

åœ¨ JAX ç¨‹åºä¸­ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œå‡ºç° NANs ä¸ä¼šå¯¼è‡´é”™è¯¯ã€‚`jnp.divide(0.0,0.0)# DeviceArray(nan, dtype=float32, weak_type=True)`

æ‚¨å¯ä»¥æ‰“å¼€ NAN æ£€æŸ¥å™¨ï¼Œç¨‹åºå°†åœ¨å‡ºç° NAN æ—¶æŠ¥é”™ã€‚æ‚¨åº”ä»…åœ¨è°ƒè¯•æ—¶ä½¿ç”¨ NAN æ£€æŸ¥å™¨ï¼Œå› ä¸ºå®ƒä¼šå¯¼è‡´æ€§èƒ½é—®é¢˜ã€‚æ­¤å¤–ï¼Œå®ƒä¸é€‚ç”¨äº `[pmap]`ï¼Œè¯·æ”¹ç”¨ `[vmap]`ã€‚

`from jax.config import config`

`config.update("jax_debug_nans", True)`

`jnp.divide(0.0,0.0)`

# `FloatingPointError: åœ¨é™¤æ³•ä¸­é‡åˆ°æ— æ•ˆå€¼ (nan)`

## **åŒç²¾åº¦ï¼ˆ64 ä½ï¼‰**

JAX å¼ºåˆ¶ä½¿ç”¨å•ç²¾åº¦æ•°ã€‚ä¾‹å¦‚ï¼Œåˆ›å»º `[float64]` æ•°å­—æ—¶ä¼šæ”¶åˆ°è­¦å‘Šã€‚å¦‚æœæ£€æŸ¥æ•°å­—ç±»å‹ï¼Œä¼šå‘ç°å®ƒæ˜¯ `[float32]`ã€‚

`x = jnp.float64(1.25844)`

# `/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_num py.py:1806`: UserWarning: ä½ åœ¨æ•°ç»„ä¸­æ˜¾å¼è¯·æ±‚çš„ `float64` ç±»å‹ä¸å¯ç”¨ï¼Œå°†è¢«æˆªæ–­ä¸º `float32` ç±»å‹ã€‚è¦å¯ç”¨æ›´å¤šæ•°æ®ç±»å‹ï¼Œè¯·è®¾ç½® `jax_enable_x64` é…ç½®é€‰é¡¹æˆ– `JAX_ENABLE_X64` shell ç¯å¢ƒå˜é‡ã€‚è¯¦æƒ…è¯·è§ [`github.com/google/jax#current-gotchas`](https://github.com/google/jax#current-gotchas)ã€‚ `# lax_internal._check_user_dtype_supported(dtype, "array")` `# DeviceArray(1.25844, dtype=float32)`

æ‚¨å¯ä»¥é€šè¿‡åœ¨é…ç½®ä¸­è®¾ç½® `[jax_enable_x64]` æ¥ä½¿ç”¨åŒç²¾åº¦æ•°å­—ã€‚

# `åœ¨ç¨‹åºå¼€å¤´è®¾ç½®æ­¤é…ç½® from jax.config import config`

`config.update("jax_enable_x64", True)`

`x = jnp.float64(1.25844)`

`x`

# `DeviceArray(1.25844, dtype=float64)`

## **ä»€ä¹ˆæ˜¯ pytreeï¼Ÿ**

Pytree æ˜¯ä¸€ä¸ªå®¹å™¨ï¼Œå¯ä»¥å®¹çº³ Python å¯¹è±¡ã€‚åœ¨ JAX ä¸­ï¼Œå®ƒå¯ä»¥åŒ…å«æ•°ç»„ã€å…ƒç»„ã€åˆ—è¡¨ã€å­—å…¸ç­‰ã€‚Pytree åŒ…å«å¶å­èŠ‚ç‚¹ã€‚ä¾‹å¦‚ï¼Œåœ¨ JAX ä¸­ï¼Œæ¨¡å‹å‚æ•°å°±æ˜¯ pytreeã€‚

`example_trees = [`

`[1, 'a', object()],`

`(1, (2, 3), ())`

`[1, {'k1': 2, 'k2': (3, 4)}, 5], {'a': 2, 'b': (2, 3)},`

`jnp.array([1, 2, 3]),`

`]`

# è®©æˆ‘ä»¬çœ‹çœ‹å®ƒä»¬å„è‡ªæœ‰å¤šå°‘ä¸ªå¶å­èŠ‚ç‚¹ï¼š

`for pytree in example_trees:`

`leaves = jax.tree_leaves(pytree)`

`print(f"{repr(pytree):<45} æœ‰ {len(leaves)} ä¸ªå¶å­èŠ‚ç‚¹: {leave s}")`

# `[1, 'a', <object object at 0x7f280a01f6d0>] [1, 'a', <object object at 0x7f280a01f6d0>]`

# `(1, (2, 3), ())`

`[1, 2, 3]`

# `[1, {'k1': 2, 'k2': (3, 4)}, 5]`

`[1, 2, 3, 4, 5]`

# `{'a': 2, 'b': (2, 3)}`

`[2, 2, 3]`

# `DeviceArray([1, 2, 3], dtype=int64) [DeviceArray([1, 2, 3], dtype=int64)] æœ‰ 3 ä¸ªå¶å­èŠ‚ç‚¹:`

æœ‰ 3 ä¸ªå¶å­èŠ‚ç‚¹: æœ‰ 5 ä¸ªå¶å­èŠ‚ç‚¹: æœ‰ 3 ä¸ªå¶å­èŠ‚ç‚¹: æœ‰ 1 ä¸ªå¶å­èŠ‚ç‚¹:

## **åœ¨ JAX ä¸­å¤„ç†çŠ¶æ€**

è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹é€šå¸¸æ¶‰åŠçŠ¶æ€ï¼Œä¾‹å¦‚æ¨¡å‹å‚æ•°ã€ä¼˜åŒ–å™¨çŠ¶æ€å’Œç±»ä¼¼ BatchNorm çš„æœ‰çŠ¶æ€å±‚ã€‚ç„¶è€Œï¼Œjit ç¼–è¯‘çš„å‡½æ•°å¿…é¡»æ²¡æœ‰å‰¯ä½œç”¨ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ç§æ–¹æ³•æ¥è·Ÿè¸ªå’Œæ›´æ–°æ¨¡å‹å‚æ•°ã€ä¼˜åŒ–å™¨çŠ¶æ€å’Œæœ‰çŠ¶æ€å±‚ã€‚è§£å†³æ–¹æ¡ˆæ˜¯æ˜¾å¼å®šä¹‰çŠ¶æ€ã€‚

## **ä½¿ç”¨ JAX åŠ è½½æ•°æ®é›†**

JAX å¹¶ä¸é™„å¸¦ä»»ä½•æ•°æ®åŠ è½½å·¥å…·ã€‚ä¸è¿‡ï¼ŒJAX å»ºè®®ä½¿ç”¨æ¥è‡ª `PyTorch` å’Œ `TensorFlow` çš„æ•°æ®åŠ è½½å™¨ã€‚

import `tensorflow` as `tf`

# ç¡®ä¿ TF çœ‹ä¸åˆ° GPU å¹¶æŠ¢å æ‰€æœ‰ GPU å†…å­˜ã€‚`tf.config.set_visible_devices([], device_type='GPU')`

import `tensorflow_datasets` as `tfdsdata_dir = '/tmp/tfds'`

# è·å–å®Œæ•´æ•°æ®é›†ä»¥è¿›è¡Œè¯„ä¼°

# `tfds.load è¿”å› tf.Tensorsï¼ˆå¦‚æœ batch_size != -1 åˆ™ä¸º tf.data.Datasetsï¼‰`

# æ‚¨å¯ä»¥ä½¿ç”¨ `tfds.dataset_as_numpy` å°†å…¶è½¬æ¢ä¸º NumPy æ•°ç»„ï¼ˆæˆ– NumPy æ•°ç»„çš„å¯è¿­ä»£å¯¹è±¡ï¼‰ã€‚

`mnist_data, info = tfds.load(name="mnist", batch_size=-1, data_dir=data_dir, with_info=True)`

`mnist_data = tfds.as_numpy(mnist_data)`

`train_data`, `test_data` = `mnist_data['train']`, `mnist_data['test']` `num_labels = info.features['label'].num_classes`

`h, w, c = info.features['image'].shape`

`num_pixels = h * w * c`

# å®Œæ•´çš„è®­ç»ƒé›†

`train_images`, `train_labels` = `train_data['image']`, `train_data['l abel']`

`train_images = jnp.reshape(train_images, (len(train_images), nu m_pixels))`

`train_labels = one_hot(train_labels, num_labels)`

# å®Œæ•´çš„æµ‹è¯•é›†

`test_images`, `test_labels` = `test_data['image']`, `test_data['labe l']`

`test_images = jnp.reshape(test_images, (len(test_images), num_p ixels))`

`test_labels = one_hot(test_labels, num_labels)`

`print('Train:', train_images.shape, train_labels.shape)` `print('Test:', test_images.shape, test_labels.shape)`

# Train: `(60000, 784)` `(60000, 10)` # Test: `(10000, 784)` `(10000, 10)`

## ä½¿ç”¨ JAX æ„å»ºç¥ç»ç½‘ç»œ

æ‚¨å¯ä»¥ä½¿ç”¨ JAX ä»å¤´å¼€å§‹æ„å»ºæ¨¡å‹ã€‚ç„¶è€Œï¼Œå„ç§ç¥ç»ç½‘ç»œåº“éƒ½æ˜¯åŸºäº JAX æ„å»ºçš„ï¼Œä»¥ä½¿ä½¿ç”¨ JAX æ„å»ºç¥ç»ç½‘ç»œå˜å¾—æ›´åŠ å®¹æ˜“ã€‚ã€ŠJAX å’Œ Flax å›¾åƒåˆ†ç±»ã€‹æ–‡ç« å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ PyTorch åŠ è½½æ•°æ®ï¼Œå¹¶ä½¿ç”¨ JAX å’Œ Flax æ„å»ºå·ç§¯ç¥ç»ç½‘ç»œã€‚

## æ€»ç»“æ€è€ƒ

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº† JAX çš„åŸºç¡€çŸ¥è¯†ã€‚æˆ‘ä»¬çœ‹åˆ° JAX ä½¿ç”¨ XLA å’Œå³æ—¶ç¼–è¯‘æ¥æé«˜ Python å‡½æ•°çš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä»‹ç»äº†ï¼š

åœ¨ Google Colab ä¸Šé…ç½® JAX ä»¥ä½¿ç”¨ TPUsã€‚

JAX å’Œ NumPy ä¸­æ•°æ®ç±»å‹çš„æ¯”è¾ƒã€‚åœ¨ JAX ä¸­åˆ›å»ºæ•°ç»„ã€‚

å¦‚ä½•åœ¨ JAX ä¸­ç”Ÿæˆéšæœºæ•°ã€‚

JAX æ•°ç»„ä¸Šçš„æ“ä½œã€‚

JAX ä¸­çš„æ³¨æ„äº‹é¡¹ï¼Œå¦‚ä½¿ç”¨çº¯å‡½æ•°å’Œ JAX æ•°ç»„çš„ä¸å¯å˜æ€§ã€‚

å°† JAX æ•°ç»„æ”¾å…¥ GPU æˆ– TPUã€‚

å¦‚ä½•ä½¿ç”¨ JIT åŠ é€Ÿå‡½æ•°ã€‚

... ä»¥åŠæ›´å¤šå†…å®¹

## JAX å’Œ Flax ä¸­çš„ä¼˜åŒ–å™¨

åœ¨è®­ç»ƒç¥ç»ç½‘ç»œæ—¶åº”ç”¨ä¼˜åŒ–å™¨ä»¥å‡å°‘çœŸå®å€¼å’Œé¢„æµ‹å€¼ä¹‹é—´çš„è¯¯å·®ã€‚è¿™ç§ä¼˜åŒ–é€šè¿‡æ¢¯åº¦ä¸‹é™å®Œæˆã€‚æ¢¯åº¦ä¸‹é™é€šè¿‡æˆæœ¬å‡½æ•°è°ƒæ•´ç½‘ç»œä¸­çš„è¯¯å·®ã€‚åœ¨ JAX ä¸­ï¼Œä¼˜åŒ–å™¨æ¥è‡ª Optax åº“ã€‚

ä¼˜åŒ–å™¨å¯ä»¥åˆ†ä¸ºä¸¤å¤§ç±»ï¼š

åŒ…æ‹¬ Adamã€Adagradã€AdaDelta å’Œ RMSProp åœ¨å†…çš„è‡ªé€‚åº”ä¼˜åŒ–å™¨ã€‚

åŠ é€Ÿéšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰ï¼Œä¾‹å¦‚å¸¦åŠ¨é‡çš„ SGDã€é‡çƒæ–¹æ³•ï¼ˆHBï¼‰å’Œ Nesterov åŠ é€Ÿæ¢¯åº¦ï¼ˆNAGï¼‰ã€‚

è®©æˆ‘ä»¬æ¥çœ‹çœ‹åœ¨ JAX å’Œ Flax ä¸­å¸¸ç”¨çš„ä¼˜åŒ–å™¨å‡½æ•°ã€‚

## è‡ªé€‚åº” vs éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰ä¼˜åŒ–å™¨

åœ¨æ‰§è¡Œä¼˜åŒ–æ—¶ï¼Œè‡ªé€‚åº”ä¼˜åŒ–å™¨ä»¥è¾ƒå¤§çš„æ›´æ–°æ­¥éª¤å¼€å§‹ï¼Œä½†åœ¨æ¥è¿‘å…¨å±€æœ€å°å€¼æ—¶å‡å°æ­¥é•¿ã€‚è¿™ç¡®ä¿å®ƒä»¬ä¸ä¼šé”™è¿‡å…¨å±€æœ€å°å€¼ã€‚

è‡ªé€‚åº”ä¼˜åŒ–å™¨å¦‚ Adam éå¸¸å¸¸è§ï¼Œå› ä¸ºå®ƒä»¬æ”¶æ•›é€Ÿåº¦å¿«ï¼Œä½†å¯èƒ½æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚`SGD-based optimizers apply a global learning rate on all parameters, while adaptive optimizers calculate a learning rate for each parameter.` `## AdaBelief` AdaBelief çš„ä½œè€…å¼•å…¥äº†è¿™ä¸ªä¼˜åŒ–å™¨æ¥ï¼š

ä¸è‡ªé€‚åº”æ–¹æ³•ä¸€æ ·å¿«é€Ÿæ”¶æ•›ã€‚

å…·æœ‰åƒ SGD ä¸€æ ·çš„è‰¯å¥½æ³›åŒ–èƒ½åŠ›ã€‚

åœ¨è®­ç»ƒæœŸé—´ä¿æŒç¨³å®šã€‚

`AdaBelief`Â åŸºäºå¯¹å½“å‰æ¢¯åº¦æ–¹å‘çš„â€œ**ä¿¡å¿µ**â€å·¥ä½œã€‚å¦‚æœå®ƒå¯¼è‡´è‰¯å¥½çš„æ€§èƒ½ï¼Œåˆ™ä¿¡ä»»è¯¥æ–¹å‘ï¼Œå¹¶åº”ç”¨å¤§å¹…æ›´æ–°ã€‚å¦åˆ™ï¼Œå®ƒè¢«ä¸ä¿¡ä»»ï¼Œæ­¥é•¿è¢«å‡å°ã€‚

è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸ªåº”ç”¨`AdaBelief`ä¼˜åŒ–å™¨çš„`Flax è®­ç»ƒçŠ¶æ€`ã€‚ä»`flax.training`å¯¼å…¥`train_state`

def `create_train_state`(rng, learning_rate):

"""åˆ›å»ºåˆå§‹`TrainState`ã€‚"""

cnn = `CNN()``

params = `cnn.init`(rng, `jnp.ones([1, size_image, size_image,`

`3]))['params']`

tx = `optax.adabelief`(learning_rate)

è¿”å›`train_state.TrainState.create`(

apply_fn=`cnn.apply`, params=params, tx=tx)è¿™æ˜¯`AdaBelief`åœ¨å„ç§ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œç”±å…¶ä½œè€…æä¾›ï¼[](../images/00007.jpeg)`

## **`AdaGrad`**

`AdaGrad`Â åœ¨å¯¼è‡´ç¨€ç–æ¢¯åº¦çš„æƒ…å†µä¸‹æ•ˆæœå¾ˆå¥½ã€‚`Adagrad`æ˜¯ä¸€ç§åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–ç®—æ³•ï¼Œåœ¨è®­ç»ƒæœŸé—´ä¸ºæ¯ä¸ªå‚æ•°é€€ç«å­¦ä¹ ç‡â€“Â `Optax`.

from `flax.training` import `train_state`

def `create_train_state`(rng, learning_rate):

"""åˆ›å»ºåˆå§‹`TrainState`ã€‚"""

cnn = `CNN()``

params = `cnn.init`(rng, `jnp.ones([1, size_image, size_image,`

`3]))['params']`

tx = `optax.AdaGrad`(learning_rate)

è¿”å›`train_state.TrainState.create`(

apply_fn=`cnn.apply`, params=params, tx=tx)

## **`Adam â€“ è‡ªé€‚åº”çŸ©ä¼°è®¡`**

`Adam`Â æ˜¯æ·±åº¦å­¦ä¹ ä¸­å¸¸è§çš„ä¼˜åŒ–å™¨ï¼Œå› ä¸ºå®ƒä½¿ç”¨é»˜è®¤å‚æ•°èƒ½å¤Ÿè·å¾—è‰¯å¥½çš„ç»“æœï¼Œè®¡ç®—æˆæœ¬ä½ï¼Œå†…å­˜ä½¿ç”¨å°‘ã€‚

from `flax.training` import `train_state`

def `create_train_state`(rng):

"""åˆ›å»ºåˆå§‹`TrainState`ã€‚"""

model = `LSTMModel()``

params = `model.init`(rng, `jnp.array(X_train_padded[0]))['param`

`s']`

tx = `optax.adam`(0.001,0.9,0.999,1e-07)

è¿”å›`train_state.TrainState.create`(

apply_fn=`model.apply`, params=params, tx=tx)![](img/00008.jpeg) **`åœ¨ MNIST å›¾åƒä¸Šè®­ç»ƒå¤šå±‚ç¥ç»ç½‘ç»œã€‚ï¼ˆaï¼‰ä½¿ç”¨ dropout éšæœºæ­£åˆ™åŒ–çš„ç¥ç»ç½‘ç»œã€‚ï¼ˆbï¼‰å…·æœ‰ç¡®å®šæ€§æˆæœ¬å‡½æ•°çš„ç¥ç»ç½‘ç»œ`**  **## `AdamW`**

`AdamW æ˜¯å¸¦æœ‰æƒé‡è¡°å‡æ­£åˆ™åŒ–çš„ Adamã€‚æƒé‡è¡°å‡æ­£åˆ™åŒ–é€šè¿‡æƒ©ç½šæˆæœ¬å‡½æ•°åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ä½¿æƒé‡å˜å°ã€‚è¿™å¯¼è‡´æƒé‡å˜å°ï¼Œä»è€Œå¯¼è‡´æ›´å¥½çš„æ³›åŒ–ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä¸ L2 æ­£åˆ™åŒ–çš„ Adam ç›¸æ¯”ï¼Œä½¿ç”¨è§£è€¦æƒé‡è¡°å‡çš„ Adam ä¼šå¯¼è‡´æ›´å¥½çš„ç»“æœã€‚`

from `flax.training` import `train_state`

`def `create_train_state`(rng):`

"""åˆ›å»ºåˆå§‹`TrainState`ã€‚"""

model = `LSTMModel()``

params = `model.init`(rng, `jnp.array(X_train_padded[0]))['param`

`s']tx = optax.adamw(0.001,0.9,0.999,1e-07)return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)![](img/00009.jpeg) **`å­¦ä¹ æ›²çº¿ï¼ˆé¡¶éƒ¨è¡Œï¼‰å’Œåœ¨ CIFAR-10 ä¸Šä½¿ç”¨ Adam å’Œ AdamW è®­ç»ƒçš„ 26 2x96d ResNet çš„æ³›åŒ–ç»“æœï¼ˆåº•éƒ¨è¡Œï¼‰`**  **## `RAdam â€“ çŸ«æ­£çš„ Adam ä¼˜åŒ–å™¨`** `RAdam æ—¨åœ¨è§£å†³åº”ç”¨è‡ªé€‚åº”å­¦ä¹ ç‡æ—¶åœ¨æ—©æœŸè®­ç»ƒé˜¶æ®µå‡ºç°çš„å¤§æ–¹å·®ã€‚from `flax.training` import `train_state`

`def create_train_state(rng, learning_rate):`

"""åˆ›å»ºåˆå§‹`TrainState`ã€‚"""

`cnn = CNN()`

`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`

`3]))['params']`

`tx = optax.radam(learning_rate)`

`return train_state.TrainState.create(`

`apply_fn=cnn.apply, params=params, tx=tx)`

## **AdaFactor**

`AdaFactor ç”¨äºè®­ç»ƒå¤§å‹ç¥ç»ç½‘ç»œï¼Œå› ä¸ºå®ƒå®ç°äº†å‡å°‘å†…å­˜ä½¿ç”¨ã€‚from flax.training import train_state`

`def create_train_state(rng, learning_rate):`

"""åˆ›å»ºåˆå§‹`TrainState`ã€‚"""

`cnn = CNN()`

`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`

`3]))['params']`

`tx = optax.adafactor(learning_rate)`

`return train_state.TrainState.create(`

`apply_fn=cnn.apply, params=params, tx=tx)`

## **Fromage**

`Fromage å¼•å…¥äº†ä¸€ç§ç§°ä¸ºâ€œæ·±åº¦ç›¸å¯¹ä¿¡ä»»â€çš„æ·±åº¦ç¥ç»ç½‘ç»œä¸Šçš„è·ç¦»å‡½æ•°ã€‚å®ƒå‡ ä¹ä¸éœ€è¦å­¦ä¹ ç‡è°ƒæ•´ã€‚from flax.training import train_state`

`def create_train_state(rng, learning_rate): """åˆ›å»ºåˆå§‹`TrainState`ã€‚""" cnn = CNN()`

`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`

`3]))['params']`

`tx = optax.fromage(learning_rate)`

`return train_state.TrainState.create(`

`apply_fn=cnn.apply, params=params, tx=tx)`

## **Lamb â€“ å±‚è‡ªé€‚åº”å¤§æ‰¹é‡ä¼˜åŒ–**

`Lamb æ—¨åœ¨é€šè¿‡ä½¿ç”¨å¤§å‹å°æ‰¹é‡è®¡ç®—æ¢¯åº¦æ¥è®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œã€‚å®ƒåœ¨åŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹ï¼ˆå¦‚ Transformers å’Œ ResNet-50ï¼‰ä¸Šè¡¨ç°è‰¯å¥½ã€‚`

`from flax.training import train_state`

`def create_train_state(rng, learning_rate):`

"""åˆ›å»ºåˆå§‹`TrainState`ã€‚"""

`cnn = CNN()`

`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`

`3]))['params']`

`tx = optax.lamb(learning_rate)`

`return train_state.TrainState.create(`

`apply_fn=cnn.apply, params=params, tx=tx)`

## **Lars â€“ å±‚è‡ªé€‚åº”é€Ÿç‡ç¼©æ”¾**

`LarsÂ is inspired by Lamb to scale SGD to large batch sizes. Lars has been used to train AlexNet with an 8K batch size and Resnet-50 with a 32K batch size without degrading the accuracy. from flax.training import train_state`

`def create_train_state(rng, learning_rate):`

"""Creates initial `TrainState`."""

`cnn = CNN()`

`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`

`3]))['params']`

`tx = optax.lars(learning_rate)`

`return train_state.TrainState.create(`

`apply_fn=cnn.apply, params=params, tx=tx)![](img/00010.jpeg)`**LARS: Alexnet-BN with B=8K ** **## SM3 - Square-root of Minima of Sums of Maxima of Squaredgradients Method**

`SM3Â was designed to reduce memory utilization when training very large models such as Transformer for machine translation, BERT for language modeling, and AmoebaNet-D for image classification`

`from flax.training import train_state`

`def create_train_state(rng, learning_rate):`

"""Creates initial `TrainState`."""

`cnn = CNN()`

`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`

`3]))['params']`

`tx = optax.sm3(learning_rate)`

`return train_state.TrainState.create(`

apply_fn=cnn.apply, params=params, tx=tx)![](img/00011.jpeg)**Top-1 (left) and Top-5 (right) test accuracy of AmoebaNet-D on ImageNet ** **## SGDâ€“ Stochastic Gradient Descent**

SDGÂ implements stochastic gradient descent with support for momentum and Nesterov acceleration. Momentum makes obtaining optimal model weights faster by accelerating gradient descent in a certain direction.

from flax.training import train_state

def create_train_state(rng, learning_rate):

"""Creates initial `TrainState`."""

cnn = CNN()

params = cnn.init(rng, jnp.ones([1, size_image, size_image,

3]))['params']

tx = optax.sgd(learning_rate)

return train_state.TrainState.create(

apply_fn=cnn.apply, params=params, tx=tx)

## **Noisy SGD**

Noisy SGDÂ is SGD with added noise. Adding noise to gradients can prevent overfitting and improve training error and generalization in deep architectures.

from flax.training import train_state

def create_train_state(rng, learning_rate):

"""Creates initial `TrainState`."""

cnn = CNN()

params = cnn.init(rng, jnp.ones([1, size_image, size_image,

3]))['params']

tx = optax.noisy_sgd(learning_rate)

return train_state.TrainState.create(

apply_fn=cnn.apply, params=params, tx=tx)![](img/00012.jpeg) **: Noise vs. No Noise in our experiment with tables containing 5 columns. The models trained with noise generalizes almost always better**  **## Optimistic GD** An Optimistic Gradient Descent optimizer.

*Optimistic gradient descent is an approximation of extra-gradient methods which require multiple gradient calls to compute the next update. It has strong formal guarantees for last-iterate convergence in min-max games, for which standard gradient descent can oscillate or even divergeâ€“Â Optax.*

from flax.training import train_state

def create_train_state(rng, learning_rate):

"""Creates initial `TrainState`."""

cnn = CNN()

`params = cnn.init(rng, jnp.ones([1, size_image, size_image, tx = optax.optimistic_gradient_descent(learning_rate) return train_state.TrainState.create(`

apply_fn=cnn.apply, params=params, tx=tx)

## **å·®åˆ†éšç§ SGD**

å·®åˆ†éšç§ SGD ç”¨äºè®­ç»ƒå…·æœ‰æ•æ„Ÿæ•°æ®çš„ç½‘ç»œã€‚ç¡®ä¿æ¨¡å‹ä¸ä¼šæ³„éœ²æ•æ„Ÿçš„è®­ç»ƒæ•°æ®ã€‚

`from flax.training import train_state`

`def create_train_state(rng, learning_rate):`

"""åˆ›å»ºåˆå§‹`TrainState`ã€‚"""

cnn = CNN()

`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`

3]))['params']

`tx = optax.dpsgd(learning_rate)`

è¿”å› train_state.TrainState.create(

apply_fn=cnn.apply, params=params, tx=tx)

## **RMSProp**

RMSProp é€šè¿‡å°†å…¶æœ€è¿‘å¹…åº¦çš„è¿è¡Œå¹³å‡å€¼çš„æ¢¯åº¦åˆ†å‰²æ¥å·¥ä½œ - Hinton.æ¥è‡ª flax.training import train_state

`def create_train_state(rng, learning_rate):`

"""åˆ›å»ºåˆå§‹`TrainState`ã€‚"""

cnn = CNN()

`params = cnn.init(rng, jnp.ones([1, size_image, size_image, tx = optax.rmsprop(learning_rate)`

`train_state.TrainState.create(`çš„è¿”å›

apply_fn=cnn.apply, params=params, tx=tx)

## **Yogi**

`Yogi æ˜¯ç”¨äºä¼˜åŒ–éšæœºéå‡¸ä¼˜åŒ–é—®é¢˜çš„ä¿®æ”¹ç‰ˆ Adam ä¼˜åŒ–å™¨ã€‚æ¥è‡ª flax.training import train_state`

`def create_train_state(rng, learning_rate):`

"""åˆ›å»ºåˆå§‹`TrainState`ã€‚"""

`cnn = CNN()`

`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`

`3]))['params']`

`tx = optax.yogi(learning_rate)`

`return train_state.TrainState.create(`

`apply_fn=cnn.apply, params=params, tx=tx)![](img/00013.jpeg)` **æ¯”è¾ƒé«˜åº¦è°ƒæ•´çš„ RMSProp ä¼˜åŒ–å™¨ä¸ YOGI åœ¨ Imagenet ä¸Šçš„ Inception-Resnet-v2ã€‚ç¬¬ä¸€å¼ å›¾æ˜¾ç¤ºäº†è®­ç»ƒæœŸé—´æŸå¤±çš„å°æ‰¹é‡ä¼°è®¡ï¼Œè€Œå…¶ä½™ä¸¤å¼ å›¾æ˜¾ç¤ºäº†åœ¨ä¿ç•™çš„ Imagenet éªŒè¯é›†ä¸Šçš„ top-1 å’Œ top-5 é”™è¯¯ç‡**  **## æœ€åçš„æ€è€ƒ**

é€‰æ‹©æ­£ç¡®çš„ä¼˜åŒ–å™¨å‡½æ•°å†³å®šäº†è®­ç»ƒç½‘ç»œæ‰€éœ€çš„æ—¶é—´ã€‚å®ƒè¿˜å†³å®šäº†æ¨¡å‹çš„è¡¨ç°å¦‚ä½•ã€‚å› æ­¤ï¼Œåœ¨æ„å»º JAX å’Œ Flax ç½‘ç»œæ—¶é€‰æ‹©é€‚å½“çš„ä¼˜åŒ–å™¨å‡½æ•°è‡³å…³é‡è¦ã€‚æœ¬æ–‡è®¨è®ºäº†å¯ä»¥åº”ç”¨äºæ‚¨çš„ç½‘ç»œçš„å„ç§ä¼˜åŒ–å™¨å‡½æ•°ã€‚ç‰¹åˆ«æ˜¯ï¼Œæ‚¨å°†äº†è§£ä»¥ä¸‹ä¼˜åŒ–å™¨çš„ç²¾åï¼š

JAX ä¸­çš„ Adam ä¼˜åŒ–å™¨ã€‚

Flax ä¸­çš„ RMSProp ä¼˜åŒ–å™¨ã€‚

JAX ä¸­çš„éšæœºæ¢¯åº¦ä¸‹é™ã€‚

`..æåŠä¸€äº›ã€‚`

## **JAX æŸå¤±å‡½æ•°**

æŸå¤±å‡½æ•°æ˜¯è®­ç»ƒæœºå™¨å­¦ä¹ çš„æ ¸å¿ƒã€‚å®ƒä»¬å¯ä»¥ç”¨æ¥è¯†åˆ«æ¨¡å‹åœ¨æ•°æ®é›†ä¸Šçš„è¡¨ç°å¦‚ä½•ã€‚è¡¨ç°ä¸ä½³ä¼šå¯¼è‡´éå¸¸é«˜çš„æŸå¤±ï¼Œè€Œè¡¨ç°è‰¯å¥½çš„æ¨¡å‹å°†å…·æœ‰è¾ƒä½çš„æŸå¤±ã€‚å› æ­¤ï¼Œåœ¨æ„å»ºæœºå™¨å­¦ä¹ æ¨¡å‹æ—¶ï¼Œé€‰æ‹©æŸå¤±å‡½æ•°æ˜¯ä¸€ä¸ªé‡è¦çš„å†³å®šã€‚

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹çœ‹ JAX ä¸­å¯ç”¨çš„æŸå¤±å‡½æ•°ä»¥åŠå¦‚ä½•ä½¿ç”¨å®ƒä»¬ã€‚

## **ä»€ä¹ˆæ˜¯æŸå¤±å‡½æ•°ï¼Ÿ**

æœºå™¨å­¦ä¹ æ¨¡å‹é€šè¿‡è¯„ä¼°é¢„æµ‹ä¸çœŸå®å€¼çš„å·®å¼‚å¹¶è°ƒæ•´æƒé‡æ¥å­¦ä¹ ã€‚ç›®æ ‡æ˜¯è·å¾—èƒ½å¤Ÿæœ€å°åŒ–**æŸå¤±å‡½æ•°**ï¼ˆå³**è¯¯å·®**ï¼‰çš„æƒé‡ã€‚æŸå¤±å‡½æ•°ä¹Ÿç§°ä¸º**æˆæœ¬å‡½æ•°**ã€‚é€‰æ‹©æŸå¤±å‡½æ•°å–å†³äºé—®é¢˜çš„æ€§è´¨ã€‚æœ€å¸¸è§çš„é—®é¢˜æ˜¯åˆ†ç±»å’Œå›å½’é—®é¢˜ã€‚æ¯ä¸ªé—®é¢˜éœ€è¦ä¸åŒçš„æŸå¤±å‡½æ•°ã€‚

## **åœ¨ JAX ä¸­åˆ›å»ºè‡ªå®šä¹‰æŸå¤±å‡½æ•°**

åœ¨è®­ç»ƒ JAX ç½‘ç»œæ—¶ï¼Œæ‚¨éœ€è¦åœ¨è®­ç»ƒé˜¶æ®µè·å– logitsã€‚è¿™äº› logits ç”¨äºè®¡ç®—æŸå¤±ã€‚ç„¶åéœ€è¦è¯„ä¼°æŸå¤±å‡½æ•°åŠå…¶æ¢¯åº¦ã€‚æ¢¯åº¦ç”¨äºæ›´æ–°æ¨¡å‹å‚æ•°ã€‚æ­¤æ—¶ï¼Œæ‚¨å¯ä»¥ä¸ºæ¨¡å‹è®¡ç®—è®­ç»ƒæŒ‡æ ‡ã€‚

ğŸ’¡Â **Logits æ˜¯ä»€ä¹ˆï¼Ÿ** Logits æ˜¯æœªå½’ä¸€åŒ–çš„å¯¹æ•°æ¦‚ç‡ã€‚*

`def compute_loss(params,images,labels):`

`logits = CNN().apply({'params': params}, images) loss = cross_entropy_loss(logits=logits, labels=labels) return loss, logits`

`@jax.jitdef train_step(state,images, labels): """å•æ­¥è®­ç»ƒã€‚""" (_, logits), grads = jax.value_and_grad(compute_loss, has_aux`

`=True)(state.params,images,labels)`

`state = state.apply_gradients(grads=grads)`

`metrics = compute_metrics(logits=logits, labels=labels) return state, metrics`

æ‚¨å¯ä»¥ä½¿ç”¨ JAX å‡½æ•°å¦‚`[log_sigmoid]`å’Œ`[log_softmax]`æ¥æ„å»ºè‡ªå®šä¹‰æŸå¤±å‡½æ•°ã€‚ç”šè‡³å¯ä»¥ä¸ä½¿ç”¨è¿™äº›å‡½æ•°ä»å¤´å¼€å§‹ç¼–å†™æ‚¨è‡ªå·±çš„æŸå¤±å‡½æ•°ã€‚

è¿™æ˜¯è®¡ç®— Sigmoid äºŒå…ƒäº¤å‰ç†µæŸå¤±çš„ä¸€ä¸ªä¾‹å­ã€‚

`import jax`

`def custom_sigmoid_binary_cross_entropy(logits, labels): log_p = jax.nn.log_sigmoid(logits)`

`log_not_p = jax.nn.log_sigmoid(-logits)`

`return -labels * log_p - (1\. - labels) * log_not_p`

`custom_sigmoid_binary_cross_entropy(0.5,0.0) # DeviceArray(0.974077, dtype=float32, weak_type=True)`

## **JAX ä¸­æœ‰å“ªäº›æŸå¤±å‡½æ•°å¯ç”¨ï¼Ÿ**

ä¸ºæ‚¨çš„ç½‘ç»œæ„å»ºè‡ªå®šä¹‰æŸå¤±å‡½æ•°å¯èƒ½ä¼šåœ¨ç¨‹åºä¸­å¼•å…¥é”™è¯¯ã€‚æ­¤å¤–ï¼Œæ‚¨å¿…é¡»æ‰¿æ‹…ç»´æŠ¤è¿™äº›å‡½æ•°çš„è´Ÿæ‹…ã€‚ç„¶è€Œï¼Œå¦‚æœæ‚¨éœ€è¦çš„æŸå¤±å‡½æ•°ä¸å¯ç”¨ï¼Œåˆ™æœ‰ç†ç”±åˆ›å»ºè‡ªå®šä¹‰æŸå¤±å‡½æ•°ã€‚å°½ç®¡å¦‚æ­¤ï¼Œæ²¡æœ‰å¿…è¦é‡æ–°å‘æ˜è½®å­å¹¶é‡å†™å·²ç»å®ç°çš„æŸå¤±å‡½æ•°ã€‚

JAX ä¸æä¾›ä»»ä½•æŸå¤±å‡½æ•°ã€‚åœ¨ JAX ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨Â `optax`Â æ¥å®šä¹‰æŸå¤±å‡½æ•°ã€‚ç¡®ä¿æ‚¨ä½¿ç”¨ä¸ JAX å…¼å®¹çš„åº“ä»¥åˆ©ç”¨è¯¸å¦‚Â `[JIT]`ã€Â `[vmap]`Â å’ŒÂ `[pmap]`Â ç­‰å‡½æ•°ï¼Œè¿™äº›å‡½æ•°èƒ½å¤ŸåŠ å¿«ç¨‹åºè¿è¡Œé€Ÿåº¦ã€‚

è®©æˆ‘ä»¬çœ‹çœ‹åœ¨`optax`ä¸­å¯ç”¨çš„ä¸€äº›æŸå¤±å‡½æ•°ã€‚

## **Sigmoid äºŒå…ƒäº¤å‰ç†µ**

è®¡ç®— Sigmoid äºŒå…ƒäº¤å‰ç†µæŸå¤±

ä½¿ç”¨Â `optax.sigmoid_binary_cross_entropy`ã€‚è¯¥å‡½æ•°æœŸæœ›

`logits` å’Œ `class labels`ã€‚å®ƒç”¨äºç±»åˆ«ä¸äº’æ–¥çš„é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œæ¨¡å‹å¯ä»¥é¢„æµ‹å›¾åƒä¸­åŒ…å«ä¸¤ä¸ªå¯¹è±¡çš„å›¾åƒåˆ†ç±»é—®é¢˜ã€‚

`optax.sigmoid_binary_cross_entropy(0.5,0.0)# DeviceArray(0.974077, dtype=float32, weak_type=True)`

## **Softmax äº¤å‰ç†µ**

Softmax äº¤å‰ç†µå‡½æ•°ç”¨äºç±»åˆ«äº’æ–¥çš„é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œåœ¨ MNIST æ•°æ®é›†ä¸­ï¼Œæ¯ä¸ªæ•°å­—æ­£å¥½æœ‰ä¸€ä¸ªæ ‡ç­¾ã€‚è¯¥å‡½æ•°æœŸæœ›ä¸€ä¸ª logits æ•°ç»„å’Œæ¦‚ç‡åˆ†å¸ƒã€‚æ¦‚ç‡åˆ†å¸ƒæ€»å’Œä¸º 1ã€‚

`logits = jnp.array([0.50,0.60,0.70,0.30,0.25]) labels = jnp.array([0.20,0.30,0.10,0.20,0.2]) optax.softmax_cross_entropy(logits,labels) # DeviceArray(1.6341426, dtype=float32)`

## **ä½™å¼¦è·ç¦»**

ä½™å¼¦è·ç¦»è¡¡é‡ç›®æ ‡ä¸é¢„æµ‹ä¹‹é—´çš„ä½™å¼¦è·ç¦»ã€‚

`predictions = jnp.array([0.50,0.60,0.70,0.30,0.25]) targets = jnp.array([0.20,0.30,0.10,0.20,0.2]) optax.cosine_distance(predictions,targets,epsilon=0.7) # DeviceArray(0.4128204, dtype=float32)`

## **ä½™å¼¦ç›¸ä¼¼åº¦**

ä½™å¼¦ç›¸ä¼¼åº¦æŸå¤±å‡½æ•°è¡¡é‡çœŸå®å€¼å’Œé¢„æµ‹å€¼ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚ä½™å¼¦ç›¸ä¼¼åº¦æ˜¯ä¸¤ä¸ªå‘é‡ä¹‹é—´è§’åº¦çš„ä½™å¼¦ã€‚é€šè¿‡å‘é‡çš„ç‚¹ç§¯é™¤ä»¥å®ƒä»¬é•¿åº¦çš„ä¹˜ç§¯å¾—åˆ°ã€‚

ç»“æœæ˜¯-1 åˆ° 1 ä¹‹é—´çš„æ•°ã€‚0 è¡¨ç¤ºæ­£äº¤ï¼Œæ¥è¿‘-1 è¡¨ç¤ºç›¸ä¼¼åº¦é«˜ã€‚æ¥è¿‘ 1 è¡¨ç¤ºé«˜åº¦ä¸ç›¸ä¼¼ã€‚

`predictions = jnp.array([0.50,0.60,0.70,0.30,0.25]) targets = jnp.array([0.20,0.30,0.10,0.20,0.2]) optax.cosine_similarity(predictions,targets,epsilon=0.5) # DeviceArray(0.8220514, dtype=float32)`

## **Huber æŸå¤±**

Huber æŸå¤±å‡½æ•°ç”¨äºå›å½’é—®é¢˜ã€‚ä¸å¹³æ–¹è¯¯å·®æŸå¤±ç›¸æ¯”ï¼Œå®ƒå¯¹å¼‚å¸¸å€¼ä¸å¤ªæ•æ„Ÿã€‚å­˜åœ¨å¯ç”¨äºåˆ†ç±»é—®é¢˜çš„ Huber æŸå¤±å‡½æ•°çš„å˜ä½“ã€‚

`logits = jnp.array([0.50,0.60,0.70,0.30,0.25])`

`labels = jnp.array([0.20,0.30,0.10,0.20,0.2])`

`optax.huber_loss(logits,labels)`

# `DeviceArray([0.045 , 0.045 , 0.17999998, 0.005 ,`

`0.00125 ], dtype=float32)`

## **l2 æŸå¤±**

L2 æŸå¤±å‡½æ•°æ˜¯æœ€å°äºŒä¹˜è¯¯å·®ã€‚L2 æŸå¤±æ—¨åœ¨æœ€å°åŒ–çœŸå®å€¼ä¸é¢„æµ‹å€¼ä¹‹é—´çš„å¹³æ–¹å·®çš„æ€»å’Œã€‚å‡æ–¹è¯¯å·®æ˜¯æ‰€æœ‰ L2 æŸå¤±å€¼çš„å¹³å‡å€¼ã€‚

`predictions = jnp.array([0.50,0.60,0.70,0.30,0.25]) targets = jnp.array([0.20,0.30,0.10,0.20,0.2])`

`optax.l2_loss(predictions,targets)`

# `DeviceArray([0.045 , 0.045 , 0.17999998, 0.005 ,`

`0.00125 ], dtype=float32)`

## **`log cosh`**

`[`log_cosh`]`Â æ˜¯é¢„æµ‹è¯¯å·®çš„åŒæ›²ä½™å¼¦çš„å¯¹æ•°ã€‚

ğŸ’¡Â `log(cosh(x))` is approximately equal to `(x ** 2) / 2` for small `x` and to `abs(x) - log(2)` for large `x`. This means that 'logcosh' works mostly like the mean squared error, but will not be so strongly affected by the occasional wildly incorrect prediction.Â TensorFlow Docs*

`predictions = jnp.array([0.50,0.60,0.70,0.30,0.25]) targets = jnp.array([0.20,0.30,0.10,0.20,0.2])`

`optax.log_cosh(predictions,targets)`

# `DeviceArray([0.04434085, 0.04434085, 0.17013526, 0.00499171,`

`0.00124949], dtype=float32)`

## **`Smooth labels`**

`[`optax.smooth_labels`]` Â ä¸äº¤å‰ç†µæŸå¤±ä¸€èµ·ä½¿ç”¨ä»¥å¹³æ»‘æ ‡ç­¾ã€‚å®ƒè¿”å›è¾“å…¥æ ‡ç­¾çš„å¹³æ»‘ç‰ˆæœ¬ã€‚æ ‡ç­¾å¹³æ»‘å·²åº”ç”¨äºå›¾åƒåˆ†ç±»ã€è¯­è¨€ç¿»è¯‘å’Œè¯­éŸ³è¯†åˆ«ï¼Œä»¥é˜²æ­¢æ¨¡å‹è¿‡äºè‡ªä¿¡ã€‚

`labels = jnp.array([0.20,0.30,0.10,0.20,0.2])`

`optax.smooth_labels(labels,alpha=0.4)`

# `DeviceArray([0.2 , 0.26, 0.14, 0.2 , 0.2 ], dtype=float32)`

## **`Computing loss with JAX Metrics`**

JAX Metrics æ˜¯ä¸€ä¸ªç”¨äºåœ¨ JAX ä¸­è®¡ç®—æŸå¤±å’ŒæŒ‡æ ‡çš„å¼€æºåŒ…ã€‚å®ƒæä¾›äº†ç±»ä¼¼ Keras çš„ API æ¥è®¡ç®—æ¨¡å‹çš„æŸå¤±å’ŒæŒ‡æ ‡ã€‚

ä¾‹å¦‚ï¼Œè¿™é‡Œæ˜¯å¦‚ä½•ä½¿ç”¨åº“æ¥è®¡ç®—äº¤å‰ç†µæŸå¤±çš„ã€‚ä¸ Keras ç±»ä¼¼ï¼Œå¯ä»¥é€šè¿‡å®ä¾‹åŒ– [`Loss`] æˆ– [`loss`] æ¥è®¡ç®—æŸå¤±ã€‚

# `pip install jax_metrics import jax_metrics as jm`

`crossentropy = jm.losses.Crossentropy() logits = jnp.array([0.50,0.60,0.70,0.30,0.25]) y = jnp.array([0.50,0.60,0.70,0.30,0.25]) crossentropy(target=y, preds=logits)`

# `DeviceArray(3.668735, dtype=float32)`

å˜é‡å`logits`èµ‹å€¼`jnp.array([0.50,0.60,0.70,0.30,0.25])` `y`èµ‹å€¼`jnp.array([0.50,0.60,0.70,0.30,0.25])` `jm.losses.crossentropy`(`target=y`, `preds=logits`) `# DeviceArray(3.668735, dtype=float32)`

**Here is what the code would like in a JAX training step.import jax_metrics as jmmetric = jm.metrics.Accuracy()@jax.jitdef init_step`(`metric: jm.Metric`) -> `jm.Metric`: return `metric.init()`

å‡½æ•°å`loss_fn`ï¼Œå‚æ•°`params`ï¼Œ`metric`ï¼Œ`x`ï¼Œ`y`: ...

å˜é‡å`metric`èµ‹å€¼`metric.update`ï¼Œå‚æ•°`target=y`ï¼Œ`preds=logits`...

return `loss`, `metric`@jax.jitdef train_step`(`params`, `metric`, `x`, `y`):grads, metric = jax.grad`(`loss_fn`, `has_aux=True`)(

`params`, `metric`, `x`, `y`

)

...

return `params`, `metric`

`@jax.jitdef reset_step`(`metric: jm.Metric`) -> `jm.Metric`: return `metric.reset()`The losses we have seen earlier can also be computed using JAX Metrics.

`! pip install jax_metrics`

`import jax_metrics as jm`

å˜é‡å`target`èµ‹å€¼`jnp.array([50,60,70,30,25])` `preds`èµ‹å€¼`jnp.array([0.50,0.60,0.70,0.30,0.25])` `huber_loss`èµ‹å€¼`jm.losses.Huber()`

`huber_loss`(`target=target`, `preds=preds`) `# DeviceArray(46.030003, dtype=float32)`

å˜é‡å`target`èµ‹å€¼`jnp.array([50,60,70,30,25])`

å˜é‡å`preds`èµ‹å€¼`jnp.array([0.50,0.60,0.70,0.30,0.25])`

`jm.losses.mean_absolute_error`(`target=target`, `preds=preds`) `# DeviceArray(46.530003, dtype=float32)`

`rng`èµ‹å€¼`jax.random.PRNGKey(42)`

å˜é‡å`target`èµ‹å€¼`jax.random.randint(rng, shape=(2, 3), minval=0, maxval =2)`

å˜é‡å`preds`èµ‹å€¼`jax.random.uniform(rng, shape=(2, 3))`

`jm.losses.cosine_similarity`(`target`, `preds`, `axis=1`)

# `DeviceArray`([-0.8602638 , -0.33731455], dtype=float32) `target`èµ‹å€¼`jnp.array([50,60,70,30,25])`

å˜é‡å`preds`èµ‹å€¼`jnp.array([0.50,0.60,0.70,0.30,0.25])`

`jm.losses.mean_absolute_percentage_error`(`target=target`, `preds=preds`)

# `DeviceArray(98.99999, dtype=float32)`

å˜é‡å`target`èµ‹å€¼`jnp.array([50,60,70,30,25])`

å˜é‡å`preds`èµ‹å€¼`jnp.array([0.50,0.60,0.70,0.30,0.25])`

`jm.losses.mean_squared_logarithmic_error`(`target=target`, `preds=preds`)

# `DeviceArray(11.7779, dtype=float32)`

`target`èµ‹å€¼`jnp.array([0.50,0.60,0.70,0.30,0.25])` `preds`èµ‹å€¼`jnp.array([0.50,0.60,0.70,0.30,0.25])` `jm.losses.mean_squared_error`(`target=target`, `preds=preds`) `# DeviceArray(0., dtype=float32)`

## **å¦‚ä½•ç›‘æ§ JAX æŸå¤±å‡½æ•°**

ç›‘æ§ç½‘ç»œçš„æŸå¤±å¾ˆé‡è¦ï¼Œå› ä¸ºå®ƒè¡¨æ˜ç½‘ç»œæ˜¯å¦åœ¨å­¦ä¹ ã€‚æŸå¤±çš„ä¸€ç¥å¯ä»¥å‘Šè¯‰æ‚¨ç½‘ç»œä¸­æ˜¯å¦å­˜åœ¨é—®é¢˜ï¼Œæ¯”å¦‚è¿‡æ‹Ÿåˆã€‚ç›‘æ§æŸå¤±çš„ä¸€ç§æ–¹å¼æ˜¯åœ¨ç½‘ç»œè®­ç»ƒæ—¶æ‰“å°è®­ç»ƒå’ŒéªŒè¯æŸå¤±ã€‚

`![](img/00014.jpeg`)`

æ‚¨è¿˜å¯ä»¥ç»˜åˆ¶è®­ç»ƒå’ŒéªŒè¯æŸå¤±ï¼Œä»¥å¯è§†åŒ–åœ°è¡¨ç¤ºè®­ç»ƒè¿‡ç¨‹ã€‚

`![](img/00015.jpeg`)`

## **ä¸ºä»€ä¹ˆ JAX æŸå¤±ä¼šå‡ºç° NaN**

å½“æ‚¨çš„ç¨‹åºä¸­å‡ºç° NAN æ—¶ï¼ŒJAX ä¸ä¼šæ˜¾ç¤ºé”™è¯¯ã€‚è¿™æ˜¯è®¾è®¡ä¸Šçš„è€ƒè™‘ï¼Œå› ä¸ºä»åŠ é€Ÿå™¨æ˜¾ç¤ºé”™è¯¯æ¶‰åŠå¤æ‚æ€§ã€‚åœ¨è°ƒè¯•æ—¶ï¼Œæ‚¨å¯ä»¥æ‰“å¼€ NAN æ£€æŸ¥å™¨ä»¥æ˜¾ç¤º NAN é”™è¯¯ã€‚NAN åº”è¯¥è¢«ä¿®å¤ï¼Œå› ä¸ºå½“å®ƒä»¬å‡ºç°æ—¶ï¼Œç½‘ç»œåœæ­¢å­¦ä¹ ã€‚

`from jax.config import config`

`config.update`(`"jax_debug_nans"`, `True`)

`jnp.divide`(`0.0`, `0.0`)

# `FloatingPointError`: é‡åˆ°äº†æ— æ•ˆå€¼ (`nan`) åœ¨ `div` ä¸­

ç„¶è€Œï¼Œä»€ä¹ˆå¯¼è‡´ç½‘ç»œä¸­å‡ºç° NaNï¼Ÿæœ‰å„ç§å› ç´ ï¼Œä¸é™äºï¼š

æ•°æ®é›†å°šæœªç¼©æ”¾ã€‚

è®­ç»ƒé›†ä¸­å­˜åœ¨ NaNã€‚è®­ç»ƒæ•°æ®ä¸­çš„æ— é™å€¼å‡ºç°ã€‚

é”™è¯¯çš„ä¼˜åŒ–å™¨å‡½æ•°ã€‚æ¢¯åº¦çˆ†ç‚¸å¯¼è‡´è®­ç»ƒæƒé‡å¤§å¹…æ›´æ–°ã€‚ä½¿ç”¨éå¸¸å¤§çš„å­¦ä¹ ç‡ã€‚

## `æœ€åçš„æ€è€ƒ`

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å·²ç»çœ‹åˆ°é€‰æ‹©æ­£ç¡®çš„æŸå¤±å‡½æ•°å¯¹ç½‘ç»œçš„å­¦ä¹ è‡³å…³é‡è¦ã€‚æˆ‘ä»¬è¿˜è®¨è®ºäº† JAX ä¸­çš„å„ç§æŸå¤±å‡½æ•°ã€‚æ›´å‡†ç¡®åœ°è¯´ï¼Œæˆ‘ä»¬æ¶µç›–äº†ï¼š

ä»€ä¹ˆæ˜¯æŸå¤±å‡½æ•°ï¼Ÿ

å¦‚ä½•åœ¨ JAX ä¸­åˆ›å»ºè‡ªå®šä¹‰æŸå¤±å‡½æ•°ã€‚

JAX ä¸­å¯ç”¨çš„æŸå¤±å‡½æ•°ã€‚

ä½¿ç”¨ JAX æŒ‡æ ‡è®¡ç®—æŸå¤±ã€‚

åœ¨ JAX ä¸­ç›‘æ§æŸå¤±ã€‚

å¦‚ä½•åœ¨ JAX ä¸­é¿å… NaNã€‚

## `åœ¨ JAX å’Œ Flax ä¸­çš„æ¿€æ´»å‡½æ•°`

æ¿€æ´»å‡½æ•°è¢«åº”ç”¨åœ¨ç¥ç»ç½‘ç»œä¸­ï¼Œä»¥ç¡®ä¿ç½‘ç»œè¾“å‡ºæ‰€éœ€çš„ç»“æœã€‚æ¿€æ´»å‡½æ•°å°†è¾“å‡ºé™åˆ¶åœ¨ç‰¹å®šèŒƒå›´å†…ã€‚ä¾‹å¦‚ï¼Œåœ¨è§£å†³äºŒå…ƒåˆ†ç±»é—®é¢˜æ—¶ï¼Œç»“æœåº”è¯¥æ˜¯ä¸€ä¸ªä»‹äº 0 å’Œ 1 ä¹‹é—´çš„æ•°å­—ã€‚è¿™è¡¨ç¤ºç‰©å“å±äºä¸¤ä¸ªç±»åˆ«çš„æ¦‚ç‡ã€‚

ç„¶è€Œï¼Œåœ¨å›å½’é—®é¢˜ä¸­ï¼Œæ‚¨å¸Œæœ›æ•°é‡çš„æ•°å€¼é¢„æµ‹ï¼Œä¾‹å¦‚ç‰©å“çš„ä»·æ ¼ã€‚å› æ­¤ï¼Œæ‚¨åº”è¯¥ä¸ºæ‰€è§£å†³çš„é—®é¢˜é€‰æ‹©åˆé€‚çš„æ¿€æ´»å‡½æ•°ã€‚

è®©æˆ‘ä»¬æ¥çœ‹çœ‹ JAX å’Œ Flax ä¸­å¸¸è§çš„æ¿€æ´»å‡½æ•°ã€‚

## `ReLU â€“ çŸ«æ­£çº¿æ€§å•å…ƒ`

**ReLU æ¿€æ´»å‡½æ•°**ä¸»è¦ç”¨äºç¥ç»ç½‘ç»œçš„éšè—å±‚ï¼Œç¡®ä¿éçº¿æ€§ã€‚è¯¥å‡½æ•°å°†æ‰€æœ‰è¾“å‡ºé™åˆ¶åœ¨é›¶åŠä»¥ä¸Šã€‚å°äºé›¶çš„è¾“å‡ºè¢«è¿”å›ä¸ºé›¶ï¼Œè€Œå¤§äºé›¶çš„æ•°åˆ™åŸæ ·è¿”å›ã€‚è¿™ç¡®ä¿äº†ç½‘ç»œä¸­æ²¡æœ‰è´Ÿæ•°ã€‚

åœ¨ç¬¬ 9 è¡Œï¼Œæˆ‘ä»¬åœ¨å·ç§¯å±‚ä¹‹ååº”ç”¨ ReLU æ¿€æ´»å‡½æ•°ã€‚

`import flaxfrom flax import linen as nnclass CNN(nn.Module):`

`@nn.compact`

`def __call__(self, x):`

`x = nn.Conv(features=32, kernel_size=(3, 3))(x)`

`x = nn.relu(x)`

`x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))`

`x = nn.Conv(features=64, kernel_size=(3, 3))(x)`

`x = nn.relu(x)`

`x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))`

`x = x.reshape((x.shape[0], -1))`

`x = nn.Dense(features=256)(x)`

`x = nn.relu(x)`

`x = nn.Dense(features=2)(x)`

`x = nn.log_softmax(x)`

return x

## `PReLU â€“ å‚æ•°åŒ–ä¿®æ­£çº¿æ€§å•å…ƒ`

`å‚æ•°åŒ–ä¿®æ­£çº¿æ€§å•å…ƒ`æ˜¯å¸¦æœ‰é¢å¤–å‚æ•°çš„ ReLUï¼Œå‚æ•°æ•°é‡ç­‰äºé€šé“æ•°ã€‚å®ƒé€šè¿‡å¼•å…¥ä¸€ä¸ªå¯å­¦ä¹ å‚æ•° *a* æ¥å·¥ä½œã€‚PReLU å…è®¸éè´Ÿå€¼ã€‚

`![](img/00016.gif)x = nn.PReLU(x)`

## `Sigmoid`

**Sigmoid æ¿€æ´»å‡½æ•°**å°†è¾“å‡ºé™åˆ¶åœ¨ 0 åˆ° 1 ä¹‹é—´ï¼Œä¸»è¦ç”¨äºäºŒå…ƒåˆ†ç±»ä»»åŠ¡ã€‚å½“ç±»åˆ«ä¸æ˜¯äº’æ–¥çš„æ—¶å€™ä½¿ç”¨ sigmoidã€‚ä¾‹å¦‚ï¼Œä¸€å¼ å›¾ç‰‡å¯èƒ½åŒæ—¶æœ‰æ±½è½¦ã€å»ºç­‘å’Œæ ‘ç­‰å¤šç§ç‰©ä½“ã€‚ä½¿ç”¨ sigmoid å‡½æ•°æ¥å¤„ç†è¿™ç§æƒ…å†µã€‚

x = `nn.sigmoid(x)`

## **Log sigmoid**

**Log sigmoid**è®¡ç®— sigmoid æ¿€æ´»çš„å¯¹æ•°ï¼Œå…¶è¾“å‡ºåœ¨èŒƒå›´âˆ’âˆåˆ° 0 ä¹‹é—´ã€‚![](img/00017.gif)x = `nn.log_sigmoid(x)`

## **Softmax**

**Softmax æ¿€æ´»å‡½æ•°**æ˜¯ sigmoid å‡½æ•°çš„ä¸€ç§å˜ä½“ï¼Œç”¨äºå¤šç±»é—®é¢˜ï¼Œå…¶ä¸­æ ‡ç­¾æ˜¯ç›¸äº’æ’æ–¥çš„ã€‚ä¾‹å¦‚ï¼Œä¸€å¼ å›¾ç‰‡åªèƒ½æ˜¯ç°åº¦æˆ–è€…å½©è‰²ã€‚å½“åªæœ‰ä¸€ä¸ªæ­£ç¡®ç­”æ¡ˆæ—¶ä½¿ç”¨ softmax æ¿€æ´»ã€‚

x = `nn.softmax(x)`

## **Log softmax**

**Log softmax**è®¡ç®— softmax å‡½æ•°çš„å¯¹æ•°ï¼Œå°†å…ƒç´ é‡æ–°ç¼©æ”¾åˆ°èŒƒå›´âˆ’âˆåˆ° 0 ä¹‹é—´ã€‚![](img/00018.gif)x = `nn.log_softmax(x)`

## **ELU â€“ æŒ‡æ•°çº¿æ€§å•å…ƒæ¿€æ´»**

**ELU æ¿€æ´»å‡½æ•°**æœ‰åŠ©äºè§£å†³æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸é—®é¢˜ã€‚ä¸ ReLU ä¸åŒï¼ŒELU å…è®¸è´Ÿæ•°ï¼Œä»è€Œå°†å•ä½å‡å€¼æ¿€æ´»æ¨å‘é›¶é™„è¿‘ã€‚ELU å¯èƒ½å¯¼è‡´è®­ç»ƒé€Ÿåº¦æ›´å¿«ï¼Œä»¥åŠåœ¨å¤šå±‚ç½‘ç»œä¸­æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

å¯¹äºå¤§äºé›¶çš„å€¼ï¼Œè¿”å›åŸæ•°å€¼ï¼Œä½†å¯¹äºå°äºé›¶çš„æ•°å€¼ï¼Œå®ƒä»¬ä¼šå˜æˆæ¥è¿‘é›¶ä½†ç¨å°çš„æ•°ã€‚![](img/00019.gif)x = `nn.elu(x)`

## **CELU â€“ è¿ç»­å¯å¾®çš„æŒ‡æ•°çº¿æ€§å•å…ƒ**

`CELU`æ˜¯è¿ç»­å¯å¾®çš„ ELU æ¿€æ´»å‡½æ•°çš„å˜ç§ã€‚![](img/00020.gif)x = `nn.celu(x)`

## **GELU â€“ é«˜æ–¯è¯¯å·®çº¿æ€§å•å…ƒæ¿€æ´»**

-   **GELU** éçº¿æ€§æŒ‰å…¶å€¼åŠ æƒè¾“å…¥ï¼Œè€Œä¸æ˜¯åƒ ReLU é‚£æ ·æŒ‰å…¶ç¬¦å·é—¨æ§è¾“å…¥â€“ æ¥æº.![](img/00021.gif)x = `nn.gelu(x)` ![](img/00022.jpeg)

## -   **GLU â€“ é—¨æ§çº¿æ€§å•å…ƒæ¿€æ´»**

-   **GLU** æ˜¯è®¡ç®—ä¸º [`GLU ( a , b )= a âŠ— Ïƒ ( b )`]. å®ƒå·²è¢«åº”ç”¨äºç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†çš„é—¨æ§ CNNsã€‚åœ¨å…¬å¼ä¸­ï¼Œb é—¨æ§åˆ¶ç€ä¼ é€’åˆ°ä¸‹ä¸€å±‚çš„ä¿¡æ¯ã€‚GLU æœ‰åŠ©äºè§£å†³æ¶ˆå¤±æ¢¯åº¦é—®é¢˜ã€‚

x = `nn.glu(x)`

## -   **Soft sign**

**Soft sign** æ¿€æ´»å‡½æ•°å°†å€¼é™åˆ¶åœ¨ -1 åˆ° 1 ä¹‹é—´ã€‚å®ƒç±»ä¼¼äºåŒæ›²æ­£åˆ‡æ¿€æ´»å‡½æ•°â€“ tanhã€‚ä¸åŒä¹‹å¤„åœ¨äº tanh æŒ‡æ•°çº§æ”¶æ•›ï¼Œè€Œ Soft sign å¤šé¡¹å¼çº§æ”¶æ•›ã€‚

x = `nn.soft_sign(x)` ![](img/00023.gif)

## -   **Softplus**

-   **Softplus æ¿€æ´»** å°†è¿”å›å€¼ä¸ºé›¶åŠä»¥ä¸Šã€‚å®ƒæ˜¯ ReLu çš„å¹³æ»‘ç‰ˆæœ¬ã€‚x = `nn.soft_plus(x)` ![](img/00024.gif)![](img/00025.jpeg)**The Softplus activation ** **## Swishâ€“SigmoidÂ Linear Unit(Â SiLU)**

SiLU æ¿€æ´»å‡½æ•°è®¡ç®—ä¸º [`x * sigmoid(beta * x)`]ï¼Œå…¶ä¸­ beta æ˜¯ Swish æ¿€æ´»å‡½æ•°çš„è¶…å‚æ•°ã€‚å› æ­¤ï¼ŒSiLU æ˜¯é€šè¿‡å°†å…¶è¾“å…¥ä¸ sigmoid å‡½æ•°ç›¸ä¹˜æ¥è®¡ç®—çš„ã€‚

x = `nn.swish(x)`![](img/00026.gif)

## -   **åœ¨ JAX å’Œ Flax ä¸­çš„è‡ªå®šä¹‰æ¿€æ´»å‡½æ•°**

æ‚¨è¿˜å¯ä»¥åœ¨ JAX ä¸­å®šä¹‰è‡ªå®šä¹‰æ¿€æ´»å‡½æ•°ã€‚ä¾‹å¦‚ï¼Œä»¥ä¸‹æ˜¯å¦‚ä½•å®šä¹‰ LeakyReLU æ¿€æ´»å‡½æ•°ã€‚from flax import linen as nnimport jax.numpy as jnpclass `LeakyReLU`(nn.Module):`alpha` : float = 0.1def `__call__`(self, `x`):return jnp.where(`x` > 0, `x`, `self.alpha` * `x`)

## -   **æœ€ç»ˆæ€è€ƒ**

æ‚¨å·²ç»äº†è§£äº†å¯ä»¥åœ¨ JAX å’Œ Flax ä¸­ä½¿ç”¨çš„å„ç§æ¿€æ´»å‡½æ•°ã€‚æ‚¨è¿˜çœ‹åˆ°ï¼Œå¯ä»¥é€šè¿‡åˆ›å»ºä¸€ä¸ªå®ç°`[__call__]`æ–¹æ³•çš„ç±»æ¥åˆ›å»ºæ–°å‡½æ•°ã€‚

## -   å¦‚ä½•åœ¨`JAX`ä¸­åŠ è½½æ•°æ®é›†ä¸`TensorFlow`

`JAX` ä¸æä¾›æ•°æ®åŠ è½½å®ç”¨ç¨‹åºã€‚è¿™ä½¿`JAX`ä¸“æ³¨äºæä¾›ä¸€ä¸ªå¿«é€Ÿæ„å»ºå’Œè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹çš„å·¥å…·ã€‚åœ¨`JAX`ä¸­åŠ è½½æ•°æ®æ˜¯ä½¿ç”¨

è¦ä¹ˆ`TensorFlow`è¦ä¹ˆ`PyTorch`ã€‚æœ¬æ–‡å°†é‡ç‚¹ä»‹ç»å¦‚ä½•ä½¿ç”¨`TensorFlow`åœ¨`JAX`ä¸­åŠ è½½æ•°æ®é›†ã€‚

-   è®©æˆ‘ä»¬æ·±å…¥æ¢è®¨ï¼

## å¦‚ä½•åœ¨`JAX`ä¸­åŠ è½½æ–‡æœ¬æ•°æ®

è®©æˆ‘ä»¬ä½¿ç”¨`Kaggle`çš„`IMDB æ•°æ®é›†`æ¥è¯´æ˜å¦‚ä½•åœ¨`JAX`ä¸­åŠ è½½æ–‡æœ¬æ•°æ®é›†ã€‚æˆ‘ä»¬å°†ä½¿ç”¨`Kaggle`Python`åº“ä¸‹è½½æ•°æ®ã€‚è¿™éœ€è¦æ‚¨çš„ Kaggle ç”¨æˆ·åå’Œå¯†é’¥ã€‚å‰å¾€

`https://www.kaggle.com/your_username/ account to obtain the API`

å…³é”®ã€‚

è¯¥åº“å°†æ•°æ®ä¸‹è½½ä¸º zip æ–‡ä»¶ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†åœ¨ä¹‹åæå–å®ƒã€‚

import`os`

# Obtain from`https://www.kaggle.com/username/account os.environ["KAGGLE_USERNAME"]="YOUR_KAGGLE_USERNAME" os.environ["KAGGLE_KEY"]="YOUR_KAGGLE_KEY"`

import`kaggle`

!`kaggle datasets download lakshmi25npathi/imdb-dataset-of-50k-movie-reviews`

import`zipfile`

with zipfile.ZipFile('imdb-dataset-of-50k-movie-reviews.zip',

'r') as zip_ref:zip_ref.extractall('imdb-dataset-of-50k-movie-reviews')æ¥ä¸‹æ¥ï¼Œå¯¼å…¥æ ‡å‡†çš„æ•°æ®ç§‘å­¦åŒ…ï¼Œå¹¶æŸ¥çœ‹æ•°æ®çš„æ ·æœ¬ã€‚

`import`numpy as np`

`import`pandas`as`pd`

`from`numpy`import`array`

`import`tensorflow as tf`

`from`sklearn.model_selection`import`train_test_split`

`from`sklearn.preprocessing`import`LabelEncoder`

`import`matplotlib.pyplot`as`plt`

`df = pd.read_csv("imdb-dataset-of-50k-movie-reviews/IMDB Dataset.c df.head()`

!`[](../images/00027.jpeg)`

## æ¸…ç†æ–‡æœ¬æ•°æ®

åœ¨ä½¿ç”¨`TensorFlow`åŠ è½½æ•°æ®ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å¯¹æ•°æ®è¿›è¡Œä¸€äº›å¤„ç†ã€‚æ–‡æœ¬é—®é¢˜çš„æ ‡å‡†å¤„ç†æ˜¯å»é™¤åœç”¨è¯ã€‚åœç”¨è¯æ˜¯å¦‚`a`ã€`the`ç­‰å¸¸è§è¯æ±‡ï¼Œå®ƒä»¬å¯¹æ¨¡å‹è¯†åˆ«æƒ…æ„Ÿææ€§æ²¡æœ‰å¸®åŠ©ã€‚

å¥å­ã€‚`NLTK`æä¾›äº†åœç”¨è¯ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ç¼–å†™ä¸€ä¸ªå‡½æ•°ä» IMDB æ•°æ®é›†ä¸­å»é™¤å®ƒä»¬ã€‚

`import`nltk`

`from`nltk.corpus`import`stopwords`

`nltk.download('stopwords')`

`def remove_stop_words(review):`

`review_minus_sw = []`

`stop_words = stopwords.words('english')`

`review = review.split()`

`[cleaned_review = [review_minus_sw.append(word) for word in`

`review if word not in stop_words]`

`cleaned_review = ' '.join(review_minus_sw)`

`return cleaned_review`

`df['review'] = df['review'].apply(remove_stop_words) view raw`

## å¯¹æƒ…æ„Ÿåˆ—è¿›è¡Œæ ‡ç­¾ç¼–ç 

ä½¿ç”¨`Scikit-learn`çš„æ ‡ç­¾ç¼–ç å°†æƒ…æ„Ÿåˆ—è½¬æ¢ä¸ºæ•°å€¼è¡¨ç¤ºã€‚è¿™å¾ˆé‡è¦ï¼Œå› ä¸ºç¥ç»ç½‘ç»œæœŸæœ›æ•°å€¼æ•°æ®ã€‚

`labelencoder = LabelEncoder()`

`df = df.assign(sentiment = labelencoder.fit_transform(df["senti ment"]))`

!`[](../images/00028.jpeg)`

## ä½¿ç”¨`TensorFlow`è¿›è¡Œæ–‡æœ¬é¢„å¤„ç†

æˆ‘ä»¬å·²å°†æƒ…æ„Ÿåˆ—è½¬æ¢ä¸ºæ•°å­—

è¡¨ç¤ºã€‚ç„¶è€Œï¼Œè¯„è®ºä»ç„¶æ˜¯æ–‡æœ¬å½¢å¼ã€‚æˆ‘ä»¬ä¹Ÿéœ€è¦å°†å®ƒä»¬è½¬æ¢ä¸ºæ•°å­—ã€‚

æˆ‘ä»¬é¦–å…ˆå°†æ•°æ®é›†åˆ†å‰²ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚

`from`sklearn.model_selection`import`train_test_split df = df.drop_duplicates()`

`docs = df['review']`

`labels = array(df['sentiment'])`

`X_train, X_test , y_train, y_test = train_test_split(docs, labe ls , test_size = 0.20, random_state=0)`

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä½¿ç”¨ TensorFlow çš„`TextVectorization`å‡½æ•°å°†æ–‡æœ¬æ•°æ®è½¬æ¢ä¸ºæ•´æ•°è¡¨ç¤ºã€‚è¯¥å‡½æ•°æœŸæœ›ï¼š

`[standardize]`ç”¨äºæŒ‡å®šå¦‚ä½•å¤„ç†æ–‡æœ¬æ•°æ®ã€‚ä¾‹å¦‚ï¼Œé€‰é¡¹`[lower_and_strip_punctuation]`å°†æ–‡æœ¬è½¬æ¢ä¸ºå°å†™å¹¶åˆ é™¤æ ‡ç‚¹ç¬¦å·ã€‚

`max_tokens`å†³å®šè¯æ±‡è¡¨çš„æœ€å¤§å¤§å°ã€‚`[output_mode]`ç¡®å®šå‘é‡åŒ–å±‚çš„è¾“å‡ºã€‚è®¾ç½®`[int]`å°†è¾“å‡ºæ•´æ•°ã€‚

`[output_sequence_length]`æŒ‡ç¤ºè¾“å‡ºåºåˆ—çš„æœ€å¤§é•¿åº¦ã€‚è¿™ç¡®ä¿æ‰€æœ‰åºåˆ—å…·æœ‰ç›¸åŒçš„é•¿åº¦ã€‚

`import`tensorflow as tf`

`max_features = 5000`# æœ€å¤§è¯æ±‡é‡å¤§å°ã€‚

`batch_size = 32`

`max_len = 512`# å°†è¾“å‡ºå¡«å……åˆ°çš„åºåˆ—é•¿åº¦ã€‚`vectorize_layer = tf.keras.layers.TextVectorization(standardize ='lower_and_strip_punctuation',max_tokens=max_features,output_m ode='int',output_sequence_length=max_len)`

`vectorize_layer.adapt(X_train, batch_size=None)`

æ¥ä¸‹æ¥ï¼Œå°†æ­¤å±‚åº”ç”¨äºè®­ç»ƒå’Œæµ‹è¯•æ•°æ®ã€‚`X_train_padded = vectorize_layer(X_train) X_test_padded = vectorize_layer(X_test)![](img/00029.jpeg)`

å°†æ•°æ®è½¬æ¢ä¸º`TensorFlow æ•°æ®é›†ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥æ‰¹é‡è·å–æ•°æ®ã€‚æˆ‘ä»¬è¿˜å°†æ•°æ®è½¬æ¢ä¸º NumPy æ•°ç»„ï¼Œå› ä¸º JAX æœŸæœ›ä½¿ç”¨ NumPy æˆ– JAX æ•°ç»„ã€‚

import`tensorflow_datasets`as`tfds`

`training_data = tf.data.Dataset.from_tensor_slices((X_train_padded, y_train))`

`validation_data = tf.data.Dataset.from_tensor_slices((X_test_padded, y_test))`

`training_data = training_data.batch(batch_size)`

`validation_data = validation_data.batch(batch_size)`

`def get_train_batches():`

`ds = training_data.prefetch(1)`

# `tfds.dataset_as_numpy`å°†`tf.data.Dataset`è½¬æ¢ä¸º

å¯è¿­ä»£çš„ NumPy æ•°ç»„`return tfds.as_numpy(ds)`ç°åœ¨æ•°æ®æ ¼å¼æ­£ç¡®ï¼Œå¹¶å°†ä¼ é€’ç»™ Flax ç½‘ç»œã€‚

è®©æˆ‘ä»¬å¿«é€Ÿæµè§ˆä¸€ä¸‹ä½¿ç”¨è¿™äº›æ•°æ®åœ¨ Flax ä¸­è®­ç»ƒç¥ç»ç½‘ç»œæ‰€éœ€çš„å…¶ä½™æ­¥éª¤ã€‚

é¦–å…ˆï¼Œåœ¨ Flax ä¸­åˆ›å»ºä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œã€‚

# pip install`flax`

import`flax`

from`flax`import`linen as nn`

`class Model(nn.Module):`

@nn.compact

`def __call__(self, x): x = nn.Dense(features=256)(x) x = nn.relu(x)`

`x = nn.Dense(features=2)(x) x = nn.log_softmax(x)`

return`x`

å®šä¹‰ä¸€ä¸ªè®¡ç®—æŸå¤±çš„å‡½æ•°ã€‚

import`optax`

import`jax.numpy`as`jnp`

`def cross_entropy_loss(*, logits, labels):`

`labels_onehot = jax.nn.one_hot(labels, num_classes=2) return optax.softmax_cross_entropy(logits=logits, labels=labe ls_onehot).mean()`æ¥ä¸‹æ¥ï¼Œå®šä¹‰è®¡ç®—ç½‘ç»œæŒ‡æ ‡çš„å‡½æ•°ã€‚

`def compute_metrics(*, logits, labels):`

`loss = cross_entropy_loss(logits=logits, labels=labels)` `accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)` `metrics = {`

`'loss': loss,`

`'accuracy': accuracy`,

}

return `metrics`

è®­ç»ƒçŠ¶æ€ç”¨äºè·Ÿè¸ªç½‘ç»œè®­ç»ƒã€‚å®ƒè·Ÿè¸ªä¼˜åŒ–å™¨å’Œæ¨¡å‹å‚æ•°ï¼Œå¹¶å¯ä»¥ä¿®æ”¹ä»¥è·Ÿè¸ªå…¶ä»–å†…å®¹ï¼Œä¾‹å¦‚ dropout å’Œæ‰¹å½’ä¸€åŒ–ç»Ÿè®¡ä¿¡æ¯ã€‚

from `flax.training import train_state` def `create_train_state(rng, learning_rate, momentum):` """åˆ›å»ºåˆå§‹`TrainState`ã€‚"""

`model = Model()`

`params = model.init(rng, X_train_padded[0])['params']` `tx = optax.sgd(learning_rate, momentum)`

return `train_state.TrainState.create(

`apply_fn=model.apply, params=params, tx=tx)`åœ¨è®­ç»ƒæ­¥éª¤ä¸­ï¼Œæˆ‘ä»¬[åº”ç”¨]æ¨¡å‹ä»¥è·å–æŸå¤±ã€‚ç„¶åç”¨è¿™ä¸ªæŸå¤±æ¥è®¡ç®—æ›´æ–°æ¨¡å‹å‚æ•°çš„æ¢¯åº¦ã€‚

`def compute_loss(params,text,labels):`

`logits = Model().apply({'params': params}, text)` `loss = cross_entropy_loss(logits=logits, labels=labels)` return `loss, logits`

@jax.jit

`def train_step(state,text, labels):`

"""å¯¹å•ä¸ªæ­¥éª¤è¿›è¡Œè®­ç»ƒã€‚"""

`(_, logits), grads = jax.value_and_grad(compute_loss, has_aux =True)(state.params,text,labels)`

`state = state.apply_gradients(grads=grads)`

`metrics = compute_metrics(logits=logits, labels=labels)`

return `state, metrics`

è¯„ä¼°æ­¥éª¤å°†æ¨¡å‹åº”ç”¨äºæµ‹è¯•æ•°æ®ä»¥è®¡ç®—æµ‹è¯•æŒ‡æ ‡ã€‚

@jax.jit

`def eval_step(state, text, labels):`

`logits = Model().apply({'params': state.params}, text)`

return `compute_metrics(logits=logits, labels=labels)`

è¯„ä¼°å‡½æ•°è¿è¡Œä¸Šè¿°è¯„ä¼°æ­¥éª¤ä»¥è·å–è¯„ä¼°æŒ‡æ ‡ã€‚

`def evaluate_model(state, text, test_lbls):`

"""åœ¨éªŒè¯é›†ä¸Šè¿›è¡Œè¯„ä¼°ã€‚"""

`metrics = eval_step(state, text, test_lbls)` `metrics = jax.device_get(metrics)`

`metrics = jax.tree_map(lambda x: x.item(), metrics)` return `metrics`

æˆ‘ä»¬åœ¨`train_epoch`æ–¹æ³•ä¸­ä½¿ç”¨`get_train_batches`å‡½æ•°ã€‚æˆ‘ä»¬é€šè¿‡åº”ç”¨`[train_step]`æ–¹æ³•å¾ªç¯éå†æ‰¹æ¬¡ã€‚æˆ‘ä»¬è·å–è®­ç»ƒæŒ‡æ ‡å¹¶è¿”å›å®ƒä»¬ã€‚

`def train_one_epoch(state):`

"""åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œ 1 è½®è®­ç»ƒã€‚""" `batch_metrics = []`

for `text, labels in get_train_batches():`

`state, metrics = train_step(state, text, labels)` `batch_metrics.append(metrics)`batch_metrics_np = jax.device_get(batch_metrics) epoch_metrics_np = {k: np.mean([metrics[k] for metrics in batch_metrics_n p])for k in batch_metrics_np[0] }return `state, epoch_metrics_np`

æœ€åä¸€æ­¥æ˜¯åœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒç½‘ç»œï¼Œå¹¶åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œè¯„ä¼°ã€‚åœ¨è®­ç»ƒæ¨¡å‹ä¹‹å‰éœ€è¦ä¸€ä¸ªè®­ç»ƒçŠ¶æ€ã€‚è¿™æ˜¯å› ä¸º JAX æœŸæœ›çº¯å‡½æ•°ã€‚

`rng = jax.random.PRNGKey(0)`ï¼Œ`rng, init_rng = jax.random.split(rng)`

`learning_rate = 0.1 momentum = 0.9`

`seed = 0`

`state = create_train_state(init_rng, learning_rate, momentum)` del `init_rng # ä¸å†ä½¿ç”¨ã€‚`

`num_epochs = 30`

`(text, test_labels) = next(iter(validation_data))` `text = jnp.array(text)`

`test_labels = jnp.array(test_labels)`

`state` = `create_train_state(jax.random.PRNGKey(seed), learning_r ate, momentum)`

`training_loss` = []

`training_accuracy` = []

`testing_loss` = []

`testing_accuracy` = []

for epoch in range(1, num_epochs + 1):

`train_state, train_metrics = train_one_epoch(state) training_loss.append(train_metrics['loss'])`

`training_accuracy.append(train_metrics['accuracy']) print(f"Train epoch: {epoch}, loss: {train_metrics['los`

s']}, accuracy: {train_metrics['accuracy'] * 100}")`

`test_metrics` = `evaluate_model(train_state, text, test_label s)`

`testing_loss.append(test_metrics['loss'])`

`testing_accuracy.append(test_metrics['accuracy'])`

`print(f"Test epoch: {epoch}, loss: {test_metrics['loss']}, accuracy: {test_metrics['accuracy'] * 100}")`

`![](img/00030.jpeg)`

## **å¦‚ä½•åœ¨ JAX ä¸­åŠ è½½å›¾åƒæ•°æ®**

ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•ä½¿ç”¨ TensorFlow åŠ è½½å›¾åƒæ•°æ®ã€‚æˆ‘ä»¬å°†ä½¿ç”¨æ¥è‡ª Kaggle çš„æµè¡Œçš„çŒ«å’Œç‹—å›¾åƒã€‚æˆ‘ä»¬é¦–å…ˆä¸‹è½½æ•°æ®ã€‚

`import wget # pip install wgetimport zipfile`

`wget.download("https://ml.machinelearningnuggets.com/train.zi p")`

`with zipfile.ZipFile('train.zip', 'r') as zip_ref:`

`zip_ref.extractall('.')æ¥ä¸‹æ¥ï¼Œåˆ›å»ºä¸€ä¸ªåŒ…å«æ ‡ç­¾å’Œå›¾åƒè·¯å¾„çš„`Pandas DataFrame`ã€‚

`import pandas as pd`

`base_dir` = `'train'`

`filenames = os.listdir(base_dir) categories = []`

`for filename in filenames:`

`category = filename.split('.')[0] if category == 'dog':categories.append("dog")`

`else:`

`categories.append("cat") df = pd.DataFrame({'filename': filenames,'category': categorie s})`

ä¸‹ä¸€æ­¥æ˜¯ä¸ºå›¾åƒç¼©æ”¾å’Œæ‰§è¡Œç®€å•æ•°æ®å¢å¼ºå®šä¹‰ä¸€ä¸ª`ImageDataGenerator`ã€‚

`from tensorflow.keras.preprocessing.image import ImageDataGener ator`

`train_datagen` = `ImageDataGenerator(rescale=1./255,`

`shear_range=0.2, zoom_range=0.2,`

`horizontal_flip=True, width_shift_range=0.1, height_shift_range=0.1, validation_split=0.2 )`

`validation_gen` = `ImageDataGenerator(rescale=1./255,validation_s plit=0.2)`

ä½¿ç”¨è¿™äº›ç”Ÿæˆå™¨çš„`[flow_from_dataframe]`åŠ è½½å›¾åƒã€‚è¿™å°†æŠŠ DataFrame ä¸­çš„å›¾åƒè·¯å¾„ä¸æˆ‘ä»¬ä¸‹è½½çš„å›¾åƒè¿›è¡ŒåŒ¹é…ã€‚

`image_size = (128, 128)`

`batch_size = 128`

`training_set` = `train_datagen.flow_from_dataframe(df,base_dir,`

`seed=101, target_size=ima ge_size,`

`batch_size=batc h_size,`

`x_col='filenam e'`,

`y_col='åˆ†ç±»'`

subset = `"è®­ç»ƒ"`

`class_mode='bin ary')`

`validation_set` = `validation_gen.flow_from_dataframe(df,base_di r,`

`target_size=image _size`,

`batch_size=batch_b size,`

`x_col='æ–‡ä»¶å'`,

`y_col='åˆ†ç±»'`,

subset = `"éªŒè¯"`

`class_mode='binar y')`

éå†è®­ç»ƒé›†ä»¥ç¡®è®¤æ˜¯å¦ç”Ÿæˆäº†ä¸€æ‰¹å›¾åƒã€‚

`for train_images, train_labels in training_set: print('Train:', train_images.shape, train_labels.shape) break`

# è®­ç»ƒï¼š`(128, 128, 128, 3) (128,)`

ä¸‹ä¸€æ­¥æ˜¯å®šä¹‰ä¸€ä¸ªç¥ç»ç½‘ç»œå¹¶ä¼ é€’æ•°æ®ã€‚æ­¥éª¤ä¸æˆ‘ä»¬ä¸Šé¢å¯¹æ–‡æœ¬æ•°æ®æ‰€åšçš„æ­¥éª¤ç±»ä¼¼

## **å¦‚ä½•åœ¨ JAX ä¸­åŠ è½½ CSV æ•°æ®**

ä½ å¯ä»¥ä½¿ç”¨`Pandas`æ¥åŠ è½½ CSV æ•°æ®ï¼Œå°±åƒæˆ‘ä»¬åœ¨æ–‡ç« å¼€å¤´çš„æ–‡æœ¬æ•°æ®é‚£æ ·ã€‚åœ¨é¢„å¤„ç†å®Œæˆåï¼Œå°†æ•°æ®è½¬æ¢ä¸º NumPy æˆ–`JAX æ•°ç»„`ã€‚å°† Torch å¼ é‡æˆ– TensorFlow å¼ é‡ä¼ é€’ç»™`JAX`ç¥ç»ç½‘ç»œä¼šå¯¼è‡´é”™è¯¯ã€‚

## `**æœ€åçš„æ€è€ƒ**`

æœ¬æ–‡å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ TensorFlow åœ¨`JAX`å’Œ`Flax`åº”ç”¨ç¨‹åºä¸­åŠ è½½æ•°æ®é›†ã€‚æˆ‘ä»¬å·²ç»ä»‹ç»äº†å¦‚ä½•ä½¿ç”¨ TensorFlow åŠ è½½æ–‡æœ¬æ•°æ®çš„ç¤ºä¾‹ã€‚ä¹‹åï¼Œæˆ‘ä»¬è®¨è®ºäº†åœ¨`JAX`ä¸­åŠ è½½å›¾åƒå’Œ CSV æ•°æ®ã€‚

## `**ä½¿ç”¨ JAX å’Œ Flax è¿›è¡Œå›¾åƒåˆ†ç±»**`

`Flax`æ˜¯ä¸€ä¸ªé¢å‘`JAX`çš„ç¥ç»ç½‘ç»œåº“ã€‚`JAX`æ˜¯ä¸€ä¸ªæä¾›é«˜æ€§èƒ½è®¡ç®—çš„`Python`åº“ï¼Œç”¨äºæœºå™¨å­¦ä¹ ç ”ç©¶ã€‚`JAX`æä¾›äº†ä¸`NumPy`ç±»ä¼¼çš„ APIï¼Œä½¿å…¶æ˜“äºé‡‡ç”¨ã€‚`JAX`è¿˜åŒ…æ‹¬å…¶ä»–ç”¨äºæ”¹è¿›æœºå™¨å­¦ä¹ ç ”ç©¶çš„åŠŸèƒ½ã€‚å…¶ä¸­åŒ…æ‹¬ï¼š

**è‡ªåŠ¨å¾®åˆ†** .Â `JAX`Â æ”¯æŒä½¿ç”¨Â `jacrev`,Â `grad`,Â `hessian`Â å’ŒÂ `jacfwd`Â ç­‰å‡½æ•°è¿›è¡Œæ•°å€¼å‡½æ•°çš„å‰å‘å’Œåå‘è‡ªåŠ¨å¾®åˆ†ã€‚

**å‘é‡åŒ–** .Â `JAX`Â é€šè¿‡Â `[vmap]`Â å‡½æ•°æ”¯æŒè‡ªåŠ¨å‘é‡åŒ–ã€‚å®ƒè¿˜é€šè¿‡Â `[pmap]`Â å‡½æ•°ä½¿å¾—å¤§è§„æ¨¡æ•°æ®å¤„ç†æ˜“äºå¹¶è¡ŒåŒ–ã€‚

**å³æ—¶ç¼–è¯‘** .Â `JAX`Â ä½¿ç”¨Â `XLA`Â æ¥è¿›è¡Œä»£ç çš„å³æ—¶ç¼–è¯‘å’Œåœ¨ GPU å’Œ TPU ä¸Šæ‰§è¡Œã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•ä½¿ç”¨Â `JAX`Â å’Œ `Flax`Â æ¥æ„å»ºä¸€ä¸ªç®€å•çš„å·ç§¯ç¥ç»ç½‘ç»œã€‚

## **åŠ è½½æ•°æ®é›†**

æˆ‘ä»¬å°†ä½¿ç”¨æ¥è‡ª Kaggle çš„`çŒ«ç‹—æ•°æ®é›†`ã€‚è®©æˆ‘ä»¬ä»ä¸‹è½½å’Œè§£å‹å¼€å§‹ã€‚`import wget # pip install wget import zipfile`

`wget.download("https://ml.machinelearningnuggets.com/train.zi p")`

`with zipfile.ZipFile('train.zip', 'r') as zip_ref:`

`zip_ref.extractall('.')`

Flax æ²¡æœ‰ä»»ä½•æ•°æ®åŠ è½½å·¥å…·ã€‚ä½ å¯ä»¥ä½¿ç”¨æ¥è‡ªÂ `PyTorch`Â çš„æ•°æ®åŠ è½½å™¨ã€‚

æˆ–è€…ä½¿ç”¨Â `TensorFlow`ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨ PyTorch åŠ è½½æ•°æ®ã€‚é¦–å…ˆè¦å®šä¹‰æ•°æ®é›†ç±»ã€‚

`from PIL import Image`

`import pandas as pd`

`from torch.utils.data import Dataset`

`class CatsDogsDataset(Dataset):`

`def __init__(self, root_dir, annotation_file, transform=Non`

`e):`

`self.root_dir = root_dir`

`self.annotations = pd.read_csv(annotation_file) self.transform = transform`

`def __len__(self):return len(self.annotations) def __getitem__(self, index):img_id = self.annotations.iloc[index, 0]img = Image.open(os.path.join(self.root_dir, img_id)).c onvert("RGB")y_label = torch.tensor(float(self.annotations.iloc[inde x, 1]))if self.transform is not None: img = self.transform(img)return (img, y_label)`æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå°†åŒ…å«ç±»åˆ«çš„Â `Pandas DataFrame`ã€‚`import osimport pandas as pd`

`train_df = pd.DataFrame(columns=["img_path","label"]) train_df["img_path"] = os.listdir("train/") for idx, i in enumerate(os.listdir("train/")):`

`if "cat" in i:`

`train_df["label"][idx] = 0`

`if "dog" in i:`

`train_df["label"][idx] = 1`

`train_df.to_csv (r'train_csv.csv', index = False, header=True)`å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œå°†å †å æ•°æ®å¹¶ä»¥ NumPy æ•°ç»„å½¢å¼è¿”å›ã€‚

`import numpy as np`

def `custom_collate_fn(batch)`

transposed_data = `list(zip(*batch))`

labels = `np.array(transposed_data[1])`

imgs = `np.stack(transposed_data[0])`

return `imgs, labels`

ç°åœ¨æˆ‘ä»¬å‡†å¤‡å¥½å®šä¹‰è®­ç»ƒå’Œæµ‹è¯•æ•°æ®ï¼Œå¹¶ä½¿ç”¨ PyTorch çš„ DataLoaderã€‚æˆ‘ä»¬è¿˜å®šä¹‰äº†ä¸€ä¸ªç”¨äºè°ƒæ•´å›¾åƒå¤§å°çš„ PyTorch è½¬æ¢ã€‚

import `torch`

from `torch.utils.data import DataLoader` from `torchvision import transforms` import `numpy as np`

size_image = 64 batch_size = 32

transform = `transforms.Compose([`

transforms.Resize((size_image,size_image)),

`np.array]`

dataset = `CatsDogsDataset("train","train_csv.csv",transform=tra nsform)`

è®­ç»ƒé›†ã€éªŒè¯é›† = `torch.utils.data.random_split(datas et,[20000,5000])`

train_loader = `DataLoader(dataset=train_set, collate_fn=custom_collate_fn,shuffle=True, batch_size=batch_size)`

validation_loader = `DataLoader(dataset=validation_set,collate_f n=custom_collate_fn, shuffle=False, batch_size=batch_size)`

## ä½¿ç”¨ Flax å®šä¹‰å·ç§¯ç¥ç»ç½‘ç»œ

å®‰è£… Flax æ¥åˆ›å»ºä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œã€‚`pip install flax`

åœ¨ Flax ä¸­ä½¿ç”¨ Linen API åˆ›å»ºç½‘ç»œ

å­ç±»åŒ– Moduleã€‚æ‰€æœ‰çš„ Flax æ¨¡å—éƒ½æ˜¯ Python çš„ dataclassesã€‚è¿™æ„å‘³ç€å®ƒä»¬é»˜è®¤å…·æœ‰ `__init__` æ–¹æ³•ã€‚å› æ­¤ï¼Œä½ åº”è¯¥è¦†ç›– `setup()` æ–¹æ³•æ¥åˆå§‹åŒ–ç½‘ç»œã€‚ä½†æ˜¯ï¼Œä½ å¯ä»¥ä½¿ç”¨ç´§å‡‘çš„åŒ…è£…å™¨

ä½¿æ¨¡å‹å®šä¹‰æ›´åŠ ç®€æ´ã€‚

å¯¼å…¥ `flaxfrom flax import linen as nnclass CNN(nn.Module):`

@`nn.compact`

def `__call__(self, x):`

x = `nn.Conv(features=32, kernel_size=(3, 3))(x)`

x = `nn.relu(x)`

x = `nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))`

x = `nn.Conv(features=64, kernel_size=(3, 3))(x)`

x = `nn.relu(x)`

x = `nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))`

x = `x.reshape((x.shape[0], -1))`

x = `nn.Dense(features=256)(x)`

x = `nn.relu(x)`

x = `nn.Dense(features=2)(x)`

x = `nn.log_softmax(x)`

è¿”å› x

## å®šä¹‰æŸå¤±

å¯ä»¥ä½¿ç”¨ Optax åŒ…è®¡ç®—æŸå¤±ã€‚æˆ‘ä»¬åœ¨ä¼ é€’ç»™ softmax äº¤å‰ç†µå‡½æ•°ä¹‹å‰å¯¹æ•´æ•°æ ‡ç­¾è¿›è¡Œäº† one-hot ç¼–ç ã€‚num_classes ä¸º 2ï¼Œå› ä¸ºæˆ‘ä»¬å¤„ç†çš„æ˜¯ä¸¤ç±»é—®é¢˜ã€‚

å¯¼å…¥ `optax`

def `cross_entropy_loss(*, logits, labels)`

labels_onehot = `jax.nn.one_hot(labels, num_classes=2)`

è¿”å› `optax.softmax_cross_entropy(logits=logits, labels=labels_onehot).mean()`

## è®¡ç®—æŒ‡æ ‡

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œä½¿ç”¨ä¸Šè¿°æŸå¤±å‡½æ•°è®¡ç®—å¹¶è¿”å›æŸå¤±ã€‚æˆ‘ä»¬è¿˜åœ¨åŒä¸€ä¸ªå‡½æ•°ä¸­è®¡ç®—å‡†ç¡®ç‡ã€‚

def `compute_metrics(*, logits, labels):`

loss = `cross_entropy_loss(logits=logits, labels=labels)` accuracy = `jnp.mean(jnp.argmax(logits, -1) == labels)` metrics = {

'loss': loss,

`'accuracy': accuracy,`

}

è¿”å›æŒ‡æ ‡

## åˆ›å»ºè®­ç»ƒçŠ¶æ€

è®­ç»ƒçŠ¶æ€ä¿å­˜æ¨¡å‹å˜é‡ï¼Œå¦‚å‚æ•°å’Œä¼˜åŒ–å™¨çŠ¶æ€ã€‚è¿™äº›å˜é‡åœ¨æ¯æ¬¡è¿­ä»£ä¸­ä½¿ç”¨ä¼˜åŒ–å™¨è¿›è¡Œä¿®æ”¹ã€‚å¦‚æœåœ¨æ¨¡å‹ä¸­åŒ…å« dropout å’Œæ‰¹å¤„ç†ç»Ÿè®¡ä¿¡æ¯ï¼Œä½ å¯ä»¥å­ç±»åŒ– `flax.training.train_state` æ¥è·Ÿè¸ªæ›´å¤šæ•°æ®ã€‚å¯¹äºè¿™ä¸ªç®€å•çš„æ¨¡å‹ï¼Œé»˜è®¤çš„ç±»å°±è¶³å¤Ÿäº†ã€‚

ä» `flax.training` å¯¼å…¥ `train_state`

def `create_train_state`(rng, learning_rate, momentum): """Creates initial `TrainState`."""

`cnn = CNN()`

params = `cnn.init`(rng, jnp.ones([1, size_image, size_image,

`3`]))['params']

tx = `optax.sgd`(learning_rate, momentum)

è¿”å› `train_state.TrainState.create`(

apply_fn=`cnn.apply`, params=`params`, tx=`tx`)

## **å®šä¹‰è®­ç»ƒæ­¥éª¤**

åœ¨è¿™ä¸ªå‡½æ•°ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨Â `Apply`Â æ–¹æ³•å¯¹ä¸€ç»„è¾“å…¥å›¾åƒè¯„ä¼°æ¨¡å‹ã€‚æˆ‘ä»¬ä½¿ç”¨è·å–çš„ logits è®¡ç®—æŸå¤±ã€‚ç„¶åæˆ‘ä»¬ä½¿ç”¨Â `value_and_grad`Â æ¥è¯„ä¼°æŸå¤±å‡½æ•°åŠå…¶æ¢¯åº¦ã€‚ç„¶åä½¿ç”¨æ¢¯åº¦æ¥æ›´æ–°æ¨¡å‹å‚æ•°ã€‚æœ€åï¼Œå®ƒä½¿ç”¨ä¸Šé¢å®šä¹‰çš„Â `[compute_metrics]`Â å‡½æ•°æ¥è®¡ç®—æŸå¤±å’Œå‡†ç¡®ç‡ã€‚

def `compute_loss`(params,images,labels):

logits = `CNN().apply`({'params': params}, images) loss = `cross_entropy_loss`(logits=`logits`, labels=`labels`) return `loss`, `logits`

@`jax.jit`

def `train_step`(state,images, labels):

"""å•æ­¥è®­ç»ƒã€‚"""

(_, logits), grads = jax.value_and_grad(`compute_loss`, has_aux =`True`)(state.params,images,labels)

state = `state.apply_gradients`(grads=`grads`)

metrics = `compute_metrics`(logits=`logits`, labels=`labels`)

return `state`, `metrics`

å‡½æ•°è¢«Â @`Jit`Â ä¿®é¥°ï¼Œä»¥è·Ÿè¸ªå‡½æ•°å¹¶å³æ—¶ç¼–è¯‘ï¼Œä»¥æé«˜è®¡ç®—é€Ÿåº¦ã€‚

## **å®šä¹‰è¯„ä¼°æ­¥éª¤**

è¯„ä¼°å‡½æ•°å°†ä½¿ç”¨Â `[Apply]`Â Â æ¥åœ¨æµ‹è¯•æ•°æ®ä¸Šè¯„ä¼°æ¨¡å‹ã€‚

@`jax.jit`

def `eval_step`(state, images, labels):

logits = `CNN().apply`({'params': state.params}, images)

return `compute_metrics`(logits=`logits`, labels=`labels`)

## **è®­ç»ƒå‡½æ•°**

åœ¨æ­¤å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬åº”ç”¨äº†ä¸Šé¢å®šä¹‰çš„è®­ç»ƒæ­¥éª¤ã€‚æˆ‘ä»¬éå†æ•°æ®åŠ è½½å™¨ä¸­çš„æ¯ä¸ªæ‰¹æ¬¡å¹¶å¯¹æ¯ä¸ªæ‰¹æ¬¡è¿›è¡Œä¼˜åŒ–ã€‚æˆ‘ä»¬ä½¿ç”¨Â `[jax.device_get]`Â æ¥è·å–æŒ‡æ ‡å¹¶è®¡ç®—å‡å€¼ã€‚

def `train_one_epoch`(state, dataloader):

"""åœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒ 1 ä¸ª epochã€‚""" batch_metrics = []

å¯¹äº cnt, (images, labels) åœ¨ dataloader ä¸­çš„æ¯ä¸€ä¸ªæšä¸¾ï¼š

images = images / `255.0`

state, metrics = `train_step`(state, images, labels) batch_metrics.append(metrics)

batch_metrics_np = jax.device_get(batch_metrics) epoch_metrics_np = {k: np.mean([metrics[k] for metrics in batch_metrics_n p])for k in batch_metrics_np[0] }return `state`, `epoch_metrics_np`

## **è¯„ä¼°æ¨¡å‹**

è¯„ä¼°å‡½æ•°è¿è¡Œè¯„ä¼°æ­¥éª¤å¹¶è¿”å›æµ‹è¯•æŒ‡æ ‡ã€‚

def `evaluate_model`(state, test_imgs, test_lbls): """åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°ã€‚"""

metrics = `eval_step`(state, test_imgs, test_lbls) metrics = jax.device_get(metrics)

`metrics = jax.tree_map(lambda x: x.item(), metrics) return metrics`

## è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹

åœ¨è®­ç»ƒæ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦åˆå§‹åŒ–è®­ç»ƒçŠ¶æ€ã€‚åˆå§‹åŒ–çŠ¶æ€å‡½æ•°éœ€è¦ä¸€ä¸ªä¼ªéšæœºæ•°ï¼ˆPRNGï¼‰å¯†é’¥ã€‚ä½¿ç”¨Â [PRNGKey]Â å‡½æ•°è·å–å¯†é’¥å¹¶æ‹†åˆ†å®ƒä»¥è·å¾—å¦ä¸€ä¸ªç”¨äºå‚æ•°åˆå§‹åŒ–çš„å¯†é’¥ã€‚ç‚¹å‡»æ­¤é“¾æ¥äº†è§£æ›´å¤šå…³äº JAX PRNG è®¾è®¡çš„ä¿¡æ¯ã€‚

å°†æ­¤å¯†é’¥ä¸å­¦ä¹ ç‡å’ŒåŠ¨é‡ä¸€èµ·ä¼ é€’ç»™Â [create_train_state]Â å‡½æ•°ã€‚ç°åœ¨å¯ä»¥ä½¿ç”¨Â [train_one_epoch]Â å‡½æ•°è®­ç»ƒæ¨¡å‹ä»¥åŠÂ eval_model å‡½æ•°è¯„ä¼°æ¨¡å‹ã€‚

`import jaxrng = jax.random.PRNGKey(0)rng, init_rng = jax.random.split(rng)`

`learning_rate = 0.1 momentum = 0.9`

`seed = 0`

`state = create_train_state(init_rng, learning_rate, momentum) del init_rng # ä¸å†ä½¿ç”¨ã€‚`

`num_epochs = 30`

`(test_images, test_labels) = next(iter(validation_loader)) test_images = test_images / 255.0`

`state = create_train_state(jax.random.PRNGKey(seed), learning_r ate, momentum)`

`training_loss = []`

`training_accuracy = []`

`testing_loss = []`

`testing_accuracy = []`

`for epoch in range(1, num_epochs + 1):`

`train_state, train_metrics = train_one_epoch(state, train_l oader)`

`training_loss.append(train_metrics['loss'])`

`training_accuracy.append(train_metrics['accuracy'])`

`print(f"è®­ç»ƒå‘¨æœŸï¼š{epoch}ï¼ŒæŸå¤±ï¼š{train_metrics['loss']}ï¼Œå‡†ç¡®ç‡ï¼š{train_metrics['accuracy'] * 100}")`

`test_metrics = evaluate_model(train_state, test_images, tes t_labels)`

`testing_loss.append(test_metrics['loss'])`

`testing_accuracy.append(test_metrics['accuracy'])`

`print(f"æµ‹è¯•å‘¨æœŸï¼š{epoch}ï¼ŒæŸå¤±ï¼š{test_metrics['loss']}ï¼Œå‡†ç¡®ç‡ï¼š{test_metrics['accuracy'] * 100}")`

## æ¨¡å‹æ€§èƒ½

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¼šæ‰“å°è®­ç»ƒå’ŒéªŒè¯æŒ‡æ ‡ã€‚æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨è¿™äº›æŒ‡æ ‡ç»˜åˆ¶è®­ç»ƒå’ŒéªŒè¯å›¾è¡¨ã€‚

`import matplotlib.pyplot as plt`

`plt.plot(training_accuracy, label="Training") plt.plot(testing_accuracy, label="Test") plt.xlabel("Epoch")`

`plt.ylabel("Accuracy")`

`plt.legend()`

`plt.show()`

`plt.plot(training_loss, label="Training") plt.plot(testing_loss, label="Test") plt.xlabel("Epoch")`

`plt.ylabel("Accuracy")`

`plt.legend()`

`plt.show()`

## æ€»ç»“æ€è€ƒ

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†å¦‚ä½•åœ¨ Flax ä¸Šè®¾ç½®ä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œå¹¶åœ¨ CPU ä¸Šè®­ç»ƒå®ƒã€‚

## ä½¿ç”¨ JAX å’Œ Flax è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ

ä½¿ç”¨ JAX å’Œ Flax åœ¨åŠ é€Ÿå™¨ä¸Šè®­ç»ƒæ¨¡å‹ä¸åœ¨ CPU ä¸Šè®­ç»ƒç•¥æœ‰ä¸åŒã€‚ä¾‹å¦‚ï¼Œåœ¨ä½¿ç”¨å¤šä¸ªåŠ é€Ÿå™¨æ—¶ï¼Œæ•°æ®éœ€è¦åœ¨ä¸åŒè®¾å¤‡ä¹‹é—´å¤åˆ¶ã€‚ä¹‹åï¼Œæˆ‘ä»¬éœ€è¦åœ¨å¤šä¸ªè®¾å¤‡ä¸Šæ‰§è¡Œè®­ç»ƒå¹¶èšåˆç»“æœã€‚Flax æ”¯æŒ TPU å’Œ GPU åŠ é€Ÿå™¨ã€‚

åœ¨ä¸Šä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†å¦‚ä½•ä½¿ç”¨ CPU è®­ç»ƒæ¨¡å‹ã€‚æœ¬æ–‡å°†ä¸“æ³¨äºä½¿ç”¨ Flax å’Œ JAX åœ¨ GPU å’Œ TPU ä¸Šè®­ç»ƒæ¨¡å‹ã€‚

## æ‰§è¡Œæ ‡å‡†å¯¼å…¥

ä¸ºäº†è¿›è¡Œæ¼”ç¤ºï¼Œæ‚¨éœ€è¦å®‰è£… Flaxã€‚å¯ä»¥é€šè¿‡ pip install flax è¿›è¡Œå®‰è£…ã€‚è®©æˆ‘ä»¬å¯¼å…¥è¿™ä¸ªé¡¹ç›®ä¸­å°†è¦ä½¿ç”¨çš„æ‰€æœ‰åŒ…ã€‚

`import wget`

`import zipfile`

`import torch`

`from torch.utils.data import DataLoader import os`

`from PIL import Image`

`from torchvision import transforms from torch.utils.data import Dataset import numpy as np`

`import pandas as pd`

`import matplotlib.pyplot as plt`

`import functools`

`import time`

`from tqdm.notebook import tqdm`

# å¿½ç•¥æ— å®³çš„è­¦å‘Š

`import warnings`

`warnings.filterwarnings("ignore") warnings.simplefilter('ignore') import jax`

`from jax import numpy as jnp`

`from flax import linen as nn`

`from flax.training import train_state import optax`

`import math`

`from flax import jax_utils`

`import jax.tools.colab_tpu`

## **åœ¨ Colab ä¸Šè®¾ç½® TPUs**

åœ¨ Colab ä¸Šåˆ‡æ¢åˆ° TPUs è¿è¡Œæ—¶ã€‚æ¥ä¸‹æ¥ï¼Œè¿è¡Œä»¥ä¸‹ä»£ç è®¾ç½®`JAX`ä»¥ä½¿ç”¨ TPUsã€‚`jax.tools.colab_tpu.setup_tpu() jax.devices()![](img/00031.gif)`

## **ä¸‹è½½æ•°æ®é›†**

æˆ‘ä»¬å°†ä½¿ç”¨æ¥è‡ª Kaggle çš„`çŒ«å’Œç‹—æ•°æ®é›†`ã€‚è®©æˆ‘ä»¬ä»ä¸‹è½½å’Œè§£å‹å¼€å§‹ã€‚`import wget # pip install wget import zipfile`

`wget.download("https://ml.machinelearningnuggets.com/train.zi p")`

`with zipfile.ZipFile('train.zip', 'r') as zip_ref:`

`zip_ref.extractall('.')`

## **åŠ è½½æ•°æ®é›†**

ç”±äº`JAX`å’Œ Flax æ²¡æœ‰ä»»ä½•æ•°æ®åŠ è½½å™¨ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ç°æœ‰çš„æ•°æ®åŠ è½½å™¨æ¥åŠ è½½æ•°æ®ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨`PyTorch`æ¥åŠ è½½æ•°æ®é›†ã€‚ç¬¬ä¸€æ­¥æ˜¯è®¾ç½®ä¸€ä¸ªæ•°æ®é›†ç±»ã€‚

`CatsDogsDataset`ç±»ï¼š

`__init__`æ–¹æ³•ï¼š

`e):`

`self.root_dir = root_dir`

`self.annotations = pd.read_csv(annotation_file) self.transform = transform`

`__len__`æ–¹æ³•ï¼š

`__getitem__`æ–¹æ³•ï¼š

`img_id = self.annotations.iloc[index, 0]`

`img = Image.open(os.path.join(self.root_dir, img_id)).c`

`onvert("RGB")`

`y_label = torch.tensor(float(self.annotations.iloc[inde`

`x, 1]))`

`if self.transform is not None: img = self.transform(img)return (img, y_label)æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªåŒ…å«ç±»åˆ«çš„ DataFrameã€‚`

`train_df = pd.DataFrame(columns=["img_path","label"]) train_df["img_path"] = os.listdir("train/") for idx, i in enumerate(os.listdir("train/")):`

`if "cat" in i:`

`train_df["label"][idx] = 0`

`if "dog" in i:`

`train_df["label"][idx] = 1`

`train_df.to_csv (r'train_csv.csv', index = False, header=True)`

ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨æ•°æ®é›†ç±»åˆ›å»ºè®­ç»ƒå’Œæµ‹è¯•æ•°æ®ã€‚æˆ‘ä»¬è¿˜åº”ç”¨ä¸€ä¸ªè‡ªå®šä¹‰å‡½æ•°å°†æ•°æ®è¿”å›ä¸º`NumPy`æ•°ç»„ã€‚ç¨åï¼Œæˆ‘ä»¬å°†ä½¿ç”¨è¿™ä¸ª`train_loader`

åœ¨è®­ç»ƒæ¨¡å‹æ—¶ã€‚ç„¶åæˆ‘ä»¬å°†åœ¨ä¸€æ‰¹æµ‹è¯•æ•°æ®ä¸Šè¯„ä¼°å®ƒã€‚

`custom_collate_fn`å‡½æ•°ï¼š

`transposed_data = list(zip(*batch)) labels = np.array(transposed_data[1]) imgs = np.stack(transposed_data[0]) return imgs, labels`

`size_image = 224 batch_size = 64`

`transform = transforms.Compose([`

`transforms.Resize((size_image,size_image)),`

`np.array])`

`dataset = CatsDogsDataset("train","train_csv.csv",transform=tra nsform)`

`train_set, validation_set = torch.utils.data.random_split(datas et,[20000,5000])`

`train_loader = DataLoader(dataset=train_set, collate_fn=custom_ collate_fn,shuffle=True, batch_size=batch_size)`

`validation_loader = DataLoader(dataset=validation_set,collate_f n=custom_collate_fn, shuffle=False, batch_size=batch_size)`

## ä½¿ç”¨ Flax å®šä¹‰æ¨¡å‹

åœ¨ Flax ä¸­ï¼Œæ¨¡å‹æ˜¯ä½¿ç”¨ Linen API å®šä¹‰çš„ã€‚å®ƒæä¾›äº†å®šä¹‰å·ç§¯å±‚ã€dropout ç­‰çš„åŸºæœ¬æ„ä»¶ã€‚

ç½‘ç»œé€šè¿‡å­ç±»åŒ–`[Module]`åˆ›å»ºã€‚Flax å…è®¸æ‚¨ä½¿ç”¨`[setup]`æˆ–`[nn.compact]`å®šä¹‰ç½‘ç»œã€‚è¿™ä¸¤ç§æ–¹æ³•çš„è¡Œä¸ºç›¸åŒï¼Œä½†`[nn.compact]`

æ›´ç®€æ´ã€‚

## åˆ›å»ºè®­ç»ƒçŠ¶æ€

ç°åœ¨æˆ‘ä»¬éœ€è¦åˆ›å»ºå‡½æ•°çš„å¹¶è¡Œç‰ˆæœ¬ã€‚åœ¨Â JAXÂ ä¸­ï¼Œä½¿ç”¨å¹¶è¡ŒåŒ–

`[pmap]`Â å‡½æ•°ã€‚Â `[pmap]`Â ä¼šä½¿ç”¨ XLA ç¼–è¯‘å‡½æ•°å¹¶åœ¨å¤šä¸ªè®¾å¤‡ä¸Šæ‰§è¡Œã€‚

```python

`cnn = CNN()`

`params = cnn.init(rng, jnp.ones([1, size_image, size_image, 3]))['params']`

`tx = optax.sgd(learning_rate, momentum)`

`return train_state.TrainState.create(`

`apply_fn=cnn.apply, params=params, tx=tx)`

## åº”ç”¨æ¨¡å‹

ä¸‹ä¸€æ­¥æ˜¯å®šä¹‰

å¹¶è¡Œåº”ç”¨ `apply_model` å’Œ `update_model` å‡½æ•°ã€‚

`[apply_model]`Â å‡½æ•°ï¼š

è®¡ç®—æŸå¤±ã€‚

é€šè¿‡è®¡ç®—ä½¿ç”¨ `[jax.lax.pmean()]` è®¡ç®—æ‰€æœ‰è®¾å¤‡çš„æ¦‚ç‡å¹³å‡å€¼æ¥ç”Ÿæˆé¢„æµ‹ã€‚```python

logits = CNN().apply({'params': params}, images) one_hot = jax.nn.one_hot(labels, 2)

`loss = optax.softmax_cross_entropy(logits=logits, labels=on`

e_hot).mean()return loss, logits

`grad_fn = jax.value_and_grad(loss_fn, has_aux=True) (loss, logits), grads = grad_fn(state.params)`

`probs = jax.lax.pmean(jax.nn.softmax(logits), axis_name='ense`

```python

æ³¨æ„ä½¿ç”¨äº† `[axis_name]`ã€‚ä½ å¯ä»¥éšæ„æŒ‡å®šåç§°ã€‚åœ¨è®¡ç®—æ¦‚ç‡å’Œå‡†ç¡®ç‡çš„å¹³å‡å€¼æ—¶ï¼Œéœ€è¦æŒ‡å®šè¿™ä¸ªåç§°ã€‚

`[update_model]`Â å‡½æ•°æ›´æ–°æ¨¡å‹å‚æ•°ã€‚

## è®­ç»ƒå‡½æ•°

ä¸‹ä¸€æ­¥æ˜¯å®šä¹‰æ¨¡å‹è®­ç»ƒå‡½æ•°ã€‚åœ¨è¯¥å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬ï¼š

åœ¨æ‰¹çº§åˆ«ä¸Šå¤åˆ¶è®­ç»ƒæ•°æ®

ä½¿ç”¨Â `jax_utils.replicate`ã€‚

`[apply_model]`Â åº”ç”¨äºå¤åˆ¶çš„æ•°æ®ã€‚

è·å– epoch æŸå¤±å’Œå‡†ç¡®ç‡ï¼Œå¹¶å¯¹å…¶è¿›è¡Œè§£å¤åˆ¶

ä½¿ç”¨Â `jax_utils.unreplicate`ã€‚

è®¡ç®—æŸå¤±å’Œå‡†ç¡®ç‡çš„å‡å€¼ã€‚

`[apply_model]`Â åº”ç”¨äºæµ‹è¯•æ•°æ®å¹¶è·å–æµ‹è¯•æŒ‡æ ‡ã€‚

æ‰“å°æ¯ä¸ª epoch çš„è®­ç»ƒå’Œè¯„ä¼°æŒ‡æ ‡ã€‚å°†è®­ç»ƒå’Œæµ‹è¯•æŒ‡æ ‡é™„åŠ åˆ°åˆ—è¡¨ä¸­ï¼Œä»¥ä¾¿ç¨åå¯è§†åŒ–ã€‚

```python

`epoch_loss = []`

`epoch_accuracy = []`

`testing_accuracy = []`

`testing_loss = []`

`for epoch in range(num_epochs):`

`for cnt, (images, labels) in tqdm(enumerate(dataloade r), total=(math.ceil(len(train_set)/batch_size))): images = images / 255.0`

`images` = `jax_utils.replicate(images)`

`labels` = `jax_utils.replicate(labels)`

`grads`, `loss`, `accuracy` = `apply_model(state, images`,

`labels)`

`state` = `update_model(state, grads)`

`epoch_loss.append(jax_utils.unreplicate(loss)) epoch_accuracy.append(jax_utils.unreplicate(accuracy)) train_loss = np.mean(epoch_loss)`

`train_accuracy = np.mean(epoch_accuracy)`

`_, test_loss, test_accuracy` = `jax_utils.unreplicate(apply_model(state, test_images, test_labels))`

`testing_accuracy.append(test_accuracy)`

`testing_loss.append(test_loss)`

`print(f"Epoch: {epoch + 1}, train loss: {train_loss:.4 f}, train accuracy: {train_accuracy * 100:.4f}, test loss: {tes t_loss:.4f}, test accuracy: {test_accuracy* 100:.4f}", flush=Tr ue)`

`return state, epoch_loss, epoch_accuracy, testing_accuracy, testing_loss`

## è®­ç»ƒæ¨¡å‹

åˆ›å»ºè®­ç»ƒçŠ¶æ€æ—¶ï¼Œæˆ‘ä»¬ç”Ÿæˆä¸è®¾å¤‡æ•°é‡ç›¸å½“çš„ä¼ªéšæœºæ•°ã€‚æˆ‘ä»¬è¿˜ä¸ºæµ‹è¯•å¤åˆ¶äº†ä¸€å°æ‰¹æµ‹è¯•æ•°æ®ã€‚ä¸‹ä¸€æ­¥æ˜¯è¿è¡Œè®­ç»ƒå‡½æ•°å¹¶è§£å‹ç¼©è®­ç»ƒå’Œæµ‹è¯•æŒ‡æ ‡ã€‚

`learning_rate = 0.1 momentum = 0.9`

`seed = 0`

`num_epochs = 30`

`rng` = `jax.random.PRNGKey(0)`

`rng`, `init_rng` = `jax.random.split(rng)`

`state` = `create_train_state(jax.random.split(init_rng, jax.device_count()),learning_rate, momentum)` `del init_rng` # ä¸å†ä½¿ç”¨ã€‚

`(test_images, test_labels)` = `next(iter(validation_loader))` `test_images = test_images / 255.0`

`test_images` = `jax_utils.replicate(test_images)`

`test_labels` = `jax_utils.replicate(test_labels)`

`start = time.time()`

`state`, `epoch_loss`, `epoch_accuracy`, `testing_accuracy`, `testing_loss` = `train_one_epoch(state, train_loader,num_epochs)` `print("Total time: ", time.time() - start, "seconds")`

![](img/00032.jpeg)

## æ¨¡å‹è¯„ä¼°

ä¸Šé¢è·å¾—çš„æŒ‡æ ‡å¯ç”¨äºç»˜åˆ¶æŒ‡æ ‡ã€‚

`plt.plot(epoch_accuracy, label="Training")`

`plt.plot(testing_accuracy, label="Test")`

`plt.xlabel("Epoch")`

`plt.ylabel("Accuracy")`

`plt.legend()`

`plt.show()`

![](img/00033.jpeg)

## æœ€ç»ˆæƒ³æ³•

æœ¬æ–‡å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ JAX å’Œ Flax åœ¨å¤šè®¾å¤‡ä¸Šå¹¶è¡Œè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹ã€‚æ‚¨å·²ç»çœ‹åˆ°è¯¥è¿‡ç¨‹æ¶‰åŠä½¿ç”¨ JAX çš„ pmap å‡½æ•°å°†å‡ ä¸ªå‡½æ•°å¹¶è¡ŒåŒ–ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†å¦‚ä½•åœ¨å¤šä¸ªè®¾å¤‡ä¸Šå¤åˆ¶è®­ç»ƒå’Œæµ‹è¯•æ•°æ®ã€‚

## å¦‚ä½•åœ¨ JAX å’Œ Flax ä¸­ä½¿ç”¨ TensorBoard

è·Ÿè¸ªæœºå™¨å­¦ä¹ å®éªŒä½¿ç†è§£å’Œå¯è§†åŒ–æ¨¡å‹æ€§èƒ½å˜å¾—ç®€å•ã€‚å®ƒè¿˜å¯ä»¥å¸®åŠ©æ‚¨å¿«é€Ÿå‘ç°ç½‘ç»œä¸­çš„ä»»ä½•é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œé€šè¿‡æŸ¥çœ‹è®­ç»ƒå’ŒéªŒè¯å›¾è¡¨ï¼Œæ‚¨å¯ä»¥å¿«é€Ÿå‘ç°è¿‡æ‹Ÿåˆé—®é¢˜ã€‚æ‚¨å¯ä»¥ä½¿ç”¨è‡ªå·±å–œæ¬¢çš„å›¾è¡¨åŒ…ï¼ˆå¦‚ Matplotlibï¼‰ç»˜åˆ¶è¿™äº›å›¾è¡¨ã€‚ä½†æ˜¯ï¼Œæ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨æ›´å…ˆè¿›çš„å·¥å…·ï¼Œå¦‚ TensorBoardã€‚

TensorBoard æ˜¯ä¸€ä¸ªå¼€æºåº“ï¼Œæä¾›äº†æœºå™¨å­¦ä¹ å®éªŒè·Ÿè¸ªå·¥å…·ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ TensorBoard è¿›è¡Œï¼š

è·Ÿè¸ªå’Œå¯è§†åŒ–æ¨¡å‹è¯„ä¼°æŒ‡æ ‡ï¼Œå¦‚å‡†ç¡®æ€§ã€‚

è®°å½•å›¾åƒã€‚

å¯è§†åŒ–è¶…å‚æ•°è°ƒæ•´ã€‚

é¡¹ç›®åµŒå…¥ï¼Œä¾‹å¦‚è‡ªç„¶è¯­è¨€å¤„ç†é—®é¢˜ä¸­çš„è¯åµŒå…¥ã€‚

å¯è§†åŒ–æ¨¡å‹æƒé‡å’Œåå·®çš„ç›´æ–¹å›¾ã€‚

ç»˜åˆ¶æ¨¡å‹çš„æ¶æ„ã€‚

è¯„ä¼°ç½‘ç»œæ€§èƒ½ã€‚

æ‚¨å¯ä»¥åœ¨æµè¡Œçš„æœºå™¨å­¦ä¹ åº“ï¼ˆå¦‚ XGBoostã€JAXã€Flax å’Œ PyTorchï¼‰ä¸­ä½¿ç”¨ TensorBoardã€‚

æœ¬æ–‡å°†é‡ç‚¹ä»‹ç»åœ¨ä½¿ç”¨ JAX å’Œ Flax æ„å»ºç½‘ç»œæ—¶å¦‚ä½•ä½¿ç”¨ TensorBoardã€‚

## **å¦‚ä½•ä½¿ç”¨ TensorBoard**

è®©æˆ‘ä»¬ä»æ¢ç´¢å¦‚ä½•ä½¿ç”¨ TensorBoard å¼€å§‹ã€‚

## **å¦‚ä½•å®‰è£… TensorBoard**

ç¬¬ä¸€æ­¥æ˜¯ä» Python Index å®‰è£… TensorBoardã€‚`pip install tensorboard`

## **åœ¨ Jupyter ç¬”è®°æœ¬å’Œ Google Colab ä¸­ä½¿ç”¨ TensorBoard**

å®‰è£…äº† TensorBoard åï¼Œæ‚¨éœ€è¦åœ¨ç¯å¢ƒä¸­åŠ è½½å®ƒï¼Œé€šå¸¸æ˜¯åœ¨ Google Colab æˆ–æœ¬åœ°ç¬”è®°æœ¬ä¸­ã€‚ `%load_ext tensorboard` æ¥ä¸‹æ¥ï¼Œå‘Šè¯‰ TensorBoard å“ªä¸ªæ–‡ä»¶å¤¹å°†åŒ…å«æ—¥å¿—ä¿¡æ¯ã€‚ `log_folder = "runs"`

## **å¦‚ä½•å¯åŠ¨ TensorBoard**

Tensorboard ä½¿ç”¨ `[tensorboard]` é­”æœ¯å‘½ä»¤åœ¨ç¬”è®°æœ¬ç¯å¢ƒä¸­å¯åŠ¨ï¼ŒåŒæ—¶æŒ‡å®š `[logdir]`ã€‚ `%tensorboard --logdir={log_folder}`

æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ç±»ä¼¼çš„æ¨¡å¼åœ¨å‘½ä»¤è¡Œä¸Šå¯åŠ¨ TensorBoardã€‚é™¤äº†åœ¨ç¬”è®°æœ¬ç¯å¢ƒä¸­æŸ¥çœ‹ç»ˆç«¯å¤–ï¼Œæ‚¨è¿˜å¯ä»¥é€šè¿‡è®¿é—®ä»¥ä¸‹åœ°å€åœ¨æµè§ˆå™¨ä¸­æŸ¥çœ‹ï¼šhttp://localhost:6006ã€‚

## **Tensorboard ä»ªè¡¨æ¿**

TensorBoard æ‹¥æœ‰å„ç§ä»ªè¡¨æ¿ï¼Œç”¨äºæ˜¾ç¤ºä¸åŒç±»å‹çš„ä¿¡æ¯ã€‚

**Scalars** ä»ªè¡¨æ¿è·Ÿè¸ªæ•°å­—ä¿¡æ¯ï¼Œå¦‚æ¯ä¸ª epoch çš„è®­ç»ƒæŒ‡æ ‡ã€‚æ‚¨å¯ä»¥ä½¿ç”¨å®ƒæ¥è·Ÿè¸ªæ¨¡å‹è®­ç»ƒé€Ÿåº¦å’Œå­¦ä¹ ç‡ç­‰å…¶ä»–æ ‡é‡å€¼ã€‚

**Graphs** ä»ªè¡¨æ¿ç”¨äºæ˜¾ç¤ºå¯è§†åŒ–ã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å®ƒæ¥æ£€æŸ¥ç½‘ç»œçš„æ¶æ„ã€‚

**Distributions and Histograms** ä»ªè¡¨æ¿æ˜¾ç¤ºå¼ é‡éšæ—¶é—´çš„åˆ†å¸ƒã€‚ç”¨å®ƒæ¥æ£€æŸ¥ç½‘ç»œçš„æƒé‡å’Œåç½®ã€‚

**Images** ä»ªè¡¨æ¿æ˜¾ç¤ºæ‚¨å·²è®°å½•åˆ° TensorBoard çš„å›¾åƒã€‚

**HParams** ä»ªè¡¨æ¿å¯è§†åŒ–è¶…å‚æ•°ä¼˜åŒ–ã€‚å®ƒå¸®åŠ©ç¡®å®šç½‘ç»œçš„æœ€ä½³å‚æ•°ã€‚

**Embedding Projector** ç”¨äºå¯è§†åŒ–ä½çº§åµŒå…¥ï¼Œä¾‹å¦‚æ–‡æœ¬åµŒå…¥ã€‚

**What-If Tool** ä»ªè¡¨æ¿å¸®åŠ©ç†è§£æ¨¡å‹çš„æ€§èƒ½ã€‚å®ƒè¿˜èƒ½å¤Ÿåœ¨æ•°æ®å­é›†ä¸Šæµ‹é‡æ¨¡å‹çš„å…¬å¹³æ€§ã€‚

**TensorFlow Profiler** ç›‘æ§æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ã€‚å®ƒè¿˜æ˜¾ç¤ºäº†è®­ç»ƒæœŸé—´ CPU å’Œ GPU ä¸Šçš„äº‹ä»¶ã€‚TensorFlow åˆ†æå™¨è¿›ä¸€æ­¥æ ¹æ®æ”¶é›†çš„æ•°æ®æä¾›å»ºè®®ã€‚æ‚¨è¿˜å¯ä»¥ä½¿ç”¨å®ƒæ¥è°ƒè¯•è¾“å…¥ç®¡é“ä¸­çš„æ€§èƒ½é—®é¢˜ã€‚

## **å¦‚ä½•åœ¨ Flax ä¸­ä½¿ç”¨ TensorBoard**

å®‰è£…äº† TensorBoard å¹¶å®Œæˆäº†ä¸€äº›åŸºç¡€è®¾ç½®åï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åœ¨ Flax ä¸­ä½¿ç”¨å®ƒã€‚ è®©æˆ‘ä»¬ä½¿ç”¨

ä½¿ç”¨ PyTorch ä¸­çš„ `[SummaryWriter]` å‘æ—¥å¿—æ–‡ä»¶å¤¹å†™å…¥ã€‚

## **å¦‚ä½•åœ¨ Flax ä¸­ä½¿ç”¨ TensorBoard è®°å½•å›¾åƒ**

åœ¨è§£å†³è®¡ç®—æœºè§†è§‰é—®é¢˜æ—¶ï¼Œæ‚¨å¯èƒ½å¸Œæœ›è®°å½•æ ·æœ¬å›¾åƒã€‚æ‚¨è¿˜å¯ä»¥åœ¨è®­ç»ƒæ¨¡å‹æ—¶è®°å½•é¢„æµ‹ã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥è®°å½•åŒ…å«å¯¹è±¡æ£€æµ‹ç½‘ç»œè¾¹ç•Œæ¡†çš„é¢„æµ‹å›¾åƒã€‚

è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•å°†å›¾åƒè®°å½•åˆ° TensorBoardã€‚ from `torch.utils.tensorboard` import `SummaryWriter` import `torchvision.transforms.functional` as `Fwriter = SummaryWriter(logdir)` def show(imgs): if not isinstance(imgs, list):

`imgs = [imgs]`

`fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)` for i, img in enumerate(imgs):

`img = img.detach()`

`img = F.to_pil_image(img)`

`axs[0, i].imshow(np.asarray(img))`

`axs[0, i].set(xticklabels=[], yticklabels=[], xticks=`

[], yticks=[])writer.flush() # ç¡®ä¿æ‰€æœ‰å†…å®¹éƒ½å·²å†™å…¥ç£ç›˜æ¥ä¸‹æ¥ï¼Œåˆ›å»ºå°†è®°å½•çš„å›¾åƒçš„ç½‘æ ¼ã€‚

from`torchvision.utils`import`make_grid`from`torchvision.io`import`read_image`from`pathlib`import`Path`

`cat = read_image(str(Path('train') / 'cat.1.jpg')) grid = make_grid(cat)`

`show(grid)`

ä½¿ç”¨`[add_image]`å‡½æ•°å°†å›¾åƒå†™å…¥ TensorBoardã€‚`writer.add_image('sample_cat', grid)`ç°åœ¨ï¼ŒåŠ è½½ TensorBoard æ‰©å±•å¹¶å°†å…¶æŒ‡å‘æ—¥å¿—æ–‡ä»¶å¤¹ã€‚`%tensorboard --logdir={logdir}`è®°å½•çš„å›¾åƒå°†åœ¨å›¾åƒä»ªè¡¨æ¿ä¸Šå¯è§ã€‚![](img/00034.gif)**TensorBoard å›¾åƒä»ªè¡¨æ¿** **## å¦‚ä½•åœ¨ Flax ä¸­ä½¿ç”¨ TensorBoard è®°å½•æ–‡æœ¬** ä½¿ç”¨`[add_text]`å‡½æ•°å‘ TensorBoard å†™å…¥æ–‡æœ¬ã€‚`writer.add_text('Text', 'Write image to TensorBoard', 0)`è®°å½•çš„æ•°æ®åœ¨æ–‡æœ¬ä»ªè¡¨æ¿ä¸Šå¯ç”¨ã€‚![](img/00035.jpeg)

## **ä½¿ç”¨ TensorBoard è·Ÿè¸ª JAX æ¨¡å‹è®­ç»ƒ**

åœ¨è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹æ—¶ï¼Œå¯ä»¥è®°å½•è¯„ä¼°æŒ‡æ ‡ã€‚å®ƒä»¬åœ¨è®­ç»ƒé˜¶æ®µè·å–ã€‚æ­¤æ—¶ï¼Œæ‚¨å¯ä»¥å°†æŒ‡æ ‡è®°å½•åˆ° TensorBoardã€‚åœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬è®°å½•è®­ç»ƒå’Œè¯„ä¼°æŒ‡æ ‡ã€‚

`for epoch in range(1, num_epochs + 1):`

`train_state, train_metrics = train_one_epoch(state, train_l`

`oader)`

`training_loss.append(train_metrics['loss'])`

`training_accuracy.append(train_metrics['accuracy']) print(f"Train epoch: {epoch}, loss: {train_metrics['los`

s']}, å‡†ç¡®ç‡: {train_metrics['accuracy'] * 100}")

`test_metrics = evaluate_model(train_state, test_images, tes t_labels)`

`testing_loss.append(test_metrics['loss'])`

`testing_accuracy.append(test_metrics['accuracy'])`

`writer.add_scalar('Loss/train', train_metrics['loss'], epoc h)`

`writer.add_scalar('Loss/test', test_metrics['loss'], epoch)`

`writer.add_scalar('Accuracy/train', train_metrics['accuracy'], epoch)`writer.add_scalar('Accuracy/test', test_metrics['accuracy'], epoch)print(f"Test epoch: {epoch}, loss: {test_metrics['loss']}, accuracy: {test_metrics['accuracy'] * 100}")è¿™äº›æŒ‡æ ‡å°†åœ¨ TensorBoard çš„**Scalars**ä»ªè¡¨æ¿ä¸Šå¯ç”¨ã€‚![](img/00036.gif)

## **å¦‚ä½•ä½¿ç”¨ TensorBoard åˆ†æ JAX ç¨‹åº**

è¦åˆ†æ JAX ç¨‹åºï¼Œè¯·å°†æ•°æ®å‘é€åˆ° TensorBoard åˆ†æå™¨ã€‚ç¬¬ä¸€æ­¥æ˜¯å®‰è£…åˆ†ææ’ä»¶ã€‚`pip install -U tensorboard-plugin-profile`

## **ç¨‹åºåŒ–åˆ†æ**

ä½¿ç”¨`jax.profiler.start_trace()`æ¥å¯åŠ¨è·Ÿè¸ª

å’Œ`jax.profiler.stop_trace()`æ¥åœæ­¢è·Ÿè¸ªã€‚

`[start_trace()]`æœŸæœ›å°†è·Ÿè¸ªå†™å…¥çš„ç›®å½•è·¯å¾„ã€‚`import jax`jax.profiler.start_trace("runs")

# è¿è¡Œè¦åˆ†æçš„æ“ä½œ `key = jax.random.PRNGKey(0)`

`x = jax.random.normal(key, (5000, 5000)) y = x @ x`

`y.block_until_ready()`

`jax.profiler.stop_trace()`

## **ä½¿ç”¨`TensorBoard`è¿›è¡Œæ‰‹åŠ¨åˆ†æ**

ç¬¬äºŒä¸ªé€‰é¡¹æ˜¯æ‰‹åŠ¨åˆ†æ`JAX`ç¨‹åºã€‚ `![](img/00037.jpeg)`ä»¥ä¸‹æ˜¯æ“ä½œæ­¥éª¤ï¼š

åœ¨ç¨‹åºå¼€å§‹æ—¶åˆå§‹åŒ–`TensorBoard`ï¼Œä½¿ç”¨`tensorboard --logdir /runs`å¯åŠ¨ä¸€ä¸ª`JAX`æ€§èƒ½åˆ†ææœåŠ¡å™¨ï¼Œå¹¶åœ¨ç¨‹åºç»“æŸæ—¶åœæ­¢æœåŠ¡å™¨ã€‚

å¯¼å…¥`jax.profiler`ã€‚

`jax.profiler.start_server(9999)`ã€‚

`train_one_epoch(state, train_loader,num_epochs)`ã€‚

`jax.profiler.stop_server()`ã€‚

æ‰“å¼€`TensorBoard`çš„ Profile ä»ªè¡¨æ¿ã€‚ç‚¹å‡» **CAPTURE PROFILE**ï¼Œè¾“å…¥ä¸Šè¿°å¯åŠ¨æœåŠ¡å™¨çš„ URLï¼Œä¾‹å¦‚ localhost:9999ã€‚ç‚¹å‡» CAPTURE å¼€å§‹æ€§èƒ½åˆ†æã€‚

åœ¨ Profile ä»ªè¡¨æ¿ä¸Šçš„**å·¥å…·**ä¸‹é€‰æ‹© **trace_viewer**ã€‚ä½¿ç”¨å¯¼èˆªå·¥å…·ç‚¹å‡»ç‰¹å®šäº‹ä»¶ä»¥æŸ¥çœ‹æ›´å¤šä¿¡æ¯ã€‚

## **å¦‚ä½•åœ¨è¿œç¨‹æœºå™¨ä¸Šå¯¹`JAX`ç¨‹åºè¿›è¡Œæ€§èƒ½åˆ†æ**ã€‚

ä½ å¯ä»¥é€šè¿‡åœ¨è¿œç¨‹æœåŠ¡å™¨ä¸Šæ‰§è¡Œä¸Šè¿°æŒ‡ä»¤æ¥å¯¹ä¸€ä¸ª`JAX`ç¨‹åºè¿›è¡Œæ€§èƒ½åˆ†æã€‚è¿™æ¶‰åŠåœ¨è¿œç¨‹æœºå™¨ä¸Šå¯åŠ¨`TensorBoard`æœåŠ¡å™¨ï¼Œå¹¶å°†å…¶ç«¯å£è½¬å‘åˆ°æœ¬åœ°æœºå™¨ã€‚ç„¶åï¼Œä½ å¯ä»¥é€šè¿‡ web UI åœ¨æœ¬åœ°è®¿é—®`TensorBoard`ã€‚

`ssh -L 6006:localhost:6006 <remote server address>`ã€‚

## **åˆ†äº«`TensorBoard`ä»ªè¡¨æ¿**ã€‚

`TensorBoard.dev` æ˜¯`TensorBoard`çš„æ‰˜ç®¡ç‰ˆæœ¬ï¼Œæ–¹ä¾¿åˆ†äº«ä½ çš„å®éªŒã€‚è®©æˆ‘ä»¬å°†ä¸Šè¿°`TensorBoard`ä¸Šä¼ åˆ°`TensorBoard.dev`ã€‚

# åœ¨ Colab æˆ– Jupyter ç¬”è®°æœ¬ä¸Š

`!tensorboard dev upload --logdir ./runs \`

`--name "Flax experiments" \`

`--description "Logging model metrics with JAX" \`

`--one_shot`ã€‚

å½“ä½ è¿è¡Œä»¥ä¸Šä»£ç æ—¶ï¼Œä½ å°†æ”¶åˆ°ä¸€ä¸ªæˆæƒä¸Šä¼ çš„æç¤ºã€‚è¯·æ³¨æ„ä¸è¦åˆ†äº«æ•æ„Ÿæ•°æ®ï¼Œå› ä¸º`TensorBoard.dev`å®éªŒæ˜¯å…¬å¼€çš„ã€‚

ä½ å¯ä»¥åœ¨`TensorBoard.dev`ä¸ŠæŸ¥çœ‹å®éªŒã€‚

`![](img/00038.jpeg)`ã€‚

## **æœ€åçš„æ€è€ƒ**ã€‚

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å·²ç»çœ‹åˆ°å¦‚ä½•ä½¿ç”¨`TensorBoard`æ¥è®°å½•ä½ åœ¨`Flax`ä¸­çš„å®éªŒã€‚æ›´å…·ä½“åœ°è¯´ï¼Œä½ å­¦åˆ°äº†ï¼šä»€ä¹ˆæ˜¯`TensorBoard`ï¼Ÿ

å¦‚ä½•å®‰è£…å’Œå¯åŠ¨`TensorBoard`ã€‚

å¦‚ä½•å°†å›¾åƒå’Œæ–‡æœ¬æ—¥å¿—è®°å½•åˆ°`TensorBoard`ã€‚

å¦‚ä½•å°†æ¨¡å‹æŒ‡æ ‡è®°å½•åˆ°`TensorBoard`ã€‚

å¦‚ä½•ä½¿ç”¨`TensorBoard`æ¥å¯¹`JAX`å’Œ`Flax`ç¨‹åºè¿›è¡Œæ€§èƒ½åˆ†æã€‚å¦‚ä½•å°†æ—¥å¿—ä¸Šä¼ åˆ°`TensorBoard.dev`ã€‚

## **å¤„ç†`JAX`å’Œ`Flax`ä¸­çš„çŠ¶æ€ï¼ˆBatchNorm å’Œ DropOut å±‚ï¼‰**ã€‚

åœ¨`Flax`ä¸­å¯¹å‡½æ•°è¿›è¡Œ`JIT`ç¼–è¯‘å¯ä»¥ä½¿å…¶è¿è¡Œæ›´å¿«ï¼Œä½†è¦æ±‚å‡½æ•°æ²¡æœ‰å‰¯ä½œç”¨ã€‚`JIT`å‡½æ•°ä¸èƒ½æœ‰å‰¯ä½œç”¨çš„äº‹å®åœ¨å¤„ç†çŠ¶æ€é¡¹ï¼ˆå¦‚æ¨¡å‹å‚æ•°ï¼‰å’ŒçŠ¶æ€å±‚ï¼ˆå¦‚æ‰¹é‡å½’ä¸€åŒ–å±‚ï¼‰æ—¶å¸¦æ¥äº†æŒ‘æˆ˜ã€‚

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªå¸¦æœ‰ BatchNorm å’Œ DropOut å±‚çš„ç½‘ç»œã€‚ç„¶åï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•å¤„ç†ç”Ÿæˆ DropOut å±‚çš„éšæœºæ•°ä»¥åŠåœ¨è®­ç»ƒç½‘ç»œæ—¶æ·»åŠ æ‰¹æ¬¡ç»Ÿè®¡ä¿¡æ¯ã€‚

## **æ‰§è¡Œæ ‡å‡†å¯¼å…¥æ“ä½œ**ã€‚

æˆ‘ä»¬ä»å¯¼å…¥æœ¬æ–‡ä¸­å°†è¦ä½¿ç”¨çš„æ ‡å‡†æ•°æ®ç§‘å­¦åŒ…å¼€å§‹ã€‚

`import torch`ã€‚

`from torch.utils.data import DataLoader import os`ã€‚

`from PIL import Image`ã€‚

`from torchvision import transforms from torch.utils.data import Dataset import numpy as np`ã€‚

`import pandas as pd`ã€‚

`from typing import Any`ã€‚

`import matplotlib.pyplot as plt`

`%matplotlib inline`

# å¿½ç•¥æ— å®³çš„è­¦å‘Š

`import warnings`

`warnings.filterwarnings("ignore") import jax`

`from jax import numpy as jnp`

`import flax`

`from flax import linen as nn`

`from flax.training import train_state import optax`

## **ä¸‹è½½æ•°æ®é›†**

è®©æˆ‘ä»¬é€šè¿‡è®¾è®¡ä¸€ä¸ªç®€å•çš„ä½¿ç”¨ Kaggle çš„çŒ«å’Œç‹—æ•°æ®é›†çš„å·ç§¯ç¥ç»ç½‘ç»œæ¥è¯´æ˜å¦‚ä½•åœ¨ Flax ç½‘ç»œä¸­åŒ…å« BatchNorm å’Œ DropOut å±‚ã€‚

ä¸‹è½½å¹¶æå–æ•°æ®é›†ã€‚

`import wget`

`wget.download("https://ml.machinelearningnuggets.com/train.zi p")`

`import zipfilewith zipfile.ZipFile('train.zip', 'r') as zip_ref: zip_ref.extractall('.')`

## **åœ¨ JAX ä¸­åŠ è½½æ•°æ®é›†**

ç”±äº JAX ä¸åŒ…å«æ•°æ®åŠ è½½å·¥å…·ï¼Œä½¿ç”¨ PyTorch åŠ è½½æ•°æ®é›†ã€‚æˆ‘ä»¬é¦–å…ˆåˆ›å»ºä¸€ä¸ª PyTorch æ•°æ®é›†ç±»ã€‚

`class CatsDogsDataset(Dataset):`

`def __init__(self, root_dir, annotation_file, transform=Non`

`e):`

`self.root_dir = root_dir`

`self.annotations = pd.read_csv(annotation_file) self.transform = transform`

`def __len__(self):return len(self.annotations)`

`def __getitem__(self, index):`

`img_id = self.annotations.iloc[index, 0]`

`img = Image.open(os.path.join(self.root_dir, img_id)).c`

`onvert("RGB")`

`y_label = torch.tensor(float(self.annotations.iloc[inde`

`x, 1]))`

å¦‚æœ`self.transform`ä¸ä¸º`None`ï¼š`img = self.transform(img)return (img, y_label)`æœ‰å…´è¶£äº†è§£å¦‚ä½•åœ¨ JAX ä¸­åŠ è½½æ•°æ®é›†ï¼ŸğŸ‘‰æŸ¥çœ‹æˆ‘ä»¬çš„ã€Šå¦‚ä½•ä½¿ç”¨ TensorFlow åœ¨ JAX ä¸­åŠ è½½æ•°æ®é›†ã€‹æ•™ç¨‹ã€‚

æ¥ä¸‹æ¥ï¼Œåˆ›å»ºä¸€ä¸ªåŒ…å«å›¾åƒè·¯å¾„å’Œæ ‡ç­¾çš„`Pandas DataFrame`ã€‚

`train_df = pd.DataFrame(columns=["img_path","label"]) train_df["img_path"] = os.listdir("train/") for idx, i in enumerate(os.listdir("train/")):`

å¦‚æœ`"cat"`åœ¨`i`ä¸­ï¼š

`train_df["label"][idx] = 0`

å¦‚æœ`"dog"`åœ¨`i`ä¸­ï¼š

`train_df["label"][idx] = 1`

`train_df.to_csv (r'train_csv.csv', index = False, header=True)`

## **ä½¿ç”¨ PyTorch è¿›è¡Œæ•°æ®å¤„ç†**

æ¥ä¸‹æ¥ï¼Œåˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥å †å æ•°æ®é›†å¹¶å°†å…¶ä½œä¸º`NumPy array`è¿”å›ã€‚

`def custom_collate_fn(batch):`

`transposed_data = list(zip(*batch)) labels = np.array(transposed_data[1]) imgs = np.stack(transposed_data[0]) return imgs, labels`

ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨ PyTorch åˆ›å»ºè®­ç»ƒå’Œæµ‹è¯•æ•°æ®åŠ è½½å™¨ã€‚`size_image = 224 batch_size = 64`

`transform = transforms.Compose([`

`transforms.Resize((size_image,size_image)),`

`np.array])`

`dataset = CatsDogsDataset("train","train_csv.csv",transform=tra nsform)`

`train_set, validation_set = torch.utils.data.random_split(datas et,[20000,5000])`

`train_loader = DataLoader(dataset=train_set, collate_fn=custom_collate_fn,shuffle=True, batch_size=batch_size)`

x = `nn.relu(x)`

## å˜é‡ = `model.init(key, jnp.ones([1, size_image, size_imag e, 3]), training=False)`

åœ¨ç½‘ç»œä¸­ä½¿ç”¨ BatchNorm å’Œ DropOut å±‚å®šä¹‰ Flax ç½‘ç»œã€‚åœ¨ç½‘ç»œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†`[training]`å˜é‡æ¥æ§åˆ¶ä½•æ—¶æ›´æ–°æ‰¹æ¬¡ç»Ÿè®¡ä¿¡æ¯ã€‚æˆ‘ä»¬ç¡®ä¿åœ¨æµ‹è¯•æœŸé—´ä¸æ›´æ–°å®ƒä»¬ã€‚

`model = CNN()`

**å®šä¹‰å…·æœ‰ BatchNorm å’Œ DropOut çš„ Flax æ¨¡å‹**

x = `nn.Dense(features=2)(x)`

æ¦‚ç‡æ”¾å¼ƒç‡ã€‚

æ˜¯å¦æ˜¯ç¡®å®šæ€§çš„ã€‚å¦‚æœæ˜¯ç¡®å®šæ€§çš„è¾“å…¥åˆ™è¢«ç¼©æ”¾å’Œæ©ç ã€‚å¦åˆ™ï¼Œå®ƒä»¬ä¸ä¼šè¢«æ©ç å¹¶åŸæ ·è¿”å›ã€‚class `CNN(nn.Module)`:

`@nn.compact`

def `__call__(self, x, training)`:

ä¸‹ä¸€æ­¥æ˜¯åˆ›å»ºæŸå¤±å‡½æ•°ã€‚åœ¨åº”ç”¨æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬ï¼š

x = `nn.relu(x)`

x = `nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))`

x = `nn.Conv(features=64, kernel_size=(3, 3))(x)`

x = `nn.relu(x)`

x = `nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))`

x = `nn.Conv(features=32, kernel_size=(3, 3))(x)`

åœ¨ `BatchNorm` å±‚ä¸­ï¼Œæˆ‘ä»¬å°† `use_running_average` è®¾ç½®ä¸º `False`ï¼Œæ„å‘³ç€

x = `nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))`

x = `x.reshape((x.shape[0], -1))`

x = `nn.Dense(features=256)(x)`

x = `nn.Dense(features=128)(x)`

x = `nn.BatchNorm(use_running_average=not training)(x)`

}

`validation_loader = DataLoader(dataset=validation_set,collate_f n=custom_collate_fn, shuffle=False, batch_size=batch_size)`

**è®¡ç®—æŒ‡æ ‡**

x = `nn.log_softmax(x)` return `x`

## **åˆ›å»ºæŸå¤±å‡½æ•°**

`[DropOut]` å±‚ä½¿ç”¨ä»¥ä¸‹æ¯”ä¾‹ï¼š

ä¼ é€’æ‰¹é‡ç»Ÿè®¡å‚æ•°ã€‚

`[training]` ä¸º Trueã€‚å°† `[batch_stats]` è®¾ç½®ä¸º mutableã€‚

è®¾ç½®ç”¨äº `DropOut` çš„éšæœºæ•°

def `cross_entropy_loss(*, logits, labels)`:

`labels_onehot = jax.nn.one_hot(labels, num_classes=2)` return `optax.softmax_cross_entropy(logits=logits, labels=labe`

x = `nn.Dropout(0.2, deterministic=not training)(x)`

def `compute_loss(params, batch_stats, images, labels)`:

`logits,batch_stats = CNN().apply({'params': params,'batch_s tats': batch_stats},images, training=True,rngs={'dropout': jax.random.PRNGKey(0)}, mutable=['batch_stats'])`

## loss = `cross_entropy_loss(logits=logits, labels=labels)` return `loss, (logits, batch_stats)`

è®¡ç®—æŒ‡æ ‡å‡½æ•°è®¡ç®—æŸå¤±å’Œå‡†ç¡®æ€§å¹¶è¿”å›å®ƒä»¬ã€‚

def `compute_metrics(*, logits, labels)`:

loss = `cross_entropy_loss(logits=logits, labels=labels)` å‡†ç¡®ç‡ = `jnp.mean(jnp.argmax(logits, -1) == labels)` metrics = {

`'loss': loss`,

`'accuracy': accuracy`,

x = `nn.relu(x)`

return `metrics`

## **åˆ›å»ºè‡ªå®šä¹‰çš„ Flax è®­ç»ƒçŠ¶æ€**

è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰çš„ Flax è®­ç»ƒçŠ¶æ€ï¼Œç”¨äºå­˜å‚¨æ‰¹é‡ç»Ÿè®¡ä¿¡æ¯ã€‚ä¸ºæ­¤ï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„è®­ç»ƒçŠ¶æ€ç±»ï¼Œå®ƒæ˜¯ Flax çš„ `TrainState` çš„å­ç±»ã€‚

# x = `nn.Conv(features=128, kernel_size=(3, 3))(x)`

å­˜å‚¨åœ¨ `[batch_stats]` ä¸­çš„ç»Ÿè®¡ä¿¡æ¯ä¸ä¼šè¢«ä½¿ç”¨ï¼Œä½†ä¼šè®¡ç®—è¾“å…¥çš„æ‰¹æ¬¡ç»Ÿè®¡ã€‚

`key = jax.random.PRNGKey(0)`

åˆå§‹åŒ–æƒé‡

`class TrainState(train_state.TrainState): batch_stats: flax.core.FrozenDict è¦å®šä¹‰ä¸€ä¸ª Flax è®­ç»ƒçŠ¶æ€ï¼Œä½¿ç”¨ [TrainState.create] å¹¶ä¼ é€’ï¼šåº”ç”¨å‡½æ•°ã€‚`

æ¨¡å‹å‚æ•°ã€‚

ä¼˜åŒ–å™¨å‡½æ•°ã€‚æ‰¹é‡ç»Ÿè®¡ã€‚

`state = TrainState.create(`

apply_fn = model.apply,

`params = variables['params'],`

tx = optax.sgd(0.01),

batch_stats = variables['batch_stats'],

)

## **è®­ç»ƒæ­¥éª¤**

åœ¨è®­ç»ƒæ­¥éª¤ä¸­ï¼Œæˆ‘ä»¬è®¡ç®—ç›¸å¯¹äºæŸå¤±å’Œæ¨¡å‹å‚æ•°çš„æ¢¯åº¦ - **æ¨¡å‹å‚æ•°**å’Œ**æ‰¹æ¬¡ç»Ÿè®¡**ã€‚æˆ‘ä»¬ä½¿ç”¨è¿™äº›æ¢¯åº¦æ¥æ›´æ–°æ¨¡å‹å‚æ•°å¹¶è¿”å›æ–°çš„çŠ¶æ€å’Œæ¨¡å‹æŒ‡æ ‡ã€‚è¿™ä¸ªå‡½æ•°è¢«è£…é¥°

ä»¥ @jax.jit ä½¿è®¡ç®—æ›´å¿«ã€‚

@jax.jit

`def train_step(state,images, labels):`

"""è¿›è¡Œå•æ­¥è®­ç»ƒã€‚"""

`(batch_loss, (logits, batch_stats)), grads= jax.value_and_gra d(compute_loss, has_aux=True)(state.params,state.batch_stats, i mages,labels)`

`state = state.apply_gradients(grads=grads)`

`metrics = compute_metrics(logits=logits, labels=labels) return state, metricsNext, define a function that applies the training step for one epoch. The functions:`

é€šè¿‡è®­ç»ƒæ•°æ®å¾ªç¯ã€‚

å°†æ¯ä¸ªè®­ç»ƒæ‰¹æ¬¡ä¼ é€’ç»™è®­ç»ƒæ­¥éª¤ã€‚

è·å–æ‰¹æ¬¡æŒ‡æ ‡ã€‚ è®¡ç®—å‡å€¼ä»¥è·å¾— epoch æŒ‡æ ‡ã€‚

è¿”å›æ–°çŠ¶æ€å’Œåº¦é‡ã€‚

`def train_one_epoch(state, dataloader):`

"""åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œ 1 ä¸ª epoch çš„è®­ç»ƒã€‚""" batch_metrics = []

for cnt, (images, labels) in enumerate(dataloader):

`images = images / 255.0`

`state, metrics = train_step(state, images, labels) batch_metrics.append(metrics)`

`batch_metrics_np = jax.device_get(batch_metrics) epoch_metrics_np = {k: np.mean([metrics[k] for metrics in batch_metrics_n`

p])

å¯¹äº `batch_metrics_np[0]` ä¸­çš„`k` }

`return state, epoch_metrics_np`

## **è¯„ä¼°æ­¥éª¤**

æˆ‘ä»¬åœ¨è¯„ä¼°æ­¥éª¤ä¸­å°†æµ‹è¯•å›¾åƒå’Œæ ‡ç­¾ä¼ é€’ç»™æ¨¡å‹å¹¶è·å–è¯„ä¼°æŒ‡æ ‡ã€‚è¯¥å‡½æ•°è¿˜é€šè¿‡ JAX çš„å¿«é€Ÿè®¡ç®—åˆ©ç”¨ JIT ç¼–è¯‘ã€‚åœ¨è¯„ä¼°ä¸­ï¼Œå°† [training] è®¾ç½®ä¸º [False]ï¼Œä»¥ä¾¿ä¸æ›´æ–°æ¨¡å‹å‚æ•°ã€‚åœ¨è¿™ä¸€æ­¥ä¸­ï¼Œæˆ‘ä»¬è¿˜ä¼ é€’æ‰¹æ¬¡ç»Ÿè®¡å’Œ [DropOut] å±‚çš„éšæœºæ•°ç”Ÿæˆå™¨ã€‚

@jax.jitdef eval_step(batch_stats, params, images, labels): logits = CNN().apply({'params': params,'batch_stats': batch _stats}, images, training=False,rngs={'dropout': jax.random.PRN GKey(0)})return compute_metrics(logits=logits, labels=labels)`[evaluate_model]` å‡½æ•°å°†`[eval_step]` åº”ç”¨äºæµ‹è¯•æ•°æ®ï¼Œå¹¶è¿”å›è¯„ä¼°æŒ‡æ ‡ã€‚

`def evaluate_model(state, test_imgs, test_lbls):`

"""åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°ã€‚"""

`metrics = eval_step(state.batch_stats,state.params, test_im`

gs, test_lbls)

`metrics = jax.device_get(metrics)`

`metrics = jax.tree_map(lambda x: x.item(), metrics) return metrics`

## **è®­ç»ƒ Flax æ¨¡å‹**

ä¸ºäº†è®­ç»ƒæ¨¡å‹ï¼Œæˆ‘ä»¬å®šä¹‰å¦ä¸€ä¸ªå‡½æ•°æ¥å®æ–½`[train_one_epoch]`ã€‚é¦–å…ˆå®šä¹‰æ¨¡å‹è¯„ä¼°æ•°æ®ã€‚(`test_images`, `test_labels`) = next(iter(validation_loader)) `test_images = test_images / 255.0`

## **åœ¨ Flax ä¸­è®¾ç½® TensorBoard**

æ‚¨å¯ä»¥å°†æ¨¡å‹æŒ‡æ ‡è®°å½•åˆ° TensorBoard ä¸­ï¼Œæ–¹æ³•æ˜¯å°†æ ‡é‡å†™å…¥ TensorBoardã€‚

`from torch.utils.tensorboard import SummaryWriter import torchvision.transforms.functional as F logdir = "flax_logs"`

`writer = SummaryWriter(logdir)`

## **è®­ç»ƒæ¨¡å‹**

æˆ‘ä»¬è¿˜å¯ä»¥å°†æŒ‡æ ‡é™„åŠ åˆ°åˆ—è¡¨ä¸­ï¼Œå¹¶ä½¿ç”¨ Matplotlib è¿›è¡Œå¯è§†åŒ–ã€‚

`training_loss = [] training_accuracy = [] testing_loss = []`

`testing_accuracy = []`

æ¥ä¸‹æ¥ï¼Œå®šä¹‰å°†è®­ç»ƒ Flax æ¨¡å‹æŒ‡å®šè½®æ•°çš„è®­ç»ƒå‡½æ•°ã€‚

åœ¨æµ‹è¯•æ•°æ®ä¸Šè¯„ä¼°æ¨¡å‹ã€‚

å°†æŒ‡æ ‡é™„åŠ åˆ°åˆ—è¡¨ä¸­ã€‚å°†æ¨¡å‹æŒ‡æ ‡å†™å…¥ TensorBoardã€‚

åœ¨æ¯ä¸ª epoch æ‰“å°æŒ‡æ ‡ã€‚è¿”å›è®­ç»ƒåçš„æ¨¡å‹çŠ¶æ€`def train_model(epochs):for epoch in range(1, epochs + 1):train_state, train_metrics = train_one_epoch(state, tra`

`in_loader)`

`training_loss.append(train_metrics['loss'])` `training_accuracy.append(train_metrics['accuracy'])` `test_metrics = evaluate_model(train_state, test_images,`

`test_labels`

`testing_loss.append(test_metrics['loss'])`

`testing_accuracy.append(test_metrics['accuracy'])`

`writer.add_scalar('Loss/train', train_metrics['loss'], epoch)`

`writer.add_scalar('Loss/test', test_metrics['loss'], epoch)`

`writer.add_scalar('Accuracy/train', train_metrics['accuracy'], epoch)`

`writer.add_scalar('Accuracy/test', test_metrics['accuracy'], epoch)`

`print(f"Epoch: {epoch}, training loss: {train_metrics ['loss']}, training accuracy: {train_metrics['accuracy'] * 10 0}, validation loss: {test_metrics['loss']}, validation accuracy: {test_metrics['accuracy'] * 100}")`

`return train_stateRun the training function to train the model. trained_model_state = train_model(30)`

## **ä¿å­˜ Flax æ¨¡å‹**

`[save_checkpoint]` ä¿å­˜ Flax æ¨¡å‹ã€‚å®ƒæœŸæœ›ï¼š

ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹çš„ç›®å½•ã€‚

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒFlax è®­ç»ƒçš„æ¨¡å‹æ˜¯ `[trained_model_state]`ã€‚è¯¥æ¨¡å‹çš„å‰ç¼€ã€‚

æ˜¯å¦è¦†ç›–ç°æœ‰æ¨¡å‹ã€‚

`from flax.training import checkpoints`

ckpt_dir = 'model_checkpoint/'

`checkpoints.save_checkpoint(ckpt_dir=ckpt_dir)`

`target=trained_model_state`, `step=100,`

`prefix='flax_model', overwrite=True`

)![](img/00039.jpeg)

## **åŠ è½½ Flax æ¨¡å‹**

`[restore_checkpoint]` æ–¹æ³•ä»ä¿å­˜çš„ä½ç½®åŠ è½½å·²ä¿å­˜çš„ Flax æ¨¡å‹ã€‚

`loaded_model = checkpoints.restore_checkpoint(`

`ckpt_dir=ckpt_dir`, `target=state`, `prefix='flax_mode`

`l'`

## **è¯„ä¼° Flax æ¨¡å‹**

è¿è¡Œ `[evalaute_model]` å‡½æ•°æ¥æ£€æŸ¥æ¨¡å‹åœ¨æµ‹è¯•æ•°æ®ä¸Šçš„æ€§èƒ½ã€‚`evaluate_model(trained_model_state,test_images, test_labels)`

## **å¯è§†åŒ– Flax æ¨¡å‹çš„æ€§èƒ½**

è¦å¯è§†åŒ– Flax æ¨¡å‹çš„æ€§èƒ½ï¼Œå¯ä»¥ä½¿ç”¨ Matplotlib ç»˜åˆ¶æŒ‡æ ‡å›¾è¡¨æˆ–åŠ è½½ TensorBoard å¹¶æ£€æŸ¥æ ‡é‡é€‰é¡¹å¡ã€‚

`%load_ext tensorboard` `%tensorboard --logdir={logdir}`

## **æœ€ç»ˆæ€è€ƒ**

`åœ¨æœ¬æ–‡ä¸­ï¼Œæ‚¨å·²ç»çœ‹åˆ°å¦‚ä½•åœ¨ Flax ä¸­æ„å»ºåŒ…å« BatchNorm å’Œ DropOut å±‚çš„ç½‘ç»œã€‚æ‚¨è¿˜å­¦ä¹ äº†å¦‚ä½•è°ƒæ•´è®­ç»ƒè¿‡ç¨‹ä»¥é€‚åº”è¿™äº›æ–°å±‚ã€‚å…·ä½“æ¥è¯´ï¼Œæ‚¨å­¦åˆ°äº†ï¼š`

å¦‚ä½•å®šä¹‰åŒ…å« BatchNorm å’Œ DropOut å±‚çš„ Flax æ¨¡å‹ã€‚

`å¦‚ä½•åˆ›å»ºè‡ªå®šä¹‰çš„ Flax è®­ç»ƒçŠ¶æ€ã€‚`

ä½¿ç”¨ BatchNorm å’Œ DropOut å±‚è®­ç»ƒå’Œè¯„ä¼° Flax æ¨¡å‹ã€‚

`å¦‚ä½•ä¿å­˜å’ŒåŠ è½½ Flax æ¨¡å‹ã€‚`

`å¦‚ä½•è¯„ä¼° Flax æ¨¡å‹çš„æ€§èƒ½`

## `**JAX å’Œ Flax ä¸­çš„ LSTM**`

`LSTM æ˜¯ä¸€ç±»ç”¨äºè§£å†³åºåˆ—é—®é¢˜ï¼ˆå¦‚æ—¶é—´åºåˆ—å’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼‰çš„ç¥ç»ç½‘ç»œã€‚LSTM ä¿æŒä¸€äº›å†…éƒ¨çŠ¶æ€ï¼Œåœ¨è§£å†³è¿™äº›é—®é¢˜æ—¶éå¸¸æœ‰ç”¨ã€‚LSTM åº”ç”¨äºå¾ªç¯éå†æ¯ä¸ªæ—¶é—´æ­¥ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ JAX å’Œ Flax ä¸­çš„å‡½æ•°ï¼Œè€Œä¸æ˜¯ä»å¤´ç¼–å†™è¿™äº›å¾ªç¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Flax ä¸­çš„ LSTM æ„å»ºè‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹ã€‚`

`è®©æˆ‘ä»¬å¼€å§‹å§ã€‚`

## `**æ•°æ®é›†ä¸‹è½½**`

`æˆ‘ä»¬å°†ä½¿ç”¨ Kaggle æä¾›çš„ç”µå½±è¯„è®ºæ•°æ®é›†ã€‚æˆ‘ä»¬ä½¿ç”¨ Kaggle çš„ Python åŒ…ä¸‹è½½æ•°æ®é›†ã€‚`

`import os`

è¿›å…¥`#Obtain from https://www.kaggle.com/username/account` `os.environ["KAGGLE_USERNAME"]="KAGGLE_USERNAME"`

`os.environ["KAGGLE_KEY"]="KAGGLE_KEY"`

`import kaggle`

`kaggle datasets download lakshmi25npathi/imdb-dataset-of-50k-mo vie-reviews`

`æ¥ä¸‹æ¥ï¼Œæå–æ•°æ®é›†ã€‚ä½¿ç”¨ zipfile è§£å‹ç¼© zip æ–‡ä»¶'imdb-dataset-of-50k-movie-reviews.zip'ï¼Œç„¶åä½¿ç”¨ Pandas åŠ è½½æ•°æ®å¹¶æ˜¾ç¤ºéƒ¨åˆ†è¯„è®ºã€‚`

`df = pd.read_csv("imdb-dataset-of-50k-movie-reviews/IMDB Datase t.csv")`

`df.head()`

`![](img/00040.gif)`

## `**ä½¿ç”¨ NLTK è¿›è¡Œæ•°æ®å¤„ç†**`

`è¯¥æ•°æ®é›†åŒ…å«ä¸€äº›ä¸å¿…è¦çš„å­—ç¬¦ï¼Œç”¨äºé¢„æµ‹ç”µå½±è¯„è®ºæ˜¯è´Ÿé¢è¿˜æ˜¯æ­£é¢ã€‚ä¾‹å¦‚ï¼Œæ ‡ç‚¹ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä»è¯„è®ºä¸­å»é™¤è¿™äº›å­—ç¬¦ã€‚æˆ‘ä»¬è¿˜éœ€è¦å°†[sentiment]åˆ—è½¬æ¢ä¸ºæ•°å€¼è¡¨ç¤ºã€‚è¿™å¯ä»¥ä½¿ç”¨ Scikitlearn ä¸­çš„[LabelEncoder]å®Œæˆã€‚è®©æˆ‘ä»¬å¯¼å…¥è¿™äº›åŠå…¶ä»–æˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­å°†ä½¿ç”¨çš„åŒ…ã€‚`

`import numpy as np`

`import pandas as pd`

`from numpy import array`

`import tensorflow as tf`

`from sklearn.model_selection import train_test_split from sklearn.preprocessing import LabelEncoder import matplotlib.pyplot as plt`

`è¯„è®ºä¸­è¿˜åŒ…å«å¯¹æƒ…æ„Ÿé¢„æµ‹æ— ç”¨çš„è¯è¯­ã€‚è¿™äº›æ˜¯è‹±è¯­ä¸­å¸¸è§çš„è¯ï¼Œå¦‚ theï¼Œatï¼Œand ç­‰ã€‚è¿™äº›è¯ç§°ä¸º**åœç”¨è¯**ã€‚æˆ‘ä»¬ä½¿ç”¨ nltk åº“å¸®åŠ©å»é™¤å®ƒä»¬ã€‚è®©æˆ‘ä»¬å¼€å§‹å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œä»¥åˆ é™¤æ‰€æœ‰è‹±æ–‡åœç”¨è¯ã€‚`

# `pip install nltk`

`import nltk`

`from nltk.corpus import stopwords`

`nltk.download('stopwords')`

`def remove_stop_words(review):`

`review_minus_sw = []`

`stop_words = stopwords.words('english')`

`review = review.split()`

`[review_minus_sw.append(word) for word in cleaned_review]`

`review if word not in stop_words]`

`cleaned_review = ' '.join(review_minus_sw)`

`return cleaned_review`

`å°†è¯¥å‡½æ•°åº”ç”¨äºæƒ…æ„Ÿåˆ—ã€‚`df['review'] = df['review'].apply(remove_stop_words)`è®©æˆ‘ä»¬è¿˜å°†æƒ…æ„Ÿåˆ—è½¬æ¢ä¸ºæ•°å€¼è¡¨ç¤ºã€‚`

`labelencoder = LabelEncoder()`

`df = df.assign(sentiment = labelencoder.fit_transform(df["senti ment"]))`

å°†å…·æœ‰å’Œä¸å…·æœ‰åœç”¨è¯çš„è¯„è®ºè¿›è¡Œæ¯”è¾ƒï¼[](../images/00041.jpeg)

æŸ¥çœ‹ç¬¬ä¸‰æ¡è¯„è®ºæ—¶ï¼Œæˆ‘ä»¬æ³¨æ„åˆ°

`wordsÂ [this],Â [was]Â andÂ [a]Â have been dropped from the sentence. However, we can still see some special characters, such asÂ [<br>]Â in the review. Let's resolve that next.`

## **ä½¿ç”¨ Keras è¿›è¡Œæ–‡æœ¬å‘é‡åŒ–**

è¯„è®ºæ•°æ®ä»ç„¶æ˜¯æ–‡æœ¬å½¢å¼ã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬éœ€è¦å°†å…¶è½¬æ¢ä¸ºç±»ä¼¼æƒ…æ„Ÿåˆ—çš„æ•°å€¼è¡¨ç¤ºã€‚åœ¨è¿™ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å°†æ•°æ®é›†æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚

`from sklearn.model_selection import train_test_split df = df.drop_duplicates()`

`docs = df['review']`

`labels = array(df['sentiment'])`

`X_train, X_test , y_train, y_test = train_test_split(docs, labe ls , test_size = 0.20, random_state=0)`

æˆ‘ä»¬ä½¿ç”¨ Keras æ–‡æœ¬å‘é‡åŒ–å±‚å°†è¯„è®ºè½¬æ¢ä¸ºæ•´æ•°å½¢å¼ã€‚æ­¤å‡½æ•°å¯ä»¥è¿‡æ»¤æ‰æ‰€æœ‰æ ‡ç‚¹ç¬¦å·å¹¶å°†è¯„è®ºè½¬æ¢ä¸ºå°å†™ã€‚æˆ‘ä»¬ä¼ é€’ä»¥ä¸‹å‚æ•°ï¼š

`standardize`Â è®¾ç½®ä¸ºÂ `lower_and_strip_punctuation`Â ä»¥è½¬æ¢ä¸º

è½¬æ¢ä¸ºå°å†™å¹¶åˆ é™¤æ ‡ç‚¹ç¬¦å·ã€‚

`[output_mode]`Â è½¬æ¢ä¸ºÂ `[int]`Â ä»¥è·å¾—æ•´æ•°ç»“æœã€‚Â `[tf_idf]`Â å°†åº”ç”¨Â TF-IDF ç®—æ³•ã€‚

`[output_sequence_length]`Â è®¾ç½®ä¸º 50ï¼Œä»¥è·å¾—è¯¥é•¿åº¦çš„å¥å­ã€‚æ›´æ”¹æ­¤æ•°å­—ä»¥æŸ¥çœ‹å®ƒå¦‚ä½•å½±å“æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘å‘ç° 50 ç»™å‡ºäº†ä¸€äº›ä¸é”™çš„ç»“æœã€‚è¶…è¿‡æŒ‡å®šé•¿åº¦çš„å¥å­å°†è¢«æˆªæ–­ï¼Œè€Œè¾ƒçŸ­çš„å¥å­å°†ç”¨é›¶å¡«å……ã€‚

`[max_tokens]`Â è®¾ç½®ä¸º 10,000ï¼Œä»¥è·å¾—è¯¥æ•°é‡çš„è¯æ±‡é‡ã€‚è°ƒæ•´æ­¤æ•°å­—å¹¶æ£€æŸ¥æ¨¡å‹æ€§èƒ½çš„å˜åŒ–ã€‚

å®šä¹‰å‘é‡åŒ–å±‚åï¼Œæˆ‘ä»¬å°†å…¶åº”ç”¨äºè®­ç»ƒæ•°æ®ã€‚é€šè¿‡è°ƒç”¨Â `adapt`Â å‡½æ•°æ¥å®ç°ã€‚è¯¥å‡½æ•°ä»æä¾›çš„æ•°æ®é›†ä¸­è®¡ç®—è¯æ±‡è¡¨ã€‚å¦‚æœæä¾›äº†Â `[max_tokens]`ï¼Œåˆ™è¯æ±‡è¡¨å°†è¢«æˆªæ–­ã€‚

`import tensorflow as tf`

`max_features = 10000` # æœ€å¤§è¯æ±‡é‡å¤§å°ã€‚

`batch_size = 128`

`max_len = 50` # åºåˆ—é•¿åº¦ï¼Œç”¨äºå¡«å……è¾“å‡ºã€‚`vectorize_layer = tf.keras.layers.TextVectorization(standardize ='lower_and_strip_punctuation',max_tokens=max_features,output_m ode='int',output_sequence_length=max_len)`

`vectorize_layer.adapt(X_train)`

è¦æŸ¥çœ‹ç”Ÿæˆçš„è¯æ±‡è¡¨ï¼Œè¯·è°ƒç”¨Â `get_vocabulary`Â å‡½æ•°ã€‚`vectorize_layer.get_vocabulary()`ï¼[](../images/00042.jpeg)ä½¿ç”¨è®­ç»ƒå¥½çš„å‘é‡åŒ–å±‚å°†è®­ç»ƒå’Œæµ‹è¯•æ•°æ®è½¬æ¢ä¸ºæ•°å€¼å½¢å¼ã€‚`X_train_padded = vectorize_layer(X_train)` `X_test_padded = vectorize_layer(X_test)`ï¼[](../images/00043.gif)

## **åˆ›å»º tf.data æ•°æ®é›†**

è®©æˆ‘ä»¬ä»è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­ç”Ÿæˆå’Œé¢„å–æ‰¹æ¬¡ï¼Œä»¥ä½¿åŠ è½½åˆ° LSTM æ¨¡å‹æ›´é«˜æ•ˆã€‚æˆ‘ä»¬é¦–å…ˆåˆ›å»ºä¸€ä¸ªÂ `tf.data.Dataset`ã€‚

`training_data = tf.data.Dataset.from_tensor_slices((X_train_padded, y_train))`

`validation_data = tf.data.Dataset.from_tensor_slices((X_test_padded, y_test))`

`training_data = training_data.batch(batch_size)`

`validation_data = validation_data.batch(batch_size)`

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬é¢„å–ä¸€ä¸ªæ‰¹æ¬¡ï¼Œæ´—ç‰Œæ•°æ®ï¼Œå¹¶å°†å…¶ä½œä¸º `NumPy array` è¿”å›ã€‚

# `pip install tensorflow_datasets`

`import tensorflow_datasets as tfds`

`def get_train_batches():`

`ds = training_data.prefetch(1)`

`ds = ds.shuffle(3, reshuffle_each_iteration=True)` # `tfds.dataset_as_numpy` å°† `tf.data.Dataset` è½¬æ¢ä¸ºä¸€ä¸ª

ç”± NumPy æ•°ç»„ç»„æˆçš„è¿­ä»£å™¨ `return tfds.as_numpy(ds)`

## `åœ¨ Flax ä¸­å®šä¹‰ LSTM æ¨¡å‹`

æˆ‘ä»¬ç°åœ¨å¯ä»¥åœ¨ Flax ä¸­å®šä¹‰ LSTM æ¨¡å‹äº†ã€‚è¦åœ¨ Flax ä¸­è®¾è®¡ LSTMï¼Œæˆ‘ä»¬ä½¿ç”¨ `LSTMCell` æˆ– `OptimizedLSTMCell`ã€‚

`OptimizedLSTMCell` æ˜¯é«˜æ•ˆçš„ `LSTMCell`ã€‚

`[LSTMCell.initialize_carry]` å‡½æ•°ç”¨äºåˆå§‹åŒ– LSTM å•å…ƒçš„éšè—çŠ¶æ€ã€‚å®ƒæœŸæœ›ï¼š

ä¸€ä¸ªéšæœºæ•°ã€‚

æ‰¹æ¬¡ç»´åº¦ã€‚

å•å…ƒçš„æ•°é‡ã€‚

è®©æˆ‘ä»¬ä½¿ç”¨ `setup æ–¹æ³•` æ¥å®šä¹‰ LSTM æ¨¡å‹ã€‚LSTM åŒ…å«ä»¥ä¸‹å±‚ï¼š

ä¸€ä¸ªä¸å‘é‡åŒ–å±‚ä¸­å®šä¹‰çš„ç‰¹å¾æ•°å’Œé•¿åº¦ç›¸åŒçš„ `Embedding layer`ã€‚

LSTM å±‚æ ¹æ® `[reverse]` å‚æ•°åœ¨ä¸€ä¸ªæ–¹å‘ä¸Šä¼ é€’æ•°æ®ã€‚

ä¸€å¯¹ `Dense layers`ã€‚

æœ€ç»ˆçš„å¯†é›†è¾“å‡ºå±‚ã€‚from flax import linen as nn

`class LSTMModel(nn.Module):`

`def setup(self):`

`self.embedding = nn.Embed(max_features, max_len) lstm_layer = nn.scan(nn.OptimizedLSTMCell,`

`variable_broadcast="params", split_rngs={"params": False}, in_axes=1,`

`out_axes=1,`

`length=max_len,`

`reverse=False)`

`self.lstm1 = lstm_layer()`

`self.dense1 = nn.Dense(256)`

`self.lstm2 = lstm_layer()`

`self.dense2 = nn.Dense(128)`

`self.lstm3 = lstm_layer()`

`self.dense3 = nn.Dense(64)`

`self.dense4 = nn.Dense(2)`

`@nn.rematdef __call__(self, x_batch): x = self.embedding(x_batch)`

`carry, hidden = nn.OptimizedLSTMCell.initialize_carry(jax.random.PRNGKey(0), batch_dims=(len(x_batch),), size=128)`

`(carry, hidden), x = self.lstm1((carry, hidden), x)`

`x = self.dense1(x) x = nn.relu(x)`

`carry, hidden = nn.OptimizedLSTMCell.initialize_carry(jax.random.PRNGKey(0), batch_dims=(len(x_batch),), size=64)`

`(carry, hidden), x = self.lstm2((carry, hidden), x) x = self.dense2(x) x = nn.relu(x)`

`carry, hidden = nn.OptimizedLSTMCell.initialize_carry(jax.random.PRNGKey(0), batch_dims=(len(x_batch),), size=32)`

`(carry, hidden), x = self.lstm3((carry, hidden), x)`

`x = self.dense3(x)`

`x = nn.relu(x)`

`x = self.dense4(x[:, -1]) return nn.log_softmax(x)`

æˆ‘ä»¬å°† `scan å‡½æ•°` åº”ç”¨äºæ•°æ®çš„è¿­ä»£ã€‚å®ƒæœŸæœ›ï¼š`scan` å¾…å¾ªç¯çš„é¡¹ç›®ã€‚å®ƒä»¬å¿…é¡»å…·æœ‰ç›¸åŒçš„å¤§å°ï¼Œå¹¶ä¸”å°†æ²¿ç€æ‰«æè½´å †å ã€‚`carry` ä¸€ä¸ªåœ¨æ¯æ¬¡è¿­ä»£ä¸­æ›´æ–°çš„ä¼ é€’å€¼ã€‚è¯¥å€¼åœ¨æ•´ä¸ªè¿­ä»£è¿‡ç¨‹ä¸­å¿…é¡»å…·æœ‰ç›¸åŒçš„å½¢çŠ¶å’Œ `[dtype]`ã€‚

`[å¹¿æ’­]` ä¸€ä¸ªåœ¨å¾ªç¯ä¸­å°é—­çš„å€¼ ` [<axis:int>]` æ‰«æçš„è½´ã€‚

`[split_rngs]` ç”¨äºå®šä¹‰æ˜¯å¦åœ¨æ¯ä¸€æ­¥åˆ†å‰²éšæœºæ•°ç”Ÿæˆå™¨ã€‚

åœ¨ä½¿ç”¨ LSTMs è®¡ç®—é•¿åºåˆ—æ—¶ï¼Œ`[nn.remat]` è°ƒç”¨èŠ‚çœå†…å­˜ã€‚

## åœ¨ Flax ä¸­è®¡ç®—æŒ‡æ ‡

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—ç½‘ç»œçš„æŸå¤±å’Œå‡†ç¡®ç‡ã€‚

import optax

import jax.numpy as jnp

def compute_metrics(logits, labels):

loss = jnp.mean(optax.softmax_cross_entropy(logits, jax.nn.on e_hot(labels, num_classes=2)))

accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)

metrics = {

'loss': loss,

'accuracy': accuracy

}

return metrics

## åˆ›å»ºè®­ç»ƒçŠ¶æ€

è®­ç»ƒçŠ¶æ€åº”ç”¨æ¢¯åº¦å¹¶æ›´æ–°å‚æ•°å’Œä¼˜åŒ–å™¨çŠ¶æ€ã€‚Flax æä¾›äº†[train_state]ç”¨äºæ­¤ç›®çš„ã€‚æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼š

åˆ›å»º[LSTMModel]çš„ä¸€ä¸ªå®ä¾‹ã€‚

åˆå§‹åŒ–æ¨¡å‹ä»¥é€šè¿‡è®­ç»ƒæ•°æ®æ ·æœ¬è·å–[`params`]ã€‚

åœ¨åº”ç”¨ Adam ä¼˜åŒ–å™¨åè¿”å›åˆ›å»ºçš„çŠ¶æ€ã€‚from flax.training import train_state

def create_train_state(rng):

"""åˆ›å»ºåˆå§‹`TrainState`ã€‚"""

model = LSTMModel()

params = model.init(rng, jnp.array(X_train_padded[0]))['param

s']

tx = optax.adam(0.001,0.9,0.999,1e-07) return train_state.TrainState.create(

apply_fn=model.apply, params=params, tx=tx)

## å®šä¹‰è®­ç»ƒæ­¥éª¤

è®­ç»ƒå‡½æ•°æ‰§è¡Œä»¥ä¸‹æ“ä½œ:

ä½¿ç”¨`apply`æ–¹æ³•ä»æ¨¡å‹è®¡ç®—æŸå¤±å’Œ logitsã€‚

ä½¿ç”¨[value_and_grad]è®¡ç®—æ¢¯åº¦ã€‚ä½¿ç”¨æ¢¯åº¦æ›´æ–°æ¨¡å‹å‚æ•°ã€‚

ä½¿ç”¨å…ˆå‰å®šä¹‰çš„å‡½æ•°è®¡ç®— metricsã€‚è¿”å›çŠ¶æ€å’Œ metricsã€‚

åº”ç”¨[@jax.jit]ä½¿å‡½æ•°è¿è¡Œæ›´å¿«ã€‚`@jax.jit`def train_step(state, text, labels):def loss_fn(params):

logits = LSTMModel().apply({'params': params}, text) loss = jnp.mean(optax.softmax_cross_entropy( logits=logits,

labels=jax.nn.one_hot(labels, num_classes=2))) return loss, logits

grad_fn = jax.value_and_grad(loss_fn, has_aux=True) (_, logits), grads = grad_fn(state.params)

state = state.apply_gradients(grads=grads)

metrics = compute_metrics(logits, labels)

è¿”å›çŠ¶æ€å’Œ metrics

## è¯„ä¼° Flax æ¨¡å‹

[`eval_step`]ä½¿ç”¨`Module.apply`è¯„ä¼°æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°ã€‚å®ƒè¿”å›æµ‹è¯•é›†ä¸Šçš„æŸå¤±å’Œå‡†ç¡®ç‡ã€‚

[`evaluate_model`]å‡½æ•°åº”ç”¨[`eval_step`]ï¼Œä»è®¾å¤‡è·å– metricsï¼Œå¹¶ä½œä¸º[`jax.tree_map`]è¿”å›å®ƒä»¬ã€‚

@jax.jit

def eval_step(state, text, labels):

logits = LSTMModel().apply({'params': state.params}, text)

è¿”å›ä½¿ç”¨ logits å’Œ labels è®¡ç®— metrics çš„ç»“æœ

def evaluate_model(state, text, test_lbls): """åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹ã€‚"""

metrics = eval_step(state, text, test_lbls) metrics = jax.device_get(metrics)

metrics = jax.tree_map(lambda x: x.item(), metrics) return metrics

## åˆ›å»ºè®­ç»ƒå‡½æ•°

æ¥ä¸‹æ¥ï¼Œå®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œåœ¨ä¸€ä¸ª epoch ä¸Šè®­ç»ƒ Flax LSTM æ¨¡å‹ã€‚è¯¥å‡½æ•°å¯¹è®­ç»ƒæ•°æ®ä¸­çš„æ¯ä¸ªæ‰¹æ¬¡åº”ç”¨[train_step]ã€‚åœ¨æ¯ä¸ªæ‰¹æ¬¡ä¹‹åï¼Œå®ƒå°† metrics é™„åŠ åˆ°ä¸€ä¸ªåˆ—è¡¨ä¸­ã€‚

def train_one_epoch(state):

åœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒ 1 ä¸ª epochã€‚batch_metrics = []

å¯¹äº text, labels in get_train_batches():

state, metrics = `train_step(state, text, labels)` batch_metrics.append(metrics)batch_metrics_np = jax.device_get(batch_metrics) epoch_metrics_np = {k: np.mean([metrics[k] for metrics in batch_metrics_np[0] ])for k in batch_metrics_np[0] }return state, epoch_metrics_np

å‡½æ•°ä»è®¾å¤‡è·å–æŒ‡æ ‡å¹¶è®¡ç®—æ‰€æœ‰è®­ç»ƒæ‰¹æ¬¡çš„å¹³å‡å€¼ã€‚è¿™æä¾›äº†ä¸€ä¸ªæ—¶æœŸçš„æŸå¤±å’Œå‡†ç¡®æ€§ã€‚

## åœ¨ Flax ä¸­è®­ç»ƒ LSTM æ¨¡å‹

è¦è®­ç»ƒ LSTM æ¨¡å‹ï¼Œæˆ‘ä»¬è¿è¡ŒÂ [train_one_epoch]Â å‡½æ•°è¿›è¡Œå¤šæ¬¡è¿­ä»£ã€‚æ¥ä¸‹æ¥ï¼Œåº”ç”¨Â [evaluate_model]Â è·å–æ¯ä¸ªæ—¶æœŸçš„æµ‹è¯•æŒ‡æ ‡ã€‚åœ¨å¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬åˆ›å»º

ä½¿ç”¨Â [create_train_state]Â æ¥ä¿å­˜è®­ç»ƒä¿¡æ¯ã€‚è¯¥å‡½æ•°åˆå§‹åŒ–æ¨¡å‹å‚æ•°å’Œä¼˜åŒ–å™¨ã€‚æ­¤ä¿¡æ¯å­˜å‚¨åœ¨è®­ç»ƒçŠ¶æ€Â dataclassÂ ä¸­ã€‚

rng = jax.random.PRNGKey(0)rng, input_rng, init_rng = jax.random.split(rng,num=3)seed = 0state = `create_train_state(init_rng)` del init_rng # ä¸å†ä½¿ç”¨ã€‚

num_epochs = 30

(text, test_labels) = next(iter(validation_data)) text = jnp.array(text)

test_labels = jnp.array(test_labels) training_loss = []

training_accuracy = []

testing_loss = []

testing_accuracy = []

def `train_model()`:

for epoch in range(1, num_epochs + 1):

train_state, train_metrics = `train_one_epoch(state)` training_loss.append(train_metrics['loss']) training_accuracy.append(train_metrics['accuracy']) test_metrics = `evaluate_model(train_state, text, test_l`

abels)

testing_loss.append(test_metrics['loss'])

testing_accuracy.append(test_metrics['accuracy']) print(f"Epoch: {epoch}, train loss: {train_metrics['los

s']}, train accuracy: {train_metrics['accuracy'] * 100}, test l oss: {test_metrics['loss']}, test accuracy: {test_metrics['accu racy'] * 100}")

è¿”å› `train_statetrained_model_state = train_model()`æ¯ä¸ªæ—¶æœŸç»“æŸåï¼Œæˆ‘ä»¬æ‰“å°æŒ‡æ ‡å¹¶å°†å…¶é™„åŠ åˆ°åˆ—è¡¨ä¸­ã€‚

## åœ¨ Flax ä¸­å¯è§†åŒ– LSTM æ¨¡å‹æ€§èƒ½

ç„¶åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨Â `Matplotlib`Â æ¥å¯è§†åŒ–é™„åŠ åˆ°åˆ—è¡¨ä¸­çš„æŒ‡æ ‡ã€‚è®­ç»ƒè¿‡ç¨‹ä¸æ˜¯å¾ˆå¹³ç¨³ï¼Œä½†æ‚¨å¯ä»¥è°ƒæ•´ç½‘ç»œçš„æ¶æ„ã€æ¯ä¸ªè¯„å®¡çš„é•¿åº¦å’Œè¯æ±‡é‡å¤§å°ä»¥æé«˜æ€§èƒ½ã€‚

## ä¿å­˜ LSTM æ¨¡å‹

è¦ä¿å­˜ Flax æ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œè¯·ä½¿ç”¨Â [save_checkpoint]Â æ–¹æ³•ã€‚å®ƒéœ€è¦ï¼š

ä¿å­˜æ£€æŸ¥ç‚¹æ–‡ä»¶çš„ç›®å½•ã€‚

è¦ä¿å­˜çš„ Flax å¯¹è±¡ï¼Œå³Â [target]ã€‚

æ£€æŸ¥ç‚¹æ–‡ä»¶åçš„å‰ç¼€ã€‚

æ˜¯å¦è¦†ç›–å…ˆå‰çš„æ£€æŸ¥ç‚¹

from flax.training import `checkpoints`

checkpoints.save_checkpoint(ckpt_dir='lstm_model_checkpoint/', target=trained_model_state,

step=100,

prefix='lstm_model',

overwrite=False

)

è¦æ¢å¤ä¿å­˜çš„æ¨¡å‹ï¼Œè¯·ä½¿ç”¨Â [restore_checkpoint]Â æ–¹æ³•ã€‚

loaded_model = `checkpoints.restore_checkpoint(`

ckpt_dir='lstm_mod

el_checkpoint/',

target=state, prefix='lstm_mode

l'

)

loaded_model ![](img/00044.gif)æ­¤æ¨¡å‹å¯ç«‹å³ç”¨äºè¿›è¡Œé¢„æµ‹ï¼[](../images/00045.gif)

## æœ€ç»ˆæƒ³æ³•

`åœ¨æœ¬æ–‡ä¸­ï¼Œæ‚¨å·²ç»å­¦ä¼šäº†å¦‚ä½•åœ¨ JAX å’Œ Flax ä¸­è§£å†³è‡ªç„¶è¯­è¨€å¤„ç†é—®é¢˜ã€‚ç‰¹åˆ«æ˜¯æ‚¨æ¶µç›–çš„å…³é”®ç‚¹åŒ…æ‹¬ï¼š`

`å¦‚ä½•ä½¿ç”¨ NLTK å¤„ç†æ–‡æœ¬æ•°æ®ã€‚`

`ä½¿ç”¨ Keras è¿›è¡Œæ–‡æœ¬å‘é‡åŒ–ã€‚`

ä½¿ç”¨ Keras å’Œ TensorFlow åˆ›å»ºæ–‡æœ¬æ•°æ®æ‰¹å¤„ç†ã€‚

`å¦‚ä½•åœ¨ JAX å’Œ Flax ä¸­åˆ›å»º LSTM æ¨¡å‹ã€‚å¦‚ä½•åœ¨ Flax ä¸­è®­ç»ƒå’Œè¯„ä¼° LSTM æ¨¡å‹ã€‚ä¿å­˜å’Œæ¢å¤ Flax LSTM æ¨¡å‹ã€‚`

## **Flax vs. TensorFlow**

`Flax æ˜¯å»ºç«‹åœ¨ JAX ä¸Šçš„ç¥ç»ç½‘ç»œåº“ã€‚TensorFlow æ˜¯ä¸€ä¸ªæ‹¥æœ‰å¤§é‡å·¥å…·å’Œèµ„æºçš„æ·±åº¦å­¦ä¹ åº“ã€‚Flax å’Œ TensorFlow åœ¨æŸäº›æ–¹é¢ç›¸ä¼¼ä½†åˆä¸åŒã€‚ä¾‹å¦‚ï¼ŒFlax å’Œ TensorFlow éƒ½å¯ä»¥åœ¨ XLA ä¸Šè¿è¡Œã€‚`

è®©æˆ‘ä»¬ä»ä½¿ç”¨è¿™ä¸¤ä¸ªåº“çš„ç”¨æˆ·è§’åº¦æ¥çœ‹ä¸€ä¸‹ Flax å’Œ TensorFlow çš„åŒºåˆ«ã€‚

## **TensorFlow å’Œ Flax ä¸­çš„éšæœºæ•°ç”Ÿæˆ**

`åœ¨ TensorFlow ä¸­ï¼Œæ‚¨å¯ä»¥è®¾ç½®å…¨å±€æˆ–å‡½æ•°çº§ç§å­ã€‚åœ¨ TensorFlow ä¸­ç”Ÿæˆéšæœºæ•°éå¸¸ç®€å•ã€‚`tf.random.set_seed(6853)`

ç„¶è€Œï¼Œåœ¨ Flax ä¸­å¹¶éå¦‚æ­¤ã€‚Flax æ˜¯å»ºç«‹åœ¨ JAX ä¹‹ä¸Šçš„ã€‚JAX æœŸæœ›çº¯å‡½æ•°ï¼Œå³æ²¡æœ‰ä»»ä½•å‰¯ä½œç”¨çš„å‡½æ•°ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼ŒJAX å¼•å…¥äº†æ— çŠ¶æ€çš„ä¼ªéšæœºæ•°ç”Ÿæˆå™¨ï¼ˆPRNGsï¼‰ã€‚ä¾‹å¦‚ï¼Œä» NumPy è°ƒç”¨éšæœºæ•°ç”Ÿæˆå™¨æ¯æ¬¡éƒ½ä¼šå¾—åˆ°ä¸åŒçš„æ•°å­—ã€‚

`import numpy as np`

`print(np.random.random()) print(np.random.random()) print(np.random.random())`

`![](img/00046.jpeg)`

`åœ¨ JAX å’Œ Flax ä¸­ï¼Œæ¯æ¬¡è°ƒç”¨æ—¶ç»“æœåº”è¯¥ç›¸åŒã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä»éšæœºçŠ¶æ€ç”Ÿæˆéšæœºæ•°ã€‚çŠ¶æ€ä¸åº”è¯¥è¢«é‡ç”¨ã€‚å¯ä»¥æ‹†åˆ†çŠ¶æ€ä»¥è·å–å¤šä¸ªä¼ªéšæœºæ•°ã€‚`

`import jax`

`key = jax.random.PRNGKey(0)`

`key1, key2, key3 = jax.random.split(key, num=3)`

`![](img/00047.gif)`

## **Flax å’Œ TensorFlow çš„æ¨¡å‹å®šä¹‰**

`åœ¨ TensorFlow ä¸­ï¼Œé€šè¿‡ Keras API å¯ä»¥è½»æ¾å®šä¹‰æ¨¡å‹ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ Keras å®šä¹‰é¡ºåºæˆ–åŠŸèƒ½å‹ç½‘ç»œã€‚Keras æä¾›äº†è®¸å¤šå±‚ç”¨äºè®¾è®¡å„ç§ç±»å‹çš„ç½‘ç»œï¼Œä¾‹å¦‚ CNN å’Œ LSTMã€‚`

åœ¨ Flax ä¸­ï¼Œç½‘ç»œå¯ä»¥ä½¿ç”¨ setup æˆ–ç´§å‡‘æ–¹å¼è®¾è®¡ã€‚setup æ–¹æ³•æ˜¾å¼ï¼Œè€Œç´§å‡‘æ–¹å¼å†…è”ã€‚Setup éå¸¸ç±»ä¼¼äº PyTorch ä¸­è®¾è®¡ç½‘ç»œçš„æ–¹å¼ã€‚ä¾‹å¦‚ï¼Œè¿™é‡Œæ˜¯ä½¿ç”¨ setup æ–¹å¼è®¾è®¡çš„ç½‘ç»œã€‚

`class MLP(nn.Module):def setup(self):`

# `å­æ¨¡å—åç§°ç”±æ‚¨åˆ†é…çš„å±æ€§æ´¾ç”Ÿè€Œæ¥ã€‚åœ¨è¿™`

# `case`, `"dense1"` å’Œ `"dense2"`ã€‚è¿™éµå¾ªäº† PyTorch çš„é€»è¾‘ã€‚

`self.dense1 = nn.Dense(32)`

`self.dense2 = nn.Dense(32)`

`def __call__(self, x): x = self.dense1(x) x = nn.relu(x)`

`x = self.dense2(x) return x`

è¿™é‡Œæ˜¯åŒä¸€ä¸ªç½‘ç»œçš„ç´§å‡‘è®¾è®¡æ–¹å¼ã€‚ç´§å‡‘çš„æ–¹å¼æ›´ä¸ºç›´æ¥ï¼Œå› ä¸ºä»£ç é‡å¤è¾ƒå°‘ã€‚`class MLP(nn.Module):`

`@nn.compact`

`def __call__(self, x):`

`x = nn.Dense(32, name="dense1")(x)`

`x = nn.relu(x)`

`x = nn.Dense(32, name="dense2")(x)`

`return x`

## **Flax å’Œ TensorFlow ä¸­çš„æ¿€æ´»å‡½æ•°**

`[tf.keras.activations]`æ¨¡å—åœ¨ TensorFlow ä¸­æä¾›è®¾è®¡ç½‘ç»œæ—¶æ‰€éœ€çš„å¤§éƒ¨åˆ†æ¿€æ´»å‡½æ•°ã€‚åœ¨ Flax ä¸­ï¼Œæ¿€æ´»å‡½æ•°é€šè¿‡ linen æ¨¡å—æä¾›ã€‚

## **åœ¨ Flax å’Œ TensorFlow ä¸­çš„ä¼˜åŒ–å™¨**

`[tf.keras.optimizers]`ä¸­çš„ä¼˜åŒ–å™¨åœ¨ TensorFlow ä¸­æœ‰æµè¡Œçš„ä¼˜åŒ–å™¨å‡½æ•°ã€‚ä½†æ˜¯ï¼ŒFlax ä¸æä¾›ä»»ä½•ä¼˜åŒ–å™¨å‡½æ•°ã€‚Flax ä¸­ä½¿ç”¨çš„ä¼˜åŒ–å™¨ç”±å¦ä¸€ä¸ªåä¸º Optax çš„åº“æä¾›ã€‚

## **åœ¨ Flax å’Œ TensorFlow ä¸­çš„æŒ‡æ ‡**

åœ¨ TensorFlow ä¸­ï¼ŒæŒ‡æ ‡å¯ä»¥é€šè¿‡

`[tf.keras.metrics]`æ¨¡å—ã€‚æˆªè‡³ç›®å‰ï¼ŒFlax æ²¡æœ‰æŒ‡æ ‡æ¨¡å—ã€‚æ‚¨éœ€è¦ä¸ºæ‚¨çš„ç½‘ç»œå®šä¹‰æŒ‡æ ‡å‡½æ•°æˆ–ä½¿ç”¨å…¶ä»–ç¬¬ä¸‰æ–¹åº“ã€‚

`import optax`

`import jax.numpy as jnp`

`def compute_metrics(logits, labels):`

`loss = jnp.mean(optax.softmax_cross_entropy(logits, jax.nn.on e_hot(labels, num_classes=2)))`

`accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)`

`metrics = {`

`'loss': loss,`

`'accuracy': accuracy`

`}`

`return metrics`

## **åœ¨ Flax å’Œ TensorFlow ä¸­è®¡ç®—æ¢¯åº¦**

`[jax.grad]`å‡½æ•°ç”¨äºåœ¨ Flax ä¸­è®¡ç®—æ¢¯åº¦ã€‚å®ƒæä¾›åŒæ—¶è¿”å›æŸå¤±å’Œæ¢¯åº¦çš„èƒ½åŠ›ï¼Œä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥åŒæ—¶è¿”å›æŸå¤±å’Œæ¢¯åº¦ã€‚

`@jax.jitdef sum_logistic(x):return jnp.sum(1.0 / (1.0 + jnp.exp(-x))),(x + 1)`

`x_small = jnp.arange(6.)`

`derivative_fn = jax.grad(sum_logistic, has_aux=True) print(derivative_fn(x_small))`

# `(DeviceArray([0.25 , 0.19661194, 0.10499357, 0.04517666, 0.01766271,`

# `0.00664806], dtype=float32), DeviceArray([1., 2.,`

`3., 4., 5., 6.], dtype=float32))é«˜çº§è‡ªåŠ¨å¾®åˆ†ä¹Ÿå¯ä»¥å®Œæˆ`

ä½¿ç”¨`jax.vjp()`å’Œ`jax.jvp()`ã€‚

åœ¨ TensorFlow ä¸­ï¼Œä½¿ç”¨`[tf.GradientTape]`è®¡ç®—æ¢¯åº¦ã€‚def grad(model, inputs, targets):with tf.GradientTape() as tape:loss_value = loss(model, inputs, targets, training=True) return loss_value, tape.gradient(loss_value, model.trainable_ variables)`

é™¤éæ‚¨åœ¨ TensorFlow ä¸­åˆ›å»ºè‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ï¼Œå¦åˆ™ä¸ä¼šå®šä¹‰æ¢¯åº¦å‡½æ•°ã€‚å½“æ‚¨è®­ç»ƒç½‘ç»œæ—¶ï¼Œè¿™æ˜¯è‡ªåŠ¨å®Œæˆçš„ã€‚

## **åœ¨ Flax å’Œ TensorFlow ä¸­åŠ è½½æ•°æ®é›†**

TensorFlow æä¾›äº†åŠ è½½æ•°æ®çš„å®ç”¨ç¨‹åºã€‚Flax ä¸é™„å¸¦ä»»ä½•æ•°æ®åŠ è½½å™¨ã€‚æ‚¨å¿…é¡»ä½¿ç”¨æ¥è‡ªå…¶ä»–åº“ï¼ˆå¦‚ TensorFlowï¼‰çš„æ•°æ®åŠ è½½å™¨ã€‚åªè¦æ•°æ®æ˜¯ JAX NumPy æˆ–å¸¸è§„æ•°ç»„ï¼Œå¹¶ä¸”å…·æœ‰é€‚å½“çš„å½¢çŠ¶ï¼Œå°±å¯ä»¥ä¼ é€’ç»™ Flax ç½‘ç»œã€‚

## **åœ¨ Flax ä¸ TensorFlow ä¸­è®­ç»ƒæ¨¡å‹**

åœ¨ TensorFlow ä¸­ï¼Œé€šè¿‡ç¼–è¯‘ç½‘ç»œå¹¶è°ƒç”¨ fit æ–¹æ³•æ¥è®­ç»ƒæ¨¡å‹ã€‚ç„¶è€Œï¼Œåœ¨ Flax ä¸­ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªè®­ç»ƒçŠ¶æ€æ¥ä¿å­˜è®­ç»ƒä¿¡æ¯ï¼Œç„¶åå°†æ•°æ®ä¼ é€’ç»™ç½‘ç»œã€‚

`from flax.training import train_state`

`def create_train_state(rng):`

"""åˆ›å»ºåˆå§‹`TrainState`ã€‚"""

`model = LSTMModel()`

`params = model.init(rng, jnp.array(X_train_padded[0]))['param`

`s']`

`tx = optax.adam(0.001,0.9,0.999,1e-07)`

`return train_state.TrainState.create(`

`apply_fn=model.apply, params=params, tx=tx)`

ä¹‹åï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªè®­ç»ƒæ­¥éª¤ï¼Œè®¡ç®—æŸå¤±å’Œæ¢¯åº¦ã€‚ç„¶åä½¿ç”¨è¿™äº›æ¢¯åº¦æ¥æ›´æ–°æ¨¡å‹å‚æ•°å¹¶è¿”å›æ¨¡å‹æŒ‡æ ‡å’Œæ–°çŠ¶æ€ã€‚

`@jax.jitdef train_step(state, text, labels):def loss_fn(params):`

`logits = LSTMModel().apply({'params': params}, text) loss = jnp.mean(optax.softmax_cross_entropy( logits=logits,`

`labels=jax.nn.one_hot(labels, num_classes=2))) return loss, logits`

`grad_fn = jax.value_and_grad(loss_fn, has_aux=True) (_, logits), grads = grad_fn(state.params)`

`state = state.apply_gradients(grads=grads)`

`metrics = compute_metrics(logits, labels)`

`return state, metrics`

ä½¿ç”¨`Elegy`æ¥è®­ç»ƒç±»ä¼¼äº Keras çš„ç½‘ç»œã€‚Elegy æ˜¯ä¸€ä¸ªåŸºäº JAX ç¥ç»ç½‘ç»œåº“çš„é«˜çº§ APIã€‚

## **Flax å’Œ TensorFlow ä¸­çš„åˆ†å¸ƒå¼è®­ç»ƒ**

åœ¨ TensorFlow ä¸­ä»¥åˆ†å¸ƒå¼æ–¹å¼è®­ç»ƒç½‘ç»œæ˜¯é€šè¿‡åˆ›å»º`distributed strategy.mirrored_strategy = tf.distribute.MirroredStrategy()`æ¥å®Œæˆçš„ã€‚

`with mirrored_strategy.scope():`

`model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_s`

`hape=(1,))])`

`model.compile(loss='mse', optimizer='sgd')`

è¦åœ¨ Flax ä¸­ä»¥åˆ†å¸ƒå¼æ–¹å¼è®­ç»ƒç½‘ç»œï¼Œæˆ‘ä»¬å®šä¹‰æˆ‘ä»¬çš„ Flax å‡½æ•°çš„åˆ†å¸ƒå¼ç‰ˆæœ¬ã€‚è¿™é€šè¿‡`pmap`å‡½æ•°å®Œæˆï¼Œè¯¥å‡½æ•°åœ¨å¤šä¸ªè®¾å¤‡ä¸Šæ‰§è¡Œå‡½æ•°ã€‚ç„¶åï¼Œæ‚¨å°†è®¡ç®—æ‰€æœ‰è®¾å¤‡ä¸Šçš„é¢„æµ‹å¹¶è·å¾—å¹³å‡å€¼ã€‚

ä½¿ç”¨[`jax.lax.pmean()`]ã€‚ä½ è¿˜éœ€è¦ä½¿ç”¨[`jax_utils.replicate`]åœ¨æ‰€æœ‰è®¾å¤‡ä¸Šå¤åˆ¶æ•°æ®ï¼Œä»¥è·å–æŒ‡æ ‡ã€‚

è®¾å¤‡ä½¿ç”¨`jax_utils.unreplicate`ã€‚

## **ä½¿ç”¨ TPU åŠ é€Ÿå™¨**

æ‚¨å¯ä»¥ä½¿ç”¨ Flax å’Œ TensorFlow ä¸ TPU å’Œ GPU åŠ é€Ÿå™¨ã€‚è¦åœ¨ Colab ä¸Šä½¿ç”¨ Flax ä¸ TPUï¼Œæ‚¨éœ€è¦è®¾ç½®å®ƒï¼š`jax.tools.colab_tpu.setup_tpu() jax.devices()![](img/00048.gif)`å¯¹äº TensorFlowï¼Œè®¾ç½®`TPU distributed strategy.cluster_resolver = tf.distribute.cluster_resolver.TPUClusterRes olver(tpu=tpu_address)`

`tf.config.experimental_connect_to_cluster(cluster_resolver) tf.tpu.experimental.initialize_tpu_system(cluster_resolver) tpu_strategy = tf.distribute.TPUStrategy(cluster_resolver)`

## **æ¨¡å‹è¯„ä¼°**

TensorFlow æä¾›äº†`evaluate`å‡½æ•°ç”¨äºè¯„ä¼°ç½‘ç»œã€‚Flax æ²¡æœ‰æä¾›è¿™æ ·çš„å‡½æ•°ã€‚æ‚¨éœ€è¦åˆ›å»ºä¸€ä¸ªåº”ç”¨æ¨¡å‹å¹¶è¿”å›æµ‹è¯•æŒ‡æ ‡çš„å‡½æ•°ã€‚`Elegy`æä¾›ç±»ä¼¼ Keras çš„åŠŸèƒ½ï¼Œä¾‹å¦‚`evaluate`æ–¹æ³•ã€‚

`@jax.jit`

`def eval_step(state, text, labels):`

`logits = LSTMModel().apply({'params': state.params}, text)`

`return compute_metrics(logits=logits, labels=labels)`

`def evaluate_model(state, text, test_lbls): """Evaluate on the validation set."""`

`metrics = eval_step(state, text, test_lbls) metrics = jax.device_get(metrics)`

`metrics = jax.tree_map(lambda x: x.item(), metrics) return metrics`

## **å¯è§†åŒ–æ¨¡å‹æ€§èƒ½**

æ¨¡å‹çš„å¯è§†åŒ–åœ¨ Flax å’Œ TensorFlow ä¸­æ˜¯ç±»ä¼¼çš„ã€‚ä¸€æ—¦è·å¾—æŒ‡æ ‡ï¼Œå¯ä»¥ä½¿ç”¨è¯¸å¦‚Â `Matplotlib`Â ä¹‹ç±»çš„è½¯ä»¶åŒ…æ¥å¯è§†åŒ–æ¨¡å‹çš„æ€§èƒ½ã€‚ä½ ä¹Ÿå¯ä»¥åœ¨ Flax å’Œ TensorFlow ä¸­ä½¿ç”¨Â `TensorBoard`ã€‚

## **æœ€åçš„æ€è€ƒ**

ä½ å·²ç»çœ‹åˆ°äº† Flax å’Œ TensorFlow åº“ä¹‹é—´çš„å·®å¼‚ã€‚ç‰¹åˆ«æ˜¯ï¼Œåœ¨æ¨¡å‹å®šä¹‰å’Œè®­ç»ƒæ–¹é¢æœ‰æ‰€ä¸åŒã€‚

## åœ¨ Flax ä¸­ä»å¤´å¼€å§‹è®­ç»ƒ ResNetï¼ˆåˆ†å¸ƒå¼ ResNet è®­ç»ƒï¼‰

é™¤äº†è®¾è®¡å®šåˆ¶çš„ CNN æ¶æ„å¤–ï¼Œæ‚¨è¿˜å¯ä»¥ä½¿ç”¨å·²ç»æ„å»ºå¥½çš„æ¶æ„ã€‚ResNet å°±æ˜¯è¿™æ ·ä¸€ä¸ªæµè¡Œçš„æ¶æ„ä¹‹ä¸€ã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œä½¿ç”¨è¿™æ ·çš„æ¶æ„ä¼šè·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæ‚¨å°†å­¦ä¹ å¦‚ä½•åœ¨ Flax ä¸­è¿›è¡Œ ResNet æ¨¡å‹çš„åˆ†å¸ƒå¼è®­ç»ƒã€‚

## å®‰è£… Flax æ¨¡å‹

[flaxmodels]Â åŒ…ä¸º Jax å’Œ Flax æä¾›äº†é¢„è®­ç»ƒæ¨¡å‹ï¼ŒåŒ…æ‹¬ï¼š

StyleGAN2

`GPT2`

`VGG`

`ResNet`

`git clone https://github.com/matthias-wright/flaxmodels.git pip install -r flaxmodels/training/resnet/requirements.txt`

åœ¨è¿™ä¸ªé¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬å°†ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹â€”â€”ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬ä¸ä¼šä½¿ç”¨é¢„è®­ç»ƒçš„æƒé‡ã€‚åœ¨å¦ä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å·²ç»è®¨è®ºäº†å¦‚ä½•ä½¿ç”¨ ResNet è¿›è¡Œè¿ç§»å­¦ä¹ ã€‚

## æ‰§è¡Œæ ‡å‡†å¯¼å…¥

å®‰è£…äº†Â [flaxmodels]Â åï¼Œè®©æˆ‘ä»¬å¯¼å…¥æœ¬æ–‡ä¸­ä½¿ç”¨çš„æ ‡å‡†åº“ã€‚

import`wget`# pip install wget

import`zipfile`

`import torch`

from`torch.utils.data`import`DataLoader import os`

from`PIL`import`Image`

from`torchvision`import`transforms from torch.utils.data import Dataset import numpy as np`

`import pandas as pd`

`import matplotlib.pyplot as plt`

%matplotlib inline

# å¿½ç•¥æ— å®³çš„è­¦å‘Š

`import warnings`

`warnings.filterwarnings("ignore") import jax`

from`jax`import`numpy as jnp`

`import flax`

from`flax`import`linen as nn`

from`flax.training`import`train_state import optax`

import`time`

`from tqdm.notebook import tqdm`

`import math`

from`flax`import`jax_utils`

## ä¸‹è½½æ•°æ®é›†

æˆ‘ä»¬å°†è®­ç»ƒ ResNet æ¨¡å‹æ¥é¢„æµ‹æ¥è‡ªçŒ«å’Œç‹—æ•°æ®é›†çš„ä¸¤ç±»ã€‚ä¸‹è½½å¹¶æå–çŒ«å’Œç‹—çš„å›¾åƒã€‚

`wget.download("https://ml.machinelearningnuggets.com/train.zi p")`

with`zipfile.ZipFile('train.zip', 'r') as zip_ref:`

`zip_ref.extractall('.')`

## åœ¨ Flax ä¸­åŠ è½½æ•°æ®é›†

ç”±äº JAX å’Œ Flax ä¸åŒ…å«ä»»ä½•æ•°æ®åŠ è½½å™¨ï¼Œæˆ‘ä»¬ä½¿ç”¨ PyTorch æˆ– TensorFlow ä¸­çš„æ•°æ®åŠ è½½å·¥å…·ã€‚å½“ä½¿ç”¨ PyTorch æ—¶ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ›å»ºä¸€ä¸ªæ•°æ®é›†ç±»ã€‚

`class CatsDogsDataset(Dataset):`

`def __init__(self, root_dir, annotation_file, transform=Non`

e):

`self.root_dir = root_dir`

`self.annotations = pd.read_csv(annotation_file) self.transform = transform`

`def __len__(self):return len(self.annotations)`

`def __getitem__(self, index):`

`img_id = self.annotations.iloc[index, 0]`

`img = Image.open(os.path.join(self.root_dir, img_id)).c`

`convert("RGB")`

`y_label = torch.tensor(float(self.annotations.iloc[inde`

x, 1]))

å¦‚æœå­˜åœ¨è½¬æ¢ï¼Œåˆ™è¿›è¡Œè½¬æ¢ï¼šimg = self.transform(img)return (img, y_label)æ¥ä¸‹æ¥ï¼Œåˆ›å»ºåŒ…å«å›¾åƒè·¯å¾„å’Œæ ‡ç­¾çš„ Pandas DataFrameã€‚

`train_df = pd.DataFrame(columns=["img_path","label"]) train_df["img_path"] = os.listdir("train/") for idx, i in enumerate(os.listdir("train/")):`

`if "cat" in i:`

`train_df["label"][idx] = 0`

`if "dog" in i:`

`train_df["label"][idx] = 1`

`train_df.to_csv (r'train_csv.csv', index = False, header=True)`

## Flax ä¸­çš„æ•°æ®è½¬æ¢

å®šä¹‰ä¸€ä¸ªå°†æ•°æ®å †å å¹¶ä½œä¸º NumPy æ•°ç»„è¿”å›çš„å‡½æ•°ã€‚

`def custom_collate_fn(batch):`

`transposed_data = list(zip(*batch)) labels = np.array(transposed_data[1]) imgs = np.stack(transposed_data[0]) return imgs, labels`

åˆ›å»ºç”¨äºè°ƒæ•´å›¾åƒå¤§å°çš„è½¬æ¢ã€‚æ¥ä¸‹æ¥ï¼Œå°†è¯¥è½¬æ¢åº”ç”¨äºæ—©æœŸåˆ›å»ºçš„æ•°æ®é›†ã€‚`size_image = 224`

`transform = transforms.Compose([`

`transforms.Resize((size_image,size_image)), np.array])`

`dataset = CatsDogsDataset("train","train_csv.csv",transform=transform)`

å°†æ­¤æ•°æ®é›†æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œå¹¶ä¸ºæ¯ä¸ªé›†åˆ›å»ºæ•°æ®åŠ è½½å™¨ã€‚`batch_size = 32`

`train_set, validation_set = torch.utils.data.random_split(dataset,[20000,5000])`

`train_loader = DataLoader(dataset=train_set, collate_fn=custom_collate_fn,shuffle=True, batch_size=batch_size)`

`validation_loader = DataLoader(dataset=validation_set,collate_fn=custom_collate_fn, shuffle=False, batch_size=batch_size)`

## å®ä¾‹åŒ– Flax ResNet æ¨¡å‹

æœ‰äº†æ•°æ®å‡†å¤‡å°±ç»ªï¼Œä½¿ç”¨`[flaxmodels]`åŒ…å®ä¾‹åŒ– Flax ResNet æ¨¡å‹ã€‚å®ä¾‹åŒ–éœ€è¦ï¼š

æ‰€éœ€çš„ç±»åˆ«æ•°ã€‚

è¾“å‡ºç±»å‹ã€‚

æ•°æ®ç±»å‹ã€‚

æ˜¯å¦é¢„è®­ç»ƒæ¨¡å‹ - åœ¨è¿™ç§æƒ…å†µä¸‹`[False]`ã€‚import jax.numpy as jnp import flaxmodels as fm

`num_classes = 2`

`dtype = jnp.float32 model = fm.ResNet50(output='log_softmax', pretrained=None, num_classes=num_classes, dtype=dtype)`

## è®¡ç®—æŒ‡æ ‡

å®šä¹‰è¯„ä¼°æ¨¡å‹è®­ç»ƒæœŸé—´çš„æŒ‡æ ‡ã€‚è®©æˆ‘ä»¬é¦–å…ˆåˆ›å»ºæŸå¤±å‡½æ•°ã€‚

`def cross_entropy_loss(*, logits, labels):`

`labels_onehot = jax.nn.one_hot(labels, num_classes=num_classes)`

`s)`

`return optax.softmax_cross_entropy(logits=logits, labels=labels)`

`ls_onehot).mean()`

æ¥ä¸‹æ¥ï¼Œå®šä¹‰ä¸€ä¸ªè®¡ç®—å¹¶è¿”å›æŸå¤±å’Œå‡†ç¡®ç‡çš„å‡½æ•°ã€‚

`def compute_metrics(*, logits, labels):`

`loss = cross_entropy_loss(logits=logits, labels=labels) accuracy = jnp.mean(jnp.argmax(logits, -1) == labels) metrics = {`

`'loss': loss',`

`'accuracy': accuracy`,

`}`

`return metrics`

## åˆ›å»º Flax æ¨¡å‹è®­ç»ƒçŠ¶æ€

Flax æä¾›äº†ç”¨äºå­˜å‚¨è®­ç»ƒä¿¡æ¯çš„è®­ç»ƒçŠ¶æ€ã€‚å¯ä»¥ä¿®æ”¹è®­ç»ƒçŠ¶æ€ä»¥æ·»åŠ æ–°ä¿¡æ¯ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éœ€è¦ä¿®æ”¹è®­ç»ƒçŠ¶æ€ä»¥æ·»åŠ æ‰¹æ¬¡ç»Ÿè®¡ä¿¡æ¯ï¼Œå› ä¸º ResNet æ¨¡å‹è®¡ç®—`[batch_stats]`ã€‚

`class TrainState(train_state.TrainState): batch_stats: flax.core.FrozenDict`

æˆ‘ä»¬éœ€è¦æ¨¡å‹å‚æ•°å’Œæ‰¹æ¬¡ç»Ÿè®¡æ¥åˆ›å»ºè®­ç»ƒçŠ¶æ€å‡½æ•°ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡å°†æ¨¡å‹åˆå§‹åŒ–ä¸º`[train]`ä¸º`[False]`æ¥è®¿é—®è¿™äº›å†…å®¹ã€‚

`key = jax.random.PRNGKey(0)`

`variables = model.init(key, jnp.ones([1, size_image, size_image, 3]), train=False)`

`TrainState`çš„`create`æ–¹æ³•éœ€è¦ä»¥ä¸‹å‚æ•°ï¼š`[apply_fn]`â€“ æ¨¡å‹åº”ç”¨å‡½æ•°ã€‚æ¨¡å‹å‚æ•°â€“ `variables['params']`ã€‚

ä¼˜åŒ–å™¨é€šå¸¸ä½¿ç”¨ Optax å®šä¹‰ã€‚

æ‰¹æ¬¡ç»Ÿè®¡â€“ `variables['batch_stats']`ã€‚

æˆ‘ä»¬å¯¹è¿™ä¸ªå‡½æ•°åº”ç”¨`[pmap]`æ¥åˆ›å»ºä¸€ä¸ªåˆ†å¸ƒå¼ç‰ˆæœ¬çš„è®­ç»ƒçŠ¶æ€ã€‚ `[pmap]` ç¼–è¯‘è¯¥å‡½æ•°ä»¥åœ¨å¤šä¸ªè®¾å¤‡ä¸Šæ‰§è¡Œï¼Œä¾‹å¦‚å¤šä¸ª GPU å’Œ TPUã€‚

`import functools`

`@functools.partial(jax.pmap)`

`def create_train_state(rng):`

"""åˆ›å»ºåˆå§‹çš„`TrainState`ã€‚"""

`return TrainState.create(apply_fn = model.apply,params = variables['params'],tx = optax.adam(0.01,0.9),batch_stats = variables['batch_stats'])`

## **åº”ç”¨æ¨¡å‹å‡½æ•°**

æ¥ä¸‹æ¥ï¼Œå®šä¹‰ä¸€ä¸ªå¹¶è¡Œæ¨¡å‹è®­ç»ƒå‡½æ•°ã€‚ä¼ é€’ä¸€ä¸ª`[axis_name]`ä»¥ä¾¿æ‚¨å¯ä»¥ä½¿ç”¨å®ƒæ¥èšåˆæ¥è‡ªæ‰€æœ‰è®¾å¤‡çš„æŒ‡æ ‡ã€‚è¯¥å‡½æ•°ï¼š

è®¡ç®—æŸå¤±ã€‚

é€šè¿‡è®¡ç®—ä½¿ç”¨`[jax.lax.pmean()]` çš„æ¦‚ç‡çš„å¹³å‡å€¼æ¥ä»æ‰€æœ‰è®¾å¤‡ä¸Šè®¡ç®—é¢„æµ‹ã€‚

åœ¨åº”ç”¨æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬è¿˜åŒ…æ‹¬äº†æ‰¹æ¬¡ç»Ÿè®¡å’Œ`[DropOut]`çš„éšæœºæ•°ã€‚ç”±äºè¿™æ˜¯è®­ç»ƒå‡½æ•°ï¼Œ`[train]`å‚æ•°æ˜¯`[True]`ã€‚åœ¨è®¡ç®—æ¢¯åº¦æ—¶ï¼Œä¹ŸåŒ…æ‹¬äº†`[batch_stats]`ã€‚`[update_model]`å‡½æ•°åº”ç”¨è®¡ç®—å¾—åˆ°çš„æ¢¯åº¦â€“ æ›´æ–°æ¨¡å‹å‚æ•°ã€‚

`@functools.partial(jax.pmap, axis_name='ensemble')` `def apply_model(state, images, labels):` `def loss_fn(params,batch_stats):`

`logits, batch_stats = model.apply({'params': params, 'batch_stats': batch_stats}, images, train=True, rngs={'dropout': jax.random.PRNGKey(0)}, mutable=['batch_stats'])`

`one_hot = jax.nn.one_hot(labels, num_classes)` `loss = optax.softmax_cross_entropy(logits=logits, labels=one_hot).mean()` è¿”å›æŸå¤±ï¼Œ`(logits, batch_stats)` `(loss, (logits, batch_stats)), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params,state.batch_stats)` `probs = jax.lax.pmean(jax.nn.softmax(logits), axis_name='ensemble')` `accuracy = jnp.mean(jnp.argmax(probs, -1) == labels)` è¿”å›æ¢¯åº¦ï¼Œ`æŸå¤±ï¼Œå‡†ç¡®ç‡@jax.pmap` `def update_model(state, grads):` åº”ç”¨æ¢¯åº¦ `return state.apply_gradients(grads=grads)`

## **åœ¨ Flax ä¸­ä½¿ç”¨ TensorBoard**

ä¸‹ä¸€æ­¥æ˜¯è®­ç»ƒ ResNet æ¨¡å‹ã€‚ä½†æ˜¯ï¼Œæ‚¨å¯èƒ½æœ‰å…´è¶£ä½¿ç”¨`[TensorBoard]`è·Ÿè¸ªè®­ç»ƒã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨éœ€è¦é…ç½® TensorBoardã€‚æ‚¨å¯ä»¥ä½¿ç”¨ PyTorch çš„`SummaryWriter`å°†æŒ‡æ ‡å†™å…¥ TensorBoardã€‚

`rm -rf ./flax_logs/`

`from torch.utils.tensorboard import SummaryWriter` `import torchvision.transforms.functional as F` `logdir = "flax_logs"`

`writer = SummaryWriter(logdir)`

## **è®­ç»ƒ Flax ResNet æ¨¡å‹**

è®©æˆ‘ä»¬åœ¨æ•´ä¸ªè®­ç»ƒé›†ä¸Šè®­ç»ƒ ResNet æ¨¡å‹ï¼Œå¹¶åœ¨æµ‹è¯•é›†çš„å­é›†ä¸Šè¿›è¡Œè¯„ä¼°ã€‚æ‚¨ä¹Ÿå¯ä»¥åœ¨æ•´ä¸ªæµ‹è¯•é›†ä¸Šè¿›è¡Œè¯„ä¼°ã€‚å°†æµ‹è¯•é›†å¤åˆ¶åˆ°å¯ç”¨è®¾å¤‡ä¸Šã€‚

`(test_images, test_labels) = next(iter(validation_loader))` `test_images = test_images / 255.0`

`test_images = np.array(jax_utils.replicate(test_images))` `test_labels = np.array(jax_utils.replicate(test_labels))`

åˆ›å»ºä¸€äº›åˆ—è¡¨ä»¥ä¿å­˜è®­ç»ƒå’Œè¯„ä¼°æŒ‡æ ‡ã€‚

`epoch_loss = []`

`epoch_accuracy = []` `testing_accuracy = []` `testing_loss = []`

æ¥ä¸‹æ¥ï¼Œå®šä¹‰ ResNet æ¨¡å‹è®­ç»ƒå‡½æ•°ã€‚è¯¥å‡½æ•°æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š

å¾ªç¯éå†è®­ç»ƒæ•°æ®é›†å¹¶å¯¹å…¶è¿›è¡Œç¼©æ”¾ã€‚

åœ¨å¯ç”¨è®¾å¤‡ä¸Šå¤åˆ¶æ•°æ®ã€‚

åœ¨æ•°æ®é›†ä¸Šåº”ç”¨æ¨¡å‹å¹¶è®¡ç®—æŒ‡æ ‡ã€‚ä»è®¾å¤‡è·å–æŒ‡æ ‡

`using jax_utils.unreplicate.`

å°†æŒ‡æ ‡é™„åŠ åˆ°åˆ—è¡¨ã€‚

è®¡ç®—æŸå¤±å’Œå‡†ç¡®åº¦çš„å‡å€¼ä»¥è·å–æ¯ä¸ª epoch çš„æŒ‡æ ‡ã€‚

å°†æ¨¡å‹åº”ç”¨äºæµ‹è¯•é›†å¹¶è·å–æŒ‡æ ‡ã€‚

å°†æµ‹è¯•æŒ‡æ ‡é™„åŠ åˆ°åˆ—è¡¨ã€‚

å°†è®­ç»ƒå’Œè¯„ä¼°æŒ‡æ ‡å†™å…¥ TensorBaordã€‚

æ‰“å°è®­ç»ƒå’Œè¯„ä¼°æŒ‡æ ‡ã€‚

`images = images / 255.0`

`images = jax_utils.replicate(images)`

`labels = jax_utils.replicate(labels)`

`grads, loss, accuracy = apply_model(state, images,`

`labels)state = update_model(state, grads)`

`epoch_loss.append(jax_utils.unreplicate(loss)) epoch_accuracy.append(jax_utils.unreplicate(accuracy)) train_loss = np.mean(epoch_loss)`

`train_accuracy = np.mean(epoch_accuracy)`

`_`, `test_loss`, `test_accuracy` = `jax_utils.unreplicate(app ly_model(state, test_images, test_labels))`

`testing_accuracy.append(test_accuracy)`

`testing_loss.append(test_loss)`

`writer.add_scalar('Loss/train', np.array(train_loss), e poch)`

`writer.add_scalar('Loss/test', np.array(test_loss), epo ch)`

`writer.add_scalar('Accuracy/train', np.array(train_accu racy), epoch)`

`writer.add_scalar('Accuracy/test', np.array(test_accura cy), epoch)`

`print(f"Epoch: {epoch + 1}, train loss: {train_loss:.4 f}, train accuracy: {train_accuracy * 100:.4f}, test loss: {tes t_loss:.4f}, test accuracy: {test_accuracy* 100:.4f}", flush=Tr ue)`

è¿”å› `state, epoch_loss, epoch_accuracy, testing_accuracy, testing_loss` é€šè¿‡ç”Ÿæˆä¸è®¾å¤‡æ•°é‡ç›¸åŒçš„éšæœºæ•°æ¥åˆ›å»ºè®­ç»ƒçŠ¶æ€ã€‚

`seed = 0`

`rng = jax.random.PRNGKey(seed)`

`rng, init_rng = jax.random.split(rng)`

`state = create_train_state(jax.random.split(init_rng, jax.devic e_count()))`

`del init_rng # Must not be used anymore.`

é€šè¿‡ä¼ é€’è®­ç»ƒæ•°æ®å’Œ epochs æ•°é‡æ¥è®­ç»ƒ ResNet æ¨¡å‹ã€‚

`start = time.time()`

`num_epochs = 30`

`state, epoch_loss, epoch_accuracy, testing_accuracy, testing_lo ss = train_one_epoch(state, train_loader,num_epochs) print("Total time: ", time.time() - start, "seconds")`

`![](img/00049.jpeg)`

## **ä½¿ç”¨ TensorBoard è¯„ä¼°æ¨¡å‹**

è¿è¡Œ TensorBoard æŸ¥çœ‹ TensorBoard ä¸Šè®°å½•çš„æ ‡é‡ã€‚%load_ext tensorboard%tensorboard --logdir={logdir}![](img/00050.jpeg)![](img/00051.jpeg)

## **å¯è§†åŒ– Flax æ¨¡å‹æ€§èƒ½**

å­˜å‚¨åœ¨åˆ—è¡¨ä¸­çš„æŒ‡æ ‡å¯ä»¥ä½¿ç”¨ Matplotlib ç»˜åˆ¶ã€‚

`plt.plot(epoch_accuracy, label="Training")`

`plt.plot(testing_accuracy, label="Test")`

`plt.xlabel("Epoch")`

`plt.ylabel("Accuracy")`

`plt.legend()`

`plt.show()`

`plt.plot(epoch_loss, label="Training")`

`plt.plot(testing_loss, label="Test")`

`plt.xlabel("Epoch")`

`plt.ylabel("Accuracy")`

`plt.legend()`

`plt.show()`

`![](img/00052.jpeg)![](img/00053.jpeg)`

## ä¿å­˜ Flax ResNet æ¨¡å‹

`ä¿å­˜è®­ç»ƒå¥½çš„ Flax ResNet æ¨¡å‹ä½¿ç”¨`

save_checkpointÂ å‡½æ•°ã€‚è¯¥å‡½æ•°æœŸæœ›ï¼š

å°†ä¿å­˜ ResNet æ¨¡å‹çš„æ–‡ä»¶å¤¹ã€‚

è¦ä¿å­˜çš„æ¨¡å‹â€“ [ç›®æ ‡]ã€‚è®­ç»ƒæ­¥éª¤â€“ è®­ç»ƒæ­¥éª¤ç¼–å·ã€‚

æ¨¡å‹å‰ç¼€ã€‚æ˜¯å¦è¦†ç›–ç°æœ‰æ¨¡å‹ã€‚

`!pip install tensorstore`

`from flax.training import checkpoints`

`ckpt_dir = 'model_checkpoint/'`

`checkpoints.save_checkpoint(ckpt_dir=ckpt_dir,`

`target=state, step=100,`

`prefix='flax_model', overwrite=True`

`)![](img/00054.jpeg)`

## åŠ è½½ Flax RestNet æ¨¡å‹

ä¿å­˜çš„ ResNet Flax æ¨¡å‹ä¹Ÿå¯ä»¥åŠ è½½ä»¥è¿›è¡Œé¢„æµ‹ã€‚Flax æ¨¡å‹ä½¿ç”¨Â [restore_checkpoint]Â å‡½æ•°åŠ è½½ã€‚è¯¥å‡½æ•°æœŸæœ›ï¼š

ç›®æ ‡çŠ¶æ€ã€‚

åŒ…å«ä¿å­˜æ¨¡å‹çš„æ–‡ä»¶å¤¹ã€‚

æ¨¡å‹çš„å‰ç¼€ã€‚

`loaded_model = checkpoints.restore_checkpoint(`

`ckpt_dir=ckpt_dir, target=state, prefix='flax_mode`

`l') ![](img/00055.gif)`

## æœ€åçš„æƒ³æ³•

åœ¨æœ¬æ–‡ä¸­ï¼Œæ‚¨å·²ç»å­¦ä¹ äº†å¦‚ä½•åœ¨ Flax ä¸­ä»å¤´å¼€å§‹è®­ç»ƒ ResNet æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæ‚¨å·²ç»æ¶µç›–äº†ï¼š

åœ¨ Flax ä¸­åˆ›å»º ResNet æ¨¡å‹ã€‚

ä¸º ResNet Flax æ¨¡å‹å®šä¹‰è®­ç»ƒçŠ¶æ€ã€‚

åœ¨åˆ†å¸ƒå¼æ–¹å¼ä¸‹è®­ç»ƒ Flax ResNet æ¨¡å‹ã€‚ä½¿ç”¨ TensorBoard è·Ÿè¸ª Flax ResNet æ¨¡å‹çš„æ€§èƒ½ã€‚

ä¿å­˜å’ŒåŠ è½½ Flax ResNet æ¨¡å‹ã€‚

## ä½¿ç”¨ JAX å’Œ Flax è¿›è¡Œè¿ç§»å­¦ä¹ 

è®­ç»ƒå¤§å‹ç¥ç»ç½‘ç»œå¯èƒ½éœ€è¦å‡ å¤©æˆ–å‡ å‘¨ã€‚ä¸€æ—¦è¿™äº›ç½‘ç»œè¢«è®­ç»ƒå¥½ï¼Œæ‚¨å¯ä»¥åˆ©ç”¨å®ƒä»¬çš„æƒé‡å¹¶å°†å®ƒä»¬åº”ç”¨äºæ–°ä»»åŠ¡â€“ è¿ç§»å­¦ä¹ ã€‚å› æ­¤ï¼Œæ‚¨å¯ä»¥å¾®è°ƒä¸€ä¸ªé¢„è®­ç»ƒçš„ ResNet ç½‘ç»œï¼Œå¹¶åœ¨çŸ­æ—¶é—´å†…è·å¾—è‰¯å¥½çš„ç»“æœã€‚è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åœ¨ JAX å’Œ Flax ä¸­å¯¹é¢„è®­ç»ƒçš„ ResNet ç½‘ç»œè¿›è¡Œå¾®è°ƒã€‚

## å®‰è£… JAX ResNet

`æˆ‘ä»¬å°†ä½¿ç”¨Â jax-resnetÂ åº“æä¾›çš„ ResNet æ£€æŸ¥ç‚¹ã€‚pip install jax-resnet è®©æˆ‘ä»¬ä¸€èµ·å¯¼å…¥å®ƒå’Œæœ¬æ–‡ä¸­ä½¿ç”¨çš„å…¶ä»–è½¯ä»¶åŒ…ã€‚`

# `pip install flax`

`import numpy as np`

`import pandas as pd`

`from PIL import Image`

`import jax`

`import optax`

`import flax`

`import jax.numpy as jnp`

`from jax_resnet import pretrained_resnet, slice_variables, Sequ ential`

`from flax.training import train_state from flax import linen as nn`

`from flax.core import FrozenDict,frozen_dict from functools import partial`

`import os`

`import torch`

`from torch.utils.data import DataLoader from torchvision import transforms`

`from torch.utils.data import Dataset import matplotlib.pyplot as plt`

`%matplotlib inline`

# å¿½ç•¥æ— å®³çš„è­¦å‘Š

`import warnings`

`warnings.filterwarnings("ignore")`

## ä¸‹è½½æ•°æ®é›†

æˆ‘ä»¬å°†å¾®è°ƒ ResNet æ¨¡å‹ä»¥é¢„æµ‹æ¥è‡ªçŒ«ç‹—æ•°æ®é›†çš„ä¸¤ä¸ªç±»ã€‚ä¸‹è½½å¹¶æå–çŒ«å’Œç‹—çš„å›¾åƒã€‚

`pip install wget`

`import wget`

`wget.download("https://ml.machinelearningnuggets.com/train.zi p")`

`import zipfile`

`with zipfile.ZipFile('train.zip', 'r') as zip_ref:`

`zip_ref.extractall('.')`

## åœ¨ JAX ä¸­åŠ è½½æ•°æ®

JAX ä¸åŒ…å«æ•°æ®åŠ è½½å·¥å…·ã€‚æˆ‘ä»¬ä½¿ç”¨ç°æœ‰çš„ TensorFlow å’Œ PyTorch æ•°æ®åŠ è½½å™¨åŠ è½½æ•°æ®ã€‚è®©æˆ‘ä»¬ä½¿ç”¨ PyTorch åŠ è½½å›¾åƒæ•°æ®ã€‚

ç¬¬ä¸€æ­¥æ˜¯åˆ›å»ºä¸€ä¸ª PyTorch [æ•°æ®é›†] ç±»ã€‚

class CatsDogsDataset(Dataset):`

def __init__(self, root_dir, annotation_file, transform=Non

e):

self.root_dir = root_dir

self.annotations = pd.read_csv(annotation_file) self.transform = transform

`def __len__(self):return len(self.annotations)`

`def __getitem__(self, index):`

img_id = self.annotations.iloc[index, 0]

img = Image.open(os.path.join(self.root_dir, img_id)).c

`onvert("RGB")`

`y_label = torch.tensor(float(self.annotations.iloc[inde`

`x, 1])`

å¦‚æœ self.transform ä¸ä¸ºç©ºï¼šimg = self.transform(img) return (img, y_label)

## **æ•°æ®å¤„ç†**

æ¥ä¸‹æ¥ï¼Œä½¿ç”¨å›¾åƒè·¯å¾„å’Œæ ‡ç­¾åˆ›å»ºä¸€ä¸ª Pandas DataFrameã€‚

`train_df = pd.DataFrame(columns=["img_path","label"]) train_df["img_path"] = os.listdir("train/") for idx, i in enumerate(os.listdir("train/")):`

å¦‚æœ "cat" åœ¨ i ä¸­ï¼š`train_df["label"][idx] = 0if "dog" åœ¨ i ä¸­ï¼štrain_df["label"][idx] = 1train_df.to_csv (r'train_csv.csv', index = False, header=True)![](img/00056.jpeg)å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥å †å æ•°æ®å¹¶å°†å›¾åƒå’Œæ ‡ç­¾ä½œä¸º NumPy æ•°ç»„è¿”å›ã€‚

`def custom_collate_fn(batch):`

`transposed_data = list(zip(*batch))`

`labels = np.array(transposed_data[1])`

imgs = np.stack(transposed_data[0])

è¿”å›å›¾åƒå’Œæ ‡ç­¾ä½œä¸º NumPy æ•°ç»„

æˆ‘ä»¬è¿˜éœ€è¦è°ƒæ•´å›¾åƒå¤§å°ä»¥ç¡®ä¿å®ƒä»¬çš„å°ºå¯¸ä¸€è‡´ã€‚åœ¨é…ç½®å­—å…¸ä¸­å®šä¹‰å°ºå¯¸ã€‚ç¨åæˆ‘ä»¬å°†ä½¿ç”¨å…¶ä»–é…ç½®å˜é‡ã€‚

config = {

'æ ‡ç­¾æ•°é‡': 2,

'æ‰¹å¤„ç†å¤§å°': 32,

'è®­ç»ƒå‘¨æœŸ': 5,

`'å­¦ä¹ é€Ÿç‡': 0.001,`

`'å›¾åƒå°ºå¯¸': 224,`

`'æƒé‡è¡°å‡': 1e-5, 'å†»ç»“éª¨å¹²ç½‘ç»œ': True,`

}

ä½¿ç”¨ PyTorch è½¬æ¢è°ƒæ•´å›¾åƒå¤§å°ã€‚æ¥ä¸‹æ¥ï¼Œä½¿ç”¨ CatsDogsDataset ç±»å®šä¹‰è®­ç»ƒå’Œæµ‹è¯•æ•°æ®åŠ è½½å™¨ã€‚

`transform = transforms.Compose([`

transforms.Resize((config["IMAGE_SIZE"],config["IMAGE_SIZ

`E"]))`,

`np.array`

dataset = CatsDogsDataset("train","train_csv.csv",transform=tra

nsform)

`train_set`, `validation_set = torch.utils.data.random_split(datas`

`et,[20000,5000])`

`train_loader = DataLoader(dataset=train_set, collate_fn=custom_`

collate_fn,shuffle=True, batch_size=config["BATCH_SIZE"])

`validation_loader = DataLoader(dataset=validation_set,collate_f`

n=custom_collate_fn, shuffle=False, batch_size=config["BATCH_SI

`ZE"])`

## **ResNet æ¨¡å‹å®šä¹‰**

é¢„è®­ç»ƒçš„ ResNet æ¨¡å‹åœ¨è®¸å¤šç±»ä¸Šè¿›è¡Œäº†è®­ç»ƒã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬çš„æ•°æ®é›†åªæœ‰ä¸¤ç±»ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨ ResNet ä½œä¸ºä¸»å¹²ï¼Œå¹¶å®šä¹‰ä¸€ä¸ªè‡ªå®šä¹‰åˆ†ç±»å±‚ã€‚

## **åˆ›å»ºå¤´éƒ¨ç½‘ç»œ**

åˆ›å»ºä¸€ä¸ªå¤´éƒ¨ç½‘ç»œï¼Œè¾“å‡ºä¸é—®é¢˜ç›¸ç¬¦ï¼Œæœ¬ä¾‹ä¸­ä¸ºäºŒè¿›åˆ¶å›¾åƒåˆ†ç±»ã€‚

"""

å‚è€ƒ - https://www.kaggle.com/code/alexlwh/happywhale-flax

`-jax-tpu-gpu-resnet-baseline`

"""

`class Head(nn.Module):`

'''head model'''`batch_norm_cls: partial = partial(nn.BatchNorm, momentum=0.`

9)

@nn.compact

`def __call__(self, inputs, train: bool):`

output_n = inputs.shape[-1]x = self.batch_norm_cls(use_running_average=not train)

`(inputs)

`x = nn.Dropout(rate=0.25)(x, deterministic=not train) x = nn.Dense(features=output_n)(x)

`x = nn.relu(x)

`x = self.batch_norm_cls(use_running_average=not train)

`(x)

`x = nn.Dropout(rate=0.5)(x, deterministic=not train) x = nn.Dense(features=config["NUM_LABELS"])(x) return x

## **å°† ResNet ä¸»å¹²ä¸å¤´éƒ¨æ¨¡å‹ç»“åˆ**

å°†é¢„è®­ç»ƒçš„ ResNet ä¸»å¹²ä¸æ‚¨ä¸Šé¢åˆ›å»ºçš„è‡ªå®šä¹‰å¤´éƒ¨ç»“åˆã€‚

`class Model(nn.Module):

```ç»“åˆä¸»å¹²å’Œå¤´éƒ¨æ¨¡å‹``` backbone: Sequential

head: Head

def __call__(self, inputs, train: bool): x = self.backbone(inputs)

# å¹³å‡æ± åŒ–å±‚

x = jnp.mean(x, axis=(1, 2)) x = self.head(x, train)

è¿”å› x

## **åŠ è½½é¢„è®­ç»ƒçš„ ResNet 50**

æ¥ä¸‹æ¥ï¼Œåˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼ŒåŠ è½½é¢„è®­ç»ƒçš„ ResNet æ¨¡å‹ã€‚çœç•¥ç½‘ç»œçš„æœ€åä¸¤å±‚ï¼Œå› ä¸ºæˆ‘ä»¬å·²å®šä¹‰äº†ä¸€ä¸ªè‡ªå®šä¹‰å¤´éƒ¨ã€‚è¯¥å‡½æ•°è¿”å› ResNet æ¨¡å‹åŠå…¶å‚æ•°ã€‚ä½¿ç”¨Â [slice_variables]Â å‡½æ•°è·å–æ¨¡å‹å‚æ•°ã€‚

```def get_backbone_and_params(model_arch: str):```

```

è·å–ä¸»å¹²å’Œå‚æ•°

1\. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹(resnet50)

2\. è·å–æ¨¡å‹å’Œå‚æ•°ç»“æ„ï¼Œé™¤äº†æœ€å 2 å±‚ 3\. æå–å˜é‡å­—å…¸çš„ç›¸åº”å­é›† è¾“å…¥ï¼šmodel_arch

RETURNS backbone , backbone_params

```

`if model_arch == 'resnet50':

resnet_tmpl, params = pretrained_resnet(50) model = resnet_tmpl()else:raise NotImplementedError

# è·å–ä¸»å¹²æ¨¡å‹åŠå…¶å‚æ•°ç»“æ„çš„èµ·å§‹ç‚¹å’Œç»“æŸç‚¹ = 0, len(model.layers) - 2 backbone = Sequential(model.layers[start:end]) backbone_params = slice_variables(params, start, end) return backbone, backbone_params

## è·å–æ¨¡å‹å’Œå˜é‡

ä½¿ç”¨ä¸Šè¿°å‡½æ•°åˆ›å»ºæœ€ç»ˆæ¨¡å‹ã€‚å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼š

åˆå§‹åŒ–ç½‘ç»œçš„è¾“å…¥ã€‚

è·å– ResNet ä¸»å¹²åŠå…¶å‚æ•°ã€‚å°†è¾“å…¥ä¼ é€’ç»™ä¸»å¹²å¹¶è·å–è¾“å‡ºã€‚

åˆå§‹åŒ–ç½‘ç»œçš„å¤´éƒ¨ã€‚ä½¿ç”¨ä¸»å¹²å’Œå¤´éƒ¨åˆ›å»ºæœ€ç»ˆæ¨¡å‹ã€‚

ç»“åˆæ¥è‡ªä¸»å¹²å’Œå¤´éƒ¨çš„å‚æ•°ã€‚

```def get_model_and_variables(model_arch: str, head_init_key: in t):```

```

Get model and variables

1\. åˆå§‹åŒ–è¾“å…¥(shape=(1,image_size,image_size,3))

2\. Get backbone and params

3\. Apply backbone model and get outputs

4\. Initialise head

5\. Create final model using backbone and head

6\. Combine params from backbone and head

INPUT model_arch, head_init_key RETURNS model, variables '''

#backboneinputs = jnp.ones((1, config['IMAGE_SIZE'],config['IMAGE_SI ZE'], 3), jnp.float32)backbone, backbone_params = get_backbone_and_params(model_a rch) key = jax.random.PRNGKey(head_init_key)backbone_output = backbone.apply(backbone_params, inputs, m utable=False)#headhead_inputs = jnp.ones((1, backbone_output.shape[-1]), jnp. float32)head = Head()head_params = head.init(key, head_inputs, train=False)

#final model

model = Model(backbone, head)

variables = FrozenDict({

`'params': {

`'backbone': backbone_params['params'], 'head': head_params['params']

},

`'batch_stats': {

`'backbone': backbone_params['batch_stats'], 'head': head_params['batch_stats']

}

})

return model, variables

æ‰€æœ‰ä¸éª¨å¹²ç½‘ç»œç›¸å…³çš„åç§°éƒ½ä»¥â€œbackboneâ€ä½œä¸ºå‰ç¼€ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ä»»ä½•åç§°ï¼Œä½†æ‰€æœ‰éª¨å¹²å˜é‡åç§°åº”è¯¥ç›¸åŒã€‚åœ¨å†»ç»“å±‚æ—¶ï¼Œè¿™ä¸€ç‚¹éå¸¸é‡è¦ï¼Œæ­£å¦‚æˆ‘ä»¬å°†åœ¨åé¢çœ‹åˆ°çš„é‚£æ ·ã€‚

æ¥ä¸‹æ¥ï¼Œä½¿ç”¨ä¸Šè¿°å®šä¹‰çš„å‡½æ•°åˆ›å»ºæ¨¡å‹ã€‚model, variables = get_model_and_variables('resnet50', 0) ![](img/00057.jpeg)

## Zero gradients

ç”±äºæˆ‘ä»¬æ­£åœ¨åº”ç”¨è¿ç§»å­¦ä¹ ï¼Œéœ€è¦ç¡®ä¿ä¸æ›´æ–°éª¨å¹²ç½‘ç»œã€‚å¦åˆ™ï¼Œæˆ‘ä»¬å°†ä»å¤´å¼€å§‹è®­ç»ƒç½‘ç»œã€‚æˆ‘ä»¬å¸Œæœ›åˆ©ç”¨é¢„è®­ç»ƒçš„æƒé‡ï¼Œå¹¶å°†å®ƒä»¬ç”¨ä½œç½‘ç»œçš„ç‰¹å¾æå–å™¨ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å†»ç»“æ‰€æœ‰ä»¥â€œbackboneâ€å’Œâ€œheadâ€å¼€å¤´çš„å±‚çš„å‚æ•°ã€‚

withÂ [backbone]. As a result, these parameters will not be updated during training.

"""

å‚è€ƒ - https://github.com/deepmind/optax/issues/159#issuec omment-896459491

"""

def zero_grads():

'''

Zero out the previous gradient computation

```

def init_fn(_):

è¿”å›()

def update_fn(updates, state, params=None):

return jax.tree_map(jnp.zeros_like, updates), () return optax.GradientTransformation(init_fn, update_fn)

"""

å‚è€ƒ - https://colab.research.google.com/drive/1g_pt2Rc3bv 6H6qchvGHD-BpgF-Pt4vrC#scrollTo=TqDvTL_tIQCH&line=2&uniqifier=1 """

def create_mask(params, label_fn):

def _map(params, mask, label_fn):for k in params:if label_fn(k):

mask[k] = 'zero'

else:

if isinstance(params[k], FrozenDict): mask[k] = {}

_map(params[k], mask[k], label_fn)

else:

mask[k] = 'adam'

mask = {}

_map(params, mask, label_fn)

return frozen_dict.freeze(mask)

## å®šä¹‰ Flax ä¼˜åŒ–å™¨

åˆ›å»ºä¸€ä¸ªä»…åº”ç”¨äºå¤´éƒ¨è€Œä¸æ˜¯éª¨å¹²å±‚çš„ä¼˜åŒ–å™¨ã€‚è¿™æ˜¯é€šè¿‡ optax.multi_transform å®ç°çš„ï¼ŒåŒæ—¶ä¼ é€’æ‰€éœ€çš„å˜æ¢ã€‚

adamw = optax.adamw(

learning_rate=config['LR'],

b1=0.9, b2=0.999,

eps=1e-6, weight_decay=1e-2

)

optimizer = optax.multi_transform(

{'adam': adamw, 'zero': zero_grads()},

create_mask(variables['params'], lambda s: s.startswith('ba ckbone'))

)

## å®šä¹‰ Flax æŸå¤±å‡½æ•°

ä¸‹ä¸€æ­¥ï¼Œå®šä¹‰è®¡ç®—æŸå¤±å‡½æ•°çš„å‡½æ•°ã€‚

def cross_entropy_loss(*, logits, labels):

labels_onehot = jax.nn.one_hot(labels, num_classes=config["NU

M_LABELS"])

return optax.softmax_cross_entropy(logits=logits, labels=labe

ls_onehot).mean()

`åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è®¡ç®—æŸå¤±æ—¶ï¼Œå°†[train]è®¾ç½®ä¸º[True]ã€‚æ‚¨è¿˜éœ€è¦ï¼š`

è®¾ç½®[batch_stats]å®šä¹‰[dropout]å±‚çš„éšæœºæ•°ã€‚å°†[batch_stats]è®¾ç½®ä¸ºå¯å˜ã€‚

def compute_loss(params, batch_stats, images, labels): logits,batch_stats = model.apply({'params': params,'batch_s

tats': batch_stats},images, train=True,rngs={'dropout': jax.ran

dom.PRNGKey(0)}, mutable=['batch_stats'])

loss = cross_entropy_loss(logits=logits, labels=labels) return loss, (logits, batch_stats)

## å®šä¹‰ Flax æŒ‡æ ‡

`ä½¿ç”¨æŸå¤±å‡½æ•°ï¼Œå®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œåœ¨è®­ç»ƒæœŸé—´è¿”å›æŸå¤±å’Œå‡†ç¡®åº¦ã€‚`

`def compute_metrics(*, logits, labels):`

`loss = cross_entropy_loss(logits=logits, labels=labels) accuracy = jnp.mean(jnp.argmax(logits, -1) == labels) metrics = {`

`'loss': loss,`

`'accuracy': accuracy,`

`}`

`return metrics`

## **åˆ›å»º Flax è®­ç»ƒçŠ¶æ€**

`Flax æä¾›ä¸€ä¸ªè®­ç»ƒçŠ¶æ€æ¥å­˜å‚¨è®­ç»ƒä¿¡æ¯ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æ·»åŠ [batch_stats]ä¿¡æ¯ã€‚`

`state = TrainState.create(`

`apply_fn = model.apply,`

`params = variables['params'],`

`tx = optimizer,`

`batch_stats = variables['batch_stats'],`

`)`

## **è®­ç»ƒæ­¥éª¤**

`è®­ç»ƒæ­¥éª¤æ¥æ”¶å›¾åƒå’Œæ ‡ç­¾ï¼Œå¹¶è®¡ç®—ç›¸å¯¹äºæ¨¡å‹å‚æ•°çš„æ¢¯åº¦ã€‚ç„¶åè¿”å›æ–°çš„çŠ¶æ€å’Œæ¨¡å‹æŒ‡æ ‡ã€‚`

`@jax.jit`

`def train_step(state: TrainState,images, labels):`

"""è®­ç»ƒå•æ­¥ã€‚"""

`(batch_loss, (logits, batch_stats)), grads= jax.value_and_gra d(compute_loss, has_aux=True)(state.params,state.batch_stats, i mages,labels)`

`state = state.apply_gradients(grads=grads)`

`metrics = compute_metrics(logits=logits, labels=labels)`

`return state, metrics`

`è¦è®­ç»ƒç½‘ç»œä¸€ä¸ª epochï¼Œå¾ªç¯éå†è®­ç»ƒæ•°æ®å¹¶åº”ç”¨è®­ç»ƒæ­¥éª¤ã€‚`

`def train_one_epoch(state, dataloader):`

"""åœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒ 1 ä¸ª epochã€‚""" batch_metrics = []

`for cnt, (images, labels) in enumerate(dataloader):`

`images = images / 255.0`

`state, metrics = train_step(state, images, labels) batch_metrics.append(metrics)`

`batch_metrics_np = jax.device_get(batch_metrics) epoch_metrics_np = {k: np.mean([metrics[k] for metrics in batch_metrics_n`

`p])`

`for k in batch_metrics_np[0] }`

`return state, epoch_metrics_np`

## **è¯„ä¼°æ­¥éª¤**

`æ¨¡å‹è¯„ä¼°æ­¥éª¤æ¥å—æµ‹è¯•æ ‡ç­¾å’Œå›¾åƒï¼Œå¹¶å°†å…¶åº”ç”¨äºç½‘ç»œã€‚ç„¶åè¿”å›æ¨¡å‹è¯„ä¼°æŒ‡æ ‡ã€‚åœ¨è¯„ä¼°è¿‡ç¨‹ä¸­ï¼Œå°†[train]å‚æ•°è®¾ä¸º[False]ã€‚è¿˜éœ€å®šä¹‰[batch_stats]å’Œ[dropout]å±‚çš„éšæœºæ•°ã€‚`

`@jax.jitdef eval_step(batch_stats, params, images, labels): logits = model.apply({'params': params,'batch_stats': batch _stats}, images, train=False,rngs={'dropout': jax.random.PRNGKe y(0)})return compute_metrics(logits=logits, labels=labels)`

`def evaluate_model(state, test_imgs, test_lbls):`

"""åœ¨éªŒè¯é›†ä¸Šè¿›è¡Œè¯„ä¼°ã€‚"""

`metrics = eval_step(state.batch_stats,state.params, test_im`

`gs, test_lbls)`

`metrics = jax.device_get(metrics)`

`metrics = jax.tree_map(lambda x: x.item(), metrics) return metrics`

## **åœ¨ Flax ä¸­è®­ç»ƒ ResNet æ¨¡å‹**

é€šè¿‡åº”ç”¨`[train_one_epoch]`å‡½æ•°æ¥è®­ç»ƒ ResNet æ¨¡å‹ã€‚ç”±äºæˆ‘ä»¬åœ¨å¾®è°ƒç½‘ç»œï¼Œæ‰€ä»¥åªéœ€è¦å‡ ä¸ª epochsã€‚

## **åœ¨ Flax ä¸­è®¾ç½® TensorBoard**

`è¦é€šè¿‡ TensorBoard ç›‘æ§æ¨¡å‹è®­ç»ƒï¼Œå¯ä»¥å°†è®­ç»ƒå’ŒéªŒè¯æŒ‡æ ‡å†™å…¥ TensorBoardã€‚`

`from torch.utils.tensorboard import SummaryWriter import torchvision.transforms.functional as F logdir = "flax_logs"`

`writer = SummaryWriter(logdir)`

## è®­ç»ƒæ¨¡å‹

`å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹ï¼Œå¹¶å°†æŒ‡æ ‡å†™å…¥ TensorBoardã€‚(test_images, test_labels) = next(iter(validation_loader)) test_images = test_images / 255.0`

`training_loss = [] training_accuracy = [] testing_loss = []`

`testing_accuracy = []`

`def train_model(epochs):for epoch in range(1, epochs + 1):train_state, train_metrics = train_one_epoch(state, tra`

`in_loader)`

`training_loss.append(train_metrics['loss']) training_accuracy.append(train_metrics['accuracy'])`

`test_metrics = evaluate_model(train_state, test_images, test_labels)`

`testing_loss.append(test_metrics['loss'])`

`testing_accuracy.append(test_metrics['accuracy'])`

`writer.add_scalar('Loss/train', train_metrics['loss'], epoch)`

`writer.add_scalar('Loss/test', test_metrics['loss'], epoch)`

`writer.add_scalar('Accuracy/train', train_metrics['accu racy'], epoch)`

`writer.add_scalar('Accuracy/test', test_metrics['accura cy'], epoch)`

`print(f"Epoch: {epoch}, training loss: {train_metrics ['loss']}, training accuracy: {train_metrics['accuracy'] * 10 0}, validation loss: {test_metrics['loss']}, validation accurac y: {test_metrics['accuracy'] * 100}")`

`return train_stateRun the training function. trained_model_state = train_model(config["N_EPOCHS"])`

## ä¿å­˜ Flax æ¨¡å‹

ä½¿ç”¨`[save_checkpoint]`ä¿å­˜è®­ç»ƒåçš„ Flax æ¨¡å‹ã€‚

`from flax.training import checkpoints`

`ckpt_dir = 'model_checkpoint/'`

`checkpoints.save_checkpoint(ckpt_dir=ckpt_dir,`

`target=trained_model_state, step=100,`

`prefix='resnet_model', overwrite=True`

`)`

## åŠ è½½ä¿å­˜çš„ Flax æ¨¡å‹

ä½¿ç”¨`[restore_checkpoint]`æ–¹æ³•åŠ è½½ä¿å­˜çš„ Flax æ¨¡å‹ã€‚

`loaded_model = checkpoints.restore_checkpoint(

`ckpt_dir=ckpt_dir, target=state, prefix='resnet_mod`

`el') loaded_model`

## è¯„ä¼° Flax ResNet æ¨¡å‹

`è¦è¯„ä¼° Flax æ¨¡å‹ï¼Œè¯·å°†æµ‹è¯•å’Œè®­ç»ƒæ•°æ®ä¼ é€’ç»™`

`theÂ evalaute_modelÂ function.evaluate_model(loaded_model,test_images, test_labels) ![](img/00058.gif)`

## å¯è§†åŒ–æ¨¡å‹æ€§èƒ½

ä½ å¯ä»¥é€šè¿‡ TensorBoard æ£€æŸ¥ç½‘ç»œçš„æ€§èƒ½ï¼Œæˆ–è€…ä½¿ç”¨ Matplotlib ç»˜åˆ¶æŒ‡æ ‡ã€‚

## Final thoughts

å¯ä»¥åº”ç”¨è¿ç§»å­¦ä¹ æ¥åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶ä¸”åªéœ€ä»˜å‡ºæœ€å°çš„åŠªåŠ›å°±èƒ½è·å¾—ç»“æœã€‚ä½ å·²ç»å­¦ä¼šäº†å¦‚ä½•åœ¨ Flax ä¸­è®­ç»ƒ ResNet æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œä½ å·²ç»è¦†ç›–äº†ï¼š

`How to define the ResNet model in Flax.`

å¦‚ä½•å†»ç»“ ResNet ç½‘ç»œçš„å±‚ã€‚

`Training a ResNet model on custom data in Flax.`

`Saving and loading a ResNet model in Flax.`

## Elegy(High-level API for deep learning in JAX & Flax)

`Training deep learning networks in Flax is done in a couple of steps. It involves creating the following functions:`

Model definition.

`Compute metrics.`

`Training state.`

Training step.

Training and evaluation function.

`Flax and JAX give more control in defining and training deep learning networks. However, this comes with more verbosity. EnterÂ Elegy. Elegy is a high-level API for creating deep learning networks in JAX. Elegy's API is like the one in Keras.`

`Let's look at how to use Elegy to define and train deep learning networks in Flax.`

## `Data pre-processing`

`To make this illustration concrete, we'll use theÂ movie review data from KaggleÂ to create an LSTM network in Flax.`

`The first step is to download and extract the data.`

`import os`

`import kaggle`

# `Obtain from https://www.kaggle.com/username/account os.environ["KAGGLE_USERNAME"]="KAGGLE_USERNAME"`

`os.environ["KAGGLE_KEY"]="KAGGLE_KEY"`

`!kaggle datasets download lakshmi25npathi/imdb-dataset-of-50k-m ovie-reviews`

`import zipfile`

`with zipfile.ZipFile('imdb-dataset-of-50k-movie-reviews.zip',`

`'r') as zip_ref:zip_ref.extractall('imdb-dataset-of-50k-movie-reviews')Next, we define the following processing steps:`

`Split the data into a training and testing set. Remove stopwords from the data.`

æ¸…ç†æ•°æ®ï¼Œå»é™¤æ ‡ç‚¹å’Œå…¶ä»–ç‰¹æ®Šå­—ç¬¦ã€‚

`Convert the data to a TensorFlow dataset.`

`Conver the data to numerical representation using the Keras vectorization layer.`

`import numpy as np`

`import pandas as pd`

`from numpy import array`

`import tensorflow_datasets as tfds`

`import tensorflow as tf`

`from sklearn.model_selection import train_test_split from sklearn.preprocessing import LabelEncoder import matplotlib.pyplot as plt`

`from sklearn.model_selection import train_test_split import tensorflow as tf`

`df = pd.read_csv("imdb-dataset-of-50k-movie-reviews/IMDB Datase t.csv")`

`import nltk`

`from nltk.corpus import stopwords`

`nltk.download('stopwords')`

`def remove_stop_words(review):`

`review_minus_sw = []`

`stop_words = stopwords.words('english')`

`review = review.split()`

`cleaned_review = [review_minus_sw.append(word) for word in`

`review if word not in stop_words]`

`cleaned_review = ' '.join(review_minus_sw)`

`return cleaned_review`

`df['review'] = df['review'].apply(remove_stop_words) labelencoder = LabelEncoder()`

`df = df.assign(sentiment = labelencoder.fit_transform(df["senti ment"]))`

`df = df.drop_duplicates()`

`docs = df['review']`

`labels = array(df['sentiment'])`

`X_train, X_test , y_train, y_test = train_test_split(docs, labe ls , test_size = 0.20, random_state=0)`

`max_features = 10000 # æœ€å¤§è¯æ±‡é‡å¤§å°ã€‚`

`batch_size = 128`

`max_len = 50 # åºåˆ—é•¿åº¦ï¼Œç”¨äºå¡«å……è¾“å‡ºã€‚vectorize_layer = tf.keras.layers.TextVectorization(standardize ='lower_and_strip_punctuation',max_tokens=max_features,output_m ode='int',output_sequence_length=max_len)`

`vectorize_layer.adapt(X_train)`

`X_train_padded = vectorize_layer(X_train)`

`X_test_padded = vectorize_layer(X_test)`

`training_data = tf.data.Dataset.from_tensor_slices((X_train_pad ded, y_train))`

`validation_data = tf.data.Dataset.from_tensor_slices((X_test_pa dded, y_test))`

`training_data = training_data.batch(batch_size)`

`validation_data = validation_data.batch(batch_size) def get_train_batches():`

`ds = training_data.prefetch(1)`

`ds = ds.repeat(3)`

`ds = ds.shuffle(3, reshuffle_each_iteration=True)` # `tfds.dataset_as_numpy` converts the `tf.data.Dataset` into an

`iterable of NumPy arraysreturn tfds.as_numpy(ds)`

## **Elegy æ¨¡å‹å®šä¹‰**

é¦–å…ˆå®‰è£…`Elegy`ï¼Œ`Flax`å’Œ`JAX`ã€‚`pip install -U elegy flax jax jaxlib`æ¥ä¸‹æ¥ï¼Œå®šä¹‰ LSTM æ¨¡å‹ã€‚

`import jax`

`import jax.numpy as jnp import elegy as eg`

`from flax import linen as nn`

`class LSTMModel(nn.Module):`

`def setup(self):`

`self.embedding = nn.Embed(max_features, max_len)` `lstm_layer = nn.scan(nn.OptimizedLSTMCell, variable_broadcast="params", split_rngs={"params": False}, in_axes=1,`

`out_axes=1,`

`length=max_len,`

`reverse=False)`

`self.lstm1 = lstm_layer()`

`self.dense1 = nn.Dense(256)`

`self.lstm2 = lstm_layer()`

`self.dense2 = nn.Dense(128)`

`self.lstm3 = lstm_layer()`

`self.dense3 = nn.Dense(64)`

`self.dense4 = nn.Dense(2)`

`@nn.rematdef __call__(self, x_batch): x = self.embedding(x_batch)`

`carry, hidden = nn.OptimizedLSTMCell.initialize_carry(j ax.random.PRNGKey(0), batch_dims=(len(x_batch),), size=128)`

`(carry, hidden)`, `x = self.lstm1((carry, hidden), x)`

`x = self.dense1(x) x = nn.relu(x)`

`carry, hidden = nn.OptimizedLSTMCell.initialize_carry(j ax.random.PRNGKey(0), batch_dims=(len(x_batch),), size=64)`

`(carry, hidden), x = self.lstm2((carry, hidden), x)`

`x = self.dense2(x) x = nn.relu(x)`

`carry, hidden = nn.OptimizedLSTMCell.initialize_carry(j ax.random.PRNGKey(0), batch_dims=(len(x_batch),), size=32)`

`(carry, hidden), x = self.lstm3((carry, hidden), x)`

`x = self.dense3(x)`

`x = nn.relu(x)`

`x = self.dense4(x[:, -1]) return nn.log_softmax(x)`

ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨ä¸Šè¿°ç½‘ç»œåˆ›å»ºä¸€ä¸ª Elegy æ¨¡å‹ã€‚æ­£å¦‚ä½ æ‰€è§ï¼ŒæŸå¤±å’ŒæŒ‡æ ‡çš„å®šä¹‰ç±»ä¼¼äº`Keras`ã€‚æ¨¡å‹çš„ç¼–è¯‘åœ¨æ„é€ å‡½æ•°ä¸­å®Œæˆï¼Œå› æ­¤æ‚¨æ— éœ€æ‰‹åŠ¨æ‰§è¡Œæ­¤æ“ä½œã€‚

`import optax`

`model = eg.Model(`

`module=LSTMModel(),`

`loss=[`

`eg.losses.Crossentropy()`, `eg.regularizers.L2(l=1e-4)`, ],

`metrics=eg.metrics.Accuracy(),`

`optimizer=optax.adam(1e-3), )`

## **Elegy æ¨¡å‹æ‘˜è¦**

åƒåœ¨ Keras ä¸­ä¸€æ ·ï¼Œæˆ‘ä»¬å¯ä»¥æ‰“å°æ¨¡å‹çš„æ‘˜è¦ã€‚`model.summary(jnp.array(X_train_padded[:64]))` ![](img/00059.jpeg)

## **Elegy ä¸­çš„åˆ†å¸ƒå¼è®­ç»ƒ**

è¦åœ¨ Flax ä¸­ä»¥åˆ†å¸ƒå¼æ–¹å¼è®­ç»ƒæ¨¡å‹ï¼Œæˆ‘ä»¬å®šä¹‰æ¨¡å‹è®­ç»ƒå‡½æ•°çš„å¹¶è¡Œç‰ˆæœ¬ã€‚

ä½†æ˜¯ï¼Œåœ¨ Elegy ä¸­ï¼Œæˆ‘ä»¬è°ƒç”¨ `[distributed]` æ–¹æ³•æ¥è¿›è¡Œåˆ†å¸ƒå¼æ–¹æ³•ã€‚`model = model.distributed()`

## **Flax ä¸­ç±»ä¼¼ Keras çš„å›è°ƒ**

Elegy æ”¯æŒç±»ä¼¼ Keras å›è°ƒçš„å›è°ƒå‡½æ•°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹å›è°ƒå‡½æ•°è®­ç»ƒæ¨¡å‹ï¼š

`TensorBoard.`

`æ¨¡å‹æ£€æŸ¥ç‚¹.`

`Early stopping.callbacks = [ eg.callbacks.TensorBoard("summaries"), eg.callbacks.ModelCheckpoint("models/high-level", save_best_only=True),eg.callbacks.EarlyStopping(monitor = 'val_loss',pa tience=10)]`

## **è®­ç»ƒ Elegy æ¨¡å‹**

Elegy æä¾›äº† `[fit]` æ–¹æ³•æ¥è®­ç»ƒæ¨¡å‹ã€‚è¯¥æ–¹æ³•æ”¯æŒä»¥ä¸‹æ•°æ®æºï¼š

`Tensorflow æ•°æ®é›†.`

`Pytorch DataLoader`

`Elegy DataLoaderï¼Œå¹¶ä¸”`

`Python ç”Ÿæˆå™¨.`

`history = model.fit(`

`training_data,`

`epochs=100,`

validation_data=(validation_data), callbacks=callbacks,

)![](img/00060.gif)

## **è¯„ä¼° Elegy æ¨¡å‹**

è¦è¯„ä¼° Elegy æ¨¡å‹ï¼Œè¯·ä½¿ç”¨Â [evaluate]Â å‡½æ•°ã€‚model.evaluate(validation_data)![](img/00061.jpeg)

## **ä½¿ç”¨ TensorBoard å¯è§†åŒ– Elegy æ¨¡å‹**

ç”±äºæˆ‘ä»¬åº”ç”¨äº† TensorBoard å›è°ƒï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ TensorBoard ä¸­æŸ¥çœ‹æ¨¡å‹çš„æ€§èƒ½ã€‚%load_ext tensorboard%tensorboard --logdir summaries ![](img/00062.jpeg)

## **ä½¿ç”¨ Matplotlib ç»˜åˆ¶æ¨¡å‹æ€§èƒ½**

æˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨ Matplotlib ç»˜åˆ¶æ¨¡å‹çš„æ€§èƒ½ã€‚import matplotlib.pyplot as plt

def plot_history(history):

n_plots = len(history.history.keys()) // 2

plt.figure(figsize=(14, 24))

for i, key in enumerate(list(history.history.keys())[:n_plo

ts]):

metric = history.history[`key`]

val_metric = history.history[f`val_{key}`]

plt.subplot(n_plots, 1, i + 1)

plt.plot(metric, label=f`Training {key}`) plt.plot(val_metric, label=f`Validation {key}`) plt.legend(loc=`lower right`)

plt.ylabel(`key`)

plt.title(`Training and Validation {key}`)

plt.show()plot_history(history) ![](img/00063.jpeg)

## **ä½¿ç”¨ Elegy æ¨¡å‹è¿›è¡Œé¢„æµ‹**

åƒ Keras ä¸€æ ·ï¼ŒElegy æä¾›äº†è¿›è¡Œé¢„æµ‹çš„Â [predict]Â æ–¹æ³•ã€‚(text, test_labels) = next(iter(validation_data)) y_pred = model.predict(jnp.array(text)) ![](img/00064.jpeg)

## **ä¿å­˜å’ŒåŠ è½½ Elegy æ¨¡å‹**

Elegy æ¨¡å‹ä¹Ÿå¯ä»¥åƒ Keras æ¨¡å‹ä¸€æ ·ä¿å­˜ï¼Œå¹¶ç«‹å³ç”¨äºé¢„æµ‹ã€‚

# æ‚¨å¯ä»¥ä½¿ç”¨ `save` ä½†æ˜¯ `ModelCheckpoint` å·²ç»åºåˆ—åŒ–äº†æ¨¡å‹

# model.save(`model`)

# å½“å‰æ¨¡å‹å‚è€ƒ print(`current model id:`, id(model))

# ä»ç£ç›˜åŠ è½½æ¨¡å‹

model = eg.load(`models/high-level`)

# æ–°æ¨¡å‹å‚è€ƒ

print(`new model id: `, id(model))

# æ£€æŸ¥å…¶æ˜¯å¦æ­£å¸¸å·¥ä½œï¼model.evaluate(validation_data) ![](img/00065.jpeg)

## **æœ€ç»ˆæƒ³æ³•**

è¿™ç¯‡æ–‡ç« ç®€è¦ä»‹ç»äº† Elegy â€”â€” ä¸€ç§æ‚¨å¯ä»¥ç”¨æ¥æ„å»ºå’Œè®­ç»ƒ Flax ç½‘ç»œçš„ JAX é«˜çº§ APIã€‚æ‚¨å·²ç»çœ‹åˆ° Elegy éå¸¸ç±»ä¼¼äº Kerasï¼Œå¹¶ä¸”å…·æœ‰ç”¨äº Flax çš„ç®€å• APIã€‚å®ƒè¿˜åŒ…å«ç±»ä¼¼äº Keras çš„åŠŸèƒ½ï¼Œå¦‚ï¼š

æ¨¡å‹è®­ç»ƒã€‚

è¿›è¡Œé¢„æµ‹ã€‚

åˆ›å»ºå›è°ƒã€‚

å®šä¹‰æ¨¡å‹æŸå¤±å’ŒæŒ‡æ ‡ã€‚

## **é™„å½•**

æœ¬ä¹¦çš„æä¾›ç¬¦åˆæˆ‘ä»¬çš„æ¡æ¬¾å’Œéšç§æ”¿ç­–ã€‚

## **å…è´£å£°æ˜**

æœ¬ç”µå­ä¹¦ä¸­çš„ä¿¡æ¯ä¸é€‚ç”¨äºç›´æ¥åœ¨ç”Ÿäº§ä¸­åº”ç”¨

ç¯å¢ƒã€‚å°†å…¶åº”ç”¨äºç”Ÿäº§ç¯å¢ƒæ—¶ï¼Œæ‚¨éœ€æ‰¿æ‹…å…¨éƒ¨è´£ä»»

ç”±äºæ‚¨çš„è¡Œä¸ºã€‚

ä½œè€…å·²å°½ä¸€åˆ‡åŠªåŠ›ç¡®ä¿ä¿¡æ¯çš„å‡†ç¡®æ€§

æœ¬ä¹¦å‡ºç‰ˆæ—¶çš„ä¿¡æ¯æ˜¯æ­£ç¡®çš„ã€‚ä½œè€…ä¸æ‰¿æ‹…ä»»ä½•è´£ä»»ï¼Œå¹¶ä¸”

ç‰¹æ­¤å£°æ˜å¯¹ä»»ä½•å› æŸå¤±ã€æŸåæˆ–é€ æˆä¸­æ–­è€Œå¯¼è‡´çš„è´£ä»»ä¸æ‰¿æ‹…ã€‚

ç”±äºæ„å¤–ã€

ç–å¿½æˆ–å…¶ä»–åŸå› ã€‚

æœªç»ä»»ä½•å½¢å¼æˆ–ä»»ä½•

æ‰‹æ®µï¼Œç”µå­æˆ–æœºæ¢°ï¼Œå½•åˆ¶æˆ–é€šè¿‡ä»»ä½•ä¿¡æ¯å­˜å‚¨å’Œ

æ£€ç´¢ç³»ç»Ÿï¼Œæœªç»ä½œè€…ä¹¦é¢è®¸å¯ã€‚

## **ç‰ˆæƒ**

JAX å’Œ Flax ä¹¦ç±â€”â€” ä½¿ç”¨ Flax å’Œ JAX è¿›è¡Œæ·±åº¦å­¦ä¹  Â© ç‰ˆæƒ Derrick Mwitiã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚

## **å…¶ä»–å­¦ä¹ å†…å®¹**

å­¦ä¹  Python

å­¦ä¹ æ•°æ®ç§‘å­¦

å­¦ä¹  Streamlit
