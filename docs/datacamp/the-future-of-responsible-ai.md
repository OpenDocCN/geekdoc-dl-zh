# 负责任人工智能的未来

> 原文：<https://web.archive.org/web/20221129053839/https://www.datacamp.com/blog/the-future-of-responsible-ai>

[![](img/55c88bd391168ac447fd6d28575c75a6.png)](https://web.archive.org/web/20220627140223/https://www.datacamp.com/community/podcast/the-future-of-responsible-ai)

[https://web.archive.org/web/20220627140223if_/https://embed.podcasts.apple.com/us/podcast/68-the-future-of-responsible-ai/id1336150688?i=1000531441805](https://web.archive.org/web/20220627140223if_/https://embed.podcasts.apple.com/us/podcast/68-the-future-of-responsible-ai/id1336150688?i=1000531441805)

DataCamp 播客 DataFramed 的主持人 Adel Nehme 最近采访了普华永道英国的负责人和 AI for Good Lead 的玛利亚露丝安娜阿申特。

## [介绍玛利亚·露丝安娜·阿申特](https://web.archive.org/web/20220627140223/https://www.datacamp.com/community/podcast/the-future-of-responsible-ai)

Adel Nehme:你好。我是来自 DataCamp 的 Adel Nehme，欢迎来到 DataFramed，这是一个涵盖所有数据及其对世界各地组织的影响的播客。我想我们都同意，当公众听到人工智能这个术语时，他们首先想到的是人工智能的伦理及其对人类的潜在影响。然而，很多时候，与人工智能相关的风险概念是由流行媒体和流行文化中的概念以及人工智能在公共场合的高调失败所驱动的。

Adel Nehme:这些失败促使许多组织重新考虑他们在生产中推出的模型，并分析人工智能的风险，以便负责任地部署它们。这就是为什么我很高兴今天的播客能邀请到玛丽亚·露丝安娜·阿申特。Maria 负责英国普华永道的人工智能和人工智能。在她的职位上，Maria 领导公司在人工智能中实施道德规范，同时与行业、学术界、政府、非政府组织和民间社会合作，以道德和负责任的方式利用人工智能的力量，同时承认其在许多生活领域的好处和风险。她在普华永道英国人工智能卓越中心(该公司的人工智能战略)的开发和设立中发挥了至关重要的作用，最近，她还开发了普华永道负责任的人工智能工具包(该公司将道德嵌入人工智能的方法)。

Adel Nehme: Maria 是全球公认的人工智能伦理专家，英国各党派议会人工智能小组的顾问委员会成员，BSI/ISO 和 IEEE 人工智能标准小组的成员，RSA 的研究员，人工智能时代性别多样性、儿童和青年权利的倡导者。在这一集里，Maria 和我将谈论她的背景，负责任的人工智能与人工智能道德的交叉和分歧，负责任的人工智能在当今组织中的状态，负责任的人工智能如何与组织文化和价值观相联系，最重要的是，数据科学家今天可以做什么来确保他们的工作被组织道德和负责任地使用，以及自下而上的行动主义如何推动组织朝着正确的方向发展。

Adel Nehme:如果你喜欢今天和 Maria 的对话，并且想看看之前的播客和节目笔记，一定要去 www.datacamp.com/community/podcast。玛丽亚，很高兴你能上节目。我真的很高兴能与你们谈论负责任的人工智能、人工智能治理和问责制的状况，以及组织如何开始负责任的人工智能之旅。但在此之前，能否简单介绍一下你的背景，以及你是如何进入数据和伦理领域的？

玛丽亚·露丝安娜·阿申特:当然。大家好，谢谢阿黛尔邀请我。很高兴与你交谈，让我们开始探索什么是嗡嗡声，人工智能的伦理有什么大惊小怪的？你为什么要谈论它，我们能做些什么？所以，简单介绍一下我的背景。所以，我在英国普华永道工作，这可能你们大多数人都知道。这是一家专业服务公司，问题可能是，普华永道在人工智能领域做什么？希望在我们今天的谈话中，你会越来越清楚为什么像普华永道这样的公司需要在塑造有道德和负责任的人工智能的故事中发挥作用。所以，我七年前加入了这家公司，我的背景是数字化转型方面的业务，所以我去建立业务，转型业务，然后进入技术和数字领域，在技术的帮助下进行这种转型。

玛丽亚·露丝安娜·阿申特:在某个时候，我有机会将注意力从广泛的新兴技术转移到更专业的领域，即人工智能。这是天作之合，因为当我们开始用其他镜头探索普华永道，然后是技术时，我们意识到了解这种新技术被开发和使用的整个背景是多么重要，以及了解设计中需要考虑的因素是多么重要，远远超出了经验设计的传统界限，并回到业务或背景将如何因使用一种工具而改变，这种工具有自己的机构或现在与所有以前的技术非常不同。

玛丽亚·露丝安娜·阿申特:所以，在过去的四年里，我一直是人工智能卓越中心的一员。这是一个迷人的旅程，因为我从一开始就在那里，所以我帮助建立它，把战略放在一起。这很像是我们开发的一个新项目。渐渐地，基于我以前的经验和我所受的教育，我开始探索道德层面，以及我们需要考虑的关键道德因素是什么，从商业角度来看这意味着什么？如果我们对人工智能的美好生活有这样的愿景，那么我们如何实现它呢？什么需要到位？需要改变什么？这就是为什么我们提出了负责任的人工智能这一概念，它使我们能够用人工智能创造美好生活的愿景，但也创造实现这一愿景所需的东西，并且非常实用，因为归根结底，我们是一家盈利性公司，需要证明我们所创造的东西的附加值。所以，我们不能只看到新兴技术。我们需要能够以一种可持续的方式来实现它。

### 定义负责任的人工智能

Adel Nehme:太棒了，我很高兴能与您一起揭开所有这些。所以，我想首先问你如何定义负责任的人工智能。在过去的十年里，许多关于人工智能风险的讨论都属于人工智能伦理的范畴，在过去的几年里，我们已经看到了负责任的人工智能的逐渐兴起。我很想知道你能否具体定义负责任的人工智能，它与人工智能伦理如何交叉和不交叉。

玛丽亚·露丝安娜·阿申特:我认为这是一个很好的问题。我想我非常感谢你问了这个问题，因为我认为我们需要开始构建这些概念，并理解它们是如何重叠的。如果有重叠，它们之间的关系是什么，这样我们就清楚了？我们总是知道，如果你没有很好地构建一个新概念，那么你将很难实现它。你将努力让它离开地面。所以，我想说，对我们和我自己来说，这是一种负责任的领导。我在普华永道英国工作，所以我们以下面的方式定义这两个术语。可以说，人工智能伦理学是一门正在发展中的新的应用伦理学学科。因此，我们可能会说，它是数字伦理或数据伦理的一个分支，或者是一般的信息哲学，但它绝对是应用伦理的一个新分支，涉及研究我们称为人工智能的技术的道德含义。

玛丽亚·露丝安娜·阿申特:这些技术最终有一些关键的独特特征，使它们与我们以前见过的所有技术完全不同，主要是他们展示或拥有，他们拥有代理权。一方面，这意味着它们将与外部环境互动。他们会适应这个外部环境，在一定程度上，塑造这个外部环境。并且它们有一定程度的自主性，不受人类的监督。这些类型的新资产要求在规范性问题上有新的思维方式。我们使用那些机器是对还是错？我们应该如何对待这些类型的代理人？所以，这就是人工智能的伦理纪律，它有点抽象，也有点前瞻性，因为我们正在步入我们以前没有去过的领域。在人类历史上，我们还没有像这样的资产与我们并肩作战，并推动是非对错的界限。

玛丽亚·露丝安娜·阿申特:我们在小说中想过，但在现实生活中没有。所以，我们必须能够推理，讨论和辩论，那些最小的影响是什么？而在这个过程中，人工智能的伦理将允许我们与人工智能一起规划美好的生活。那么，这意味着什么呢？如果我们有这种既有好处又有风险的强大工具，我们如何确保他们使用它的方式与一些人类目标一致？我们不仅仅是为了做而创造人工智能，或者因为我们有某种复杂的记录，我们让它符合人类的目的。我们作为一个人类所拥有的那些目标是非常有趣的，因为我们还没有走到一起说，或者也许我们已经说了。我一会儿就会谈到这一点。

玛丽亚·露丝安娜·阿申特:但准确地说，我们希望通过这种方式帮助一切蓬勃发展。事实上，我们有。这是人权。但是人工智能伦理学是一门学科，它不仅允许我们理解道德含义，还允许我们说，“这是我们前进的方向。这是可以接受的。”可接受的是不仅仅是为了做而做。是因为我们想实现一个与人类相关的目标。与此相关的是，我们如何实现愿景？对吗？因为每一个愿景都必须有某种操作方式或运作模式，才能让我们达到目标。因此，我们需要有另一个学科体系来收集我们实现这一愿景所需的一切。这就是负责任的人工智能被放置的地方，因为负责任的人工智能更具战术性。实际上，现在我们已经了解了什么是道德上允许的，什么是风险领域，哪里可能出错，我们可以将这一愿景转化为战术计划。

玛丽亚·露丝安娜·阿申特:我们有一套方法、工具和思维方式，这些方法、工具和思维方式本质上是多学科的，本质上是整体的，因为我们意识到人工智能带来的破坏将改变我们，因此，我们需要更加整体地处理这个问题，负责任的人工智能是让我们实现这一愿景的引擎，如果你想的话，我们有不同的方法。它有风险角度，这意味着，不仅要了解人工智能带来的风险，人工智能带来的众多风险，还要了解每个组织以及社会和个人层面的当前风险将因人工智能而增加。然后你就有了新的运营模式。你是如何治理的？你如何控制一个以不同方式运行的自我学习的人工制品，它在本质上是随机的？

玛丽亚·露丝安娜·阿申特:因此，我们需要升级我们的线性流程，使之更加实时和动态。此外，如果我们同意我们有道德愿景，那么需要在每一个单独的用例及上下文中纳入哪些价值观，特定的应用程序将运行并整合所有这些，风险方面、治理方面，以及需要纳入的价值观。它给了你这个新的学科，需要来自广泛学科的输入，一个案例一个案例，一个例子一个例子，这些学科聚集在一起实际上说，“这是我们需要做的，以在这个特定的环境中实现一个好的人工智能的愿景。”

所以，如果我错了，请纠正我。总之，人工智能的伦理是关于我们应该如何将我们的道德价值观与人工智能系统相结合，而负责任的人工智能更多的是关于如何将这些道德价值观与其他学科一起操作化，以便我们从负责任的人工智能中创造价值。对吗？

玛丽亚·露丝安娜·阿申特:这是在这些线路上的东西，但我会说，人工智能的伦理不是框架。这是一个愿景。这是我们想去的地方。框架是带领我们到达那里的工具。

## 负责任的人工智能采用的现状

Adel Nehme:好的，太棒了。因此，你与大量数据和商业领袖合作，试图将道德实践整合到他们的人工智能开发过程中。如何看待负责任的 AI 采用现状？你认为这是大多数公司正在投资和考虑的事情吗？

玛丽亚·露丝安娜·阿申特:我认为这是一个有趣的故事，我认为我们需要稍微分开一下，公开制造的嗡嗡声，我们看到的市场营销和共同的叙事与该领域的现实。我认为，要真正理解负责任的人工智能的状态，我们有两条途径。把制造出来的噪音放在一边，因为很明显，过去几年给我们带来了很多关于人工智能的坏例子，比如人工智能被滥用，不充分使用，过度使用，并且被媒体广泛报道。因此，我们已经看到公众对人工智能的负面影响和后果给予了很多关注。但当我们看到企业内部负责任的人工智能的状态时，我们有理由感到乐观。我认为这一切都是因为这样一个事实:当涉及到像人工智能这样强大而未知的技术时，我们需要彻底改变思维模式。

玛丽亚·露丝安娜·阿申特:这不仅仅是关于这项技术将带来的一系列好处，也是一系列与之相关的潜在风险和潜在危害。我们需要消除这两种不平衡，我们需要有这样的心态，不仅因为我们有所需的东西，我们有数据，我们有处理能力，我们就能建造它，而且我们应该这样做吗？这对于“我能做到吗？”来说是一个巨大的改变这是支撑计算机科学社区的哲学。对吗？如果你开始思考，“我应该这么做吗？”突然，你发现新的尝试既有好处也有风险。当你制定一个商业计划或在某种程度上，任何使用这项技术的计划时，你会比较两者，你会一直小心谨慎地进行，因为你总是会超过收益与风险。

玛丽亚·露丝安娜·阿申特:这让我们有理由乐观，因为公众的叙述帮助了人工智能领域的高管，或者对人工智能有某种监督的高管重新考虑。对吗？所以，这是利益和风险的问题。以此为起点，许多组织已经开始积极地将它们整合在一起。正如你所说，如果是这样，如果我们还需要考虑好处，那么很明显，让我们从风险的角度出发，识别人工智能可能引发的所有潜在风险，不仅要识别它们，还要能够定义缓解策略，并以此了解我们的组织是否有能力缓解与人工智能相关的风险。我们去年一直在做的研究，我们调查了来自世界各地的大约 1000 名负责任的人工智能高管。他们告诉我们，越来越多的高管将拥有人工智能风险战略，他们将把风险视为更广泛的人工智能战略的一部分。

玛丽亚·露丝安娜·阿申特:另一方面，如果我们超越人工智能的风险并承认，最终，绝大多数人工智能风险实际上是道德风险。当涉及到人工智能的更广泛的道德观时，我们已经看到了相当多的吸收，建立了将使公司能够探索人工智能的道德后果的倡议。因此，能够有一个更长远的观点，而不仅仅是被动的，这与风险有关，并说，“我们将寻求制定内部政策，允许我们从我们公司的角度探索，在某些情况下使用人工智能和各种技术时，我们应该朝着什么方向前进，谁需要参与？决策过程是怎样的？什么是我们最终应该嵌入的价值观，编码到将导致这些技术的构建、部署和使用的过程中，并确保我们有办法控制这一切？”

玛丽亚·露丝安娜·阿申特:现在听起来很宏大，听起来需要做很多事情。我们已经看到很多公司有行为准则和原则，相当多的公司。我们已经看到公司对建立道德委员会感兴趣，这将允许他们探索和辩论，并在道德决策中有一个透明和建设性的过程，但也开发影响评估和其他类型的工具，这将允许他们权衡，开发人工智能的后果是什么？

玛丽亚·露丝安娜·阿申特:我们有足够的证据表明事情正朝着正确的方向发展。需要做的还有很多，但至少我们收到了明确的信号，即当有人提到人工智能这个词时，脑海中出现的下一个想法是，我们需要思考道德、风险识别和问责制。我们需要确保所有这些都包含在我们面前的任何计划中，这样，我们不仅可以获得这项技术承诺的所有成果和所有好处，而且我们还可以保持控制，并了解哪里可能出错以及如何最好地处理它。

阿黛尔·内姆:这非常令人鼓舞。您在这里提到，存在对道德风险做出反应的被动型组织和更主动的组织。您认为将负责任的人工智能视为一种重要的价值水平而非他人的组织之间的主要区别是什么？

玛丽亚·露丝安娜·阿申特:我认为有两类公司。第一，这是我们看到人工智能采用率相当高的地方。我先说一下科技公司，因为它们完全处于不同的阶段，我认为，挑战在于，我们拥有的大型科技公司与它们生产和使用的技术的道德规范关系不大。这更多的是与企业和商业道德有关。而说到非科技公司，其他人就不说了。我认为，从我们从客户那里看到的情况来看，这些影响在很大程度上与人工智能采用的成熟度有关。在理解人工智能和大规模部署人工智能方面更加成熟，引发了这些考虑，因为当你开始看到人工智能在你的业务中如何运作时，他们更有可能经历了一些负面影响，特别是偏见和歧视，这使他们在对待人工智能的方式上更加谨慎。

玛丽亚·露丝安娜·阿申特:但是这也和他们经营的行业有很大关系。例如，当我们审视不同行业的各种道德原则时，我们发现，不出我们所料，但事实上，可靠性、稳健性和安全性是所有公司最关心的问题。因此，它的优先次序，确保解决方案是强大和稳定的。

玛丽亚·露丝安娜·阿申特:但是当我们看其他人的优先事项时，他们因行业而异。对吗？值得一提的是，在可靠性、稳健性和安全性的同一水平上，我们的数据隐私一直是每个家庭的首要道德问题，因为在世界上的一些地方，这是一项强制性要求，因此，在所有数据驱动的技术中嵌入隐私，这是合规流程的一部分。但对于各种行业来说，例如科技、媒体和电信，人事代理是最受关注的。在公共服务和卫生领域，有益的人工智能是最受关注的问题。对吗？能源，问责制是这个领域的高管优先考虑的问题。

玛丽亚·露丝安娜·阿申特:所以，一方面，除了采用的成熟度，另一方面是行业如何形成，他们将部署什么样的应用程序，以及这些应用程序将如何增强运营。他们会更贴近顾客吗？会不会触及个人资料？或者它会是更后台类型的人工智能应用吗？当你提出这种不平衡时，你会看到成熟的公司，他们会，不只是开始思考，他们已经有了计划。开始这一旅程的人将考虑这些不同的影响，但他们可能会在采用方面放缓，因为这将与人工智能的采用速度密切相关。

## 如何在组织内实施负责任的人工智能

Adel Nehme:你在这里提到了健壮性、安全性和可靠性，我想接下来讨论如何在组织内实施负责任的人工智能。我在负责任的人工智能上看到的最好的资源之一是你的团队在负责任的人工智能上开发的一个框架，名为:负责任的人工智能工具包。你能给我们介绍一下这个框架和其中的不同组件吗？

玛丽亚·露丝安娜·阿申特:是的。谢谢欣赏。我认为我们对我们所做的工作感到非常自豪，因为我们在其中投入了大量的热情。所以，当我们三年前创建这个工具时，当我们两年前推出它时，我们是一群来自不同地区的人，他们基于我们以前的经验、客户工作和内部经验来创建一个工具，这个工具既灵活又具有前瞻性，而且本质上是整体的，因为我们知道人工智能具有颠覆性的潜力，负责任的人工智能需要能够在一个保护伞下汇集各种方法，允许这种灵活而整体的方法。所以，我们最终创造了一套类似乐高积木的工具包，既有基于代码的，也有非基于代码的，用来解决不同的问题。

玛丽亚·露丝安娜·阿申特:所以，我们有资产可以测试可靠性、健壮性和安全性，也可以测试可解释性、公平性和歧视性，这些都是即插即用型的东西。就创建允许对算法性能进行特别测试的解决方案而言，这正是整个行业所处的位置。但与此同时，我们也有非基于代码的资产，本质上更像是咨询顾问，这将允许评估组织在理解他们所支持的价值观方面处于什么位置，当涉及到开发人工智能时，他们能够将这些价值观转化为原则并进而转化为设计要求的能力有多强，以及这些价值观与组织的背景和各个地区的法规有多同步。最后，您如何保持控制力？你如何开发一个跨越人工智能生命周期的治理模型，而不仅仅是从业务需求开始。

玛丽亚·露丝安娜·阿申特:这是模型设计。这是应用程序设计。但事实是，人工智能的生命周期始于你决定的战略忽视，人工智能将被纳入的战略优先事项是什么，你如何接近它，以及谁将参与其中。人工智能的治理汇集了各种各样的工具，允许你操作各种风格的人工智能或各种类型的人工智能，无论你是在家里构建的，还是从第三方获得的。你必须有一个虚拟的运营模式，让你能够控制局面，至少在人工智能的这种破坏或变化引发商业流程的线性变化之前，工作结构的方式，工作文化，将逐渐适应自主代理。拥有这些不同的资产使我们能够对客户说，“如果您主要关心的是识别风险，我们可以帮助您识别这些风险，创建正确的控制，而且还可以更新您的运营模式，以便您实际上有能力解决这些风险，如偏见或合作伙伴。”

玛丽亚·露丝安娜·阿申特:同样，如果公司说，“我担心他们是否在丹麦，比如说，丹麦法律要求公司有一个关于数据道德的行为准则。”并且说，“我的伦理原则应该是什么？我应该如何对齐它？我应该如何将其转化为我的内部政策？谁应该参与将这一政策付诸实施？”我们将能够解决这个问题。但同时，我们总是盯着更长远的眼光，更长远的眼光，这才是有 AI 的美好生活。虽然我们做所有这些不同的个人工作，但对负责任的人工智能采取这样一种整体方法的原因是，如果你是认真的，如果你致力于提供道德的人工智能，那么你需要采取更多的步骤，这意味着，不管你是否已经开始了解决人工智能风险的旅程，如果可能的话，你应该解决所有这些不同的因素，因为没有它，人工智能将很难实现道德的结果。

### 道德行为准则

阿黛尔·奈姆:我很乐意解开你在这里提到的东西。所以，您提到的一件事是帮助组织创建一个道德行为准则，并将这些价值观集成到他们的人工智能系统中。您能告诉我们这个过程是什么样子的，以及组织在这里可以做什么吗？您在这里提到了丹麦用例。假设所有组织都像丹麦一样，他们如何着手实施道德宪章？

玛丽亚·露丝安娜·阿申特:我认为，首先，要理解你必须回归组织的价值观。你不能凭空拿起你想应用于人工智能的伦理原则，或者在了解你作为一个群体是谁之前，与其他组织结盟。这是大多数科技公司都会遇到的问题，因为他们作为一个集体签署的价值观和需要推动所有这些组织的愿景和雄心的组织价值观之间似乎存在脱节，以及这些价值观如何实际转化为他们的运营方式，包括他们开发和使用的技术。这可能是第一步，也是最重要的一步，承认你有这些价值观，承认在人工智能的世界里，将这些价值观转化为设计需求，需要比以前更诚实的态度。

玛丽亚·露丝安娜·阿申特:如果在以前，你不会有那么多的能见度，或者证明你的价值观是否一致和嵌入的方法，现在是时候了。我一直对人们说，好的组织产生好的人工智能。坏的组织，他们会产生不同类型的 AI。我不是说不好。我说的是不太道德的组织。所以，我认为第一步真的很重要，要明白你有一套价值观，最终需要反映在你做的和说的每一件事上，而不仅仅是说，你做的每一件事。第二个层面是，如果是这样的话，人工智能引发的关键伦理道德问题是什么？这是我们花相当多的时间去理解的地方，基于我们在这个领域的许多杰出专家的研究，他们已经思考了很长时间，看着人工智能，评估和重复他们处理的各种道德含义，那些基于此的伦理原则。

玛丽亚·露丝安娜·阿申特:我们从[Asilomar Hwan]这样的团体开始，然后我们有了可能是迄今为止最大的 IEEE 倡议，因为他们花了三四年时间，聘请了该领域的 300 多名专家，以期收集这些伦理和道德问题，然后能够提炼出一些指南，这些指南很容易被经验较少的人，即工程师使用，一旦他们需要非常明确的指导和规则，如何处理这些影响以及如何翻译这些影响。像经合组织和欧洲委员会。显然，欧盟的人工智能和经合组织的可信规则，它们都是相互联系的，因为最终，它始于一群人工智能专家和一些哲学家一起头脑风暴这些影响，以及各个小组单独做这件事。

玛丽亚·露丝安娜·阿申特:然后越来越多的其他团体将重复并进一步加强这一思想，并给了我们近 200 个不同的文件，155 个不同的原则，当我们把它们放在一起时，我们汇总成九个元伦理原则。当然，它们是数据隐私、健壮性和安全性、透明性/可解释性、有益的人工智能、问责制、安全性、合法性和合规性以及人类机构。但这就是我们在普华永道的方式，我们把所有的原则组合在一起说，如果你看看所有那些由所有那些团体起草的 155 个不同的原则，无论是盈利性的，非盈利性的，跨国的还是超国家的组织，它们都有很多共同点。这只是你如何表达一些问题的问题。所以，当我们合计，我们发现这九个，但如果你看看经合组织是怎么做的，或者欧洲委员会是怎么做的，这是非常相似的。对吗？

玛丽亚·露丝安娜·阿申特:能够把所有的都汇总起来说，“这些是我们现在所有的道德考虑，虽然还会有其他的，我们需要考虑更长远的，现在发生的事情，它需要把这些规则纳入指导设计。”对吗？第二步是说，看看所有这些不同的原则，挑选出与一个人的组织更相关的原则，追溯到价值观，但也非常重要的是，展示那些将被转化为规范和设计要求的道德原则是如何与人权保持一致的，因为最终人权是世界上几乎所有 190 个国家都签署的价值体系，它实际上是一项法律，具有约束力。因此，我们需要证明一项具体的原则是如何与各种人权条款联系在一起的，以及各种应用是如何实现这些原则或者有违反这些原则的危险。

玛丽亚·露丝安娜·阿申特:我不会在这个方向上花太多时间，但我想说的是，实施过程的第三步是，确保当你制定这个章程时，你与组织中的每个人都进行了磋商。对吗？对于一群人或某个组织中的某个负责人来说，“好吧，我实际上选择了负责人，我起草了他们，然后你就可以走了”是不够的那不会持续很久。你需要经历这个过程，这是每个人实际上所处的位置，不是每个人，但很多公司都在跨越式地试图走捷径，也就是说，“我有原则就够了。我就是要推这个政策。”这是最慢最痛苦的旅程。你需要将不同的团体、不同的利益相关者聚集在一起，能够签署这些原则，并能够与每一个会影响价值观整合方式的人协商这些原则。

玛丽亚·露丝安娜·阿申特:只有你这样做，在某些情况下，制定这个政策可能需要同意。OECD 原则，制定一个传呼机大概花了两年时间。但在这背后的过程中，与利益相关者的广泛磋商吃了确保道德原则将被正确操作的秘方，因为在这个过程中，你不仅得到了需要参与的每个人的支持和咨询，而且你还开始了改变思维模式的过程，因为在这个过程中，你开始辩论为什么这些原则很重要，并能够根据你组织中现有的例子进行迭代，你如何使用数据和人工智能或潜在的例子，将会发生什么。通过让每个人都参与进来，你就开始了实施这些原则的过程。当你开始设计框架和工具的时候，你已经完成了一半，因为人们会对为什么这很重要以及为什么需要这样做有更高程度的认识和理解。

Adel Nehme:太棒了。你提到人工智能治理和问责制是负责任的人工智能的一个方面。人工智能治理可能需要数据科学家、商业领袖、专家、流程经理、运营专家以及任何组织中存在的各种不同的人和角色的协作和责任。并且他们可能不都具有相同的“数据语言”或数据技能或相同的数据素养水平。指出这一点，显然，数据素养和人工智能素养对组织来说很重要，但你认为我们应该如何扩展我们的数据素养概念，以解决工作场所中的人工智能道德，风险和责任？

玛丽亚·露丝安娜·阿申特:我认为在我们谈论人工智能和数据素养之前，我认为我们需要开始谈论一般的数字素养，以及技术含义范围内的素养。我认为，每次我们旨在教育人们掌握技术的时候，我们都避免描述，但是什么会出错呢？还有其他选择吗？因此，我们一直认为人工智能技术是解决人类所有问题的灵丹妙药。我们需要远离这种态度，一起重新考虑我们在这里试图解决的问题，以及建立像人工智能这样的技术的后果。越来越多的学者站出来说，“开发人工智能有很多我们看不到的隐性成本。我们认为人工智能的复杂程度是理所当然的，事实上，有很多隐藏的工作和努力没有被承认。”

玛丽亚·露丝安娜·阿申特:有一本漂亮的书刚刚出版。它将在英国上市，可能在未来几天内。我认为它已经在欧洲上市了。它叫人工智能地图集，作者是我最喜欢的人工智能领域的人之一，凯特·克劳福德。凯特所做的绝对是聪明的，她将人工智能描述为一种现象，这种现象不仅汇集了数据和算法，这是在这一领域工作的每个人的参考框架，尤其是工程师，而是汇集了所有其他元素，为我们提供数据和算法。从地球表面获取的自然资源是什么？那么这样做的生态代价是什么？例如，训练一个模型，一个语言模型的环境成本是多少？如果我们开始复制它，然后如果我们开始有更多的这种有万亿参数的模型，这对环境意味着什么？但也非常强调有多少隐藏的劳动被用于标记数据，有多少劳动被排除在人工智能的供应链之外，如果你想的话，并给人一种人工智能比实际上更聪明的印象，以至于她总结道，“除了数据，那不是石油，不是可以收获的自然资源，但事实上，它关乎人们的生活。”

玛丽亚·露丝安娜·阿申特:所以，在我们继续前进之前，我们仍然必须找到并同意数据对我们的意义。凯特的结论是，最终，人工智能既不是人工的，也不是智能的。我不想给我们的听众带来太多的惊喜，但我想说的是，这本书正是我们在谈到人工智能时需要的叙事类型，理解人工智能的影响的全部长度， 它来自哪里，谁拥有它，背后的利益是什么，以便我们共同走到一起，挑战那些实体和那些目前似乎不成比例地拥有强大人工智能部分的实体，并能够说，“我们需要有一个不同的方法来实现它。 我们需要以不同的方式来考虑。”

玛丽亚·露丝安娜·阿申特:虽然有些人可能会说可能有点太晚了，但我认为现在正是重新考虑的时候。对于现在加入人工智能的人来说，这是一个正确的时间来重新思考整个现象，并说，通过不平等的加剧，我们现在知道人工智能可以在不知道的情况下变得如此糟糕，没有隐藏的自动化，这已经存在于世界上许多地方的公共服务以及对环境的影响，这是进行这场对话的正确时间。现在是揭开人工智能隐藏部分的时候了，不要再认为它只是一个数据集和一个模型，看看背后是什么，我们是如何获得数据集的？我们是如何创建这个数据集的，这些数据集中代表了哪些人？

玛丽亚·露丝安娜·阿申特:那么，如果我们朝着那个方向发展，我们将如何改变这么多不同人的生活？我知道这听起来像是问题的蓄水池，但我认为为了让我们避免使用人工智能的绝对灾难，我们需要开始从这些方面思考。虽然像 Kate Crawford 这样的人和她在 AI Now 的出色团队以及世界各地的许多其他活动组织都做得非常出色，但我认为在我们自己的小团队和组织中，我认为我们可以从这些学者和远见卓识中学习和启发的是，我们需要超越我们的感知和愿景的直接边界，思考我作为一名数据科学家所做的事情实际上会改变，以及这将如何改变你应该拥有的责任和问责水平，并开始作为变革的倡导者。

玛丽亚·露丝安娜·阿申特:有时候，责任的界限需要从团队延伸到更高的业务部门、更高的公司以及社会。除非我们有数据科学家级别的草根行动主义，否则我们无法完全改变或转变整个技术世界的这种思维模式，因为我们需要内部的人，建立这种思维模式的人，承认他们的工作更有影响力，编写一段代码或处理一个数据集实际上正在改变人们的生活。

玛丽亚·露丝安娜·阿申特:虽然没有法律，但也没有什么会强迫你去想它。但我有信心，在这个行业有很多优秀的人，他们会理解什么是危险的，他们会学习如何成为变革的良好代理人，并以一种解释这种负面影响的方式构建人工智能。这就是我希望我们在这些地方开始具备的素养，而不是太多的学习，“哦，这就是你如何建立一个机器学习算法。这就是如何建立一个语音助手。”不，这是理解的含义和影响，然后从那里开始工作，如何最好地建立它，以便我们实现积极的结果，并能够保持消极的控制。

## 行动呼吁

Adel Nehme:我完全同意你对人工智能和数据素养的愿景，它包含了人工智能伦理和人工智能价值链及其外观。当我们结束这篇鼓舞人心的文章时，你对节目听众的最后呼吁是什么？

玛丽亚·露丝安娜·阿申特:不要把事情看得太重，要勇于挑战。挑战一切。我们需要你们去挑战，去告诉你们自己这项技术的真正潜力是什么，是谁在背后支持它，以及你们个人如何改变你们的现状。如果我没有接触到像凯特·克劳福德这样的人和许多像她一样的人的出色工作，我就不会这样说，他们不知疲倦地倡导人工智能的不同方法。我认为只有我们每个人都告诉自己，在我们要求我们的公司为我们提供框架、方法和政策之前，首先努力找到我们所处的位置并改变我们的心态，我认为我们有很多优势，我们自己是主要的建设者，是最接近建立这些工具来进行改变的人。

玛丽亚·露丝安娜·阿申特:虽然事情正朝着正确的方向发展，我希望看到自上而下方法领域的更多进展，公司围绕负责任的人工智能开发正确的框架，但如果我们没有自下而上的方法，让像你们一样的人理解这确实是历史上一个独特的时刻，我们手中的技术可以让我们成为人类的一个非常好的地方，也可以让我们成为一个黑暗的地方，这不会让我们走得太远。

玛丽亚·露丝安娜·阿申特:虽然我从来不太喜欢埃隆·马斯克或斯蒂芬·霍金等人所说的话，但我认为向那个方向发出警报是有好处的，因为这几乎就像他说，“那是你不想去的地方。”所以，如果你不想走到那一步，那就振作起来，朝着一个不同的结果努力，而不是向你展示这是可能的，因为无论你如何否认，都有可能实现。对于技术，包括政治家在内的绝大多数人几乎一无所知。

玛丽亚·露丝安娜·阿申特:这很容易被政治化，不，不会是人工智能接管世界。这将是其他人开发和使用人工智能的方式，将攫取更多的权力到自己手中。所以，我们需要小心。最好的方法是开始积极参与其中，而不只是说，“编码只是我的工作。清理这个数据集只是我的工作。”不仅仅是那些家伙。只有我们团结起来，我们才能做到。我们仍然是一个小社区，但我希望新的一代即将到来。那些正在接受培训以进入未来人工智能工作的人，他们将受到鼓舞，脱下这一愿景，他们将加入我们，我们将一起继续推动人工智能现在如何被创造以及人工智能在未来应该如何被开发和使用的边界。

Adel Nehme: Maria，非常感谢你参加播客。我真的很感谢分享你的见解。

玛丽亚·露丝安娜·阿申特:非常感谢你邀请我。

Adel Nehme:今天的 DataFramed 节目就到这里了。谢谢你和我们在一起。我真的很喜欢 Maria 关于数据科学家如何在他们的工作中承担更多责任的富有同情心的行动呼吁，以及她对负责任的人工智能状态的见解。如果你喜欢这个播客，一定要在 iTunes 上留下评论。我们的下一集将与布伦特·戴克斯一起探讨如何通过有效的数据讲述来获得更有影响力的数据科学。我希望这对你有用，我们下次在 DataFramed 上再见。