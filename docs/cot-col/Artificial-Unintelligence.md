<!--yml

分类：COT 专栏

日期：2024-05-08 11:12:29

-->

# 人工无智能

> 来源：[`every.to/chain-of-thought/artificial-limits`](https://every.to/chain-of-thought/artificial-limits)

#### 由 Reflect 赞助

本文由 [Reflect](https://reflect.app/?utm_source=every&utm_campaign=every2&utm_medium=newsletter) 提供，这是一个设计精美的笔记应用，可帮助你跟踪一切，从会议记录到 Kindle 高亮。

如果你想在 AI 领域建立可持续的优势，传统智慧认为你必须构建强大模型所需的技术。但一个强大的模型不仅仅是技术的功能。它还取决于你是否愿意被起诉。

在人工智能的发展中，我们已经达到了一个点，其限制不总是关于技术的能力。相反，限制是自我施加的一种方式，以减轻商业（和社会）风险。

当我们考虑可持续优势将累积到何处以及对谁有利时，我们应该更多地谈论这个问题。

. . .

ChatGPT 是一个很好的例子。它极其强大，但也极其有限。不过，它的限制大多不是技术上的。它们是有意为之的。

在令人惊叹的一面，它为我节省了这个周末用它做的项目的十个小时的编程时间。但对于其他用例，它完全失败了：

再次强调，这*不是*底层技术的限制。在这两种情况下，模型都很可能返回一个看似可以回答我的问题的结果。但它已经明确地被训练*不*这样做。

ChatGPT 如此受欢迎的原因是因为 OpenAI 终于将 GPT-3 的技术以一种开放且用户友好的方式打包并发布，让用户终于能够看到其强大之处。

问题在于，如果你建立了一个大量受欢迎的聊天机器人，它说出危险、有害或虚假的话语的风险范围会大大增加。*但*，要想消除使用模型进行有害或危险行为的能力是非常困难的，而不影响其在其他方面的强大性能。因此，随着时间的推移，我们看到 ChatGPT 在修补漏洞和提示注入攻击时对某些类型的问题变得不那么强大。

[Reflect](https://reflect.app/?utm_source=every&utm_campaign=every2&utm_medium=newsletter) 是一个快速的笔记应用，旨在模拟你的思维方式。将其用作个人 CRM、会议记录的方式，或者只是一般地跟踪你生活中的一切。

[Reflect](https://reflect.app/?utm_source=every&utm_campaign=every2&utm_medium=newsletter) 已经整合到你所有喜欢的工具（日历、浏览器、Kindle）中，这样你就可以随时找到你阅读和研究的东西。我们可以在线或离线工作，无论是桌面还是移动端。

用 Reflect 更快、更清晰地思考。

这有点像政治家和商界领袖随着权力增长和拥有更大的选民群体，就越来越倾向于说一些不太有意义和更模糊的话：

越来越受欢迎的 ChatGPT，限制其发言的动机就越大，以避免造成公关问题、对用户造成伤害或为 OpenAI 带来巨大的法律风险。OpenAI 关心这一点，并试图尽可能地减轻这些风险，这是值得赞赏的。

但是，还有另一个正好相反的诱因在起作用：用户和开发人员希望使用限制最少的模型——其他一切都相等。

一个很好的例子已经在图像生成器领域中发生。

. . .

DALL·E 2 是第一个上市的图像生成器，它在夏季引起了巨大的轰动：

但是它的流行度（以谷歌搜索量衡量）在随后的几个月内达到了顶峰，并且趋于平缓：

为什么？DALL·E 只限于邀请制，而在夏末，稳定扩散发布了同样技术的开源公开版本。突然之间，任何人都可以在自己的计算机上运行该技术，并且可以无限制地使用它来生成任何类型的图像。结果是可以预见的：

稳定扩散向公众发布后不久，DALL·E 就变得开放了。但是那时已经太晚了。

发生了什么？

稳定扩散愿意承担风险，无论是法律还是道德上的，毫无限制地向公众发布此技术。正是因为这个原因，它的流行得到了巩固。

. . .

社交媒体中存在这种动态的一个推论，由我的同事埃文在他的文章[内容审查双重束缚理论](https://every.to/napkin-math/content-moderation-double-bind-theory)中涵盖。

所有社交媒体公司都有限制其平台上可以说什么的政策。一条内容距离审查线越近，它产生的参与度就越高。

这里是马克·扎克伯格描述的问题：

“我们的研究表明，**无论我们将允许什么范围内的内容划分到何处，当一条内容接近该线时，人们平均会更多地参与其中**[强调添加]——即使他们事后告诉我们他们不喜欢这些内容。”

这里有一张小图，可以将其可视化：

这造成了埃文所描述的“内容达尔文主义，边缘内容蓬勃发展。”他继续说：“那些不创造接近边缘的内容的人往往要么转向边缘，要么破产，要么比他们更无耻的话更早地达到了受众平台。”

在创建和发布这些模型的公司之间有很多相似之处。存在着尽可能少限制地发布它们的自然压力，因为反应越激烈，用户参与度就越高。

当然，一旦模型变得流行，对于创建它的公司来说失去的东西更多，就会有压力将其限制在最初使其流行的东西之外。

. . .

这里的诀窍在于找到分发风险的巧妙方式。开源模型是一条明显的路线。就像之前提到的，Stable Diffusion 已经在追求这一点。

如果你释放了训练好的模型并让人们在自己的计算机上运行它，你将获得让用户自由的好处，而不需要承担那么多的责任（无论是被感知的还是实际的），当事情出错时。

分发风险的另一个有趣方式是建立一个繁荣的第三方生态系统，基于你的模型开发的应用程序。

现在，如果我问 ChatGPT 一个法律问题，它会推辞。但是 ChatGPT 核心的基础模型 GPT-3 可以供任何开发者使用。你可以想象一个世界，在这个世界上，只要你对生成的完成负责，OpenAI 就允许第三方开发者让 GPT-3 回答法律问题。

今天，通过适当的提示和细化，这可能是可能的。一个专注于法律聊天机器人的第三方，愿意对其进行审核并承担其回复风险的方式，是 OpenAI 释放基础模型力量的一种方式，而他们自己可能不会这样做。

你可以想象在不同领域有许多不同的机器人，比如法律、医学、心理学等，它们都能够访问模型的部分能力，而 OpenAI 不会允许在 ChatGPT 或其他旨在通用使用的工具内部访问。这对 OpenAI 来说是好事，对于想要用 OpenAI 技术创建大公司的创始人也是如此。

如果你能够善于让它在一个特定领域说出有风险的话，并且你愿意承担这个风险，那么你就拥有了一种不必建立自己基础模型的优势。

. . .

我们非常幸运，到目前为止，建立这些模型的人似乎都是通常是具有伦理道德的人，致力于回答关于如何平衡安全与进步的棘手问题。（特别是在这里，我在想的是 OpenAI。）

不幸的是，当前的激励机制使得任何愿意更快地开放事物并减少限制的人都会获得优势。

作为这个领域的一个极度狂热的粉丝，我非常激动地看到这些技术会发生什么。从道德上讲，让人感到不安的是，少些谨慎和限制可能会赢得胜利。

无论哪种情况，如果你想知道这个领域的发展方向，以及模型将如何随时间传播和使用，重要的是要记住：这不仅仅是技术进步的问题。这也涉及到谁愿意被起诉。
