# 可解释性在人工智能决策中的重要性

> 原文：<https://www.algolia.com/blog/ai/what-is-explainable-ai-and-why-is-transparency-so-important-for-machine-learning-solutions/>

人工智能技术在商业和政府中的应用一直在悄然发展。机器学习模型已经成为在海量数据中识别模式的专家。人工智能正与人类决策一起使用，目标是创造更好的结果，部分是通过让人们管理它如何运作，并使他们相信它是一个可行的合作伙伴。

到目前为止，一切顺利。除了可能有一个关键的权衡:人工智能工具做出的决定缺乏透明度。

想象一下…

*   根据学校工作人员使用的算法，你被第一志愿大学拒绝录取。
*   **你在生产线上工作，** 而新“优化”的人工智能为一个流程生成的指令似乎并不安全。
*   **你医生的 AI** **软件** 根据你对自己身体的了解，得出了一个看似错误的结论。
*   **在黑色星期五，** 你的电子商务客户没有做人工智能软件 [预测的](https://www.algolia.com/blog/ai/increase-your-ecommerce-roi-by-predicting-users-next-shopping-stages/) 他们会成群结队地做的事情。你的销售额在下降，时间也不多了。

## [](#the-need-for-clear-explanations)需要(明确)的解释

在所有这些由人工智能模型输出创建的理论案例研究中，你可能会感到慌乱。你会想知道为什么你没能打动招生委员会的机器人，你的新工作流程是否会引发事故，你是否需要第二种意见，你如何修复你的网站。

你会想知道，模型预测究竟是如何由机器学习系统做出的，而这些机器学习系统无疑是如此的智能。在某些方面很聪明——毕竟，它是人工智能——但就你而言，并不完全是聪明的计算机科学。你会觉得你应该——甚至有权利——知道系统的精确学习方法和决策流程。

你还希望能够完全信任模型的性能。为了对结论满意，你会觉得有必要知道非人类数据科学家是如何应用他们的训练数据的。如果你怀疑解释的准确性有问题或者数据集有问题，你会希望有人干预。然而，你无法证明机器人拿错了工具箱里的东西。

## [](#beyond-the-individual)超越个体

不仅仅是你担心人工智能技术。与工作的重要特征相关的透明度对于参与该过程的每个人都是必不可少的。如果你是公司内部的一员，比如首席招生官或医疗保健团队主管，你会希望决策过程完全透明。你会感受到这一决定的影响力，如果不只是因为你害怕诉讼，而是因为你的公司优先考虑公平和道德标准。如果你是监管者，你可能需要一个完整、清晰的解释。

所以你可以看到为什么详细描述人工智能可解释方法的能力是至关重要的。能够解释机器学习模型可以增加对模型的信任，这在影响许多金融、医疗保健和生死决策的场景中至关重要。

## [](#what-is-explainable-ai)什么是可交代的 AI？

可解释的人工智能——或可解释的人工智能(XAI)——基本上意味着它听起来的样子:解释通常围绕人工智能内部工作的“黑匣子”中正在发生的事情。可解释的机器学习是负责任的，可以“展示它的工作”

“可解释性是表达人工智能系统为什么会做出特定决定、建议或预测的能力，”2022 年 [麦肯锡&公司](https://www.mckinsey.com/capabilities/quantumblack/our-insights/why-businesses-need-explainable-ai-and-how-to-deliver-it) 报告称。

[IBM](https://www.ibm.com/watson/explainable-ai) 将可解释的 AI 定义为“一组允许人类用户理解和信任机器学习算法创建的结果和输出的过程和方法。”

卡内基-梅隆大学 [软件工程学院](https://insights.sei.cmu.edu/blog/what-is-explainable-ai/) 的 Violet Turri 指出，截至 2022 年，确切的定义仍未被真正采用:“可解释性旨在回答利益相关者关于人工智能系统决策过程的问题。”

很难想象算法能够展示他们决策背后的理由，并指出他们选择的优缺点。但这正是它的本质:智能技术提供了人们可以轻松理解的信息，并带有“面包屑”,可以追溯到决策制定的时间。

## [](#ai-can%e2%80%99t-go-rogue-can-it)AI 不能去流氓(可以吗？)

所以你看到了问题所在:在一个不透明的系统中，人工智能有可能为所欲为，而人们没有办法遵循它的逻辑或理解它的结论。

当一个 ML 模型得出一个可能有深远影响的结论时，人类需要理解它是如何运作的。受决策影响的人——更不用说政府机构(例如，[【DARPA】](https://www.darpa.mil/program/explainable-artificial-intelligence))——通常想知道结论是如何得出的。对于内部利益相关者来说也是如此，例如保险公司的销售人员，他们依赖人工智能模型的建议来确定保险金额，他们既需要获得客户的认可，又需要维护自己的公司声誉。

因此，企业和政府，更不用说个人消费者，都同意人工智能决策必须清楚地传达它是如何做出决定的。不幸的是，由于人工智能过程的复杂性，这不一定总是可以实现的，随后的决策是在最初决策的基础上做出的。

## [](#can-they-explain)他们能解释吗？

也许如果机器人有发言权，它们会告诉我们，我们不够聪明，无法完成它们完成的复杂动作，然后问我们为什么需要知道。但是他们不负责，至少现在还不负责。(嗯；也许我们比我们意识到的更接近他们开始秘密决定事情的那一天，无视我们对透明度的请求。)

所以一定有 AI-process 的解释。人们的舒适程度、公司的声誉、甚至人类的最终生存都很可能取决于此。简而言之，商人、他们的客户和合作伙伴以及监督机构都必须能够审计和理解人工智能决策过程的每个方面。

此外，这些解释必须用我们的语言表述。它们必须在逻辑上有意义，并且(我们希望)让我们人类觉得我们理解——并且我们可以接受——即使我们对此感到失望——决定了什么。

## [](#why-transparency-is-critical-for-machine-learning-solutions)为什么透明度对机器学习解决方案至关重要

如果人工智能电子纸轨迹对人类来说不可理解怎么办？

简而言之，混乱会在各个层面接踵而至。2017 年，斯蒂芬·霍金警告说，“除非我们学会如何准备和避免潜在的风险，否则人工智能可能是我们文明史上最糟糕的事件。”按照这些思路，无法识别或记录人工智能程序的“思维过程”可能会对消费者、公司以及世界是一个公正和公平的地方的总体感觉造成[](https://builtin.com/artificial-intelligence/risks-of-artificial-intelligence)的毁灭性打击。

因此，所有受影响的群体都有足够的动力来确保透明度。

## [](#progress-so-far)迄今进展

目前人工智能的透明度没有一个统一的全球标准，但人们普遍认为人工智能的运作必须得到解释。随着法律和其他问题的增加，XAI 世界将会适应任何变化的要求。

一些行业中正在利用人工智能的公司已经受到监管要求有一段时间了。欧洲已经实施了 [通用数据保护法规](https://gdpr.eu/) (GDPR，2016)，该法规要求公司向消费者解释人工智能如何做出影响他们的决定。

虽然美国尚未效仿欧洲，通过这种类型的全面法律，但《加州消费者隐私法 (CCPA，2020 年)表达了同样的观点:用户有权知道人工智能系统对他们做出的推断，以及这些推断使用了哪些数据。

2021 年，国会 [将可解释性](https://crsreports.congress.gov/product/pdf/R/R46795) 确定为促进人工智能系统中的信任和透明度不可或缺的一部分。

### [](#lower-level-initiatives)下级倡议

也有机构和公司提到这个目标，以及开发各种版本的可解释的人工智能原理。美国国防部一直致力于创建一个“稳健负责的人工智能生态系统”，包括创建 [原则](https://www.defense.gov/News/Releases/Release/Article/2091996/dod-adopts-ethical-principles-for-artificial-intelligence/) (2020)。以及 [健康与人类服务](https://www.hhs.gov/sites/default/files/final-hhs-ai-strategy.pdf) 旨在“促进道德、值得信赖的人工智能使用和发展。”

包括 [谷歌](https://ai.google/principles/) 在内的各种组织也承担起了开发负责任的人工智能(RAI)原则的责任。虽然不是所有的公司都优先考虑这些标准，但它们是成长的良好基础。

## [](#explainability-has-its-rewards)有交代就有回报

除了能提供一个清晰的解释让你安心之外，让人工智能决策透明化还有经济上的好处。

[麦肯锡](https://www.mckinsey.com/capabilities/quantumblack/our-insights/global-survey-the-state-of-ai-in-2021) 发现，更好的可解释性导致了更好地采用人工智能，最佳实践和工具也随着技术一起发展。它还了解到，当公司将数字信任作为客户的优先事项时，例如通过在算法模型中纳入可解释性，这些公司更有可能将其年收入增长 10%或更多。

[IBM](https://www.ibm.com/watson/explainable-ai) 也有同样令人印象深刻的证据:其 XAI 平台的用户实现了 15 %- 30%的模型准确性提升和 410 万-1560 万美元的利润。

## [](#challenges-of-xai)挑战 XAI

### [](#complex-models-can-cloud-transparency)复杂模型可以云透明

随着人工智能能力的进步，它们已经被用于解决越来越困难的问题。

随着这一切的发生，关于它是如何发生的问题也越来越多:为什么人工智能软件模型会做出这样的决定？人类队友是否理解模型如何运行，并对用于训练它的数据输入有坚实的理解？如何解释*AI 做出的每一个决定* ，而不仅仅是最初的模型行为？

“一个人工智能系统变得越复杂，就越难精确定位它是如何获得特定洞察力的，”麦肯锡[](https://www.mckinsey.com/capabilities/quantumblack/our-insights/why-businesses-need-explainable-ai-and-how-to-deliver-it)说。…“解开一阶洞见并解释人工智能如何从 A 到 B 可能相对容易。但随着人工智能引擎对数据进行插值和重新插值，insight 审计跟踪变得更加难以追踪。”

问题是很多 AI 模型都是 [不透明](https://journals.sagepub.com/doi/full/10.1177/2053951715622512) :很难或者不可能看到幕后发生的事情。一位 [研究员](https://journals.sagepub.com/doi/full/10.1177/2053951715622512) 说，它们是在没有透明度的情况下创建的，要么是有意的(例如，为了保护企业隐私)，要么是无意的，但使用了非技术人员难以理解的技术数据，要么是无意的“来自机器学习算法的特征和有效应用它们所需的规模”。

此外，AI 还有多种途径，包括卷积 [神经网络](https://www.ibm.com/cloud/learn/neural-networks) 、递归神经网络、迁移学习、深度学习等。因为有如此多的操作方式，找到人工智能可解释性问题的根源会更加棘手。

不透明是一回事；人们仍然不理解的口头上的透明可能是另一个原因。解释太专业怎么办？如果您的最终用户不能理解他们需要理解的概念，如 [深度神经网络](https://www.techopedia.com/definition/32902/deep-neural-network) ，您如何确保对系统的信任？如果您不能以一致的方式向他们传达数据发生了什么，该怎么办？

### [](#lack-of-consensus-on-ai-explainability-definitions)对人工智能可解释性定义缺乏共识

人工智能领域仍在兴起，所以还没有很多关于选择、实现和测试人工智能解释的实用知识。专家们甚至无法就如何定义基本术语达成一致。例如， *可解释 AI* 与 *可解释 AI* 是同一个概念吗？

一些研究人员和专家说是的，并交替使用这些术语。其他人强烈反对。有些人认为创建具有 *内置* 透明度的模型是必要的，这样决策在制定后就可以被人们轻松地解释(事后解释)。可解释的机器学习是一种不同的方法，不同于前述的正式问责制，也不同于试图解释人工智能黑盒模型中的内容，然后再解释 *。*

这种对概念缺乏共识的情况导致了在不同行业使用人工智能的各种学者和商界人士之间的尴尬话语，也抑制了集体进步。

### [](#the-academia-bubble)学术界的泡沫

人工智能社区的问题在很大程度上仍在学术界争论，而不是在平民世界成为主流。事实上，对人工智能的理解和信任还没有站稳脚跟的一个结果是，人们可能天生不信任它做出影响他们的决定。

### [](#lacking-in-social-accountability)缺乏社会责任感

一些人工智能倡导者正在强调围绕人工智能决策的人类经验；这就是所谓的“社会透明度”他们驳斥了传统的假设，即如果人类能够进入黑匣子并理解所发现的东西，一切都会好的。但是计算机科学家 [乌波尔·厄桑](https://thegradient.pub/human-centered-explainable-ai/) 指出，“并非所有重要的东西都在人工智能的黑匣子里”。“关键的答案可以在它之外。因为人类就在那里。”

### [](#bias-in-ai-models)偏向于 AI 模式

目前在人工智能领域工作的人并不是一个特别多样化的群体(该领域由白人男性主导)。因此，多样性支持者认为，可解释模型的创建和运行方式存在一些固有的偏见。这听起来很准确，但如果没有使用机器学习模型的多样化员工群体，你如何解决这个问题？给定模型的决策中是否存在偏见——如果存在，如何解决——是人们一直关注的问题。

### [](#possible-mistakes)可能出现的错误

可解释的模型如此有价值的一个原因是，人工智能模型有时会犯与算法相关的错误，这可能导致从小的误解到股市崩盘和其他灾难。作家 [迈克·托马斯](https://builtin.com/artificial-intelligence/risks-of-artificial-intelligence) 指出:“当谈到正确时，我们认为计算机是最重要的，但人工智能实际上仍然和为它编程的人类一样聪明。”。

### [](#human-reactions-and-business-impacts)人的反应和商业影响

如果一个基于数据科学的决策普遍不受欢迎——也不被受其影响的人理解，你可能会遇到很大的阻力。如果一个心怀不满的人倾向于抱怨感知的不公平，比如在金融服务业，会损害公司的声誉吗？这种情况是公司高管的噩梦。

公众可能不理解人工智能模型可解释性方法是如何工作的，但如果模型是负责任地创建和监督的，并可靠地提供准确的结果，那就是朝着确保接受这一不断发展的技术迈出的一步。随着对人工智能算法的依赖不断增加，以不同的用例做出重大的商业、医疗和其他类型的选择，能够解释和审计人工智能应用程序做出的决定将有助于建立对人工智能工作的信任和接受。

## 交代搜索数据的好处

在搜索数据领域，强大的人工智能透明度是成功的关键。复杂的 [排名](https://www.algolia.com/blog/ai/what-is-ai-search-ranking/) 公式(例如，通过混合属性权重和单词之间的接近度而产生)本质上构成了不透明的模型。如果你的目标是网站优化，你可能想知道为什么你的搜索结果以特定的顺序出现，它们可以被测试、提炼和调整以获得你想要的相关性吗？

有了 Algolia，你不必相信算法的可解释性技术，也不必担心系统如何工作或模型的可解释性。对于如何计算搜索相关性，我们提供搜索的客户享受一个白盒模型 [透明度](https://www.algolia.com/blog/ai/using-ai-to-deliver-smarter-search/) 。例如，您可以看到搜索结果是如何基于个性化和相关性因素进行排名的，然后根据实际需要进行手动调整。

想要利用人工智能 [的力量](https://www.algolia.com/products/ai-search/) 为您的用户创建出色的搜索，并为您的团队带来真正的透明度？查看我们的 [搜索和发现 API](https://www.algolia.com/) 和 [立即联系我们的专家。](https://www.algolia.com/contactus/)