# 合成数据与蒸馏

从人类反馈中进行强化学习深深植根于保持人类对我们所构建的模型的影响这一理念中。当第一个模型成功使用 RLHF 进行训练时，人类数据是改善模型这种方式的唯一可行途径。

人类是唯一能够创建足够高质量响应以进行训练的方式。人类是唯一能够收集可靠和具体反馈数据以训练奖励模型的方式。

随着人工智能模型变得越来越好，这个假设迅速瓦解了。合成数据（成本远低且易于迭代）的可能性使得从强化学习与人类反馈（RLHF）成为关注的中心，到更广泛的“后训练”塑造模型的想法得以扩散。本章简要概述了合成数据是如何以及为什么取代或扩展 RLHF 流程中的许多部分。

我们首先从对合成数据的常见批评开始，以突出数据实际上具有极高的能力。许多报告都提到了合成数据如何导致“模型崩溃”或其他模型问题 [[302]](ch021.xhtml#ref-shumailov2024ai)，但在领先的语模型中这一点被明确反驳 [[303]](ch021.xhtml#ref-gerstgrasser2024model) [[304]](ch021.xhtml#ref-feng2024beyond)。合成数据 *确实* 可能会导致模型出现性能问题，但这通常是由于使用了重复数据或仅由训练模型输出的数据（限制了其潜在分布）而不是全面的数据来源。

领先的模型 **需要合成数据** 来达到最佳性能。现代后训练中的合成数据包括许多训练环节——语言模型被用来从种子示例生成新的训练提示 [[305]](ch021.xhtml#ref-wang2022self)，修改现有提示，生成对提示的完成 [[306]](ch021.xhtml#ref-numina_math_7b)，提供 AI 反馈以创建偏好数据 [[23]](ch021.xhtml#ref-cui2023ultrafeedback)，过滤完成 [[307]](ch021.xhtml#ref-li2024superfiltering)，等等。合成数据是后训练的关键。

合成数据能够产生如此大的影响，这一能力随着 GPT-4 级模型的出现而出现。在早期的语言模型，如 Llama 2 和 GPT-3.5-Turbo 中，模型在生成或监督数据管道方面不够可靠。在 1-2 年内，语言模型在生成答案方面远胜于人类。在 GPT-3.5 到 GPT-4 级模型过渡期间，模型执行 LLM 作为裁判任务的能力也出现了。GPT-4 或更好的模型在生成反馈或评分方面对内容的稳健性和一致性远超以往。

自从这次转变以来，合成数据在语言模型训练中的作用仅是增长。否则，还有两个明显的领域，人类数据仍然非常重要。

1.  人类数据在模型中的角色继续处于能力边缘——人类必须在 AI 尚未具备任何能力的地方生成数据。一旦第一个强大模型存在，合成数据就会迅速增多。

1.  尽管学术研究表明合成版本的表现同样出色，但人类偏好数据仍然被用于领先模型中。人类偏好在文献中的作用仍在建立中。

“蒸馏”这个术语已经成为围绕合成数据在语言模型中作用的讨论中最有力的形式。这个术语来源于深度学习文献中对教师-学生知识蒸馏的技术定义 [[51]](ch021.xhtml#ref-hinton2015distilling)。

“蒸馏”通俗地说，是指使用更强模型的输出训练一个较小的模型。在后训练中，这种蒸馏的一般概念有两种常见形式：

1.  作为在广泛的后训练过程中使用的数据引擎：用于指令的补全、偏好数据（或宪法 AI），或用于强化学习（RL）的验证。

1.  将特定技能从更强的模型转移到较弱的模型，这通常用于特定的技能，如数学推理或编码。

随着语言模型在回答各种任务时比人类更可靠，第一种策略越来越受欢迎。GPT-4 类模型将这一范围扩展到使用更强模型的蒸馏来完成复杂任务，如数学和代码（如上所述）。在这里，蒸馏促使拥有一个模型套件，通常实验室会训练一个大型内部模型，如 Claude Opus 或 Gemini Ultra，这些模型不会公开发布，仅用于内部使用以制作更强的模型。对于开源模型，常见的做法是将封闭 API 模型的训练数据蒸馏到更小、公开可用的权重 [[21]](ch021.xhtml#ref-tunstall2023zephyr)。在此过程中，精心制作高质量的提示和过滤来自教师模型的响应对于最大化性能至关重要。

将特定技能转移到较小的语言模型时，使用与蒸馏相同的原理——获取尽可能好的训练数据。在这里，许多论文研究了使用来自更强模型的有限数据集来提高一致性 [[13]](ch021.xhtml#ref-zhou2023lima)，数学推理 [[308]](ch021.xhtml#ref-shridhar2023distilling) [[309]](ch021.xhtml#ref-hsieh2023distilling)，以及测试时缩放 [[258]](ch021.xhtml#ref-muennighoff2025s1)。
