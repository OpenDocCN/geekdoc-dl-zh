# 产品、用户体验和模型性格

RLHF 和后训练的前沿展示了这些技术在公司内部如何用于制造领先的产品。随着 RLHF 的日益成熟，它所解决的问题变得更加复杂。在本章中，我们讨论了一系列领先的 AI 实验室认为 RLHF 和后训练是重要的用例，这些用例在学术文献中基本上未被研究。

## 性格训练

性格训练是后训练的一部分，旨在通过塑造模型响应的方式而不是内容来构建模型内的特质 [[357]](ch021.xhtml#ref-maiya2025open)。尽管性格训练对于语言模型聊天机器人的用户体验很重要，但在公共领域实际上并未得到研究。

我们不知道性格训练的权衡，我们不知道如何确切地研究它，我们不知道它能在 ChatBotArena 上改善用户偏好吗，但我们应该知道。我们知道的是，性格训练使用了本书中讨论的相同方法，但针对模型使用的语言特征有更精确的目标。性格训练涉及广泛的数据过滤和合成数据方法，如宪法 AI，这些方法专注于模型行为的方式。这些变化通常很难在我们在本章“评估”中提到的所有基准体系中衡量，因为 AI 实验室使用性格训练在一段时间内对性格进行微小调整，以改善用户体验。

例如，Anthropic 将性格训练添加到了其 Claude 3 模型中 [[358]](ch021.xhtml#ref-anthropic2024claude)：

> Claude 3 是第一个在我们对齐微调过程中添加“性格训练”的模型：这是在初始模型训练之后发生的训练部分，以及将模型从预测文本模型转变为人工智能助手的部分。性格训练的目标是使 Claude 开始拥有更加细腻、丰富的特质，如好奇心、开放性和周到。

在接下来的几个月里，模型行业的性格变得更加鲜明。这个过程极其依赖于合成数据，但需要艺术家的触感，正如博客文章稍后所述：“它依赖于人类研究人员密切检查每个特质如何改变模型的行为。”

性格训练成为发展的焦点，这表明 RLHF 和相关方法已经从其哲学动机的对齐转向了主要是一个经验工具。模型可以捕捉到许多不同的行为，但让它们可靠地按照我们的意愿行事是最困难的部分。目前来看，这似乎更像是捕捉 RLHF 作为性能工具的正面效果，而不是安全性。

关于性格训练的少数公开讨论之一来自阿曼达·阿斯凯尔在 Lex Fridman 播客中的亮相（摘自转录）：

> Lex Fridman (03:41:56) 当你提到性格训练时，性格训练中包含了哪些内容？是 RLHF 还是我们在谈论什么？
> 
> Amanda Askell (03:42:02) 它更像是宪法 AI，所以它是该管道的一个变体。我通过构建模型应该具备的性格特征进行了工作。它们可以是较短的特性，也可以是更丰富的描述。然后让模型生成与该特性相关的人类可能给出的查询。然后它生成响应，并根据性格特征对响应进行排序。以这种方式，在查询生成之后，它与宪法 AI 非常相似，有一些不同之处。我非常喜欢它，因为它就像 Claude 在自己的性格中进行训练，因为它没有任何……它就像宪法 AI，但没有任何人类数据。

总结来说，Anthropic 使用他们用于宪法 AI 和一般后训练的相同技术来训练这些模型的性格。

## 模型规范

OpenAI 最近分享了他们所谓的“模型规范”[[124]](ch021.xhtml#ref-openai2024modelspec)，这是一份详细说明他们在点击微调运行前的目标模型行为的文档。它关于模型的行为现在，OpenAI 如何通过 API 背后引导他们的模型，以及他们的模型在未来将如何转变。

模型规范是行业中少数几个工具之一，在 RLHF 中，可以比较模型的实际行为与设计师的意图。正如我们在本书中所述，训练模型是一个复杂且多方面的过程，因此预期最终结果与输入（如数据标注员指示或训练数据中任务平衡）不同。例如，模型规范比宪法 AI 中使用的原则列表更有揭示性，因为它涉及到过程的意图，而不是列出充当中间训练变量的行为。

模型规范为参与模型发布过程的每个利益相关者提供价值：

+   **模型设计师**：模型设计师能够从需要明确他们希望和希望不希望的行为中受益。这使得在数据上的优先级决策更容易，有助于集中精力在可能偏离长期方向的工作上，并使他们在复杂的评估套件中评估模型的整体图景。

+   **开发者**：模型的使用者对他们遇到的行为可能是故意的——即某些类型的拒绝——或者训练的副作用有一个更清晰的了解。这可以让开发者更有信心使用来自这个提供者的未来更智能的模型。

+   **观察公众**：公众从模型规范中受益，因为它是在训练中优先考虑的信息的少数公共来源之一。这对于监管监督和制定关于 AI 模型应该做什么和不应该做什么的有效政策至关重要。

## 产品周期、用户体验和 RLHF

随着强大的 AI 模型越来越接近产品而非实验机器学习过程中的单一艺术品，RLHF（强化学习与人类反馈）已成为模型与产品之间关系的一个接口点。要使模型易于使用，不仅仅是确保最终模型权重正确，还需要考虑快速推理、合适的工具使用（例如搜索或代码执行）、可靠且易于理解的用户界面（UX）等多方面因素。由于 RLHF 被视为一种实时理解用户对产品偏好的方式，并且它是发布前的最终训练阶段，因此 RLHF 研究已成为测试这些因素的一个接口。将新功能添加到模型中最快的方式是在训练后尝试整合，因为此时训练更快且成本更低。这一循环已在图像理解、工具使用、更好的行为等方面得到体现。最初作为产品问题的问题很快就会变成 RLHF 建模问题，如果在那里取得成功，它就会反向传播到其他更早的训练阶段。
