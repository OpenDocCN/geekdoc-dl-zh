+   [人类反馈的强化学习之书](README.md)
+   [引言](rlhf-bk_ch001.md)
+   [关键相关作品](rlhf-bk_ch002.md)
+   [定义与背景](rlhf-bk_ch003.md)
+   [训练概述](rlhf-bk_ch004.md)
+   [偏好的本质](rlhf-bk_ch005.md)
+   [偏好数据](rlhf-bk_ch006.md)
+   [奖励模型](rlhf-bk_ch007.md)
+   [正则化](rlhf-bk_ch008.md)
+   [指令微调](rlhf-bk_ch009.md)
+   [拒绝采样](rlhf-bk_ch010.md)
+   [强化学习（即策略梯度算法）](rlhf-bk_ch011.md)
+   [直接对齐算法](rlhf-bk_ch012.md)
+   [宪章 AI 与 AI 反馈](rlhf-bk_ch013.md)
+   [推理训练与推理时间缩放](rlhf-bk_ch014.md)
+   [工具使用与函数调用](rlhf-bk_ch015.md)
+   [合成数据与蒸馏](rlhf-bk_ch016.md)
+   [评估](rlhf-bk_ch017.md)
+   [过度优化](rlhf-bk_ch018.md)
+   [风格与信息](rlhf-bk_ch019.md)
+   [产品、用户体验和模型性格](rlhf-bk_ch020.md)
+   [参考文献](rlhf-bk_ch021.md)