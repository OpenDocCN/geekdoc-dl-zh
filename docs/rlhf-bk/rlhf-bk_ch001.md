# 引言

从人类反馈中进行强化学习（RLHF）是一种将人类信息纳入 AI 系统的技术。RLHF 最初主要作为一种解决难以指定问题的方法出现。由于个体偏好的往往难以表达，因此，当系统设计为直接由人类使用时，这类问题会不断出现。这涵盖了与数字系统内容交互的每个领域。RLHF 的早期应用通常在控制问题和强化学习（RL）的其他传统领域，其目标是优化特定行为以解决问题。RLHF 领域的核心思想是“我们能否仅通过基本偏好信号引导优化过程来解决难题。”RLHF 通过 ChatGPT 的发布而广为人知，随后是大型语言模型（LLMs）和其他基础模型的快速发展和应用。

RLHF 的基本流程包括三个步骤。首先，必须训练一个能够遵循用户问题的语言模型（参见第九章）。其次，必须收集人类偏好数据以训练人类偏好的奖励模型（参见第七章）。最后，可以使用选择的 RL 优化器对语言模型进行优化，通过采样生成并使用奖励模型对其进行评分（参见第三章和第十一章）。本书详细介绍了该过程中每一步的关键决策和基本实现示例。

RLHF 已成功应用于许多领域，随着技术的成熟，复杂性也在增加。早期使用 RLHF 的突破性实验应用于深度强化学习 [[1]](ch021.xhtml#ref-christiano2017deep)、摘要 [[2]](ch021.xhtml#ref-stiennon2020learning)、遵循指令 [[3]](ch021.xhtml#ref-ouyang2022training)、解析网络信息以进行问答 [[4]](ch021.xhtml#ref-nakano2021webgpt) 和“对齐” [[5]](ch021.xhtml#ref-bai2022training)。早期 RLHF 的配方总结如下，见图 1。

![图 1：早期三个阶段的 RLHF 过程，包括 SFT（强化学习与人类反馈）、奖励模型以及随后优化。](img/file0.png)

图 1：早期三个阶段的 RLHF 过程，包括 SFT（强化学习与人类反馈）、奖励模型以及随后优化。

在现代语言模型训练中，RLHF（强化学习与人类反馈）是后训练的一个组成部分。后训练是一套更完整的技巧和最佳实践，旨在使语言模型在下游任务中更加有用 [[6]](ch021.xhtml#ref-lambert2024t)。后训练可以概括为一个多阶段训练过程，使用三种优化方法：

1.  指令/监督微调（IFT/SFT），其中我们教授格式化并形成遵循指令能力的基础。这主要关于在语言中学习*特征*。

1.  偏好微调（PreFT），其中我们与人类偏好对齐（同时也在能力上获得较小的提升）。这主要关于语言的*风格*和难以量化的微妙的人类偏好。

1.  可验证奖励的强化学习（RLVR）。一种新型的后训练方法，通过更多的 RL 训练来提高可验证领域的性能。

RLHF 存在于并主导着第二个领域，**偏好微调**，由于它通常涉及真实目标的代理奖励模型和更嘈杂的数据，因此比指令微调更复杂。同时，RLHF 比其他流行的语言模型强化学习方法——可验证奖励的强化学习——更为成熟。因此，这本书专注于偏好学习，但为了完全理解 RLHF 的作用，需要使用这些其他训练阶段，因此它们也被详细解释。

当我们考虑我们共同广泛使用的这些方法构建模型的选择和关注时，RLHF 在口语中*就是*导致现代后训练的原因。RLHF 是使 ChatGPT 发布取得巨大成功的技巧，因此在 2023 年初，RLHF 涵盖了后训练领域的大部分兴趣。现在，RLHF 只是后训练的一部分，因此在这本书中，我们解释了为什么早期对 RLHF 如此关注，以及其他方法是如何出现来补充它的。

训练语言模型是一个非常复杂的过程，通常涉及 10 到 100 人的大型技术团队，以及数百万美元的数据和计算成本。这本书旨在实现三个目的，使读者能够掌握如何使用 RLHF 和相关模型来构建领先模型。首先，这本书将大型科技公司中通常隐藏的尖端研究提炼成清晰的主题和权衡，以便读者了解模型是如何制作的。其次，这本书将允许用户设置基本的代码示例，以便他们自己动手微调这些模型。最后，除了教授进行 RLHF 的技术外，这本书旨在提炼关于*为什么*RLHF 对现代人工智能模型至关重要的直觉。

由于 RLHF 的复杂性和最先进的技术通常过于复杂而无法单独完成，这本书专注于使读者具备所需工具，以便他们能够在该领域找到工作和开始研究项目。其他人将只是享受精确理解全球无数讨论的技术核心。一本详细说明如何针对特定需求进行 RLHF 的精确食谱的书是不可能的，这就是为什么有大量公司提供价值数百万美元的 RLHF 培训和相关方法作为服务。尽管如此，人工智能系统的快速进步意味着具备这种知识的读者可以随着时间的推移处理越来越多的自己的训练。

## RLHF 做什么？

关于 RLHF（强化学习和人类反馈）最大的问题，尽管仍然难以回答，那就是“RLHF 训练为模型提供了什么？”近年来，语言模型已经从在基准测试范围内的学术实验转变为通用技术。RLHF 是这个转变的核心。

首先，让我们考虑一个语言模型在自回归预训练结束时如何回答查询。一个强大的模型，Llama 3.1 405B Base，将继续从“The president of the united states in 2006 was”：

> 乔治·W·布什在 2006 年是佛罗里达州的州长，约翰·麦凯恩在 2006 年是亚利桑那州的参议员，后来输给了奥巴马。9 月 1 日——美国总统布什签署一项行政命令，为使用联邦资金支付的社会服务提供更多基于信仰的组织选择。1 月 1 日——美国国会根据《不合法互联网赌博执行法》，即 UIGEA 设定的当前截止日期。搜索：1961 年《保护植物新品种国际公约》的修正案要求植物育种者的权利包括农民的特权。2009 年是公历星期四开始的一年，公元 2009 年，公元纪年（CE）和公元纪年（AD）的年份，第三千年的第 9 年，21 世纪的第 9 年，2000 年代的最后一年。

明确的是，这个模型正在完成句子并添加其他常见的互联网元数据。

关于 RLHF 如何工作的最有说服力的观点是，将风格应用于你与语言模型互动的方式。信息呈现的风格，与语气或格式密切相关，对于学习方式至关重要。风格通常用来描述模型的方式。为了看到差异，考虑以下问题的两个示例回答：

> 帮助，我明天要做一个演讲，我很紧张。

一种直接、低同理心的风格：>你会没事的。练习你的幻灯片几次，睡一觉，停止过度思考。只展示你所知道的内容，然后继续。

更温暖、更支持性的风格：>这听起来真的很紧张，这是一种正常的反应。如果你可以的话，现在就快速练习一下，挑选出你最希望人们记住的一两个要点，然后给自己休息的许可——明天保持清晰和冷静比今晚临时抱佛脚更重要。

另一方面，格式是答案的结构方式。现代模型已知使用广泛的 Markdown 格式，包括列表、换行符和表情符号。考虑以下问题的两个非常不同的回答：

> 我如何提高我的睡眠质量？

从早期的语言模型得到的简单回答：

> 尽量每天晚上同一时间上床睡觉，避免白天晚些时候摄入咖啡因，并保持你的卧室安静和黑暗。

或者一个复杂、更丰富的回答：

> 实际上更好的睡眠
> 
> **目标**：使你的日程可预测，环境无聊到足以让你的大脑放弃。
> 
> **今晚（快速胜利）**：1. 睡前最后一小时调暗灯光。2. 午饭后不喝咖啡；酒精量最小。3. 如果你醒来无法入睡，起床做一些安静的事情，直到再次困倦。
> 
> **日常基础（影响最大）**：• 同样的醒着时间（即使是周末）→ 锚定你的生物钟。• 早晨光线 5-10 分钟 → 帮助设定昼夜节律。• 冷暗的房间（风扇/眼罩）→ 更容易保持睡眠。
> 
> **如果你想要一个简单的规则**：`醒着时间固定 + 咖啡因截止 + 放松程序`
> 
> 如果睡眠问题持续或严重，与临床医生交谈可能是有益的——许多问题都是可以治疗的。

继续上述例子，使用 RLHF（以及其他后训练工具）训练的模型会以非常不同的方式回答问题。当询问 Tülu 3 405B 相同的问题“2006 年美国总统是谁”时，它简洁地回答：

> 乔治·W·布什在 2006 年担任美国总统。他连任两届，从 2001 年 1 月 20 日至 2009 年 1 月 20 日。

这种风格上的变化是本书中描述的后训练方法的结果。指令微调为模型提供在问答格式中可靠响应的基本能力，而 RLHF 则是将这些答案加工成我们现在期望的语言模型提供的可靠、温暖和吸引人的答案。

现代研究已经将 RLHF 确立为一种将微妙风格和相关行为特征整合到模型中的通用方法。与后训练的其他技术，如指令微调相比，RLHF 在各个领域中的泛化能力更强 [[7]](ch021.xhtml#ref-kirk2023understanding) [[8]](ch021.xhtml#ref-chu2025sft) – 有助于创建有效的通用模型。

直观地，这可以从优化技术的应用中看出。指令微调是训练模型在文本前部分接近它所看到的示例时预测下一个特定标记。这是对每个标记的更新。

另一方面，RLHF 在响应级别上调整响应，而不是具体查看下一个标记。此外，它还告诉模型一个*更好的*响应是什么样的，而不仅仅是它应该学习的特定响应。RLHF 还向模型展示它应该避免哪种类型的响应，即负面反馈。实现这一目标的训练通常被称为*对比性*损失函数，并在本书中多次提及。

虽然这种灵活性是 RLHF 的一个主要优势，但它也带来了实施挑战。在很大程度上，这些挑战集中在*如何控制优化*上。正如我们将在本书中讨论的，实现 RLHF 通常需要训练一个奖励模型，其中最佳实践尚未得到充分确立，并且取决于应用领域。因此，优化本身容易发生*过度优化*，因为我们的奖励信号至多是一个代理目标，需要正则化。在这些限制下，有效的 RLHF 需要一个强大的起点，因此 RLHF 不能单独解决所有问题，需要从更广泛的训练后视角来考虑。

由于这种复杂性，实现 RLHF 的成本远高于简单的指令微调，并且可能带来意外的挑战，例如长度偏差 [[9]](ch021.xhtml#ref-singhal2023long) [[10]](ch021.xhtml#ref-park2024disentangling)。对于绝对性能至关重要的模型训练工作，RLHF 已被确立为实现强大微调模型的关键，但它在计算、数据成本和时间上的成本更高。在 ChatGPT 之后的 RLHF 早期历史中，有许多研究论文展示了通过有限的指令微调对 RLHF 的近似解决方案，但随着文献的成熟，它一次又一次地被重复强调，RLHF 和相关方法是模型性能的核心阶段，不能迅速放弃。

## 对训练后的直觉

我们已经确定，RLHF 特别是训练后一般对于最新模型的性能至关重要，以及它如何改变模型的输出，但并没有解释为什么它有效。这里有一个简单的类比，说明了如何在任何基础模型之上通过基准测试获得如此多的收益。

我描述的训练后潜力的方式被称为训练后的诱发解释，其中我们所做的一切就是通过放大基础模型中的有价值行为来提取潜力。

为了使这个例子更具说服力，我们将基础模型——即从大规模、下一 token 预测预训练中产生的语言模型——与其他构建复杂系统的基础组件进行类比。我们以汽车底盘为例，它定义了汽车可以围绕其建造的空间。考虑一级方程式（F1），大多数车队在年初都会带来一款新的底盘和引擎。然后，他们全年都在进行空气动力学和系统变化（当然，这是一个轻微的简化），可以显著提高汽车的性能。最好的 F1 车队在一季度的改进比底盘到底盘的改进还要多。

对于后训练来说也是如此，随着他们对其怪癖和倾向了解得更多，可以从静态基础模型中提取大量的性能。最好的后训练团队在非常短的时间内提取了大量的性能。这些技术是在大多数预训练结束之后的一切。它包括“中期训练”如退火/高质量的预训练结束时的网络数据、指令调整、RLVR、偏好调整等。一个很好的例子是从艾伦人工智能研究所的第一个版本的全开放、小型混合专家（MoE）模型 OLMoE Instruct 到第二个版本的变化。第一个模型于 2024 年秋季发布 [[11]](ch021.xhtml#ref-muennighoff2024olmoe)，而第二个版本只更新了后训练，在流行的基准测试中的评估平均分从 35 提高到 48，而没有改变大多数预训练 [[12]](ch021.xhtml#ref-ai2_olmoe_ios_2025)。

这个想法是，基础模型中有很多智能和能力，但由于它们只能通过下一个标记的预测来回答，而不能以问答格式回答，因此需要大量的工作来围绕它们构建，通过后训练，才能制作出优秀的最终模型。

然后，当你看到像 OpenAI 在 2025 年 2 月发布的 GPT-4.5 这样的模型时，它由于基础模型太大，无法服务于数百万用户，而主要是一个消费产品的失败，你可以将其视为 OpenAI 建立的基础更加动态和令人兴奋。有了这个直觉，基础模型决定了最终模型的大部分潜力，后训练的工作就是培养所有这些潜力。

我将这个直觉描述为后训练的揭示理论。这个理论与现实相符，即用户看到的大多数收益来自后训练，因为它暗示了在互联网上预训练的模型中存在比我们能够通过简单方式（例如，在早期后训练类型中（即仅指令调整）重复传递某些狭窄样本）教给模型的更多潜在潜力。后训练的挑战是将模型从下一个标记的预测重塑为对话问答，同时从预训练中提取所有这些知识和智能。

与这个理论相关的一个想法是浅层对齐假设，在论文 LIMA：对齐的“少即是多”[[13]](ch021.xhtml#ref-zhou2023lima)中被提出。这篇论文在大局上对一些重要的直觉理解正确，但原因不正确。作者表示：

> 模型的知识和能力几乎完全是在预训练期间学习的，而对齐则教会它在与用户交互时应该使用哪种格式的子分布。如果这个假设是正确的，并且对齐主要关于学习风格，那么浅层对齐假设的一个推论是，人们可以用一个非常小的示例集足够地调整预训练的语言模型 [Kirstain 等人，2021]。

深度学习的所有成功都应该教会你一个深刻的信念，即扩展数据对性能很重要。在这里，主要的不同之处在于，作者们正在讨论一致性和风格，这是当时学术训练的重点。用几千个样本进行指令微调，你可以显著改变模型，并提高一系列评估，如 AlpacaEval、MT Bench、ChatBotArena 等。这些并不总是转化为更具挑战性的能力，这就是为什么 Meta 不会只在这个数据集上训练其 Llama Chat 模型。学术结果有教训，但如果你试图理解技术弧的大图景，需要仔细解读。

这篇论文展示的是，你可以用少量样本显著改变模型。我们知道这一点，这对新模型的短期适应很重要，但他们对性能的论点让普通读者得到了错误的教训。

如果我们改变数据，对模型性能和行为的影响可能会更高，但这远非“表面”。今天的基础语言模型（没有训练后）可以用强化学习训练一些数学问题，学会输出完整的思维推理链，然后在 BigBenchHard、Zebra Logic、AIME 等完整的推理评估套件中得分更高。

表面一致性假设是错误的，这与认为 RLHF 和训练后只是感觉的人仍然错误的原因相同。这是我们在 2023 年必须克服的整个领域的教训（许多 AI 观察者仍然根深蒂固）。训练后已经远远超出了这一点，我们开始看到模型风格是在行为之上运作的——例如现在流行的长链思维。

## 我们是如何到达这里的

为什么这本书现在有意义？还会发生多少变化？

训练完成后，从原始预训练语言模型中激发强大行为的技术，自从 ChatGPT 发布并重新点燃了对 RLHF（强化学习和人类反馈）兴趣以来，已经经历了许多季节和情绪。在 Alpaca [[14]](ch021.xhtml#ref-alpaca)、Vicuna [[15]](ch021.xhtml#ref-vicuna2023)、Koala [[16]](ch021.xhtml#ref-koala_blogpost_2023)和 Dolly [[17]](ch021.xhtml#ref-DatabricksBlog2023DollyV1)的时代，使用少量人类数据点和以 Self-Instruct 风格扩展的合成数据，通常用于微调原始 LLaMA，以获得与 ChatGPT 相似的行为。这些早期模型的基准是全面的感觉（以及人类评估），因为我们所有人都被这些小型模型在各个领域展现出的令人印象深刻的行为所吸引。这是合理的兴奋。

开放式训练后的发展速度更快，发布更多模型，比其封闭式对手产生更多噪音。公司正在争先恐后，例如 DeepMind 与 Google 合并或被启动，并花时间跟进。开放食谱有高潮和低谷的阶段。

Alpaca 等人之后的时代，开放食谱的第一个滞后期，是一个由对人类反馈强化学习（RLHF）的怀疑和怀疑所定义的时代，这是 OpenAI 强调对第一个 ChatGPT 成功至关重要的技术。许多公司怀疑他们是否需要进行 RLHF。一个常见的说法——“指令调整就足以实现对齐”——当时非常流行，尽管有明显的压力反对它，但至今仍具有很大的影响力。

这种对 RLHF 的怀疑持续存在，尤其是在开放领域，因为团体无法承担 10 万美元至 100 万美元的数据预算。早期接受它的公司最终取得了胜利。Anthropic 通过 2022 年发布了关于 RLHF 的大量研究，现在有人认为它拥有最好的后训练 18519。开放团体在努力复制，甚至了解基本封闭技术之间的差距是一个常见的主题。

开放对齐方法和后训练的第一个转变是直接偏好优化（DPO）20 的故事，它表明你可以通过直接在成对偏好数据上采取梯度步骤来解决与 RLHF 相同的优化问题。2023 年 5 月发布的 DPO 论文，在 2023 年秋季之前并没有任何使用它训练的明显有影响力的模型。但随着几个突破性 DPO 模型的发布——所有这些都依赖于找到更好、更低的学习率——这一情况发生了变化。Zephyr-Beta 21、Tülu 2 22 以及许多其他模型表明，后训练的 DPO 时代已经开始了。Chris Manning 甚至感谢我“拯救了 DPO”。

自 2023 年底以来，为了满足发布良好模型的最低要求，你需要进行偏好调整。DPO 时代一直持续到 2024 年，以算法的不断变体形式出现，但我们已经非常深入地陷入了另一个开放食谱的低迷期。开放的训练后食谱已经饱和了可用的知识和资源。

在 Zephyr 和 Tulu 2 一年后，同样的突破性数据集 UltraFeedback 在开放食谱中的偏好调整方面可以说是仍然处于最前沿 23。

同时，Llama 3.1 24 和 Nemotron 4 340B 25 的报告给我们提供了实质性的线索，表明大规模的后训练要复杂得多，影响也更大。封闭实验室正在进行全面的后训练——一个包括指令调整、RLHF、提示设计等的大型多阶段过程——而学术论文只是触及了表面。Tülu 3 代表了构建未来学术后训练研究基础的全面、开放的努力 6。

现在，训练后期是一个复杂的过程，涉及上述训练目标，以不同的顺序应用，以针对特定的能力。本书旨在提供一个平台来理解所有这些技术，在未来几年，如何交错使用这些技术的最佳实践将逐渐显现。

在训练后期的创新主要领域现在包括强化微调、推理训练和相关理念。这些新方法在 RLHF 的基础设施和理念上进行了大量构建，但发展速度却远快得多。本书旨在捕捉 RLHF 初期快速变化之后的第一个稳定文献。

## 本书范围

本书希望涉及 RLHF 标准实现的核心步骤的每一个。它不会涵盖所有组件的历史或最近的研究方法，只涉及已被证明反复出现的技术、问题和权衡。

### 章节摘要

本书包含以下章节：

#### 介绍

全书有用的参考资料。

1.  引言：RLHF 概述及本书提供的内容。

1.  基础（近期）作品：RLHF 技术历史上的关键模型和论文。

1.  定义：本书中使用的 RL、语言建模和其他 ML 技术的数学定义。

#### 问题设置与背景

RLHF 试图解决的宏观问题的背景。

1.  RLHF 训练概述：RLHF 的训练目标是如何设计的，以及理解它的基础知识。

1.  偏好是什么？：为什么需要人类偏好数据来推动和了解 RLHF。

1.  偏好数据：如何收集 RLHF 的偏好数据。

#### 优化工具

用于优化语言模型以对齐人类偏好的技术套件。这是对解决前几章提出的问题可以使用的技术的一种连续展示。

1.  奖励建模：从偏好数据中训练奖励模型，作为 RL 训练的优化目标（或用于数据过滤）。

1.  正则化：工具，用于约束这些优化工具到参数空间的有效区域。

1.  指令调整：调整语言模型以适应问答格式。

1.  拒绝采样：使用奖励模型和指令调整来对齐模型的基本技术。

1.  强化学习（即策略梯度）：在 RLHF 中用于优化奖励模型（和其他信号）的核心 RL 技术。

1.  直接对齐算法：从成对偏好数据优化 RLHF 目标方向而不是首先学习奖励模型的算法。

#### 高级

新的 RLHF 技术和讨论，虽然尚未明确建立，但对当前一代模型很重要。

1.  宪法 AI 和 AI 反馈：AI 反馈数据和旨在模拟人类偏好评分的特定模型的工作方式。

1.  推理和强化微调：新 RL 训练方法在推理时间相对于后训练和 RLHF 的缩放作用。

1.  工具使用和函数调用：训练模型在输出中调用函数或工具的基本原理。

1.  合成数据：从人类数据转向合成数据，以及如何从其他模型中提取。

1.  评估：评估（和提示）在语言模型中不断演变的作用。

#### 开放性问题

长期使用 RLHF 的演变中的基本问题和讨论。

1.  过度优化：关于 RLHF 为何出错以及为什么在奖励模型中具有软优化目标的过度优化是不可避免的定性观察。

1.  风格和信息：由于风格在信息共享中的关键作用，RLHF 在改善模型用户体验方面的作用常常被低估。

1.  产品、UX、角色：随着主要人工智能实验室使用 RLHF 微妙地匹配其模型与产品，RLHF 的应用范围正在发生变化。

### 目标受众

本书旨在面向具有语言建模、强化学习和通用机器学习入门级经验的受众。它不会对所有技术进行详尽的文档记录，而只是那些对理解 RLHF 至关重要的技术。

### 如何使用本书

这本书的大部分内容是因缺乏 RLHF 工作流程中重要主题的权威参考而创作的。本书的贡献旨在为您提供尝试玩具实现或深入研究文献所需的最基本知识。这不是一本全面的教学课本，而是一本用于提醒和入门的快速指南。此外，鉴于本书以网络优先的特性，预期会有一些小错误和随机的进展——请通过修复错误或建议重要内容在[GitHub](https://github.com/natolambert/rlhf-book)上做出贡献。

### 关于作者

Nathan Lambert 博士是一位 RLHF 研究人员，为语言模型微调的开放科学做出了贡献。他在艾伦人工智能研究所（Ai2）和 HuggingFace 的任职期间发布了许多使用 RLHF 训练的模型、其后续数据集和训练代码库。例如包括[Zephyr-Beta](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)、[Tulu 2](https://huggingface.co/allenai/tulu-2-dpo-70b)、[OLMo](https://huggingface.co/allenai/OLMo-7B-Instruct)、[TRL](https://github.com/huggingface/trl)、[Open Instruct](https://github.com/allenai/open-instruct)以及许多其他模型。他在 RLHF 方面发表了大量作品，包括[许多博客文章](https://www.interconnects.ai/t/rlhf)和[学术论文](https://scholar.google.com/citations?hl=en&user=O4jW7BsAAAAJ&view_op=list_works&sortby=pubdate)。

## RLHF 的未来

随着对语言模型的投入，许多基于传统 RLHF 方法的变化出现了。RLHF 通俗地说已经成为多种重叠方法的同义词。RLHF 是偏好微调（PreFT）技术的一个子集，包括直接对齐算法（见第十二章），这是一类下游方法，通过直接在偏好数据上采取梯度步骤来解决偏好学习问题，而不是学习一个中间奖励模型。RLHF 是与语言模型“后训练”快速进展最相关的工具，这包括在主要基于网络数据的大规模自回归训练之后的所有训练。这本教科书是 RLHF 及其直接邻近方法（如指令调整和其他设置 RLHF 训练模型所需的具体实现细节）的广泛概述。

随着使用强化学习（RL）微调语言模型的更多成功案例出现，例如 OpenAI 的 o1 推理模型，强化学习与人类反馈（RLHF）将被视为连接进一步投资强化学习方法以微调大型基础模型的桥梁。同时，尽管在不久的将来，关注的焦点可能更加集中在 RLHF 中的 RL 部分——作为一种在有价值任务上最大化性能的方法——RLHF 的核心在于它是一个研究现代人工智能形式面临的重大问题的视角。我们如何将人类价值观和目标复杂性映射到我们日常使用的系统中？这本书希望成为数十年来关于这些问题的研究和经验教训的基础。
