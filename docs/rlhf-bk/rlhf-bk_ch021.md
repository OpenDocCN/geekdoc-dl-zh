# 参考文献

[1]P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei, “从人类偏好中进行深度强化学习”，*神经信息处理系统进展*，第 30 卷，2017 年。[2]N. Stiennon 等人，*利用人类反馈进行总结学习*，*神经信息处理系统进展*，第 33 卷，第 3008-3021 页，2020 年。[3]L. Ouyang 等人，*在人类反馈下训练语言模型以遵循指令*，*神经信息处理系统进展*，第 35 卷，第 27730-27744 页，2022 年。[4]R. Nakano 等人，*Webgpt：带有人类反馈的浏览器辅助问答*，*arXiv 预印本 arXiv:2112.09332*，2021 年。[5]Y. Bai 等人，*通过人类反馈的强化学习训练一个有用且无害的助手*，*arXiv 预印本 arXiv:2204.05862*，2022 年。[6]N. Lambert 等人，*ULU 3：推动开放语言模型后训练的边界*，*arXiv 预印本 arXiv:2411.15124*，2024 年。[7]R. Kirk 等人，*理解 RLHF 对 LLM 泛化和多样性的影响*，*arXiv 预印本 arXiv:2310.06452*，2023 年。[8]T. Chu 等人，*Sft 记住，rl 泛化：基础模型后训练的比较研究*，*arXiv 预印本 arXiv:2501.17161*，2025 年。[9]P. Singhal, T. Goyal, J. Xu, 和 G. Durrett，*RLHF 中的长度相关性研究*，*arXiv 预印本 arXiv:2310.03716*，2023 年。[10]R. Park, R. Rafailov, S. Ermon, 和 C. Finn，*从直接偏好优化中分离长度和质量*，*arXiv 预印本 arXiv:2403.19159*，2024 年。[11]N. Muennighoff 等人，*Olmoe：开放混合专家语言模型*，*arXiv 预印本 arXiv:2409.02060*，2024 年。[12]艾伦人工智能研究所，*OLMoE，遇见 iOS*。[`allenai.org/blog/olmoe-app`](https://allenai.org/blog/olmoe-app)，2025 年。[13]C. Zhou 等人，*Lima：对齐的“少即是多”*，*神经信息处理系统进展*，第 36 卷，第 55006-55021 页，2023 年。[14]R. Taori 等人，*斯坦福 alpaca：遵循指令的 LLaMA 模型*，*GitHub 仓库*。[`github.com/tatsu-lab/stanford_alpaca`](https://github.com/tatsu-lab/stanford_alpaca)；GitHub，2023 年。[15]W.-L. Chiang 等人，*Vicuna：一个开源聊天机器人，以 90%* ChatGPT 质量给 GPT-4 留下深刻印象*。2023 年。[16]X. Geng 等人，*Koala：用于学术研究的对话模型*。博客文章，2023 年。[在线]。可访问：[`bair.berkeley.edu/blog/2023/04/03/koala/`](https://bair.berkeley.edu/blog/2023/04/03/koala/)。[17]M. Conover 等人，*Hello dolly：用开放模型民主化 ChatGPT 的魔力*。[在线]。可访问：[`www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html`](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html)。[18]A. Askell 等人，*作为对齐实验室的一般语言助手*，*arXiv 预印本 arXiv:2112.00861*，2021 年。[19]Y. Bai 等人，*宪法 AI：从 AI 反馈中获得无害性*，*arXiv 预印本 arXiv:2212.08073*
