# llm.c 小书

## 第一章. 导航

### 1\. 什么是 *llm.c*

想象一下，你想要窥探一个现代 AI 模型的内部——不是通过阅读隐藏在巨大框架中的数千行优化后的 C++ 或 CUDA 代码，而是通过打开一个小巧的文件夹，看到整个训练流程展现在你面前。这正是 *llm.c* 给你的。

在其核心，*llm.c* 是一个如何训练和运行 GPT-2 风格语言模型的参考实现，用纯 C（和 CUDA）编写。关键词是 *参考*：代码旨在最小化、可读性和教育性。你不需要通过抽象层或设备特定的宏。相反，你得到的是一个看起来几乎像是伪代码的版本，但仍然可以在你的计算机上编译和运行。

#### 为什么这个项目存在

深度学习框架如 PyTorch 和 TensorFlow 对于快速让模型工作非常出色，但它们隐藏了大部分实际机制。在底层，有很多事情在发生：张量在内存中分配，梯度通过反向传播计算，优化器状态更新，调度器调整学习率。我们大多数人从未看到这些细节，因为框架为我们处理了它们。

*llm.c* 反转了这个概念。它说：*如果我们移除了黑盒，并逐行向你展示 GPT-2 模型的训练过程会怎样？* 这不是关于速度或生产部署。这是关于清晰度、教育和揭示大型语言模型的工作原理。

#### 关键特性

+   极简主义：CPU 版本 (`train_gpt2.c`) 避免了复杂的优化，以便初学者可以理解逻辑。即使是 CUDA 版本也尽量保持简单，只调用必要的 cuBLAS/cuDNN 调用。

+   自包含：无外部框架。代码定义了自己的分词器、数据加载器、优化器和调度器。你需要的一切都在仓库中。

+   与 PyTorch 的相似之处：C/CUDA 实现中的每个函数在 PyTorch 中都有一个对应函数。仓库甚至还附带 Python 测试文件，以证明输出在公差范围内匹配。

+   步骤化可扩展性：你可以从 CPU 上的小型模型开始，一旦你理解了基础知识，就可以切换到 GPU、多 GPU 或甚至多节点训练。结构保持不变，只是更快。

#### 你可以用它做什么

1.  从头开始训练 GPT-2：从一个小的数据集（如 Tiny Shakespeare）开始，看看模型如何学习语言中的模式。

1.  尝试不同的配置：改变层数、序列长度或隐藏大小，然后观察内存和训练时间如何缩放。

1.  学习 GPU 训练内部机制：从 CPU 到 CUDA，再到使用 MPI/NCCL 的多 GPU，看看真正的分布式训练在底层是如何工作的。

1.  性能分析：仓库包括性能分析工具，因此你可以测量 FLOPs、内存带宽和内核执行时间。

1.  重新训练大型模型：有了足够的硬件，你可以实际上使用与 README 中描述的完全相同的设置重新训练 GPT-2 124M 或更大的版本。

#### 你为什么应该关心

如果你曾经使用过大型语言模型，并想 *“这感觉就像魔法”*，*llm.c* 就是你的机会，揭开神秘的面纱。你会看到：

+   文本是如何转换为数字的（分词）。

+   那些数字是如何输入到矩阵中并进行转换的。

+   注意力在代码中实际看起来是怎样的。

+   梯度是如何计算和权重是如何更新的。

+   如何从单个 GPU 到多个 GPU 的扩展进行管理。

它不再是魔法——它是数学加上你可以阅读的代码。

#### 整体图景

将 *llm.c* 视为一个现代 AI 的教学实验室：

+   在第一天，你可以在 CPU 上运行一个小型演示，并看到损失值随着模型的学习而下降。

+   在第十天，你可能会分析 CUDA 内核或调试跨 GPU 的 NCCL 同步。

+   在每一步，代码都易于理解，因为它避免了不必要的复杂性。

这种哲学使 *llm.c* 独特：它不是试图成为最快的或功能最丰富的。它试图成为了解大型语言模型实际工作原理的最清晰窗口。

**总结**：*llm.c* 是 GPT-2 训练和推理的一个最小化、透明的重新实现，旨在教你如何理解语言模型在底层的工作原理。它并不关乎生产性能——而是通过逐个 C 文件，带你参观现代 AI 的机器。

你希望我现在以同样的友好新手语气扩展第二部分（“仓库巡游”），展示每个文件夹和文件的内容吗？

### 2. 仓库巡游

当你第一次打开 *llm.c* 仓库时，它可能看起来像是一堆随机的 `.c`、`.cu`、`.py` 和 shell 脚本。但下面隐藏着清晰的架构。每个部分都有其目的，一旦你知道了事物的位置，在仓库中导航就会感觉自然。让我们来一次有指导的漫步，看看主要的部分。

#### 根目录：入口点

在顶级目录中，你会找到核心训练程序。这些是你实际编译和运行的文件：

+   `train_gpt2.c` - CPU 的参考实现。这是 GPT-2 训练最简单、最易读的版本。它避免了特殊优化，以便你可以逐步跟踪数学和逻辑。

+   `train_gpt2.cu` - CUDA 的实现。更快，使用 GPU 内核、cuBLAS 和可选的 cuDNN FlashAttention。这是你用于严肃训练运行版本。

+   `train_gpt2_fp32.cu` - 一个传统的 CUDA 路径，使用纯 FP32 精度而不是混合精度。它速度较慢，但作为调试基线很有用。

+   `train_gpt2.py` - PyTorch 的参考实现。这是一个微型 Python/PyTorch 脚本，用于训练相同的 GPT-2，以便你可以比较输出并验证正确性。

其他重要的根级文件：

+   `Makefile` - 定义了如何构建不同版本。例如，`make train_gpt2` 或 `make train_gpt2cu` 是你的入口点。

+   `README.md` - 运行实验、安装依赖项和重现模型的主要指南。

#### `llmc/` 目录：工具和构建块

这个文件夹包含可重用的 C 工具，主训练文件包括：

+   `utils.h` - 安全包装（`fopenCheck`、`mallocCheck`）和辅助函数。

+   `tokenizer.h` - 使用 C 语言实现了 GPT-2 的分词器：将文本编码成标记 ID，并解码回文本。

+   `dataloader.h` - 定义了如何加载和提供训练批次，处理数据集拆分和迭代。

+   `rand.h` - 随机数实用工具，类似于 PyTorch 的 `manual_seed` 和正态分布。

+   `schedulers.h` - 学习率调度，如带有预热周期的余弦衰减。

+   `sampler.h` - 实现文本生成中的 softmax 抽样和辅助随机数生成器。

+   `logger.h` - 最小化日志功能，用于跟踪进度。

将 `llmc/` 视为保持主要文件整洁和可读性的库。而不是在 `train_gpt2.c` 中添加辅助函数，这里一切都是模块化的。

#### `dev/` 目录：脚本和附加内容

这个文件夹充满了支持工具，使实验更容易进行：

+   `dev/download_starter_pack.sh` - 获取 GPT-2 124M 权重、分词器和数据集。这是快速开始的最快方式。

+   `dev/data/` - 包含准备数据集（如 Tiny Shakespeare 或 OpenWebText）的脚本，这些数据集是以二进制格式提供的，符合 *llm.c* 的期望。

+   `dev/cuda/` - 这是一个用于实验独立 CUDA 内核的地方。如果你想对主训练器之外的定制 GPU 代码进行修改，你会去这里。

#### `doc/` 目录：学习资源

深入特定主题的文档。例如：

+   `doc/layernorm/layernorm.md` - 一篇教程风格的解释，包括数学和代码，帮助你理解 GPT-2 的核心组件之一，在深入 C 语言实现之前。

这个文件夹是一个学习辅助工具。每当一个概念感觉过于复杂时，可以在这里找到更温和的讲解。

#### 测试文件

在 *llm.c* 中，测试被认真对待，因为目标是证明 C/CUDA 实现与 PyTorch 相比是正确的：

+   `test_gpt2.c` - 在 CPU 上运行前向传递和训练步骤，并将输出与 PyTorch 进行比较。

+   `test_gpt2cu.cu` - 与此类似，但针对 CUDA，包括 FP32 和混合精度运行。

这些文件确保一切真实：你可以始终验证你的构建产生的结果与官方 PyTorch 模型相同。

#### 性能分析工具

对于性能深入研究：

+   `profile_gpt2.cu` - 一个 CUDA 性能分析工具，用于基准测试内核并测量吞吐量。

+   `profile_gpt2cu.py` - Python 端的性能分析器，用于分析 GPU 利用率、内存带宽和浮点运算次数。

如果你好奇训练过程中时间都花在哪里，这些文件会告诉你如何进行测量。

#### 数据集和工件

当你运行 `download_starter_pack.sh` 时，你会得到：

+   `gpt2_tokenizer.bin` - GPT-2 的字节对编码分词器，以二进制序列化。

+   数据集 `.bin` 文件 - 训练和验证集，分词并准备好供数据加载器使用。

这些文件默认不在仓库中，但会下载或本地生成。

#### 组装起来

仓库的结构就像一个教学实验室：

+   根文件是主要实验。

+   `llmc/` 是构建模块的库。

+   `dev/` 提供了额外的工具和脚本。

+   `doc/` 以教程形式解释复杂的概念。

+   测试和性能分析器确保一切与 PyTorch 一致且运行高效。

一旦您看到这种模式，仓库看起来就不那么令人生畏了。每个文件都在讲述一个故事，说明如何从头开始使用 C 和 CUDA 构建一个 GPT-2 模型。

### 3. Makefile 目标和标志

每个 C 或 CUDA 程序都需要一个构建系统，在 *llm.c* 中，这个角色由一个简单但强大的 Makefile 处理。如果您以前从未使用过 `make`，可以将其视为一本食谱书：您在终端中输入 `make <target>`，然后它将遵循指令将代码编译成可执行文件。在 *llm.c* 中，此文件是您选择要构建哪个训练器、是否启用 GPU 以及要开启哪些可选功能的控制中心。

#### 为什么需要 Makefile？

而不是记住带有数十个标志的长 `gcc` 或 `nvcc` 编译命令，Makefile 一次捕获这些指令并给它们一个简短的名称。例如，构建 CPU 训练器就像这样：

```c
[](#cb2-1)make train_gpt2
```

在幕后，这调用 `gcc`，设置优化标志，包含正确的头文件，并将所有内容链接在一起。CUDA 构建 `nvcc` 也适用同样的方法。

#### 核心目标

这里是您会发现的最重要的构建目标：

+   `train_gpt2` - 构建仅使用 CPU 的参考训练器。使用 `gcc`（或 `clang`）并链接 OpenMP 以实现并行循环。

+   `train_gpt2cu` - 使用混合精度和可选的 cuDNN FlashAttention 构建 CUDA 训练器。使用 `nvcc`。

+   `train_gpt2_fp32` - 构建保持纯 FP32（较慢但更简单）的遗留 CUDA 训练器。

+   `test_gpt2` - 编译 CPU 测试程序以与 PyTorch 进行结果比较。

+   `test_gpt2cu` - 编译 CUDA 测试程序以检查与 PyTorch 的 GPU 一致性。

+   `profile_gpt2.cu` - 编译 CUDA 性能分析工具包，用于基准测试内核和 FLOPs。

这些中的每一个都会生成一个可以直接运行的二进制文件，例如：

```c
[](#cb3-1)./train_gpt2
[](#cb3-2)./train_gpt2cu
[](#cb3-3)./test_gpt2
```

#### 您可以切换的关键标志

Makefile 还暴露了几个开关，允许您自定义构建。您在运行 `make` 时设置它们，如下所示：

```c
[](#cb4-1)make train_gpt2cu USE_CUDNN=1
```

这里是最重要的标志：

+   `USE_CUDNN` - 如果您的系统已安装 cuDNN，则启用 cuDNN FlashAttention。这可以为注意力机制提供很大的速度提升，但这是可选的。默认情况下，它是关闭的。

+   `OMP=1` - 告诉 CPU 训练器使用 OpenMP 启用编译。这允许多线程执行，使 CPU 运行速度大大提高。通常在检测到 OpenMP 时默认开启。

+   `DEBUG=1` - 使用调试符号（`-g`）而不是最大优化进行编译。在 IDE 中逐步执行代码或使用调试器时很有用。

+   `PROFILE=1` - 添加性能分析钩子，帮助您分析执行时间和性能。

#### 优化选择

默认构建使用 `-O3` 优化，这使得代码运行速度快，但有时更难调试。如果您正在学习并希望清晰，可以切换到：

```c
[](#cb5-1)make train_gpt2 DEBUG=1
```

这会创建一个运行速度较慢的二进制文件，但让你可以在调试器中逐行执行。对于性能基准测试，请坚持使用优化的默认设置。

#### 多 GPU 和 MPI 支持

在构建 CUDA 训练器时，如果已安装，Makefile 也可以链接到 MPI 和 NCCL。这就是实现多 GPU 和多节点训练的原因。通常你不需要更改任何东西——Makefile 会自动检测这些库，并在可用时包含它们。

#### 整合所有内容

将 Makefile 视为整个项目的交换板：

+   想要运行简单的 CPU 演示吗？→ `make train_gpt2`

+   想要在 GPU 上更快地训练吗？→ `make train_gpt2cu`

+   想要调试内核？→ `make train_gpt2cu DEBUG=1`

+   想要测试与 PyTorch 的兼容性吗？→ `make test_gpt2` 或 `make test_gpt2cu`

只需几个按键，你就可以控制是运行一个适合初学者的 CPU 演示、高性能 GPU 构建还是调试会话。

吸取的教训：Makefile 是你的控制中心。它抽象了复杂的编译器命令，并为你提供了一个干净的选项菜单：CPU vs GPU、FP32 vs 混合精度、调试 vs 优化、单 GPU vs 多 GPU。掌握它是舒适地在*llm.c*中进行实验的第一步。

### 4. 快速入门：CPU 参考路径（`train_gpt2.c`）

开始探索 *llm.c* 的最简单方式是使用仅 CPU 的参考实现。这个文件，`train_gpt2.c`，被故意设计成最小化、可读和易于接近。它不会在库或宏后面隐藏复杂性。相反，它展示了如何一步一步地使用纯 C 和一点 OpenMP 来加速训练一个 GPT-2 模型。

#### 为什么从 CPU 开始？

+   首先清晰：GPU 增加了复杂性层次（CUDA 内核、内存传输、cuBLAS）。在 CPU 上，你可以专注于核心算法，不受干扰。

+   可移植性：任何有 C 编译器的机器都可以运行它——不需要特殊硬件。

+   可调试性：错误更容易追踪，你可以在 IDE 中单步执行代码。

CPU 版本较慢，但在这里这是一个特性——它迫使你真正看到底层发生了什么。

#### 构建 CPU 训练器

从存储库的根目录，你只需输入：

```c
[](#cb6-1)make train_gpt2
```

这会将`train_gpt2.c`编译成一个名为`train_gpt2`的可执行文件。如果你的系统有 OpenMP，Makefile 会检测到它并添加正确的标志。

#### 运行你的第一次训练运行

在运行之前，下载启动包（分词器、数据集、配置）：

```c
[](#cb7-1)./dev/download_starter_pack.sh
```

现在开始训练：

```c
[](#cb8-1)./train_gpt2
```

你会看到如下输出：

```c
[GPT-2]
max_seq_len: 1024
vocab_size: 50257
padded_vocab_size: 50304
num_layers: 12
num_heads: 12
channels: 768
num_parameters: 124475904
train dataset num_batches: 1192
val dataset num_batches: 128
num_activations: 73347840
val loss 5.325529
step 0: train loss 4.677779 (took 1987.546000 ms)
step 1: train loss 5.191576 (took 1927.230000 ms)
...
```

每一行都会告诉你：

+   模型大小和配置（序列长度、词汇量大小、层、头、通道）。

+   数据集统计（训练和验证的批次数量）。

+   激活内存大小（衡量中间状态大小的指标）。

+   训练进度（步骤编号、训练损失、验证损失、每步时间）。

#### 在训练循环内部

虽然你目前不需要深入研究代码，但这里是在`train_gpt2.c`中的高级流程：

1.  加载分词器和数据集 → 将文本转换为标记。

1.  初始化模型参数 → 嵌入、注意力权重、MLP、归一化。

1.  对于每个批次：

    +   前向传播 → 计算 logits 和损失。

    +   反向传播 → 计算梯度。

    +   更新参数 → 优化器步骤。

1.  记录进度 → 打印损失，偶尔运行验证。

这与 PyTorch 中发生的情况完全一致，只是用 C 语言表述出来。

#### 性能注意事项

在 CPU 上，不要期望速度。训练 GPT-2 124M 可能需要几天或几周。但这不是重点。CPU 参考路径就像一个玻璃盒子：一切尽收眼底，没有捷径。你将使用它来学习机制，并验证你的 GPU 运行是否与相同的结果匹配。

如果你想要稍微加快速度，你可以：

+   增加 OpenMP 线程：

    ```c
    [](#cb10-1)OMP_NUM_THREADS=8 ./train_gpt2
    ```

+   使用更小的数据集（Tiny Shakespeare）以看到更快的进展。

+   通过更改配置值（减少层、减小通道）来减小模型大小。

#### 何时继续前进

一旦你对 CPU 上的训练外观感到舒适——损失值下降、检查点写入、日志出现——你就可以准备过渡到 GPU 版本（`train_gpt2.cu`）。在那里，性能和扩展性变得重要，但 CPU 运行为你提供了概念基础。

吸取的教训：运行`train_gpt2.c`是你在`llm.c`中第一次亲身体验 GPT-2 训练。它速度较慢、透明度高，且旨在学习。在深入 CUDA 的复杂性之前，你将一步一步地看到模型中的每一部分是如何工作的。

### 5. 快速入门：1-GPU 旧版路径（`train_gpt2_fp32.cu`）

一旦你看到了 CPU 训练器的实际运行，下一步自然的步骤就是尝试在 GPU 上训练。文件`train_gpt2_fp32.cu`是最简单的 GPU 入口点。它早于更高级的混合精度训练器（`train_gpt2.cu`），并且以全 32 位浮点数（FP32）精度运行所有内容。这使得它更容易跟踪和调试，尽管它比现代方法慢。把它看作是`llm.c`中 GPU 训练的“训练轮”。

#### 为什么这条路径存在

现代 GPU 训练几乎总是使用混合精度（FP16/BF16 以提高速度和节省内存，FP32 以保证稳定性）。但混合精度引入了额外的复杂性：缩放损失、维护主权重、检查溢出。对于初学者来说，所有这些都可能分散注意力。

FP32 路径避免了这些复杂性：

+   每个张量（激活、权重、梯度）都存储为 32 位浮点数。

+   不需要特殊处理损失缩放。

+   使用 PyTorch 调试不匹配是直接的。

代价是性能——这个版本运行速度明显较慢，且使用更多内存。

#### 构建 FP32 CUDA 训练器

从存储库的根目录：

```c
[](#cb11-1)make train_gpt2_fp32
```

这将调用`nvcc`（NVIDIA CUDA 编译器）并链接 cuBLAS 以进行矩阵乘法。输出是一个名为`train_gpt2_fp32`的可执行文件。

#### 运行它

就像 CPU 版本一样，确保你首先下载了启动包：

```c
[](#cb12-1)./dev/download_starter_pack.sh
```

然后在你的 GPU 上启动训练：

```c
[](#cb13-1)./train_gpt2_fp32
```

如果 CUDA 安装正确，程序将检测到你的 GPU 并开始训练。你会看到类似于 CPU 训练器的日志，但步骤时间要短得多。例如，在 CPU 上花费约 2 秒的训练步骤可能在 GPU 上只需要约 50 毫秒。

#### 内部机制

虽然从表面上看训练循环看起来相同，但在 GPU 上运行时，内部有很多变化：

1.  张量分配在 GPU 内存中（不是系统 RAM）。

1.  矩阵乘法（注意力和 MLP 层的核心）由 cuBLAS 执行，这是 NVIDIA 的高性能线性代数库。

1.  元素级操作的内核（如添加残差、应用 softmax 或归一化）是用 CUDA 编写的或使用内置原语。

1.  梯度和优化器状态完全在设备上更新，CPU↔GPU 传输最小化。

这使得训练速度显著提高，但与 CPU 版本相比，代码结构仍然可识别。

#### 何时使用 FP32 与混合精度

+   当使用 FP32（此路径）时：

    +   你正在逐步学习 GPU 训练的工作原理。

    +   你想要与 CPU 训练器进行干净的比较。

    +   你在调试正确性问题时无需担心损失缩放。

+   当使用混合精度（`train_gpt2.cu`）时：

    +   你想要真正的性能（2–4×更快的训练）。

    +   你正在训练更大的模型（774M，1.6B 个参数），内存效率很重要。

    +   你旨在在现代 GPU 上重现已发布的 GPT-2 运行。

#### 常见陷阱

1.  CUDA 未安装 → 如果找不到`nvcc`，Makefile 将失败。你需要安装 CUDA Toolkit。

1.  驱动程序不匹配 → 你的 NVIDIA 驱动程序必须与 CUDA 版本匹配。

1.  内存不足错误 → FP32 使用更多的 GPU 内存，因此如果你在较小的 GPU 上，你可能需要降低批处理大小。

#### 为什么这一步很重要

FP32 训练器就像一座桥梁：

+   一边是 CPU 参考路径，慢但清晰。

+   另一边是混合精度 CUDA 路径，速度快但更复杂。

通过走过这座桥，你可以在不被优化所淹没的情况下学习 GPU 加速的工作原理。

吸取教训：`train_gpt2_fp32.cu`是你在*llm.c*中第一次尝试真正的 GPU 训练。它跳过了高级技巧，并展示了干净、单 GPU、全精度实现。它不是最快的，但它是理解训练如何从 CPU 迁移到 GPU 的最友好方式。

### 6. 快速入门：现代 CUDA 路径（`train_gpt2.cu`）

这是大多数人日常使用的高性能训练器。它运行在单个 NVIDIA GPU 上（也是多 GPU 的基础），使用混合精度（在安全的情况下使用 FP16/BF16，在需要时使用 FP32），并可选择启用 cuDNN FlashAttention 以实现快速注意力。与 FP32 传统路径相比，它速度更快，内存使用更少，同时保持训练循环易于跟踪。

#### “混合精度”是什么意思（用简单的话说）

+   权重与激活：以 FP16 或 BF16 的速度和较低的内存存储/处理。

+   主权重：用于稳定更新的 FP32 参数副本。

+   损失缩放：在反向之前乘以损失以避免下溢；在优化器步骤之前未缩放梯度。

+   类似于 Autocast 的行为：代码为每个操作选择安全的 dtype（张量核心中的 GEMMs，FP32 中的减少等）。

您可以在许多 GPU 上获得 2-4 倍的速度提升，并且当配置正确时，最终精度相同。

#### 构建现代 CUDA 训练器

```c
[](#cb14-1)make train_gpt2cu
```

常见变体：

+   如果有 cuDNN FlashAttention（如果可用）：

    ```c
    [](#cb15-1)make train_gpt2cu USE_CUDNN=1
    ```

+   使用调试符号（较慢，但更容易逐步执行）：

    ```c
    [](#cb16-1)make train_gpt2cu DEBUG=1
    ```

这将生成一个名为`train_gpt2cu`的可执行文件。

#### 一次性数据和工件

如果您还没有：

```c
[](#cb17-1)./dev/download_starter_pack.sh
```

这将获取分词器和一个小型数据集，以便您可以立即运行。

#### 运行您的第一个 GPU 训练会话

```c
[](#cb18-1)./train_gpt2cu
```

您应该看到一个配置标题（模型维度、词汇、序列长度），然后是逐步损失打印。步骤时间将比 CPU 短得多，并且比 FP32 明显快，尤其是在张量核心 GPU（Turing 和更新的）上。

立即使用的速度提示：

+   如果内存允许，使用更大的全局批量大小可以提高 GPU 利用率。

+   设置任何 CPU 预处理的环境线程：

    ```c
    [](#cb19-1)OMP_NUM_THREADS=8 ./train_gpt2cu
    ```

#### 与 FP32 相比，底层有什么不同

+   张量核心：GEMMs 通过 cuBLAS/cuBLASLt 在 FP16/BF16 路径上运行，以实现大吞吐量。

+   缩放损失与未缩放传递：正向计算损失，乘以缩放因子；反向更新前，将梯度除以相同的因子。

+   主 FP32 复制：优化器（AdamW）更新此复制，然后将其转换为低精度以进行下一个正向操作。

+   混合/快速注意力（可选）：通过`USE_CUDNN=1`，注意力可能通过 cuDNN FlashAttention 后端路由。

您仍然可以识别出相同的循环：加载批量 → 正向 → 损失 → 反向 → AdamW 步骤 → 记录。

#### 选择 FP16 与 BF16

+   FP16：最佳速度，需要损失缩放；广泛支持。

+   BF16：更易于数值处理（通常需要少量/无缩放），需要硬件支持（Ampere+）；比 FP16 略大的内存，但通常更简单。

训练器选择您的 GPU 支持的或代码默认的；如果您想强制使用一个，稍后可以公开一个标志。

#### 常见命令模式

+   小型 GPU（较少的 VRAM）：

    ```c
    [](#cb20-1)./train_gpt2cu --batch_size 4 --micro_batch_size 1 --seq_len 512
    ```

+   使用余弦调度加快预热：

    ```c
    [](#cb21-1)./train_gpt2cu --warmup_steps 1000 --lr 6e-4 --scheduler cosine
    ```

+   定期评估以进行合理性检查：

    ```c
    [](#cb22-1)./train_gpt2cu --eval_interval 200 --eval_batches 50
    ```

（上面的标志名称反映了典型模式；根据二进制文件的打印帮助进行调整。）

#### 验证正确性（强烈推荐）

+   运行 CUDA 测试二进制文件，以比较 PyTorch 参考在小型批量上的表现：

    ```c
    [](#cb23-1)make test_gpt2cu
    [](#cb23-2)./test_gpt2cu
    ```

+   检查 logits/loss 在小的公差范围内是否匹配。如果发生不匹配，请在不进行优化或禁用 cuDNN 快速路径（`USE_CUDNN=0`）的情况下重新编译，以隔离问题。

#### 启用 FlashAttention（当可用时）

```c
[](#cb24-1)make train_gpt2cu USE_CUDNN=1
[](#cb24-2)./train_gpt2cu
```

好兆头：更快的注意力时间和更低的步骤延迟。如果您遇到构建/运行时错误，请确保您的 CUDA、cuDNN 和驱动程序版本兼容；在解决问题时回退到`USE_CUDNN=0`。

#### 内存与性能调整清单

+   批处理：增加`micro_batch_size`，直到达到约 90%的 GPU 利用率而不发生内存不足。

+   序列长度：较长的序列在注意力中计算量呈二次增长；如果内存紧张，请减少`--seq_len`。

+   梯度累积：通过多个微批次的累积来保持全局批次大小。

+   固定主机内存和异步复制：在合理的地方已经使用；尽量减少 CPU↔GPU 传输。

+   分析器：一旦运行，分析热点以确认 GEMMs 占主导地位（如预期）并且除非 FlashAttention 关闭，否则注意力不是瓶颈。

#### 故障排除

+   `cudaErrorNoKernelImageForDevice`：工具包对于你的 GPU 来说太新/太旧；使用正确的 `-arch=` 重建或更新驱动程序。

+   `CUBLAS_STATUS_ALLOC_FAILED` / OOM：降低批次大小、序列长度，或如果支持则切换到 BF16。

+   使用 FP16 的发散损失：增加损失缩放（如果可配置）或尝试 BF16；确认主权重更新是在 FP32 中。

+   cuDNN 错误：不使用 `USE_CUDNN` 重建以验证基本路径是否正常工作，然后重新访问版本/路径。

吸取的经验：`train_gpt2.cu` 是实用的快速训练器：混合精度，可选的 FlashAttention，并且可以扩展。你保持相同的可读训练循环，同时利用 GPU 的张量核心实现大幅加速和更好的内存效率。

### 7. 入门工具和数据处理 (`dev/download_starter_pack.sh`, `dev/data/`)

在你实际上可以在 *llm.c* 中训练或测试模型之前，你需要一些基本工具：分词器、数据集和配置文件。这些文件没有直接存储在存储库中（它们太大，并且通常有不同的许可证），因此项目提供了脚本来检索或生成它们。这就是 `dev/` 文件夹发挥作用的地方。

#### 入门套件脚本

开始的最简单方法是使用：

```c
[](#cb25-1)./dev/download_starter_pack.sh
```

此脚本下载一个现成的捆绑包，其中包含：

+   `gpt2_tokenizer.bin` - GPT-2 字节对编码 (BPE) 分词器的二进制格式。

+   `train.bin` / `val.bin` - 预分词的训练和验证数据集，通常基于 OpenWebText 或 Tiny Shakespeare 用于演示。

+   模型配置 - 一个 JSON 或头部文件，用于设置 GPT-2 124M 的超参数，如层数、隐藏大小和头数。

将其视为你的“入门套件”：它包含足够的资源来运行演示并看到训练损失下降，而无需自己设置完整规模的数据集管道。

#### 分词器文件 (`gpt2_tokenizer.bin`)

这是 GPT-2 分词器词汇的二进制表示。它将原始文本（如 `"Hello world"`) 映射到整数标记 ID，这些 ID 是模型的实际输入。

+   为什么是二进制？在 C 中加载比解析基于文本的词汇更快。

+   大小？约 500 KB，代表约 50,000 个标记。

+   在训练中的作用？既用于数据加载器（准备输入）也用于采样器（解码输出）。

没有这个文件，模型根本无法理解文本——它只会操作无意义的数字。

#### 数据集文件 (`train.bin`, `val.bin`)

每个数据集文件都是一个包含二进制数据块，其中：

1.  一个描述序列长度、词汇大小和其他元数据的头部（大约 1 KB）。

1.  一系列标记 ID (`uint16`)，代表已经分词的文本语料库。

这种设计意味着 C 数据加载器可以简单地使用 `fread()` 将标记块读取到内存中，而无需在运行时对文本进行分词。它既快又节省内存，非常适合像 *llm.c* 这样的精简项目。

脚本通常会获取两个版本：

+   训练集（`train.bin`）

+   验证集（`val.bin`）

这样，训练循环可以偶尔切换到验证模式并报告验证损失，帮助您跟踪过拟合。

#### `dev/data/` 文件夹

如果您想生成自己的数据集，您将在这里找到工具：

+   用于 Tiny Shakespeare、OpenWebText 或其他语料库的脚本。

+   使用 GPT-2 分词器对文本进行分词并输出 `.bin` 格式的实用工具。

+   小型 Python 片段，用于检查数据集统计信息（如标记数或平均序列长度）。

例如，如果您想尝试在自己的文本文件上微调 GPT-2，您会：

1.  在 `dev/data/` 中运行预处理脚本以分词并保存您的语料库。

1.  将 `train_gpt2.c` 或 `train_gpt2.cu` 指向您的 `train.bin` 和 `val.bin`。

1.  按照常规启动训练。

#### 为什么预处理很重要

在 Python 中，分词和数据集准备可能会非常耗时，尤其是对于大型语料库。通过将所有内容预计算到紧凑的 `.bin` 文件中，*llm.c* 使运行时训练循环尽可能简单——只需读取整数数组并将它们输入到模型中。

这种关注点的分离（预处理与训练）使得训练代码干净且专注。

#### 快速检查

在运行 `download_starter_pack.sh` 之后，您应该在您的当前工作目录中看到这些文件：

```c
gpt2_tokenizer.bin
train.bin
val.bin
```

如果有任何缺失，重新运行脚本。没有它们，训练器将因找不到文件而退出。

总结：入门包是您立即运行 *llm.c* 的通行证。它提供了 C 代码期望的格式化的分词器和数据集。稍后，当您准备好在自己的文本上训练或扩展时，`dev/data/` 文件夹会向您展示如何以相同的方式准备自定义数据集。

### 8. 调试技巧与 IDE 逐步执行（`-g`）

尽管*llm.c*设计得小巧且易于阅读，但训练一个转换器模型仍然是一个包含许多部件的大程序。当出现问题时——无论是段错误、`NaN` 损失还是意外结果——您将希望能够有效地进行调试。这就是调试构建和 IDE 逐步执行发挥作用的地方。

#### 为什么存在调试模式

默认情况下，Makefile 使用重优化（`-O3`）进行编译。这使得代码运行得很快，但也使得调试更加困难：

+   变量可能会被优化掉。

+   函数可能会被内联，因此您无法清楚地逐步执行它们。

+   调试器可能会不可预测地跳转。

添加 `-g` 标志（通过 `DEBUG=1` 启用）告诉编译器在二进制文件中包含额外的信息，以便您可以在运行时看到代码的确切行为。

#### 构建调试二进制文件

要构建带有调试信息的程序：

```c
[](#cb27-1)make train_gpt2 DEBUG=1
```

这会产生一个较慢的可执行文件，但它可以无缝地与像以下工具一起工作：

+   gdb - 经典的 GNU 调试器。

+   lldb - 默认在 macOS 上。

+   VS Code / CLion / Xcode - 集成调试器和 GUI 界面的 IDE。

#### 在 Linux/macOS 上使用 gdb

在 gdb 下启动你的程序：

```c
[](#cb28-1)gdb ./train_gpt2
```

在 gdb 内部：

+   运行程序：`run`

+   在 `main` 处设置断点：`break main`

+   逐行执行：`step` 或 `next`

+   检查变量：`print loss`、`print i`

+   退出：`quit`

这是查看崩溃发生位置最快的方法。

#### 使用 IDE

如果命令行调试让你感到害怕，你可以使用 VS Code 或 CLion 这样的 IDE：

+   打开项目文件夹。

+   配置调试器（选择 `gdb` 或 `lldb` 后端）。

+   通过点击行号旁边添加断点。

+   运行调试构建（`train_gpt2` 并设置 `DEBUG=1`）。

+   步过前向传递、反向传递或优化器更新。

这样，你可以通过每一步来直观地观察变量更新。

#### CUDA 代码调试

CUDA 调试可能有点棘手，但仍然可行：

+   `cuda-gdb` - NVIDIA 的 GPU 调试器，类似于 gdb，但支持进入内核。

+   Nsight Systems / Nsight Compute - 图形化的分析器/调试器，允许你跟踪内核启动、内存传输和 GPU 利用率。

如果你的 CUDA 代码崩溃并显示像 `非法内存访问`、`cuda-gdb` 这样的神秘信息，可以帮助定位内核甚至确切的行。

#### 在 llm.c 中调试常见问题

1.  文件未找到 → 确保已下载 `gpt2_tokenizer.bin`、`train.bin` 和 `val.bin`。

1.  在 malloc/fread 处发生段错误 → 检查文件路径和数据集大小。

1.  损失变为 NaN →

    +   在 CPU 上：检查归一化中的除以零。

    +   在 GPU 上：检查损失缩放（混合精度）或尝试 FP32 路径进行比较。

1.  与 PyTorch 测试不匹配 → 运行 `test_gpt2` 或 `test_gpt2cu` 并比较输出；这通常可以隔离错误是否出现在前向传递、反向传递或优化器中。

#### 日志记录与合理性检查

在调试时，添加额外的日志记录很有帮助。仓库已经有一个轻量级的日志记录器，但你也可以在 CPU 上添加 `printf`s 或在 GPU 上添加 `cudaDeviceSynchronize(); printf(...)` 来跟踪值。例如：

```c
[](#cb29-1)printf("Step %d: loss=%f\n", step, loss);
```

有时最快的修复方法就是打印出来看看发生了什么。

#### 初学者最佳实践

+   学习时从 CPU 构建开始 - 它比 CUDA 更容易调试。

+   总是保持一个小数据集（如 Tiny Shakespeare），以便快速迭代。

+   将其与同一批次的 PyTorch 参考进行比较以捕捉细微的错误。

+   每当你遇到奇怪的行为时，使用 `DEBUG=1` - 你会牺牲速度以换取清晰度，这在学习时通常值得。

吸取的教训：调试构建（`-g`）将 *llm.c* 从一个黑盒变成了一个可以逐步学习的工具。使用 gdb、lldb 或 IDE，你可以在任何一行暂停，检查变量，并确切了解 GPT-2 训练在 C 或 CUDA 中的工作方式。它速度较慢，但这是学习并修复问题的最清晰方式。

### 9. 项目约束与可读性契约

*llm.c*项目并不是试图成为最快的或功能最丰富的 GPT-2 训练器。相反，它有一套非常明确的约束——作者对代码库施加的规则，以保持其可接近性和教育性。您可以将这些视为代码和读者之间的“契约”：某些事情故意保持简单，即使这会牺牲一些性能。

#### 简约胜于优化

+   没有处理器特定的内联函数：您在 CPU 路径中不会看到 AVX、NEON 或其他硬件优化的汇编调用。

+   没有花哨的模板元编程：与 C++框架不同，这里您得到的是普通的 C 结构和函数。

+   没有奇特的库：除了用于 GPU 加速的 cuBLAS/cuDNN 外，大多数功能都是直接实现的。

这意味着代码几乎可以在任何地方运行，您不需要理解深度的编译器技巧就能理解正在发生的事情。

#### 透明度高于抽象

+   每个操作都在源代码中可见。例如，您不会调用像`nn.CrossEntropyLoss`这样的框架函数，而是会发现用 C 编写的显式前向和反向传递代码。

+   数据加载、标记化、优化器步骤和调度器都在`llmc/`中作为独立的、小的模块实现。

+   您不需要猜测正在发生的事情——如果您好奇，您可以打开相应的`.h`文件并查看确切的代码。

指导思想：如果某事对训练 GPT-2 至关重要，您应该能够阅读和理解它。

#### 在需要的地方提供性能（但不超过）

+   CPU 构建允许使用 OpenMP 指令，因为它们可以提供大量的速度提升，而额外代码最少。

+   cuBLAS/cuDNN 用于 GPU 矩阵乘法和注意力，因为重新实现它们会分散注意力，并使项目变得不可行。

+   但项目避免了不必要的复杂性——没有内核融合，没有复杂的缓存层，没有半实现的“框架”抽象。

这种平衡确保您仍然可以以合理的速度运行实验，但代码永远不会牺牲可读性。

#### 教育优先

代码的编写是为了教学，而不是为了赢得基准测试。这意味着：

+   变量名具有描述性，而不是晦涩难懂。

+   注释不仅解释了发生了什么，还解释了为什么。

+   文件被保持得小而专注，而不是横跨数十层抽象。

+   有一个匹配的 PyTorch 参考实现，这样您就可以始终将您的理解与熟悉的基线进行比较。

#### 您应该预期的限制

+   训练速度比 PyTorch/XLA/JAX 或 DeepSpeed 调优运行慢。

+   多 GPU 扩展功能正常，但未进行大量优化。

+   只涵盖 GPT-2 架构——不要期待 GPT-3 或 transformer 变体。

+   故意省略了像数据集流、检查点分片或高级分布式技巧这样的功能。

这些不是错误——它们是有意识的权衡，以保持代码库小、锐利和教学性。

#### 这为什么对您很重要

如果您正在学习 transformers 的工作原理，这份契约是一份礼物：

+   您不会迷失在性能黑客中。

+   你不会在抽象丛林中挣扎。

+   你将始终知道你所阅读的内容接近“纯粹”的算法思想。

从另一方面来看，如果你追求生产级速度，你需要在上面添加更多层。但这超出了*llm.c*的任务范围。

吸取的教训是：*llm.c*受限于可读性合约：清晰胜于原始速度，透明胜于抽象，简约胜于复杂。这些限制使项目足够小，可以装进你的头脑，同时仍然足够强大，可以重现 GPT-2 的训练。它是一个教学实验室，而不是赛车——这正是它的价值所在。

### 10. 社区、讨论和学习路径

快速入门的最后一步根本不是关于代码，而是关于项目和周围的人及资源。“*llm.c*”已经不仅仅是一个单一存储库；它已成为想要将大型语言模型简化到其本质的学习者、修补者和研究者的聚会点。理解这一社区层与理解代码本身同样重要。

#### GitHub 上的讨论和问题

项目讨论标签页充满了有价值的背景信息：

+   询问不同平台（Linux、macOS、Windows）上构建错误的开发者。

+   探索如何将*llm.c*扩展到训练更大的 GPT-2 模型（355M、774M、1.6B）。

+   多 GPU 和 MPI 运行的报告，包括解决 NCCL 挂起和性能瓶颈。

+   关于混合精度与 FP32 与 BF16 稳定性的辩论。

阅读这些帖子就像是在数百名其他学习者的肩膀上观察。你不仅会看到官方答案，还会看到人们实时解决问题的思维过程。

#### 路线图和贡献

README 和问题有时会暗示项目可能的发展方向：

+   在`dev/cuda/`中使 CUDA 内核更模块化。

+   简化集群的多 GPU 启动。

+   添加小型教程风格的文档（如 LayerNorm 教程）。

该项目欢迎贡献，但遵循相同的极简主义哲学。如果你打算贡献，请记住：目标是清晰第一，性能第二。

#### 外部学习资源

虽然*llm.c*是自包含的，但它与外部材料搭配得很好：

+   `train_gpt2.py`中的 PyTorch 参考实现是正确性的权威来源。

+   GPT-2 论文提供了架构背景。

+   CUDA 和 cuBLAS/cuDNN 文档解释了项目调用的 GPU API。

+   社区博客文章通常用简单的英语解释代码的特定部分，这使得消化起来更容易。

通过结合代码、论文和这些资源，你可以获得更深入的理解。

#### 建议的学习路径

如果你作为初学者来到*llm.c*，这里有一个自然的进展：

1.  在 Tiny Shakespeare 上运行 CPU 训练器（`train_gpt2.c`），观察损失下降。

1.  使用`DEBUG=1`逐步检查代码，确认你理解了正向、反向和优化步骤。

1.  转到 FP32 CUDA 训练器，看看相同的循环如何在 GPU 上运行。

1.  切换到现代 CUDA 训练器（`train_gpt2.cu`）并了解混合精度是如何工作的。

1.  在`dev/data/`中的数据集脚本上进行实验，尝试你自己的文本语料库。

1.  在`doc/`目录中阅读 LayerNorm 文档，以加深你的理论与实践联系。

1.  如果你有多块 GPU，可以探索使用 MPI/NCCL 的多 GPU 运行。

1.  关注 GitHub 讨论，了解实际的调试和扩展故事。

#### 这为什么重要

仅代码是不够的。社区背景、讨论和学习路径使`llm.c`成为一个活生生的项目。通过参与其中，你可以避免孤立学习的感受。你会看到其他人正在努力克服相同的挑战，你将更清楚地了解下一步该尝试什么。

吸取的教训：除了文件和脚本之外，`llm.c`是一个社区驱动的学习环境。GitHub 问题、讨论、参考文档和外部教程都构成了“扩展课堂”的一部分。如果代码是实验室的工作台，那么社区就是那些帮助你一路解决问题的实验室伙伴。

## 第二章：数据、分词和加载器

### 11. GPT-2 分词器工件（`gpt2_tokenizer.bin`）

类似于 GPT-2 的语言模型并不直接理解英语、越南语或其他任何自然语言。相反，它理解数字。这些数字被称为标记。分词器是介于人类文本和标记之间的翻译工具。在`llm.c`中，GPT-2 分词器存储在一个名为`gpt2_tokenizer.bin`的小文件中。这个文件是模型能够读取输入文本并产生我们可以理解的输出文本的关键。

#### 该文件包含的内容

文件`gpt2_tokenizer.bin`是 GPT-2 分词器的二进制版本。它包括：

| 组件 | 目的 |
| --- | --- |
| 字节词汇（0–255） | 确保每个可能的字符都可以表示。 |
| 合并规则（BPE） | 将“ing”或“the”等频繁出现的序列合并为单个标记以提高效率。 |
| 词汇量大小（约 50,257） | 定义了 GPT-2 可以处理的独特标记的数量。 |
| 映射 ID 与文本 | 允许程序将模型输出（数字）转换回人类可读的字符串。 |

分词器不是以 JSON 或文本的形式编写，而是以二进制形式存储。这允许`llm.c`通过简单的文件读取快速加载它，从而保持代码简洁和快速。

#### 它从何而来

你不需要手动构建这个文件。仓库提供了一个脚本用于下载它，以及一些小的训练和验证数据集：

```c
[](#cb30-1)./dev/download_starter_pack.sh
```

运行脚本后，你应该在你的工作目录中看到`gpt2_tokenizer.bin`、`train.bin`和`val.bin`。如果分词器缺失，程序将无法运行，因为它不知道如何解释文本。

#### 代码如何使用它

在训练期间，分词器不活跃，因为数据集（`train.bin`和`val.bin`）已经预先分词为整数。这使训练循环快速且简单。

在采样或评估期间，tokenizer 再次变得重要。在模型预测一系列标记 ID 之后，tokenizer 将这些数字转换回你可以在屏幕上阅读的文本。

在 `llmc/tokenizer.h` 中定义的 tokenizer 的 C API 提供了三个主要功能：

```c
[](#cb31-1)int tokenizer_init(Tokenizer *t, const char *filename);
[](#cb31-2)int tokenizer_decode(Tokenizer *t, const int *ids, int n, char *out);
[](#cb31-3)void tokenizer_free(Tokenizer *t);
```

这就是你需要的：从文件中初始化 tokenizer，将标记解码为文本，并在完成后释放内存。

#### 实践中的示例工作流程

1.  初始化 tokenizer：

    ```c
    [](#cb32-1)Tokenizer t;
    [](#cb32-2)tokenizer_init(&t, "gpt2_tokenizer.bin");
    ```

1.  将一系列标记解码回文本：

    ```c
    [](#cb33-1)char buf[512];
    [](#cb33-2)tokenizer_decode(&t, tokens, ntokens, buf);
    [](#cb33-3)printf("%s\n", buf);
    ```

1.  当你不再需要它时清理内存：

    ```c
    [](#cb34-1)tokenizer_free(&t);
    ```

这个小循环足以将模型输出转换为可读的句子。

#### 为什么这很重要

没有 tokenizer，模型无法进行交流。tokenizer 就像是人类和神经网络之间共享的词典。如果你给模型文本，tokenizer 将其转换为模型能理解的数字。当模型响应时，tokenizer 将其数字转换回文本。如果 tokenizer 与数据集不匹配，模型的预测将变成乱码。保持 tokenizer 和数据集同步对于正确的训练和评估至关重要。

#### 试试看

这里有一些小练习可以帮助你更好地理解 tokenizer：

1.  检查文件是否存在：在运行启动包脚本后，验证 `gpt2_tokenizer.bin` 是否在你的目录中。尝试在没有它的情况下运行训练器，并观察错误信息。

1.  检查词汇表大小：运行训练器，寻找打印 `vocab_size: 50257` 的行。将其与 `padded_vocab_size: 50304` 进行比较。你认为为什么填充有助于 GPU？

1.  手动解码一系列标记：编写一个简短的 C 程序，加载 tokenizer 并解码一系列固定的标记 ID（例如 `[464, 3290, 318]`）。观察你得到什么文本。

1.  不匹配实验：如果你使用不同的 tokenizer（比如自定义词汇表）构建自己的数据集，尝试用 `gpt2_tokenizer.bin` 解码它。注意输出变得毫无意义，这显示了为什么一致性很重要。

1.  数据集 + tokenizer 链接：在十六进制查看器中打开 `train.bin`。你会发现它只是数字。使用 tokenizer 解码前几百个标记，你会看到真实的文本出现。

#### 吸取的经验

`gpt2_tokenizer.bin` 是一个微小但至关重要的文件。它是模型和人类能够使用同一种语言交流的桥梁。训练效率高，因为所有数据都是预先分词的，当你想查看模型写的内容时，tokenizer 将原始数字转换回单词。没有它，整个系统将陷入沉默。

### 12. 二进制数据集格式 (`train.bin` 和 `val.bin`)

就像 tokenizer 将文本转换为数字一样，*llm.c* 中的数据集也是以数字的形式存储的。与读取纯文本文件（如 `.txt`）不同，训练数据和验证数据被保存在简单的二进制文件中：`train.bin` 和 `val.bin`。这些文件是训练循环的燃料。

#### 这些文件看起来是什么样子

初看，如果你在文本编辑器中打开 `train.bin` 和 `val.bin`，它们看起来像不可读的块。这是因为它们不是为人类可读而设计的。它们包含：

| 部分 | 描述 |
| --- | --- |
| 一个微小的头部（约 1 KB） | 存储诸如序列长度和词汇大小等元数据。 |
| 一串标记 ID (`uint16`) | 数据集中的每个标记化词元，保存为 16 位整数。 |

每个整数代表来自标记器词汇表中的一个标记。由于 GPT-2 的词汇量约为 50,000 个标记，16 位整数（`uint16_t`）足以存储它们。

#### 为什么选择二进制格式？

+   效率：每次不需要重新标记文本，数据只需预先标记一次并存储为数字。训练器只需直接读取它们。

+   速度：从文件中读取整数比解析和处理原始文本要快。

+   简单性：训练循环只需处理整数数组——没有字符串处理，没有解析，没有意外。

这个选择使得 *llm.c* 中的训练代码更加简洁和快速。

#### 数据加载器如何使用它们

当训练开始时，数据加载器从 `train.bin` 中读取数字块。每个块对应于大小为 B × T 的一个批次：

+   B = 批大小（每个批次中的示例数）。

+   T = 序列长度（每个示例中的标记数）。

例如，如果 `B = 8` 且 `T = 1024`，数据加载器将从文件中读取 `8 × 1024 = 8192` 个标记 ID，将它们重塑为序列，并将它们喂给模型。

验证文件（`val.bin`）的工作方式相同，但在训练过程中仅偶尔使用来衡量验证损失。这有助于检测过拟合。

#### 代码中的工作流程

在仓库内部，你会在 `llmc/dataloader.h` 中看到这些函数：

```c
[](#cb35-1)int dataloader_init(Dataloader *loader, const char *filename, int B, int T);
[](#cb35-2)int dataloader_next_batch(Dataloader *loader, int *inputs, int *targets);
[](#cb35-3)void dataloader_reset(Dataloader *loader);
[](#cb35-4)void dataloader_free(Dataloader *loader);
```

下面是逐步发生的过程：

1.  使用二进制文件和批/序列大小进行初始化。

1.  下一个批次将下一个 B × T 个标记读入输入数组和目标数组。

1.  重置允许在开始新的纪元时从开头重新读取。

1.  训练结束时，Free 会清理资源。

目标数组只是将相同的序列向前移动一个标记——因为语言模型预测的是 *下一个* 标记。

#### 它为什么重要

数据集格式是使 *llm.c* 实用的原因。没有它，代码需要在每个训练步骤中处理混乱的文本、编码和标记化。通过存储干净的标记 ID 数组，训练循环变得非常短且易于跟踪。这是一个设计决策，使项目最小化，同时忠实于真实的训练流程。

#### 尝试自己操作

1.  检查文件大小：运行 `ls -lh train.bin` 并注意它与普通 `.txt` 文件相比有多大。为什么它更小或更大？

1.  查看内部：使用十六进制查看器（`xxd train.bin | head`）来查看原始数字。它们看起来不像文本，但它们是模型训练所用的标记。

1.  计数标记：编写一个简短的 Python 或 C 脚本来计算存储在 `train.bin` 中的标记 ID 的数量。这让你对数据集的大小有一个概念。

1.  小型数据集：尝试使用 `dev/data/` 中的脚本从小的 `.txt` 文件生成自己的数据集。看看 `.bin` 文件是如何创建的。

1.  验证实验：在训练过程中，将验证集减少到只有几个批次，并观察验证损失与训练损失相比是如何稳定或波动的。

#### 吸收要点

`train.bin` 和 `val.bin` 可能看起来像是乱码，但它们是精心准备的包含标记 ID 的二进制文件。它们使训练更快、更简单、更可重复。*llm.c* 中的数据加载器以整洁的块读取这些数字，并直接将它们提供给模型，让你专注于学习 transformers 的工作原理，而不是与原始文本解析纠缠。

### 13. `dev/data/` 中的数据集脚本

仓库不仅提供像 `train.bin` 和 `val.bin` 这样的现成二进制数据集。它还在 `dev/data/` 文件夹内提供脚本，展示如何创建自己的数据集。这些脚本很重要，因为它们展示了原始文本是如何转换为 *llm.c* 数据加载器所期望的二进制格式的。

#### `dev/data/` 内部内容

此文件夹包含小的 Python 脚本，它们：

| 脚本 | 目的 |
| --- | --- |
| `prepare_shakespeare.py` | 将 Tiny Shakespeare 数据集转换为 `train.bin` 和 `val.bin`。 |
| `prepare_openwebtext.py` | 准备一个与 GPT-2 训练时类似的大型数据集。 |
| 其他辅助工具 | 将原始 `.txt` 文件标记化，分割成训练/验证集，并保存到二进制格式。 |

每个脚本都遵循相同的基本配方：

1.  从源文件中读取原始文本。

1.  应用 GPT-2 标记化器将文本转换为标记 ID。

1.  将标记分割成训练和验证部分。

1.  将 ID 写入 *llm.c* 可以直接读取的二进制文件。

#### 为什么预处理发生在 C 语言之外

在 C 语言中，处理包含 Unicode、标点和不同编码的文本文件很混乱。相反，预处理在 Python 中只进行一次，那里使用标记化器更容易。结果以简单的二进制格式（`uint16` ID）保存。从那时起，C 语言只需要处理整数数组——干净且高效。

这种设计使训练循环最小化：没有文本解析，没有字符串处理，只有数字。

#### 示例：Tiny Shakespeare

其中最简单的数据集之一是 Tiny Shakespeare，大约有 1MB 的莎士比亚戏剧文本。脚本 `prepare_shakespeare.py` 将：

+   读取 `input.txt`（原始文本）。

+   使用 GPT-2 标记化器（`gpt2_tokenizer.bin`）将每个单词和符号转换为标记 ID。

+   将 90%的数据分割成 `train.bin`，10%分割成 `val.bin`。

运行脚本后，你将得到小的二进制文件，让你在 CPU 或 GPU 上几分钟内从头开始训练 GPT-2。

#### 示例：OpenWebText

脚本 `prepare_openwebtext.py` 展示了如何标记化一个更大的数据集，更接近 GPT-2 最初训练的数据集。这需要更多的磁盘空间，但如果你想要尝试将训练扩展到更大的模型，这很有用。

#### 为什么这很重要

这些脚本不仅是便利工具，它们还是如何将 llm.c 适配到你的数据的示例。如果你有一系列电子邮件、诗歌或编程代码，你可以：

1.  将它们放入一个单独的 `.txt` 文件中。

1.  修改 `dev/data/` 中的脚本之一。

1.  生成新的 `train.bin` 和 `val.bin` 文件。

1.  在你的文本上训练 GPT-2。

通过将数据集创建与训练分离，*llm.c* 保持 C 代码小巧，并使实验灵活。

#### 尝试自己操作。

1.  运行莎士比亚脚本：

    ```c
    [](#cb36-1)python dev/data/prepare_shakespeare.py
    ```

    然后检查 `train.bin` 和 `val.bin` 是否已创建。

1.  使用十六进制查看器打开二进制文件，并确认它们只包含数字。

1.  修改脚本以标记化不同的文本文件（例如，你自己的写作）。

1.  比较数据集大小：Tiny Shakespeare 很小（MBs），OpenWebText 很大（GBs）。观察训练速度如何根据数据集大小而变化。

1.  使用你自定义的数据集重新运行训练，并观察模型如何开始以你的风格生成文本。

#### 吸收要点

`dev/data/` 脚本是原始人类文本和训练中使用的二进制数据集之间的桥梁。它们允许你准备小型演示数据集或扩展到更大的语料库。通过实验这些脚本，你可以学习如何将你的数据带入 `llm.c` 并在喜欢的任何事物上训练 GPT 风格的模型。

### 14. DataLoader 设计（批处理、步长、周期）

现在数据集已准备为 `.bin` 文件，我们需要一种方法在训练期间将它们喂入模型。这是 `llm.c` 中 DataLoader 的任务。你可以在 `llmc/dataloader.h` 中找到其接口，其目的是非常简单的：从 `train.bin` 或 `val.bin` 中获取大量令牌 ID 流，将其切割成可管理的块，并将这些块作为批次提供给训练循环。

#### 核心思想

训练语言模型需要为每个批次两个数组：

+   输入：令牌 ID 的序列，例如 `[The, cat, sat, on]`

+   目标：与原始序列偏移一个位置的相同序列，例如 `[cat, sat, on, the]`

模型学习预测序列中的每个下一个令牌。DataLoader 自动从巨大的数据集文件中切割这些数组。

#### 接口

在代码中，你会看到像这样的函数声明：

```c
[](#cb37-1)int dataloader_init(Dataloader *loader, const char *filename, int B, int T);
[](#cb37-2)int dataloader_next_batch(Dataloader *loader, int *inputs, int *targets);
[](#cb37-3)void dataloader_reset(Dataloader *loader);
[](#cb37-4)void dataloader_free(Dataloader *loader);
```

这里是每个函数的作用：

+   `dataloader_init`: 打开数据集文件，记住批大小 `B` 和序列长度 `T`。

+   `dataloader_next_batch`: 返回下一个 `B × T` 令牌（输入）及其偏移版本（目标）。

+   `dataloader_reset`: 当周期结束时，回滚到文件开头。

+   `dataloader_free`: 关闭文件并释放内存。

这种设计使训练循环保持简洁：只需调用 `next_batch`，你就可以获得用于前向/反向传递的数据。

#### B × T 解释

最重要的两个参数是：

| 符号 | 含义 | 示例 |
| --- | --- | --- |
| B | 批大小（每步多少个序列） | 16 |
| T | 序列长度（每个序列多少个令牌） | 1024 |

因此，一个批次包含 `B × T` 个标记。例如，当 `B = 16` 和 `T = 1024` 时，每个批次包含 16,384 个标记。DataLoader 简单地从二进制文件中读取这么多数字，并将它们排列在内存中。

#### 在数据集中穿梭步长

当你调用 `dataloader_next_batch` 时，加载器会通过 `B × T` 个标记每次向前移动一次。当它到达数据集文件的末尾时，它要么：

+   重置到开始位置（`dataloader_reset`），或者

+   根据训练循环的需求从训练切换到验证。

这种基于步长的读取方式效率很高：没有随机访问，只是从文件中进行顺序读取。

#### 训练轮次和洗牌

在深度学习中，一个训练轮次意味着对数据集的一次完整遍历。*llm.c* 中的 DataLoader 很简单：它从开始到结束线性移动。它不像 PyTorch 的 `DataLoader` 那样洗牌数据。为什么？因为语言数据已经非常多样化，项目更重视代码的简洁性而不是额外的功能。如果你需要洗牌，你可以在创建 `.bin` 文件之前对数据集进行不同的预处理。

#### 为什么这很重要

DataLoader 是训练中的沉默的功臣。它确保每个步骤都能看到一个新的标记序列批次，始终与匹配的输入和目标相对应。通过将数据集读取与训练循环分离，代码保持简洁和专注。这种设计也使得数据集的交换变得容易——一旦生成了 `.bin` 文件，加载器就不关心它来自哪里。

#### 尝试自己操作

1.  打印第一个批次：修改代码以打印前 20 个输入标记及其目标。看看每个输入标记是如何与下一个目标标记对齐的。

1.  尝试 B 和 T 的不同值：将 `B = 2` 和 `T = 8` 设置为观察加载器如何将数据集切割成小块。然后尝试更大的值，看看内存使用如何变化。

1.  检查训练轮次长度：编写一个小循环来计算在调用 `dataloader_reset` 之前你能得到多少个批次。这是否与 `B × T` 除以总标记数相等？

1.  验证检查：观察训练循环切换到 `val.bin` 的频率。随着时间的推移，验证损失与训练损失是如何比较的？

1.  自定义步长：修改代码，使 DataLoader 在批次之间跳过一些标记。这对训练有什么影响？

#### 吸收要点

*llm.c* 中的 DataLoader 故意设计得很简单。它以固定大小的批次流式传输标记 ID，逐个步长前进，并在完成后重置。这种直接的设计避免了复杂性，同时将重点放在模型本身上，同时仍然教授你在语言模型训练中批处理和序列处理的基本机制。

### 15. 评估加载器和验证流程

训练模型不仅仅是观察训练损失下降。要知道模型是否实际上在学习可以推广的模式——而不仅仅是记住训练数据——你需要运行验证。在*llm.c*中，验证由一个名为 EvalLoader 的组件处理，它的工作方式与 DataLoader 相同，但读取的是验证数据集（`val.bin`）而不是训练数据集（`train.bin`）。

#### 为什么我们需要验证

想象一下，只通过反复用相同的数学问题来教学生。他们可能会在这些问题上变得非常擅长，但面对新的问题时可能会完全失败。验证就像给学生一个突然的测验，其中包含未见过的题目。如果他们做得好，你就知道他们实际上已经学会了这些概念。

对于语言模型，验证有助于检测过拟合：当训练损失持续改进，但验证损失保持平稳或甚至变差时。

#### EvalLoader 的工作原理

EvalLoader 与 DataLoader（`llmc/dataloader.h`）位于同一代码文件中，但它指向不同的数据集文件。其工作流程几乎相同：

1.  打开`val.bin`并准备读取。

1.  提供大小为`B × T`的批次（批量大小×序列长度）。

1.  以与训练 DataLoader 相同的方式提供输入和目标。

1.  完成一次完整遍历文件后重置。

训练循环通常每隔一段时间调用 EvalLoader，例如，每几百步一次，这样你就可以在训练期间获得验证损失的快照。

#### 验证过程中发生的事情

当触发验证时：

1.  当前模型参数被冻结（没有梯度更新）。

1.  从`val.bin`中读取几个批次。

1.  模型只运行前向传递，计算每个批次的损失。

1.  损失被平均并报告为验证损失。

这不会花费太多时间，因为它通常只采样验证数据集的一个子集，而不是整个文件。

#### 带验证的训练循环

在伪代码中，循环看起来像这样：

```c
[](#cb38-1)for (step = 0; step < max_steps; step++) {
[](#cb38-2)    dataloader_next_batch(&train_loader, inputs, targets);
[](#cb38-3)    forward_backward_update(model, inputs, targets);
[](#cb38-4) 
[](#cb38-5)    if (step % eval_interval == 0) {
[](#cb38-6)        float val_loss = 0.0f;
[](#cb38-7)        for (int i = 0; i < eval_batches; i++) {
[](#cb38-8)            evalloader_next_batch(&val_loader, inputs, targets);
[](#cb38-9)            val_loss += forward_only(model, inputs, targets);
[](#cb38-10)        }
[](#cb38-11)        val_loss /= eval_batches;
[](#cb38-12)        printf("step %d: val loss %.4f\n", step, val_loss);
[](#cb38-13)    }
[](#cb38-14)}
```

这只是一个简化，但它展示了这个想法：验证循环嵌套在训练循环中，偶尔运行而不是每步都运行。

#### 为什么它很重要

验证是训练的现实检查。没有它，你可能会永远训练下去，并庆祝低训练损失，但最终发现你的模型在新文本上产生的是无意义的输出。通过跟踪验证损失，你可以：

+   早期检测过拟合。

+   调整超参数（如学习率或批量大小）。

+   了解何时训练停滞，是时候停止了。

在专业设置中，验证曲线通常实时绘制，但在*llm.c*中，极简主义的方法是只将数字打印到控制台。

#### 试试你自己

1.  观察验证损失：运行训练并注意验证损失与训练损失的对比。它们是否同时下降？

1.  过拟合演示：在一个非常小的数据集（如 10 KB 的文本）上训练。注意训练损失急剧下降，但验证损失停滞或上升。

1.  改变评估间隔：减少`eval_interval`，使验证每步运行。训练感觉慢了多少？

1.  更改评估批次：将 `eval_batches` 设置为 1 与 100。这会对报告的验证损失稳定性产生什么影响？

1.  验证作为停止规则：当验证损失在多个间隔内停止改善时停止训练。这会如何影响最终性能？

#### 吸取的经验

EvalLoader 是 DataLoader 的孪生兄弟，但用于验证。它为模型提供在训练期间从未见过的数据，而得到的验证损失告诉你模型是否在学习有用的模式，而不是仅仅记忆。这是防止浪费计算的最简单保障，也是每个训练循环的必要部分——即使在简化版的 *llm.c* 世界中也是如此。

### 16. 序列长度与内存预算

在 *llm.c* 中训练 GPT-2 时，你做出的最重要的决定之一是选择序列长度（通常称为 T）。这个值决定了模型在单次前向传递中处理多少个标记。这听起来可能只是另一个参数，但序列长度对模型能学习的内容、使用的内存以及训练速度有巨大影响。

#### 序列长度意味着什么

序列长度仅仅是每个训练示例中的标记数。如果 `T = 1024`，模型将按行读取 1,024 个标记（如单词或子词），并在每个位置尝试预测下一个标记。

想象一下：如果你给模型一段文本，序列长度就是它一次能看到的这段文本的多少。较短的长度给模型提供的上下文较少，而较长的长度则允许它捕捉到更大的模式，比如整个段落甚至多页内容。

#### 它在代码中的位置

在日志中，你经常会看到如下行：

```c
max_seq_len: 1024
```

这个数字在 GPT-2 配置中定义，并传递给 DataLoader。DataLoader 从 `train.bin` 和 `val.bin` 中切出恰好 `T` 个标记的块。模型本身具有固定大小的 `T` 位置嵌入，因此它无法处理超过这个最大值的序列。

#### 较长序列的内存成本

Transformer 很强大，但成本高昂。注意力机制会将序列中的每个标记与其他每个标记进行比较。这意味着内存和计算与序列长度的平方成正比：

| 序列长度 (T) | 相对注意力成本 |
| --- | --- |
| 256 | 1× |
| 512 | 4× |
| 1024 | 16× |
| 2048 | 64× |

因此，将 `T` 加倍不仅会加倍成本，还会将其乘以四。这就是为什么在长上下文长度下训练需要大量的 GPU 内存。

#### 权衡

+   较短的序列：更快，内存更少，但上下文有限。适合快速实验或像微型莎士比亚这样的小型数据集。

+   较长的序列：需要更多内存，速度较慢，但模型可以理解更大的文本范围。对于大规模 GPT-2 训练是必需的。

你可以将序列长度视为一个旋钮：将其调高会增加模型“记忆”的能力，但也会使训练变得更加沉重。

#### 在 *llm.c* 中的实际选择

+   微型莎士比亚示例：通常为了速度，训练时使用 `T = 64` 或 `128`。

+   GPT-2 小型（124M）：通常使用 `T = 1024`，与原始论文相同。

+   如果你的 GPU 内存有限，你可能需要缩小 `T` 和/或批大小 `B`。

#### 为什么这很重要

选择序列长度是关于平衡学习能力和硬件限制。序列长度太小可能会阻止模型捕捉长期依赖关系。太大可能会使你的硬件上的训练变得不可能。*llm.c* 的每一次运行都是你希望模型看到的内容和你的系统可以处理的内容之间的协商。

#### 尝试自己操作

1.  短与长：使用 `T = 64` 训练 Tiny Shakespeare，然后使用 `T = 256`。比较生成的文本的速度和连贯性。

1.  内存测试：逐步增加 `T` 步骤，直到遇到内存不足（OOM）错误。注意 GPU 可以处理的最大值。

1.  批处理权衡：尝试在增加 `T` 的同时减少批大小 `B`。你能在保持 GPU 内存稳定的同时给模型提供更多上下文吗？

1.  验证影响：使用不同的 `T` 值运行，观察验证损失如何变化。更长的上下文总是有帮助吗？

1.  检查嵌入：打印出位置嵌入的形状。注意它们总是与 `T` 相关联。

#### 要点总结

序列长度 (`T`) 控制模型可以看到多少上下文。它直接决定了位置嵌入的大小、批次的结构和注意力所需的内存。在 *llm.c* 中，调整 `T` 是探索速度、内存和模型能力之间权衡的最快方式之一。

### 17. 运行间的可重复性和种子设置

在训练机器学习模型时，通常会注意到，使用相同代码和相同数据集的两个运行不会产生完全相同的结果。这是因为训练的许多部分都涉及随机性。在 *llm.c* 中，可重复性由随机种子控制。种子是随机数生成器的起点。如果你总是从相同的种子开始，那么“随机”数字的序列将是相同的，训练运行也将是相同的。

#### 随机性出现的地方

即使是在像 *llm.c* 这样的小项目中，随机性也会出现在几个地方：

| 组件 | 随机角色 |
| --- | --- |
| 权重初始化 | 模型的参数（如注意力矩阵）在开始时随机设置。 |
| 优化器状态 | 一些优化器使用随机噪声（尽管 AdamW 主要为确定性）。 |
| 样本输出 | 在生成文本时，如果概率接近，随机性决定选择哪个标记。 |
| 并行性 | 在 GPU 上，线程可能以略微不同的顺序执行，有时会引入微小的非确定性。 |

没有固定的种子，每次训练运行都可能有所不同，即使所有设置看起来都相同。

#### *llm.c* 如何处理种子

仓库提供了一个小的随机实用工具模块：`llmc/rand.h`。在里面你可以找到如下函数：

```c
[](#cb40-1)void manual_seed(uint64_t seed);
[](#cb40-2)float normal_(float mean, float std);
```

+   `manual_seed` 设置内部随机数生成器的种子，确保可重复性。

+   `normal_` 用于使用高斯噪声初始化权重，类似于 PyTorch 的 `torch.nn.init.normal_`。

当您调用 `manual_seed(1337);` 时，模型权重将每次以相同的方式初始化。

#### 为什么种子不能保证完美的可重复性

即使有固定的种子，您仍然可能看到小的差异：

+   GPU 内核有时会使用不是位确定性的并行算法。

+   浮点数数学在不同的硬件上可能会产生略微不同的舍入结果。

+   多 GPU 运行（通过 NCCL/MPI）可能会引入非确定性 reduce 操作。

这些差异通常很小-验证损失可能变化 0.001，但它们确实存在。对于大多数教育目的，*llm.c* 的种子足以使实验可重复。

#### 典型默认值

在许多示例中，您会看到：

```c
[](#cb41-1)manual_seed(1337);
```

这个“魔法数字” 1337 只是一个约定。您可以将其更改为任何整数。在运行中使用相同的种子保证相同的起始权重，这有助于比较超参数。

#### 为什么这很重要

在机器学习中，可重复性至关重要，因为它让您：

+   有效调试：如果出现错误，您希望它始终如一地出现。

+   比较设置：通过保持其他所有设置不变，您可以公平地测试学习率或批量大小。

+   分享结果：其他人可以运行您确切的设置并看到相同的结果。

没有种子，就很难判断差异是来自您的超参数更改还是只是随机运气。

#### 试试看

1.  使用相同的种子运行两次：使用 `manual_seed(1337)` 设置训练 GPT-2。您是否得到相同的训练损失曲线？

1.  改变种子：尝试 `manual_seed(42)` 并比较损失曲线。它们有多相似？它们是否收敛到大约相同的最终验证损失？

1.  移除种子：注释掉种子行并再次运行。注意运行如何发散。

1.  样本实验：使用固定的种子多次生成文本。然后更改种子再次生成。看看输出如何变化。

1.  多 GPU 测试：如果您有多个 GPU，请在设备上运行相同的种子。结果是否完全相同或只是近似相同？

#### 吸收要点

在 *llm.c* 中，可重复性来自为随机数生成器设置种子。虽然浮点数的怪癖意味着您不能总是得到完美的位对位匹配，但种子让您可以控制最大的随机源：权重初始化和采样。有了种子，您可以自信地进行调试、比较和分享结果。

### 18. 来自不良数据的错误表面（界限，断言）

在 *llm.c* 中训练模型时，一切取决于您输入的数据的质量和正确性。如果数据集或批次包含错误，训练过程可能会迅速偏离轨道——有时会直接崩溃，有时会产生像 `NaN` 这样的奇怪损失值。为了防止这种情况，代码使用界限检查和断言来早期捕捉问题。

#### 数据可能出什么问题

有几个常见的数据问题：

| 问题 | 发生的情况 |
| --- | --- |
| 标记 ID 越界 | 模型期望 ID 在 0 到`vocab_size-1`之间。错误的 ID 可能导致数组索引错误。 |
| 空或短数据集 | DataLoader 可能在填充一个批次之前就耗尽了标记。 |
| 不匹配的分词器 | 如果你使用不同的分词器构建数据集，ID 可能不会对应于`gpt2_tokenizer.bin`中的 GPT-2 分词器。这会产生无意义的输出。 |
| 损坏的`.bin`文件 | 如果文件不完整或写入错误，DataLoader 可能会读取垃圾值。 |

这些错误在训练过程中表现为段错误、无效内存访问或损失爆炸。

#### *llm.c* 如何防御不良数据

仓库大量使用断言——如果发生意外情况，它们会立即停止程序。例如，在`llmc/utils.h`中，`freadCheck`和`mallocCheck`等函数确保文件读取和内存分配成功。如果不成功，它们会打印错误消息并终止，而不是静默失败。

在 DataLoader 内部，通常会对标记 ID 进行验证，以确保它们落在预期的词汇范围内。如果你尝试访问嵌入表中的无效索引，程序会迅速崩溃，这比继续使用损坏的值要好。

#### 示例：词汇范围检查

在训练过程中，每个输入标记都会用于在嵌入矩阵中查找一行。如果标记 ID 太大，你会访问矩阵外的内存。这就是为什么检查`0 <= id < vocab_size`是至关重要的。在 C 语言中，断言提供了这种安全网。

```c
[](#cb42-1)assert(id >= 0 && id < vocab_size);
```

这种检查可能看起来很简单，但它可以节省数小时的调试神秘崩溃的时间。

#### 损失函数中的误差面

即使你的程序没有崩溃，不良数据也可能在损失函数中创建“误差面”：

+   NaN 值：当无效值通过 softmax、layernorm 或除法操作传播时出现。

+   平坦损失：如果数据集为空或重复，模型永远不会改进。

+   不匹配行为：如果训练集和验证集使用不一致的分词，训练损失会下降，但验证损失会保持较高。

这些是数据集或预处理有问题的一些迹象。

#### 为什么这很重要

C 是一种默认安全性很低的底层语言。一个越界的索引可以损坏内存并导致不可预测的错误。通过积极检查假设（文件大小、词汇边界、标记 ID），*llm.c*将难以找到的错误转化为立即、清晰的失败。对于学习者来说，这使得理解出错原因变得容易得多。

#### 尝试自己操作

1.  损坏数据集：打开`train.bin`并删除一些字节。运行训练并查看出现什么错误。注意断言如何快速捕捉到它。

1.  强制一个不良 ID：修改 DataLoader 以向标记添加`+100000`。模型会因断言而崩溃吗？

1.  跳过断言：暂时禁用检查并重新运行。比较找出问题所在有多困难。

1.  验证不匹配：使用不同的分词器对文件进行分词并保存为`val.bin`。观察验证损失与训练损失的行为差异。

1.  打印调试信息：添加日志以显示每个批次的第一个 20 个标记。你能在它崩溃之前发现坏数据吗？

#### 吸取教训

坏数据可能会无声地破坏训练，但*llm.c*使用断言和边界检查来使错误响亮且清晰。这种设计选择有助于学习者专注于 transformers 的真实逻辑，而不是追逐由损坏或不匹配的数据集引起的隐藏错误。在机器学习中，良好的数据卫生和严格的验证与模型本身一样重要。

### 19. Tokenization Edge Cases (UNKs, EOS, BOS)

Tokenization 看起来一开始很简单：取文本，将其分割成标记，并为每个标记分配一个 ID。但在实践中，总会有一些棘手的情况。"llm.c"继承了 GPT-2 标记化器的怪癖，它是基于字节的 BPE（字节对编码）。这种设计主要避免了“未知”标记，但在准备数据集或解释输出时，仍有一些细节需要你理解。

#### GPT-2 中没有真正的“UNK”

一些标记化器，如早期 NLP 系统中使用的标记化器，包括一个特殊的`UNK`（未知）标记，用于不在词汇表中的单词。GPT-2 通过在字节级别工作来避免这个问题：

+   每个可能的字节（0-255）都在基本词汇表中。

+   如果标记化器不知道如何分割一个字符或单词，它就回退到原始字节。

这意味着你永远不会在*llm.c*中看到`UNK`标记。任何输入文本总是可以表示的。这是 GPT-2 的标记化器如此健壮的主要原因之一。

#### 特殊标记：EOS 和 BOS

尽管 GPT-2 不使用`UNK`，但它确实使用其他特殊标记：

| 标记 | ID | 用途 |
| --- | --- | --- |
| EOS（序列结束） | 50256 | 标记文本段的结束。在训练和采样期间使用。 |
| BOS（序列开始） | GPT-2 中不明确 | GPT-2 不使用固定的 BOS 标记。相反，模型假设生成从位置 0 开始。 |

在*llm.c*中，你经常会看到训练序列末尾或采样文本时的`EOS`。如果你生成文本并看到奇怪的结尾，通常是因为模型预测了`EOS`。

#### 空白字符怪癖

标记化器还以略微不寻常的方式处理空白字符。例如，单词“hello”和带前导空格的单词” hello”映射到不同的标记。这就是为什么生成的文本有时以空格开头的原因——它是标记定义的一部分。

示例：

+   `"hello"` → 标记 ID 31373

+   `"hello"` → 标记 ID 15496

这对 GPT-2 来说是正常的行为。它有助于模型一致地捕捉间距和标点符号。

#### Unicode 和稀有字符

因为它是基于字节的，GPT-2 可以编码表情符号、带重音的字符，甚至二进制垃圾数据。但 BPE 合并优化了英语，所以稀有字符经常被分割成多个字节标记。这意味着包含大量稀有符号（如中文或表情符号）的序列将比普通英语文本使用更多的标记。

#### 为什么这很重要

分词中的边缘情况会影响数据集准备和模型输出。如果你看到奇怪的间距或早期的 `EOS` 标记，这不是错误——这只是分词器的工作方式。理解这些怪癖有助于你调试输出并准备数据集时没有意外。

#### 试试看

1.  `EOS` 检查：使用十六进制查看器打开 `val.bin` 并查找标记 ID `50256`。这些标记表示文本段落的结束。

1.  空白字符检查：使用分词器将`"hello"`和`" hello"`编码。比较标记 ID。

1.  表情符号测试：将包含表情符号的字符串（例如，“🙂🙂🙂”）编码，看看它变成了多少个标记。

1.  稀有字符数据集：创建一个包含带音标的字符的小 `.txt` 文件并对其进行分词。每个字符消耗多少字节？

1.  样本实验：生成文本，直到你看到 `EOS` 标记出现。注意模型“知道”何时停止。

#### 吸收要点

GPT-2 中的分词是健壮的，但它有一些怪癖。由于字节级编码，没有未知标记，但空白字符和像 `EOS` 这样的特殊标记扮演着重要的角色。通过实验这些边缘情况，你会对原始文本如何映射到驱动 *llm.c* 训练和生成的数字有更深的理解。

### 20. 数据卫生和日志记录

当使用 *llm.c* 训练时，拥有干净的数据与拥有正确的模型代码一样重要。如果数据集包含错误、重复或格式问题，模型可能会浪费能力去记忆噪声而不是学习有用的模式。这就是数据卫生的重要性——确保你的训练和验证集被正确准备。与此同时，日志记录确保你可以在训练过程中监控发生的情况并及早发现问题。

#### 数据卫生的含义

数据卫生是确保你的数据集既有效又有用。对于语言模型，这包括：

| 检查 | 为什么它很重要 |
| --- | --- |
| 正确的分词 | 必须与分词器（`gpt2_tokenizer.bin`）匹配，否则 ID 不会对齐。 |
| 无损坏文件 | 二进制 `.bin` 文件必须是完整的；部分写入会导致崩溃。 |
| 平衡分割 | 训练和验证集应来自相同的分布。 |
| 合理的大小 | 过小 → 过拟合。过大 → 慢或不可行。 |
| 去重 | 重复的段落（例如，网络爬取）会使模型记住而不是泛化。 |

`dev/data/` 中的脚本通过一致地分词并将数据集分割成训练/验证集来处理基本卫生。但如果你带来了自己的数据集，你负责首先对其进行清理。

#### 训练过程中的日志记录

一旦开始训练，日志就成为了你了解发生情况的窗口。*llm.c* 使用一个最小的日志系统（`llmc/logger.h`）将进度打印到控制台。典型的日志包括：

```c
step 0: train loss 5.19, val loss 5.32
step 100: train loss 4.87, val loss 5.01
step 200: train loss 4.62, val loss 4.88
```

这些数字让你可以跟踪：

+   训练损失：模型是否拟合数据？

+   验证损失：它是泛化还是过拟合？

+   步骤计时：每个批次花费的时间，对于分析很有用。

即使在这样的小项目中，这个日志循环也为你提供了调试运行所需的大部分信息。

#### 为什么卫生和日志记录是相辅相成的

坏数据通常会在日志中暴露出来。例如：

+   如果验证损失远高于训练损失，你的验证集可能不匹配。

+   如果损失突然变为 `NaN`，你的数据集中可能包含损坏的标记。

+   如果损失在较高值处停滞不前，你可能数据太少或预处理不当。

通过保持数据清洁并密切观察日志，你可以及早发现这些问题，而不是浪费数小时的计算时间。

#### 尝试自己操作

1.  污染数据集测试：取一个 `.txt` 文件，添加随机符号或二进制垃圾，并准备一个 `.bin` 数据集。训练损失会发生什么变化？

1.  重复段落：将相同的段落复制 100 次到训练文件中。验证损失是否提高，或者模型只是记忆了？

1.  日志频率：修改代码以记录每一步而不是每 N 步。结果有多嘈杂？

1.  自定义日志记录器：扩展日志记录器以打印梯度范数或学习率值。这有助于你更好地理解训练动态吗？

1.  比较分割：构建两个具有不同训练/验证分割的数据集。哪一个给出了更稳定的验证损失？

#### 吸取的教训

数据卫生确保模型从干净、一致输入中学习，而日志记录确保你可以看到学习是否真正发生。在 *llm.c* 中，它们共同构成了可靠实验的基础。如果你仔细清理数据并关注日志，你将在问题变得严重之前捕捉到大多数问题。

## 第三章 模型定义和权重

### 21. GPT-2 配置：词汇量、层、头、通道

任何 GPT-2 模型，无论大小，都是由一些配置数字定义的。这些数字决定了模型的大小、所需的内存以及它可能变得多强大。在 *llm.c* 中，这些设置存储在一个简单的配置结构体中，并在训练开始时打印出来。它们描述了变换器的“蓝图”。

#### 核心参数

这里是日志中你会看到的最重要的一些值：

| 参数 | 含义 | 示例（GPT-2 小型） |
| --- | --- | --- |
| `vocab_size` | 从分词器来的不同标记的数量。 | 50,257 |
| `padded_vocab_size` | 四舍五入到最接近的倍数的词汇量大小（为了 GPU 效率）。 | 50,304 |
| `max_seq_len` | 模型可以处理的最长标记序列。 | 1,024 |
| `num_layers` | 堆叠在一起的变换器块的数量。 | 12 |
| `num_heads` | 每个块的注意力头数。 | 12 |
| `channels` | 隐藏状态宽度（嵌入维度）。 | 768 |
| `num_parameters` | 模型中的总可训练权重。 | ~124M |

这些值共同定义了模型的结构和容量。

#### 它们控制的内容

+   词汇量大小将模型与分词器连接起来。每个输入标记 ID 必须小于 `vocab_size`。填充版本使得 GPU 矩阵乘法更容易。

+   最大序列长度固定了位置嵌入的大小。如果你将其设置为 1024，则模型在一次遍历中不能读取超过 1024 个标记。

+   层数控制模型深度。每一层包含一个注意力块和一个 MLP。更多的层数意味着更强的表示能力。

+   头数将注意力分割成并行的“子空间”。使用 12 个头，模型可以同时跟踪文本中的不同类型的关系。

+   通道数设置嵌入和隐藏向量的维度。更大的通道数意味着更丰富的表示，但也需要更多的计算。

+   参数是所有这些的总和。这个数字告诉你模型训练的重量以及它将消耗多少内存。

#### GPT-2 不同规模的配置

原始 GPT-2 模型有几个不同的规模：

| 模型 | 层数 | 头数 | 通道数 | 参数 |
| --- | --- | --- | --- | --- |
| GPT-2 小型 | 12 | 12 | 768 | 124M |
| GPT-2 中型 | 24 | 16 | 1024 | 355M |
| GPT-2 大型 | 36 | 20 | 1280 | 774M |
| GPT-2 XL | 48 | 25 | 1600 | 1.6B |

*llm.c* 只需在配置结构体中更改几个数字就可以在这些规模之间进行扩展。

#### 配置在代码中的位置

在 `train_gpt2.c` 和 `train_gpt2.cu` 中，你会看到类似以下的内容：

```c
[](#cb44-1)GPT2Config config = {
[](#cb44-2)    .vocab_size = 50257,
[](#cb44-3)    .max_seq_len = 1024,
[](#cb44-4)    .num_layers = 12,
[](#cb44-5)    .num_heads = 12,
[](#cb44-6)    .channels = 768,
[](#cb44-7)};
```

之后，模型使用此结构体进行初始化，日志打印所有派生信息（如 `num_parameters`）。

#### 为什么这很重要

配置是您的数据集和模型之间的合同。

+   如果 `vocab_size` 与您的分词器不匹配，您会得到崩溃。

+   如果 `max_seq_len` 太小，你会丢失上下文。

+   如果 `num_layers` 或 `channels` 对于您的 GPU 来说太大，您会耗尽内存。

通过调整配置，你可以决定是想要一个用于学习的微型模型，还是一个接近 GPT-2 XL 的巨型模型。

#### 尝试自己操作

1.  打印配置：运行训练器并注意打印的值。将它们与表中的 GPT-2 规模进行比较。

1.  缩小模型：将 `num_layers = 4`、`num_heads = 4` 和 `channels = 256` 进行更改。在 Tiny Shakespeare 上进行训练，看看运行速度有多快。

1.  增加序列长度：尝试设置 `max_seq_len = 2048`。你的 GPU 是否还能处理，或者你会得到内存不足的错误？

1.  参数计数检查：计算您的自定义配置有多少参数。将其与报告的 `num_parameters` 进行比较。

1.  分词器不匹配测试：故意设置 `vocab_size = 30000`，并观察在加载分词器时出现什么错误。

#### 要点总结

*llm.c* 中的 GPT-2 配置结构体虽小但功能强大。它定义了模型架构的所有内容：词汇表、序列长度、深度、宽度和总参数。只需调整几个整数，你就可以从在 CPU 上运行的玩具模型扩展到具有数十亿参数的巨型模型（如果您的硬件允许的话）。理解这些数字是理解如何控制 Transformer 容量的第一步。

### 22. 参数张量和内存布局

一旦设置好 GPT-2 配置，下一步重要的步骤就是分配模型的参数。这些是可训练的数字——权重和偏差——定义了模型如何处理输入标记。在 *llm.c* 中，参数以浮点数扁平数组的形式存储，而不是像 PyTorch 中的深度嵌套对象。这种选择使得代码更容易阅读，并使内存访问可预测。

#### 什么是参数？

变换器的每个部分都有自己的可训练权重：

+   嵌入表：一个用于标记，一个用于位置。

+   注意力层：查询、键、值和输出投影。

+   MLP 层：两个线性层及其偏差。

+   层归一化：缩放（`gamma`）和偏移（`beta`）值。

+   最终投影：将隐藏状态映射回词汇大小以生成 logits。

总共加起来有数亿个数字，即使是 GPT-2 小型也是如此。

#### *llm.c* 中的扁平内存设计

*llm.c* 不是单独分配每个参数，而是将所有参数存储在一个连续的内存块中。每个层都分配了这个大数组的一个切片。

这有两个好处：

1.  简单性：你只需要为所有参数分配一次 malloc（或 cudaMalloc）。

1.  性能：在 CPU 和 GPU 上，连续内存访问更快。

为了跟踪每个层的权重在块内的位置，代码使用偏移量。

#### 代码示例

在 `train_gpt2.c` 中，参数被打包到一个单独的数组中：

```c
[](#cb45-1)float* params = (float*)mallocCheck(config.num_parameters * sizeof(float));
```

之后，辅助函数计算每个子模块的指针。例如，标记嵌入权重只是第一个切片：

```c
[](#cb46-1)float* token_embedding_table = params;
```

然后程序继续前进，将块分配给位置嵌入、注意力权重等。

#### 张量的形状

即使参数存储在 1D 内存中，它们在概念上形成 2D 或 3D 张量。例如：

| 参数 | 形状 | 目的 |
| --- | --- | --- |
| 标记嵌入 | `[vocab_size, channels]` | 将标记 ID 映射到向量。 |
| 位置嵌入 | `[max_seq_len, channels]` | 添加位置信息。 |
| 注意力权重（Q, K, V, O） | `[channels, channels]` | 投影隐藏状态。 |
| MLP 层 | `[channels, 4×channels]` 和 `[4×channels, channels]` | 扩展和收缩隐藏状态。 |
| 层归一化缩放/偏移 | `[channels]` | 归一化和重新缩放特征。 |

当你看代码时，记住：这些形状是“虚拟”的。它们只是大 1D 数组切片的视图。

#### 为什么这种布局效果很好

PyTorch 或 TensorFlow 使用大量抽象来管理参数张量。*llm.c* 去掉了这些抽象：你看到的是原始内存、确切的参数数量以及它们布局的顺序。这使得模型的大小一目了然，以及为什么它使用这么多 RAM 或 VRAM。

这也意味着你可以轻松地通过直接将扁平数组写入或读取磁盘来保存和加载检查点。无需复杂的序列化格式。

#### 为什么这很重要

理解参数布局有助于你：

+   看看随着层数、头数或通道数的增加，模型的大小是如何爆炸的。

+   通过检查每个切片的大小来调试内存问题。

+   认识到训练中有多少只是大浮点数数组上的线性代数。

这种视角很强大，因为它揭示了深度学习的神秘：在其核心，GPT-2 只是反复乘以一个巨大浮点数数组的切片。

#### 试试你自己

1.  打印参数数量：在代码中添加一行以打印`config.num_parameters`。将其与 GPT-2 Small/Medium/Large 的表格进行比较。

1.  检查一个切片：打印嵌入表的前 10 个数字。它们看起来是随机的（来自初始化）。

1.  修改精度：修改代码以分配`half`（FP16）而不是`float`。你将节省多少内存？

1.  检查点预览：保存一个检查点，然后在十六进制查看器中打开它。这只是参数存储平铺的原始浮点数证明。

1.  参数缩放：将层数加倍，看看`num_parameters`如何变化。你能预测增加量吗？

#### 吸收要点

在`llm.c`中，参数不是隐藏在类或对象中。它们生活在一个扁平的内存块中，按照惯例被切割成嵌入、注意力矩阵、MLP 权重和范数。这种设计使得模型架构与内存之间的关系非常清晰，并提醒你，即使是一个十亿参数的转换器也“只是”一个巨大的数字数组。

### 23. 嵌入表：标记 + 位置

在一个转换器能够对文本进行推理之前，它首先需要将标记转换为向量。在`llm.c`中，这项工作由嵌入表处理：一个用于标记，一个用于位置。这些表是 GPT-2 的第一层，它们将普通的整数 ID 转换为神经网络可以处理的连续值。

#### 标记嵌入表

当你输入一个标记 ID 批次时，模型会在标记嵌入表中查找它们对应的向量。

+   形状：`[vocab_size, channels]`

    +   `vocab_size ≈ 50,257`（对于 GPT-2）

    +   `channels = hidden size`（GPT-2 Small 为 768）

+   每一行对应词汇表中的一个标记。

+   每行是一个大小为`channels`的密集向量。

因此，如果你的输入批次大小为`(B, T)`，查找嵌入将给出形状为`(B, T, channels)`的张量。

在代码中，这被实现为一个从平铺参数块中的数组切片：

```c
[](#cb47-1)float* token_embedding_table = params;  // first slice of parameters
```

在运行时，标记 ID 直接索引到这个表中。

#### 位置嵌入表

转换器本身并不了解词序。这就是位置嵌入的作用。

+   形状：`[max_seq_len, channels]`

    +   GPT-2 Small 中的`max_seq_len = 1024`

    +   与标记嵌入相同的通道维度

+   每个位置（0，1，2，…，1023）都有自己的向量。

在训练过程中，当模型在位置`j`看到标记`i`时，它将标记嵌入向量与位置嵌入向量`j`相加。这给模型提供了词身份和词位置。

在`llm.c`中，位置嵌入紧随标记嵌入之后在平铺参数数组中。

#### 将它们相加

嵌入层的正向传播很简单：

```c
embedding_out[token, pos] = token_embedding[token] + positional_embedding[pos]
```

这导致了一个形状为`(B, T, channels)`的张量，成为第一个转换器块的输入。

#### 这为什么重要

嵌入是离散令牌和连续数学之间的桥梁。没有它们，模型无法使用线性代数来学习模式。通过添加位置嵌入，GPT-2 能够知道以下区别：

+   “dog bites man” → `dog` 在前，`man` 在后

+   “man bites dog” → 相同的令牌，但交换位置会改变意义

这一小步至关重要：在注意力开始之前，必须同时捕捉顺序和身份。

#### 尝试自己操作

1.  检查形状：在初始化期间打印令牌和位置嵌入表的大小。确认它们匹配 `[vocab_size, channels]` 和 `[max_seq_len, channels]`。

1.  查看前五行：打印令牌嵌入表的前 5 个向量。它们应该看起来像初始化时的小随机浮点数。

1.  改变 max_seq_len：在配置中将 `max_seq_len` 加倍。这如何改变位置表的大小？训练是否仍然有效？

1.  覆盖嵌入：尝试将令牌嵌入表设置为全零。训练损失会发生什么变化？

1.  采样实验：训练了几步之后，不添加位置嵌入解码输出。结果是否变得无意义或重复？

#### 吸收要点

嵌入表是 GPT-2 的基础。令牌嵌入赋予符号意义，而位置嵌入赋予序列结构。在 *llm.c* 中，它们只是平坦参数数组中的两个切片，在正向传递的非常开始时相加——但如果没有它们，transformer 将对单词和顺序都视而不见。

### 24. 注意力栈：QKV 投影和几何

在嵌入之后，transformers 的真正魔力开始了：注意力机制。在 GPT-2 中，每个 transformer 块都包含一个注意力栈。这就是模型学习每个令牌如何与序列中的其他令牌相关联的地方——无论是关注前面的单词、句子的开头，甚至是远处的标点符号。

#### 注意力做什么

注意力让模型能够回答以下问题：

> “给定当前单词，我应该关注上下文中的哪些其他单词，以及关注程度如何？”

与独立处理单词不同，模型使用注意力在序列中建立连接。

#### Q、K、V 投影

每个注意力块都以三个线性投影开始：

| 名称 | 形状 | 目的 |
| --- | --- | --- |
| Q (Query) | `[channels, channels]` | 代表每个令牌在询问什么。 |
| K (Key) | `[channels, channels]` | 代表每个令牌如何被 *识别*。 |
| V (Value) | `[channels, channels]` | 代表实际 *信息* 要传递的内容。 |

这里是流程：

1.  每个输入向量（来自嵌入或前一个块）都乘以这三个矩阵，以产生 Q、K 和 V 向量。

1.  注意力分数是通过比较 Q 和 K 来计算的。

1.  这些分数用于加权 V，将来自其他令牌的信息混合到当前令牌中。

#### 注意力的几何

+   Q 和 K 定义了一个相似度分数：这个令牌与另一个令牌匹配得有多好？

+   V 承载实际特征（如意义、语法提示）。

+   结果是一个加权总和：标记根据注意力分数从其他标记那里借用信息。

在方程中：

```c
scores = Q × K^T / sqrt(d_k)
weights = softmax(scores + mask)
output  = weights × V
```

通过除以 `sqrt(d_k)` 来对分数进行归一化，这样当维度增长时，分数不会爆炸。

#### 多头注意力

GPT-2 不仅使用一个注意力投影——它并行使用多个，称为头部。每个头部都学会专注于不同类型的关系：

+   一个头部可能跟踪主语-谓语一致。

+   另一个可能关注标点符号和引号。

+   另一个可能将代词与其指代者联系起来。

对于 GPT-2 小型：

+   每层 12 个头部

+   每个头部在一个减少的维度（`channels / num_heads`）上工作

+   输出被连接并投影回 `channels`

这种设置是赋予转换器其灵活性的原因。

#### 在 *llm.c* 中的实现

在参数数组中，每个转换器块都有 Q、K、V 和输出投影（O）的切片。在正向传递期间：

1.  将输入乘以 Q、K、V 矩阵。

1.  重新塑形为头部。

1.  计算注意力分数（被掩码以防止向前看）。

1.  应用 softmax。

1.  乘以 V 以获得加权值。

1.  连接头部并应用 O 投影。

所有这些操作都是通过普通的矩阵乘法和 softmax 调用完成的——没有超出线性代数的魔法。

#### 为什么这很重要

注意力是 GPT-2 的核心。它是模型如何捕捉文本中从短期语法到长期连贯性的依赖关系的方式。没有 QKV，嵌入将保持孤立，模型永远无法构建上下文感知的表示。

#### 尝试自己操作

1.  打印形状：记录一层中 Q、K、V 矩阵的形状。确认它们匹配 `[channels, channels]`。

1.  可视化分数：在正向传递后，打印一个头部的注意力权重。它们是否集中在最近的标记上，还是分布在整个序列中？

1.  减少头部：将 `num_heads` 从 12 更改为 4。验证损失会发生什么？

1.  打破对称性：用零初始化所有 Q、K、V 矩阵。训练损失是否有所下降？

1.  遮蔽实验：禁用因果掩码（允许向前看）。模型是否通过完美预测未来标记来“作弊”？

#### 吸取的经验教训

注意力堆栈是标记停止孤立并开始相互交流的地方。Q、K 和 V 投影将上下文转换为加权关系，多头注意力使模型能够同时处理多种类型的依赖关系。在 *llm.c* 中，这是通过简单的线性代数实现的，使现代 NLP 中最强大的想法变得可见和可访问。

### 25. MLP 块：线性层 + 激活

在注意力在标记之间混合信息之后，GPT-2 在每个块内部应用第二个转换：MLP（多层感知器）。这部分不查看其他标记——它独立处理每个位置。但它的作用同样重要，因为它给模型提供了额外的容量，在将隐藏特征传递到下一层之前进行转换和细化。

#### MLP 的样子

每个 transformer 块都包含一个 MLP，其中包含两个线性层和一个非线性激活层之间：

```c
hidden = Linear1(x)
hidden = GELU(hidden)
out    = Linear2(hidden)
```

这种结构扩展了特征维度，然后将其压缩回，这使得网络能够学习更丰富的表示。

#### 层的形状

如果隐藏大小（通道数）是`d_model`，MLP 的工作方式如下：

| 步骤 | 形状 | 目的 |
| --- | --- | --- |
| 输入 | `[B, T, d_model]` | 每个标记的注意力输出。 |
| Linear1 | `[d_model, 4 × d_model]` | 将特征扩展 4 倍更宽。 |
| GELU | 元素级 | 引入非线性。 |
| Linear2 | `[4 × d_model, d_model]` | 将特征投影回原始大小。 |
| 输出 | `[B, T, d_model]` | 与输入相同的形状，准备进行残差添加。 |

对于 GPT-2 Small（`d_model = 768`），Linear1 扩展到 3072 个通道，然后 Linear2 将其缩减回 768。

#### 激活：GELU

GPT-2 中的激活函数是 GELU（高斯误差线性单元）。它比 ReLU 更平滑，给模型提供了处理零周围值的一种更细腻的方式。在代码中，GELU 看起来像：

```c
[](#cb51-1)float gelu(float x) {
[](#cb51-2)    return 0.5f * x * (1.0f + tanhf(0.79788456f * (x + 0.044715f * x * x * x)));
[](#cb51-3)}
```

这个公式看起来可能很复杂，但理念很简单：它平滑地将负值推向零，并保持正值流动。

#### 为什么扩展和缩减？

扩展到`4 × d_model`可能看起来很浪费，但这是故意的：

+   扩展给模型提供了更多表示每个标记中模式的能力。

+   缩放保持整体参数数量可管理。

+   一起，它们就像一个瓶颈层，迫使模型更有效地转换信息。

这种“扩展→激活→缩减”设计是 transformer 能够很好地扩展的主要原因之一。

#### 在*llm.c*中的实现

就像注意力一样，MLP 参数位于浮点数的平面数组中。每个块存储两个权重矩阵和两个偏差向量。在正向传递过程中：

1.  将输入乘以`Linear1`权重，并添加偏差。

1.  元素级应用 GELU。

1.  将其乘以`Linear2`权重，并添加偏差。

1.  通过残差连接传递结果。

由于每个位置都是独立处理的，因此 MLP 可以轻松地在标记之间并行化。

#### 为什么这很重要

MLP 是 transformer 块的非线性细化器。注意力传播信息，但 MLPs 在原地转换它，赋予模型更多的表达能力。没有 MLP，网络将主要是线性的，限制其捕获文本中复杂模式的能力。

#### 试试你自己

1.  打印形状：记录 Linear1 和 Linear2 权重的维度在一个块中。它们是否与 GPT-2 Small 的`[768, 3072]`和`[3072, 768]`相匹配？

1.  交换激活：在代码中将 GELU 替换为 ReLU。训练是否仍然有效？验证损失如何比较？

1.  减少扩展：将扩展从 4×改为 2×（`[768, 1536]`）。这对参数数量和性能有什么影响？

1.  清零 MLP：将 MLP 权重设置为零。模型是否仍然学习到任何东西，或者性能是否崩溃？

1.  比较速度：测量启用和未启用 MLP 的训练步骤时间。慢了多少？

#### 收获

GPT-2 中的 MLP 块是一个简单的两层网络，具有 GELU 激活，独立应用于每个标记。它扩展、激活和压缩特征，赋予模型非线性能力以重塑隐藏状态。在 *llm.c* 中，它通过基本的矩阵乘法和平滑的 GELU 函数实现，证明了即使是小的构建块也可以对模型学习语言的能力产生重大影响。

### 26. LayerNorm：理论实现（`doc/layernorm`）

深度神经网络在激活值过高或过低时往往会遭受不稳定的训练。为了稳定这一点，GPT-2 在每个 transformer 块内部使用 Layer Normalization（LayerNorm）。在 *llm.c* 中，LayerNorm 直接用 C 语言实现，并在存储库的 `doc/layernorm` 文件中有详细的解释，以帮助学习者理解它是如何工作的。

#### 归一化的理念

当向量通过许多层时，它们的值可能会变得不平衡——一些特征占主导地位，而其他特征则缩小。归一化通过以下方式解决这个问题：

1.  定中心：从向量均值中减去。

1.  缩放：除以标准差。

这使得每个特征向量都有均值为 0 和方差为 1，提高了稳定性。

#### 为什么叫“层”归一化？

存在着不同类型的归一化（BatchNorm、InstanceNorm 等）。LayerNorm 是特殊的，因为：

+   它在单个标记（“层”）的特征上归一化，而不是在整个批次上。

+   这使其与批大小无关，这对于批大小可能变化的 NLP 来说非常重要。

因此，如果一个隐藏向量有 768 个通道，LayerNorm 会为每个标记计算这些 768 个数字的均值和方差。

#### 可训练参数

LayerNorm 不仅是一种归一化，它还具有两个可训练的向量：

+   γ（gamma）：在归一化后缩放每个特征。

+   β（beta）：在归一化后移动每个特征。

这些允许网络在必要时“撤销”归一化，提供灵活性。

#### 公式

对于每个大小为 `d` 的输入向量 `x`：

```c
mean = (1/d) * Σ x_i
var  = (1/d) * Σ (x_i - mean)²
x_norm = (x - mean) / sqrt(var + eps)
y = γ * x_norm + β
```

其中 `eps` 是一个非常小的常数（如 `1e-5`），以避免除以零。

#### 在 *llm.c* 中的实现

在代码中，LayerNorm 被实现为一个简单的函数，它遍历特征，计算均值和方差，并应用上述公式。它并没有隐藏在框架内部——它就在 C 语言中，因此你可以逐行进行调试。

例如，前向传递看起来像这样（简化版）：

```c
[](#cb53-1)void layernorm_forward(float* out, float* inp, float* weight, float* bias, int N) {
[](#cb53-2)    float mean = 0.0f, var = 0.0f;
[](#cb53-3)    for (int i = 0; i < N; i++) mean += inp[i];
[](#cb53-4)    mean /= N;
[](#cb53-5)    for (int i = 0; i < N; i++) var += (inp[i] - mean) * (inp[i] - mean);
[](#cb53-6)    var /= N;
[](#cb53-7)    float inv_std = 1.0f / sqrtf(var + 1e-5f);
[](#cb53-8)    for (int i = 0; i < N; i++) {
[](#cb53-9)        out[i] = (inp[i] - mean) * inv_std * weight[i] + bias[i];
[](#cb53-10)    }
[](#cb53-11)}
```

这是一种清晰、低级的实现，使得 *llm.c* 具有教育意义。

#### 它在 GPT-2 中的位置

每个 transformer 块包含两个 LayerNorm：

+   在注意力之前。

+   在 MLP 之前。

GPT-2 使用 Pre-LN 架构：在每个子层之前对输入进行归一化。这使得训练更加稳定，梯度流动得更好。

#### 它为什么很重要

LayerNorm 可能看起来是一个小细节，但没有它，GPT-2 就无法可靠地训练。它平滑了激活流的流动，以便注意力和 MLP 层可以完成它们的工作。在实践中，这是关键的“胶水”组件之一，使得深度变压器能够在规模上可训练。

#### 试试看

1.  打印统计数据：在应用 LayerNorm 后，打印输出的均值和方差。它们是否接近 0 和 1？

1.  移除 γ 和 β：将 gamma 强制设为 1，beta 设为 0。模型是否仍然可以训练？比较损失。

1.  禁用归一化：取消注释 LayerNorm 并训练。训练变得有多不稳定？

1.  比较位置：尝试切换到 Post-LN（在注意力/MLP 之后应用归一化）。这会改变收敛速度吗？

1.  改变 epsilon：将 `1e-5` 改为 `1e-2` 或 `1e-8`。训练有多敏感？

#### 吸收要点

LayerNorm 是 GPT-2 的安静稳定器。它确保每个标记的特征保持平衡，而 γ 和 β 保持灵活性。在 *llm.c* 中，它通过清晰的 C 代码直接实现，让你看到归一化是如何计算的。它是 Transformer 模式中的一个微小但不可或缺的部分。

### 27. 残差连接：保持信号流动

GPT-2 这样的 Transformer 并不是盲目地堆叠层。它们使用残差连接——这是一种允许层的输入加回到其输出的技巧。这种简单的加法有助于信号在网络中流动，而不会消失或爆炸，并使得训练深度模型成为可能。

#### 基本思想

想象你有一个函数 `F(x)` 代表某种转换（如注意力或 MLP）。而不是仅仅计算：

```c
y = F(x)
```

Transformer 做的是：

```c
y = F(x) + x
```

这意味着该层只学习需要添加到输入中的 *差异*，而不是完全替换它。

#### 为什么这有帮助

残差解决了深度网络中的两个大问题：

1.  梯度流：在反向传播过程中，梯度在通过许多层时会变得越来越小。将输入加回确保梯度始终有一条直通路径。

1.  信息保留：即使 `F(x)` 扭曲了信号，原始的 `x` 仍然存在。这防止了模型“忘记”重要信息。

1.  更快的训练：网络不必重新学习恒等映射——它只需通过跳跃连接传递它们。

#### 在 *llm.c* 中的实现

*llm.c* 中的残差实现为一个简单的逐元素加法：

```c
[](#cb56-1)void residual_forward(float* out, float* inp1, float* inp2, int N) {
[](#cb56-2)    for (int i = 0; i < N; i++) {
[](#cb56-3)        out[i] = inp1[i] + inp2[i];
[](#cb56-4)    }
[](#cb56-5)}
```

这里：

+   `inp1` 是层的输出（如注意力）。

+   `inp2` 是原始输入。

+   `out` 是组合结果。

这是对每个标记位置和特征通道都进行的。

#### 残差的使用位置

在 GPT-2 中，每个 Transformer 模块包含两个残差连接：

1.  注意力残差：将注意力层的输入加到其输出上。

1.  MLP 残差：将 MLP 的输入加到其输出上。

因此，通过网络流动的数据始终携带新的转换和原始信号。

#### 为什么这很重要

没有残差连接，堆叠 12-48 个 Transformer 模块几乎不可能进行训练。梯度会消失，模型要么停止学习，要么永远无法收敛。残差让深度 Transformer 能够平滑地扩展。

他们还添加了一个直观的解释：每个块就像一个“细化步骤”，而不是对表示的全面重写。

#### 试试看

1.  移除剩余项：在代码中取消注释的添加。训练会崩溃吗？

1.  缩放剩余项：在相加之前将输入乘以 0.5。这会减慢收敛速度吗？

1.  检查损失曲线：比较前 500 步有和无剩余项的训练。

1.  检查输出：打印 `inp1`、`inp2` 和 `out` 的范数。比例是否平衡？

1.  更深的模型：将层数从 12 增加到 24。剩余项的重要性变得更加明显吗？

#### 吸收要点

剩余连接是深度变换器的“生命线”。通过简单地将输入加回到输出中，它们使得训练非常深的网络而不会丢失梯度或信息成为可能。在 *llm.c* 中，实现起来就像遍历数组并将它们相加一样简单——但效果是深远的：剩余项使得 GPT-2 能够深入且仍然有效工作。

### 28. 注意力掩码：强制因果性

GPT-2 的一个定义特征是它是一个因果语言模型。这意味着它根据所有之前的标记预测下一个标记，但永远不会通过向前看而作弊。为了强制执行这一点，GPT-2 在每个注意力层中应用一个注意力掩码。

#### 为什么需要掩码

没有掩码，注意力可以自由地将任何标记连接到任何其他标记，包括未来的标记。例如：

+   输入： “The cat sat on the”

+   目标： “mat”

如果模型在计算注意力时可以查看“mat”，那么任务将变得非常简单——它只需复制下一个单词即可。这将破坏训练目标。

掩码迫使模型在做出预测时只使用当前位置或之前的标记。

#### 掩码是如何工作的

在计算注意力分数（`Q × K^T / sqrt(d_k)`）时，结果是大小为 `[T, T]` 的矩阵，其中每一行对应一个标记正在关注所有其他标记。

掩码修改了这个矩阵：

+   允许的位置（过去和现在）：保持分数不变。

+   不允许的位置（未来）：将分数设置为 `-inf`。

应用 softmax 后，那些 `-inf` 条目变为零概率，有效地阻止了对未来的注意力。

#### 在 *llm.c* 中的实现

因果掩码在注意力前向传递期间应用。代码使用循环将无效位置清零：

```c
[](#cb57-1)for (int t = 0; t < T; t++) {
[](#cb57-2)    for (int u = t + 1; u < T; u++) {
[](#cb57-3)        scores[t][u] = -1e9; // block future positions
[](#cb57-4)    }
[](#cb57-5)}
```

这里 `T` 是序列长度。这确保了标记 `t` 只能关注自身和更早的标记。

#### 可视化掩码

将掩码想象成一个三角形矩阵：

|  | 0 | 1 | 2 | 3 |
| --- | --- | --- | --- | --- |
| 0 | ✓ |  |  |  |
| 1 | ✓ | ✓ |  |  |
| 2 | ✓ | ✓ | ✓ |  |
| 3 | ✓ | ✓ | ✓ | ✓ |

每行显示给定位置可以查看的过去标记。未来的位置保持空白。

#### 为什么这很重要

掩码使得 GPT-2 成为一个预测模型，而不是像 BERT 这样的双向编码器。没有它，模型可能会“作弊”，训练目标将不再与推理时间（逐个生成文本）的使用相匹配。

这个小细节——只是将矩阵的一部分填充为 `-inf`——对于使自回归文本生成成为可能至关重要。

#### 试试看

1.  禁用掩码：取消注释掩码代码。观察验证损失不切实际地下降，然后注意文本生成产生了垃圾。

1.  反转掩码：阻止过去并允许未来。模型是否仍然训练？它预测了什么？

1.  部分掩码：只允许关注前 5 个标记（滑动窗口）。这如何影响学习长距离结构？

1.  打印分数：在掩码前后记录一行注意力分数。注意未来位置变成了巨大的负数。

1.  可视化：编写一个小脚本，将注意力掩码作为矩阵绘制。它应该看起来是严格下三角的。

#### 吸收要点

注意力掩码是一个简单但至关重要的技巧。通过在 softmax 之前用`-inf`填充未来位置，GPT-2 确保每个标记只能关注其过去。在*llm.c*中，这通过几个循环实现——但它将一个通用的 transformer 转变为真正的因果语言模型。

### 29. 输出头：从隐藏状态到词汇表

在标记通过嵌入、注意力、MLP、LayerNorm 和残差后，我们得到了序列中每个位置的隐藏状态。但 GPT-2 的最终任务不是输出向量——它必须从词汇表中预测下一个标记。这是由输出头，模型的最后阶段来处理的。

#### 输出头的作用

输出头将形状为`(B, T, channels)`的隐藏状态映射到形状为`(B, T, vocab_size)`的 logits。每个 logit 代表模型在下一步中特定标记可能性的“原始分数”。

管道看起来像这样：

```c
hidden states → Linear projection → Logits → Softmax → Probabilities
```

+   Logits：实数，每个词汇表中的标记一个。

+   Softmax：将 logits 转换为总和为 1 的概率。

+   预测的标记：概率最高的标记（或从分布中采样）。

#### 与嵌入绑定的权重

在 GPT-2 中，标记嵌入表和输出头共享权重。这意味着相同的矩阵既用于：

+   在开始时将标记映射到向量（嵌入查找）。

+   在最后将向量映射回标记（输出头）。

从数学上讲，这提高了效率并有助于对齐输入和输出表示。

在*llm.c*中，这是通过简单地让嵌入和输出头指向相同的参数切片来实现的。

```c
[](#cb59-1)// token embedding table
[](#cb59-2)float* token_embedding_table = params;
[](#cb59-3)// output head reuses the same memory
[](#cb59-4)float* output_head = token_embedding_table;
```

当模型将隐藏状态投影回词汇空间时，它使用这个共享矩阵进行矩阵乘法。

#### 形状的实际应用

对于 GPT-2 Small：

+   隐藏状态：`[B, T, 768]`

+   输出投影（嵌入转置）：`[768, 50257]`

+   Logits：`[B, T, 50257]`

这比每个位置 50k 个分数还要多，每个词汇表中的标记一个。

#### 为什么权重绑定有帮助

1.  内存效率：您不需要为输出头单独使用一个巨大的矩阵。

1.  更好的学习：代表进入的标记的相同向量也代表它们出去，这加强了一致性。

1.  简单的代码：只需重用相同的参数切片。

这个技巧是为什么 GPT-2 可以在不过度增加参数计数的情况下扩展词汇大小。

#### 为什么这很重要

输出头是所有东西汇集的地方。对于每个位置，模型将其隐藏表示折叠成可能的下一个标记的分布。这就是 GPT-2 逐个步骤生成文本的方式。如果没有这一步，你将只有抽象的隐藏状态——虽然对内部有用，但不是你可以阅读的内容。

#### 尝试自己操作

1.  打印 logits：在正向传播之后，打印最后一个标记的 logits。它们在初始化时看起来像是随机的浮点数吗？

1.  检查概率和：对 logits 应用 softmax 并验证概率之和是否为 1。

1.  解绑权重：使输出头成为自己的矩阵而不是重用嵌入。训练是否仍然有效？参数计数如何变化？

1.  Top-k 样本采样：修改采样以保留 softmax 之前的 top 5 logits。这会产生什么样的文本？

1.  贪婪解码与随机采样：比较贪婪解码（argmax）与从概率中进行随机采样的结果。哪一个会产生更有趣的输出？

#### 吸收要点

输出头是隐藏向量与实际单词之间的最终桥梁。通过重用标记嵌入矩阵，GPT-2 将隐藏状态重新投影到词汇空间，并为每个可能的标记生成 logits。在 *llm.c* 中，这一步只是另一个矩阵乘法——但它将内部数学转化为实际的文本预测。

### 30. 损失函数：词汇表上的交叉熵

训练 GPT-2 意味着教会它预测序列中的下一个标记。为了衡量其表现的好坏，我们需要一个损失函数，该函数将模型的预测概率与真实标记 ID 进行比较。在 *llm.c* 中，这是通过交叉熵损失来完成的——这是分类任务的标准选择。

#### 从 logits 到概率

在输出头之后，我们得到形状为 `(B, T, vocab_size)` 的 logits。这些是原始分数。要将它们转换为概率：

```c
probs = softmax(logits)
```

Softmax 确保两件事：

+   所有值都在 0 到 1 之间。

+   它们在词汇表中加起来为 1。

因此，对于每个位置，你都会得到一个覆盖所有可能下一个标记的概率分布。

#### 交叉熵定义

交叉熵比较预测分布 `p` 与真实分布 `q`。对于语言建模：

+   `q` 是一个 one-hot 向量（除了在真实标记索引处的 1 以外，其余都是 0）。

+   `p` 是 softmax 的概率向量。

单个标记的公式：

```c
loss = -log(p[true_token])
```

对于一个批次，你会在所有序列的所有标记上平均。

#### 在 *llm.c* 中的实现

在 C 语言中，这归结为：

```c
[](#cb62-1)float loss = 0.0f;
[](#cb62-2)for (int b = 0; b < B; b++) {
[](#cb62-3)    for (int t = 0; t < T; t++) {
[](#cb62-4)        int target = targets[b*T + t];
[](#cb62-5)        float logit_max = -1e9;
[](#cb62-6)        for (int v = 0; v < vocab_size; v++) {
[](#cb62-7)            if (logits[b*T*vocab_size + t*vocab_size + v] > logit_max) {
[](#cb62-8)                logit_max = logits[b*T*vocab_size + t*vocab_size + v];
[](#cb62-9)            }
[](#cb62-10)        }
[](#cb62-11)        // compute softmax denominator
[](#cb62-12)        float sum = 0.0f;
[](#cb62-13)        for (int v = 0; v < vocab_size; v++) {
[](#cb62-14)            sum += expf(logits[b*T*vocab_size + t*vocab_size + v] - logit_max);
[](#cb62-15)        }
[](#cb62-16)        float logprob = logits[b*T*vocab_size + t*vocab_size + target] - logit_max - logf(sum);
[](#cb62-17)        loss += -logprob;
[](#cb62-18)    }
[](#cb62-19)}
[](#cb62-20)loss /= (B * T);
```

这个片段展示了 *llm.c* 如何在循环中显式地计算 softmax 和交叉熵。没有黑盒——只有原始数学。

#### 直觉

+   如果模型将高概率分配给正确标记 → 损失较小。

+   如果模型将低概率分配给正确标记 → 损失较大。

+   最小化损失意味着将概率质量推向正确答案。

#### 为什么交叉熵适用于语言

语言建模本质上是一个巨大的多类分类问题：在每一步，下一个单词是什么？交叉熵在这里是完美的，因为它直接按模型自信程度成比例惩罚错误预测。

#### 为什么这很重要

损失函数是模型关于其表现如何的唯一信号。所有其他——参数更新、权重调整、学习动态——都从这个单一数字中流出。一个实现良好的交叉熵确保训练稳定且有意义。

#### 尝试自己操作

1.  检查值：打印前几步后的损失。在训练之前，它应该接近 `log(vocab_size)`（对于 50k 词汇量约为 10.8）。

1.  过拟合小批量：仅在一个序列上训练。经过足够的步骤后，损失是否接近 0？

1.  改变目标：用随机的一个替换真实标记。损失是否会立即增加？

1.  比较词汇量大小：使用较小的词汇量（例如，100 个标记）进行训练。初始损失是否下降到`log(100) ≈ 4.6`？

1.  检查概率：对于单个标记，打印前 5 个预测概率。随着训练的进行，真实标记是否上升到顶部？

#### 吸收要点

交叉熵损失是指导 GPT-2 训练的指南。它将原始 logits 转换为概率，并衡量模型预测正确下一个标记的能力。在`llm.c`中，它通过显式循环和数学实现，让你看到概率和损失是如何计算的。没有这一步，模型将无法从其错误中学习。

## 第四章. CPU 推理（仅前向）

### 31. 前向传递概述

当我们谈论 GPT-2 中的*前向传递*时，我们指的是将输入句子（如“猫坐在”）转换为下一个单词预测的过程。简单来说，这是模型在给出答案之前“思考”的方式。在`train_gpt2.c`中，这发生在`gpt2_forward`函数内部。让我们一步一步地慢慢走，这样你可以看到数字是如何通过模型并在这个过程中转换的。

#### 1. 从单词到数字

计算机不理解像*猫*或*坐*这样的单词。它们只理解数字。在前向传递开始之前，文本已经被标记化为 ID（整数）。例如：

```c
"The cat sat" → [464, 3290, 616]
```

每个数都是一个标记 ID。模型目前还不知道“464”在普通英语中的含义——它只知道它是一个指向表的数字。

#### 2. 嵌入：赋予单词意义

前向传递的第一步是嵌入查找。想象我们有一个巨大的字典，但每个单词 ID 指向一个由数字组成的长向量（例如，GPT-2 小型为 768 个数字）。

+   单词嵌入（`wte`）：每个标记 ID 变成一个向量，它捕捉单词的意义。

+   位置嵌入（`wpe`）：每个标记也为其位置获得一个向量：第一个单词、第二个单词、第三个单词等。

模型将这两个向量相加。这样，它不仅知道单词是什么，还知道它在句子中的位置。

例如：

| 标记 | 词 | 词嵌入（缩短） | 位置嵌入（缩短） | 合成向量 |
| --- | --- | --- | --- | --- |
| 464 | “The” | [0.2, -0.5, 0.1, …] | [0.0, 0.1, -0.3, …] | [0.2, -0.4, -0.2, …] |
| 3290 | “cat” | [0.9, -0.2, 0.4, …] | [0.1, -0.1, -0.2, …] | [1.0, -0.3, 0.2, …] |

现在每一个标记都是一个内置了意义和位置的向量。

#### 3. 变换层：思考步骤

GPT-2 有多个相同的层堆叠在一起。每一层有两个主要部分：注意力和 MLP（前馈网络）。

注意力（环顾四周）：

+   每个词都会问：“我现在应该关注哪些其他词？”

+   对于“sat”，注意力可能会高度集中在“cat”上，因为这两个词相关。

+   代码为每个词计算 *queries*、*keys* 和 *values*，然后进行点积、softmax 和加权求和以混合信息。

MLP（深度处理）：

+   注意力之后，每个标记通过一个微型神经网络（两个矩阵乘法，中间有一个非线性 GELU 函数）。

+   这有助于每个词细化其理解，即使它没有直接关注另一个词。

两个块都有残差连接：输入被加回到输出中，就像在保留原有笔记的同时添加新的见解。这防止了信息丢失。

#### 4. 归一化：保持数字稳定

在许多点上，模型会规范化向量，以防止它们尺寸爆炸或变得太小。这被称为 LayerNorm。它确保训练稳定，就像确保你的烹饪锅不会溢出或烧干一样。

#### 5. 最终预测层

在所有层之后，模型为每个位置生成一个最终向量。然后：

+   它将这些向量再次乘以嵌入表（但转置）。

+   这给出了 logits：词汇表中每个词的原始分数（大约 50k 个选项）。

示例：对于最后一个标记“on the”，logits 可能如下：

| 词 | Logit | 概率（softmax 后） |
| --- | --- | --- |
| “mat” | 7.2 | 0.85 |
| “dog” | 5.1 | 0.10 |
| “car” | 3.0 | 0.05 |

最高概率的是“mat”。

#### 6. Softmax：将分数转换为概率

logits 是大数字，但它们没有多少意义，直到我们应用 softmax。softmax 将它们转换为概率，这些概率之和为 1。这样，我们可以将它们解释为机会：“下一个词是 *mat* 的概率是 85%。”

#### 7. 交叉熵损失：衡量错误

如果我们在训练，我们也会给模型正确的下一个词。模型检查它给了这个词多少概率。如果它给了高概率，损失就低。如果它给了低概率，损失就高。

+   正确： “mat”（概率 0.85 → 损失 ≈ 0.16，较小）。

+   错误： “car”（概率 0.05 → 损失 ≈ 3.0，较大）。

这个损失是所有标记的平均值，它是告诉反向传播如何更新模型的信号。

#### 8. 它为什么重要

前向传递是 GPT-2 生成预测的部分。没有它，模型就无法“思考”或理解输入。它就像大脑在决定做什么之前处理感官输入。在 `train_gpt2.c` 中，前向传递是用纯 C 循环编写的，这使得数学运算清晰可见，而不是隐藏在深度学习库中。

#### 9. 尝试自己操作

1.  打印嵌入：修改代码以打印第一个标记的向量。看看它只是数字，但那些数字是“意义”。

1.  检查概率：在前向传递之后，打印一个位置的 softmax 概率。它们应该总和为 1.0。

1.  改变序列长度：将 `T` 从 64 增加到 128。注意验证速度会减慢，因为注意力机制需要比较所有标记与其他所有标记（`T²` 缩放）。

1.  基准损失：在训练之前，测量损失。它应该大约是 `log(vocab_size)`（对于 GPT-2 小型模型约为 10.8）。这是随机猜测的损失。

1.  遮蔽实验：暂时移除注意力中的因果遮蔽。模型将通过向前看“作弊”，损失将不切实际地下降。

#### 吸收要点

前向传递就像 GPT-2 的思维过程。输入单词变成向量，向量通过注意力和 MLP 混合，一切都被归一化，最后模型为下一个单词产生概率。这是一场精心编排的数学运算舞蹈，所有代码都在 `train_gpt2.c` 中的纯 C 循环中编写。一旦你理解了这个流程，你就可以精确地了解 GPT-2 如何将原始标记转换为智能预测。

### 32. 标记和位置嵌入查找

在 GPT-2 能够对文本进行任何智能操作之前，它需要将原始数字（标记 ID）转换为包含意义和上下文的向量。这是嵌入的作用。在 `train_gpt2.c` 中，这一步由函数 `encoder_forward` 处理。让我们更详细地看看它是如何工作的以及为什么它很重要。

#### 标记只是数字

假设你输入：

```c
"The cat sat on the mat."
```

在标记化之后，这个句子可能看起来像：

```c
[464, 3290, 616, 319, 262, 1142, 13]
```

这些只是 ID。模型本身并不知道 `3290` 代表“猫”。它只知道它需要使用这些数字从表中获取向量。

#### 嵌入表

模型在内存中存储了两个重要的表：

1.  词标记嵌入 (`wte`)

    +   大小：`(V, C)` 其中 `V` 是词汇量（对于 GPT-2 小型模型约为 50,000）和 `C` 是通道（768）。

    +   每一行对应一个标记 ID。

    +   示例：行 3290 可能是 `[0.12, -0.45, 0.88, …]`。

1.  位置嵌入 (`wpe`)

    +   大小：`(maxT, C)` 其中 `maxT` 是最大序列长度（例如 1024）。

    +   每一行对应一个位置索引：0 对应第一个标记，1 对应第二个，等等。

    +   示例：位置 2 可能是 `[0.07, 0.31, -0.22, …]`。

两个表都填充了可训练的值。一开始，它们是随机的。随着训练的进行，优化器会更新它们，以便它们能够编码有用的模式。

#### 将它们相加

对于位置 `t` 的每个标记：

+   从 `wte` 中查找其词向量。

+   从 `wpe` 中查找其位置向量。

+   将它们逐元素相加。

这给出了一个大小为 `C` 的最终向量，它代表了标记是什么以及它在哪。

使用简化数字的示例：

| 标记 ID | 单词 | 单词嵌入 | 位置 | 合并 |
| --- | --- | --- | --- | --- |
| 464 | The | [0.1, -0.2, 0.3] | [0.2, 0.0, -0.1] | [0.3, -0.2, 0.2] |
| 3290 | cat | [0.4, 0.5, -0.3] | [0.0, 0.1, 0.2] | [0.4, 0.6, -0.1] |

现在，这个向量不仅仅意味着“猫”，它意味着“位置 1 的猫”。

#### 为什么位置很重要

没有位置，模型将处理：

+   “The cat sat”

+   “Sat cat the”

因为它们使用相同的标记，所以它们看起来是相同的。但在语言中，单词顺序至关重要。通过添加位置嵌入，GPT-2 能够区分“dog bites man”和“man bites dog”。

#### 代码内部

嵌入查找是用 C 语言中的循环明确编写的：

```c
[](#cb66-1)for (int b = 0; b < B; b++) {
[](#cb66-2)    for (int t = 0; t < T; t++) {
[](#cb66-3)        float* out_bt = out + b * T * C + t * C;
[](#cb66-4)        int ix = inp[b * T + t];
[](#cb66-5)        float* wte_ix = wte + ix * C;
[](#cb66-6)        float* wpe_t = wpe + t * C;
[](#cb66-7)        for (int i = 0; i < C; i++) {
[](#cb66-8)            out_bt[i] = wte_ix[i] + wpe_t[i];
[](#cb66-9)        }
[](#cb66-10)    }
[](#cb66-11)}
```

这里发生了什么：

+   遍历批次（`b`）和序列位置（`t`）。

+   找到标记 ID `ix`。

+   获取其嵌入 `wte_ix`。

+   获取其位置嵌入 `wpe_t`。

+   逐元素相加。

结果 `out_bt` 是这个位置这个标记的向量。

#### 类比

想象它就像会议上的名牌：

+   单词嵌入是你的名字：“Alice。”

+   位置嵌入是你的桌子号：“桌子 7。”

+   一起，它们告诉会议工作人员你是谁以及你坐在哪里。

没有表格号，他们可能知道你是谁，但不知道在哪里找到你。没有你的名字，他们只知道桌子 7 号有一个人，但不知道是谁。两者都是正确上下文所必需的。

#### 为什么这很重要

嵌入是整个模型的基础。如果这一步出错，其他所有步骤都会崩溃。它们将无意义的 ID 转换成携带语义和位置信息的丰富向量。这是语言开始变得神经网络可以推理的地方。

#### 尝试自己操作

1.  打印标记嵌入：修改代码以打印特定标记 ID（如“猫”）的 `wte_ix`。你会看到一个浮点向量，这是学习到的表示。

1.  打印位置嵌入：对位置 0、1、2 等处的 `wpe_t` 做同样的操作，注意位置有独特但一致的规律。

1.  检查总和：验证 `out_bt[i] = wte_ix[i] + wpe_t[i]`。这正是单词和位置融合的方式。

1.  混洗单词：尝试输入“cat sat”与“sat cat。” 嵌入将不同，因为位置向量改变了，尽管单词是相同的。

1.  观察训练过程中的增长：经过一些训练步骤后，再次导出嵌入。你会注意到它们不再随机，而是开始显示结构。

#### 吸取的经验

嵌入查找是前向传递的第一步。它通过结合标记身份和位置将原始数字变得有意义，为更深的 transformer 层准备输入。尽管 C 代码看起来简单——几个嵌套循环——但它正在做给单词赋予模型可以理解的数学形状的关键工作。

### 33. 注意：CPU 上的 Matmuls、掩码和 Softmax

注意力机制是 GPT-2 的核心。它决定了输入序列中的每个单词在形成其表示时，要查看哪些其他单词。在 `train_gpt2.c` 中，这发生在 `attention_forward` 函数内部，该函数使用纯 C 循环和矩阵乘法实现多头自注意力。让我们仔细分解，一步一步，以便即使是完全的初学者也能理解流程。

#### 注意力的核心思想

想象你正在阅读：

> “The cat sat on the mat.”

当模型试图理解单词 *sat* 时，它不仅仅关注 *sat* 本身。它还想要考虑其他单词，如 *cat*（主题）和 *mat*（可能是宾语）。注意力机制为每个标记提供了一种“咨询”更早标记并决定它们重要性的方式。

这通过将每个标记投影到三个角色：查询（Q）、键（K）和值（V）来数学上完成。

+   查询（Q）：“我在寻找什么？”

+   键（K）：“我提供什么？”

+   值（V）：“我携带什么信息？”

#### 第 1 步：创建 Q、K 和 V

对于每个大小为 `C`（例如，768）的输入向量，代码执行三个独立的线性投影（矩阵乘法）。这些产生 Q、K 和 V 向量，其尺寸较小，分配给注意力头。

在代码中：

```c
[](#cb67-1)matmul_forward(acts.q, acts.ln1, params.wq, params.bq, B, T, C, C);
[](#cb67-2)matmul_forward(acts.k, acts.ln1, params.wk, params.bk, B, T, C, C);
[](#cb67-3)matmul_forward(acts.v, acts.ln1, params.wv, params.bv, B, T, C, C);
```

在这里：

+   `acts.ln1` 是前一步的归一化输入。

+   `params.wq`、`params.wk`、`params.wv` 是权重矩阵。

+   输出形状是 `(B, T, C)`。

因此，每个标记现在有三个新的表示：Q、K 和 V。

#### 第 2 步：计算注意力分数

对于位置 `t` 的每个标记，我们想知道它应该对每个更早的标记（包括它自己）关注多少。这是通过其查询与所有键之间的点积来完成的。

数学上：

```c
score[t][u] = (Q[t] ⋅ K[u]) / sqrt(dk)
```

+   `t` = 当前标记。

+   `u` = 位置 `t` 或之前的另一个标记。

+   `sqrt(dk)` 是一个缩放因子（dk = 每个头的尺寸），以保持值稳定。

在代码中，这些点积在循环中显式完成。

#### 第 3 步：应用因果掩码

GPT-2 是一个自回归模型，这意味着它只能从过去预测未来，而不是反过来。为了强制执行这一点，注意力矩阵被掩码：

+   位置 `t` 的标记只能查看位置 `≤ t`。

+   任何超过 `t` 的内容都被设置为非常负的值（`-1e9`），在 softmax 后变为实际上为零。

这确保了例如，当预测第 3 个单词时，模型不会通过查看第 4 个单词来作弊。

#### 第 4 步：将分数转换为概率

分数是原始数字，可能很大且不稳定。为了将它们转换为有意义的权重，代码应用了 softmax：

```c
attention_weights[t][u] = exp(score[t][u]) / Σ exp(score[t][v])
```

这使得所有权重都为正，并确保它们的总和为 1。现在每个标记都有一个关于更早标记的概率分布。

*sat* 这个单词的例子：

| 关注的标记 | 原始分数 | 经过 Softmax |
| --- | --- | --- |
| The | 1.2 | 0.10 |
| cat | 3.4 | 0.80 |
| sat (自身) | 0.7 | 0.10 |
| on | 遮蔽 | 0.00 |

显然，*sat* 最强烈地关注 *cat*。

#### 第 5 步：值的加权和

一旦计算了注意力权重，模型就使用它们对值向量取加权求和：

```c
output[t] = Σ attention_weights[t][u] * V[u]
```

这为每个标记产生一个新的表示，其中融合了来自其他标记的信息。

对于*sat*，其新向量将主要受*cat*影响，但也受*The*和它自己的一小部分影响。

#### 第 6 步：多头注意力

在实践中，注意力被分成多个头部（GPT-2 小的 12 个）。每个头部处理向量的小块（C/heads）。

+   第 1 个头部可能专注于主谓关系。

+   第 2 个头部可能跟踪距离（例如“这个标记有多远？”）。

+   第 3 个头部可能专门处理标点符号。

在所有头部计算完它们的输出后，结果通过另一个矩阵乘法连接并投影回大小`C`。

#### 第 7 步：残差连接

最后，将注意力块的输出加回到原始输入（残差连接）上。这保持了原始信号流动，即使注意力引入了扭曲。

```c
[](#cb71-1)residual_forward(acts.residual2, acts.ln1, acts.att, B*T, C);
```

这确保了信息不会丢失，并且在训练过程中梯度流动顺畅。

#### 为什么这很重要

注意力是让 GPT-2 捕捉词语之间关系的机制。没有它，模型将独立处理每个标记，丢失上下文。通过为每个标记显式计算“我应该看谁？”，GPT-2 学习到主谓一致、长距离依赖甚至风格细微差别等模式。

#### 试试你自己

1.  检查注意力掩码：打印出掩码前后的分数。注意未来标记被设置为巨大的负值。

1.  可视化权重：在简短的句子上运行注意力并绘制权重。你会看到哪些词关注哪些词。

1.  改变序列长度：尝试增加`T`并观察计算如何呈二次方增长（`T²`）。注意力是昂贵的！

1.  尝试使用头部：强制模型只使用 1 个头部而不是 12 个。看看这如何限制它所能捕捉到的模式多样性。

1.  检查权重总和：对于单个标记，验证所有注意力权重在 softmax 后加起来等于 1.0。

#### 吸取的经验

注意力是使 transformers 强大的原因。它允许每个词动态决定哪些其他词对其在句子中的作用很重要。在`train_gpt2.c`中，这个过程通过显式的循环和矩阵乘法来表述，因此你可以跟随数学的每一步。理解这一部分给你提供了理解为什么 GPT-2 以及所有现代 LLM 工作得如此之好的关键。

### 34. MLP：GEMMs 和激活函数

在注意力块让标记“互相交谈”之后，GPT-2 应用第二种称为 MLP 块（多层感知器）的转换。与混合标记之间信息的注意力不同，MLP 独立处理每个标记，丰富其内部表示。尽管它看起来比注意力简单，但 MLP 对于捕捉语言中的复杂关系至关重要。

#### MLP 的功能

每个标记的向量（大小`C`，例如 GPT-2 小的 768）都会经过：

1.  线性扩展：从大小 `C` 投影到大小 `4C`（GPT-2 小型中的 3072）。

1.  非线性激活：应用 GELU 函数，增加了灵活性。

1.  线性投影回：将大小从 `4C` 减少到 `C`。

1.  残差连接：将输入向量加回到输出中，保持原始信号完整。

这使得模型不仅可以在标记之间共享信息（通过注意力），还可以细化每个标记如何代表自己。

#### 第 1 步：使用矩阵乘法扩展

第一步是将每个标记的向量从 768 维扩展到 3072 维。这是通过一般矩阵乘法（GEMM）完成的：

```c
[](#cb72-1)matmul_forward(acts.mlp_in, acts.ln2, params.wfc, params.bfc, B, T, C, 4*C);
```

+   `acts.ln2`：来自先前残差的归一化输入。

+   `params.wfc`：大小为 `(C, 4C)` 的权重矩阵。

+   `params.bfc`：大小为 `(4C)` 的偏置向量。

+   `acts.mlp_in`：结果，形状 `(B, T, 4C)`。

想象它就像拉伸橡皮筋——突然，标记有更多的空间来表达更丰富的特征。

#### 第 2 步：GELU 激活

扩展后，每个数字都通过 GELU（高斯误差线性单元）。

公式：

```c
GELU(x) = 0.5 * x * (1 + tanh(√(2/π) * (x + 0.044715 * x³)))
```

这看起来很复杂，但关键思想是：

+   对于小的负数，输出 ≈ 0（忽略弱信号）。

+   对于大的正数，输出 ≈ x（传递强信号）。

+   对于中间的数字，它平滑地混合。

与 ReLU 不同，ReLU 只切断负数，GELU 以概率方式让小信号通过。这使得它在语言处理中更好，因为即使是微小的提示也很重要。

类比：想象你在批改作业。如果一个答案完全错误，你给 0 分（ReLU 风格）。如果它完美无缺，你给满分。但如果它部分正确，你给部分分数。GELU 就像那样——软性、细微的评分。

#### 第 3 步：向下投影

一旦标记向量被扩展并通过 GELU，它就被投影回原始大小 `C`：

```c
[](#cb74-1)matmul_forward(acts.mlp_out, acts.mlp_in_gelu, params.wproj, params.bproj, B, T, 4*C, C);
```

+   `params.wproj`：投影权重，大小 `(4C, C)`。

+   `params.bproj`：偏置，大小 `(C)`。

+   `acts.mlp_out`：结果，形状 `(B, T, C)`。

现在每一个标记都已经经历了一个非线性的“思考步骤”，混合和重塑特征。

#### 第 4 步：残差连接

就像注意力一样，MLP 输出被加回到输入中：

```c
[](#cb75-1)residual_forward(acts.residual3, acts.residual2, acts.mlp_out, B*T, C);
```

这意味着标记在添加新的改进的同时保留了其旧的表达方式。如果在训练早期 MLP 出现错误，残差确保标记不会失去所有意义。

#### 代码内部：简洁性

尽管在深度学习库（如 PyTorch）中的 MLP 是一行代码（`nn.Linear` + `nn.GELU` + `nn.Linear`），但在 C 中你可以看到每个步骤都明确写出：

+   第一次 GEMM 扩展到 4C。

+   循环逐元素应用 GELU。

+   第二次 GEMM 将其投影回 C。

+   残差将输入和输出相加。

这就像观看魔术师揭示魔术技巧而不是只看到最终的幻象。

#### 为什么扩展很重要

你可能会问：为什么扩展到 4C 然后再缩小？为什么不保持大小不变？

扩展允许模型捕捉更复杂的特征组合。通过分散信息，应用非线性变换，然后再压缩，模型可以发现不适合较小空间的模式。

想象一下在巨大的白板上头脑风暴。你将所有想法（4C）展开，重新组织它们，然后将最好的想法浓缩成一个整洁的总结（C）。

#### 示例演示

假设我们在处理句子“The cat sat”中的标记“cat”。

1.  输入向量（大小 768）： `[0.12, -0.08, 0.33, …]`

1.  第一次矩阵乘法后：扩展到 `[1.2, -0.9, 0.5, …]`（大小 3072）。

1.  GELU 之后： `[1.1, -0.0, 0.4, …]`（平滑非线性）。

1.  投影后：回到 `[0.15, -0.02, 0.27, …]`（大小 768）。

1.  添加回原始输入： `[0.27, -0.10, 0.60, …]`。

现在，“cat”已经通过新的内部特征得到了丰富，这些特征有助于模型预测接下来会发生什么。

#### 为什么这很重要

MLP 是 GPT-2 中允许每个标记自我精炼的部分。注意力从邻居那里提供上下文，但 MLP 深化标记自身的表示。没有它，模型将缺乏检测细粒度模式的能力。

#### 尝试自己操作

1.  打印中间大小：添加调试打印以查看标记向量如何增长到 4C 并缩小回 C。

1.  交换激活：在代码中将 GELU 替换为 ReLU 并进行训练。比较损失，你会注意到 GPT-2 更偏好 GELU。

1.  禁用残差：暂时移除残差添加。观察模型如何努力学习，因为它无法保留信息。

1.  可视化值：追踪在 GELU 之前和之后接近 0 的值的数量。你会看到 GELU 软化地消除了弱信号。

1.  更小的扩展：尝试在代码中将 4C 改为 2C。你会节省内存，但会失去精度，因为 MLP 的表达能力更弱。

#### 吸取的教训

MLP 块是标记的个人深度思考者。它将表示拉伸得很宽，通过 GELU 过滤，再次压缩，然后添加回原始值。当注意力处理单词之间的对话时，MLP 确保每个单词处理和细化其自身的角色。共同，它们创造了使 GPT-2 非常强大的分层推理能力。

### 35. CPU 上的 LayerNorm（逐步）

GPT-2 中最重要但往往被忽视的成分之一是层归一化，或简称 LayerNorm。虽然注意力和 MLP 是大明星，但 LayerNorm 更像是幕后保持一切顺利的舞台工作人员。它确保通过网络流动的数字保持稳定和平衡，防止爆炸或崩溃，这可能导致训练无法进行。在 `train_gpt2.c` 中，LayerNorm 通过显式循环实现，这样你可以看到每个计算。让我们仔细地走一遍。

#### 为什么我们需要归一化？

想象一个教室，每个学生说话的音量都不同。有些人低声细语，有些人高声喊叫。如果你试图同时听所有人的声音，大声的声音会淹没安静的声音。

神经网络面临类似的问题。层的输出可以具有截然不同的尺度。如果向量的一个维度远大于其他维度，它将主导。训练变得不稳定，梯度可能消失或爆炸。

LayerNorm 通过确保对于每个层的每个标记，向量都有：

+   均值 = 0（围绕零中心）

+   方差 = 1（值的一致分布）

之后，可训练参数缩放和偏移结果，以便模型仍然可以学习灵活的转换。

#### LayerNorm 背后的数学

对于给定的大小为 `C` 的标记向量 `x`（例如，768）：

1.  计算均值：

    ```c
    μ = (1/C) * Σ x[i]
    ```

1.  计算方差：

    ```c
    σ² = (1/C) * Σ (x[i] - μ)²
    ```

1.  正则化：

    ```c
    x_norm[i] = (x[i] - μ) / sqrt(σ² + ε)
    ```

    其中 `ε` 是一个非常小的常数（如 1e-5），以避免除以零。

1.  使用可训练权重 `g`（gamma）和 `b`（beta）进行缩放和偏移：

    ```c
    y[i] = g[i] * x_norm[i] + b[i]
    ```

因此，最终输出具有受控的统计信息，但仍然足够灵活，以便模型可以调整。

#### 代码在 `train_gpt2.c` 中

这里是一个来自存储库的简化版本：

```c
[](#cb80-1)void layernorm_forward(float* out, float* inp, float* weight, float* bias, int B, int T, int C) {
[](#cb80-2)    for (int b = 0; b < B; b++) {
[](#cb80-3)        for (int t = 0; t < T; t++) {
[](#cb80-4)            float* x = inp + b*T*C + t*C;
[](#cb80-5)            float* o = out + b*T*C + t*C;
[](#cb80-6)
[](#cb80-7)            // mean
[](#cb80-8)            float mean = 0.0f;
[](#cb80-9)            for (int i = 0; i < C; i++) mean += x[i];
[](#cb80-10)            mean /= C;
[](#cb80-11)
[](#cb80-12)            // variance
[](#cb80-13)            float var = 0.0f;
[](#cb80-14)            for (int i = 0; i < C; i++) {
[](#cb80-15)                float diff = x[i] - mean;
[](#cb80-16)                var += diff * diff;
[](#cb80-17)            }
[](#cb80-18)            var /= C;
[](#cb80-19)
[](#cb80-20)            // normalize, scale, shift
[](#cb80-21)            for (int i = 0; i < C; i++) {
[](#cb80-22)                float norm = (x[i] - mean) / sqrtf(var + 1e-5f);
[](#cb80-23)                o[i] = norm * weight[i] + bias[i];
[](#cb80-24)            }
[](#cb80-25)        }
[](#cb80-26)    }
[](#cb80-27)}
```

注意结构：

+   外层循环遍历批次 `B` 和序列长度 `T`。

+   内层循环计算均值、方差，然后对每个长度为 `C` 的标记向量应用归一化。

+   `weight` 和 `bias` 是可学习的 gamma 和 beta。

这正是 LayerNorm 的含义：对每个标记的每个层的输入进行归一化。

#### 示例说明

假设我们有一个单个标记向量（C=4）= `[2.0, -1.0, 3.0, 0.0]`。

1.  均值：`(2 - 1 + 3 + 0)/4 = 1.0`

1.  方差：`((2-1)² + (-1-1)² + (3-1)² + (0-1)²)/4 = (1 + 4 + 4 + 1)/4 = 2.5`

1.  正则化：减去均值并除以 sqrt(2.5)：

    ```c
    [ (2-1)/1.58, (-1-1)/1.58, (3-1)/1.58, (0-1)/1.58 ]
    = [0.63, -1.26, 1.26, -0.63]
    ```

1.  尺度和偏移（例如 weight=[1,1,1,1]，bias=[0,0,0,0]）：

    ```c
    [0.63, -1.26, 1.26, -0.63]
    ```

现在向量具有均值 0，方差 1，并准备好进入下一层。

#### 类比

将 LayerNorm 想象成一个烘焙食谱。如果一个成分太强（比如加了五倍的盐），整个菜肴就会被毁了。LayerNorm 尝了尝混合物，平衡了所有的味道，然后让你通过可学习的 gamma（缩放）和 beta（偏移）调整调味。

#### 为什么这很重要

没有 LayerNorm，模型会迅速变得不稳定：

+   一些标记会主导，而其他标记会减弱。

+   梯度可能会爆炸，导致损失剧烈波动。

+   训练会在批次之间不一致。

使用 LayerNorm，每个层都使用干净、归一化的输入。这允许更深的注意力堆叠和 MLP 块可靠地学习。

#### 试试你自己

1.  打印统计数据：添加调试代码以检查 LayerNorm 前后均值和方差。之前：mean ≠ 0，variance ≠ 1。之后：mean ≈ 0，variance ≈ 1。

1.  移除 LayerNorm：在代码中注释掉 LayerNorm。观察训练崩溃-损失不会正确减少。

1.  改变 epsilon：尝试将 `ε = 1e-1` 或 `ε = 1e-12`。看看过大或过小的值如何破坏稳定性。

1.  观察 gamma 和 beta：初始化 gamma=1，beta=0。在训练过程中，观察这些参数如何漂移，微调归一化。

1.  尝试批量归一化：用 BatchNorm 替换 LayerNorm（对于 transformer 来说不典型）。你会发现它效果不佳，因为 transformer 处理可变长度的序列，而每批次的统计数据变化太大。

#### 总结

LayerNorm 是 GPT-2 中安静但至关重要的稳定器。它确保每个标记向量在进入注意力或 MLP 之前都是平衡的、居中的和缩放的。在 `train_gpt2.c` 中，你可以看到它的工作方式：计算均值、计算方差、归一化，然后缩放和偏移。尽管这只是几行 C 代码，但它是不深度 transformer 能够堆叠数十层而不崩溃的主要原因之一。

### 36. 残差添加和信号流

一旦计算了嵌入、注意力和 MLP 块，仍然还有一个部分可以保持整个网络稳定和有效：残差连接。在 `train_gpt2.c` 中，这些出现在像 `residual_forward` 这样的函数中，其中层的输出被添加回它们的输入。这个看似简单的步骤是 GPT-2 和其他深度 transformer 模型能够堆叠许多层而不崩溃的关键原因之一。

#### 核心思想

残差连接表示：

```c
output = input + transformation(input)
```

与用新表示替换旧表示不同，模型将它们相加。这样，原始信号总是得以保留，即使新的转换有噪声或不完美。

想象一下像做课堂笔记。每次老师讲解更多，你不会丢弃你的旧笔记。你会在旁边添加新的细节。这样，你就能保留到目前为止所学的一切，同时在上面叠加新的见解。

#### 为什么残差至关重要

1.  防止信息丢失：如果你只应用了转换，一些特征可能会永远消失。添加输入确保没有信息丢失。

1.  帮助梯度流动：在反向传播过程中，梯度必须通过许多层向后传播。没有捷径，它们可能会消失或爆炸。残差为梯度创建直接路径，使学习稳定。

1.  提高训练速度：有了残差，更深的网络收敛得更快，因为模型可以在使用恒等映射的同时“跳过”不良的转换。

#### `train_gpt2.c` 中的代码

这里是残差添加的实现：

```c
[](#cb84-1)void residual_forward(float* out, float* inp1, float* inp2, int N) {
[](#cb84-2)    for (int i = 0; i < N; i++) {
[](#cb84-3)        out[i] = inp1[i] + inp2[i];
[](#cb84-4)    }
[](#cb84-5)}
```

它看起来很简单：

+   `inp1` 是原始输入。

+   `inp2` 是新的转换（来自注意力或 MLP）。

+   `out` 存储总和。

+   `N` 是浮点数的总数（`B * T * C`）。

即使它只是循环内部的一行，这也是为什么能够堆叠 12+个 transformer 块的原因。

#### 示例说明

假设我们正在处理标记“猫”。

+   输入向量（简化，大小=3）：`[0.5, -0.3, 0.7]`

+   在注意力块之后：`[0.2, 0.1, -0.4]`

残差添加：

```c
[0.5 + 0.2, -0.3 + 0.1, 0.7 - 0.4] = [0.7, -0.2, 0.3]
```

现在，“猫”的表示既包含原始信号，也包含来自注意力的上下文信息。

在 MLP 之后：

+   再次输入：`[0.7, -0.2, 0.3]`

+   MLP 输出：`[0.1, -0.5, 0.4]`

+   残差：`[0.8, -0.7, 0.7]`

逐步地，向量变得更加丰富，同时不失其基础。

#### 类比

残差连接就像在 Photoshop 中构建图层。每个新图层添加调整，但你总是保留原始照片在下面。如果新的调整不好，你仍然可以看到原始的。这使得最终的合成更强，并且更容易进行实验。

#### 模型中的残差

在 GPT-2 的前向传递中，残差在每个变压器块内部出现两个主要位置：

1.  在注意力之后：

    ```c
    x = x + Attention(x)
    ```

1.  MLP 之后：

    ```c
    x = x + MLP(x)
    ```

与每个块之前的 LayerNorm 一起，这些构成了变压器架构的核心：

```c
x → LayerNorm → Attention → Residual Add → LayerNorm → MLP → Residual Add
```

#### 为什么这很重要

没有残差连接，GPT-2 将难以训练超过几层。更深层的堆叠会失去原始信号，梯度会消失，性能会停滞。残差是整个架构的粘合剂，使得具有数十亿参数的模型能够有效地训练。

#### 试试看

1.  移除残差：暂时注释掉`residual_forward`调用。训练将很快失败——损失不会适当减少。

1.  打印前/后：检查残差添加前后的标记向量。注意数字是如何平滑变化而不是被覆盖的。

1.  尝试扩展：尝试将`out[i] = inp1[i] + inp2[i];`替换为`out[i] = inp1[i] + 0.1 * inp2[i];`。这减少了新转换的影响——有时在高级架构中使用。

1.  与无跳过的 RNN 比较：研究没有残差的旧循环网络在扩展深度时遇到的困难。你会看到为什么残差是一个变革者。

1.  信号链：跟踪单个标记的向量如何在所有 12 层中演变。你会注意到它保持了其核心身份，同时吸收了新的上下文。

#### 吸取的经验教训

残差连接看起来像简单的加法，但它们是深度学习在变压器中取得成功的关键。它们保留信息，稳定训练，并允许 GPT-2 堆叠多层而不会崩溃。在`train_gpt2.c`中，这一想法被明确地表达出来：几行 C 代码实现了现代神经网络中最强大的技巧之一。

### 37. CPU 上的交叉熵损失

在嵌入、注意力、MLP、LayerNorm 和残差完成工作后，模型生成 logits——序列中每个位置的每个词汇的原始分数。但仅凭 logits 本身并不能告诉我们模型在预测正确单词方面是“好”还是“坏”。为了衡量性能并指导学习，GPT-2 使用交叉熵损失函数。在`train_gpt2.c`中，这通过`crossentropy_forward`函数实现。

#### 什么是 logits？

在前向传递的最后阶段，每个标记位置都有一个长度为`V`（词汇量大小，约 50k）的向量。例如，模型可能为 3 个单词的小型词汇表生成以下 logits：

```c
logits = [5.2, 1.1, -2.7]
```

logits 只是数字——更大的数字意味着“更有可能”，较小的数字意味着“不太可能”——但它们还不是概率。

#### 第 1 步：Softmax – 将分数转换为概率

要比较预测与真实目标，我们首先将 logits 转换为概率。用于此的工具是 softmax 函数：

```c
p[i] = exp(logits[i]) / Σ exp(logits[j])
```

Softmax 有两个重要的影响：

1.  它使所有值都为正。

1.  它将它们正规化，使它们的总和为 1，形成一个概率分布。

示例：

+   Logits: `[5.2, 1.1, -2.7]`

+   减去最大值（5.2）以提高稳定性 → `[0.0, -4.1, -7.9]`

+   指数化 → `[1.0, 0.017, 0.0004]`

+   正则化 → `[0.98, 0.017, 0.0004]`

现在模型说的是：

+   单词 0：98% 的机会

+   单词 1：1.7% 的机会

+   单词 2：0.04% 的机会

#### 第 2 步：交叉熵 – 测量错误

交叉熵比较预测的正确单词的概率与理想情况（概率 = 1）。

公式：

```c
loss = -log(probability_of_correct_word)
```

+   如果模型为正确单词分配高概率，损失就小。

+   如果模型分配低概率，损失就大。

示例：

+   正确单词 = 单词 0，概率 = 0.98 → 损失 = -log(0.98) ≈ 0.02 (优秀)。

+   正确单词 = 单词 1，概率 = 0.017 → 损失 = -log(0.017) ≈ 4.1 (差)。

#### 第 3 步：批处理平均

实际上，我们不是只在一个单词上训练，而是在一系列序列上训练。代码遍历每个批次中的每个标记，收集它们的损失，并取平均值。

从 `train_gpt2.c`：

```c
[](#cb92-1)float loss = 0.0f;
[](#cb92-2)for (int i = 0; i < B*T; i++) {
[](#cb92-3)    int target = targets[i];
[](#cb92-4)    float logit_max = -1e9;
[](#cb92-5)    for (int j = 0; j < Vp; j++) {
[](#cb92-6)        if (logits[i*Vp + j] > logit_max) logit_max = logits[i*Vp + j];
[](#cb92-7)    }
[](#cb92-8)    float sum = 0.0f;
[](#cb92-9)    for (int j = 0; j < Vp; j++) {
[](#cb92-10)        sum += expf(logits[i*Vp + j] - logit_max);
[](#cb92-11)    }
[](#cb92-12)    float log_sum = logf(sum);
[](#cb92-13)    float correct_logit = logits[i*Vp + target];
[](#cb92-14)    loss += (log_sum + logit_max - correct_logit);
[](#cb92-15)}
[](#cb92-16)loss /= (B*T);
```

这里发生的事情：

1.  对于每个标记，找到最大的 logit (`logit_max`) 以提高数值稳定性。

1.  计算 softmax 分母（`sum`）。

1.  计算正确标记的对数概率。

1.  在所有标记上累积损失。

1.  除以总标记数（`B*T`）以获得平均值。

#### 数值稳定性技巧

如果不减去 `logit_max`，`exp(logits)` 可能溢出。例如，`exp(1000)` 是无穷大。通过减去最大值，最大的 logit 变为 0，因此其指数为 1，其他所有值都 ≤ 1。这保持了数字的可管理性，同时保留了概率比率。

#### 带有句子的示例

句子：*猫坐在垫子上。*

假设模型预测最后一个标记的概率：

+   “垫子”：0.85

+   “狗”：0.10

+   “车”：0.05

正确单词 = “垫子。”

损失 = `-log(0.85) ≈ 0.16`。

如果模型猜测“狗”的概率为 0.10，损失 = `-log(0.10) ≈ 2.3`。错误惩罚更高。

#### 类比

交叉熵就像批改多项选择题。如果学生自信地选择了正确答案（高概率），他们几乎不会丢分。如果他们犹豫不决或答错，他们会丢更多分。在许多问题（标记）中，你计算他们的平均得分——训练损失。

#### 为什么这很重要

交叉熵损失是整个训练过程的指导信号。它告诉优化器：

+   “增加正确单词的概率。”

+   “降低错误单词的概率。”

没有它，GPT-2 就无法知道其预测是否在改进。

#### 试试你自己

1.  检查基线损失：在训练之前打印损失。它应该接近 `log(vocab_size)` (~10.8 for GPT-2 small)，这对应于随机猜测。

1.  检查 softmax 求和：对于单个标记，求和所有概率。它应该等于 ~1.0。

1.  强制错误答案：暂时将目标更改为一个错误的单词。观察损失如何急剧上升。

1.  在训练过程中观察损失：每一步打印损失。随着模型的学习，损失应该稳步下降。

1.  与准确度比较：跟踪模型最高预测与目标匹配的频率。损失和准确度将一起移动，但损失更平滑且更有信息量。

#### 吸收要点

交叉熵损失将原始模型得分转换为清晰的训练信号。它惩罚错误预测，奖励自信的正确预测，并确保优化器确切知道如何调整权重。在`train_gpt2.c`中，你可以看到这是明确实现的，没有任何库快捷方式——只有循环、指数和对数。理解这一部分是理解 GPT-2 如何从错误中学习的关键。

### 38. 将所有内容整合：`gpt2_forward`函数

到目前为止，我们已经逐个探讨了前向传递的各个部分——嵌入、注意力、前馈层、层归一化、残差连接，最后是损失。但一个模型并不是作为独立的部件存在的；它们都在一个单一的功能中汇集在一起，驱动推理：`gpt2_forward`。这个函数是代码实际执行我们一直在讲述的故事的地方。让我们仔细地走一遍，这样你就可以看到每个构建块是如何融入整个图画的。

#### `gpt2_forward`的作用

将`gpt2_forward`想象成戏剧的导演。演员（嵌入、注意力、MLP、层归一化等）已经知道他们的角色。导演按照正确的顺序将他们召唤到舞台上，并确保他们顺利地将剧本传递给下一个演员。在我们的情况下：

1.  标记以整数（词 ID）的形式输入。

1.  它们被转换为嵌入（标记+位置）。

1.  每个 Transformer 模块通过注意力、MLP、层归一化和残差处理序列。

1.  最终隐藏状态被映射回词汇空间。

1.  如果提供了标签，则计算损失。

#### 代码框架

这里是`train_gpt2.c`中真实函数的简化摘录（为了可读性略有缩短）：

```c
[](#cb93-1)void gpt2_forward(GPT2 *model, int *tokens, int *labels, int B, int T) {
[](#cb93-2)    // Step 1: Embedding lookup
[](#cb93-3)    embedding_forward(model->token_embedding, tokens, B, T);
[](#cb93-4)    embedding_forward(model->position_embedding, positions, B, T);
[](#cb93-5)
[](#cb93-6)    // Step 2: Transformer blocks
[](#cb93-7)    for (int l = 0; l < model->config->n_layer; l++) {
[](#cb93-8)        attention_forward(&model->blocks[l].attn, ...);
[](#cb93-9)        mlp_forward(&model->blocks[l].mlp, ...);
[](#cb93-10)        layernorm_forward(&model->blocks[l].ln, ...);
[](#cb93-11)        residual_forward(...);
[](#cb93-12)    }
[](#cb93-13)
[](#cb93-14)    // Step 3: Final normalization + logits
[](#cb93-15)    layernorm_forward(model->final_ln, ...);
[](#cb93-16)    matmul_forward(model->lm_head, ...); // project to vocab
[](#cb93-17)
[](#cb93-18)    // Step 4: Loss (optional)
[](#cb93-19)    if (labels != NULL) {
[](#cb93-20)        crossentropy_forward(...);
[](#cb93-21)    }
[](#cb93-22)}
```

如果这看起来令人生畏，请不要担心——我们将用通俗易懂的语言解码每个部分。

#### 第 1 步：嵌入查找

在模型能够对单词进行推理之前，它必须将标记 ID 映射到连续向量。这就是嵌入表的作用所在：

+   `token_embedding`将每个整数标记 ID 转换为大小为`C`（通道维度）的密集向量。

+   `position_embedding`对位置（0，1，2，…，T-1）做同样的处理。

+   这两个被相加，给每个标记既赋予了意义（词身份）又在句子中的位置。

#### 第 2 步：Transformer 模块

每个模块就像一个微型管道，处理序列并将其传递下去。在循环内部：

1.  注意力：通过学习到的 Q/K/V 投影加权比较标记。

1.  MLP：扩展每个标记向量，应用非线性 GELU 激活，然后将其投影回下。

1.  层归一化：对值进行归一化，以实现稳定的训练和推理。

1.  残差：将块的输入加回到其输出中，以保持信息流动。

这个循环运行 `n_layer` 次 - 对于 GPT-2 124M，那就是 12 个块。

#### 步骤 3：最终归一化和 logits

在最后一个块之后，标记表示的序列通过一个最终的层归一化。然后，一个大的矩阵乘法（`lm_head`）将每个标记的隐藏状态投影到词汇量的大小（GPT-2 约为 50,000）。结果是包含每个下一个标记的原始预测分数的形状为 `(B, T, vocab_size)` 的张量。

#### 步骤 4：可选的损失计算

如果你将 `labels`（正确的下一个标记）传递给 `gpt2_forward`，该函数将调用 `crossentropy_forward`。这比较预测分数与真实标记，并输出一个数字：损失。损失告诉你“模型有多错”，这在训练期间至关重要。但如果你只是进行推理，你不需要这一步。

#### 零部件如何连接

下面是一个表格，将我们之前的章节映射到 `gpt2_forward` 的部分：

| 代码步骤 | 概念 | 之前章节涵盖的内容 |
| --- | --- | --- |
| 嵌入 | 标记 + 位置向量 | 32 |
| 注意力 | QKV 投影、掩码、softmax | 33 |
| MLP | 前馈扩展和压缩 | 34 |
| 层归一化 | 稳定性归一化 | 35 |
| 残差 | 用于信号流的跳过连接 | 36 |
| 交叉熵 | 比较预测与标签 | 37 |

因此，`gpt2_forward` 实际上只是对已经学到的所有内容的巧妙编排。

#### 为什么这很重要

理解 `gpt2_forward` 给你完整的推理心理图。它展示了嵌入、注意力、MLP、归一化和残差如何在代码中协同工作，将一批标记转换为预测。没有这个集成步骤，模型将只是一个脱节的部件集合。

#### 亲自尝试

1.  打印形状：在 `gpt2_forward` 中添加 `printf` 语句，以打印嵌入后的张量形状、每个块之后和 logits 之后。这有助于你看到数据流。

1.  使用单个块：将循环更改为只运行 1 个 transformer 块而不是所有 12 个。观察输出如何退化 - 模型失去了推理的深度。

1.  禁用位置嵌入：取消注释添加 `position_embedding` 的行。尝试运行推理。你会注意到模型在处理词序方面变得更差。

1.  损失与无损失：带有和没有标签调用 `gpt2_forward`。比较差异 - 带有标签你得到一个标量损失，没有标签你只得到 logits。

1.  较小的词汇量：尝试使用具有小词汇量的玩具标记器并重新运行投影步骤。你会看到 logits 缩小到 `(B, T, tiny_vocab_size)`。

#### 吸取的经验

`gpt2_forward` 是 GPT-2 推理真正发生的地方。它将每个概念——嵌入、注意力、前馈层、归一化、残差以及最终投影到词汇空间——联系在一起。一旦你理解了这个函数，你不仅知道 GPT-2 的各个部分，还知道它们实际上是如何协同工作以产生预测的。它是推理的“主舞台”，掌握它意味着你可以自信地说你理解了 transformer 在 CPU 上正向运行的方式。

### 39. 并行循环的 OpenMP Pragmas

在 `train_gpt2.c` 中的 CPU 训练故意是“纯 C”，但它仍然通过在热点循环中添加几个 OpenMP Pragmas（`#pragma omp …`）来挤出很多速度。OpenMP 允许编译器将循环的迭代分配到多个 CPU 核心——无需手动创建线程，无需管理锁。如果你没有 OpenMP 支持地编译，这些 Pragma 将被简单地忽略，代码仍然可以运行（只是速度较慢）。

下面我们将（1）展示 OpenMP 的确切使用位置，（2）解释为什么这些循环是很好的候选者，（3）提供一些实用的技巧，以在您的机器上获得稳定的加速效果。

#### 本文件中的 OpenMP：出现的位置和原因

| 位置 / 功能 | 使用的 Pragma | 并行化什么 | 为什么它非常适合 |
| --- | --- | --- | --- |
| `matmul_forward_naive` | `#pragma omp parallel for collapse(2)` | 对 `b`（批次）和 `t`（时间）进行外层循环 | 每个 `(b,t)` 行计算一个独立的输出向量；没有写冲突。大量、规则的工作 = 容易扩展。 |
| `matmul_forward`（分块） | `#pragma omp parallel for` | 在分块中的折叠 `B*T` 循环 | 模型中最重的计算；分块 + 每线程分块保持缓存活跃。 |
| `matmul_backward`（第一部分） | `#pragma omp parallel for collapse(2)` | 在 `(b,t)` 上反向传播到 `inp` | 每个 `(b,t)` 读取权重和 `dout`，写入 `dinp` 的私有切片 → 没有重叠。 |
| `matmul_backward`（第二部分） | `#pragma omp parallel for` | 在 `o`（输出通道）上反向传播到 `weight`/`bias` | 每个线程拥有一个输出通道的梯度行，避免了原子操作。 |
| `softmax_forward` | `#pragma omp parallel for collapse(2)` | 在 `(b,t)` 位置上 | 每个 softmax 是独立的；完美的“令人尴尬的并行”循环。 |
| `attention_forward` | `#pragma omp parallel for collapse(3)` | 在 `(b, t, h)` 上 = 批次、时间、头 | 每个 `(b,t,h)` 头的工作是独立的；大 3-D 网格并行化效果极佳。 |

几个需要注意的关键模式：

+   合并子句（`collapse(2)` / `collapse(3)`）将嵌套循环合并为一个大的迭代空间，以便调度器可以分配更多、更小的块——当 `B`、`T` 或 `NH` 适中时，非常适合负载均衡。

+   沿独立维度并行化可以避免竞争条件。例如，在 `matmul_backward` 中，写入 `dinp[b,t,:]` 的操作在 `(b,t)` 上并行化，因此没有两个线程更新相同的内存。

+   自主行策略：当累积 `dweight` 时，循环遍历 `o`（输出通道），因此每个线程都写入自己的梯度行 `dweight[o,:]`。不需要原子操作。

#### 快速复习：OpenMP 在做什么

一个典型的模式如下：

```c
[](#cb94-1)#pragma omp parallel for collapse(2)
[](#cb94-2)for (int b = 0; b < B; b++) {
[](#cb94-3)    for (int t = 0; t < T; t++) {
[](#cb94-4)        // compute outputs for (b, t) independently
[](#cb94-5)    }
[](#cb94-6)}
```

当使用 OpenMP 编译时，编译器创建一个线程团队，并将迭代空间（本例中的 `B*T`）分配给它们。每个线程执行其分配的迭代；当循环结束时，线程在隐式屏障处同步。

因为每个 `(b,t)`（或 `(b,t,h)`）都写入输出数组的一个不重叠的切片，所以不需要锁或原子操作。这就是为什么这些循环可以干净地扩展到核心。

#### 安全启用 OpenMP

+   源代码使用以下方式保护 OpenMP 头部：

    ```c
    [](#cb95-1)#ifdef OMP
    [](#cb95-2)#include <omp.h>
    [](#cb95-3)#endif
    ```

    因此您可以在构建中定义 `OMP` 并添加您的编译器开关。示例（GCC/Clang）：

    ```c
    -D OMP -fopenmp
    ```

+   如果您忘记了 `-fopenmp`（或您平台的等效项），则省略了这些指令，程序以单线程运行。

+   您可以在运行时控制线程：

    ```c
    export OMP_NUM_THREADS=8
    ```

    一个好的经验法则是从您 CPU 上的物理核心数开始。

#### 为什么这些循环受益最大

1.  矩阵乘法主导运行时间。`matmul_forward`/`matmul_backward` 消耗了大部分 CPU 时间。并行化它们可以获得最大的端到端加速。

1.  Softmax 每个位置都是独立的。每个 `(b,t)` softmax 计算一个最大值，然后是指数和求和，位置之间没有交叉干扰。

1.  注意力在批处理/时间/头之间分散。对 `(b,t,h)` 的三重循环在每次迭代中有很多工作（Q·K，softmax，加权求和），使得线程开销与有用计算相比可以忽略不计。

1.  最小化同步和原子操作。通过选择拥有独家输出切片的迭代空间，我们避免了昂贵的同步。

#### 更好扩展的实际技巧

+   将 `OMP_NUM_THREADS` 设置为您的 CPU。线程过多可能会造成损害（过度订阅）。从物理核心开始，然后进行实验。

+   锚定线程（可选，高级）。一些 OpenMP 运行时支持 `OMP_PROC_BIND=close` 以提高缓存局部性。

+   注意内存带宽。在宽 CPU 上，GEMMs 可能会变成带宽限制。更大的 `B`/`T` 提高了算术强度；微批次未能充分利用核心。

+   使用分块填充缓存。分块 `matmul_forward` 将小累加器保留在寄存器中，并在 `LOOP_UNROLL` 内部迭代中重用加载的权重。

+   避免隐藏共享。如果您添加新的并行循环，请确保每个线程写入唯一的内存区域。如果您必须累积到同一位置，请重新结构（如“自主行”）或使用每个线程的临时缓冲区然后进行汇总。

#### 微型概述：为什么 `collapse` 有帮助

考虑 `softmax_forward`：

```c
[](#cb98-1)#pragma omp parallel for collapse(2)
[](#cb98-2)for (int b = 0; b < B; b++) {
[](#cb98-3)    for (int t = 0; t < T; t++) {
[](#cb98-4)        // 1) find max over V
[](#cb98-5)        // 2) exp/log-sum
[](#cb98-6)        // 3) normalize first V entries; zero out padded V..Vp)
[    }
[](#cb98-8)}
```

如果 `B=4`，`T=64`，那么就有 256 个独立的 softmax。使用 `collapse(2)`，OpenMP 会看到 256 次迭代的单个循环来均匀分配；如果没有 `collapse`，它可能会首先按 `b` 块（只有 4 个大块），这可能会导致负载不平衡。

#### 常见陷阱（以及此代码如何避免它们）

+   竞态条件：两个线程写入相同的`out[i]`。*设计上避免：*每个并行循环写入不同的切片（例如，每个`(b,t)`或每个`o`)。

+   假共享：线程在相同缓存行上的相邻内存位置写入。通过每个线程的大连续切片（整个行/瓦片）来最小化，但如果你在代码中添加了细粒度并行性，请记住这一点。

+   小循环：开销可能超过工作量。该文件仅并行化大循环（GEMMs、注意力、softmax），而不是小的标量操作。

#### 试试看

1.  改变线程数：使用`OMP_NUM_THREADS=1,2,4,8,…`运行并记录步骤时间。绘制速度与线程的关系图。

1.  切换指令：仅在`matmul_forward`中注释掉`#pragma omp`。测量减速；你会看到大部分时间在哪里。

1.  尝试`collapse`：在`softmax_forward`中移除`collapse(2)`。在小`B`的情况下，你可能会看到更差的扩展性。

1.  每层分析：在`matmul_forward`、`attention_forward`和`softmax_forward`周围打印经过的时间，以查看哪个在你的 CPU 上受益最大。

1.  调度策略（高级）：尝试在重循环上使用`#pragma omp parallel for schedule(static)`与`dynamic`，看看是否改变了负载平衡（默认值通常在这里是合适的）。

#### 吸取的经验教训

几个放置得当的 OpenMP 指令可以显著提高 CPU 性能，通过并行化最昂贵的循环（GEMMs、注意力、softmax）跨核心，而不使代码复杂化。该设计确保每个线程独立工作在独立的切片上，因此没有锁定、没有原子操作，并且开销非常小。如果你使用启用了 OpenMP 的编译器编译，你将获得快速的多核训练；如果没有，你仍然有一个干净、可读的参考实现。

### 40. CPU 内存占用和性能

当你使用`train_gpt2.c`在你的 CPU 上训练 GPT-2 时，两个大问题通常几乎立即出现：*这将占用多少内存？*和*它将运行得多快？*让我们以初学者友好的方式逐一探讨这两个问题，这样你不仅了解代码中发生了什么，而且了解为什么它以这种方式表现。

#### 内存：它都去哪里了？

想象训练 GPT-2 就像在一个小厨房里做一顿大餐。你需要空间放食材，混合用的碗，以及准备用的台面。你的 CPU 内存就是那个厨房。GPT-2 需要几个“碗”来存放计算的不同部分：

1.  参数（模型的权重）。这些是“固定配方”——网络实际学习的数字。它们来自你开始时加载的检查点文件。对于 GPT-2 124M，这大约是 1.24 亿个浮点数。每个数占用 4 字节，所以仅仅是权重就有大约 500 MB。

1.  优化器状态（AdamW）。训练不仅仅是盲目地调整权重；它还跟踪每个权重两个额外的移动平均值，称为*m*和*v*。这意味着对于每个单独的参数，你存储三个数字：权重、m 和 v。因此，优化器状态的内存通常是权重本身大小的两倍。对于 GPT-2 124M，这大约是 1 GB 更多。

1.  梯度。每次我们运行反向传递时，我们都会存储每个权重应该改变多少。这是另一个与权重大小大致相同的缓冲区——另一个 500 MB。

1.  激活（中间结果）。这是隐藏的一个。每次前向传递都会产生临时张量，如嵌入、注意力图和前馈输出。它们的大小取决于批量大小（B）和序列长度（T）。如果 B=4 且 T=64，激活大约是几百 MB。如果 B=32 且 T=1024，它们可以膨胀到几个 GB。

这里是 GPT-2 124M 在小型设置（B=4，T=64）下的粗略心理预算：

+   参数：~500 MB

+   优化器状态：~1 GB

+   梯度：~500 MB

+   激活：~200–300 MB 总计：~2–2.5 GB

即使对于“小巧”的 GPT-2，你也需要几 GB 的 RAM 来训练。在笔记本电脑上，这可以迅速把你推到极限。

#### 性能：时间都去哪了？

现在让我们谈谈速度。当你用 CPU 运行`train_gpt2.c`时，你会看到如下类似的行：

```c
step 1: train loss 5.191576 (took 1927.230000 ms)
```

“花费了 X 毫秒”告诉你一个步骤花费了多长时间。为什么它这么慢？主要有三个原因：

1.  矩阵乘法（matmuls）。这是神经网络的核心。每个注意力头和每个 MLP 层都执行这些操作。在 CPU 上，你大部分的时间都花在这里。这就是为什么代码使用 OpenMP 指令（`#pragma omp`）来并行化跨核心的循环。

1.  注意力 softmax。注意力将序列中的每个标记与序列中的每个其他标记进行比较。如果你的序列长度是 1024，那么每个头每个层的比较次数超过一百万。在 CPU 上，这种二次增长是痛苦的。

1.  内存带宽。CPU 只能以如此快的速度将数字从 RAM 移动到核心。即使你有无限的 FLOPs，你仍然会受到如何快速检索和存储这些巨大张量的速度限制。

#### 一个简单的实验

你可以自己看到这些效果：

+   改变批量大小（B）。用 B=1 运行，然后用 B=8 运行。注意内存使用量和步骤时间是如何成比例增加的。

+   改变序列长度（T）。尝试 T=16，然后 T=256。你会看到注意力成本显著增加。

+   改变线程数。设置`OMP_NUM_THREADS=1`与`OMP_NUM_THREADS=8`。随着线程数的增加，你通常会看到速度提升，但提升的幅度仅限于 CPU 的物理核心数。

#### 为什么这很重要

对于初学者来说，CPU 运行非常适合学习：

+   你可以用小批量和短序列进行调试。

+   你可以用调试器进入函数并观察张量的创建。

+   你不需要 GPU 就能理解训练是如何工作的。

但是当涉及到*严肃*的训练——更大的 GPT-2 模型或甚至长序列时——CPU 简单地太慢了。在 GPU 上只需几秒就能完成的事情，在 CPU 上可能需要几分钟。这就是为什么在实践中，人们使用 CPU 进行学习和测试，而使用 GPU 进行大规模训练。

#### 吸取的经验

在 CPU 上训练 GPT-2 就像在小键盘上练习钢琴。它速度较慢，功能有限，无法演奏最大的作品，但对于学习基础知识来说非常棒。内存使用来自权重、优化器状态、梯度和激活，性能主要由矩阵乘法和注意力操作主导。一旦你了解了资源去向，你就可以调整批处理大小、序列长度和线程，以找到适合你机器的最佳点。

## 第五章\. 训练循环（CPU 路径）

### 41\. 反向传播遍历

到目前为止，我们一直在关注正向传播。这是模型中处理标记的部分，它将标记推过嵌入、注意力、前馈层，最终产生 logits 或损失。对于推理，正向传播就足够了。但如果你想训练一个模型，正向传播只是故事的一半。

训练意味着调整模型的权重，使其预测随时间变得更好。为此，我们需要一种方法来确定每个权重有多错误，以及它应该朝哪个方向移动以减少损失。这就是反向传播的任务。

反向传播也称为反向传播。它是将信息反向传递到网络中的算法：从损失，通过最终的 logits，通过每个 Transformer 块，直到嵌入。在这个过程中，它计算梯度——这些小数字告诉我们每个权重对误差的贡献程度。

#### 主要思想：链式法则的实际应用

反向传播的核心是来自微积分的一个非常熟悉的概念：链式法则。如果网络的输出依赖于许多堆叠在一起的功能（嵌入 → 注意力 → MLP → … → 损失），那么损失相对于早期参数的导数是整个链中偏导数的乘积。

在 `train_gpt2.c` 代码中，为了避免编写冗长的公式，简单地以相反的顺序调用每一层的反向函数。梯度逐层反向流动，每层使用局部规则计算自己的贡献。

想象它就像接力赛，但方向相反：损失将一个“责任接力棒”传给输出头，然后输出头将其传给最后一个 Transformer 块，以此类推，直到它到达第一个嵌入表。

#### 遍历 `gpt2_backward`

下面是代码中反向函数的简化草图（为了可读性，名称已缩短）：

```c
[](#cb100-1)void gpt2_backward(GPT2 *model, int *tokens, int *labels, int B, int T) {
[](#cb100-2)    // Step 1: loss gradient
[](#cb100-3)    crossentropy_backward(...);
[](#cb100-4)
[](#cb100-5)    // Step 2: final projection (lm_head)
[](#cb100-6)    matmul_backward(model->lm_head, ...);
[](#cb100-7)
[](#cb100-8)    // Step 3: transformer blocks in reverse
[](#cb100-9)    for (int l = model->config->n_layer - 1; l >= 0; l--) {
[](#cb100-10)        residual_backward(...);
[](#cb100-11)        layernorm_backward(&model->blocks[l].ln, ...);
[](#cb100-12)        mlp_backward(&model->blocks[l].mlp, ...);
[](#cb100-13)        attention_backward(&model->blocks[l].attn, ...);
[](#cb100-14)    }
[](#cb100-15)
[](#cb100-16)    // Step 4: embeddings
[](#cb100-17)    embedding_backward(model->token_embedding, tokens, ...);
[](#cb100-18)    embedding_backward(model->position_embedding, positions, ...);
[](#cb100-19)}
```

让我们逐行分析这一行。

#### 第 1 步：从损失开始

旅程从损失函数开始。在训练中，最常用的损失是交叉熵。其反向函数比较预测概率与真实标签，并为 logits 生成梯度。

+   如果模型以高置信度预测“猫”，而真实标签是“狗”，则梯度会将 logits 推离“猫”并朝向“狗”。

+   这个梯度是传播整个网络的反向传播的起始信号。

#### 第 2 步：反向通过输出头

损失之后，下一个目的地是最终的线性投影（`lm_head`）。这只是一个将隐藏状态转换为词汇对数的大矩阵乘法。它的反向函数计算两件事：

1.  与`lm_head`权重相关的梯度。

1.  与其输入的隐藏状态相关的梯度。

这个隐藏状态梯度随后被传递回最后一个变换器块。

#### 第 3 步：反向的变换器块

接下来是重头戏。每个块都有多个组件，它们的反向函数以与正向传播完全相反的顺序调用。

1.  剩余反向传播：跳跃连接将梯度分为两条路径——一条流向转换后的输出，一条流向原始输入。

1.  LayerNorm 反向传播：计算与其缩放（`gamma`）和偏移（`beta`）相关的梯度，并将梯度传递回归一化输入。

1.  MLP 反向传播：将链式法则应用于两个线性层和 GELU 激活。代码重用了正向传播中的临时值（如激活），以提高效率。

1.  注意力反向传播：这是最棘手的。它计算 Q、K 和 V 投影的梯度，以及 softmax 后的注意力权重。它必须再次应用因果掩码，以确保没有非法的梯度流动。

这个循环会一直持续到所有变换器块都被处理。

#### 第 4 步：回到嵌入

最后，梯度到达嵌入表。这是模型首次查找标记和位置向量的地方。现在它计算每个嵌入对误差的贡献程度。这些梯度被添加到嵌入矩阵中，告诉优化器如何更新它们。

#### 这为什么重要

反向传播是使学习成为可能的原因。没有它，模型将永远输出相同的预测，永远不会改进。通过将“责任”向后传递，每个参数学习如何微调自己，以便下一次正向传播变得更好。

尽管代码看起来像很多函数调用，但原理很简单：从损失开始，逐层反向传播，局部应用链式法则，并收集梯度。

#### 试试看

1.  打印梯度范数：添加一个`printf`来查看每层的平均梯度幅度。注意它们是如何变化的——有时爆炸，有时消失。

1.  冻结一层：注释掉一个块的`mlp_backward`，看看模型如何无法正确更新。

1.  检查嵌入：训练了几步之后，输出几行标记嵌入矩阵。你会看到数字因为梯度更新而改变。

1.  小数据集实验：在一个非常小的数据集（如 10 词语料库）上训练，并观察反向传播如何迅速推动嵌入来记住它。

1.  检查对称性：比较`gpt2_forward`和`gpt2_backward`中的调用顺序。它们是相反的——正向构建，反向拆解。

#### 要点

反向传播是神经网络的学习引擎。在 `llm.c` 中，反向传播被明确写出，展示了梯度如何从损失，通过输出头部，反向通过每个转换器块，最终进入嵌入层。一旦你理解了这个流程，你就可以看到训练是如何将正向和反向步骤结合在一起，逐渐将一个随机的模型塑造成一个有效的语言模型。

### 42. 训练循环的骨架

反向传播为我们提供了梯度，但仅仅梯度本身并不能训练模型。训练需要一个循环：一个反复运行正向、反向和更新步骤的循环，直到模型改进。这个循环被称为训练循环，它是每个深度学习程序的心跳。在 `train_gpt2.c` 中，循环是用 C 明确编写的，这意味着你可以看到每一部分，而不是它被隐藏在框架中。

#### 基本节奏

每个训练步骤都遵循相同的节奏：

1.  获取一批数据（输入标记及其标签）。

1.  运行正向传播来计算预测和损失。

1.  运行反向传播来计算梯度。

1.  使用 AdamW 等优化器更新权重。

1.  记录进度，偶尔进行验证。

这种节奏会重复数千或数百万次。每次重复，权重都会略有变化，推动模型朝着更低的损失和更好的预测方向前进。

#### 循环在代码中的样子

这里是 `train_gpt2.c`（为了清晰起见省略了一些细节）的简化草图：

```c
[](#cb101-1)for (int step = 0; step < max_steps; step++) {
[](#cb101-2)    // 1\. Load batch of tokens and labels
[](#cb101-3)    dataloader_next_batch(&train_loader, tokens, labels, B, T);
[](#cb101-4)
[](#cb101-5)    // 2\. Forward pass
[](#cb101-6)    gpt2_forward(&model, tokens, labels, B, T);
[](#cb101-7)
[](#cb101-8)    // 3\. Zero gradients
[](#cb101-9)    gpt2_zero_grad(&model);
[](#cb101-10)
[](#cb101-11)    // 4\. Backward pass
[](#cb101-12)    gpt2_backward(&model, tokens, labels, B, T);
[](#cb101-13)
[](#cb101-14)    // 5\. Optimizer step (AdamW)
[](#cb101-15)    adamw_update(&opt, &model, learning_rate);
[](#cb101-16)
[](#cb101-17)    // 6\. Logging and validation
[](#cb101-18)    if (step % log_interval == 0) { print_loss(step, model.loss); }
[](#cb101-19)    if (step % val_interval == 0) { run_validation(...); }
[](#cb101-20)}
```

这个循环捕捉了完整的训练生命周期：数据、正向传播、反向传播、更新和监控。

#### 步骤 1：批量数据

数据加载器将小块标记喂入循环。它不是一次性发送整个数据集，而是将其分解为大小为 `B`（每个批次的序列数）和长度为 `T`（每个序列的标记数）的批次。

+   示例：如果 `B=4` 且 `T=128`，则每个批次包含 512 个标记。

+   每个序列都有一个匹配的标签集，这些标签只是将相同的标记向前移动一个位置（因此模型总是预测*下一个*单词）。

这种批量处理使内存使用保持可控，并帮助模型看到许多小样本而不是少数大样本。

#### 步骤 2：正向传播

正向传播计算批次中所有标记的预测并计算损失。这是“评估”步骤——模型在这个批次上做得怎么样？结果是存储在 `model.loss` 中。

#### 步骤 3：梯度归零

在计算新的梯度之前，必须清除旧的梯度。如果你跳过这一步，先前批次的梯度会累积并破坏更新。在 PyTorch 等框架中，你会调用 `optimizer.zero_grad()`。这里它是一个普通的 C 函数：

```c
[](#cb102-1)gpt2_zero_grad(&model);
```

它遍历所有参数并将它们的梯度缓冲区重置为零。

#### 步骤 4：反向传播

现在调用反向函数。它将梯度反向推回网络，计算每个权重如何影响误差。此时，每个参数都有一个存储在内存中的相关梯度。

#### 步骤 5：优化器更新

准备好梯度后，优化器（本代码中的 AdamW）更新每个参数：

```c
new_weight = old_weight - learning_rate * gradient (with AdamW tweaks)
```

这一步实际上改变了模型。没有它，模型将永远无法学习——正向和反向传递将永远重复相同的结果。

#### 第 6 步：记录和验证

每过几步，循环会打印出有用的数字：当前步骤、损失、所用时间，有时还有吞吐量（每秒的 token 数量）。这种反馈对于检查训练是否真正有效非常重要。

每过几百或几千步，循环还会在保留的数据上运行一个验证过程。这告诉你模型是否只是在记忆训练数据，或者真正地学习了一般化的模式。

#### 为什么训练循环很重要

训练循环看似简单，但它是机器学习的引擎室。模型性能的每一次改进都是因为这个循环多次运行的结果。通过在 C 中明确编写它，`llm.c`揭示了高级框架通常隐藏的细节：归零梯度、传递数组指针、直接调用反向和优化器函数。

这使其成为一个完美的学习工具。你可以清楚地看到：

+   数据从哪里进来，

+   预测在哪里进行，

+   梯度在哪里计算，

+   而学习实际上发生在这里。

#### 亲自尝试

1.  打印损失曲线：在循环中添加`printf`并将损失写入文件。绘制它——你应该会看到它随时间下降。

1.  改变批量大小：设置`B=1`与`B=8`。注意，随着批量大小的减小，循环变得更嘈杂，而批量大小时则更平滑。

1.  跳过反向：注释掉`gpt2_backward`和优化器更新。运行循环。你会看到损失永远不会下降——这是正向训练不有效的明显证明。

1.  尝试不同的步数：尝试`max_steps=10`与`max_steps=1000`。短运行没有改进；长运行开始减少损失。

1.  慢下来：在循环中插入`sleep(1);`。这使得节奏一步一步地变得可见，因此你可以真正地看到模型在训练时的“呼吸”。

#### 吸取的经验

训练循环的骨架是学习的核心循环。它将数据输入模型，计算预测，找出错误，将它们反向传递，更新权重，并记录进度。其他所有东西——优化器、调度器、分布式训练、混合精度——只是对这个基本循环的增强。如果你理解了`llm.c`中这个循环的工作原理，你就理解了深度学习训练的跳动心脏。

### 43. C 语言中的 AdamW 实现

训练神经网络就是调整数百万个参数，使模型逐渐变得擅长预测文本。`train_gpt2.c`中的`gpt2_update`函数负责这一调整。它实现了 AdamW 优化器，这是深度学习中应用最广泛的算法之一。让我们一起来探讨其理论和实际实现。

#### 从梯度下降到 AdamW

最基本的优化器是梯度下降：

```c
new_param = old_param - learning_rate * gradient
```

这种方法有效，但它有弱点。步长（学习率）必须仔细调整：太小则训练缓慢，太大则训练发散。此外，所有参数使用相同的步长，尽管某些参数可能需要更温和的更新。

AdamW 通过跟踪梯度的移动平均值并自适应地缩放更新来改进这一点。它还引入了权重衰减，这防止参数变得过大，并有助于正则化模型。

#### AdamW 的工作原理

AdamW 将几种技术结合成一个单一的更新规则。首先，它使用动量：它不仅依赖于当前的梯度，还平均了最近的梯度。这平滑了噪声更新。其次，它维护一个平方梯度的运行估计，在梯度持续较大的方向上缩小步长。这些有时被称为第一和第二矩。

由于两个运行平均值都是从零开始的，算法在最初的几步中应用偏差校正。如果没有这个，早期的更新会太小。最后，AdamW 在更新中直接应用权重衰减，每次略微缩小参数值。

将其组合，每个参数更新看起来像这样：

```c
m_t = β1 * m_(t-1) + (1 - β1) * g_t
v_t = β2 * v_(t-1) + (1 - β2) * g_t²
m̂_t = m_t / (1 - β1^t)
v̂_t = v_t / (1 - β2^t)

new_param = old_param - lr * ( m̂_t / (sqrt(v̂_t) + ε) + λ * old_param )
```

在这里 `m` 是动量，`v` 是方差，`lr` 是学习率，`ε` 是一个小的常数，用于稳定性，而 `λ` 是权重衰减因子。

#### 在 `train_gpt2.c` 中的实现

```c
[](#cb106-1)void gpt2_update(GPT2 *model, float learning_rate, float beta1, float beta2,
[](#cb106-2)                 float eps, float weight_decay, int t) {
[](#cb106-3)    if (model->m_memory == NULL) {
[](#cb106-4)        model->m_memory = (float*)calloc(model->num_parameters, sizeof(float));
[](#cb106-5)        model->v_memory = (float*)calloc(model->num_parameters, sizeof(float));
[](#cb106-6)    }
[](#cb106-7)
[](#cb106-8)    for (size_t i = 0; i < model->num_parameters; i++) {
[](#cb106-9)        float param = model->params_memory[i];
[](#cb106-10)        float grad = model->grads_memory[i];
[](#cb106-11)
[](#cb106-12)        float m = beta1 * model->m_memory[i] + (1.0f - beta1) * grad;
[](#cb106-13)        float v = beta2 * model->v_memory[i] + (1.0f - beta2) * grad * grad;
[](#cb106-14)
[](#cb106-15)        float m_hat = m / (1.0f - powf(beta1, t));
[](#cb106-16)        float v_hat = v / (1.0f - powf(beta2, t));
[](#cb106-17)
[](#cb106-18)        model->m_memory[i] = m;
[](#cb106-19)        model->v_memory[i] = v;
[](#cb106-20)        model->params_memory[i] -= learning_rate *
[](#cb106-21)            (m_hat / (sqrtf(v_hat) + eps) + weight_decay * param);
[](#cb106-22)    }
[](#cb106-23)}
```

第一个 `if` 块在优化器第一次运行时为移动平均值 `m` 和 `v` 分配内存。然后，对于每个参数，代码计算新的平均值，应用偏差校正，并最终使用 AdamW 公式更新参数。

#### 示例演练

假设我们在第一次训练步骤中有一个参数 `w = 0.5` 和梯度 `g = 0.2`。使用 β1 = 0.9 和 β2 = 0.999：

+   动量：

    ```c
    m = 0.9 * 0 + 0.1 * 0.2 = 0.02
    ```

+   方差：

    ```c
    v = 0.999 * 0 + 0.001 * 0.04 = 0.00004
    ```

+   偏差校正：

    ```c
    m̂ = 0.02 / (1 - 0.9) = 0.2
    v̂ = 0.00004 / (1 - 0.999) = 0.04
    ```

+   最终更新（lr = 0.001，weight_decay = 0.01）：

    ```c
    update = 0.001 * (0.2 / sqrt(0.04) + 0.01 * 0.5)
           = 0.001 * (1.0 + 0.005)
           = 0.001005
    ```

因此，参数变为 `w = 0.498995`。

#### 直觉

想象一个球沿着斜坡滚动。梯度就是斜坡本身。动量使球即使在斜坡短暂变平的情况下也能继续滚动。方差项使球在斜坡变化迅速的崎岖地面上减速。偏差校正确保球在开始时不会过于胆怯。权重衰减增加摩擦，使球不会失控滚动。

#### 为什么这很重要

优化器是模型平稳训练与发散或卡住之间的区别。AdamW 因其结合了稳定性和效率而变得流行。它自动适应每个参数的规模，减少了手动调整学习率的必要性，并以一种原则性的方式包括权重衰减。对于具有数亿参数的 GPT 风格模型，这些特性使得训练变得可行。

#### 试试看

1.  在代码中将学习率从 `0.001` 改为 `0.01`，看看模型会多快发散。

1.  将 `weight_decay` 设置为 `0` 并在几个 epoch 后比较验证损失。模型可能会更快地过拟合。

1.  在训练期间打印出`m_memory`和`v_memory`的前 10 个值，以观察它们随步骤的变化。

1.  将 AdamW 替换为普通的 SGD（只是`param -= lr * grad`）并比较训练速度和稳定性。

1.  尝试β1 = 0（无动量）或β2 = 0（无方差平滑）并看看更新变得多嘈杂。

#### **总结**

AdamW 在速度、稳定性和泛化能力之间提供了平衡。在实践中，它允许像 GPT-2 这样的模型比使用标准梯度下降训练得更加可靠。`llm.c`中的 C 实现表明，在数学之下，它只是一个简单的循环，为每个参数应用几个算术运算。

### 44. 梯度累积与微批量

现代语言模型非常庞大，我们希望在训练期间提供给它们的文本批量也同样庞大。但实际硬件有限：单个 GPU 或 CPU 可能没有足够的内存一次性处理一个大批量。为了解决这个问题，训练代码通常使用梯度累积和微批量。这两个想法都允许我们在不要求比我们的硬件能提供的更多内存的情况下模拟使用较大批量的训练。

#### **我们正在解决什么问题？**

当您处理一个数据批量时，您会运行正向和反向传递来计算梯度。如果您的批量大小非常大，您会得到更平滑的梯度（更少噪声），这通常有助于模型更好地收敛。但是，大批量可能不适合内存。

想象一下，在一个只能一次处理 128 个序列的 GPU 上，尝试用包含 1024 个序列的批量进行训练。如果没有技巧，您将不得不使用较小的批量大小，从而放弃大批量的好处。梯度累积通过允许您将大批量拆分成更小的微批量，一次处理一个，并像一次性处理大批量一样累积结果，来解决这个问题。

#### **实际应用中的工作原理**

假设我们想要一个有效的批量大小为 1024，但我们的硬件只支持 128。我们将大批量拆分成 8 个每个 128 的微批量：

1.  在微批量 1 上运行正向+反向，并存储梯度。

1.  在微批量 2 上运行正向+反向，并将其梯度添加到存储的梯度中。

1.  重复直到处理完所有 8 个微批量。

1.  一旦所有 8 个微批量的梯度都累积完毕，执行优化器更新。

重要的是第 4 步：我们只在每个有效批量更新一次参数，而不是在每个微批量之后。这保留了使用大批量训练的效果。

#### **伪代码示例**

这在简化的伪代码中可能看起来是这样的：

```c
[](#cb111-1)int accumulation_steps = 8;
[](#cb111-2)for (int step = 0; step < total_steps; step++) {
[](#cb111-3)    zero_grad(&model);
[](#cb111-4)    for (int i = 0; i < accumulation_steps; i++) {
[](#cb111-5)        dataloader_next_batch(&train_loader, tokens, labels, B, T);
[](#cb111-6)        forward(&model, tokens, labels, B, T);
[](#cb111-7)        backward(&model, tokens, labels, B, T);
[](#cb111-8)        // do NOT call optimizer update yet
[](#cb111-9)    }
[](#cb111-10)    adamw_update(&model, lr, beta1, beta2, eps, weight_decay, step+1);
[](#cb111-11)}
```

注意，尽管梯度是在多个微批量之间累积的，但优化器只在外部循环迭代一次运行。

#### **为什么梯度累积有帮助**

+   **内存效率**：您可以在不需要更多硬件的情况下使用较大的有效批量大小进行训练。

+   训练稳定性：较大的批量减少了梯度的方差，使得训练更少噪声。

+   **灵活性**：根据您的需求，您可以调整有效的批量大小，而无需更改硬件。

#### 微批处理与累积的比较

微批处理是指将批次分成更小部分的行为。梯度累积是在微批处理之后进行的：求和这些部分的梯度。一起，它们允许你在内存限制内模拟任何批大小的训练。

#### 为什么这很重要

训练质量通常取决于批大小。如果你不能直接放入一个大的批，梯度累积确保你仍然能获得好处。这是那些“工程技巧”之一，使得在有限的资源上训练最先进的模型成为可能。

#### 亲自尝试

1.  使用批大小 = 16 和无累积运行训练。观察损失曲线看起来有多嘈杂。

1.  现在设置微批大小 = 4 和 accumulation_steps = 4。这模拟了批大小 = 16，但以更小的块进行。比较损失曲线。

1.  将 accumulation_steps 增加到模拟批大小 = 32。观察训练是否变得更加平滑。

1.  在保持相同有效批大小的同时，尝试开启和关闭累积。注意每个 epoch 中优化器更新的差异。

1.  打印出优化器被调用的次数。使用累积，它应该少于微批次的数量。

#### 吸取的经验

梯度累积和微批处理是让你在硬件限制内使用大有效批大小进行训练的技术。它们保留了大型批次的优点——稳定性和更平滑的梯度——而不需要额外的内存。在`llm.c`中，训练循环的简单性意味着你可以清楚地看到累积是如何适应的：梯度在微批次之间求和，然后优化器才介入。这在代码上是一个小的调整，但在实践中是一个巨大的推动力。

### 45. 日志和进度报告

每个训练循环都需要一种方式来显示底层正在发生的事情。没有日志，你就不知道模型是否在改进，代码是否运行高效，或者是否有什么问题悄然发生。在`train_gpt2.c`中，日志有意被保持最小化但高度信息丰富：每个训练步骤会打印出步骤编号、当前的训练损失以及该步骤运行所需的时间。

#### 实际的日志代码

这里是`train_gpt2.c`中的相关代码片段：

```c
[](#cb112-1)// do a training step
[](#cb112-2)clock_gettime(CLOCK_MONOTONIC, &start);
[](#cb112-3)dataloader_next_batch(&train_loader);
[](#cb112-4)gpt2_forward(&model, train_loader.inputs, train_loader.targets, B, T);
[](#cb112-5)gpt2_zero_grad(&model);
[](#cb112-6)gpt2_backward(&model);
[](#cb112-7)gpt2_update(&model, 1e-4f, 0.9f, 0.999f, 1e-8f, 0.0f, step+1);
[](#cb112-8)clock_gettime(CLOCK_MONOTONIC, &end);
[](#cb112-9)
[](#cb112-10)double time_elapsed_s = (end.tv_sec - start.tv_sec) +
[](#cb112-11)                        (end.tv_nsec - start.tv_nsec) / 1e9;
[](#cb112-12)printf("step %d: train loss %f (took %f ms)\n",
[](#cb112-13)       step, model.mean_loss, time_elapsed_s * 1000);
```

这个小块完成了两件事：

1.  它使用`clock_gettime`测量训练步骤的持续时间。

1.  它报告了步骤编号、损失和经过的时间（以毫秒为单位）。

训练时的输出看起来是这样的：

```c
step 0: train loss 4.677779 (took 1987.546 ms)
step 1: train loss 5.191576 (took 1927.230 ms)
step 2: train loss 4.438685 (took 1902.987 ms)
```

#### 理解所报告的内容

+   步骤编号（`step`）告诉你训练的进度。由于深度学习通常需要运行数千步，这就像一个进度条。

+   训练损失（`model.mean_loss`）显示了模型如何拟合训练批次。较低的值通常意味着更好的预测。随着时间的推移观察这个数字的下降是学习正在发生的最主要信号。

+   步骤持续时间 (`time_elapsed_s * 1000`) 用于衡量性能。如果一步需要 2000 毫秒，那么 5000 步大约需要 3 个小时。监控这一点有助于你估计总训练时间并发现性能退化（例如，如果新的更改突然将步骤时间加倍）。

#### 为什么这很重要

日志是了解训练过程的窗口。如果损失平稳下降，则训练健康。如果它突然上升或保持平稳，则可能存在问题——可能是学习率太高，或者模型已经没有容量了。时间信息也很重要：你需要知道代码是否运行高效，或者是否在浪费周期。

#### 尝试自己操作

1.  将学习率从 `1e-4` 改为 `1e-2` 并观察损失的行为。如果它跳跃或变得不稳定，你将直接在日志中看到。

1.  通过每 100 步运行模型在保留的数据集上并打印 `val_loss` 来添加验证日志。将其与 `train_loss` 进行比较。

1.  使用以下命令将日志输出到文件：

    ```c
    [](#cb114-1)./train_gpt2 > log.txt
    ```

    然后在 Python 或 Excel 中绘制 `train_loss` 随步骤的变化曲线以可视化。

1.  添加吞吐量报告：通过将批量大小乘以序列长度 (`B*T`) 除以步骤时间来打印每秒的令牌数。这给出了更清晰的效率感。

1.  尝试禁用 `clock_gettime` 并只打印损失。注意没有时间信息时判断性能变得多么困难。

#### 吸收要点

即使是最简单的日志也能告诉你很多信息。只需一行记录，包括步骤、损失和持续时间，你就能知道训练的速度如何，是否在收敛，以及需要多长时间。在更大的框架中，这类信息通常隐藏在仪表板和监控工具后面，但核心思想是相同的：只有当你能看见并解释其进度时，训练才有用。

### 46. 验证运行在训练循环中

当你训练一个模型时，仅仅看它在训练数据上的表现是不够的。真正的测试是模型是否学会了适用于新、未见数据的模式。这就是验证的作用。验证就像模型在训练过程中不时参加的测验。它不计入学习——它只是检查模型真正理解了多少。

在 `train_gpt2.c` 中，验证直接构建在训练循环中。每隔一段时间，程序会暂停更新权重，并在一组它从未训练过的令牌上运行模型。然后它会打印出平均验证损失。这个数字告诉你模型是否真正泛化，而不仅仅是记忆。

#### 验证代码的样貌

下面是处理验证的实际代码块：

```c
[](#cb115-1)if (step % 10 == 0) {
[](#cb115-2)    float val_loss = 0.0f;
[](#cb115-3)    dataloader_reset(&val_loader);
[](#cb115-4)    for (int i = 0; i < val_num_batches; i++) {
[](#cb115-5)        dataloader_next_batch(&val_loader);
[](#cb115-6)        gpt2_forward(&model, val_loader.inputs, val_loader.targets, B, T);
[](#cb115-7)        val_loss += model.mean_loss;
[](#cb115-8)    }
[](#cb115-9)    val_loss /= val_num_batches;
[](#cb115-10)    printf("val loss %f\n", val_loss);
[](#cb115-11)}
```

初看，这看起来可能只是几行 C 语言代码。但背后是关于机器学习模型在训练过程中如何测试的几个重要思想。让我们一步步来看。

#### 逐步解释

第一行检查是否是运行验证的时间：

```c
[](#cb116-1)if (step % 10 == 0) {
```

这意味着验证每 10 步发生一次。`%` 运算符是“取模”，它返回除法的余数。如果步骤号能被 10 整除（如 0、10、20、30），则执行该块。通过这种方式间隔执行，验证不会过多地减慢训练速度，但仍然会定期提供更新。

接下来，代码设置了一个存储验证损失运行总量的地方：

```c
[](#cb117-1)float val_loss = 0.0f;
```

然后它重置验证数据加载器：

```c
[](#cb118-1)dataloader_reset(&val_loader);
```

这确保了验证数据集每次都是从开始处开始的。这样，结果是一致的——你总是在相同的文本集上检查模型，而不是从随机位置开始。

现在是遍历验证批次的循环：

```c
[](#cb119-1)for (int i = 0; i < val_num_batches; i++) {
[](#cb119-2)    dataloader_next_batch(&val_loader);
[](#cb119-3)    gpt2_forward(&model, val_loader.inputs, val_loader.targets, B, T);
[](#cb119-4)    val_loss += model.mean_loss;
[](#cb119-5)}
```

这是在内部发生的事情：

+   `dataloader_next_batch` 从验证集中获取下一批次的标记和标签。

+   `gpt2_forward` 在这些标记上运行模型，预测每个标记的下一个单词，并计算与真实标签的损失。

+   那个批次的损失被加到 `val_loss` 上。

注意，没有调用 `gpt2_zero_grad`，没有 `gpt2_backward`，也没有 `gpt2_update`。这是因为验证并不训练模型。它只测量性能。

最后，程序平均了批次的损失：

```c
[](#cb120-1)val_loss /= val_num_batches;
```

并打印出结果：

```c
[](#cb121-1)printf("val loss %f\n", val_loss);
```

这为你提供了一个单一的数字，总结了模型在训练的这个点上对未见数据的性能。

#### 如何读取验证损失

想象你在训练时看到这样的日志：

```c
step 0: train loss 4.677779 (took 1987.546 ms)
val loss 4.901234
step 1: train loss 5.191576 (took 1927.230 ms)
step 2: train loss 4.438685 (took 1902.987 ms)
...
step 10: train loss 3.912342 (took 1890.321 ms)
val loss 4.100321
```

每一步都会打印训练损失，而验证损失每 10 步出现一次。如果两个数字都在下降，这表明模型真正在学习。如果训练损失下降，但验证损失保持不变或开始上升，模型可能是在记忆训练集——这被称为过拟合。

#### 为什么验证很重要

没有验证，你可能会被误导，以为模型正在改进，只是因为训练损失正在下降。但那可能只是意味着它已经记住了训练数据。验证检查通过显示模型是否可以处理之前未见过的数据来防止这种情况。这就像学生用旧试卷（训练）练习，与用新问题（验证）测试一样。

#### 一些重要的细节

代码通过 `val_num_batches` 计算验证损失的均值，这个值之前设置为 5。这意味着它只检查 5 个批次，而不是整个验证数据集。这是一个捷径——这使得验证更快，但牺牲了一些测量的准确性。但对于训练反馈来说，这通常足够。

验证中的批大小 `B` 和序列长度 `T` 与训练相同。这保持了训练和验证之间损失的可比性。

#### 尝试自己操作

你可以尝试实验验证过程以更好地理解它。以下是一些想法：

1.  将频率从每 10 步改为每 5 步或甚至每步。你会看到更多的验证更新，但训练会变慢。

1.  将`val_num_batches`增加到 20。验证损失将变得不那么嘈杂，但每次检查将花费更长的时间。

1.  注释掉验证块并再次训练。注意你将失去对模型是否真正泛化的感觉。

1.  将验证损失值保存到文件并绘制它们。将曲线与训练损失曲线进行比较。你会看到它们是如何一起移动或分离的。

1.  尝试使用一个非常小的验证数据集。观察与更大的、更稳定的数据集相比，损失如何跳来跳去。

#### 吸取的教训

验证运行是简短的单向测试，可以让你有信心模型正在学习适用于新文本的模式。它们很容易实现——`train_gpt2.c`中的几行代码——但它们是监控训练的最重要工具之一。通过定期检查验证损失，你可以确保你的模型不仅仅是记忆，而是在语言建模方面变得更好。

### 47. 检查点参数和优化器状态

训练一个模型可能需要数小时、数天甚至数周。如果你在程序中途停止——无论是意外（崩溃、断电）还是故意（暂停以节省计算）——你不想从头开始。通过将模型的参数和优化器状态保存到磁盘，检查点解决了这个问题，这样你就可以稍后恢复训练。

#### 检查点包含的内容

检查点就像是机器学习的“保存游戏”。至少需要以下内容：

1.  模型参数 – 神经网络的实际权重，以内存中的浮点数形式存储。这些定义了模型迄今为止学到了什么。

1.  优化器状态 – 对于 AdamW 来说，这包括梯度的运行平均值（`m_memory`）和平方梯度的运行平均值（`v_memory`）。没有这些，优化器会失去对过去更新的“记忆”，这可能会使恢复的训练不稳定。

1.  步数计数器 – 到目前为止完成的步数。这对于 AdamW 中的偏差校正和学习率调度很重要。

这三个要素共同捕捉了完整的训练状态。

#### 保存检查点

虽然`train_gpt2.c`文件被保持最小化，并且不包括完整的检查点代码，但思路是直接的。你分配一个文件，写入所有参数、优化器缓冲区和元数据，然后关闭文件。在伪代码中，它看起来像这样：

```c
[](#cb123-1)FILE *f = fopen("checkpoint.bin", "wb");
[](#cb123-2)fwrite(model.params_memory, sizeof(float), model.num_parameters, f);
[](#cb123-3)fwrite(model.m_memory, sizeof(float), model.num_parameters, f);
[](#cb123-4)fwrite(model.v_memory, sizeof(float), model.num_parameters, f);
[](#cb123-5)fwrite(&step, sizeof(int), 1, f);
[](#cb123-6)fclose(f);
```

这是一个模型和优化器的二进制转储。稍后，你可以使用`fread`调用将文件加载回相同的内存位置。

#### 加载检查点

加载是相反的过程：

```c
[](#cb124-1)FILE *f = fopen("checkpoint.bin", "rb");
[](#cb124-2)fread(model.params_memory, sizeof(float), model.num_parameters, f);
[](#cb124-3)fread(model.m_memory, sizeof(float), model.num_parameters, f);
[](#cb124-4)fread(model.v_memory, sizeof(float), model.num_parameters, f);
[](#cb124-5)fread(&step, sizeof(int), 1, f);
[](#cb124-6)fclose(f);
```

一旦加载，训练可以继续从上次停止的地方开始。

#### 为什么优化器状态很重要

可能看起来只保存模型的参数就足够了。但 AdamW 依赖于过去梯度的移动平均值。如果你丢弃这些值，只使用参数重新开始，优化器的行为将不同。学习可能会突然变得不稳定，或者有效学习率可能感觉不正确。这就是为什么保存参数和优化器状态可以提供最忠实的重启。

#### 为什么检查点很重要

训练很少是平滑的。服务器重启，实验被中断，发现错误。没有检查点，任何中断都意味着计算资源的浪费和进度丢失。有了检查点，你可以随意暂停和恢复。它们还让你能够保存训练中的关键时刻——例如，当验证损失最低时保存模型，而不仅仅是结束时。

#### 亲自尝试

1.  编写一个小的函数，在每 100 步后保存模型的参数。然后在中途终止程序并从保存的文件中重新加载。确认恢复训练从上次停止的地方继续。

1.  尝试只保存参数而不保存优化器状态。恢复训练并比较损失曲线。你会看到运行与原始运行发生了偏差。

1.  在多个步骤中保存检查点，并在之后重新加载它们以比较模型生成（模型在 10 步、100 步、1000 步后是否产生了更流畅的文本？）。

1.  故意损坏检查点文件的一部分（翻转几个字节）并尝试重新加载。这有助于你理解为什么在实际系统中经常添加一致性检查或校验和。

1.  以版本化的方式存储检查点（例如，`checkpoint_step100.bin`，`checkpoint_step200.bin`），这样你就可以在后续的训练阶段性能下降时回滚。

#### 要点

检查点使得长时间运行的训练变得可行。通过保存参数、优化器状态和步骤计数器，你不仅保留了模型所知道的内容，还保留了它的学习过程。在实际项目中，检查点是实验和生产之间的桥梁：它们让你能够停止、恢复、比较和部署模型，而无需从头开始。即使 `llm.c` 并没有完全实现它，这个概念简单而宝贵。

### 48. 可复现性和小偏差

当训练深度学习模型时，表面上看起来相同的两次运行仍可能表现出不同的行为。一次运行可能快速收敛，另一次可能需要更长的时间，有时即使使用了相同的 dataset 和代码，损失也可能发散。这是因为随机性和数值精度在训练过程中的交互方式。可复现性是关于控制这些因素，以确保结果一致且有意义。

#### 随机性的来源

训练过程中有多个地方会引入随机性：

+   数据顺序：如果批次以不同的方式打乱，模型将看到新的序列中的标记。早期步骤可以影响训练的轨迹。

+   权重初始化：初始参数通常设置为随机。不同的种子导致略微不同的起始点。

+   Dropout 和采样：虽然 `train_gpt2.c` 最小化且不包含 dropout 层，但许多神经网络都包含。Dropout 在训练过程中随机禁用激活。

+   浮点运算：在 CPU 和 GPU 上，求和或并行归约的顺序可能导致微小的舍入差异。经过许多步骤，这些小的变化会累积。

#### llm.c 如何处理可复现性

仓库中包含了`llmc/rand.h`中的`manual_seed`和`random_f32`等函数。这些是简单的随机数生成器，可以用一个固定的值来初始化。例如：

```c
[](#cb125-1)manual_seed(1337); 
```

如果你在这之前调用，随机数生成器每次运行都会从相同的状态开始。这意味着权重初始化和采样将是可重复的。

数据加载器也有可重复性选项。当你初始化一个`DataLoader`时，你可以决定是否对批次进行洗牌。保持这一致性确保模型每次运行都看到相同的数据顺序。

#### 为什么仍然会发生小的偏差

即使使用固定的种子，你也可能注意到两次运行并不完全相同。在 CPU 上，差异通常来自 OpenMP 并行循环——线程可能以不同的顺序求和数字，产生略微不同的结果。在 GPU 上，并行性和库实现（如 cuBLAS 或 cuDNN）也能做到同样的事情。

这些差异通常非常小，但深度学习系统是混沌的：早期步骤中的微小变化可能会在后期产生可见的差异。这并不意味着代码是错误的——这仅仅意味着浮点数计算有局限性。

#### 为什么可重复性很重要

可重复性不仅仅是关于安心。它有实际的应用：

+   调试：如果出现错误，你希望重现完全相同的运行来诊断它。

+   比较：当测试新的优化器、调度器或架构时，你希望在相同条件下进行公平的比较。

+   科学：可重复的结果对于研究论文和基准测试至关重要。

同时，在大型的并行系统中，绝对位对位的可重复性通常是不切实际的。相反，目标是实际的可重复性：确保运行足够相似，以得出相同的结论。

#### 示例实验

假设你使用`manual_seed(1337)`对训练进行种子化，并使用相同的数据集。你可能会得到一个像这样的损失曲线：

```c
Run A: step 1000 → val loss 3.42  
Run B: step 1000 → val loss 3.43 
```

数字并不完全相同，但它们很接近。重要的是模型的学习轨迹是稳定的，结果是可比较的。

如果你移除种子并允许完全随机，你可能会得到：

```c
Run A: step 1000 → val loss 3.42  
Run B: step 1000 → val loss 3.89 
```

这两种方法都是有效的，但比较起来更困难。

#### 试试你自己

1.  不设置种子运行两次训练。比较在步骤 500 时训练损失和验证损失的不同。

1.  在构建模型之前，使用`manual_seed(42)`设置一个固定的种子。运行两次训练并再次比较。你应该会看到更接近的数字。

1.  使用多线程启用 OpenMP，然后以单线程运行。注意由于浮点数求和顺序的不同，结果会有轻微的差异。

1.  从具有不同种子的运行中保存两个检查点。使用该模型生成文本并比较输出。你会看到不同的措辞，但两者在语法上都是合理的。

1.  增加数据集的大小并检查运行之间的差异是否缩小。随着数据的增加，随机性变得不那么重要。

#### 吸取的教训

训练的可重复性在于尽可能控制随机性，并在无法控制时接受小的偏差。在 `llm.c` 中，通过简单的种子函数和确定性的数据加载器选项，可重复性得到了明确。完美的位级可重复性并不是目标——目标是确保结果稳定、可比，并且科学合理，即使有微小的数值差异。

### 49. 命令行标志和默认值

当你运行训练程序时，你通常希望在不编辑源代码的情况下更改某些设置。例如，你可能想尝试不同的批处理大小，调整学习率，或者进行更多的训练步骤。命令行标志使得这一点成为可能。在 `train_gpt2.c` 中，默认值在程序内部设置，但它也可以编译为接受参数，这样你在保持代码最小化的同时增加了灵活性。

#### 标志为什么存在

深度学习实验对超参数非常敏感——如学习率、批处理大小、序列长度或步骤数等值。如果每次更改都需要修改源代码、重新编译和重新运行，实验将会很慢且容易出错。标志允许你在运行时快速配置这些参数。

在许多大型框架（如 PyTorch 或 TensorFlow）中，命令行参数是通过辅助库解析的。在 `llm.c` 中，其哲学是简单：标志要么在代码中定义为常量，要么你可以通过标准的 C 参数解析扩展 `main` 函数以覆盖默认值。

#### `train_gpt2.c` 中的默认值

通过查看代码，以下是 `main` 函数中硬编码的主要默认值：

+   批处理大小 (`B`):

    ```c
    [](#cb128-1)int B = 4; // number of sequences per batch
    ```

+   序列长度 (`T`):

    ```c
    [](#cb129-1)int T = 64; // tokens per sequence
    ```

+   验证批次：

    ```c
    [](#cb130-1)int val_num_batches = 5;
    ```

+   训练步骤：

    ```c
    [](#cb131-1)for (int step = 0; step <= 40; step++) {
    ```

    在此示例中，默认只运行 40 步。

+   优化器超参数（在 `gpt2_update` 调用中）：

    ```c
    [](#cb132-1)gpt2_update(&model, 1e-4f, 0.9f, 0.999f, 1e-8f, 0.0f, step+1);
    ```

    这里学习率是 `1e-4`，AdamW 的 beta 值为 `0.9` 和 `0.999`，epsilon 是 `1e-8`，权重衰减是 `0.0`。

这些默认值被选择是为了使参考训练循环快速且可预测，尤其是在像 Tiny Shakespeare 或 Tiny Stories 这样的小数据集上。

#### 如何添加标志

如果你想增加灵活性，可以通过参数解析扩展 `main` 函数：

```c
[](#cb133-1)int main(int argc, char argv) {
[](#cb133-2)    int B = 4;
[](#cb133-3)    int T = 64;
[](#cb133-4)    int max_steps = 40;
[](#cb133-5)    if (argc > 1) B = atoi(argv[1]);
[](#cb133-6)    if (argc > 2) T = atoi(argv[2]);
[](#cb133-7)    if (argc > 3) max_steps = atoi(argv[3]);
[](#cb133-8)    ...
[](#cb133-9)}
```

现在，你可以运行：

```c
[](#cb134-1)./train_gpt2 8 128 100
```

这将批处理大小设置为 8，序列长度设置为 128，步骤设置为 100，而无需更改源代码。

#### 为什么这很重要

命令行标志使得实验效率大大提高。你可以在一天之内尝试多个配置，而无需重新编译或反复编辑文件。这在运行集群上的作业时特别有用，你希望脚本能够自动以不同的参数启动许多实验。

默认值同样重要：它们为你提供了一个安全、可预测的起点。初学者可以在不思考标志的情况下运行代码，而高级用户可以根据需要覆盖值。

#### 亲自试试

1.  保持默认的批处理大小为 4 和序列长度为 64。运行训练并注意每步的时间。

1.  通过编辑代码将批量大小更改为 8。观察训练速度如何变化以及内存使用如何增加。

1.  修改循环以训练 200 步而不是 40 步。观察损失如何进一步下降。

1.  添加参数解析以接受学习率作为标志。实验`1e-3`与`1e-5`，看看训练如何快速发散或停滞。

1.  创建一个 shell 脚本，多次运行训练，并使用不同的`B`和`T`值。比较结果。

#### 吸取的经验教训

命令行标志和默认值在简单性和灵活性之间取得平衡。默认值使得代码可以直接运行，而标志允许你在不经常编辑源代码的情况下扩展实验。在`train_gpt2.c`中，这种设计保持了训练循环的最小化，但仍然具有适应性，鼓励清晰性和实验性。

### 50. 示例 CPU 训练日志和输出

理解训练循环在做什么的最好方法之一是阅读其日志。日志是程序告诉你训练进展的方式：损失是多少，运行速度如何，以及验证检查是否在改善。在`train_gpt2.c`中，日志被故意设置为最小化，这样你可以轻松地看到关键信息而不会被淹没。

#### 日志的样式

下面是运行 Tiny Shakespeare 上的 CPU 训练循环的输出片段：

```c
train dataset num_batches: 1192
val dataset num_batches: 128
[GPT-2]
max_seq_len: 1024
vocab_size: 50257
padded_vocab_size: 50304
num_layers: 12
num_heads: 12
channels: 768
num_parameters: 124475904
num_activations: 73347840
val loss 5.325529
step 0: train loss 4.677779 (took 1987.546000 ms)
step 1: train loss 5.191576 (took 1927.230000 ms)
step 2: train loss 4.438685 (took 1902.987000 ms)
...
```

输出的每一部分都有意义：

+   数据集大小：有多少训练和验证批次可用。

+   模型配置：确认 GPT-2 模型正确加载（序列长度、词汇量、层数等）。

+   验证损失：衡量模型在未见数据上的表现的平均指标。

+   训练步骤日志：对于每个步骤，你都会看到训练损失以及该步骤花费的毫秒数。

#### 理解损失值

损失是告诉我们模型预测与正确答案之间距离的数字。越低越好。

+   大约 5.3 的损失意味着模型基本上是在猜测。

+   随着训练的进行，你希望看到这个数字缓慢下降。

+   如果数字停滞不前或上升，这可能表明学习率、数据集或实现存在问题。

想象它就像一份成绩单：一开始，模型在每次测试中都失败，但随着它练习（训练），成绩（损失值）会提高。

#### 速度测量

“耗时…毫秒”部分显示了每个步骤花费的时间。在 CPU 上，这通常很慢，有时每个步骤可能需要几秒钟。在 GPU 上，相同的步骤可能只需要几十毫秒。

时间日志很有用，因为它们可以帮助你：

+   估计完整训练需要多长时间。

+   比较不同机器的性能。

+   如果训练突然变慢，要留意出现的问题。

#### 偶尔的验证检查

每过几步，代码就会切换到验证数据并打印一个`val loss`。这是至关重要的：如果模型记住了训练集，训练损失总是会下降，但验证损失会告诉你它是否实际上在学习可以推广的模式。

如果训练损失下降但验证损失保持较高，这是过拟合的迹象。

#### 生成的样本

在某些步骤中，代码还会打印出如下生成的文本：

```c
generating:

 The King had not
 that the Duke of Northumberland and the Duke of
... 
```

即使文本一开始看起来很奇怪，这也是一个强有力的迹象，表明模型正在学习。一开始，输出完全是胡言乱语，但随着训练的继续，你开始看到可识别的单词和模式。

#### 为什么这很重要

日志是了解训练过程的窗口。没有它们，训练将是一个黑盒——你可能会等上几个小时，却不知道它是否在正常工作。通过观察损失曲线、步骤时间和样本输出，你可以做出明智的调整，并确信模型正在正确的轨道上。

#### 试试看

1.  按原样运行训练循环并保存控制台输出。标记步骤 0 和步骤 40 之间损失的变化。

1.  将步数增加到 200，并比较损失如何演变。

1.  将批处理大小从 4 改为 8，并注意训练速度和损失行为。

1.  编辑代码以每步打印验证损失，而不是每 10 步打印一次。趋势看起来是否更平滑？

1.  在步骤 20 和 40 时保存生成的样本。比较质量如何变化。

#### 吸取的经验

训练日志是模型进度的日记。它们显示了模型学习速度有多快，泛化能力有多好，计算运行有多快。通过仔细阅读和解释日志，你可以指导实验，早期发现问题，并欣赏模型内部发生的进步。

## 第六章 测试和性能分析

### 51. 调试状态结构及其作用

当构建和训练像 GPT-2 这样复杂的模型时，你需要方法来窥视内部，检查传递的值是否有意义。这就是调试状态结构体发挥作用的地方。在 *llm.c* 中，代码是用纯 C 语言编写的，没有像 PyTorch 这样的框架提供的丰富调试工具。这意味着开发者必须创建自己的机制来存储、检查和比较中间值。

C 语言中的结构体只是一个将相关变量组合在一起的容器。对于调试，你可以将结构体想象成一个小的笔记本，程序在计算时会将数字记录下来。这些数字可能包括：

+   标记的原始嵌入。

+   在 softmax 前后注意力的分数。

+   每个 MLP 块的输出。

+   下一个标记的预测概率。

通过将这些保存到结构化格式中，程序可以在以后将它们与受信任的参考实现（通常是 PyTorch）的输出进行比较。

#### 实际工作原理

在 *llm.c* 中，有一些地方将浮点数数组（如隐藏状态或 logits）复制到调试状态结构体中。一旦存储，这些值就可以打印出来，写入文件，或与 PyTorch 的“黄金”结果进行比较。

想象你正在测试一小批输入标记。正向传播像往常一样运行，但在特定的检查点（比如，在注意力之后或最终线性投影之后），程序将这些数组写入结构体。稍后，当使用相同的输入和权重运行 PyTorch 模型时，可以逐元素比较这两个输出。

这对于捕捉细微的错误至关重要：

+   矩阵乘法中的错误转置。

+   忘记在注意力中应用掩码。

+   softmax 中的浮点精度不匹配。

没有结构体，你只知道模型损失看起来“不正常”。有了结构体，你知道 *确切* 哪个步骤出了问题。

#### 为什么这很重要

调试结构体架起了 C 和 Python 生态系统之间的桥梁。PyTorch 拥有十年的实战层，因此它是正确性的黄金标准。通过在 C 中保存中间激活并与之比较 PyTorch，开发者确保每个层的行为完全相同。这增强了信心，即 *llm.c* 代码库不仅仅是“大致正确”，而是精确地重现了 GPT-2 的数学。

对于任何修改代码的人来说——例如，编写新的激活函数或进行量化实验——调试结构体就像一个安全网。你可以快速查看你的更改是否意外地改变了输出，从而破坏了与原始模型的兼容性。

#### 亲自尝试

1.  跟踪正向传播：使用 tiny batch 运行 CPU 版本并启用调试转储。查看嵌入、注意力分数和最终 logits。

1.  与 PyTorch 进行交叉检查：将相同的输入通过 Hugging Face GPT-2。打印相同的张量。手动比较几个条目——它们是否非常接近？

1.  引入一个错误：更改注意力中的缩放因子（例如，删除 `1/√d` 项）。再次运行并查看不匹配在调试结构体中显示得多快。

1.  扩展结构体：为关心的中间步骤添加一个新字段，例如 LayerNorm 输出。在调试期间打印它，以查看归一化如何改变激活。

#### 吸取的经验教训

调试状态结构体是 *llm.c* 的显微镜。它们允许你暂停数字的流动，记录它们，并将它们与已知良好的模型进行比较。没有它们，开发就像蒙着眼睛工作。有了它们，你可以精确地追踪错误，确保与 PyTorch 的兼容性，并且可以自信地扩展系统，因为你有一个可靠的安全网。

### 52. `test_gpt2.c`：CPU 与 PyTorch

测试是 *llm.c* 项目最重要的部分之一。文件 `test_gpt2.c` 存在的目的是检查 GPT-2 的 C 实现是否产生与 PyTorch 相同的输出，这是可信的参考。没有这个文件，你只能知道最终的训练损失是否合理。有了它，你可以验证正向传播的每个部分是否匹配。

在其核心，`test_gpt2.c`运行一个非常受控的实验：它加载一个 GPT-2 模型检查点（从 PyTorch 导出），准备一小批输入标记，在 C 中执行前向传递，并将输出与 PyTorch 中的相应张量进行比较。如果一切在严格的数值公差范围内匹配，你就知道 C 代码是正确的。如果不匹配，你会有一个清晰的信号表明有问题。

#### 测试是如何工作的

1.  加载检查点 测试首先读取一个二进制检查点文件，例如`gpt2_124M.bin`，其中包含 GPT-2 模型的权重。这些权重最初是在 PyTorch 中训练的，然后导出为*llm.c*可以理解的二进制格式。

1.  准备输入 测试使用一个已知的标记 ID 序列——有时来自 Tiny Shakespeare 这样的数据集，有时只是几个手工挑选的标记。这确保了相同的输入可以通过 PyTorch 和 C 实现运行。

1.  运行 C 前向传递 函数`gpt2_forward`在 CPU 上执行。所有嵌入、注意力层、MLP 和最终 logits 的计算方式与实际推理或训练期间完全相同。

1.  与 PyTorch 比较 对于每个主要张量（例如，隐藏状态、注意力输出、最终 logits），其值将与 PyTorch 保存的输出进行比较。由于浮点数数学在库之间可能略有差异，比较通常允许非常小的差异。`1e-5`这样的公差是常见的。

1.  报告差异 如果任何元素超出允许的公差范围，测试报告将显示差异。开发者可以随后调查差异开始的地方，通常是通过添加更多中间状态的调试输出。

#### 为什么这个测试至关重要

C 代码是低级的且不容忍错误。一个索引错误、错误的步长或注意力中缺少缩放因子都可能使输出剧烈偏离。由于 GPT-2 有数百万个参数，这样的错误几乎不可能手动发现。通过将实现与 PyTorch 联系起来，`test_gpt2.c`提供了一个基准检查。

这也确保了科学可重复性。如果其他人下载*llm.c*并运行`test_gpt2.c`，他们应该看到与 PyTorch 相同程度的协议。这样，他们可以相信训练运行、损失和模型输出不是损坏实现的产物。

#### 动作中的示例

假设你刚刚修改了注意力代码以优化矩阵乘法。你重新编译并运行`test_gpt2.c`。如果你看到如下错误：

```c
Mismatch at position [0, 12, 42]: C=0.1234, Torch=0.1235
```

你知道两者在公差范围内匹配——一切正常。但如果你看到：

```c
Mismatch at position [0, 12, 42]: C=0.3456, Torch=0.1235
```

那是一个红旗。这意味着优化引入了一个错误。如果没有这个测试，你可能会在训练无法收敛的更晚时候才注意到。

#### 为什么这很重要

`test_gpt2.c`是保证 C 实现不仅仅是“足够接近”，而是*忠实*的依据。它确保改进、优化或甚至重写不会无声地破坏模型的行为。它是 C 内部实验世界和 PyTorch 稳固基线之间的直接桥梁。

#### 亲自试试

1.  在仓库中运行`make test_gpt2`。观察输出是否与 PyTorch 匹配。

1.  故意更改`gpt2_forward`中的一行代码——例如，移除注意力缩放因子。再次运行测试，看看它会多快失败。

1.  添加自己的打印语句以显示正在比较哪些张量。观察数字几乎完全对齐。

1.  尝试使用不同的检查点（例如，124M vs 355M）来查看是否在各个尺度上保持一致性。

#### 吸取教训

`test_gpt2.c`不仅仅是仓库中的另一个文件——它是真相计。它让你确信 GPT-2 的复杂层已经在 C 中正确实现，并且与 PyTorch 保持一致。这种信心是允许进一步工作——无论是训练、分析还是扩展模型——在稳固基础上进行的前提。

### 53. 在公差内匹配输出

一旦设置了像`test_gpt2.c`这样的测试，下一个挑战就是确定输出需要多接近才能通过测试。计算机在进行浮点运算时并不总是产生位对位完全相同的结果。操作顺序、指令的精度，甚至硬件类型（CPU vs GPU）都可能导致微小的差异。

如果你要求完全匹配，即使实现是正确的，大多数测试也会失败。这就是为什么*llm.c*使用基于公差的比较。代码不是询问“这些数字是否完全相等？”，而是询问“这些数字*是否足够接近*？”

#### 绝对公差与相对公差

定义“足够接近”有两种常见方式：

+   绝对公差：检查两个数字之间的差值是否小于一个阈值。例如，

    ```c
    |0.123456 - 0.123455| = 0.000001 < 1e-5
    ```

    这对于接近零的值来说效果很好。

+   相对公差：检查差值相对于数字大小的相对大小。例如，

    ```c
    |1000.0 - 1000.1| / 1000.0 = 0.0001
    ```

    虽然绝对差值为 0.1，但与 1000 的规模相比，这非常小。

实际上，代码通常结合了两者。如果差值小于绝对公差或相对公差，则通过测试。

#### 为什么需要公差

想象你在 C 和 PyTorch 中在 CPU 上运行前向传递。PyTorch 可能会使用融合内核或更高精度的累加。如果你比较最终的 logits，你可能会看到如下值：

+   PyTorch: `-3.4521234`

+   C: `-3.4521255`

差值仅为`0.0000021`。从实用角度来说，它们是相同的。没有公差，这个微小的差异将无法通过测试。有了公差，你可以安全地说两种实现是一致的。

#### 调试示例

假设你在 softmax 之后比较概率。你可能会得到：

+   PyTorch: `0.3333333, 0.3333333, 0.3333333`

+   C: `0.3333334, 0.3333333, 0.3333333`

在这里，第一个值在最后一位小数上有所不同。容差规则说这没关系，因为绝对误差小于`1e-7`。

但如果你看到了类似的情况：

+   PyTorch: `0.3333333, 0.3333333, 0.3333333`

+   C: `0.5000000, 0.2500000, 0.2500000`

不匹配非常严重——没有任何容差规则会允许这种情况。这是一个明显的错误。

#### 为什么这很重要

在容差范围内匹配不仅仅是一个技术细节；它关乎信任。它让你有信心地说，实现是数学上忠实于参考的。你不会浪费时间追踪无害的十进制噪声，但你也不会错过真正的错误。

这种方法也是使跨平台开发成为可能的原因。相同的`*llm.c*`代码可以在 Linux、macOS 上运行，甚至可以在不同的编译器中运行，只要结果在容差范围内，你就知道模型行为得到了保留。

#### 试试看

1.  运行`test_gpt2.c`并查看输出日志。注意有多少位小数匹配。

1.  将代码中的容差阈值从`1e-5`改为更严格的值，例如`1e-8`。看看测试是否因为无害的浮点噪声而开始失败。

1.  添加一个故意的错误——例如，在注意力代码中跳过分母`√d`——然后重新运行。不匹配将远大于容差，证明错误是真实的。

1.  将 CPU 结果与 PyTorch 进行比较，然后使用不同的编译器标志（如`-O0`与`-O3`）重新编译，并检查结果是否仍在容差范围内。

#### 吸取的教训

基于容差的测试使得`*llm.c*`既严格又现实。它确保只有当差异重要时才会标记差异，同时忽略浮点数学的无害怪癖。这使得测试套件成为捕捉真正错误的可靠工具，而不会让你被虚假警报淹没。

### 54. 使用`profile_gpt2.c`进行性能分析

在验证输出与 PyTorch 匹配后，下一个重大问题是：代码运行得有多快？正确性是必要的，但性能是使像`*llm.c*`这样的最小 C 实现值得做的关键。这就是`profile_gpt2.c`的作用。它是一个小的程序，通过 GPT-2 运行受控的前向传递并测量它们所需的时间，帮助你了解瓶颈在哪里。

#### 性能分析的含义

性能分析是衡量程序性能的行为，而不仅仅是其正确性。而不是问“这个数字与 PyTorch 匹配吗？”，性能分析会问：

+   一个前向传递需要多少毫秒？

+   模型中哪一部分消耗的时间最多？

+   使用 OpenMP 线程实际上能加快速度吗？

+   批处理大小如何影响运行时间？

通过回答这些问题，你可以做出关于优化的明智决策。

#### `profile_gpt2.c`是如何工作的

性能分析程序的结构类似于简化的推理循环。

1.  模型设置它将 GPT-2 检查点（例如，`gpt2_124M.bin`）加载到内存中，并为激活分配空间。

1.  虚拟输入而不是使用真实文本，它创建随机的标记 ID。这样，测量的成本纯粹来自计算，而不是来自数据加载或标记化。

1.  使用时钟函数计时在每个前向传递之前和之后，使用`clock_gettime(CLOCK_MONOTONIC, &start)`和`&end`记录时间戳。差异给出以秒为单位的运行时间，这通常被转换为毫秒。

1.  循环以稳定结果由于你的计算机上的后台进程，单次运行可能会很嘈杂。为了使结果平滑，`profile_gpt2.c`多次运行前向传递并平均结果。

1.  报告结果最后，它打印出每次前向传递的平均时间。有时它还会估计 FLOPs（每秒浮点运算数），这给你一个与 CPU 理论峰值相比效率的大致概念。

#### 从性能分析中你可以学到什么

在 CPU 上运行`profile_gpt2.c`可以得到如下见解：

+   注意力块主导了运行时间，因为它们涉及大矩阵乘法。

+   增加批大小会使运行时间更长，但不是成比例的——有时更大的批次使用硬件更有效率。

+   当有多个 CPU 核心可用时，OpenMP 可以加快速度，但线程数量达到一定程度后，扩展可能会趋于平坦。

这有助于决定在哪里投入精力。例如，如果 LayerNorm 占运行时间的 2%，而注意力层占 70%，你知道优化应该集中在注意力代码上。

#### 为什么这很重要

性能分析不仅仅是关于数字。它关乎指导你的开发选择。没有性能分析，你可能会花费数周手动优化 LayerNorm，结果发现它几乎不影响整体运行时间。有了性能分析，你可以立即看到哪里有减速，并可以专注于真正的瓶颈。

它还提供了基线性能数据。如果你在实现中做了改变，重新运行`profile_gpt2.c`会告诉你是否加快了速度或减慢了速度。这个反馈循环对于优化工作至关重要。

#### 尝试自己操作

1.  使用小型模型检查点编译并运行`profile_gpt2.c`。注意每次前向传递报告的运行时间。

1.  改变批大小`B`和序列长度`T`，然后重新运行。观察运行时间如何随着更大的输入而扩展。

1.  设置`OMP_NUM_THREADS=1`以禁用多线程，并与`OMP_NUM_THREADS=4`或更高值进行比较。多核心能提高多少速度？

1.  修改代码以计时单个层（嵌入层、注意力层、MLP 层）。这能提供更精确的洞察，了解哪些部分主导了计算。

#### 吸取的经验

`profile_gpt2.c`将性能从猜测变成了硬数据。它告诉你前向传递确切需要多长时间，多少线程有帮助，以及真正的瓶颈在哪里。有了它，你可以在优化代码时跟踪进度，确保你的更改不仅使模型正确，而且高效。

### 55. 测量 FLOPs 和 CPU 性能

在谈论深度学习的性能时，仅仅说“这次运行花费了 200 毫秒”是不够的。为了真正理解效率，我们需要一个与硬件和输入大小无关的度量。这就是 FLOPs 的作用：浮点运算。一个 FLOP 是一个涉及实数的单一数值计算——比如加法、乘法或除法。

通过计算模型所需的 FLOPs 并与计算机每秒可以执行的数量进行比较，您可以评估您的实现与 CPU 理论最大速度的接近程度。

#### 实际中 FLOPs 的含义

GPT-2 的每一层—嵌入、注意力、前馈—都可以分解为一系列乘法和加法。例如：

+   两个大小为`M × K`和`K × N`的矩阵之间的矩阵乘法需要`2 × M × K × N` FLOPs。

+   在一个大小为`d`的向量上应用 softmax 需要大约`3d` FLOPs（指数、求和和除法）。

+   一个具有隐藏大小`h`和中间大小`4h`的 MLP 块需要多次矩阵乘法，每个训练步骤累计数十亿 FLOPs。

当您将它们跨所有层求和时，即使是像 124M 参数的小型 GPT-2 模型，在单次正向传播中也涉及数吉 FLOPs（数十亿操作）。

#### `profile_gpt2.c`如何估计 FLOPs

分析代码并不真正计算每一次乘法。相反，它使用从矩阵维度推导出的公式：

1.  配置结构体(`model.config`)提供了层数、头数、嵌入大小和序列长度。

1.  对于每个块（注意力+MLP），代码应用了矩阵乘法和 softmax 的标准 FLOPs 公式。

1.  这些计数被加起来以估计正向传播的总 FLOPs。

1.  将总 FLOPs 除以测量的运行时间（以秒为单位）得到 FLOPs/second，也称为吞吐量。

例如，如果正向传播需要 0.1 秒并涉及 2000 亿 FLOPs，那么吞吐量约为 200 GFLOPs/s。

#### 为什么这很有用

1.  比较硬件：您可以在笔记本电脑 CPU 和服务器 CPU 上测试相同型号，然后比较 FLOPs/s 以查看服务器有多快。

1.  比较实现：如果您修改注意力以使用不同的算法，FLOPs 计数不会改变，但如果运行时间减少，吞吐量增加—这表明优化是有效的。

1.  了解你的限制：由于内存瓶颈，CPU 通常只能达到其理论峰值 FLOPs 的一小部分。分析显示了您在实践中接近的程度。

#### 示例

让我们来说：

+   正向传播 FLOPs：150 亿

+   运行时间：0.2 秒

+   吞吐量：75 GFLOPs/s

如果您的 CPU 数据表显示峰值是 200 GFLOPs/s，那么您的效率大约是 37%。这个差距可能是由内存延迟、缓存未命中或缺乏向量化引起的。

#### 试试看

1.  运行`profile_gpt2.c`并注意报告的 FLOPs 和运行时间。

1.  改变序列长度`T`并观察 FLOPs 如何与其成线性关系。

1.  增加模型配置中的层数—观察 FLOPs 相应增加。

1.  将你测量的 FLOPs/s 与你 CPU 列出的理论最大值进行比较。你有多接近？

#### 吸取的教训

FLOPs 将性能从“感觉很快”转化为可以跨运行、机器和实现进行比较的硬数值。通过了解操作数和实际吞吐量，你可以清楚地了解 *llm.c* 代码的真正效率以及进一步优化的可能收益。

### 56. 在 CPU 上捕获内存使用情况

虽然 FLOPs 告诉我们模型需要多少原始计算，但性能不仅仅是关于速度——它还关乎内存使用。现代语言模型非常庞大，在有限的 RAM 的 CPU 上，内存可能成为真正的瓶颈。这就是为什么 *llm.c* 也强调在推理和训练过程中监控内存使用量。

#### GPT-2 中消耗内存的内容

有几个关键组件会占用空间：

1.  参数（权重）：GPT-2 124M 大约有 1240 万个参数。每个参数都存储为 32 位浮点数（4 字节）。仅此一项就大约是 500 MB。

1.  梯度：在训练过程中，每个参数的梯度都会被存储。这会使内存使用量加倍。

1.  优化器状态：AdamW 每个参数需要两个额外的内存槽（`m` 和 `v`），这又将其加倍。结合参数、梯度和优化器状态，训练可能需要内存中参数大小的 4 倍。

1.  激活：这些是每一层的中间输出（注意力分数、MLP 结果、归一化状态）。对于反向传播，前向传递的激活必须保留，直到计算梯度。根据批处理大小和序列长度，激活可能和参数内存相媲美。

#### 实际测量内存使用

在 CPU 上，可以通过几种方式检查内存使用情况：

+   操作系统工具：`top`、`htop` 或活动监视器显示程序使用的总内存。

+   代码中的手动会计：*llm.c* 知道有多少参数、梯度和优化器状态存在。通过将它们的计数乘以 4 字节，它可以精确地估计使用量。

+   分析期间的仪器：你可以在前向或反向传递的不同阶段添加检查点，以打印内存使用情况。

例如：

+   仅参数：约 ~500 MB。

+   参数 + 梯度：约 ~1 GB。

+   参数 + 梯度 + 优化器：约 ~2 GB。

+   添加激活：2.5–3 GB，具体取决于批处理大小和序列长度。

#### 为什么内存对 CPU 的重要性

在 CPU 上，你不仅关心“它能否适应 RAM？”你还关心缓存效率。现代 CPU 有多个级别的缓存（L1、L2、L3），它们的速度比主 RAM 快得多。如果激活或权重不适合缓存，即使技术上你有足够的 RAM，性能也可能受到影响。

内存占用也限制了实验的灵活性。例如，将序列长度从 64 增加到 1024 会将激活存储量乘以 16。在 `T=64` 时可以运行的程序可能在 `T=1024` 时崩溃或交换。

#### 示例场景

假设你使用 GPT-2 124M：

+   批处理大小 `B=4`

+   序列长度 `T=64`

这可能需要 ~2.5 GB 的内存进行训练。如果你将 `T` 提高到 512，激活量会突然膨胀，总使用量可能超过 10 GB。在 8 GB RAM 的笔记本电脑上，这根本无法工作。

通过仔细监控内存，你可以避免神秘的崩溃，并现实地规划运行。

#### 试试你自己

1.  使用小批量（`B=2`，`T=64`）进行训练，并使用 `htop` 检查内存使用情况。

1.  逐步增加 `T` 步（128，256，512）并记录增长。观察激活量如何超过一定长度后主导。

1.  手动计算参数内存：`num_params × 4 bytes`。将其与操作系统报告的值进行比较。差异来自激活量和优化器状态。

1.  修改代码，在创建数组时显式打印内存分配。这会在每个步骤提供使用情况的内部日志。

#### 吸取的经验教训

内存是 FLOPs 的无声伙伴：你需要两者来高效地训练和运行模型。没有跟踪内存的配置文件是不完整的——你可能有一个运行速度快但无法在你的机器上运行的模型。通过捕获和理解内存使用情况，你能够负责任地扩展，平衡批量大小和序列长度，并保持你的实验稳定。

### 57. 仅 CPU 的已知损失曲线再现

一旦模型正确并且其性能得到测量，下一步重要的步骤是检查它是否以我们期望的方式学习。在深度学习中，我们通常通过损失曲线来监控这一点——一个显示随着模型看到更多数据，训练损失随时间下降的图表。

对于 GPT-2 和其他语言模型，标准的损失函数是交叉熵，它衡量预测的标记概率分布与数据集中实际下一个标记匹配得有多好。如果实现正确，当在 Tiny Shakespeare 或 Tiny Stories 等文本上训练时，损失应该以可预测的方式下降。

#### 一个“已知”损失曲线看起来像什么

社区已经在 PyTorch 中运行了无数 GPT-2 实验，所以我们大致知道曲线应该是什么样子：

+   在一开始，损失很高（大约 5–6），因为模型基本上是在猜测。

+   经过几百步之后，损失开始稳步下降。

+   对于 Tiny Shakespeare，使用一个小型的 GPT-2 模型（124M 参数）时，损失通常下降到 ~2.0。

+   具体的数字可能会有所不同，但形状——一个向下趋势，波动很小——是一致的。

如果 C 实现产生类似的曲线，那么这是一个强烈的迹象，表明前向传递、反向传递和优化器都在正确工作。

#### 实际操作中如何测试再现性

1.  使用小数据集进行训练 测试通常使用 Tiny Shakespeare 或 Tiny Stories，因为它们足够小，可以在 CPU 上快速运行。

1.  记录每步的损失 每个训练步骤会打印出类似的内容：

    ```c
    step 0: train loss 4.87
    step 10: train loss 4.12
    step 20: train loss 3.75
    ...
    ```

1.  绘制曲线 保存损失值，并使用步骤作为 x 轴，损失作为 y 轴制作简单的图表。

1.  与 PyTorch 比较 如果你在 PyTorch 中使用相同的超参数训练相同的模型，损失曲线应该几乎相同。由于随机种子或浮点数数学的差异，小的差异是正常的。

#### 为什么这很重要

重现已知的损失曲线不仅仅是进行合理性检查。它告诉你：

+   数学是正确的：梯度、优化器更新和调度器逻辑正在正常工作。

+   数据管道是正确的：标记被正确地输入，批次是一致的。

+   没有什么是默默出错的：没有这个，一个错误可能直到训练的后期才被发现。

这在 CPU 上尤为重要，因为训练较慢，你可能只能运行几百步。如果曲线以预期的方式开始下降，你就知道你走在正确的道路上。

#### 示例场景

假设你使用`B=4`和`T=64`在 Tiny Shakespeare 上训练 GPT-2 124M。损失开始大约在 4.9，到第 200 步时下降到大约 3.2。如果 PyTorch 显示出类似的轨迹，那么你的实现就得到了验证。

但如果你的损失保持平坦，比如在几百步中保持在 5.0，那么这是一个红旗。这可能意味着梯度没有流动，优化器没有更新权重，或者你的数据加载器反复提供相同的批次。

#### 亲自尝试

1.  使用 C 代码在 Tiny Shakespeare 上训练 200 步，并保存打印出的损失。

1.  在 PyTorch 中用相同的设置进行训练。将两条曲线一起绘制并比较。

1.  故意破坏某些东西——例如，注释掉优化器更新步骤——并观察损失不再下降。

1.  尝试调整学习率。过高可能会导致曲线上下跳动，而过低会使它下降得很慢。

#### 吸取的经验

重现已知的损失曲线是最终的集成测试。它证明了整个管道——数据、模型、训练循环、优化器——协同工作。当你的损失曲线与参考匹配时，你可以相信你的 GPT-2 的 C 实现不仅在理论上正确，而且在实践中也是有效的。

### 58. 调试数值稳定性（NaNs，Infs）

即使模型大多数时候都能产生正确的输出，深度学习中仍存在一个潜在的危险：数值不稳定性。这种情况发生在计算中的浮点数膨胀到无穷大（`Inf`）或崩溃成“不是一个数字”（`NaN`）。当这种情况发生时，训练通常会停止——损失变得未定义，梯度爆炸，参数不再有意义地更新。

#### 为什么会出现 NaN 和 Inf

神经网络涉及许多乘法、指数和除法。在纸上，所有这些都是好的。但计算机使用有限精度（在这种情况下是 32 位浮点数）来存储数字。当值变得过大或过小时，它们就不再能被正确表示。

常见原因包括：

+   Softmax 溢出：在大的正数上计算`exp(x)`会导致`Inf`。

+   除以非常小的数字：例如，在 AdamW 中除以`sqrt(v_hat) + eps`可能会在`eps`太小的情况下产生不稳定性。

+   梯度爆炸：在反向传播过程中，错误会在多个层中累积，产生极大的值。

+   不当的初始化或学习率：过大的权重或过于激进的学习率步长可能会将激活推到不稳定范围内。

#### 如何检测 *llm.c* 中的不稳定性

由于代码是用 C 编写的，没有自动检查，NaNs 和 Infs 除非你寻找它们，否则会无声地传播。一些有用的策略包括：

1.  断言：在循环中插入 `assert(!isnan(value) && !isinf(value));` 以立即捕获坏值。

1.  调试打印：每步记录激活或梯度的样本值，以查看它们是否漂移到极大的数字。

1.  检查损失：如果损失突然变为 `nan` 或 `inf`，这表明上游可能出现了问题。

1.  小规模运行：在微小的序列和批次上进行测试，更容易直接检查值。

#### 如何修复不稳定性

几种实用技术有助于保持数值稳定：

+   添加一个 epsilon：在除法或平方根中，添加一个小的常数（如 `1e-8`）以防止除以零。

+   在 softmax 之前重新缩放：在计算指数之前从向量中减去最大值。这保持值在安全范围内。

+   梯度裁剪：限制梯度，使其不超过一定的范数。这可以阻止更新失控。

+   调整学习率：如果训练发散，降低学习率通常可以恢复稳定性。

+   检查数据：损坏的输入或意外的标记可能会将极端值注入模型。

#### 示例场景

假设你在 Tiny Shakespeare 上训练 GPT-2。最初的几步看起来正常：

```c
step 0: train loss 4.95
step 1: train loss 4.72
step 2: train loss nan
```

这种突然跳到 `nan` 的现象表明不稳定。检查梯度会发现在注意力权重中存在极端大的值。修复方法可能是将学习率从 `1e-4` 降低到 `5e-5` 或启用梯度裁剪。

#### 亲自尝试

1.  使用非常高的学习率 (`1e-2`) 进行训练，并观察 NaNs 出现的速度。

1.  在 `gpt2_forward` 中添加一个调试检查，当任何激活超过 `1e6` 时打印。运行几步并观察值是否爆炸。

1.  修改 softmax 代码以省略减去最大值。比较修改前后的稳定性。

1.  添加梯度裁剪例程并测量它是否可以防止损失发散。

#### 吸取的经验

数值稳定性是模型平稳训练与训练几步后崩溃之间的区别。通过预测 NaNs 和 Infs 可能出现的地方，添加检查，并应用稳定技巧，使 *llm.c* 变得健壮。这确保了实验的可靠性，并且调试重点放在真正的算法问题上，而不是可避免的数值陷阱。

### 59. 从单元测试到完整训练准备

单元测试是第一道防线：它们检查代码的小部分、孤立的部分——如嵌入、注意力和 softmax——是否产生正确的输出。但通过单元测试并不意味着为全面训练做好准备。从“这一层工作”到“整个系统在数千步中正确学习”的转变涉及额外的挑战。

#### 单元测试与训练之间的差距

+   单元测试检查正确性：例如，验证`gpt2_forward`在单个批次上产生的 logits 与 PyTorch 相同。

+   训练准备性检查鲁棒性：确保模型可以反复运行数千步而不会崩溃、发散或泄漏内存。

想象一下测试一辆车。单元测试就像单独检查刹车、车灯和转向。训练准备性就像开车进行 500 英里的长途旅行，确保没有任何部件过热、松动或在压力下失效。

#### 需要验证的训练准备性

1.  损失曲线行为 运行训练循环数百步。训练损失应该稳步下降，与 PyTorch 的已知曲线匹配。如果停滞或激增，则梯度或优化器中存在问题。

1.  验证运行 在训练期间定期测量验证损失。如果最初下降然后稳定，这表明模型正在泛化。如果下降得太快然后突然上升，这表明可能过拟合。

1.  内存稳定性 训练使用的内存比推理多，因为梯度优化器状态。内存泄漏——忘记释放数组或未释放就重新分配——会导致程序在许多步骤后崩溃。

1.  优化器状态更新 检查 AdamW 在多次迭代中正确累积`m`和`v`。如果缺少偏差校正，损失曲线将偏离预期的基线。

1.  可重现性 使用相同的随机种子，两次运行应产生几乎相同的损失曲线。小差异是正常的，但主要偏差表明存在非确定性错误。

#### 为什么这很重要

没有这一步，你可能会在单元测试后认为你的实现已经完成，但最终发现训练在 500 步时默默地失败了。训练准备性确保系统不仅在数学上在小部分正确，而且在长时间运行的实验中实际可用。

这也是代码部署的信心来源。通过训练准备性意味着其他人可以克隆仓库，运行脚本，并期望稳定训练而不会出现神秘的崩溃。

#### 示例场景

你运行`test_gpt2.c`，所有输出都在公差范围内与 PyTorch 匹配——太好了。然后你启动训练 5000 步。在第 600 步后，损失变为`nan`。调查发现`gpt2_update`没有正确应用偏差校正，所以优化器变得不稳定。这是一个你用单个批次的单元测试永远无法捕捉到的错误，但训练准备性揭示了它。

#### 试试你自己

1.  在 Tiny Shakespeare 上运行 1000 步的训练，并每 10 步记录一次损失。检查它是否平稳下降。

1.  每隔 100 步添加验证运行。注意训练损失（较低）和验证损失（略高）之间的经典差距。

1.  使用`htop`或类似工具监控训练期间的内存使用情况。确认它保持稳定，而不是缓慢上升。

1.  使用相同的种子运行相同的训练两次。比较两个损失曲线——它们是否几乎相同？

#### 吸取的经验教训

单元测试证明各个部分是正确的。训练准备性证明整个系统在真实条件下能够正常工作。这两者都是必要的。共同之处在于，它们让你有信心，*llm.c*不仅仅是一系列工作的部件，而是一个能够从头到尾训练 GPT-2 模型的完整引擎。

### 60. CPU 测试的局限性

在 CPU 上测试 GPT-2 对于验证正确性非常有价值，但它有明显的限制。了解这些限制有助于你正确地解释结果，并知道何时是时候转向基于 GPU 的实验。

#### 速度限制

最明显的限制是速度。CPU 针对通用任务进行了优化，而不是神经网络所要求的巨大并行性。在 GPT-2 124M 上单次正向和反向传递可能需要几秒钟甚至几分钟，而 GPU 可能只需毫秒就能处理。这使得：

+   全规模训练不切实际：在 CPU 上训练 GPT-2 124M 以达到收敛可能需要数周或数月。

+   实验周期较慢：测试新的优化或调试会变慢，因为每次运行都需要更长的时间。

因此，CPU 测试最适合进行小规模的健康检查，而不是完整的训练运行。

#### 内存开销

CPU 内存通常比 GPU VRAM 更丰富，但速度较慢。瓶颈往往不是“我们是否有足够的 RAM？”而是“我们能否快速将数据移动到内存中或从内存中移除？”随着序列长度`T`的增长，激活量激增，缓存效率下降。这使得即使是中等规模的运行也会变得缓慢。

#### 有限的现实感

虽然 CPU 运行确认了数学是正确的，但它们并不总是反映 GPU 执行的现实。例如：

+   CUDA 内核具有不同的数值特性（融合操作，不同的舍入）。

+   GPU 内存布局可能会暴露 CPU 数组隐藏的 bug。

+   并行执行可能会产生在 CPU 上从未出现的时间或同步问题。

因此，尽管与 PyTorch 的 CPU 兼容性是必要的，但并不足够。一旦引入 CUDA 代码，就必须重新进行测试。

#### 缺乏规模洞察力

CPU 测试可以证明几个批次的正确性，但它不会告诉你代码在重负载下的扩展性。在 GPU 上，你了解内核效率、内存吞吐量和分布式训练。CPU 测试根本不会暴露这些担忧。

#### 为什么这很重要

CPU 测试是基础：它证明了算法是正确实现的，一步一步地，不依赖于专用硬件。但如果你就止步于此，你将错过性能和可扩展性的更大图景。CPU 结果应被视为继续前进的绿灯，而不是准备就绪的最终结论。

#### 示例场景

假设你在 Tiny Shakespeare 上运行了 500 个训练步骤。损失曲线如预期般精确下降——成功。但 CPU 上的训练速度如此之慢，完成一个 epoch 需要几个小时。这验证了正确性，但很明显，进行有意义的实验需要 GPU。

#### 亲自试试

1.  在 CPU 上训练 GPT-2 124M 100 步，并记录每步的时间。外推运行 100k 步需要多长时间。

1.  将序列长度从 64 增加到 512，并观察内存访问时间如何影响吞吐量。

1.  将你的 CPU 损失曲线与 PyTorch 的 GPU 运行结果进行比较。注意它们的形状一致，但速度差异很大。

1.  使用分析工具（`perf`、`valgrind`或`gprof`）来查看哪些 CPU 函数主导了运行时间。

#### 吸取的经验

CPU 测试是安全实验室，你可以在这里验证正确性，捕捉数值错误，并重现已知的损失曲线。但其局限性——速度慢、现实感降低和缺乏可扩展性见解——意味着它只是一个第一步。一旦 CPU 测试通过，旅程将继续进行 GPU 测试、分析和多设备扩展。

## 第七章. CUDA 训练（`train_gpt2.cu`）

### 61. CUDA 架构概述（流、内核）

当*llm.c*的 CPU 版本运行时，它会在你的处理器核心上依次执行指令。这对于小型模型或调试来说是可行的，但对于深度学习工作负载——特别是像 GPT-2 这样的 Transformer——需要大量的浮点运算。为了处理这一点，*llm.c*还包括了 CUDA 版本的训练循环，将计算转移到 NVIDIA GPU 上。

从高层次来看，CUDA 是 NVIDIA 的编程模型，允许开发者编写代码直接在 GPU 上运行。与可能只有几个针对通用任务优化的核心的 CPU 不同，GPU 包含成千上万个设计用于并行处理大量数据的简单核心。CUDA 提供了组织工作的工具，以便这些核心保持忙碌。

#### 内核：在 GPU 上运行的程序

在 CUDA 中，*内核*是运行在 GPU 上的函数。当你启动一个内核时，你不会像调用普通 C 函数那样只调用一次——你同时启动成千上万的副本。每个副本处理数据的不同部分。例如，如果你想乘以两个包含一百万个元素的向量，你可以启动一百万个 GPU 线程，每个线程乘以一对数字。

在*llm.c*中，内核用于可以表示为大量小型、独立任务的运算。例如包括：

+   将 GeLU 激活函数逐元素应用于大张量。

+   在每个维度上添加残差连接。

+   在 LayerNorm 层中归一化值。

对于像矩阵乘法（GEMMs）这样的大规模结构化操作，CUDA 代码通常依赖于如 cuBLAS 或 cuBLASLt 这样的专用库，这些库针对 NVIDIA GPU 进行了高度优化。

#### 流：重叠工作

GPU 能够同时处理多个任务。CUDA 引入了*流*的概念，这些流是按照相对顺序运行的操作的序列，但可以与其他流中的操作重叠。这意味着：

+   当一个内核正在执行时，另一个内核可以开始将数据在 CPU 和 GPU 之间传输。

+   计算和通信可以重叠，减少空闲时间。

在*llm.c*的训练循环中，流允许你安排工作批次，以便数据准备和模型计算可以并行进行。这对于保持 GPU 充满有用的工作而不是等待 CPU 至关重要。

#### 内存层次结构

CUDA 编程也受到 GPU 内存层次结构的影响：

+   寄存器：最快的，每个线程私有的。

+   共享内存：在块中的线程之间共享的小块内存；比全局内存快得多。

+   全局内存：大，但较慢。这是张量如权重、激活和梯度通常所在的地方。

+   主内存（CPU RAM）：与 GPU 内存分开；在它们之间传输可能会很慢，应该最小化。

例如，在注意力内核中，在处理序列的块之前，部分结果可能存储在寄存器或共享内存中，然后再将最终结果写回全局内存。

#### 这如何适用于*llm.c*

在`train_gpt2.cu`中，大部分繁重的工作是通过 cuBLAS/cuBLASLt 和 cuDNN 对矩阵乘法和注意力进行调用完成的。但了解 CUDA 模型——内核、流和内存——有助于解释：

+   为什么我们以这种方式进行批量操作。

+   为什么最小化 CPU 和 GPU 之间的数据传输如此重要。

+   GPU 内核如何自然地映射到 GPT-2 所需的张量操作类型。

#### 为什么这很重要

没有 CUDA，训练 GPT-2 将会非常缓慢，即使在功能强大的 CPU 上也是如此。CUDA 提供了访问数千个并行工作的核心，但它也要求进行仔细的编程以避免瓶颈。了解内核、流和内存层次结构是理解后面章节的基础，在这些章节中我们将深入探讨矩阵乘法、注意力和优化策略。

#### 尝试自己动手做

1.  编写一个简单的 CUDA 内核，用于逐元素相加两个数组。将其性能与 CPU 循环进行比较。

1.  修改内核以使用共享内存，并查看它是否提高了大型数组的性能。

1.  创建两个 CUDA 流：一个用于计算内核，另一个用于复制数据。测量操作是否在时间上重叠。

1.  使用`nvprof`或`nsys`来分析 CUDA 程序，并观察内核和内存传输在时间线上的表现。

1.  考虑你将如何将一个大矩阵乘法分配给数千个线程——每个线程计算一行、一列或一个元素？有哪些权衡？

#### 吸收要点

CUDA 不仅仅是为 GPU 编写代码——它关于重新思考计算，将其视为成千上万的可以并行运行的小任务。内核处理每个线程的工作，流允许你调度和重叠操作，而内存层次结构决定了如何组织数据以实现最大速度。所有这些想法都在 *llm.c* 的 CUDA 实现中汇聚，使得 GPT-2 等模型的可训练性成为可能。

### 62. 通过 cuBLAS/cuBLASLt 进行矩阵乘法

矩阵乘法——通常称为 GEMM（通用矩阵-矩阵乘法）——是深度学习的核心。在 GPT-2 中，大部分计算都来自于乘以大矩阵：将嵌入投影到查询、键和值向量中，应用注意力权重，以及处理 MLP 前馈层。在 CPU 上，我们看到了使用嵌套循环和轻微优化的实现。然而，在 GPU 上，我们需要更高效的途径。这就是 cuBLAS 和 cuBLASLt 发挥作用的地方。

#### 为什么矩阵乘法如此核心

几乎每个 Transformer 的步骤都涉及到乘以两个大矩阵：

+   嵌入查找可以看作是 one-hot 令牌向量和嵌入表之间的矩阵乘法。

+   注意力机制计算查询和键之间的点积，然后是一个加权求和的值。

+   MLP 应用两个全连接层，每个层本质上都是一个 GEMM。

如果你分析 GPT-2，你会发现 GEMM 操作主导了运行时间。这就是为什么 NVIDIA 的库投入了巨大的努力来尽可能快地实现这些乘法。

#### cuBLAS：经典的工作马

cuBLAS 是 NVIDIA 的 GPU 加速版本的 BLAS（基本线性代数子程序）库。它提供了 GEMM 和相关例程的高度优化实现。在底层，cuBLAS：

+   将大矩阵分割成适合 GPU 共享内存的瓦片。

+   调度成千上万的线程以并行计算不同的瓦片。

+   使用融合乘加（FMA）指令以实现高吞吐量。

+   适应不同的 GPU 架构，以利用可用的 Tensor Cores。

一个典型的调用看起来像这样：

```c
[](#cb143-1)cublasHandle_t handle;
[](#cb143-2)cublasCreate(&handle);
[](#cb143-3)
[](#cb143-4)float alpha = 1.0f, beta = 0.0f;
[](#cb143-5)cublasSgemm(handle,
[](#cb143-6)    CUBLAS_OP_N, CUBLAS_OP_N,
[](#cb143-7)    m, n, k,
[](#cb143-8)    &alpha,
[](#cb143-9)    A, m,
[](#cb143-10)    B, k,
[](#cb143-11)    &beta,
[](#cb143-12)    C, m);
```

这里 `A` 和 `B` 是输入矩阵，`C` 是结果。该函数处理所有低级调度。

#### cuBLASLt：灵活的后继者

虽然 cuBLAS 功能强大，但有些僵化。cuBLASLt（轻量级 cuBLAS）是一个较新的 API，它增加了：

+   更好的混合精度支持（例如，FP16 或 BF16 输入与 FP32 累加）。

+   更多的算法选择控制，以便开发者可以针对性能或内存使用进行调优。

+   像尾随操作这样的功能，允许你将额外的操作（例如，偏置添加、激活函数）直接融合到 GEMM 中，减少内存传输。

实际上，cuBLASLt 通常比 cuBLAS 表现更好，因为它可以更积极地利用 Tensor Cores，并将多个步骤融合到一个内核调用中。

#### 精度和 Tensor Cores

在现代 NVIDIA GPU（Volta、Turing、Ampere、Hopper）上，使用 FP16、BF16 或 TF32 时，Tensor Cores 可以显著加速矩阵乘法。这些特殊硬件单元可以在单个指令中执行小数字块的矩阵乘法和累加。

例如：

+   在 CPU 上，两个 16×16 矩阵的乘法是通过许多标量乘法完成的。

+   在具有 Tensor Cores 的 GPU 上，整个块可以在一个融合操作中计算。

在 GPT-2 训练中，使用 cuBLASLt 的 FP16 可以显著提高吞吐量，同时保持主权重在 FP32 以保持数值稳定性。

#### *llm.c*中的实际示例

在`train_gpt2.cu`中，大多数执行线性层的调用——例如将输入激活投影到查询、键和值矩阵中——都是使用 cuBLAS/cuBLASLt 实现的。例如：

+   输入`(B, T, C)`与权重矩阵`(C, 3C)`相乘，以产生`(B, T, 3C)`。

+   之后，注意力输出`(B, T, C)`被乘以另一个投影矩阵`(C, C)`。

而不是为每个情况编写自定义核，代码将委托给 cuBLAS/cuBLASLt，确保在所有 GPU 架构上实现最大性能。

#### 为什么这很重要

矩阵乘法非常频繁且计算量大，其性能直接决定了你训练 GPT-2 的速度。通过依赖 cuBLAS/cuBLASLt，*llm.c*避免了重新发明轮子，并接近 GPU 效率的峰值。这使得代码更加简洁、易于维护，并且可以扩展到更大的模型。

#### 尝试自己动手

1.  编写一个小型 CUDA 程序，使用原始内核乘以两个矩阵，并将其性能与 cuBLAS 进行比较。

1.  尝试 FP32 与 FP16 输入的对比，并观察启用 Tensor Cores 时的加速效果。

1.  启用 cuBLASLt 的 epilogues 将偏置添加融合到 GEMM 中，并测量内存节省。

1.  使用`nvprof`或`nsys`分析 GPT-2 的训练，以查看在 GEMM 调用中花费了多少时间。

1.  尝试扩大矩阵大小以模拟更大的模型，并注意性能相对于 CPU 实现的增长。

#### 吸收要点

矩阵乘法是 GPT-2 的计算引擎，在 GPU 上由 cuBLAS 和 cuBLASLt 提供动力。这些库利用 GPU 的架构——分块、Tensor Cores、混合精度——以榨取最大效率。了解它们的工作原理可以揭示为什么*llm.c*的 GPU 版本比 CPU 版本运行得快得多，并为注意力内核和其他 CUDA 加速组件奠定了基础。

### 63. 注意力内核：cuDNN FlashAttention

注意力机制是每个 Transformer 的核心。它允许模型在生成输出时权衡输入序列的不同部分。对于 GPT-2 来说，这意味着在生成下一个标记时，模型不仅查看最后一个单词——它还考虑了它之前的整个单词序列，调整每个过去标记的贡献程度。但注意力是昂贵的。直观地，它与序列长度成平方关系：对于 1024 个标记的序列，你需要计算一个 1024×1024 的注意力矩阵。这超过了一百万个条目，每个都必须计算、归一化并乘回值向量。

在 CPU 上，我们看到了它是如何一步一步实现的：查询、键和值通过矩阵乘法进行投影，计算查询和键之间的点积，应用 softmax，然后将结果乘以值。在 GPU 上，我们想要做同样的事情，但更快。这就是 cuDNN 的 FlashAttention 发挥作用的地方。

#### 什么是 FlashAttention？

FlashAttention 是一种重新思考注意力计算方式的算法。它不是在内存中实现完整的注意力矩阵，而是以流式的方式计算 softmax 和加权求和。这减少了内存使用并提高了缓存效率。

通常，注意力涉及以下步骤：

1.  计算得分 = Q × Kᵀ（查询乘以键的转置）。

1.  对得分应用 softmax 以获得注意力权重。

1.  将权重乘以 V（值）以获得输出。

问题：步骤 1 生成一个大小为 `(sequence_length × sequence_length)` 的巨大得分矩阵。存储和处理这个完整矩阵成为瓶颈。

FlashAttention 通过分块计算注意力来避免存储完整矩阵。它处理查询和键的块，逐步应用 softmax，并将结果直接累加到输出中。这极大地减少了内存带宽需求，这对于 GPU 来说是至关重要的。

#### cuDNN FlashAttention 实践

在 *llm.c* 的 CUDA 训练代码中，当 `USE_CUDNN` 被启用时，代码可以利用 cuDNN 对 FlashAttention 的实现。这意味着：

+   该库自动处理分块和流式处理。

+   它可以利用 Tensor Cores 进行混合精度计算（FP16/BF16 输入与 FP32 累加）。

+   它减少了内存使用，这使得在不会耗尽 GPU 内存的情况下训练更长的序列成为可能。

从开发者的角度来看，启用 cuDNN FlashAttention 通常涉及向 cuDNN 例程传递特定的描述符和标志，而不是编写自定义内核。你不需要手动管理循环和 softmax 稳定性技巧，而是将责任交给 cuDNN，它有一个高度优化的内核。

#### 这为什么是一个变革者

注意力的二次成本一直是扩展 Transformer 的瓶颈。随着 FlashAttention 的出现，瓶颈转移了。计算仍然是 O(N²)，但由于内存处理得更加高效，GPU 在内存加载上的等待时间减少，有更多时间进行实际计算。这意味着：

+   即使在相同的序列长度下，训练速度也可以更快。

+   你可以将序列长度推得更大（例如，2K 或 4K 标记），而不会耗尽 GPU 内存。

+   由于避免了全局内存的冗余读写，能效得到了提升。

#### 示例：为什么内存访问很重要

让我们想象一个有 4 个标记的玩具示例。一个原生实现可能会构建一个 4×4 的注意力矩阵，计算 softmax，然后乘以值。对于 4 个标记来说这没问题，但如果有 1024 个标记，你将需要处理百万条记录的矩阵。即使每个条目只是 2 字节（FP16），每一步都会有兆字节的临时存储。在真实的 GPU 上，不断将数据在全局内存中移动会减慢一切。

FlashAttention 表示：与其存储全部一百万条记录，不如分块计算，即时归一化，并立即用于更新输出。这样，只有小块临时数据块存在于内存中，全局内存压力显著降低。

#### 它如何在 GPT-2 训练中体现

当 GPT-2 处理一批序列时，模型的每个块都会应用注意力。在 *llm.c* 的 CUDA 版本中，这些注意力调用可以通过 cuDNN FlashAttention 进行路由。实际上，这意味着训练的内部循环——原本会在那些巨大的注意力矩阵上缓慢进行的部分——变得更加精简和快速。

当模型变大时，这一点尤为重要。对于 GPT-2 124M（12 层，12 头，1024 序列长度），注意力已经非常昂贵。对于 GPT-2 1.5B 或具有更长上下文的 LLaMA-style 模型，FlashAttention 可能是可行训练和“内存不足”错误之间的区别。

#### 它为什么重要

注意力是 Transformer 的定义性操作，但也是它们的阿喀琉斯之踵。FlashAttention 解决了最大的低效问题——内存带宽——而不改变模型的输出。通过使用 cuDNN 的优化内核，*llm.c* 确保它在接近硬件峰值性能的同时，仍然产生正确的结果。对于任何学习深度学习系统的人来说，这是一个算法创新（流式 softmax）和硬件级优化（Tensor Cores，tiling）如何结合以实现最先进训练的完美示例。

#### 尝试自己动手

1.  在 *llm.c* 中使用 `USE_CUDNN=0` 运行 GPT-2 训练，然后使用 `USE_CUDNN=1`。比较训练速度和 GPU 内存使用情况。

1.  编写一个简单的 CUDA 内核来构建完整的注意力矩阵，然后将其与 cuDNN FlashAttention 进行基准测试。

1.  改变序列长度（128、512、1024、2048），看看原生实现和 FlashAttention 实现之间的性能差异。

1.  检查混合精度如何与 FlashAttention 交互——尝试 FP16 与 BF16。

1.  探索 FlashAttention 论文，并将其算法解释与你在使用*llm.c*实践中看到的内容进行比较。

#### 吸取的教训

注意力成本高昂，但不必造成致命打击。FlashAttention 表明，巧妙的算法设计加上对硬件的感知实现可以显著减少内存瓶颈。通过依赖 cuDNN 的实现，*llm.c*可以更有效地训练 GPT-2 模型，学习者可以真正了解深度学习库如何从 GPU 中挤出性能。

### 64. 混合精度：FP16/BF16 与主 FP32 权重

训练像 GPT-2 这样的大型模型涉及在每一步训练中乘以和添加大量的数字——每个训练步骤有数十亿个操作。GPU 可以非常快速地完成这些操作，但你所使用的数字类型非常重要。传统上，训练是在 32 位浮点数（FP32）下进行的，这提供了良好的精度，但内存和计算成本很高。现代 GPU 提供了特殊的硬件——Tensor Cores，当使用降低精度（如 FP16（半精度浮点数）或 BF16（bfloat16））时，运行速度更快。这种技术称为混合精度训练。

#### 混合精度为什么有帮助

使用 FP16 或 BF16 有两个主要好处：

1.  速度：GPU 每时钟周期可以执行比 FP32 更多的 FP16/BF16 操作。例如，NVIDIA Tensor Cores 专门设计用于加速半精度数学，通常提供 2 倍或更高的吞吐量。

1.  内存：FP16/BF16 值占 FP32 存储空间的一半。这意味着你可以将更大的批次或更长的序列放入相同的 GPU 内存中，这对于模型扩展至关重要。

但降低精度伴随着权衡：数字更容易下溢（变为零）或上溢（变为无穷大），这可能会破坏训练稳定性。

#### FP32 主权重

在*llm.c*（以及 PyTorch 和 TensorFlow）中使用的技巧是保留 FP32 权重的原始副本。以下是这个过程：

1.  在前向传递过程中，权重被转换为 FP16/BF16，以便 GPU 可以在 Tensor Cores 上运行数学运算。

1.  梯度也在降低精度下计算。

1.  当需要更新参数时，优化器将更新应用于 FP32 主副本。

1.  更新后的主权重被转换回 FP16/BF16，以便进行下一次前向传递。

这样，你可以在不完全失去 FP32 稳定性的情况下获得混合精度的速度和内存节省。

#### FP16 与 BF16 的比较

FP16 和 BF16 都使用 16 位，但它们分割位的方式不同：

| 格式 | 指数位 | 尾数位 | 范围 | 精度 |
| --- | --- | --- | --- | --- |
| FP16 | 5 | 10 | 更小 | 小数的高精度 |
| BF16 | 8 | 7 | 更宽 | 粗糙精度，更好的范围 |

+   FP16 在零附近的精度更好，但范围较窄，因此更容易溢出。

+   BF16 与 FP32 具有相同的指数大小，因此它具有更宽的范围但精度较低。

现代 NVIDIA GPU（Ampere、Hopper）都支持这两种格式，但 BF16 通常因为在大模型中的稳定性而更受欢迎。

#### 实践中的例子

想象一下用序列长度为 1024 和批大小为 32 来训练 GPT-2。使用 FP32，激活可能需要大约 12 GB 的 GPU 内存。切换到 FP16 将其减半到大约 6 GB，为更大的模型或更多的序列留出空间。

在*llm.c*中，启用混合精度意味着前向传递可能看起来像这样：

1.  将嵌入、权重和激活转换为 FP16/BF16。

1.  在 Tensor Cores（非常快）上运行矩阵乘法。

1.  在降低精度中计算梯度。

1.  将梯度转换回 FP32 以进行稳定的更新。

这种流程对高级代码不可见，但在 CUDA/cuBLAS/cuDNN 调用中内部处理。

#### 常见挑战

混合精度引入了新的问题：

+   损失缩放：在 FP16 中，小的梯度可能下溢为零。解决方案是在反向传播期间将损失乘以一个大的因子，然后稍后除以梯度。这保留了信息。

+   调试：当切换到 FP16 时，NaNs 和 Infs 变得更加常见。需要仔细监控以捕捉这些早期情况。

+   性能调整：并非所有操作都能从 FP16 中获得同等的好处。例如，减少（如求和大型数组）可能会丢失太多精度，除非在 FP32 中执行。

#### 为什么它很重要

混合精度是现代 Transformer 能够在今天的硬件上高效训练的关键原因之一。没有它，许多模型将需要双倍的 GPU 内存和更多的时间来训练。通过结合 FP16/BF16 以实现速度和内存效率，以及 FP32 主权重以实现稳定性，*llm.c*反映了生产框架中使用的策略。这展示了即使是极简的代码库也能传授那些推动现实世界大规模训练的尖端技巧。

#### 尝试自己操作

1.  在*llm.c*中使用 FP32 仅训练 GPT-2，然后使用 FP16 重复训练。比较每步的内存使用情况（`nvidia-smi`）和运行时间。

1.  如果你的 GPU 同时支持 FP16 和 BF16，请尝试比较它们。观察哪一个更稳定。

1.  故意移除 FP32 主权重（仅在 FP16 中更新参数）并观察训练如何快速发散。

1.  使用 FP32、FP16 和 BF16 运行绘制验证损失曲线，以查看模型质量是否有所不同。

1.  尝试使用 FP16 增加批大小，并注意你可以在同一 GPU 中放入多大的模型。

#### 吸收要点

混合精度结合了两种技术的优点：FP16/BF16 的速度和内存效率以及 FP32 的稳定性。这项技术已成为深度学习中的标准，*llm.c*以清晰、易于理解的方式展示了它。这不仅是一种整洁的优化——这是使在现代 GPU 上训练大型语言模型成为可能的关键。

### 65. 混合精度训练中的损失缩放

在混合精度（FP16 或 BF16）训练时，最大的挑战之一是数值下溢。梯度可能变得非常小，以至于在 16 位格式中表示时四舍五入为零。如果这种情况发生得太频繁，优化器将停止接收有意义的更新，训练可能会停滞或崩溃。为了解决这个问题，框架引入了一种称为损失缩放的技术。

#### 核心思想

损失缩放通过在开始反向传播之前将损失值乘以一个常数因子（称为缩放因子）来实现。由于梯度与损失成正比，这也将所有梯度乘以相同的因子，使它们更大，在存储在 FP16 中时不太可能发生下溢。

在反向传播结束时，将梯度除以相同的缩放因子，恢复它们在优化器步骤之前的正确值。

从数学上讲：

1.  `scaled_loss = loss × scale`

1.  计算`scaled_loss`的梯度→产生`scaled_gradients`

1.  `true_gradients = scaled_gradients ÷ scale`

优化器随后使用`true_gradients`来更新权重。

#### 静态与动态缩放

有两种常见的方法：

+   静态缩放：在整个训练过程中使用固定的缩放因子。例如，始终将损失乘以`1024`。这很简单，但风险很大；如果缩放因子过高，梯度可能会溢出到无穷大。如果过低，仍然会发生下溢。

+   动态缩放：动态调整缩放因子。如果检测到溢出（NaNs 或 Infs），则减少缩放因子。如果训练顺利进行，则逐渐增加缩放因子。这平衡了稳定性和效率。

实际上，动态缩放是标准。像 PyTorch 的`GradScaler`这样的库会自动处理这种逻辑，因此用户不需要手动调整值。

#### 它在*llm.c*中的表现

*llm.c*的最小设计尚未包括自动损失缩放，但这个想法非常适合其训练循环。在调用`gpt2_backward`之前，你会缩放损失。在计算梯度之后，你会在`gpt2_update`之前取消缩放。从概念上讲：

```c
[](#cb144-1)float scale = 1024.0f; // example scale factor
[](#cb144-2)float scaled_loss = model.mean_loss * scale;
[](#cb144-3)gpt2_backward_scaled(&model, scaled_loss); // backprop with scaled loss
[](#cb144-4)unscale_gradients(&model, scale); // divide gradients by scale
[](#cb144-5)gpt2_update(&model, lr, beta1, beta2, eps, weight_decay, step);
```

这还不是存储库中的内容，但这是如何扩展*llm.c*以支持稳定的 FP16 训练的方法。

#### 为什么这很重要

没有损失缩放，混合精度可能会无声失败。训练可能看起来在运行，但许多参数的梯度可能实际上为零。这浪费了 GPU 时间，并产生了较差的结果。有了损失缩放，FP16/BF16 训练既快又可靠，结合了硬件加速和数值稳定性。

#### 示例场景

假设你正在使用 FP16 训练 GPT-2，并注意到在几百步之后验证损失几乎没有减少。可能的原因之一是梯度下溢。通过启用缩放因子为 512 或 1024 的损失缩放，你可能会突然看到损失曲线恢复正常，与 FP32 基线相匹配。

#### 亲自尝试

1.  使用 FP16 进行训练但不进行损失缩放。监控损失是否显著减少。

1.  添加一个静态缩放因子（如 512）并重新运行。观察稳定性方面的改进。

1.  实现一个简单的动态缩放器：从 128 开始，如果 100 步内没有出现 NaNs，则将其加倍，如果检测到 NaNs，则将其减半。

1.  比较训练曲线（FP32 与带缩放和不带缩放的 FP16）以查看效果。

1.  尝试使用非常大的缩放因子来有意触发溢出，然后观察动态缩放如何恢复。

#### 吸取的经验

损失缩放是使混合精度训练成为可能的关键成分。通过重新缩放损失和梯度，我们保护了在 FP16 下消失的小数字，同时仍然享受巨大的性能和内存优势。即使在像 *llm.c* 这样的最小代码库中，理解损失缩放也能弥合训练效果不佳的模型与在成本减半的情况下匹配 FP32 性能的模型之间的差距。

### 66. 激活检查点和内存权衡

训练像 GPT-2 这样的深度网络需要存储大量的激活——在正向传递的每一层产生的中间输出。这些激活在反向传递的后期需要用来计算梯度。问题是它们会占用大量的 GPU 内存。对于一个具有长序列和大型批次的 12 层 GPT-2，激活可能比模型权重本身消耗更多的内存。

#### 为什么激活很重要

假设你有一个批大小为 8，序列长度为 1024，隐藏大小为 768，和 12 层。每一层产生一个形状为 `(batch_size, sequence_length, hidden_size)` 的激活张量，即 `8 × 1024 × 768`。这大约是每层 630 万个数字。乘以 12 层，你就有大约 7500 万个数字。在 FP16 下，这大约是每次正向传递仅存储激活的 150 MB，并且随着模型规模的增大而增长。

如果你扩展到 GPT-2 Medium 或 GPT-2 XL，这个数字会迅速膨胀到几 GB，这可能不适合 GPU 内存。

#### 检查点（Checkpointing）的概念

激活检查点提供了一种权衡：不是存储所有激活，你只保留一小部分（检查点），并在反向传递期间重新计算其余部分。

1.  在前向传递期间：仅保存检查点（例如，每个转换器块末尾的激活）。

1.  在反向传递期间：当需要层的梯度时，通过再次运行部分前向传递来重新计算缺失的激活。

这以额外的计算为代价节省了内存。

#### 它在 GPT-2 中的工作方式

GPT-2 块有多个步骤：嵌入查找、注意力、MLP、层归一化、残差连接。通常，你会存储每个输出的张量。使用检查点（checkpointing）时，你可能只存储每个块的输入并丢弃中间结果。当反向传播到达该块时，你会在本地重新运行前向传递以重新生成这些结果，然后计算梯度。

这几乎以丢弃的激活数量的线性方式减少内存使用，但代价是大约 30-40% 的额外计算。

#### 在 *llm.c* 上下文中

*llm.c* 的最小实现中尚未包括激活检查点，但它是一个自然的扩展。在 CUDA 中，这可以通过使用一个决定是否保存或丢弃激活的“检查点”函数来包装代码块来实现。在 PyTorch 中，等效的是 `torch.utils.checkpoint`。

如果你使用更长的序列（例如，2048 个标记而不是 1024 个），检查点可能意味着在内存中适应或遇到内存不足（OOM）错误的区别。

#### 为什么这很重要

现代 GPU 具有巨大的计算能力，但内存仍然是瓶颈。检查点改变了权衡：你花费更多的计算（重新运行一些前向传递）以换取释放数 GB 的内存。这让你：

+   在相同的硬件上训练更大的模型。

+   使用更长的序列长度以获得更好的上下文处理。

+   增加批量大小以获得更稳定的梯度。

在实践中，这种技术几乎被用于今天几乎所有的规模化 Transformer 训练运行中。

#### 举例类比

想象一下为考试做准备。你可以在教科书每一页上做详细的笔记（存储所有激活），但你的笔记本会变得很大。或者，你只需标记章节标题（检查点），并在复习时需要时重新阅读章节（重新计算）。这需要更多时间，但可以节省笔记本空间。

#### 尝试自己操作

1.  在 GPU 上运行带有长序列的训练，直到遇到内存不足错误。

1.  实现一个简单的检查点方案，其中你只存储每隔一层激活。

1.  使用 `nvidia-smi` 测量内存使用量减少了多少，以及每步运行时间增加了多少。

1.  尝试不同的检查点频率——每层、每 2 层、每 4 层——并找到内存节省和计算开销之间的平衡。

1.  将验证损失曲线进行比较，以确认检查点不会影响训练质量（只有运行时间）。

#### 吸收教训

激活检查点是弯曲 GPU 内存限制的一种巧妙策略。通过按需丢弃和重新计算激活，你可以适应模型或序列长度，否则将是不可能的。权衡是额外的计算，但以今天的硬件而言，计算通常比内存便宜。这项技术是扩展 Transformer 到数十亿参数的幕后推动者之一。

### 67. GPU 内存规划：参数、梯度、状态

当在 GPU 上训练 GPT-2 模型时，最重要的实际挑战之一是管理内存。每个张量——模型的参数、梯度、优化器状态和激活——都必须适合有限的 GPU RAM。与 CPU 不同，你通常可以依赖交换空间或大型的 RAM 池，GPU 内存紧张且不容忍。如果你超出限制，程序会因内存不足错误而崩溃。

#### 分解占用内存的部分

1.  参数（权重）这些是模型的可训练值：嵌入、注意力投影、MLP 权重等。对于 GPT-2 124M，大约有 124 百万参数。

    +   在 FP32 中，这大约是 500 MB。

    +   在 FP16/BF16 中，这大约是 250 MB。更大的 GPT-2 模型（355M、774M、1.5B）按比例增加。

1.  梯度对于每个参数，反向传播会产生一个同样大小的梯度张量。如果参数占用 500 MB，梯度在 FP32 下也占用约 500 MB。混合精度可以将其减半。

1.  优化器状态像 AdamW 这样的优化器不仅存储梯度，还跟踪移动平均值（`m`和`v`）。每个都添加了一个全尺寸张量。使用 AdamW，你通常会有参数大小的 3 倍：权重 + m + v。

1.  激活在正向传播过程中，每个层的中间输出必须存储以供反向传播使用。这通常是内存消耗最大的单个组件。对于一个 12 层的 GPT-2，序列长度为 1024，批次大小为 8，激活可以轻松超过几个 GB。检查点（如前所述）有助于减少这一点。

#### 简单计算示例

对于 FP32 下的 GPT-2 124M：

+   参数：约 500 MB

+   梯度：约 500 MB

+   AdamW 状态：约 1 GB（两份副本）

+   激活：2–4 GB（取决于批次大小和序列长度）

总计：约 3–6 GB，这适合大多数现代 GPU。

对于 FP32 下的 GPT-2 774M：

+   参数：约 3 GB

+   梯度：约 3 GB

+   AdamW 状态：约 6 GB

+   激活：8–12 GB

总计：约 20+ GB——除非使用 FP16 和检查点等技巧，否则对于许多 GPU 来说太大。

#### 内存效率策略

1.  混合精度使用 FP16/BF16 可以将参数、梯度和优化器的大小减半。而不是 20 GB，你可能只需要约 10 GB。

1.  激活检查点存储较少的激活并在反向传播期间重新计算它们。这通常可以节省多个 GB。

1.  梯度累积而不是一次性用一个大批次进行训练，将其分成更小的微批次，并在它们之间累积梯度。这减少了激活内存需求。

1.  参数分片（高级）在多 GPU 设置中，参数和优化器状态可以跨设备分割（例如，DeepSpeed 中的 ZeRO 优化器）。虽然*llm.c*中没有实现，但这是一种常见的规模化技术。

#### 在*llm.c*中

`GPT2`结构将内存组织成`params_memory`、`grads_memory`、`m_memory`和`v_memory`等字段。这些作为平面数组分配，使得计算它们的大小变得容易。这种最小化设计突显了现实：对于每个参数，至少有一个匹配的梯度，可能还有两个优化器状态值。

这种结构反映了像 PyTorch 这样的完整框架如何分配内存，但*llm.c*透明地暴露了它，这样你可以确切地看到占用空间的是什么。

#### 为什么这很重要

在训练模型时，内存通常是首先遇到的限制，而不是计算。即使你的 GPU 足够强大以处理数学运算，如果你内存不足，你无法继续。了解参数、梯度、优化器状态和激活之间的交互可以帮助你设计出实际可行的训练运行。

#### 亲自尝试

1.  使用 FP32 与 FP16 计算 GPT-2 124M、355M 和 774M 的内存使用量。将你的数字与你的 GPU 内存大小进行比较。

1.  以递增的批次大小运行*llm.c*，直到遇到内存不足错误。记录它崩溃的确切点。

1.  启用混合精度和检查点，看看你可以将序列长度或批量大小推进多远。

1.  编写一个脚本来打印*llm.c*中`params_memory`、`grads_memory`和优化器状态的大小。将其与训练期间的`nvidia-smi`输出进行比较。

1.  尝试减少优化器状态（例如，尝试使用 SGD 而不是 AdamW）以查看内存差异。

#### 吸收要点

训练 Transformer 不仅仅是编写正向和反向传递——它还涉及到精心规划内存。参数、梯度、优化器状态和激活都会竞争有限的 GPU RAM。通过理解这些类别并使用混合精度和检查点等技术，你可以在相同的硬件上适应更大的模型或更长的上下文。这种内存和计算之间的平衡是现代深度学习扩展的核心。

### 68. 内核启动配置和占用率

当为训练 GPT-2 等模型编写 CUDA 代码时，性能中最重要的微妙因素之一是内核的启动方式。内核只是在 GPU 上运行的一个函数，但你配置它的方式——线程数、块数以及工作如何划分——可以决定 GPU 在 10%效率运行和接近峰值利用率之间的差异。

#### 线程、块和网格

CUDA 以分层方式组织计算：

+   线程：执行的最小单元。每个线程运行一次内核代码。

+   块：一组共享快速片上内存（称为共享内存）的线程集合。

+   网格：一组块。一起，网格代表整个内核启动。

当你启动内核时，你决定：

```c
[](#cb145-1)my_kernel<<<num_blocks, threads_per_block>>>(...);
```

例如：

```c
[](#cb146-1)int threads = 256;
[](#cb146-2)int blocks = (N + threads - 1) / threads;
[](#cb146-3)my_kernel<<<blocks, threads>>>(...);
```

在这里，`N`可能是要处理的元素总数。这个除法确保所有元素都被覆盖。

#### 什么是占用率？

占用率指的是 GPU 上相对于最大可能的线程数量。更高的占用率通常意味着 GPU 得到了更好的利用，但这不是唯一因素——内存访问模式和指令吞吐量也很重要。

每个 GPU 都有固定数量的流多处理器（SM），每个 SM 一次只能支持一定数量的线程和块。如果你的内核启动没有提供足够的线程，GPU 将会被低效利用。如果你启动太多，它们可能会竞争共享内存和寄存器，导致效率低下。

#### 示例：一个简单的向量加法

假设你想要添加两个大小为`N = 1e6`的数组。

+   如果你使用`threads_per_block = 32`，你将会有很多微小的块。这浪费了并行性，因为现代 GPU 被设计为每个 SM 运行数百个线程。

+   如果你使用`threads_per_block = 1024`，你可能会遇到硬件限制，但运行非常大的块，这会限制调度灵活性。

+   一个好的平衡可能是每个块 256 或 512 个线程，这样可以让 GPU 有效地重叠计算和内存访问。

#### 在 Transformer 训练中

在*llm.c*中，GPT-2 的大部分重负载由以下部分完成：

+   矩阵乘法（由 cuBLAS/cuBLASLt 处理）。这些库会自动选择内核启动参数。

+   注意力和归一化内核（自定义或 cuDNN）。当手动编写时，启动配置变得至关重要。

例如，在一个包含 1024 个标记的 softmax 内核中：

+   你可能需要为每个序列行启动一个块，每个块有 1024 个线程（每个线程处理一个标记）。

+   或者，如果需要，你可以为每行启动多个块，每个块处理一个标记块。

选择得当可以加倍或三倍性能。

#### 平衡因素

在配置内核时，你需要平衡：

1.  占用率：是否有足够的线程活跃以充分利用 GPU？

1.  内存对齐：线程是否以对齐的、顺序的块访问内存（这很快）或以分散的模式访问（这很慢）？

1.  共享内存和寄存器：每个块都有有限的资源。如果你的内核使用了过多的共享内存，每个 SM 中可以容纳的块数就会减少，从而降低占用率。

1.  运算强度：如果内核在每次内存加载时进行大量数学运算，则占用率不那么重要；如果是内存受限，则占用率更重要。

#### 为什么这很重要

在 GPU 训练中，内核启动决策直接控制硬件的使用效率。两个实现相同数学运算的内核，仅因为启动配置的不同，其运行时间可能相差 5-10 倍。cuBLAS 和 cuDNN 自动化了大多数矩阵密集型操作，但理解这一点在编写自定义内核时至关重要。

#### 尝试自己操作

1.  编写一个简单的 CUDA 内核用于具有不同`threads_per_block`值的向量加法（32、128、256、512、1024）。测量运行时间并查看哪个最快。

1.  使用`nvprof`或`nsys`在 GPT-2 训练期间检查内核的占用率。注意哪些内核的占用率低于<50%。

1.  尝试使用 softmax 内核：每行启动一个块与每行启动多个块进行比较。比较性能和内存使用。

1.  通过人为增加共享内存使用量来探索每个块共享内存分配如何影响占用率。

1.  将 cuBLAS GEMM（矩阵乘法）性能与原始 CUDA 实现进行比较，并观察内核配置如何解释速度差异。

#### 吸收要点

内核启动配置是 GPU 性能的隐藏杠杆。通过调整线程和块的分配方式，你可以控制 GPU 的忙碌程度，内存带宽的使用情况以及计算的流畅度。对于像 GPT-2 这样的模型，库处理大多数内核，但了解底层发生的事情对于编写或调试高效的 CUDA 代码至关重要。

### 69. CUDA 错误处理和调试

当编写或运行 CUDA 代码时，最令人沮丧的部分之一是错误通常不会立即出现。与 CPU 代码不同，无效的指针或除以零可以立即崩溃，CUDA 启动内核是异步的。这意味着主机代码（在 CPU 上运行）将 GPU 工作排队并继续执行，而 GPU 在后台处理。如果在内核内部出现问题，错误可能不会立即可见——有时只有在尝试同步后才会出现。

#### 常见错误来源

1.  越界内存访问 内核线程试图读取或写入数组的末尾。这可能会静默地产生不正确的结果或导致程序崩溃。

1.  无效的内存对齐 一些 CUDA 操作需要指针对齐。未对齐的访问可能会降低性能或触发错误。

1.  非法指令或不受支持的硬件功能 在较旧的 GPU 上使用 Tensor Core，或使用不支持您 GPU 计算能力的指令，可能会失败。

1.  内存不足（OOM） 分配比可用的 GPU 内存更多的内存会导致运行时错误。与 CPU 内存不同，GPU 不能“交换”到磁盘。

1.  竞态条件 块内或跨块访问相同内存位置而没有同步的线程可以破坏结果。

#### 如何报告 CUDA 错误

每个 CUDA 运行时 API 调用都返回一个错误代码。例如：

```c
[](#cb147-1)cudaError_t err = cudaMalloc(&ptr, size);
[](#cb147-2)if (err != cudaSuccess) {
[](#cb147-3)    printf("Error: %s\n", cudaGetErrorString(err));
[](#cb147-4)}
```

类似地，在启动内核后，你应该检查：

```c
[](#cb148-1)my_kernel<<<blocks, threads>>>(...);
[](#cb148-2)cudaError_t err = cudaGetLastError();
[](#cb148-3)if (err != cudaSuccess) {
[](#cb148-4)    printf("Kernel launch failed: %s\n", cudaGetErrorString(err));
[](#cb148-5)}
[](#cb148-6)cudaDeviceSynchronize(); // forces the GPU to finish and report errors
```

这种模式确保了如果出现问题，你可以快速看到它，而不是之后。

#### 调试工具

1.  cuda-gdb 一个具有 GPU 感知的调试器。允许您像在 CPU 代码上使用 gdb 一样逐步执行 CUDA 内核。

1.  cuda-memcheck 检测越界访问、竞态条件和未对齐的内存操作。当内核产生“神秘”的错误输出时，这是必不可少的。

1.  Nsight Systems / Nsight Compute 分析工具，可以显示内核时间线、占用率、内存吞吐量和错误。

1.  代码中的合理性检查 通常，只需简单地插入断言（`assert(i < N)`) 或将内存初始化为零，就可以在早期捕获问题。

#### 在 *llm.c* 中调试

在 *llm.c* 中，大部分 CUDA 繁重的工作由 cuBLAS 和 cuDNN 处理。但当你实验自定义内核（例如 softmax、掩码或 layernorm）时，调试变得至关重要。一个小小的索引错误可能会导致训练发散或使用 `nan` 损失崩溃。通过在每次内核启动后添加 `cudaGetLastError()` 检查，你可以在问题发生的地方立即捕获问题。

#### 示例：一个 Softmax 错误

想象一个内核正在每行计算 1024 个标记的 softmax。如果某个线程索引意外超过了 1024，它可能会读取垃圾内存。如果没有错误检查，你可能会在 100 步后看到“损失是 NaN”。使用 `cuda-memcheck`，你会立即看到：

```c
Invalid global read of size 4
    at softmax.cu:42
    by thread (1025,0,0) in block (1,0,0)
```

现在你确切地知道在哪里修复这个错误。

#### 为什么这很重要

训练大型模型成本高昂。CUDA 内核中的一个错误可能会浪费数小时的 GPU 时间，产生无效的梯度，或者静默地损坏权重。健壮的错误处理和调试实践不仅可以节省挫折，还可以节省显著的成本。

#### 试试你自己

1.  编写一个带有故意错误的 CUDA 核心函数（例如，忘记检查数组边界）。运行它并带有 `cuda-memcheck`，以查看差异。

1.  在简单项目中，在每个核函数后添加 `cudaGetLastError()` 并观察它是如何更早地定位问题的。

1.  尝试使用 Nsight Systems：运行 GPT-2 训练并检查内核启动，检查错误或意外的停滞。

1.  使用不良初始化（例如，输入中的 NaN）进行训练，并查看错误检查如何报告失败。

1.  通过两个线程在没有 `__syncthreads()` 的情况下更新相同的内存来引入竞态条件。使用 `cuda-memcheck` 进行调试。

#### 吸收的经验

CUDA 的异步特性使得错误处理不如 CPU 编程直接。但是，有了正确的工具——错误代码、同步、cuda-memcheck 和调试器——你可以系统地捕捉和修复问题。在 *llm.c* 中，这种纪律确保 CUDA 核心不仅运行得快，而且运行得正确，这在训练大规模模型时同样重要。

### 70. `dev/cuda/`：从简单核函数到高性能

在 *llm.c* 仓库内部，有一个名为 `dev/cuda/` 的文件夹。乍一看，它可能看起来像是一个辅助实验，但实际上它是项目中最具教育意义的部分之一。主要的训练文件（`train_gpt2.cu`，`train_gpt2_fp32.cu`）严重依赖于 cuBLAS 和 cuDNN——这些优化库已经提供了接近峰值性能。但如果你想要了解 CUDA 究竟是如何在底层工作的，你必须查看从零开始编写的核心，这正是这个文件夹所展示的。

#### 为什么这个文件夹存在

`dev/cuda/` 的目标不是取代 cuBLAS 或 cuDNN。相反，它作为一个沙盒，用于：

+   建立关于 GPU 核心结构的直觉。

+   尝试小规模实现如向量加法、矩阵乘法或归一化等操作。

+   比较原始的 CUDA 实现与高度优化的库调用。

+   教育开发者如何内存布局、线程同步和共享内存影响性能。

它是一座桥梁：从简单的“hello world”风格核函数开始，然后逐步接近 NVIDIA 专业库使用的性能技巧。

#### 从原始到优化的旅程

1.  简单的元素级核函数 第一步通常是一个核函数，其中每个线程处理一个元素。例如，添加两个向量 `C[i] = A[i] + B[i]`。这教会了索引、内存归约以及网格和块的概念。

1.  减少核函数接下来，你将转向稍微困难一些的任务，比如求和数组。现在你需要线程合作和同步 (`__syncthreads()`)，以及共享内存的使用。

1.  矩阵乘法（GEMM）一个原始内核可能让每个线程通过遍历输入维度来计算一个输出元素。它可行，但速度很慢，因为它反复从全局内存中加载数据。优化版本使用瓦片：将矩阵的瓦片加载到共享内存中，让线程多次重用它，然后移动到下一个瓦片。这可以将性能提高 10 倍或更多。

1.  后续的高级优化中可能会添加 warp 级别的原语、向量化的加载和 Tensor Core 的使用。这些可以将性能提升到接近 cuBLAS。

#### 教育价值

将这些步骤并排放置，使性能故事非常具体：

+   一个原始的 GEMM 内核可能只达到 cuBLAS 速度的 1%。

+   瓦片共享内存 GEMM 可以提升到 30-40%。

+   通过仔细的 warp 调度，它可以达到 60-70%。

+   cuBLAS 通过手动优化的汇编和 Tensor Core 进一步推进，达到 90-95%。

这说明优化不是魔法——它是一系列逻辑改进的序列，每次都减少了低效。

#### 它对 GPT-2 训练的重要性

即使你从未计划自己重新实现矩阵乘法，了解`dev/cuda/`中发生的事情也有助于解释为什么`train_gpt2.cu`中的主要训练循环如此之快。你看到为什么 cuBLAS/cuDNN 内核是效率的黑盒子：因为在这个级别上编写自己的代码极其困难。

但这也意味着当你需要时，你将更好地准备编写自定义内核。例如，你可能想测试一个新的激活函数或不同的注意力机制。通过从实验内核中借用模式，你可以构建自己的内核，测试它们，并与基线进行比较。

#### 示例：向量加法内核

这里有一个你可能在这个文件夹中找到的简单内核：

```c
[](#cb150-1)__global__ void vector_add(const float* A, const float* B, float* C, int N) {
[](#cb150-2)    int i = blockIdx.x * blockDim.x + threadIdx.x;
[](#cb150-3)    if (i < N) {
[](#cb150-4)        C[i] = A[i] + B[i];
[](#cb150-5)    }
[](#cb150-6)}
```

与 GPT-2 的注意力机制相比，这很简单，但这是每个人开始的地方。从这里开始，你将扩展到矩阵的 2D 索引，然后到瓦片共享内存模式。

#### 试试看

1.  运行`dev/cuda/`中的一个内核，该内核执行原始矩阵乘法。将其运行时间与相同维度的 cuBLAS 进行比较。

1.  将原始的 GEMM 修改为使用带有共享内存的瓦片。测量性能如何提升。

1.  检查 NVCC 为简单内核生成的 PTX（中间汇编）代码，观察内存加载是如何被转换的。

1.  在内核周围添加计时代码，以查看性能如何随着不同块大小的变化而扩展。

1.  实现一个新的自定义内核（例如，ReLU 激活函数）并将其速度与通过 cuDNN 应用 ReLU 的速度进行比较。

#### 吸收

`dev/cuda/`文件夹不是关于生产训练的。它是关于学习和实验的。它从最简单的 CUDA 内核开始，逐步构建到注重性能的设计。这种进展反映了专业库如何实现其速度。通过在这里学习和实验，你将更深入地理解让 GPU 全速运行需要什么——并且你将获得编写自己内核的技能，当库没有提供你所需要的东西时。

## 第八章. 多 GPU 和多节点训练

### 71. *llm.c* 中的数据并行

当你想训练大型模型时，单个 GPU 通常不足以满足需求。要么模型无法装入内存，要么训练时间过长。在多个 GPU 上扩展训练的最简单和最广泛使用的方法之一是数据并行。这个想法在概念上是简单的：不是将所有训练数据都给一个 GPU，而是将其分成更小的批次，给每个 GPU 发送一部分，让它们独立处理，然后合并它们的结果。

#### 核心思想

假设你有一个包含 128 个序列和 4 个 GPU 的批次。在数据并行中：

+   GPU 0 处理序列 0–31

+   GPU 1 处理序列 32–63

+   GPU 2 处理序列 64–95

+   GPU 3 处理序列 96–127

每个 GPU 运行前向传播，计算损失，并为其切片计算梯度。在步骤结束时，梯度在 GPU 之间平均，确保所有模型保持同步。每个 GPU 都持有模型参数的完整副本，因此在梯度平均后始终一致。

#### 在 *llm.c*

*llm.c* 仓库保持最小化，因此没有完整的 DeepSpeed 或 PyTorch DDP 实现。但原理是相同的：

+   每个 GPU 都获得 GPT-2 模型的一个副本。

+   批次在设备之间分割。

+   在反向传播之后，所有 GPU 的梯度必须同步。

这种同步通常使用 NCCL 全量减少（在下节中介绍），但设计本质上仍然是数据并行。

#### 为什么数据并行有效

前向和反向传播在不同数据样本之间是令人尴尬的并行。在计算梯度时，序列 A 中的一个标记不需要知道序列 B 中的一个标记。只要所有 GPU 在每一步后都同意参数更新，分割批次就是完全有效的。

#### 示例说明

假设我们在 TinyStories 上使用批大小 `B = 32` 和序列长度 `T = 64` 训练 GPT-2。

+   在单个 GPU 上，前向传播计算所有 32 个序列的嵌入、注意力、MLP 和损失。

+   使用 2 个 GPU，我们在每个 GPU 上设置 `B = 16`。每个 GPU 并行处理 16 个序列。

+   反向传播后，两个 GPU 都持有其一半批次的梯度。在应用优化器之前，梯度被平均，以便权重更新等效于使用 32 个完整批次进行训练。

从模型的角度来看，它就像没有任何变化一样——它只看到整个批次的梯度。

#### 内存和速度优势

+   内存：每个 GPU 只存储其本地批次的激活。这减少了每个 GPU 的内存使用，使得使用更大的全局批次进行训练成为可能。

+   速度：由于多个 GPU 分享工作，训练步骤完成得更快。例如，将 GPU 数量加倍通常可以将每步训练时间几乎减半，尽管通信开销阻止了完美的扩展。

#### 局限性

1.  通信开销跨 GPU 同步梯度可能会变得昂贵，特别是对于大型模型或在多个节点上运行时。

1.  I/O 瓶颈将数据快速喂给多个 GPU 需要高效的 dataloader 和预取。

1.  使用 AdamW 优化器，每个 GPU 还需要存储优化器状态（m 和 v）。这意味着内存随着 GPU 数量的增加而扩展，而不是缩小。

#### 为什么这很重要

数据并行性是深度学习扩展的功臣。它在概念上容易理解，实现简单，即使对于大型模型也表现良好。在实践中，几乎所有大规模 GPT 训练都是从数据并行性开始的，通常通过梯度累积或混合精度等技术进行增强。

#### 试试你自己

1.  在*llm.c*上使用单个 GPU 训练 GPT-2，然后使用`CUDA_VISIBLE_DEVICES`将批处理分割到两个 GPU 上。比较吞吐量和损失曲线。

1.  在保持每个 GPU 批大小不变的同时，尝试增加全局批大小。注意验证损失如何变化。

1.  通过编写一个简单的脚本来平均两个进程的数组来模拟梯度平均。将这个想法与 NCCL all-reduce 的工作方式联系起来。

1.  测量使用 1 个和 2 个 GPU 训练时每个 GPU 内存使用量的差异。

1.  使用不同数量的 GPU（1、2、4）进行小实验，并绘制每步训练时间的变化。

#### 吸取的经验

数据并行性通过分割批处理将工作负载分配到 GPU 上。每个 GPU 在数据的一部分上训练完整的模型副本，然后同步梯度，以确保更新一致。这很简单但很强大，是*llm.c*和大多数深度学习框架中扩展策略的基础。没有它，在现代数据集上训练 GPT-2 和更大的模型将是不切实际的。

### 72. MPI 进程模型和 GPU 亲和性

当你将训练扩展到单个 GPU 之外时，你需要一种方式来管理多个进程和设备。在*llm.c*代码库中，最小化方法依赖于 MPI（消息传递接口），这是一个在高性能计算中存在了数十年的库。MPI 提供了一个简单的抽象：你启动多个进程，每个进程分配一个排名（一个 ID 号），并且它们可以通过发送和接收消息相互通信。

在分布式深度学习中，MPI 通常与 NCCL（NVIDIA 集体通信库）一起工作。MPI 处理进程管理——启动工作进程、分配 GPU、设置环境变量——而 NCCL 处理实际的梯度同步。

#### MPI 进程和排名

假设你想在 4 个 GPU 上训练。MPI 将启动 4 个进程。每个进程：

+   加载相同的 GPT-2 模型代码。

+   在一个 GPU 上初始化 CUDA。

+   根据设置读取训练数据的一部分或相同的数据集。

每个进程都会得到一个排名：

+   排名 0 → GPU 0

+   排名 1 → GPU 1

+   排名 2 → GPU 2

+   排名 3 → GPU 3

排名很重要，因为它们决定了角色。例如，排名 0 通常充当“主进程”，打印日志或处理检查点，而其他进程则专注于计算。

#### GPU 亲和性

如果你没有明确地将进程映射到 GPU，它们都可能尝试使用相同的设备。这会导致过度订阅——多个进程争夺一个 GPU，而其他 GPU 则闲置。为了防止这种情况，你需要设置 GPU 亲和性。

环境变量 `CUDA_VISIBLE_DEVICES` 是实现这一点的最简单方法。例如：

```c
[](#cb151-1)# Run with 4 GPUs
[](#cb151-2)mpirun -np 4 \
[](#cb151-3)  -x CUDA_VISIBLE_DEVICES=0,1,2,3 \
[](#cb151-4)  ./train_gpt2_mpi
```

MPI 自动将进程 0 分配给 GPU 0，进程 1 分配给 GPU 1，依此类推。在代码内部，你可以通过调用 `cudaSetDevice(rank)` 来确认这一点。

在多节点集群上，GPU 亲和性还需要考虑网络拓扑。你希望每个进程靠近其 GPU，并且理想情况下与节点的网络卡对齐，以便更快地进行 NCCL 通信。

#### 同步和通信。

在每次前向和反向传递之后，每个 MPI 进程都有其局部梯度。这些梯度必须在进程之间平均，以保持模型权重的连续性。MPI 本身提供了集体操作，如 `MPI_Allreduce`，但在实践中，*llm.c* 使用 NCCL 进行 GPU 到 GPU 的通信，因为它更快且具有拓扑感知性。MPI 设置组，NCCL 执行繁重的工作。

#### 示例工作流程。

1.  启动：`mpirun -np 4 ./train_gpt2` 启动 4 个进程。

1.  初始化：每个进程确定其排名并使用 `cudaSetDevice(rank)` 设置其 GPU。

1.  训练循环：

    +   在每个进程的 GPU 上进行前向传递。

    +   反向传递以计算梯度。

    +   使用 NCCL All-Reduce 平均梯度。

1.  更新：每个进程都会更新其权重的副本。

1.  同步：在下一步中，所有模型副本都是相同的。

#### 调试 GPU 亲和性问题。

如果你意外地错误配置了 GPU 亲和性，症状包括：

+   两个进程尝试使用相同的 GPU → 内存不足错误。

+   由于没有进程分配，空闲的 GPU。

+   由于进程在套接字或 PCIe 通道上分布不均导致的减速。

一种快速调试的方法是在启动时打印排名和 GPU ID：

```c
[](#cb152-1)int rank;
[](#cb152-2)MPI_Comm_rank(MPI_COMM_WORLD, &rank);
[](#cb152-3)int device;
[](#cb152-4)cudaGetDevice(&device);
[](#cb152-5)printf("Process %d is using GPU %d\n", rank, device);
```

#### 为什么这很重要。

MPI 和 GPU 亲和性可能感觉像是低级管道，但对于扩展来说至关重要。如果你没有正确设置，训练可能只能达到预期速度的一小部分，或者直接崩溃。对于小型设置（2-4 个 GPU），这可能感觉像是过度配置，但对于具有 8、16 或 64 个 GPU 的大型集群，仔细映射是成功与浪费计算时间的区别。

#### 尝试自己操作。

1.  使用 `mpirun -np 2` 训练 GPT-2 并验证每个进程打印不同的 GPU ID。

1.  故意错误配置 `CUDA_VISIBLE_DEVICES`，使两个进程都映射到 GPU 0，然后观察 OOM 错误。

1.  在多 GPU 机器上，尝试在不同的 GPU 上运行固定进程。测量训练吞吐量。

1.  使用 `nvidia-smi topo -m` 查看你的 GPU 的 PCIe 拓扑。尝试将 MPI 排名与附近的 GPU 对齐以获得更好的性能。

1.  打印不同映射的全局减少操作花费的时间，以查看 GPU 亲和性如何影响通信开销。

#### 吸取的经验。

MPI 是分布式训练中管理多个进程的骨干，GPU 亲和力确保每个进程都有权独占访问正确的设备。它们共同为*llm.c*中的高效多 GPU 训练奠定基础。正确处理这些细节，扩展就会顺利；处理不当，就会遇到内存崩溃、空闲 GPU 或通信瓶颈。

### 73. NCCL All-Reduce 用于梯度同步

一旦每个 GPU 完成其正向和反向传播，它就有一组仅反映其训练数据部分的梯度。为了保持所有 GPU 上的模型参数一致，这些梯度必须同步。在现代深度学习系统中，这种同步的标准方式是使用 All-Reduce，NVIDIA 的 NCCL（NVIDIA 集体通信库，发音为“Nickel”）提供了优化的实现。

#### All-Reduce 的作用

All-Reduce 是一种集体通信操作。每个进程（GPU）从自己的本地缓冲区开始，这里是指梯度，操作将它们（通常使用归约，通常是求和）组合起来，并将结果分发给所有进程。

从数学上讲，如果 GPU 0 有`g0`，GPU 1 有`g1`，GPU 2 有`g2`，GPU 3 有`g3`，那么在 All-Reduce 之后，每个 GPU 都有相同的结果：

```c
g_all = (g0 + g1 + g2 + g3) / 4
```

除以 4 是可选的——这取决于你是否想要总和还是平均值——但平均对于梯度更新来说是常见的。

这确保了每个 GPU 都应用相同的权重更新，并与其他 GPU 保持同步。

#### 为什么选择 NCCL？

虽然 MPI 提供了 All-Reduce 原语，但 NCCL 专门针对 GPU 进行了优化。它了解 PCIe、NVLink、NVSwitch 和 Infiniband 拓扑，并安排通信以最大化带宽和最小化延迟。其一些关键策略包括：

+   环形 All-Reduce：GPU 按环形排列。每个 GPU 将其数据发送到下一个 GPU，同时从上一个 GPU 接收数据，随着数据流动累积部分总和。这很好地扩展到许多 GPU。

+   树形 All-Reduce：将通信组织成树形，以带宽为代价减少深度（延迟）。

+   混合方案：NCCL 根据 GPU 数量和拓扑动态选择策略。

通过利用拓扑感知，NCCL 可以饱和可用的通信通道。

#### *llm.c* 训练中的示例

在仅 CPU 的训练循环中，梯度直接更新而不进行通信。在多 GPU CUDA 路径中，在反向传播（`gpt2_backward`）之后，每个 GPU 在内存中都有自己的本地梯度。在这个时候：

```c
[](#cb154-1)ncclAllReduce(model.grads_memory,
[](#cb154-2)              model.grads_memory,
[](#cb154-3)              model.num_parameters,
[](#cb154-4)              ncclFloat32,  // or half precision
[](#cb154-5)              ncclSum,
[](#cb154-6)              comm, stream);
```

在这个调用之后，每个 GPU 上的`model.grads_memory`包含所有 GPU 的汇总梯度。除以 GPU 数量将其转换为平均值。

#### 为什么梯度同步很重要

没有梯度同步，每个 GPU 都会偏离，独立更新权重。这将相当于训练多个较小的模型，而不是一个统一的模型。同步确保所有副本的行为像一个单一的大批量训练作业。

#### 内存和性能考虑

1.  带宽限制：随着模型大小的增长，梯度同步通常占主导地位。对于 GPT-2 774M，每一步的梯度本身就可以达到几个 GB。

1.  通信与计算的叠加：高级系统将梯度交换与反向计算叠加。当较晚的层正在计算梯度时，较早的层已经正在同步。

1.  精度：梯度可以以 FP16/BF16 的精度同步，以将通信带宽减半。这被称为梯度压缩。

#### 类比

想象四个厨师在各自的厨房里烹饪相同的菜肴。每个厨师品尝自己的版本并提出调整（梯度）。如果他们不交流，他们的食谱就会发散。有了 All-Reduce，厨师们分享笔记，平均他们的调整，并应用相同的更改——所以四个厨房最终都烹饪了相同的菜肴。

#### 尝试自己动手做

1.  在 2 个 GPU 上运行训练，有和无梯度同步（通过注释掉 All-Reduce）。观察模型在损失上的快速发散。

1.  使用 NCCL 的 `NCCL_DEBUG=INFO` 环境变量来打印通信模式。观察选择的环形/树形策略。

1.  尝试 FP32 与 FP16 梯度同步，并测量带宽节省。

1.  使用 `nsys` 或 `nvprof` 分析训练，以查看在 All-Reduce 中花费了多少时间。

1.  从 2 个 GPU 扩展到 4 或 8 个 GPU，并测量同步开销的增长。

#### 吸收要点

NCCL All-Reduce 是 *llm.c* 中多 GPU 训练的骨干。它确保在单独的 GPU 上计算的梯度被组合成一个单一、一致的更新。通过利用拓扑感知算法，如环形和树形减少，NCCL 即使在模型和 GPU 数量增加的情况下也能保持同步效率。没有它，分布式训练将产生不一致、漂移的模型，而不是统一的模型。

### 74. 构建和运行多 GPU 训练器

让多个 GPU 协同工作不是自动的——你需要设置环境，初始化通信，并确保每个进程都知道要使用哪个 GPU。在 *llm.c* 中，设计故意简约，但它仍然需要与 MPI（消息传递接口）和 NCCL 集成，以允许跨多个 GPU 进行训练。

#### 第 1 步：MPI 启动

多 GPU 训练从 MPI 开始。你不需要一次性运行程序；你需要使用 `mpirun` 或 `mpiexec` 来启动它，这将为每个 GPU 启动一个进程。例如：

```c
[](#cb155-1)mpirun -np 4 ./train_gpt2_cu
```

这里，`-np 4` 启动了四个进程。每个进程将连接到一个 GPU。

MPI 提供：

+   排名：每个进程的唯一 ID（0，1，2，3）。

+   世界大小：进程总数（此处为 4）。

每个进程都知道自己的身份和有多少对等进程。

#### 第 2 步：GPU 分配

一旦 MPI 分配了排名，每个进程必须选择一个 GPU。这通常是通过以下方式完成的：

```c
[](#cb156-1)cudaSetDevice(rank);
```

因此，进程 0 获取 GPU 0，进程 1 获取 GPU 1，依此类推。如果没有这一步，进程可能会都堆在同一个 GPU 上，导致混乱。

#### 第 3 步：NCCL 通信器

接下来，代码创建一个 NCCL 通信器。把它想象成所有 GPU 之间的“电话会议”。NCCL 在设备之间设置通信路径（环、树）。典型的设置看起来像：

```c
[](#cb157-1)ncclCommInitRank(&comm, world_size, nccl_id, rank);
```

在这里：

+   `world_size` 是 GPU 的数量。

+   `nccl_id` 是通过 MPI 获得的共享标识符（所有进程都必须使用相同的标识符）。

+   `rank` 是本地 ID。

现在 GPU 可以互相通信。

#### 第 4 步：训练循环集成

一旦建立了通信，训练循环看起来并没有太大不同。每个 GPU：

1.  加载它自己的数据批次（因此数据集被分配到 GPU 上）。

1.  执行前向传递。

1.  执行反向传递。

1.  调用 NCCL All-Reduce 来同步梯度。

1.  更新参数。

唯一的新成分是第 4 步。没有它，每个 GPU 都会带着自己的梯度四处游荡。

#### 示例命令

假设你的机器上有 2 个 GPU。你可以用以下方式训练：

```c
[](#cb158-1)mpirun -np 2 ./train_gpt2_cu -batch_size 8 -seq_len 128
```

每个 GPU 在 128 个标记的 8 个序列上训练。结合起来，就像批量大小为 16 的训练，但分布在 GPU 上。

#### 常见陷阱

+   忘记按排名设置设备：所有进程都在争夺 GPU 0。

+   不匹配的 NCCL ID：通信器初始化失败。

+   MPI 与 NCCL 版本：一些构建很挑剔，你可能需要使用匹配的 CUDA/NCCL 重新编译。

+   网络问题：在多节点设置中，防火墙或缺少 InfiniBand 驱动器可能会阻止通信。

#### 为什么这很重要

构建一个多 GPU 训练器是扩展的门户。单个 GPU 可能需要几周时间来训练一个大模型，但将工作分散到 4、8 或 16 个 GPU 上可以显著减少时间。*llm.c* 的简单性表明，分布式训练不需要庞大的框架——只需谨慎使用 MPI 和 NCCL。

#### 亲自尝试

1.  使用 1 个 GPU 和然后 2 个 GPU 启动训练，保持全局批量大小相同。比较训练速度。

1.  使用 2 个 GPU 启动，但忘记 All-Reduce。注意验证损失在每个 GPU 上的行为如何不同。

1.  使用 `NCCL_DEBUG=INFO` 来查看 NCCL 如何设置通信。

1.  尝试故意不匹配排名和设备——观察崩溃以了解为什么分配很重要。

1.  在训练期间使用 `nvidia-smi` 测量 GPU 利用率，以确认两个 GPU 都在工作。

#### 吸收要点

*llm.c* 中的多 GPU 训练器建立在三个支柱之上：MPI 用于管理进程，NCCL 用于同步梯度，CUDA 用于运行数学运算。一旦这些就位，训练循环仍然熟悉，但计算无缝地分布在 GPU 上。这种设计在保持代码最小化的同时，仍然释放了显著的扩展能力。

### 75. 使用 MPI 进行多节点引导

到目前为止，在单台机器上跨多个 GPU 运行相对简单：每个进程通过共享内存或高速互连（如 NVLink）进行通信。当训练需要跨多台机器（通常称为“节点”）扩展时，事情变得更有趣——例如，当你想在 2 台服务器上运行，每台服务器有 4 个 GPU，总共 8 个 GPU 时。

#### MPI 世界

MPI 是为此而设计的。当你运行：

```c
[](#cb159-1)mpirun -np 8 -hostfile myhosts ./train_gpt2_cu
```

+   `-np 8` 表示你想要 8 个进程。

+   `-hostfile myhosts`列出机器（以及每个机器上要运行的进程数）。

MPI 然后在节点间启动进程并为每个进程分配一个排名。从程序的角度来看，两个排名是否在同一台机器或不同机器上并不重要——它们都看到一个大小为 8 的全局通信者。

#### 在节点间设置 NCCL

NCCL 无法自己找到其他机器。它依赖于 MPI 来交换一个唯一的 NCCL ID。典型的流程是：

1.  排名 0 创建一个新的 NCCL ID。

1.  排名 0 使用 MPI 将 ID 广播到所有其他排名。

1.  每个进程都调用`ncclCommInitRank`，使用共享 ID、总世界大小和自己的排名。

这确保了所有 GPU，即使在不同机器上，也能加入同一个“电话会议”。

#### 网络考虑事项

当跨节点扩展时，网络变得至关重要：

+   以太网与 InfiniBand：标准以太网可以工作但可能较慢。高性能集群使用 InfiniBand 以获得更高的带宽和更低的延迟。

+   防火墙规则：NCCL 需要开放的端口来连接节点。防火墙或严格的安全设置可能会阻止通信。

+   环境变量：例如`NCCL_SOCKET_IFNAME`（选择正确的网络接口）等变量通常需要设置。例如：

    ```c
    [](#cb160-1)export NCCL_SOCKET_IFNAME=eth0
    ```

#### 示例主机文件

一个简单的`myhosts`文件可能看起来像这样：

```c
node1 slots=4
node2 slots=4
```

这表示节点 1 和节点 2 各有 4 个 GPU。MPI 将在每个节点上启动 4 个进程，总共 8 个。

#### 节点间的同步

因为现在通信跨越了机器，同步开销变得更加明显。梯度全归约不仅要在一台服务器的 GPU 之间移动数据，还要通过网络。有效的扩展取决于：

+   足够大的批量大小（计算时间超过通信时间）。

+   与计算重叠的通信（高级优化）。

+   机器间快速互连。

#### 为什么这很重要

训练大型模型很少在单台机器上进行。多节点训练是研究人员和公司扩展模型到数十亿参数的方法。通过展示如何在节点间启动 MPI 和 NCCL，*llm.c*展示了分布式 AI 训练系统的基础，但以最小化和透明的方式。

#### 尝试自己操作

1.  准备两台安装了 CUDA 和 NCCL 的机器，通过相同的网络连接。

1.  编写一个包含两台机器的主机文件，然后使用`mpirun`启动。

1.  设置`NCCL_DEBUG=INFO`以观察 NCCL 如何在节点间连接。

1.  比较具有相同 GPU 数量的单节点和双节点运行的吞吐量。

1.  尝试环境变量，如`NCCL_SOCKET_IFNAME`或`NCCL_IB_DISABLE=1`，以查看网络选择如何影响速度。

#### 吸收要点

启动多节点训练是扩展单节点多 GPU 训练的相同原则，但加入了网络。MPI 处理进程管理，NCCL 设置通信，CUDA 运行数学运算。只需几行设置，*llm.c*就可以从笔记本电脑上的一个 GPU 扩展到多个服务器上分布的数十个 GPU。

### 76. SLURM 和 PMIx 注意事项

在许多研究集群或超级计算机上，你不需要手动使用`mpirun`和主机文件来启动作业。相反，你与作业调度器交互，最常见的是 SLURM。SLURM 负责分配资源、在节点间启动进程以及执行配额。虽然这可以让你免于手动管理主机文件，但它引入了自己的一套需要你理解的细节。

#### SLURM 基础知识

在 SLURM 中，你通常使用脚本或命令来请求 GPU 和节点，例如：

```c
[](#cb162-1)salloc -N 2 -G 8 --time=01:00:00
```

+   `-N 2`请求 2 个节点。

+   `-G 8`请求 8 个 GPU（跨这些节点）。

+   `--time=01:00:00`设置了一个一小时的时限。

一旦作业开始，SLURM 会设置环境变量，例如：

+   `SLURM_NTASKS`：任务总数（进程）。

+   `SLURM_PROCID`：当前进程的排名。

+   `SLURM_NODEID`：表示此进程正在哪个节点上运行。

MPI 实现（OpenMPI、MPICH）和 NCCL 可以使用这些来自动启动通信。

#### PMIx 集成

现代 SLURM 通常与 PMIx（用于 Exascale 的进程管理接口）一起工作。PMIx 允许 MPI 和其他运行时直接从 SLURM 查询进程信息，而不依赖于较旧的启动器。在实践中，这意味着：

+   你可能根本不会使用`mpirun`。相反，SLURM 提供了`srun`。

+   例如：

    ```c
    [](#cb163-1)srun -n 8 ./train_gpt2_cu
    ```

    在这里`-n 8`会在你的分配节点上启动 8 个任务。SLURM/PMIx 处理排名分配。

#### 常见错误

1.  MPI 版本不匹配 如果你的集群安装了多个 MPI 库，你可能会不小心用其中一个编译，用另一个运行。始终确认你使用的`mpicc`和`mpirun`与你的作业链接的库相匹配。

1.  环境变量传播 NCCL 依赖于环境变量如`NCCL_DEBUG`、`NCCL_SOCKET_IFNAME`和`NCCL_IB_HCA`。有时 SLURM 不会将这些变量转发到所有节点，除非你进行配置。使用`--export=ALL`或在作业脚本中添加导出可以解决这个问题。

1.  GPU 可见性 SLURM 通过`CUDA_VISIBLE_DEVICES`管理 GPU 分配。每个进程只能“看到”分配给它的 GPU。如果你的代码假设了一个全局的 GPU 视图，它可能会出错。在*llm.c*中，排名和 GPU ID 之间的映射需要遵守这一点。

1.  网络布线不匹配 在大型集群中，你可能有多达多个网络布线（以太网、InfiniBand）。如果 NCCL 选择了错误的一个，性能会急剧下降。显式设置`NCCL_SOCKET_IFNAME`或`NCCL_IB_DISABLE`可以解决这个问题。

#### 为什么这很重要

学习使用 SLURM 跨节点运行对于你想要将训练扩展到单个服务器之外至关重要。虽然本地的`mpirun`命令适用于开发，但几乎所有严肃的训练运行——无论是学术还是工业——都是在 SLURM 或类似的作业管理器下进行的。了解 SLURM 和 PMIx 的怪癖可以确保你的代码平稳扩展，不会出现神秘的挂起或减速。

#### 尝试自己操作

1.  编写一个小的 SLURM 作业脚本，请求 2 个 GPU，运行 10 分钟，并运行一个模拟的*llm.c*训练循环。

1.  使用`srun`启动程序，并打印出每个进程的`SLURM_PROCID`和`SLURM_NODEID`。

1.  在你的作业脚本中设置 `NCCL_DEBUG=INFO` 并观察 NCCL 如何初始化通信。

1.  通过实验 `srun --ntasks-per-node` 来控制每个节点上落地的进程数量。

1.  故意配置错误的 `CUDA_VISIBLE_DEVICES` 来查看它如何影响排名到 GPU 的映射。

#### 吸取的经验

SLURM 和 PMIx 在大型集群上简化了分布式训练，但它们增加了另一层复杂性。原则仍然是相同的——MPI 排名、NCCL 通信者和 CUDA 内核，但调度器决定进程的放置方式和环境的设置。经过一些实践，这些工具允许 *llm.c* 从简单的多 GPU 实验转移到可扩展的集群级训练运行。

### 77. 调试多 GPU 挂起和停滞

在多个 GPU 上训练时，最令人沮丧的体验之一就是作业简单地挂起——没有错误，没有崩溃，只是冻结的进程。在分布式深度学习中，挂起几乎总是与同步不匹配有关。每个 GPU 工作者都应该在通信点（如梯度全归约）处“会合”，如果任何一个进程丢失，整个组就会停滞。

#### 挂起的常见原因

1.  集体调用不匹配 如果一个排名调用 `ncclAllReduce`，而另一个排名跳过它或调用 `ncclBroadcast`，系统就会死锁。所有 GPU 将永远等待，因为在这个步骤中它们没有使用相同的“语言”。

1.  批次大小不均 如果训练数据不能完美地分配到 GPU 上，一个进程可能会比其他进程更早耗尽数据。代码试图同步梯度，但某些排名永远不会达到那个点。

1.  静默忽略 CUDA 错误 一个 GPU 上的内核启动失败可能会阻止它达到同步。如果没有错误检查，你将不会看到失败直到程序挂起。

1.  网络问题 NCCL 依赖于可靠的网络连接。如果一个节点有坏的 InfiniBand 卡、防火墙规则或配置错误的接口，通信就会中断。

#### 调试策略

+   启用 NCCL 调试 设置：

    ```c
    [](#cb164-1)export NCCL_DEBUG=INFO
    [](#cb164-2)export NCCL_DEBUG_SUBSYS=ALL
    ```

    这会产生日志，显示每个排名何时进入和离开集体操作。通过比较排名，你可以看到谁卡住了。

+   总是检查 CUDA 错误，或者运行：

    ```c
    [](#cb165-1)cuda-memcheck ./train_gpt2_cu
    ```

    这可以检测无效的内存访问或可能导致停滞的内核失败。

+   简化设置 从单个节点上的 2 个 GPU 开始。如果它工作，增加到 4 个 GPU，然后扩展到多个节点。这可以隔离错误是在 GPU 逻辑还是在网络通信中。

+   超时和看门狗 NCCL 提供了如 `NCCL_TIMEOUT` 这样的环境变量，可以帮助检测集体操作何时停滞。虽然它不能修复挂起，但它可以防止浪费数小时等待无果。

#### 为什么这很重要

在多 GPU 训练中，挂起并不罕见——它是调试旅程的一部分。理解挂起通常意味着“一个 rank 失去同步”有助于你系统地解决问题。通过检查日志、验证批量大小和仔细测试集体调用，你可以避免无尽的挫败感和浪费 GPU 时间。

#### 尝试自己动手做

1.  运行一个 2-GPU 训练作业，故意配置错误代码，使只有一个 rank 调用 `gpt2_backward`。观察系统如何挂起。

1.  启用 `NCCL_DEBUG=INFO` 并比较两个 rank 之间的日志。

1.  修改数据加载器，使一个 GPU 获得的批量比另一个 GPU 少。观察训练在第一次梯度同步时如何停滞。

1.  尝试使用 `cuda-memcheck` 来捕获简单内核中的静默 CUDA 错误。

1.  练习从 1 个节点扩展到 2 个节点，以了解挂起更可能出现在哪里。

#### 吸取的经验

分布式训练中的挂起几乎总是可以追溯到不匹配的同步、不平衡的工作负载或隐藏的错误。通过使用 NCCL 的调试工具、添加错误检查和系统性地测试，你可以将神秘的冻结变成可解决的问题。多 GPU 训练不仅仅是关于原始速度，它是关于学习如何使许多移动部件同步。

### 78. 扩展故事：GPT-2 124M → 774M → 1.6B

*llm.c* 最令人兴奋的部分之一是它不仅仅停留在玩具模型上。在 Tiny Shakespeare 上训练小型 GPT-2 模型的相同代码可以扩展到更大的模型，如 GPT-2 774M 甚至 1.6B。但扩展不仅仅是让数字变大——它几乎改变了你训练的方方面面：内存需求、通信成本、优化器稳定性，甚至你的工作流程。

#### 从小开始：GPT-2 124M

124M 参数模型是 GPT-2 训练的“hello world”。它可以在单个现代 GPU 上舒适地运行，你甚至可以在 CPU 上运行一个精简版本。在这个规模下：

+   批量大小可以保持较小（例如，4-8）。

+   内存需求适度——几 GB 的 VRAM。

+   训练速度相对较快，因此你可以快速迭代。

+   目的：进行合理性检查、调试内核、验证正确性。

将 124M 视为训练轮阶段：你正在学习保持平衡，而不是还在比赛。

#### 移动到 GPT-2 774M

在 ~774M 参数时，情况发生了变化：

+   单个 GPU 仍然可以 *容纳* 模型，但训练速度会显著减慢。

+   在多个 GPU 之间同步梯度变得至关重要，以获得合理的吞吐量。

+   通信成本开始变得重要：每一步数百兆字节的 all-reduce 都会压榨 PCIe 和网络带宽。

+   稳定性变得更加敏感：学习率和预热计划需要更仔细的调整。

在这里，训练更多地关乎“代码是否运行”，而不是“系统是否扩展？”这个规模常用于 GPT-2 的学术复制，因为它足够大以引起兴趣，但又不至于过于昂贵。

#### GPT-2 1.6B：扩展到边缘

在 1.6B 参数时，模型太大，单个 GPU 无法高效训练。你需要：

+   使用 NCCL all-reduce 的多 GPU 设置来共享梯度更新。

+   当即使是 8 个 GPU 也不够时，在集群上进行多节点训练。

+   仔细的优化器调优——如果没有为 AdamW 和调度器设置适当的设置，模型可能会发散。

+   使用混合精度（FP16/BF16）和梯度检查点等内存技巧来适应激活。

训练 GPT-2 1.6B 是一个重大的工程挑战，但它证明*llm.c*不仅仅是一个玩具项目——它是一个最小化但真实的实现，可以将规模推到十亿参数级别。

#### 规模经验教训

当你从 124M 爬升到 774M 到 1.6B 时，会出现几个教训：

1.  小规模调试，大规模扩展——在尝试更大模型之前，始终在 124M 上进行测试。

1.  通信占主导地位——在 774M 及以上，移动梯度所花费的时间通常超过计算时间。

1.  超参数会演变——适用于 124M 的学习率在 1.6B 时可能会使损失爆炸。

1.  基础设施很重要——GPU、互连和调度器与代码一样重要。

#### 为什么这很重要

规模故事表明，深度学习不仅仅是编写一个巧妙的算法——它是关于让这个算法在越来越重的负载下工作。每次规模的跳跃都会揭示新的瓶颈和新的工程挑战。通过遵循这条路径，你可以获得对大型模型在实际中是如何训练的直觉。

#### 尝试自己操作

1.  在 Tiny Shakespeare 上训练 GPT-2 124M，直到损失稳定。记录每一步需要多长时间。

1.  在 OpenWebText 上用 124M 尝试相同的实验——看看现在数据集大小是如何成为限制因素的。

1.  如果你有多个 GPU 的访问权限，将 GPT-2 355M 或 774M 扩展到 GPT-2，并测量在 NCCL all-reduce 与计算相比花费的时间。

1.  如果你有权访问集群，尝试使用`srun`或`mpirun`跨节点运行 774M。

1.  研究 GPT-2 1.6B 发布的训练日志，并将其与你的日志进行比较——规模如何改变损失曲线的形状？

#### 吸取的经验

规模不仅仅是“更大的模型需要更大的 GPU”。模型规模的每次增加都会重塑训练过程，引入新的瓶颈并需要新的技术。*llm.c*之所以有价值，正是因为它使这些过渡变得透明：你可以从一个小型模型开始，并逐渐体验训练最先进语言模型的真正工程挑战。

### 79. NCCL 调优和重叠机会

一旦你的训练运行扩展到单个 GPU 之外，通信开销就成为一个核心挑战。每个训练步骤都需要在 GPU 之间交换梯度，以便优化器更新保持同步。这就是 NCCL（NVIDIA 集体通信库）发挥作用的地方。NCCL 提供了集体操作（如 all-reduce、all-gather 和 broadcast）的高效实现。但仅仅使用 NCCL 是不够的：你如何调优它，以及你如何将通信与计算重叠，可以决定训练是缓慢的还是接近线性的扩展。

#### NCCL 在训练中的工作原理

在`llm.c`中，当多个 GPU 一起训练时，每个 GPU 在反向传播期间计算其局部梯度。在反向传播结束时，NCCL 的全归约将跨 GPU 合并梯度，使得每个 GPU 最终具有相同的值。只有在这种情况下，优化器才能向前迈进。

没有 NCCL，您将不得不使用`cudaMemcpyPeer`或 MPI 编写自定义点对点代码，这将既慢又难以维护。NCCL 确保通信模式对底层硬件（PCIe、NVLink 或 InfiniBand）是高效的。

#### 关键 NCCL 调整参数

1.  NCCL_DEBUG 设置`NCCL_DEBUG=INFO`有助于您了解 NCCL 正在做什么。对于性能调整，日志是必不可少的。

1.  NCCL_SOCKET_IFNAME 在多节点集群上，这决定了 NCCL 绑定到哪个网络接口。使用错误的接口（如以太网而不是 InfiniBand）可能会以数量级降低训练速度。

1.  NCCL_ALGO 决定了集体操作如何执行：

    +   *环形*：适用于大消息大小，性能稳定。

    +   *树形*：对于小消息更快，延迟更低。一些训练运行可以从尝试两者中受益。

1.  NCCL_IB_DISABLE 如果您想强制 NCCL 避免 InfiniBand 并坚持使用 TCP/IP，通常用于调试网络问题。

#### 通信与计算的叠加

反向传播不需要等待所有梯度计算完毕才开始通信。实际上，较早层的梯度可以在较晚层仍在计算梯度时开始全归约。这被称为通信-计算叠加。

例如：

+   不使用叠加：计算所有层的梯度 → 运行所有梯度全归约 → 更新参数。

+   使用叠加：当较高层的梯度仍在计算时，开始较早层的全归约。

这减少了空闲时间，通常会导致显著的吞吐量提升。一些框架（如 PyTorch 的 DistributedDataParallel）会自动实现这一点。在像`llm.c`这样的低级系统中，这需要仔细的内核启动顺序和流管理。

#### 实际示例

想象您正在使用 8 个 GPU 训练 GPT-2 774M。每次反向传播会产生约 3 GB 的梯度。如果您等待所有梯度都准备好后再同步，全归约可能需要 200 毫秒。如果您的计算步骤也花费 200 毫秒，那么您的一半训练时间将花费在空闲上。使用叠加，您可以将大部分通信隐藏在计算时间内，可能将步骤时间几乎减半。

#### 为什么这很重要

随着模型大小的增加，通信成本可以与计算相媲美或超过计算。如果没有调整，GPU 将花费更多时间等待数据到达，而不是实际训练模型。通过理解 NCCL 并应用叠加技术，您可以解锁高效扩展到数十甚至数百个 GPU 的能力。

#### 尝试自己操作

1.  使用`NCCL_DEBUG=INFO`启用多 GPU 训练作业，并观察通信模式。

1.  在 `NCCL_ALGO` 之间切换 `Ring` 和 `Tree` 并测量对步骤时间的影响。

1.  尝试设置 `CUDA_LAUNCH_BLOCKING=1` 以消除重叠，然后再次移除以查看通信和计算如何交错。

1.  如果你有集群，尝试强制 NCCL 使用以太网而不是 InfiniBand，并比较带宽。

1.  使用 `nvprof` 或 Nsight Systems 对多 GPU 运行进行性能分析，并检查 NCCL 集合是否与内核执行重叠。

#### 吸取教训

高效的分布式训练不仅关于拥有更多的 GPU——它关于保持它们忙碌。NCCL 提供了通信骨干，但如何配置和重叠其操作决定了你是否接近线性扩展或浪费资源。掌握这些细节将多 GPU 训练从“仅仅工作”转变为真正高效的大规模计算。

### 80. 常见的多 GPU 错误和修复

当在多个 GPU 上运行 *llm.c* 时，错误可能从令人困惑的挂起到神秘的 NCCL 消息。这些问题在分布式训练中很常见，但如果不识别模式，它们可能会消耗数小时。好消息是，大多数错误都落入几个常见的类别，一旦你了解了典型的原因，它们就更容易诊断和修复。

#### 错误类型 1：无输出挂起进程

症状：训练开始但随后冻结。没有错误消息，没有崩溃，只有沉默。原因：通常，一个或多个排名不同步。这可能意味着：

+   每个排名上的批处理大小不同（一个排名剩余的标记较少）。

+   集合调用不匹配——例如，一个 GPU 调用 `all_reduce`，而另一个跳过它。

+   一个进程中的 CUDA 错误阻止它达到同步。修复：

+   确认数据加载器为每个排名提供相同数量的步骤。

+   在 CUDA 调用中添加错误检查。

+   启用 `NCCL_DEBUG=INFO` 以跟踪哪个排名卡住。

#### 错误类型 2：`NCCL WARN Net: 未找到接口`

症状：NCCL 报告找不到网络接口，或训练速度极慢。原因：NCCL 无法发现用于节点间通信的正确接口。默认情况下，它可能会尝试使用以太网而不是 InfiniBand。修复：

+   将 `NCCL_SOCKET_IFNAME` 设置为正确的接口，例如：

    ```c
    [](#cb166-1)export NCCL_SOCKET_IFNAME=ib0
    ```

+   与你的系统管理员确认哪些网络接口可用于高性能 GPU 通信。

#### 错误类型 3：`CUDA_ERROR_OUT_OF_MEMORY`

症状：在分配模型权重或反向传播过程中进程崩溃。原因：

+   模型太大，无法适应可用的 GPU 内存。

+   批处理大小过高。

+   重复分配导致的内存碎片。修复：

+   减少批处理大小 `B` 或序列长度 `T`。

+   如果支持，尝试混合精度（FP16/BF16）。

+   重新启动进程以清除内存碎片。

#### 错误类型 4：`未处理的系统错误，NCCL 版本不匹配`

症状：一个进程记录 NCCL 版本 `2.17`，另一个记录 `2.14`。训练失败。原因：节点间使用不同的 NCCL 库。这发生在软件环境不相同的情况下。修复：

+   在所有节点上使用相同的容器或模块环境。

+   使用 `ldd` 或 `conda list` 确认 NCCL 版本。

#### 错误类型 5：在多 GPU 上验证损失发散，但在单 GPU 上不会

症状：仅在跨多个 GPU 运行时损失值爆炸。原因：梯度同步可能已损坏——例如，只有参数子集正在执行 all-reduce。另一种可能性是由于批大小缩放而使用不同的有效学习率。修复：

+   确认所有参数都参与梯度同步。

+   正确缩放学习率：如果你通过使用更多 GPU 将全局批大小加倍，你可能需要调整学习率。

#### 为什么这很重要

多 GPU 训练功能强大但不容忍错误：即使是环境、数据或同步中的微小不匹配也可能导致错误。与其将这些视为随机神秘的事物，不如识别这些模式。每个错误信息或挂起都有可能的原因，学会将症状映射到解决方案将使分布式训练更加顺畅。

#### 试试看

1.  故意减少数据集大小，以便一个排名提前耗尽数据——观察挂起。

1.  不设置 `NCCL_SOCKET_IFNAME` 就启动多节点运行，看看性能如何崩溃。

1.  增加批大小，直到触发 `CUDA_ERROR_OUT_OF_MEMORY`，然后逐步减少以查看限制。

1.  如果你有多个环境，尝试在节点之间进行不匹配的 NCCL 版本实验，然后通过标准化来修复它。

1.  使用不同批量大小的模型运行小模型，并研究验证损失如何发散。

#### 吸收要点

一旦你了解了底层发生的事情，大多数多 GPU 错误就不会神秘。它们通常归结为同步不匹配、网络配置错误、内存限制或环境不一致。有了正确的调试工具和系统性的思维方式，你可以快速修复这些问题，并使训练运行继续进行。

## 第九章：扩展代码库

### 81. 用于自定义内核的 `dev/cuda` 库

到目前为止，*llm.c* 中的大多数 CUDA 逻辑都依赖于 NVIDIA 的优化库，如 cuBLAS 和 cuDNN。这些库非常强大且高效，但有时你想要更多的控制：也许你正在尝试新的注意力机制，或者你可能想将多个操作融合到一个内核中，以减少内存流量。这就是 `dev/cuda` 目录的作用所在。它是自定义内核的游乐场。

#### `dev/cuda` 中有什么

如果你查看存储库结构，你会注意到一个名为 `dev/cuda` 的文件夹。这不是最小训练路径的一部分——你可以在不接触它的情况下训练 GPT-2 模型。相反，它包含实验性内核，展示了如何从简单的 CUDA 示例过渡到更高级、生产级别的实现。

在内部，你通常会找到：

+   Hello World 内核：基本示例，如逐元素加法，以熟悉 CUDA。

+   融合操作：结合步骤如偏置添加+激活的简单原型。

+   基准代码：测量内核性能与 cuBLAS/cuDNN 相比的小型程序。

这些文件不是经过打磨的生产代码。它们旨在被阅读、修改和玩耍——就像 CUDA 开发的实验笔记本。

#### 为什么自定义内核很重要

类似于 cuBLAS 这样的库旨在覆盖广泛的使用场景，但它们并不总是能满足你特定工作负载的甜蜜点。编写自定义内核允许你：

+   融合操作：而不是为偏置添加、激活和 dropout 分别启动内核，你可以在一个内核中完成所有这三个操作，从而在内存读写上节省时间。

+   尝试新的算法：如果你发明了一种新的注意力或归一化类型，你不能依赖于 cuDNN——你需要自己实现它。

+   学习 GPU 的实际工作原理：阅读和编写自定义内核教你关于线程块、内存层次结构和 warp 调度，所有这些都加深了你对于 GPU 编程的理解。

#### 示例：一个简单的逐元素内核

这里有一个来自`dev/cuda`目录中玩具示例的非常小的内核：

```c
__global__ void add_kernel(const float* a, const float* b, float* out, int N) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < N) {
        out[i] = a[i] + b[i];
    }
}
```

这个内核逐元素地添加两个数组`a`和`b`。与注意力机制相比，这微不足道，但它说明了 GPU 执行模型：

+   每个线程处理一个索引`i`。

+   线程被分组到块中，块形成一个网格。

+   内存访问是显式的：你完全控制`out`的写入方式。

将其扩展到真实工作负载意味着增加更多复杂性——共享内存、warp 洗牌、半精度数学——但原则保持不变。

#### 为什么这很重要

`dev/cuda`目录不仅仅是为了乐趣实验。它是“使用 GPU 库”和“设计 GPU 算法”之间的桥梁。通过在这里学习编写内核，你可以获得在标准库提供之外进行定制和优化的自由。如果你想在模型架构上进行创新或从硬件中榨取最后一丝性能，这项技能变得至关重要。

#### 试试看

1.  在`dev/cuda`目录中打开一个示例`.cu`文件，并用`nvcc`编译它。

1.  修改逐元素内核，使其执行`out[i] = a[i] * b[i]`而不是加法。

1.  将你的内核与等效的 cuBLAS 调用（例如，`cublasSaxpy`）进行基准测试并比较性能。

1.  编写一个融合内核，在一次遍历中执行偏置添加和 ReLU 激活。

1.  使用`nvprof`或 Nsight Systems 来测量正向传递中发生的内核启动次数，并想象自定义融合如何减少它们。

#### 吸收要点

`dev/cuda`库是你在学习 CUDA 和实验 CUDA 时的沙盒。它不是运行 GPT-2 所必需的，但它是你构建超越库并设计自己的 GPU 操作技能的地方。无论你是优化速度还是测试新的研究想法，这个目录是 GPU 编程中理论与实践相遇的地方。

### 82. 添加新的数据集管道（`dev/data/*`）

训练语言模型不仅仅是拥有一个聪明的模型——同样重要的是你给它喂的数据。在 *llm.c* 中，与 PyTorch 等框架相比，数据集的处理方式非常轻量级。项目不使用复杂的抽象，而是保持简单：一次性标记你的文本，将其保存为 `.bin` 文件，然后将标记批次流式传输到模型中。

`dev/data/` 目录是这里发生的地方。它包含用于准备不同数据集的脚本和实用程序，从小型玩具语料库如 Tiny Shakespeare 到更大的集合如 TinyStories 或 OpenWebText 子集。理解这个目录的工作原理是插入您自己的数据集的关键。

#### 在 *llm.c* 中数据管道是如何工作的

从高层次来看，管道遵循三个步骤：

1.  下载或提供原始文本数据。例如，`tiny_shakespeare.txt` 只是一个简单的文本文件，其中包含了拼接在一起的剧本。

1.  使用 GPT-2 标记化器一次性标记数据。标记化器根据 `gpt2_tokenizer.bin` 将文本转换为整数。

1.  将标记写入二进制文件（`.bin`）。这是一个存储为 32 位值的整数平面数组，这使得在训练期间快速内存映射和流式传输变得很快。

一旦生成了 `.bin` 文件，`dataloader_init` 可以打开它们，将它们分成训练和验证分割，并为模型生成形状为 `(B, T)` 的批次。

#### `dev/data/` 中有什么

文件夹包含如下小型脚本：

+   `download_starter_pack.sh`——下载 Tiny Shakespeare 和 TinyStories。

+   标记化脚本——通常是运行在原始文本上的小型 Python 片段，使用 GPT-2 标记化器。

+   预建的 `.bin` 文件——这些在快速入门中使用，因此你不需要自己重新生成它们。

这里的设计选择是极简主义：而不是一个重量级的数据集框架，你得到的是普通文件和简短的脚本。你可以在几分钟内阅读和理解一切。

#### 添加您自己的数据集

假设你想在公司支持聊天记录或你找到的新数据集上进行训练。过程如下：

1.  以简单格式准备原始文本（一个文本文件即可）。

1.  在 *llm.c* 上运行标记化器：

    ```c
    [](#cb168-1)python dev/data/tokenizer.py --input my_corpus.txt --output my_corpus.bin
    ```

    这将生成一个二进制标记文件。

1.  将文件放入 `dev/data/`。你可能将其命名为 `my_corpus_train.bin` 和 `my_corpus_val.bin`。

1.  在你的训练代码中将数据加载器指向它：

    ```c
    [](#cb169-1)dataloader_init(&train_loader, "dev/data/my_corpus_train.bin", B, T, 0, 1, 1);
    [](#cb169-2)dataloader_init(&val_loader, "dev/data/my_corpus_val.bin", B, T, 0, 1, 0);
    ```

就这样——你现在有一个新的数据集管道与相同的训练循环集成。

#### 为什么这很重要

许多框架将数据预处理隐藏在多层抽象之后。*llm.c* 采取了相反的方法：它使过程变得透明。你可以看到文本如何变成标记，这些标记如何变成批次，以及模型如何消费它们。这种透明性使得调试、扩展和定制变得更加容易。添加新的数据集不再是神秘的事情——只需编写一个文件并更新路径即可。

#### 亲自尝试

1.  探索 `dev/data/` 目录并阅读提供的脚本。

1.  标记化你选择的新小型数据集（一部小说、一组维基百科页面或你自己的文本）。

1.  在你的新数据集上训练一个 124M 模型，并观察损失曲线。

1.  比较 Tiny Shakespeare 和你的数据集之间的验证损失——模型的行为有何不同？

1.  尝试增加序列长度 `T`，看看批处理如何与较长的文档交互。

#### 吸收要点

`dev/data` 文件夹是您将语言模型连接到现实世界的地方。它展示了原始文本如何几乎无开销地成为训练就绪的二进制文件。通过学习添加自己的管道，您将能够以快速和可理解的方式在任意数据集上训练 *llm.c* —— 从经典文学到特定领域的语料库。

### 83. 将新的优化器添加到代码库中

到目前为止，*llm.c* 专注于 AdamW，这是训练转换器模型的工作马优化器。但深度学习是一个快速发展的领域：新的优化器出现，旧的优化器有时会重新出现，某些工作负载从替代方案中受益。*llm.c* 的简单性使其成为学习如何实现和实验优化器的绝佳环境。

#### 优化器在 *llm.c* 中的位置

在 CPU 训练路径中，优化器逻辑直接在 `gpt2_update` 函数中以 C 语言实现。这个函数遍历每个参数，应用 AdamW 的动量更新，应用偏差校正，然后就地修改参数值。

因为参数、梯度和优化器状态都存储在内存中的连续数组中（`params_memory`、`grads_memory`、`m_memory`、`v_memory`），添加新的优化器通常意味着：

1.  分配任何需要的新的状态数组。

1.  在训练循环中定义更新规则。

1.  添加你的新优化器的函数调用，类似于 `gpt2_update`。

#### 示例：实现带有动量的 SGD

带有动量的随机梯度下降（SGD）比 AdamW 简单得多。更新规则如下：

```c
[](#cb170-1)// SGD with momentum update
[](#cb170-2)void gpt2_update_sgd(GPT2 *model, float learning_rate, float momentum) {
[](#cb170-3)    if (model->m_memory == NULL) {
[](#cb170-4)        model->m_memory = (float*)calloc(model->num_parameters, sizeof(float));
[](#cb170-5)    }
[](#cb170-6)
[](#cb170-7)    for (size_t i = 0; i < model->num_parameters; i++) {
[](#cb170-8)        float grad = model->grads_memory[i];
[](#cb170-9)        float m = momentum * model->m_memory[i] + grad;
[](#cb170-10)        model->m_memory[i] = m;
[](#cb170-11)        model->params_memory[i] -= learning_rate * m;
[](#cb170-12)    }
[](#cb170-13)}
```

在这里，`m_memory` 存储速度（过去梯度的指数衰减平均值）。没有像 AdamW 那样的第二阶矩估计，因此在代码和内存使用上更简洁。

#### 比较优化器

添加新的优化器让你可以实验和比较行为：

| 优化器 | 优点 | 缺点 | 内存需求 |
| --- | --- | --- | --- |
| SGD | 简单、稳定、超参数较少 | 大模型上收敛速度慢 | 低 |
| SGD + 动量 | 收敛速度更快，平滑更新 | 仍然不如 Adam 自适应 | 低 |
| Adam | 逐参数调整学习率 | 可能会过拟合小数据集 | 中等 |
| AdamW | 与 Adam + 正确的权重衰减相同 | 更复杂 | 中等 |
| Adagrad/RMSProp | 适用于稀疏特征 | 对于转换器来说不一定稳定 | 中等 |

在 *llm.c* 中，每个优化器只是一个遍历参数的循环，其中包含一些数学运算。这使得它成为观察不同优化器在实际中如何表现的最佳游乐场。

#### 为什么这很重要

优化器控制你的模型如何学习。虽然像 GPT-2 这样的架构得到了很多关注，但优化中的小变化可能会在收敛平稳的模型和发散的模型之间产生差异。通过添加你自己的优化器，你可以更清楚地理解深度学习中的这个经常是“黑箱”的部分。

#### 亲自尝试

1.  实现上面显示的带有动量的 SGD 函数，并将其替换到训练循环中而不是 AdamW。

1.  在 Tiny Shakespeare 上运行训练，比较损失达到 2.0 所需的步数。

1.  修改代码以实现 RMSProp（类似于 Adam，但第一动量没有使用）。

1.  基准内存使用：注意 AdamW 同时分配了`m_memory`和`v_memory`，而 SGD 只使用一个。

1.  尝试使用非常小的数据集运行 AdamW 与 SGD——哪一个更快地过拟合？

#### 吸收要点

优化器只是数组上的数学。通过在*llm.c*中编写和测试新的优化器，你会揭开学习在参数级别实际发生的神秘面纱。这使得更容易理解为什么 AdamW 成为 transformers 的默认选择，同时也为你提供了一个干净、透明环境中的探索替代方案的工具。

### 84. 添加新的调度器（余弦、步长等）

训练不仅仅是选择一个优化器；你调整学习率的方式在时间上同样重要。调度器告诉优化器在每一步*如何快速学习*。没有调度器，你会使用一个固定的学习率，这对于大型模型通常效果不佳。在*llm.c*中，调度器被有意保持简单，这样你可以清楚地看到它们如何影响训练。

#### 调度器的适用位置

如果你查看训练循环，每一步都会调用优化器如下：

```c
[](#cb171-1)gpt2_update(&model, lr, beta1, beta2, eps, weight_decay, step+1);
```

这里的`lr`不必是常数。相反，调度器函数可以根据步数计算它。在*llm.c*中，这个逻辑位于`schedulers.h`和相关辅助函数中。

#### 你可以添加的常见调度器

1.  步长衰减：每 N 步减少一个固定的因子。

    ```c
    [](#cb172-1)float step_decay(int step, float base_lr, int decay_every, float decay_factor) {
    [](#cb172-2)    int k = step / decay_every;
    [](#cb172-3)    return base_lr * powf(decay_factor, k);
    [](#cb172-4)}
    ```

1.  余弦衰减：平滑地按照余弦曲线降低学习率。

    ```c
    [](#cb173-1)float cosine_decay(int step, int max_steps, float base_lr) {
    [](#cb173-2)    float progress = (float)step / max_steps;
    [](#cb173-3)    return base_lr * 0.5f * (1.0f + cosf(M_PI * progress));
    [](#cb173-4)}
    ```

1.  线性预热 + 余弦衰减：从逐渐增加（预热）开始，以避免不稳定性，然后切换到余弦衰减。这是 transformers 中最常见的选择。

#### 示例：带有预热的余弦函数

下面是如何在*llm.c*中实现带有预热的余弦函数的示例：

```c
[](#cb174-1)float lr_scheduler(int step, int warmup_steps, int max_steps, float base_lr) {
[](#cb174-2)    if (step < warmup_steps) {
[](#cb174-3)        return base_lr * (float)(step + 1) / warmup_steps;
[](#cb174-4)    } else {
[](#cb174-5)        float progress = (float)(step - warmup_steps) / (max_steps - warmup_steps);
[](#cb174-6)        return base_lr * 0.5f * (1.0f + cosf(M_PI * progress));
[](#cb174-7)    }
[](#cb174-8)}
```

这意味着：

+   步骤 0–`warmup_steps`：从 0 线性缩放到`base_lr`。

+   热身之后：使用余弦函数平滑地衰减学习率。

#### 为什么这很重要

调度器有助于稳定训练。在开始时，梯度可能非常嘈杂，因此缓慢预热可以防止发散。在结束时，降低学习率有助于模型收敛而不是在最小值周围弹跳。没有调度器，你通常需要更多的调整才能得到相同的结果。

#### 亲自尝试

1.  在 Tiny Shakespeare 上以恒定学习率进行训练，并记录损失曲线。

1.  切换到步长衰减调度器，看看是否可以提高收敛性。

1.  实现带有预热的余弦衰减并与恒定学习率（LR）进行比较——哪个达到更低的验证损失？

1.  尝试使用不同的预热长度（例如，10 步与 100 步）并观察训练稳定性如何变化。

1.  尝试在 TinyStories 上运行相同的实验，看看数据集大小是否会影响哪个调度器效果最好。

#### 总结

调度器是具有重大影响的小段代码。它们不会改变模型或优化器，但它们控制着学习的*节奏*。通过向*llm.c*添加新的调度器，你可以亲身体验为什么现代训练配方几乎总是将预热与平滑衰减调度相结合。

### 85. 替代注意力机制

Transformer 因其自注意力机制而闻名，但“注意力”并非一个单一的固定公式。研究人员已经探索了许多权衡内存使用、速度和准确性的替代方案。在*llm.c*中，默认使用缩放点积注意力，但没有任何东西阻止你尝试新的方法。

#### 默认：缩放点积注意力

标准的注意力公式看起来是这样的：

<semantics><mrow><mtext mathvariant="normal">Attention</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>Q</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi>V</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mtext mathvariant="normal">softmax</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mi>V</mi></mrow> <annotation encoding="application/x-tex">\text{Attention}(Q, K, V) = \text{softmax}\!\left(\frac{QK^T}{\sqrt{d_k}}\right) V</annotation></semantics>

+   Q = 查询

+   K = 关键

+   V = 值

+   <semantics><msub><mi>d</mi><mi>k</mi></msub><annotation encoding="application/x-tex">d_k</annotation></semantics> = 键维度（用于缩放）

在*llm.c*中，这是通过矩阵乘法和掩码来实现的，以强制执行因果性。它是正确且忠实于 GPT-2 的，但在序列长度<semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics>上有二次成本。

#### 你可以添加的变体

1.  稀疏注意力而不是关注每个标记，将注意力限制在局部窗口或一组重要位置。

    +   适合长序列。

    +   节省计算和内存。

    +   示例：“滑动窗口”注意力，其中每个标记只回顾 128 步。

1.  Linformer / 低秩注意力近似 <semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">QK^T</annotation></semantics> 使用低秩投影。

    +   将内存从<semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>T</mi><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(T²)</annotation></semantics>减少到<semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>T</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(T)</annotation></semantics>。

    +   当序列中存在冗余时表现良好。

1.  使用线性注意力（Performer）用核近似替换 softmax，使注意力在序列长度上呈线性。

    +   以精确性换取可扩展性。

    +   在相同硬件上允许更长的序列。

1.  ALiBi（带线性偏置的注意力）添加简单的位置相关偏置而不是完整的位置嵌入。

    +   极其高效。

    +   有助于外推到比训练中看到的更长的序列。

#### 在*llm.c*中如何进行实验

注意力实现位于`train_gpt2.c`中的`attention_forward`和`attention_backward`例程（以及它们的 CUDA 等效例程）。要尝试替代方案：

+   将计算<semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">QK^T</annotation></semantics>的部分替换为你选择的方法。

+   保持接口不变：给定输入 Q，K，V，返回形状为`(B, T, C)`的输出。

+   对基准进行单元测试（`test_gpt2.c`），以确保输出保持合理。

#### 为什么这很重要

注意力通常是 transformers 的瓶颈。二次时间内存使用限制了你序列的长度。通过实验替代方案，你不仅可以提高效率，还可以了解新的研究想法是如何在实践中实现的。今天许多“高效的 transformers”都源于对这个块的简单调整。

#### 尝试自己操作

1.  修改注意力，使每个标记只关注最后 16 个标记（稀疏注意力的玩具形式）。在 Tiny Shakespeare 上训练，比较速度与准确率。

1.  通过添加线性位置相关偏置项实现 ALiBi，看看你的模型是否更好地泛化到更长的文本。

1.  使用 Nsight Systems 或`nvidia-smi`比较标准注意力与你的自定义版本的 GPU 内存占用。

1.  尝试移除缩放因子<semantics><mrow><mn>1</mn><mi>/</mi><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mrow><annotation encoding="application/x-tex">1/\sqrt{d_k}</annotation></semantics>——训练是否变得不稳定？

1.  将 softmax 替换为简单的 ReLU，看看模型的表现如何（提示：通常发散，但它说明了 softmax 的重要性）。

#### 吸收要点

注意力块是 transformers 中发生许多魔法的地方，但也是最大的瓶颈。通过在*llm.c*中实验替代方案，你将更深入地了解标准公式的原理，它的弱点，以及如何直接在代码中测试研究中的新想法。

### 86. 分析和测试新内核

当您开始添加自定义 CUDA 内核或尝试新的注意力机制时，下一个重大问题是：您如何知道它们是否正确且高效？这正是分析和测试的用武之地。*llm.c*使此过程最小化但透明，这样您可以确切地了解如何验证正确性和性能。

#### 正确性优先：与参考进行测试

您编写的任何新内核都应该与已知良好的实现进行比较。在*llm.c*中，PyTorch 通常作为“神谕”。例如：

1.  在*llm.c*和 PyTorch 中生成随机的输入张量。

1.  在*llm.c*中运行您的自定义内核。

1.  在 PyTorch 中运行等效操作。

1.  在小范围内比较输出（例如，差异小于`1e-5`）。

这确保了您的内核不会默默计算错误的结果。没有这一步，您可能训练数小时后才会意识到您的模型正在学习无意义的内容。

#### 性能：内核分析

一旦确立了正确性，下一步就是性能。NVIDIA 提供了几个工具：

+   nvprof（较旧）：仍然广泛使用，易于启动。

+   Nsight Systems / Nsight Compute（现代）：更详细，让您可以看到内核计时、内存传输、占用率等更多信息。

实际上：

+   在启用分析的情况下运行您的训练循环。

+   识别占用时间最多的内核。

+   检查您的自定义内核是否比基线（例如，cuBLAS 或 cuDNN）更快。

#### 常见指标

+   内核时间（每次启动需要多长时间）。

+   占用率（相对于最大值有多少 CUDA 核心是活跃的）。

+   内存吞吐量（您是否饱和了内存带宽？）。

+   启动计数（你是否调用内核次数过多而不是融合操作？）。

即使内核是正确的，如果它没有高效地使用 GPU，它也可能比库实现慢。

#### 示例工作流程

假设您编写了一个融合偏差 + ReLU 内核。您可以像这样测试它：

+   在 C 和 PyTorch 中生成一个随机的张量。

+   将您的融合内核与 PyTorch 的单独`+`和`ReLU`操作进行比较。

+   比较结果的正确性。

+   分析两种方法：您的内核是否更快？它是否减少了内核启动开销？

#### 为什么这很重要

自定义内核编写起来很有趣，但没有测试和分析，它们只是猜测。许多研究想法在理论上看起来很有希望，但在实践中却失败了，因为它们运行得更慢或破坏了正确性。通过学习系统地测试和分析，您可以区分真正有用的想法和仅仅是实验的想法。

#### 尝试自己动手做

1.  编写一个简单的融合内核用于偏差 + ReLU。将其与 PyTorch 的`x + bias`后跟`relu(x)`进行比较。

1.  使用`nvprof`检查每个版本中发生多少内核启动。

1.  运行 Nsight Systems 并查看时间线：您是否看到您的融合内核与其他 GPU 活动重叠得更好？

1.  尝试将序列长度`T`扩展到非常大的值——您的内核是否仍然表现良好？

1.  在内核运行前后记录内存使用情况。与未融合版本相比，是否有差异？

#### 吸收要点

性能分析和测试将内核黑客从随机的尝试变成真正的工程。有了正确性的参考和性能测量的工具，你可以自信地迭代，知道你的更改是否真正是改进。这就是 *llm.c* 如何在学习项目和真正的 GPU 系统工作之间架起桥梁。

### 87. 使用 PyTorch 参考作为 Oracle

*llm.c* 的一个指导原则是保持小巧、可读和最小化——但这并不意味着你没有任何安全网。当你实现像变压器这样数学密集型的东西时，你怎么知道你的 C 或 CUDA 代码正在做正确的事情？答案是使用 PyTorch 作为参考实现，通常称为“Oracle”。

#### 这里的“Oracle”是什么意思？

Oracle 简单来说是一个你可以与之比较的信任系统。PyTorch 被信任是因为：

+   它在生产和研究中被广泛使用。

+   它的操作符（矩阵乘法、注意力、layernorm 等）已经过彻底测试。

+   它为你提供具有稳定数值行为的 CPU 和 GPU 实现。

如果你的 *llm.c* 前向或反向传递在小的误差容忍度内与 PyTorch 匹配，你可以有信心你的实现是正确的。

#### 比较的工作方式

工作流程通常如下所示：

1.  在 PyTorch 和 *llm.c* 中使用相同的权重设置相同的模型。

1.  将相同的输入提供给两个模型。

1.  比较输出——logits、损失或梯度。

1.  允许由于浮点运算而导致的微小差异，通常在 `1e-5` 到 `1e-6` 之间。

例如，存储库中的 `test_gpt2.c` 运行 C 中的前向传递并比较 logits 与 PyTorch GPT-2 检查点产生的 logits。

#### 示例

假设你正在测试嵌入查找。在 PyTorch 中，你可能编写：

```c
[](#cb175-1)import torch
[](#cb175-2)from transformers import GPT2Model, GPT2Tokenizer
[](#cb175-3)
[](#cb175-4)tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
[](#cb175-5)model = GPT2Model.from_pretrained("gpt2")
[](#cb175-6)tokens = torch.tensor([[50256, 200]])  # EOT and an example token
[](#cb175-7)out = model.wte(tokens)  # word token embeddings
[](#cb175-8)print(out.detach().numpy())
```

在 *llm.c* 中，你会运行相同的两个标记通过嵌入查找并逐元素比较结果向量。如果它们匹配，你的嵌入实现是正确的。

#### 为什么 PyTorch 是完美的 Oracle

+   透明度：从 PyTorch 检查点（`.bin` 文件）中提取权重很容易。

+   灵活性：你可以测试单个层，而不仅仅是整个模型。

+   可调试性：如果出现问题，你可以隔离哪个层首先发散。

最后一点至关重要——而不是训练数天只发现你的损失曲线发散，你可以在层级别立即捕捉到不匹配。

#### 为什么这很重要

深度学习模型很脆弱。即使在归一化、掩码或梯度流中的微小错误也可能破坏训练。通过将你的工作锚定在 PyTorch 上，你避免了“相信直觉”并转而依赖经过实战检验的基线。这种做法并不仅限于 *llm.c* ——许多专业框架（Megatron-LM、DeepSpeed）在开发期间也会与 PyTorch 进行验证。

#### 亲自尝试

1.  从 Hugging Face Transformers 中提取 GPT-2 权重并将它们加载到 *llm.c* 中。

1.  在 PyTorch 和 *llm.c* 中使用相同的输入标记进行前向传递。从数值上比较输出。

1.  专注于单个块：检查注意力输出是否与 PyTorch 的匹配。

1.  修改一个内核（例如，将 softmax 改为 ReLU）并观察输出如何迅速偏离 PyTorch 的。

1.  使用 PyTorch 通过调用`.backward()`来验证梯度，并与*llm.c*中的`gpt2_backward`进行比较。

#### 吸收的要点

PyTorch 是你在导航 Transformer 内部密集丛林时的指南针和罗盘。将其视为一个先知，你可以自信地从一层移动到下一层，知道你手写的少量代码与一个功能齐全的深度学习框架的行为相匹配。这种做法使*llm.c*不仅仅是一个玩具项目——它成为了一个忠实、可验证的 GPT-2 重实现。

### 88. 探索 GPT-2 之外的领域：LLaMA 示例

虽然*llm.c*专注于 GPT-2 以提高清晰度，但相同的框架可以扩展到更新、更大、更现代的模型，如 LLaMA。Meta 发布的 LLaMA 使用了许多与 GPT-2 相同的构建块——嵌入、注意力层、MLP、归一化和残差流——但通过调整提高了效率和扩展性。通过*llm.c*的视角来看 LLaMA，可以帮助你看到语言模型设计是如何演变的同时仍然保持相同的基因。

#### 保持不变的部分

+   标记嵌入：GPT-2 和 LLaMA 都使用查找表将标记 ID 转换为密集向量。

+   Transformer 块：注意力→MLP→残差的基本循环保持不变。

+   自回归训练：给定所有前面的标记预测下一个标记，使用因果掩码。

这意味着*llm.c*中的大部分代码——数据加载器、嵌入、前向循环——几乎不需要修改就可以与 LLaMA 一起工作。

#### 不同之处

1.  归一化

    +   GPT-2 在每个块输出前使用 LayerNorm。

    +   LLaMA 使用 RMSNorm，它仅使用激活的根均方值进行归一化（没有均值减法）。

    +   这略微减少了计算量并提高了稳定性。

1.  位置编码

    +   GPT-2 已经学习了位置嵌入。

    +   LLaMA 使用旋转位置嵌入（RoPE），它在注意力空间中旋转查询和键来编码位置。

    +   RoPE 在更长的上下文中表现更好。

1.  词汇量

    +   GPT-2 的词汇量大小为 50,257。

    +   LLaMA 使用不同的分词器（SentencePiece/BPE），拥有更大的词汇量，接近 32k 的 LLaMA-2。

1.  模型规模

    +   GPT-2 的最大参数量为 1.6B。

    +   LLaMA-2 和 LLaMA-3 的规模从 70B+扩展到 70B 以上。这使得分布式训练成为强制性的，混合精度和检查点作为标准。

#### 将*llm.c*适配到 LLaMA

如果你想要修改*llm.c*来近似 LLaMA，主要任务将是：

+   将 LayerNorm 替换为 RMSNorm 实现。

+   将 RoPE 添加到注意力机制中。这意味着修改构建 Q 和 K 向量的步骤，根据标记位置应用旋转。

+   用在所需词汇量上训练的 SentencePiece 分词器替换 GPT-2 分词器。

管道中的其余部分——优化器、调度器、数据加载器、多 GPU 支持——仍然有效。

#### 为什么这很重要

通过在 GPT-2 的背景下研究 LLaMA，你会发现现代 LLMs 并非完全陌生。它们是在相同的 transformer 背骨上的进化改进。认识到这些小的架构变化（RMSNorm，RoPE，缩放）有助于揭示为什么新模型优于旧模型，并展示了你需要在 *llm.c* 中进行哪些调整以探索 GPT-2 之外的内容。

#### 尝试自己动手做

1.  在 C 中通过修改 *llm.c* 中的 LayerNorm 代码实现 RMSNorm。

1.  在注意力内核中添加 RoPE 的简化版本，并在 Tiny Shakespeare 上运行它。

1.  将 GPT-2 的分词器替换为 SentencePiece 模型，并在自己的数据集上训练一个类似 LLaMA 的小型模型。

1.  比较 LayerNorm 和 RMSNorm 之间的训练稳定性——损失曲线看起来是否不同？

1.  研究 GPT-2 与小型 LLaMA 风格变体的内存使用情况，并查看缩放行为。

#### 吸收要点

通过 *llm.c* 探索 LLaMA 显示了代码库的真正灵活性。只需进行一些有针对性的更改——归一化、位置编码、分词器——你就可以从复制 GPT-2 转变为实验现代 LLMs 的构建块。这使得 *llm.c* 不仅仅是一个模型的学习工具，而且是理解整个 transformer 血统的基础。

### 89. 移植指南：C → Go/Rust/Metal

*llm.c* 代码库是用纯 C 编写的，以实现最大程度的可读性和最小化依赖。但在实践中，许多开发者想要尝试其他语言或平台——例如，编写 Go 或 Rust 版本以获得更好的工具支持，或者针对苹果的 Metal API 在 Mac 上实现 GPU 加速。移植不仅仅是复制粘贴的练习；它需要仔细思考如何在不同的生态系统中映射低级内存、数学运算和并行性。

#### 为什么需要移植？

+   Go：强大的并发模型（goroutines，channels），适合构建训练服务或分布式实验。

+   Rust：无需垃圾回收的内存安全和性能，非常适合编写可靠的数值内核。

+   Metal（苹果）：在 macOS/iOS 上的 GPU 加速，如果你想在苹果硅上高效训练或运行模型，这是必须的。

每个生态系统都有其优势，使得 *llm.c* 的概念更容易接近或更适用于生产。

#### 映射核心组件

让我们看看 *llm.c* 的关键部分是如何翻译的：

| 组件 | C (llm.c) | Go 等价 | Rust 等价 | Metal 等价 |
| --- | --- | --- | --- | --- |
| 内存分配 | `malloc`，`calloc` | `make`，切片，`unsafe.Pointer` | `Vec<T>`，`Box`，如果需要 `unsafe` | 在 GPU 上分配的缓冲区 |
| 数学内核 | 手动循环，OpenMP | 循环或 cgo 绑定到 BLAS | 带迭代器的循环，Rayon 用于 CPU | Metal 计算着色器 |
| 分词器 | `fread` 二进制文件 | 标准文件 I/O，`encoding/json` | `serde`，二进制读取 | 在 CPU 上进行预处理，输入到 GPU |
| 训练循环 | for 循环，结构体 | dataloader + trainer 的 goroutines | 异步任务，channels | CPU 驱动程序，GPU 内核 |
| 并行性 | `#pragma omp` | goroutines + 同步原语 | Rayon 或显式线程 | Metal 中的 Warp/线程组 |

#### 示例：Rust 中的 LayerNorm

这里是一个 Rust 草图，展示了 LayerNorm 的前向传播可能的样子：

```c
[](#cb176-1)fn layernorm_forward(out: &mut [f32], inp: &[f32], weight: &[f32], bias: &[f32], c: usize) {
[](#cb176-2)    let mean: f32 = inp.iter().sum::<f32>() / c as f32;
[](#cb176-3)    let var: f32 = inp.iter().map(|x| (x - mean).powi(2)).sum::<f32>() / c as f32;
[](#cb176-4)    let rstd = 1.0 / (var + 1e-5).sqrt();
[](#cb176-5)
[](#cb176-6)    for i in 0..c {
[](#cb176-7)        let norm = (inp[i] - mean) * rstd;
[](#cb176-8)        out[i] = norm * weight[i] + bias[i];
[](#cb176-9)    }
[](#cb176-10)}
```

这看起来与 C 代码惊人地相似，但受益于 Rust 的类型安全和缺乏手动内存管理错误。

#### 示例：Metal 中的注意力内核

Metal 会以不同的方式处理注意力——你将使用`.metal`语言编写计算着色器：

```c
kernel void attention_forward(
    device float* out [[buffer(0)]],
    device float* qkv [[buffer(1)]],
    uint id [[thread_position_in_grid]]
) {
    // compute a dot product between query and key vectors
    // accumulate into out, using threadgroup memory for efficiency
}
```

这不是逐行移植，但它展示了概念——将查询与键相乘、应用 softmax、加权值——在实现转移到 GPU 领域时保持不变。

#### 你将面临的挑战

+   数值库：C 通常依赖于 BLAS/LAPACK 或直接编写循环。在 Go 和 Rust 中，你将绑定到这些库或重新实现它们。

+   性能可移植性：让代码在 CPU 和 GPU 上都能快速运行并不简单。在 C 中使用 OpenMP 的代码不会直接转换。

+   分词器兼容性：确保分词与字节对字节匹配是至关重要的。任何不匹配都可能破坏训练的可重复性。

#### 为什么这很重要

移植`*llm.c*`迫使你理解代码的每一部分在做什么——你不能仅仅依赖于`torch.nn.LayerNorm`或`torch.nn.MultiheadAttention`。这使得它成为真正学习变换器的优秀练习，同时也为你提供了在不同环境中的实际实现。

#### 尝试自己动手做

1.  在 Go 或 Rust 中重新实现一个内核（如 LayerNorm 或 matmul）。与 C 版本进行测试。

1.  编写一个最简的 Metal 内核，用于向量加法，然后扩展到矩阵乘法。

1.  构建一个 Rust 分词器读取器，它加载`gpt2_tokenizer.bin`并将 ID 解码回文本。

1.  比较训练速度：C 与 OpenMP、Rust 与 Rayon、Go 与 goroutines。

1.  首先尝试移植前向传播——推理比训练更容易，因为你不需要反向传播。

#### 吸取的经验

`*llm.c*`的设计本质上具有可移植性——它不会隐藏在模糊的框架后面。将其移植到 Go、Rust 或 Metal 不仅仅是关于性能或语言偏好。这是在真正理解了变换器算法之后，向自己证明该算法是通用的，并且你可以在任何地方实现它。

### 90. 保持仓库最小化和清洁

`*llm.c*`的一个定义性特征是其极简主义。仓库避免了大型框架的复杂扩展，而是坚持使用一个小巧、易读的核心。这种设计选择不是偶然的——它是一种哲学。通过保持代码库小而干净，贡献者可以专注于理解变换器的基本原理，而不是在数千行样板代码中导航。

#### 极简主义哲学

+   可读性优于性能：虽然生产框架如 PyTorch 或 TensorFlow 会积极优化，但`*llm.c*`故意牺牲了一些性能以换取清晰度。一个普通的 C 循环比隐藏在宏后面的优化 CUDA 调用链更容易研究。

+   可移植性：较小的代码库可以更容易地移植到新的环境（如 Go、Rust、Metal）中，而无需引入数十个依赖项。

+   以学习为先的设计：每一行代码都有一个明确的目的。没有“以防万一”的抽象。

这种理念使`llm.c`成为既是一个工作训练框架，也是一个教育资源。

#### 如何强制执行简约主义

1.  平坦的结构：仓库避免了深层次的目录层次结构。大多数文件直接位于`llmc/`或`dev/`下。

1.  除非是关键性的，否则不要使用外部库：你会看到 OpenMP、cuBLAS 或 cuDNN 用于性能，但不会出现庞大的依赖链。

1.  一个功能，一个文件：分词、数据加载、调度器和采样器各自有自己的小型 C 文件。这防止了“神文件”中内容过于集中。

1.  一致的命名：函数和结构体使用清晰、描述性的名称（例如，`dataloader_next_batch`、`tokenizer_decode`），这样读者就不会迷失方向。

#### 实际示例

如果你打开`train_gpt2.c`，你会发现它构建模型、初始化数据加载器并运行训练循环。它不会尝试处理每个可能的模型配置、数据集格式或分布式场景。这些属于专门的文件或外部工具。

同样，`llmc/utils.h`仅定义了绝对必要的内容：安全的文件 I/O 包装器（`fopenCheck`、`freadCheck`）和内存分配器。它没有包含与训练 GPT-2 无关的通用辅助工具。

#### 为什么这很重要

最小化的仓库降低了入门门槛。初学者可以从`main()`函数开始追踪执行，直到优化器更新，而无需绕道。研究人员可以分叉代码并修改它，而无需担心破坏数十个相互连接的模块。即使是高级开发者也能从中受益，因为简单性迫使他们在算法推理上保持清晰。

#### 尝试自己动手做

1.  在仓库中挑选任何文件并计算它的行数。大多数文件行数都在几百行以下。与 PyTorch 或 TensorFlow 中类似的文件相比。

1.  尝试添加一个新功能——比如，不同的激活函数。注意，由于结构清晰，将其插入是多么容易。

1.  探索如果使仓库“更重”会发生什么。添加过多的辅助工具、抽象或配置。这会使代码更难阅读？

1.  练习向朋友解释训练循环。如果仓库简单，你应该能够引导他们了解细节。

#### 吸收要点

保持`llm.c`最小化不仅仅是节省代码行数。它关乎保持清晰性，确保可重复性，并使仓库成为任何从好奇的学习者到经验丰富的工程师都能打开文件并理解正在发生什么的地方。简单性是关键，这也是为什么`llm.c`在臃肿的机器学习框架世界中成为罕见且宝贵的资源。

## 第十章：重现、社区和路线图

### 91. 在单节点上重现 GPT-2 124M

对于任何探索*llm.c*的人来说，第一个主要里程碑是复制 GPT-2 124M 的训练，这是 GPT-2 家族中最小的版本。这个模型大约有 1240 万个参数，足够有趣，但仍然足够小，可以在单个现代 GPU 上训练——或者甚至可以缓慢地在 CPU 上用于演示目的。

#### 为什么从 124M 开始？

GPT-2 有多种尺寸：124M、355M、774M 和 1.6B 参数。训练最大的版本需要 GPU 集群和大量的计算预算。然而，124M 版本可以舒适地安装在消费级 GPU 上，例如 NVIDIA 3090，如果你有耐心，甚至可以在笔记本电脑 CPU 上运行。它是 transformer 复制的“hello world”：小巧、易于接近，而且仍然真实。

#### 训练设置看起来像什么

训练 GPT-2 124M 涉及几个关键步骤：

1.  模型配置 124M 的配置已经嵌入到*llm.c*中，具有 12 层、12 个头、隐藏维度为 768 和最大序列长度为 1024。

    ```c
    [](#cb178-1)GPT2Config config = {
    [](#cb178-2)    .max_seq_len = 1024,
    [](#cb178-3)    .vocab_size = 50257,
    [](#cb178-4)    .num_layers = 12,
    [](#cb178-5)    .num_heads = 12,
    [](#cb178-6)    .channels = 768
    [](#cb178-7)};
    ```

1.  数据集你可以训练小型数据集，如 Tiny Shakespeare 或 Tiny Stories 进行快速运行。为了更真实地复制，你需要接近 OpenWebText 的数据。数据使用 GPT-2 标记器（`gpt2_tokenizer.bin`）进行标记化，并存储在`.bin`文件中。

1.  批大小和序列长度一个常见的设置是`B = 8, T = 1024`，这意味着 8 个序列，每个序列长度为 1024 个标记。根据可用内存进行调整。

1.  默认的优化器是 AdamW，学习率约为`3e-4`。可以启用预热和余弦衰减调度，以匹配已发布的 GPT-2 训练曲线。

#### 实践中可以期待什么

在 CPU 上，训练单步可能需要几秒钟。在单个带有 CUDA 的 GPU 上，每步可能不到 100 毫秒。对于 124M 参数，从 OpenWebText 大小的数据集从头开始训练仍然需要几天，但你可以在更小的数据集上几小时内复制关键动态（损失曲线、样本生成）。

例如，你可能看到的日志类型如下：

```c
step 0: train loss 6.9321 (took 421.5 ms)
step 100: train loss 4.2137 (took 95.2 ms)
step 200: train loss 3.8914 (took 94.8 ms)
val loss 3.7725
```

即使在几百步之内，模型也开始生成类似英语的文本，而不是纯噪音。

#### 为什么这很重要

复制 GPT-2 124M 是一个信心检查。如果你的设置正确，你的损失曲线应该与原始 OpenAI 论文或 PyTorch 参考实现中的曲线相匹配。这验证了*llm.c*是一个忠实复制品，而不仅仅是玩具。这也教会了你即使是最小的 GPT-2 模型也需要多少计算和数据，帮助你建立对扩展定律的直觉。

#### 试试看

1.  在 Tiny Shakespeare 上训练 GPT-2 124M 1000 步。观察生成的文本是如何改进的。

1.  将批大小`B`从 8 改为 4。训练速度和稳定性会发生什么变化？

1.  在 CPU 和 GPU 上运行训练。比较每个步骤所需的时间。

1.  跟踪训练和验证损失。注意当模型开始过拟合时，它们是如何发散的。

#### 吸收要点

GPT-2 124M 的运行不仅仅是演示——它是你进入真实 LLM 训练的门户。你看到数据、模型大小、优化器和硬件是如何结合在一起的。一旦你掌握了这种复制的技巧，你就可以向更大的模型和更复杂的设置迈进。这是*llm.c*中其他一切的基础。

### 92. 复制 GPT-2 355M（约束和技巧）

一旦成功复制了 GPT-2 124M，下一步合乎逻辑的步骤就是扩展到 GPT-2 355M。这个版本大约大三倍，大约有 355 百万个参数。它引入了在较小规模下不会出现的新挑战：内存压力、训练稳定性和计算成本。

#### 模型配置

355M 模型仍然使用 1024 个 token 作为最大序列长度和相同的 GPT-2 分词器。区别在于网络的深度和宽度：

+   层数：24 个 transformer 块而不是 12 个

+   隐藏维度（通道）：1024 个而不是 768 个

+   头数：16 个而不是 12 个

总参数数量从~124M 增加到~355M。这意味着每一步不仅仅是数学量增加三倍，还需要更多的内存来存储参数、梯度和优化器状态。

#### 计算挑战

对于 124M，单个具有 8-12GB VRAM 的 GPU 就足够了。对于 355M，你需要至少 16GB 才能舒适地运行，序列长度为 1024，批大小为 8。在较小的 GPU 上，你很快就会遇到“CUDA 内存不足”的错误。

一个技巧是减少批大小（B）或序列长度（T）。例如，你可能会使用`(B=4, T=512)`来代替`(B=8, T=1024)`进行训练。这减半了内存占用，但仍然允许你测试扩展动态。

另一种方法是使用梯度累积：通过运行多个小步骤并累积梯度来模拟更大的批大小。

#### 训练稳定性

较大的模型对超参数更敏感。AdamW 优化器仍然有效，但学习率调度变得更为重要。许多实践者使用：

+   学习率：峰值约为~3e-4

+   预热步骤：几千步

+   余弦衰减：逐渐减小学习率

如果你跳过预热，较大的模型可能会早期发散（损失爆炸而不是减少）。

#### 可行性技巧

1.  混合精度训练（FP16 或 BF16）：内存使用量几乎减半。在*llm.c*的 CUDA 路径中受支持。

1.  激活检查点：在反向传播期间重新计算激活以节省内存。速度较慢，但允许你拟合更大的模型。

1.  小数据集运行：在 Tiny Shakespeare 或 Tiny Stories 上训练以验证设置，然后扩展到类似 OpenWebText 的数据。

#### 示例日志

短时间内运行 GPT-2 355M 可能看起来像：

```c
step 0: train loss 7.1032 (took 321.8 ms)
step 50: train loss 5.4231 (took 310.4 ms)
step 100: train loss 4.8217 (took 311.0 ms)
val loss 4.7322
```

损失下降速度比 124M 慢，因为模型有更多的学习容量，但也需要更多的数据来泛化。

#### 为什么这很重要

355M 是进入“中等规模”LLM 的第一步。你开始感受到主导更大模型的瓶颈：VRAM 限制、训练速度和超参数调整。解决这些问题将为你准备 774M 和 1.6B 实验，在这些实验中，这些问题变得更加明显。

#### 试试看

1.  使用批大小 4 和序列长度 512 来训练 GPT-2 355M。记录每个步骤所需的时间。

1.  尝试不同的预热步骤：一次预热=0，一次预热=2000。比较稳定性。

1.  如果你有 CUDA 兼容的 GPU，请启用混合精度。测量前后内存使用情况。

1.  尝试在 Tiny Shakespeare 和 Tiny Stories 上训练。模型在较小的数据集上是否更快地过拟合？

#### 吸收要点

复现 GPT-2 355M 主要是学习如何扩展有限资源。你会发现内存节省技巧、学习率调度的重要性以及数据规模的作用。这是一次资源管理的实际练习——就像在训练今天的十亿参数模型时面临的真正挑战一样。

### 93. 复现 GPT-2 774M（扩展）

GPT-2 的 774M 参数版本通常被称为“GPT-2 Medium”。这是训练从个人实验转变为小型研究项目的转折点。它比 124M 基线大六倍左右，大约是 355M 的两倍。运行它需要仔细规划硬件、内存和软件技巧。

#### 模型配置

对于 774M，架构再次扩展：

+   层数（转换器块）：36

+   隐藏大小（通道）：1280

+   注意力头：20

+   最大序列长度：1024（未变）

这种规模的增长增加了参数存储和训练过程中必须保持的激活数量。仅优化器状态（AdamW 的*m*和*v*向量）就消耗了几个 GB。

#### 硬件要求

从头开始运行 GPT-2 774M 通常需要 24GB VRAM 或更多的 GPU（例如，NVIDIA RTX 3090/4090、A100 或 H100）。使用较小的卡，除非你积极降低批大小并使用激活检查点等技术，否则你几乎肯定会遇到内存错误。

在 CPU 上，从技术上讲可以进行训练，但速度太慢，不切实际——在 GPU 上只需毫秒的步骤可能需要几秒甚至几分钟。

#### 实际限制

1.  批大小：在实际应用中，你可能需要将`B`降低到 2 甚至 1，以适应序列长度 1024。

1.  梯度累积：模拟更大的批大小并稳定训练的必备条件。

1.  混合精度：FP16 或 BF16 可以将内存减少约一半，而不会对收敛造成太大影响。

1.  检查点：重新计算中间结果而不是存储它们，以时间换取内存。

#### 训练动态

GPT-2 774M 的损失曲线下降更加稳定，需要更多的数据才能达到其潜力。如果你只在 Tiny Shakespeare 或 Tiny Stories 上训练，模型会很快过拟合：对于如此小的数据集，模型太大。为了有意义地复现，你需要一个与 OpenWebText 规模相似的数据集。

精神检查运行的训练日志可能看起来像：

```c
step 0: train loss 8.0123 (took 512.4 ms)
step 50: train loss 5.7892 (took 490.7 ms)
step 100: train loss 5.1428 (took 495.1 ms)
val loss 5.0039
```

注意到损失开始较高（由于更大的随机初始化空间），但一旦开始训练，就会可预测地下降。

#### 为什么这很重要

774M 模型是规模法则变得明显的甜蜜点。与 124M 和 355M 相比，它具有更好的泛化能力，生成更流畅的文本，并展示了参数增长的好处。但它也显示了基础设施的重要性：如果没有仔细管理，几乎不可能在消费级硬件上训练此模型。

这也是分布式训练（在第八章中介绍）变得相关的地方，因为单个 GPU 通常不足以进行有效的扩展。

#### 尝试自己操作

1.  使用`(B=1, T=1024)`和 8 步梯度累积训练 GPT-2 774M。观察它是如何模拟 8 个批次的。

1.  比较 FP32 与混合精度训练。测量内存使用和速度。

1.  在 Tiny Stories 上运行一个简短的微调实验。观察模型如何快速记住数据集。

1.  绘制训练和验证损失曲线。较大的模型是否比 355M 更快或更慢地过拟合？

#### 吸收要点

复制 GPT-2 774M 是将扩展到真实研究工作负载的领域。你面临着严重的内存限制、数据集要求和计算成本。但如果你成功了，你将亲身体验为什么机器学习社区一直在推动向十亿参数模型发展：更大的网络解锁了明显更强的能力，即使使用相同的架构。

### 94. 在 8×H100 上复制 GPT-2 1.6B（24 小时运行）

最大的 GPT-2 模型，拥有 16 亿个参数，代表了原始 GPT-2 家族的上限。从头开始训练此模型不是你可以在单个工作站上随意尝试的事情。它需要集群级资源、分布式训练软件和仔细调整以保持一切稳定。在本节中，我们将探讨 1.6B 模型的特殊之处、所需的基础设施以及完整复制的可能外观。

#### 模型配置

从 774M 到 1.6B 的跳跃将参数数量翻倍，使网络既更深又更宽：

+   层数（转换器块）：48

+   隐藏大小（通道）：1600

+   注意力头：25

+   序列长度：仍然是 1024

在这些维度下，每次正向和反向传递都需要大量的内存和计算。仅存储 FP32 参数就需要大约 6.4 GB。一旦添加梯度、优化器状态（AdamW 的*m*和*v*）和激活，内存占用很容易超过 100 GB。

#### 硬件设置

要真实地复制此模型，你需要访问高端加速器，如 NVIDIA A100s 或 H100s。一个常见的基线是每个 80 GB 的 8 个 GPU。使用这种设置，在 24 小时内训练 GPT-2 1.6B 是可能的，前提是高效利用。

没有多 GPU，训练是不切实际的。即使你能够以某种方式将模型拟合到一个 GPU 上，运行时间也会是几周或几个月。

#### 分布式训练

主要策略是数据并行：每个 GPU 处理不同的迷你批次数据，梯度通过 NCCL 的全归约平均到所有设备上。*llm.c*中的代码路径通过 MPI 集成支持这一点，因此你可以从单 GPU 扩展到多节点设置。

训练循环看起来几乎与较小模型相同，但幕后，每个参数更新都是跨设备协调的。

#### 训练动态

与较小模型相比，1.6B 的损失曲线更加平滑。有了足够的数据，模型会继续在 774M 开始平台期的地方改进。这是原始 GPT-2 论文中的一个关键见解：缩放定律成立，性能随着大小、数据和计算量的增加而可预测地提高。

分布式运行的日志可能看起来像这样：

```c
[rank 0] step 0: train loss 8.5029 (took 312.6 ms)
[rank 0] step 50: train loss 6.3121 (took 308.2 ms)
[rank 0] step 100: train loss 5.7210 (took 309.4 ms)
[rank 0] val loss 5.5347
```

注意速度：每个步骤仍然只需要几百毫秒，这得益于多个 H100 之间的并行性。

#### 为什么这很重要

重现 GPT-2 1.6B 与其说是训练一个有用的模型，不如说是理解大型语言模型的扩展挑战。这个练习展示了随着模型的增长，计算、内存和分布式基础设施如何成为限制因素。它还说明了为什么现代研究实验室会围绕多 GPU 和多节点扩展设计整个管道。

#### 尝试自己操作

1.  通过减少模型大小但使用相同的并行训练设置，用更少的资源模拟多 GPU 运行。例如，在 2 个 GPU 上训练 GPT-2 124M 以练习工作流程。

1.  通过梯度累积实验来模拟即使在较小的集群上也能模仿大型全局批次大小。

1.  尝试启用和禁用混合精度。观察内存使用如何随着 FP16/BF16 而大幅下降。

1.  并排绘制 124M、355M、774M 和 1.6B 的验证损失曲线。注意较大模型如何更长时间地保持改进。

#### 吸收要点

GPT-2 1.6B 的重现是*llm.c*的标志性项目。它迫使你结合你所学的所有内容：数据管道、优化器、调度器、分布式训练和系统级调试。尽管很少有人会真正训练 1.6B，但了解这需要什么可以让你窥见最先进 LLM 背后的工程，并为你参与更大型的现代模型做好准备。

### 95. 仅 CPU 微调演示（微型莎士比亚）

并非每个人都能访问到强大的 GPU 或大型计算集群。*llm.c*的一个优势是它提供了一个干净、最小化的仅 CPU 路径，让你能够运行真正的实验——即使它们规模小且速度慢。探索这一点的实际方法是通过对小型数据集（如微型莎士比亚）进行微调，该数据集仅有大约 1MB 的文本。

#### 为什么是微型莎士比亚？

Tiny Shakespeare 是机器学习中的一个经典玩具数据集。它足够小，可以放入内存，同时包含丰富的词汇、角色和结构。在这样一个数据集上微调 GPT-2 只需几千步就能模仿莎士比亚风格。这并不是关于构建最先进的模型——这是关于在适度硬件上看到训练过程从头到尾的工作。

#### 设置

微调过程使用相同的`train_gpt2.c` CPU 路径，但步骤更少、批大小更小、序列长度更低，以保持快速。一个典型的设置看起来像这样：

```c
[](#cb183-1)int B = 4;    // batch size
[](#cb183-2)int T = 64;   // sequence length
[](#cb183-3)int steps = 1000;  // training iterations
```

数据集使用`gpt2_tokenizer.bin`进行一次标记，并存储在二进制的`.bin`文件中：

```c
dev/data/tinyshakespeare/tiny_shakespeare_train.bin
dev/data/tinyshakespeare/tiny_shakespeare_val.bin
```

这些文件只有几兆字节，非常适合快速实验。

#### 训练动态

当你在 Tiny Shakespeare 上进行微调时，训练日志可能看起来像这样：

```c
step 0: train loss 6.9312 (took 1220.4 ms)
step 50: train loss 4.3217 (took 1175.1 ms)
step 100: train loss 3.7120 (took 1169.4 ms)
val loss 3.5894
```

在几百步之内，损失迅速下降。当你运行到 1000 步时，模型开始生成看起来像是莎士比亚风格的文本——包括古词、不寻常的标点符号和韵律模式。

#### 示例输出

这里是一个经过微调运行的简短样本：

```c
generating:

ROMEO: But hark, what light through yonder window breaks?
JULIET: Ay me! the time is near, and I must away.
ROMEO: Fear not, sweet love, for night shall bring us peace. 
```

它并不完美，但捕捉到了莎士比亚的“感觉”，考虑到数据集很小且计算有限，这是令人瞩目的。

#### 为什么这很重要

仅使用 CPU 的 Tiny Shakespeare 演示证明了 LLMs 不仅仅适用于大型数据中心。通过最小化的设置，你可以观察模型学习、生成文本，并对数据集进行过拟合。这种动手实践有助于理解训练的作用、损失曲线的行为以及为什么扩展很重要。

#### 亲自尝试

1.  将序列长度从 64 改为 128。训练速度和损失如何变化？

1.  将训练步骤减少到 200。在生成中你还能看到莎士比亚风格的文本吗？

1.  将批大小增加到 8。损失曲线会变得更平滑还是更嘈杂？

1.  再次微调，但从头开始初始化（随机权重）。将结果与从预训练的 GPT-2 124M 微调的结果进行比较。

#### 吸取的经验

仅使用 CPU 进行训练的 GPT-2 在 Tiny Shakespeare 上进行微调是一个简单而强大的演示。你不需要 GPU 就能理解 transformers 的机制。即使是普通的笔记本电脑也能教你如何进行训练，为什么会出现过拟合，以及 LLMs 如何适应新领域。这是一个提醒，学习机器学习的最好方法是亲自动手进行实验——即使是小规模的实验。

### 96. 运行的成本和时间估算

与大型语言模型一起工作的最令人大开眼界的部分之一是意识到训练是多么昂贵和耗时。虽然*llm.c*使得使用简单的、最小的 C 代码运行所有大小的模型成为可能，但随着从 GPT-2 124M 扩展到 GPT-2 1.6B，硬件需求迅速增长。理解成本和时间估算有助于设定现实的期望，无论你是在笔记本电脑 CPU 上运行，还是在租用的加速器集群上运行。

#### 影响训练时间的关键因素

几个因素决定了训练所需的时间和成本：

+   模型大小（参数）：更大的模型意味着每前向/反向传递需要更多的乘法运算，以及更多的参数、梯度和优化器状态内存。

+   批大小和序列长度：增加任一项都会使每步的工作量增加。

+   数据集大小：更大的数据集需要更多的步骤来完成一个 epoch。

+   硬件速度：CPU 比 GPU 慢得多；对于这种工作负载，高端 GPU（如 H100s）可以比 CPU 快 100 倍。

+   并行性：多 GPU 或多节点设置让你可以分割工作，减少每步时间。

#### 大概时间估计

这里是一个简化的视图，展示了根据硬件和设置，训练 GPT-2 模型可能需要多长时间。这些是非常粗略的估计，假设在像 OpenWebText 这样的数据集上完成全部训练：

| 模型 | 参数 | 硬件 | 每步时间 | 总训练时间 |
| --- | --- | --- | --- | --- |
| GPT-2 124M | ~124M | 笔记本 CPU | 1–2 秒 | 数月 |
| GPT-2 124M | ~124M | 单个 RTX 3090 | ~100 ms | ~2–4 周 |
| GPT-2 355M | ~355M | 单个 RTX 3090 | ~300 ms | ~6–8 周 |
| GPT-2 774M | ~774M | 2× A100 40GB | ~200 ms | ~4–6 周 |
| GPT-2 1.6B | ~1.6B | 8× H100 80GB | ~300 ms | ~24 小时 |

这些数字显示了为什么扩展很重要。更大的模型不仅需要每步更多的计算能力，还需要更多的数据来发挥其潜力。这意味着除非你有顶级 GPU 的集群，否则总成本会大幅增加。

#### 云环境中的成本

如果你在像 AWS、GCP 或 Azure 这样的云服务提供商上运行这些实验，成本可能会迅速增加。例如：

+   NVIDIA A100 40GB 实例每小时大约花费 2–3 美元（spot 价格可能更便宜）。

+   训练 GPT-2 124M 一周可能需要 500–1,000 美元。

+   在 8× H100s 上训练 GPT-2 1.6B 24 小时可能需要 5,000–10,000 美元，具体取决于提供商。

这也是为什么许多研究人员首先在小数据集和小模型上测试代码路径，然后在绝对必要时才进行扩展的原因。

#### 为什么这很重要

估算成本和时间可以防止挫败感和金钱浪费。它教会你在小规模（CPU 或 124M 运行）上进行原型设计，验证你的设置，然后才扩展到中等（355M、774M）或大型（1.6B）模型。它还让你对 OpenAI 原始 GPT-2 训练运行中投入的工程和预算有一个现实的了解。

#### 尝试自己操作

1.  测量你的硬件上 GPT-2 124M 单个训练步的时间。乘以 1000 来估计 1000 步的训练时间。

1.  将批大小减半。每步时间是否线性减少，还是不是？

1.  如果你在家用机器上，运行一个短期的微调（例如，200 步）并测量电力成本。

1.  使用云 GPU 一小时。比较其成本和速度与本地 CPU。

#### 吸取的经验

训练大型语言模型需要在雄心、硬件和预算之间取得平衡。*llm.c*为你提供了从玩具演示到百亿参数复制的所有工具，但你很快就会看到为什么该领域已经转向大型实验室和共享基础设施。然而，通过周密的规划，你仍然可以通过运行较小的实验并谨慎地扩展它们来学到大量的知识。

### 97. 超参数扫描（`sweep.sh`）

让 GPT-2 这样的模型训练良好，不仅仅是编写代码或拥有足够的计算能力——还在于找到正确的超参数。这些包括学习率、批量大小、权重衰减、dropout 率以及调度器配置。一个对某个数据集或模型大小有效的设置可能对另一个完全无效。这就是超参数扫描发挥作用的地方：系统地尝试不同的配置，看看哪些能给出最佳结果。

#### `sweep.sh`的作用

在*llm.c*中，有一个简单的 shell 脚本叫做`sweep.sh`，旨在自动化超参数测试。它不是一个像 Ray Tune 或 Optuna 那样的复杂实验管理系统；相反，它是轻量级、透明且易于适应的。脚本通常看起来像是一个循环，遍历不同的学习率或批量大小，使用每个设置运行训练可执行文件，并记录输出。

一个非常简化的版本可能看起来像这样：

```c
[](#cb187-1)#!/bin/bash
[](#cb187-2)for lr in 1e-3 5e-4 1e-4
[](#cb187-3)do
[](#cb187-4)    echo "Running with learning rate $lr"
[](#cb187-5)    ./train_gpt2 -lr $lr -epochs 5 > logs/lr_$lr.txt
[](#cb187-6)done
```

这样，你可以用一条命令启动多个实验，然后比较验证损失以决定哪些超参数最佳。

#### 为什么扫描很重要

训练 Transformer 对超参数非常敏感。例如：

+   如果学习率过高，损失可能会爆炸。

+   如果过低，训练将会非常缓慢。

+   过多的权重衰减可能会损害性能，而太少的权重衰减可能会导致过拟合。

+   预热步骤的数量可以在稳定收敛和前几百次迭代失败之间产生差异。

与猜测不同，扫描可以帮助你看到模式。例如，你可能会发现`3e-4`对于 GPT-2 124M 是最佳选择，但 GPT-2 355M 更偏好`2e-4`。

#### 实践中的扫描示例

假设你想测试三个学习率和两个批量大小。你可以写一个嵌套循环：

```c
[](#cb188-1)for lr in 3e-4 2e-4 1e-4
[](#cb188-2)do
[](#cb188-3)    for B in 4 8
[](#cb188-4)    do
[](#cb188-5)        echo "Running with lr=$lr and batch_size=$B"
[](#cb188-6)        ./train_gpt2 -lr $lr -B $B -steps 500 > logs/lr_${lr}_B${B}.txt
[](#cb188-7)    done
[](#cb188-8)done
```

之后，你可以打开日志并比较每次运行结束时的验证损失。这为你提供了关于什么工作最好的数据驱动证据。

#### 为什么它很重要

超参数扫描是实际机器学习的基础。尽管*llm.c*是一个极简主义项目，但快速测试和比较运行的能力是至关重要的。它将训练从猜测转变为经验过程。你不仅仅希望你的模型会收敛——你需要在多个设置中验证它，并选择最佳方案。

#### 试试看

1.  在 Tiny Shakespeare 上对 GPT-2 124M 的`[1e-3, 3e-4, 1e-4]`学习率进行扫描。哪一个收敛得最快？

1.  尝试对序列长度（`T=32, 64, 128`）进行扫描。它如何影响速度和损失？

1.  比较有无权重衰减的运行。哪个对验证集的泛化更好？

1.  将扫描扩展到测试不同的调度器（余弦衰减与步长衰减）。

#### 吸收教训

超参数扫描是训练 LLM 的“实验肌肉”。它们教会你没有单一设置适用于所有情况，并且系统测试比直觉本身更有效。使用简单的脚本如`sweep.sh`，你可以以可重复的方式探索数十种设置，并建立信心，相信你的模型训练得尽可能好。

### 98. 验证评估和损失曲线

训练日志——每一步打印的损失值只是数字。要真正了解你的模型是否在学习，你需要验证这些数字，绘制它们，并在不同的运行中进行比较。分析评估和损失曲线的过程是机器学习中最重要技能之一。这是你了解模型是否收敛、过拟合或完全失败的方法。

#### 训练损失与验证损失

有两种损失曲线需要关注：

+   训练损失：在模型实际看到的训练批次上计算。

+   验证损失：在保留的数据集（如*llm.c*中的`*_val.bin`）上计算。

训练损失几乎总是稳步下降。验证损失是真正的测试：如果它与训练损失一起下降，则模型正在学习有用的模式。如果它在训练损失持续下降的同时停滞或增加，则模型正在过拟合。

#### 绘制损失曲线

尽管 llm.c 是一个纯 C 项目，但你可以将其训练日志重定向到文件，然后使用 Python + matplotlib 等工具进行可视化。例如：

```c
[](#cb189-1)./train_gpt2 > logs/run1.txt
```

然后用 Python 解析`logs/run1.txt`：

```c
[](#cb190-1)import re, matplotlib.pyplot as plt
[](#cb190-2)
[](#cb190-3)steps, train_loss = [], []
[](#cb190-4)for line in open("logs/run1.txt"):
[](#cb190-5)    match = re.match(r"step (\d+): train loss ([0-9.]+)", line)
[](#cb190-6)    if match:
[](#cb190-7)        steps.append(int(match.group(1)))
[](#cb190-8)        train_loss.append(float(match.group(2)))
[](#cb190-9)
[](#cb190-10)plt.plot(steps, train_loss, label="train loss")
[](#cb190-11)plt.legend()
[](#cb190-12)plt.show()
```

将验证损失添加到同一图表中使其更加有用：你会看到两条曲线及其随时间的关系。

#### 健康曲线的样子

+   早期阶段：训练和验证损失都迅速下降。

+   中期阶段：训练损失持续下降，验证损失下降较慢。

+   晚期阶段：训练损失可能继续下降，但验证损失稳定或开始上升。这是过拟合的点。

例如，一条好的曲线可能看起来像这样：

```c
step 0: train loss 6.92, val loss 6.85
step 100: train loss 4.31, val loss 4.52
step 200: train loss 3.78, val loss 4.01
step 500: train loss 2.91, val loss 3.95
```

训练损失持续下降，但验证损失在步骤 500 左右停滞。这是停止训练或调整超参数的信号。

#### 为什么这很重要

损失曲线是了解模型行为的窗口。它们揭示了你的模型是否欠拟合（损失过高）、过拟合（训练和验证之间的差距过大）或稳定训练（两个损失值同时下降）。没有它们，你就像盲人一样——只是盯着没有上下文的数字。

#### 试试看

1.  在 Tiny Shakespeare 上训练 GPT-2 124M 500 步，并绘制训练和验证损失。

1.  将数据集大小减半。观察验证损失如何因过拟合而提前恶化。

1.  进行两个不同学习率的实验。绘制两条曲线并比较其稳定性。

1.  将绘图扩展到多个运行（例如，GPT-2 124M vs. 355M）以查看缩放效应。

#### 吸收要点

验证评估和损失曲线将原始日志转化为洞察。这有助于你决定何时停止训练，如何调整超参数，以及是否扩大规模是值得的。在 *llm.c* 中，尽管项目最小化，捕捉和绘制这些曲线是理解模型内部发生情况的最有效方法。

### 99. 未来工作：内核库，减少对 cuDNN 的依赖

在 *llm.c* 中的 CUDA 路径已经使用了 cuBLAS 和 cuDNN，这是 NVIDIA 的高性能数学库，用于处理矩阵乘法和注意力操作的重负载。这些库经过实战检验且速度极快，但它们也充当了一个“黑盒”：你调用它们，它们完成工作，你得到结果却看不到里面的内容。虽然这很方便，但它限制了灵活性，并使得进行新颖优化的实验变得更加困难。

因此，未来工作中最令人兴奋的领域之一是为 *llm.c* 构建一个轻量级的自定义内核库。这意味着用手写的 CUDA 内核替换 cuDNN 的一部分，用于执行注意力、归一化和激活函数等操作。

#### 为什么减少对 cuDNN 的依赖？

1.  透明度：使用自定义内核，你可以确切地看到操作是如何实现的，这对于学习和调试非常有用。

1.  灵活性：你可以实验新的想法（例如，替代注意力机制、稀疏技巧），而无需等待 cuDNN 支持。

1.  可移植性：cuDNN 是 NVIDIA 特有的。一个自定义内核库可以使将 *llm.c* 移植到其他后端（如 AMD GPU 的 HIP 或甚至 Apple 硅的 Metal）变得更加容易。

1.  性能调优：对于小型到中型模型，手动调优的内核有时可以超越通用库调用，因为它们是根据工作负载定制的。

#### 内核库可能包含的内容

为 *llm.c* 的内核库的第一个版本可能实现以下功能：

+   使用分块和共享内存的矩阵乘法（变换器的动力源泉）。

+   带有内置数值稳定性的 softmax 内核。

+   适用于正向和反向传递的 LayerNorm 内核。

+   注意将 matmul + masking + softmax 集成在一个融合操作中的注意力内核。

+   融合形式中的激活函数，如 GELU 或 ReLU。

例如，一个用于向量加法的非常简化的 CUDA 内核可能看起来像这样：

```c
__global__ void vec_add(float* a, float* b, float* c, int N) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < N) {
        c[i] = a[i] + b[i];
    }
}
```

虽然很简单，但这说明了这样一个观点：不是调用库，而是自己编写数学公式，并控制线程的启动和内存的访问方式。

#### 前进的道路

开发内核库是一个长期努力。它需要分析、基准测试和迭代调优。最初，自定义内核可能比 cuDNN 慢，但随着时间的推移，它们可以演变成一个紧凑的、用于变压器训练的构建块的教育库。

#### 为什么这很重要

减少对 cuDNN 的依赖不仅关乎性能，还关乎控制和可移植性。通过拥有自己的内核，你将获得在更多平台上运行 *llm.c*、测试新的研究想法以及了解 GPU 内部确切发生情况的自由。对于这样一个最小化、教育性的项目来说，这是自然的下一步。

#### 尝试自己动手做

1.  编写一个简单的 CUDA 内核，用于向量加法，就像上面的例子一样，并在 C 程序中调用它。将其性能与 `cublasSaxpy` 进行比较。

1.  将 *llm.c* 中的一个小的部分（例如，softmax）替换为自定义内核。检查输出是否与 cuDNN 匹配。

1.  在小输入大小上对自定义内核与 cuDNN 进行基准测试。cuDNN 是否仍然占主导地位？

1.  尝试将你的内核移植到 HIP（用于 AMD）或 Metal（用于 Apple GPU）。

#### 总结

构建内核库是将消费者从黑盒库转变为透明、灵活工具的过渡。这是一项大量工作，但它将 *llm.c* 从仅仅是一个使用 LLMs 的项目转变为一个学习 GPU 编程深层次内部结构的平台。通过减少对 cuDNN 的依赖，你打开了真正端到端控制模型训练的大门。

### 100. 社区、GitHub 讨论区和建议的学习路径

大型语言模型很复杂，但 *llm.c* 的一个目标就是让它们易于访问。代码干净、简洁、易于接近——但学习并不仅仅止于阅读代码。围绕 *llm.c* 的更广泛社区在帮助人们理解、实验和成长方面发挥着巨大作用。本节突出了如何与他人建立联系、如何贡献以及如何构建自己的学习之旅。

#### GitHub 作为中心

*llm.c* 讨论的中心地点是 [GitHub 仓库](https://github.com/karpathy/llm.c)。在那里，你会发现：

+   问题：用户提问、报告错误或提出改进的地方。

+   讨论区：一个公开的论坛，用于分享结果、提出“我该如何……？”问题以及比较训练日志。

+   提交拉取请求：从错误修复到新功能，通常伴随着有价值的代码审查。

即使只是浏览问题和讨论，也可能是一种教育体验。你可能提出的许多问题——关于 CUDA 错误、数据集准备或优化器的怪癖——已经有人提出并得到了解答。

#### 社区的价值

单独从事语言模型工作可能会感到压力重重。通过参与社区，你：

+   看看其他人如何使用不同的数据集和硬件进行实验。

+   学习常见问题的故障排除策略（例如，内存不足错误）。

+   扩展的灵感——如自定义内核、新的优化器或非 GPT 架构。

+   寻找合作伙伴进行超出个人能力范围的实验。

#### 建议的学习路径

由于 *llm.c* 是最小化的，它非常适合作为自学工具。以下是一个建议的学习路径，以构建你的知识：

1.  从小做起

    +   使用仅 CPU 模式在 Tiny Shakespeare 上训练 GPT-2 124M。

    +   检查训练日志并观察损失如何减少。

    +   生成文本并观察模型如何快速记忆。

1.  进入 CUDA

    +   切换到`train_gpt2.cu`并使用 GPU 加速进行训练。

    +   尝试混合精度（`FP16`/`BF16`）并观察内存节省。

1.  扩展规模

    +   尝试在你的硬件（或云 GPU）上运行 GPT-2 355M 或 774M。

    +   学习如何使用梯度累积和检查点。

1.  实验

    +   修改训练循环：尝试新的调度器，调整优化器超参数。

    +   添加你自己的数据集（例如，你个人的文本语料库）。

1.  探索内部结构

    +   在`train_gpt2.c`中逐步进行正向和反向传播。

    +   编写小实验来隔离关键概念（例如，LayerNorm）。

1.  加入讨论

    +   在 GitHub Discussions 上分享你的结果。

    +   贡献改进，即使是小的改进——比如文档修复。

#### 为什么这很重要

学习 LLM 内部结构的旅程不仅仅是阅读代码——它关于主动实践、提问和与他人比较经验。社区提供了加速学习和保持动力的反馈循环。

#### 亲自尝试

1.  克隆`*llm.c*`仓库并探索开放的问题。你能为别人解答一个问题吗？

1.  运行一个训练实验并在 GitHub Discussions 上分享你的损失曲线。

1.  通过拉取请求贡献一个小改进（比如一个新的数据集脚本）。

1.  创建自己的“学习日志”来跟踪实验，就像一个公开的笔记本。

#### 摘要

`*llm.c*`不仅仅是一个代码库——它是一个邀请你加入学习社区的邀请。通过参与 GitHub、尝试实验和分享你的结果，你从被动读者转变为积极参与者。这就是最深层次的理解来源：一起学习，而不是独自一人。
