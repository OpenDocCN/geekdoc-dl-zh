<!--yml

类别：未分类

日期：2024-07-01 18:16:59

-->

# 加速持久性神经网络（Daniel Lo）：ezyang 的博客

> 来源：[`blog.ezyang.com/2017/12/accelerating-persistent-neural-networks-at-datacenter-scale-daniel-lo/`](http://blog.ezyang.com/2017/12/accelerating-persistent-neural-networks-at-datacenter-scale-daniel-lo/)

下面是[Daniel Lo](https://www.microsoft.com/en-us/research/people/dlo/)在[NIPS'17](https://nips.cc/Conferences/2017/Schedule?showEvent=8774)的[ML 系统研讨会](https://www.microsoft.com/en-us/research/blog/microsoft-unveils-project-brainwave/)的讲话记录。

* * *

部署和提供云规模的加速深度神经网络（DNN）。正如我们所见，DNN 已经实现了惊人的应用。架构在计算机视觉、语言翻译和语音识别方面实现了最先进技术。但是在大规模交互服务中存在延迟、成本和功耗限制，这是一个挑战。此外，DNN 的大小和复杂性正在增加。

我们看到了解决这一问题的初创企业的堆积如山。研究团队开发了 DNN 处理单元（DPUs），定制硬件解决方案来实现高吞吐量的有效服务 DNN。我们将它们分类为两类：快速 DPUs，其中算法和应用程序必须在设计时固定，因为它们在制造 ASIC，或者软 DPUs，FPGA。但是对于软 DPUs，我们尚未看到它们大规模部署。

为了解决这个问题，我们一直在进行 Project BrainWave 的工作。这是一个解决大规模 DNN 部署的解决方案，利用 FPGA 加速。我们设计它以实现快速、灵活和友好。使用 FPGA 的高吞吐量、低延迟加速。使用可适应的数值精度，更新最新的 AI 算法与可重配置的 FPGA。而且它用户友好，因为我们有一个全栈解决方案，编译 CNTK/Caffe/TF 并将其编译下来。这部署在我们可配置的云端，一个外层的 CPU 层，一个将所有东西整合在一起的数据中心，以及一个可重构的 FPGA 层。

我们已经部署了 DNN 模型。LSTM 模型在 CPU 上需要几十到几百毫秒的时间。我们看到的是 99th 百分位数的延迟；即使在 99th 百分位数，我们也能达到亚毫秒级别的延迟。当你达到这种加速水平时，在端到端流程中是可以忽略不计的。

接下来我将深入细节。这是一个全栈解决方案。从编译器和运行时开始，将高级框架中的模型编译到我们的体系结构中。一个灵活的 ISA 用于服务 DNN。我们有一个高吞吐量、低延迟的服务。我们在大规模持久性的情况下做到这一点，以保持模型固定在 FPGA 内存中。部署在我们广泛部署的 Intel FPGA 上，使用硬件微服务。

首先，让我们谈谈硬件微服务。这是我们在 Micro 上展示的内容。可重配置云的架构是，FPGAs 位于 CPU 和网络之间。CPU 可以在本地使用 FPGA 进行加速，但由于 FPGAs 通过网络连接，它们可以在它们之间进行分布。我们有专有的网络协议用于低延迟计算。

我们已将 FPGA 计算平面与 CPU 分开。因此，我们可以将多个 FPGAs 聚合在一起形成更大的加速器，而不必将 FPGAs 与 CPUs 的速率匹配。您可以使用少量的 FPGA 集群为大量的 CPU 提供服务，反之亦然。

接下来我将谈论编译器和运行时环境。目标是使 ML 专家能够轻松处理这些问题。典型的 ML 专家不知道如何编程这些东西。在高级框架中开发的模型，将它们编译成我们的架构。如果您首先将它们编译成中间图形表示。我们将它们分割成在 FPGA 上部分，和在 CPU 上的部分。当我们执行时，我们还有运行时环境来处理编排和调度。

我们需要为两种主要类型的 DNN 进行优化。具有非常高计算数据比的 DNNs，如卷积神经网络，这些已经研究得很好。我将重点放在另一类 DNN 上，即计算数据比较低的 DNN，例如密集层和循环神经网络。

在 FPGAs 上加速 DNNs 的常规方法是，将所有模型参数存储在 DRAM 中。当有请求进来时，你会从 DRAM 中流式传输模型参数，并返回请求。这种方法的问题在于，当你有内存带宽受限的 DNN 层时，你受限于内存带宽的速度；你无法充分发挥 FPGA 的计算能力。通常解决这个问题的方法是使用批处理；你发送一些请求，并对所有请求使用相同的模型参数。虽然你可能会获得良好的吞吐量，但延迟会增加。对于实时服务来说，这违反了你的 SLA。我们想要做的是在低或无批处理的情况下提供高性能。

我们解决这个问题的方法是使用持久化的 Dnets。FPGAs 在芯片上有大量的内存：10MB 内存。由于它们在芯片上，具有高带宽。因此，我们将保持模型参数在芯片上，这样当我们收到一个请求时，我们可以将其分布到整个 FPGA 芯片上。

显而易见的问题是，如果您的模型不能放在芯片上会发生什么？我们利用硬件微中心。我们将单个模型分布在数据中心的多个 FPGAs 上。

让我们看看我们开发的处理单元的架构和微架构。BrainWave DPU 是一种软件可编程处理器，用单线程 C 编程，但我们增加了一些用于服务 DNNs 的指令，例如矩阵乘法，卷积，非线性激活，嵌入。该处理器设计用于使用窄精度格式（float16），并且易于扩展到新的算法。

处理器的微架构，主要部分专用于矩阵向量单元；矩阵向量乘法，由大矩阵上的若干个核心组成。瓦片化使我们在保持性能的同时具备了灵活性。其他计算单元是多功能单元；向量-向量操作，如逐元素乘法、加法和激活函数。将所有这些元素连接在一起的是芯片上的网络，让我们能够同时保持所有计算的进行。

大多数芯片专用于矩阵向量单元。它由数百个多车道点积单元组成。每个点积单元由数十个加法和乘法单元组成。为了确保它们能够及时处理数据，每个点积单元都由一组专用的块 RAM 提供数据。

接下来，我想展示这种架构的性能结果。两年前，我们部署了 Stratix V FPGA。它展示了这种格式的有效 Tflops。16 位整数……我们一直在使用我们自己的 Microsoft 浮点格式。在 MSFP5.8 上的 4.5Tflops。这些 Stratix 已经相当老了。

（展示最新一代 FPGA 的演示）

查看面向吞吐量的 DPU，延迟为 65.81 毫秒。使用 Brainwave，延迟为 0.98 毫秒。低于 1 毫秒。

这是在初始工程硅上完成的。对于生产硅，我们预计以 16 位整数获得 12TOps。对于 MSFP8，则为 90TOps。一个问题是数值输出如何影响输出。这里是三个内部文本模型的归一化精度，使用 GRU 和 LSTM。橙色条显示了当您转向 MSFP9 时会发生什么，但我们已经开发出了一种调整网络以适应此精度的方法，您可以看到我们恢复了准确性。我们正在使用 MSFP8 并且看到类似的结果。

Project BrainWave 是我们用于在云端扩展 DNN（深度神经网络）的项目。我们希望它能够快速、友好并且具备云规模，扩展 AI 在云中的能力，为运行高维度 RNN 网络用于 NLP 和其他应用提供一种方式。我们计划向第三方发布，敬请关注。

Q: 当您减少批处理大小时，您正在评估哪种硬件？随着减少，硬件利用率如何？

A: 即使在减少批处理大小时，我们仍然保持高度利用；即使在高批处理大小时，我们仍然一次发送一个请求。（只有一个步骤会被处理？）对。

Q: 关于 FP9 和 FP8，九和八是否是使用的位数？（是的）它在某种程度上与 Intel 的 Flexpoint 有关吗？

A: 我们独立于 flexpoint 开发了这个项目，我无法谈论我们的数值格式。

Q: 在微软，您是否真的为您的 FPGA 编写 Verilog，还是使用高级综合工具？

A: 对于这个项目，我们正在编写 System Verilog。

Q: 需要批处理计算的批处理归一化层如何放入 FPGA 中？

A: 编译器的工作之一是在 CPU 和 FPGA 之间进行分割。因此，那些不适合在 FPGA 上运行的内容，包括批处理归一化，我们仍然在 CPU 上运行它们。
