<!--yml

category: 未分类

date: 2024-07-01 18:17:46

-->

# 链式法则 + 动态规划 = 神经网络：ezyang 的博客

> 来源：[`blog.ezyang.com/2011/05/neural-networks/`](http://blog.ezyang.com/2011/05/neural-networks/)

（猜猜 Edward 一周有什么事情：考试！这些帖子的主题可能与此有关...）

在我生命中的这一阶段，我已经两次上过介绍人工智能的课程。（不是我的错：在去剑桥之前，我碰巧已经上过 MIT 的版本，剑桥也将这些内容作为第二年课程的一部分进行教授。）我第一次学习 6.034 时，对算法的简单感到难以置信，对考试方法感到愤怒，并在最后模糊感到我真的应该更加关注。第二次学习时，我设法从课程中提炼出更多的算法内容，因为我不再过多担心细节。

今天的帖子主题是从神经网络学习过程中提炼出的算法内容。嗯，至少是对于多层感知器来说是这样——因为这通常是作为神经网络案例研究的一部分被研究的。值得注意的是，感知器实际上是一种非常简单的数学函数：它是一个多变量函数，它以一个权重向量和一个输入向量为参数，取它们的点积并通过激活函数（通常选择使其在微分时具有良好性质的函数）运行结果。“学习”在这种情况下是通过梯度下降进行的一阶优化，主要的计算内容涉及计算函数对权重向量的偏导数——这是任何学过多元微积分的人应该能够轻而易举做到的事情。

注意，我说的是*应该*。实际上，神经网络两次让我非常头痛，因为每次学习它时都很艰难。问题的一部分在于，一旦你计算出更新公式，你其实不需要理解推导过程：它们“就这样运行”。当然，任何值得尊敬的课程都不会只问你记忆相关方程的能力，所以它们通常会要求你写出推导过程。在这里，你会遇到第二个问题：大多数推导的呈现都相当冗长，不太容易“压缩”。

首次深入了解到的过程，这是我第一次学习这门课程时（最终）掌握的内容，是这些推导实际上只是重复应用链规则。因此，所有偏导数的繁琐分析可以用以下算法替代：“将感知机切割成较小的函数，计算每个函数的导数，然后将结果相乘。” 现在，这确实需要一点小心：人们通常将感知机网络视为输入值的函数，但导数是相对于权重的。此外，感知机网络是一个比通常在多变量微积分考试中找到的更为复杂的偏导数问题，因此，如果您的变量索引没有弄清楚，很容易感到困惑。（在这里，新名称和全局名称的概念非常有用，因为它为数学家自由且令人困惑的符号戏法设定了基本规则。）如果您掌握了链规则，您对输出感知机就有了一个相当令人信服的解释，并且再加上一点混乱，您可能也能应对内部感知机。

第二次深入了解到的过程直到第二次我才明白：反向传播与动态规划的相似性。这涉及到一种认识，即原则上，我可以通过跟踪“下游”节点并手动计算（更长的）导数链来计算函数对任何权重的偏导数。我可以为每个节点这样做，尽管这可能有点乏味：“反向传播”的关键思想是您可以重复使用结果以提高效率，就像动态规划一样。看到这一点也很令人满意，这解释了为什么我看过的神经网络两种处理都过于强调δ，一个看似无害的导数，实际上不应该有自己的符号。原因是这个值恰好存储在动态规划表中（在这种情况下，形状与输入神经网络相同）；权重的实际偏导数实际上并不是我们所需要的。这在竞赛动态规划问题中是相当常见的一点——其中一部分技巧是找出还需要在表中存储的中间计算。然后，反向传播只是从输出节点向输入节点填写表格。

所以你明白了：链规则 + 动态规划 = 神经网络反向传播算法。当然，这种表述要求您知道如何执行链规则，以及如何进行动态规划，但我发现这些概念要更容易记住，它们的结合也非常平凡。

*后记.* 没有讲师能抵挡住阐述他们对“人工智能”看法的诱惑。我会趁此机会插话一句：我认为人工智能既是一个问题也是一种方法：

+   人工智能是一个问题，因为问“人类能做什么计算机做不了”这个问题是挖掘计算上有趣问题的一种重要方式，和

+   人工智能是一种方法，因为自然界的智能实例暗示了计算问题的可能解决方案。

我非常尊重人工智能的力量，它能够指导研究者提出应该问什么问题，如果我们说一种方法是人工智能，因为它在这个领域处理问题的能力相当不错，那么人工智能无处不在。（这也解释了为什么人工智能在麻省理工学院，一个非常工程导向的学校，如此蓬勃发展。）然而，我对“生物启发”仍然持怀疑态度，因为这些方法似乎并不那么有效（例如，“传统”人工智能的衰落和统计自然语言处理方法的崛起），而且由此产生的方法与其生物学对应物大相径庭，任何熟悉“神经”网络的神经科学家都会证明这一点。在某些情况下，生物类比可能会有积极有害作用，掩盖了核心的数学问题。
