<!--yml

category:   分类：未分类

日期：2024-07-01 18:16:52

-->

# PyTorch 内部：ezyang 的博客

> 来源：[`blog.ezyang.com/2019/05/pytorch-internals/`](http://blog.ezyang.com/2019/05/pytorch-internals/)

这篇文章是我在 2019 年 5 月 14 日 PyTorch NYC meetup 上关于 PyTorch 内部的讲座的长篇论文版本。

大家好！今天我想谈谈[PyTorch](https://pytorch.org/)的内部。

这个讲座是为了那些使用过 PyTorch 的人，曾经想过，“如果我能为 PyTorch 做出贡献就好了”，但又被 PyTorch 庞大的 C++代码库吓到的人。我不骗你：PyTorch 的代码库有时候确实会让人感到有些压倒性。这次讲座的目的是给你一张地图：告诉你一个“支持自动求导的张量库”的基本概念结构，并给你一些在代码库中寻找方向的工具和技巧。我假设你之前写过一些 PyTorch 的代码，但不一定深入了解机器学习库是如何编写的。

讲座分为两部分：第一部分，我将首先向你介绍张量库的概念宇宙。我将从谈论你所熟知和喜爱的张量数据类型开始，更详细地讨论这个数据类型到底提供了什么，这将引导我们更好地理解它在内部是如何实现的。如果你是 PyTorch 的高级用户，你会熟悉大部分内容。我们还将讨论“扩展点”的三位一体：布局、设备和数据类型（dtype），这些概念指导着我们如何思考张量类的扩展。在 PyTorch NYC 的现场讲座中，我跳过了关于 autograd 的幻灯片，但我在这些笔记中也会稍微谈一下。

第二部分将处理在 PyTorch 中编码时实际涉及的细节。我会告诉你如何穿越大量的 autograd 代码，什么代码是真正重要的，什么是遗留的，以及 PyTorch 为编写内核提供的所有酷工具。

* * *

张量是 PyTorch 中的核心数据结构。你可能对张量直观上代表的内容有一个很好的理解：它是一个 n 维数据结构，包含某种标量类型，例如浮点数、整数等。我们可以将张量看作是由一些数据和一些元数据组成的，元数据描述了张量的大小、包含元素的类型（dtype）、张量所在的设备（CPU 内存？CUDA 内存？）。

还有一个你可能不太熟悉的小元数据：步幅。步幅实际上是 PyTorch 的一个显著特征，所以值得再多讨论一下。

张量是一个数学概念。但要在我们的计算机上表示它们，我们必须为它们定义某种物理表示。最常见的表示方法是在内存中连续地布置张量的每个元素（这就是连续这个术语的来源），按照上面所示，将每一行写入内存。在上面的示例中，我指定张量包含 32 位整数，因此您可以看到每个整数位于一个物理地址，每个偏移量相距四个字节。为了记住张量的实际维度，我们还必须记录额外的元数据来记录大小是多少。

那么，步幅与这个图有什么关系？

假设我想要访问我的逻辑表示中位置 `tensor[1, 0]` 处的元素。如何将这个逻辑位置转换为物理内存中的位置？步幅告诉我如何做到这一点：为了找出张量中任意元素的位置，我将每个索引乘以该维度的相应步幅，然后将它们全部相加。在上面的图片中，我已经用蓝色标记了第一维度，用红色标记了第二维度，因此您可以按照步幅计算中的索引和步幅。通过这个求和，我得到了两个（从零开始计数），确实，数字三位于连续数组的起始位置以下两个位置。

（谈话后面，我会讲述 TensorAccessor，一个处理索引计算的便利类。当你使用 TensorAccessor 而不是原始指针时，这个计算会在幕后为你处理。）

步幅是我们向 PyTorch 用户提供视图的基础基础。例如，假设我想提取出表示上述张量第二行的张量：

使用高级索引支持，我可以简单地写 `tensor[1, :]` 来获取这一行。这里的重要一点是：当我这样做时，我不会创建一个新的张量；相反，我只是返回一个在底层数据上的不同视图。这意味着，例如，如果我在该视图中编辑数据，它将反映在原始张量中。在这种情况下，看到如何做到这一点并不太难：三和四存储在连续的内存中，我们需要做的只是记录一个偏移量，指示这个（逻辑）张量的数据位于距离顶部两个位置。 （每个张量都记录一个偏移量，但大多数情况下是零，当情况如此时，我会从我的图表中省略它。）

> 在谈话中的问题：如果我对一个张量进行视图操作，如何释放底层张量的内存？
> 
> 答案：你必须复制视图，从而将其与原始物理内存断开连接。实际上，你没有太多其他选择。顺便说一句，如果你以前用过 Java，在旧版本中获取字符串的子串存在类似问题，因为默认情况下不会进行复制，因此子串会保留（可能非常大的字符串）。显然，他们在 [Java 7u6 中修复了这个问题](https://stackoverflow.com/questions/14161050/java-string-substring-method-potential-memory-leak)。

更有趣的情况是，如果我想取第一列：

当我们查看物理内存时，我们会看到列的元素不是连续的：每个元素之间有一个间隔。在这里，步幅可以帮助解决问题：我们不再指定步幅为一，而是指定步幅为二，表示在一个元素和下一个元素之间，需要跳过两个插槽。（顺便说一句，这就是为什么称之为“步幅”的原因：如果我们将索引视为跨越布局，步幅指定每次迈出步伐时前进的位置数。）

使用步幅表示法实际上可以让您在张量上表示各种有趣的视图；如果您想尝试一下可能性，请查看[步幅可视化工具](https://ezyang.github.io/stride-visualizer/index.html)。

让我们退后一步，思考我们实际上如何实现这个功能（毕竟，这是一个内部讨论）。如果我们可以在张量上有视图，这意味着我们必须解耦张量的概念（用户可见的您所熟悉和喜爱的概念）和实际存储张量数据的物理数据（称为存储）：

可能有多个张量共享同一存储。存储定义了张量的数据类型和物理大小，而每个张量记录了大小、步幅和偏移，定义了对物理内存的逻辑解释。

有一件事需要意识到的是，即使在“简单”情况下也始终存在张量-存储的配对，即使您不真正需要存储（例如，只是用`torch.zeros(2, 2)`分配了一个连续张量）。

> 顺便说一句，我们有兴趣让这个观点不再成立；而是不再有存储的单独概念，只需定义视图为由基本张量支持的张量。这有点复杂，但它的好处是连续张量可以更直接地表示，而不需要存储的间接性。这样的改变会使 PyTorch 的内部表示更像 NumPy。

* * *

我们已经详细讨论了张量的数据布局（有些人可能会说，如果数据表示正确，一切都会就位）。但是简要谈谈如何实现张量上的操作也是值得的。在最抽象的层面上，当您调用`torch.mm`时，会发生两次分派：

第一次分派基于张量的设备类型和布局：例如，它是 CPU 张量还是 CUDA 张量（以及例如它是步幅张量还是稀疏张量）。这是一个动态分派：它是一个虚函数调用（确切地说，这个虚函数调用发生在这次谈话的后半段）。这应该是有道理的，你需要在这里进行分派：CPU 矩阵乘法的实现与 CUDA 实现有很大不同。它是一个*动态*分派，因为这些内核可能存在于不同的库中（例如`libcaffe2.so`与`libcaffe2_gpu.so`），因此你别无选择：如果你想进入一个你没有直接依赖的库，你必须通过动态分派的方式。

第二次分派是对特定 dtype 的分派。这个分派只是一个简单的开关语句，用于支持内核选择的任何 dtype。反思之后，这也应该是有道理的，我们需要在这里进行分派：实现在`float`上的 CPU 代码（或许 CUDA 代码也是如此）与在`int`上的代码是不同的。理所当然的是，你需要为每种 dtype 编写单独的内核。

如果你试图理解 PyTorch 中操作符的调用方式，这可能是你头脑中最重要的思维模型。在查看代码时，我们将返回到这个思维模型。

* * *

既然我们已经讨论了张量，我还想花点时间介绍张量扩展的世界。毕竟，生活不仅仅是关于稠密的 CPU 浮点张量。还有各种有趣的扩展，如 XLA 张量，量化张量，MKL-DNN 张量，作为张量库的一部分，我们需要考虑如何适应这些扩展。

我们当前的扩展模型为张量提供了四个扩展点。首先，有三个参数的三位一体可以唯一确定张量是什么：

+   **设备**描述了张量物理内存实际存储位置，例如在 CPU 上，在 NVIDIA GPU（cuda）上，或者可能在 AMD GPU（hip）或 TPU（xla）上。设备的显著特征是它有自己的分配器，不与任何其他设备兼容。

+   **布局**描述了我们如何逻辑解释这个物理内存。最常见的布局是步幅张量，但稀疏张量有不同的布局，涉及一对张量，一个用于索引，一个用于数据；MKL-DNN 张量可能有更奇特的布局，如块布局，仅用步幅无法表示。

+   **dtype**描述了张量每个元素实际存储的内容是什么。这可以是浮点数或整数，或者例如量化整数。

如果你想要对 PyTorch 张量进行扩展（顺便说一句，如果这正是你想做的，请与我们联系！目前这些事情都不能在树外完成），你应该考虑扩展这些参数中的哪一个。这些参数的笛卡尔积定义了你可以生成的所有可能的张量。现在，并非所有这些组合实际上都可能有核心（谁会为 FPGA 上的稀疏、量化张量编写核心呢？），但*原则上*这些组合可能是有意义的，因此我们支持表达它，至少。

还有一种方法可以对 Tensor 功能进行“扩展”，那就是编写一个包装类，将你的对象类型实现在 PyTorch 张量周围。这听起来或许很明显，但有时人们在应该使用包装类而不是扩展这三个参数之一时，却会做出错误的选择。包装类的一个显著优点是它们可以完全在树外开发。

你何时应该编写一个张量包装类，而不是直接扩展 PyTorch 本身？关键测试是你是否需要在自动梯度反向传播过程中传递这个张量。例如，这个测试告诉我们，稀疏张量应该是一个真正的张量扩展，而不仅仅是一个包含索引和值张量的 Python 对象：在涉及嵌入的网络优化时，我们希望由嵌入生成的梯度是稀疏的。

我们对扩展的哲学也影响了张量本身的数据布局。我们真正希望我们的张量结构有一个固定的布局：我们不希望基本的（并且非常频繁调用的）操作像“张量的尺寸是多少？”需要虚拟分派。因此，当你查看张量的实际布局时（在[TensorImpl 结构](https://github.com/pytorch/pytorch/blob/master/c10/core/TensorImpl.h)中定义），我们看到的是所有“张量”样式的共同前缀，我们认为所有这些东西都普遍具有，加上一些只对分步张量真正适用的字段，但它们*如此*重要，我们已经将它们保留在主结构中，然后是可以在每个张量基础上自定义字段的后缀。例如，稀疏张量在这个后缀中存储它们的索引和值。

* * *

我们已经讨论了张量的所有内容，但如果这是 PyTorch 提供的唯一功能，那它基本上只是一个 Numpy 的克隆。PyTorch 最初发布时的显著特点是提供了张量的自动微分（如今，我们有其他很酷的功能，比如 TorchScript；但在当时，这就是它的全部！）

自动微分的作用是什么？它是负责对神经网络进行梯度计算的机制：

...并填写实际计算网络梯度的缺失代码：

请花点时间研究这个图表。有很多需要解读的内容；以下是你需要关注的内容：

1.  首先，把你的注意力放在红色和蓝色变量上。PyTorch 实现了[反向模式自动微分](https://en.wikipedia.org/wiki/Automatic_differentiation#Reverse_accumulation)，这意味着我们实际上是“反向”进行前向计算来计算梯度。如果你查看变量名，你会看到这一点：在红色部分的底部，我们计算了`loss`；然后，在程序的蓝色部分中，我们首先计算`grad_loss`。`loss`是从`next_h2`计算出来的，所以我们计算`grad_next_h2`。技术上讲，我们称之为`grad_`的这些变量并不是真正的梯度；它们实际上是雅可比矩阵左乘以一个向量，但在 PyTorch 中，我们只是称之为`grad`，大多数人都知道我们的意思。

1.  如果代码结构保持不变，行为则不同：从前向传递的每一行都被替换为代表前向操作导数的不同计算。例如，`tanh`操作被转换为`tanh_backward`操作（这两行通过图表左侧的灰色线连接）。前向和反向操作的输入和输出被交换：如果前向操作产生了`next_h2`，那么反向操作将以`grad_next_h2`作为输入。

自动求导的整个目的是执行由此图表描述的计算，但实际上并不会生成此源代码。PyTorch 的自动求导不进行源到源的转换（尽管 PyTorch JIT 确实知道如何进行符号微分）。

为了做到这一点，我们需要在对张量进行操作时存储更多元数据。让我们调整我们对张量数据结构的看法：现在不仅仅是一个指向存储的张量，我们现在有一个包装了这个张量的变量，并且还存储更多信息（AutogradMeta），这些信息在用户在他们的 PyTorch 脚本中调用`loss.backward()`时需要用来执行自动求导。

> 这是另一张幻灯片，希望很快会过时。Will Feng 正在处理一个[在 C++中的变量张量合并](https://github.com/pytorch/pytorch/issues/13638)，跟随发生在 PyTorch 前端接口的简单合并。

我们也必须更新我们关于调度的看法：

在我们调度到 CPU 或 CUDA 实现之前，还有一个关于变量的调度，负责展开变量，调用底层实现（以绿色表示），然后重新包装结果为变量，并记录必要的反向自动求导元数据。

一些实现不进行展开；它们只是调用其他变量实现。因此，你可能会在变量的宇宙中花费一些时间。然而，一旦你展开并进入非变量张量的宇宙，那就是它；你永远不会回到变量（除非从函数返回）。

* * *

在我的纽约聚会演讲中，我跳过了以下七张幻灯片。我也将延迟为它们撰写文稿；你们得等到续集才能看到一些文字。

* * *

关于概念的讨论就到此为止，让我们来看看一些代码。

PyTorch 有很多文件夹，在 [CONTRIBUTING](https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md#codebase-structure) 文档中对它们进行了详细描述，但实际上，你只需要了解四个目录：

+   首先，`torch/` 包含了你最熟悉的内容：你导入和使用的实际 Python 模块。这些都是 Python 代码，很容易进行修改和查看结果。然而，在表面之下并不太深的地方...

+   `torch/csrc/`，这是实现你可能称为 PyTorch 前端的 C++ 代码。更详细地说，它实现了在 Python 和 C++ 之间转换的绑定代码，以及 PyTorch 的一些重要组成部分，如自动求导引擎和 JIT 编译器。它还包含了 C++ 前端代码。

+   `aten/`，简称为 "A Tensor Library"（由 Zachary DeVito 创造），是一个实现张量操作的 C++ 库。如果你在寻找某些核心代码所在地，很可能就在 ATen 中。ATen 本身分为两个操作符的区域：现代的 C++ 实现的 "native" 操作符，以及传统的 C 实现的 "legacy" 操作符（TH、THC、THNN、THCUNN）。传统操作符是糟糕的地方；如果可能的话，尽量不要花太多时间在那里。

+   `c10/`，这是一个关于 Caffe2 和 A"Ten"（明白了吗？Caffe 10）的双关语，包含了 PyTorch 的核心抽象，包括 Tensor 和 Storage 数据结构的实际实现。

那里有很多地方可以寻找代码；我们可能应该简化目录结构，但目前情况就是这样。如果你想要处理运算符，你将大部分时间都花在 `aten` 目录下。

让我们看看这种代码分离在实践中是如何展开的：

当你调用像 `torch.add` 这样的函数时，实际上发生了什么？如果你记得我们讨论过的分发方式，你已经在脑海中有了基本的概念：

1.  我们需要从 Python 领域翻译到 C++ 领域（Python 参数解析）。

1.  我们处理**变量**分发（VariableType--顺便说一句，Type 实际上与编程语言类型没有关系，只是用于执行分发的一个工具）。

1.  我们处理**设备类型 / 布局**分发（Type）。

1.  我们有实际的核心代码，它可以是现代的本地函数，也可以是传统的 TH 函数。

这些步骤中的每一步都对应于一些具体的代码。让我们穿越这片丛林。

我们在 C++ 代码中的初始着陆点是 Python 函数的 C 实现，我们已经将其作为类似 `torch._C.VariableFunctions.add` 的东西暴露给了 Python 端。`THPVariable_add` 是这样一种实现的实现。

关于这段代码的一件重要事情是，它是自动生成的。如果你在 GitHub 仓库中搜索，你找不到它，因为你必须实际构建 PyTorch 才能看到它。另一件重要的事情是，你不必深入理解这段代码在做什么；你只需略过它，了解它在做什么即可。上面，我用蓝色注释了一些最重要的部分：你可以看到在这里使用了一个 `PythonArgParser` 类来从 Python 的 `args` 和 `kwargs` 中实际提取 C++ 对象；然后我们调用了一个 `dispatch_add` 函数（我已经用红色内联了它）；这会释放全局解释器锁，然后在 C++ Tensor `self` 上调用一个普通的方法。在返回时，我们将返回的 `Tensor` 重新包装成一个 `PyObject`。

（在这一点上，幻灯片上有一个错误：我应该告诉你关于变量分派代码的事情。我还没有在这里修复它。然后一些魔法发生了...）

当我们在 `Tensor` 类上调用 `add` 方法时，还没有发生虚拟分派。相反，我们有一个内联方法，它调用一个 "Type" 对象上的虚拟方法。这个方法是实际的虚拟方法（这就是为什么我说 Type 只是一个 "小工具"，让你进行动态分派）。在这个例子的特定情况下，这个虚拟调用会分派给 `TypeDefault` 类上的 `add` 实现。这是因为我们有一个对于每种设备类型（CPU 和 CUDA）都相同的 `add` 实现；如果我们有不同的实现，我们可能会得到类似 `CPUFloatType::add` 的东西。正是这个虚拟方法的实现最终将我们带到实际的内核代码。

> 希望这张幻灯片也很快就过时了；Roy Li 正在致力于用另一种机制替换 `Type` 分发，这将帮助我们更好地支持移动端的 PyTorch。

值得再次强调的是，直到我们到了内核，所有的代码都是自动生成的。

这有点扭曲，所以一旦你对正在发生的事情有了基本的了解，我建议直接跳到内核。

* * *

PyTorch 为潜在的内核编写者提供了许多有用的工具。在本节中，我们将简要介绍其中一些。但首先，你需要写一个内核的时候，需要什么？

我们通常认为 PyTorch 中的内核由以下部分组成：

1.  首先，有一些关于内核的元数据，我们写在这些元数据中，这些数据驱动着代码生成，并让你在不写一行代码的情况下就可以把所有绑定到 Python。

1.  一旦你到了内核，你已经过了设备类型/布局调度。首先要做的事情是错误检查，确保输入张量的尺寸是正确的。（错误检查非常重要！不要马虎！）

1.  接下来，我们通常需要分配结果张量，我们将把输出写入其中。

1.  现在是核心适当的时间。在这一点上，您现在应该进行第二次 dtype 分发，以跳转到专门针对它操作的核心。 （您不希望太早这样做，因为那样您将无用地复制在任何情况下看起来相同的代码。）

1.  大多数性能良好的核心需要某种并行化，以便您可以利用多 CPU 系统。 （CUDA 核心是“隐式”并行化的，因为它们的编程模型建立在大规模并行化之上）。

1.  最后，您需要访问数据并执行您想要的计算！

在随后的幻灯片中，我们将介绍 PyTorch 为帮助您执行这些步骤提供的一些工具。

要利用 PyTorch 带来的所有代码生成功能，您需要为您的运算符编写一个*模式*。该模式提供了函数的类似于 mypy 的类型，并控制我们是否为 Tensor 上的方法或函数生成绑定。您还告诉模式为给定的设备布局组合调用您的运算符的实现。查看[native 中的 README](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/README.md)获取有关此格式的更多信息。

您还可能需要在[derivatives.yaml](https://github.com/pytorch/pytorch/blob/master/tools/autograd/derivatives.yaml)中为您的操作定义一个导数。

错误检查可以通过低级或高级 API 方式完成。低级 API 只是一个宏，`TORCH_CHECK`，它接受一个布尔值，然后任意数量的参数来组成错误字符串以在布尔值不为真时渲染。这个宏的一个好处是，您可以混合字符串和非字符串数据；一切都是使用他们的`operator<<`实现格式化的，而 PyTorch 中大多数重要的数据类型都有`operator<<`的实现。

高级 API 可以避免您反复编写重复的错误消息。它的工作方式是，您首先将每个`Tensor`包装成一个`TensorArg`，其中包含关于张量来源的信息（例如，它的参数名）。然后它提供了许多预定义的函数来检查各种属性；例如，`checkDim()`测试张量的维度是否为固定数量。如果不是，该函数根据`TensorArg`元数据提供一个用户友好的错误消息。

在编写 PyTorch 操作符时要注意的一件重要事情是，您通常要签署编写*三个*操作符：`abs_out`，它在预分配的输出上操作（这实现了`out=`关键字参数），`abs_`，它是原地操作，以及`abs`，它是操作符的普通旧版功能版本。

大多数情况下，`abs_out`是真正的工作马，而`abs`和`abs_`只是围绕`abs_out`的薄包装；但有时为每种情况编写专门的实现是有必要的。

要进行数据类型分发，你应该使用 `AT_DISPATCH_ALL_TYPES` 宏。这个宏接受你想要分发的张量的数据类型，以及一个 lambda 表达式，该 lambda 表达式将针对从宏中可分派的每种数据类型进行特化。通常，这个 lambda 只是调用一个模板化的辅助函数。

这个宏不仅仅是“进行分派”，它还决定了你的内核将支持哪些数据类型。因此，实际上有很多版本的这个宏，让你选择生成特定数据类型的特化。大多数情况下，你只需要 `AT_DISPATCH_ALL_TYPES`，但要注意在某些情况下，你可能需要分派到更多类型。在 [Dispatch.h](https://github.com/pytorch/pytorch/blob/21ef4cc615a7d9d772ade52a5023900718b09e92/aten/src/ATen/Dispatch.h#L62) 中有关于如何为你的用例选择正确版本的指导。

在 CPU 上，你经常希望并行化你的代码。过去，这通常是通过直接在代码中撒入 OpenMP pragma 来完成的。

在某个时候，我们必须实际访问数据。PyTorch 为此提供了相当多的选项。

1.  如果你只是想在某个特定位置获取一个值，你应该使用`TensorAccessor`。一个张量访问器就像一个张量，但它将张量的维度和数据类型硬编码为模板参数。当你像这样检索一个访问器 `x.accessor<float, 3>();`时，我们会进行运行时测试以确保张量确实是这种格式；但在此之后，每次访问都是无检查的。张量访问器正确处理步幅，因此你应该优先使用它们而不是原始指针访问（不幸的是，一些遗留内核确实会这样做）。还有一个 `PackedTensorAccessor`，专门用于通过 CUDA 启动发送访问器，这样你可以从 CUDA 内核中获取访问器。（一个值得注意的问题：`TensorAccessor` 默认为 64 位索引，这在 CUDA 中比 32 位索引要慢得多！）

1.  如果你正在编写某种具有非常规则元素访问的操作符，例如逐点操作，最好使用更高级的抽象，即 `TensorIterator`。这个辅助类会自动处理广播和类型提升，并且非常方便。

1.  对于 CPU 上真正的速度，你可能需要使用矢量化的 CPU 指令来编写你的内核。我们也有一些辅助工具！`Vec256` 类表示标量的向量，并提供了一些方法，可以一次性对它们执行矢量化操作。像 `binary_kernel_vec` 这样的辅助工具然后让你轻松地运行矢量化操作，然后使用普通的指令完成那些无法完全适配到矢量指令的操作。这里的基础设施还会在不同的指令集下多次编译你的内核，然后在运行时测试你的 CPU 支持什么指令，并在这些情况下使用最佳内核。

PyTorch 中许多核心仍然采用传统的 TH 风格编写。（顺便说一下，TH 代表 TorcH。这是一个相当不错的首字母缩写，但不幸的是它有些负面影响；如果在名称中看到 TH，就假定它是传统的。）什么是传统的 TH 风格呢？

1.  它是以 C 风格编写的，几乎不使用 C++。

1.  它是手动引用计数的（使用 `THTensor_free` 手动调用来减少在使用张量后的引用计数），并且

1.  它位于 `generic/` 目录中，这意味着我们实际上会多次编译该文件，但使用不同的 `#define scalar_t`。

这段代码相当复杂，我们很讨厌审查它，所以请不要再增加内容。如果你喜欢编码但对内核编写了解不多，可以做的一项更有用的任务是将其中一些 TH 函数移植到 ATen。

* * *

总结一下，我想谈谈在 PyTorch 上高效工作的一些技巧。如果 PyTorch 的庞大的 C++ 代码库是阻止人们贡献到 PyTorch 的第一个关卡，那么您的工作流程的效率就是第二个关卡。如果您试图用 Python 的习惯来处理 C++，**您将会感到很痛苦**：重新编译 PyTorch 需要很长时间，而要确定您的更改是否有效也将需要很长时间。

如何高效工作可能可以单独讲一讲，但这张幻灯片指出了一些常见的反模式，我经常听到有人抱怨说：“在 PyTorch 上工作很难。”

1.  如果您编辑的是一个头文件，尤其是被许多源文件包含的头文件（特别是被 CUDA 文件包含的），请预计会有非常长的重建时间。尽量只编辑 cpp 文件，并节制地编辑头文件！

1.  我们的 CI 是一个非常棒的、零配置的测试工具，可以测试您的更改是否有效。但是请预计需要等待一到两个小时才能收到反馈信号。如果您正在进行需要大量试验的更改工作，请花些时间设置本地开发环境。同样，如果在特定的 CI 配置上遇到难以调试的问题，请在本地设置它。您可以[下载并在本地运行 Docker 镜像](https://github.com/pytorch/ossci-job-dsl)。

1.  [贡献指南解释了如何设置 ccache](https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md#use-ccache)；这是非常推荐的，因为有时它会帮助您幸运地避免在编辑头文件时进行大规模重新编译。它还有助于掩盖我们的构建系统中的错误，使我们在不应该重新编译文件时重新编译它们。

1.  最终，我们有大量的 C++ 代码，如果在配置强大的服务器上构建，您将会有更愉快的体验，因为这样做 CUDA 构建会非常慢，而笔记本电脑往往没有足够的处理能力来快速完成。

* * *

这就是对 PyTorch 内部的一个风速游览！很多很多东西都被省略了；但希望这里的描述和解释能帮助您至少掌握代码库的一个重要部分。

接下来该怎么做？您可以做哪些贡献？一个好的起点是我们的问题跟踪器。从今年年初开始，我们一直在分类问题；标记为**triaged**的问题意味着至少有一个 PyTorch 开发人员已经看过它并对问题做了初步评估。您可以使用这些标签查找我们认为是[高优先级](https://github.com/pytorch/pytorch/issues?q=is%3Aopen+is%3Aissue+label%3A%22high+priority%22+label%3Atriaged)的问题，或者查找特定模块的问题，例如[autograd](https://github.com/pytorch/pytorch/issues?q=is%3Aopen+is%3Aissue+label%3Atriaged+label%3A%22module%3A+autograd%22)，或者找到我们认为是[小问题](https://github.com/pytorch/pytorch/issues?q=is%3Aopen+is%3Aissue+label%3Atriaged+label%3Asmall)的问题（警告：有时我们也会犯错！）

即使您现在不想开始编码，也有许多其他有用的活动，比如改进文档（我*喜欢*合并文档的 PR，它们非常棒），帮助我们重现其他用户的 bug 报告，以及帮助我们在问题跟踪器上讨论 RFC。没有开源贡献者，PyTorch 就不会走到今天这一步；我们希望您也能加入我们！
