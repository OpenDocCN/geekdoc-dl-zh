# 3.1 简介

> [`boramorka.github.io/LLM-Book/en/CHAPTER-3/3.1%20Introduction/`](https://boramorka.github.io/LLM-Book/en/CHAPTER-3/3.1%20Introduction/)

将大型语言模型引入软件开发流程是 AI 产品演化的下一个转折点。本节是 LLMOps 的实用介绍，涵盖了基于 LLM 的应用程序的完整生命周期：从模型选择和微调到生产部署、监控和持续运营。LLM 能够理解和生成类似人类的文本，因此它们被用于摘要、分类、内容生成和其他许多任务。它们的优点是来自大型语料库训练的广泛知识、适应广泛场景的能力，无需进行繁重的特定任务训练，以及能够处理上下文并捕捉细微差别的能力。在此基础上，LLMOps 作为 MLOps 的 LLM 专用层：模型选择和领域准备、深思熟虑的部署以满足服务等级协议、使用指标和警报的持续监控，以及遵循道德原则和数据保护的安全和隐私。

LLMOps 路线图通常包括几个步骤。首先，根据模型大小、训练数据和基准选择一个模型：将指标与你的任务匹配，并准备一个忠实反映领域和目标的微调数据集。接下来，设计部署架构和基础设施：规划可扩展性，留有高峰期的余量，通过缓存和更短的执行路径来最小化延迟，并考虑集成。在生产中，依靠持续监控来捕捉退化和数据漂移；提前定义 KPI/SLI，并内置定期更新和回归测试。在整个过程中，保护隐私和安全：匿名化敏感字段，控制对模型的访问，防止滥用，并正式制定负责任的 AI 政策。

一个 LLM 应用程序的结构通常涉及选择和微调：评估可用选项及其与你的要求的匹配度，然后使用提示工程、PEFT/LoRA 和其他方法将模型适应你的领域——注意基础设施兼容性和微调技术的成本/效率平衡。部署通常是围绕模型的 REST API 或一个编排器；可观测性和实时指标跟踪对于理解模型健康状况和快速响应事件至关重要。自动化任何重复性任务：带有版本控制和 A/B 测试的提示管理、自动测试和 CI/CD、多步骤 LLM 链及其依赖关系的编排。数据准备是有效微调的基础：使用 SQL/ETL 和开源工具构建干净的数据集市；编排复杂的流程以满足服务等级协议，将重试和幂等性作为一等属性。

最佳实践建立在三个支柱之上：自动化（测试和 CI/CD 加快了可靠的发布）、提示管理（上下文感知的动态和稳定的 A/B 测试提高了质量），以及逐案扩展（模块化架构在不破坏现有功能的情况下添加新场景，以及负载容量规划）。鉴于 LLM 和 MLOps 变化之快，请内置灵活性：跟随趋势，与社区互动，并定期参加课程和研讨会。

从实践中来：使用大型语言模型聊天机器人和动态提示管理来自动化支持，可以减少响应时间并提高服务质量；在出版领域，总结和编辑流程加上提示管理可以极大地加快文章的生产速度。总的来说，采用结构化的 LLMOps 方法——包括自动化、坚实的提示管理、深思熟虑的可扩展性和持续学习的文化——是构建和运营成功的 LLM 应用的关键。为了进行更深入的研究，请保留以下资料：WhyLabs 的《LLMOps 指南》中包含有关提示、评估、测试和扩展的材料；《Weights & Biases “Understanding LLMOps”》——对开源和专有 LLM 的回顾，包括监控实践；以及 DataRobot AI Wiki，它将 LLMOps 定位为 MLOps 的一个子集，并涵盖了相关主题。
