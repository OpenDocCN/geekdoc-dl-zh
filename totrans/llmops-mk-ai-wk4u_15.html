<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>2.4 The Power of Embeddings</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>2.4 The Power of Embeddings</h1>
<blockquote>原文：<a href="https://boramorka.github.io/LLM-Book/en/CHAPTER-2/2.4%20The%20Power%20of%20Embeddings/">https://boramorka.github.io/LLM-Book/en/CHAPTER-2/2.4%20The%20Power%20of%20Embeddings/</a></blockquote>
                
                  


  
  



<blockquote>
<p><strong>Requirements:</strong> <code>pip install langchain langchain-community langchain-openai chromadb</code></p>
<p><strong>Note:</strong> LangChain imports have been updated. For latest versions, use <code>from langchain_community.vectorstores import Chroma</code> and <code>from langchain_community.document_loaders import PyPDFLoader</code> instead of the older <code>langchain.*</code> paths.</p>
</blockquote>
<p>Embeddings are numeric representations of text: words, sentences, and documents are mapped to vectors in a high‑dimensional space, and semantically similar texts end up close together geometrically. These representations are learned from large corpora: the model associates a word with its context and captures semantic relations, so synonyms and terms that appear in similar contexts lie nearby. As a result, semantic search goes beyond exact “keyword” matching: compute an embedding for each document (or chunk) and for the user query, compare vector proximity via cosine or another metric, and rank materials by semantic similarity — even without exact matches. This shifts how we analyze, store, and search: interactions become more meaningful and recommendations more precise.</p>
<p>On top of embeddings sit vector stores — databases optimized for vector storage and fast nearest‑neighbor search. They use specialized indexes and algorithms to answer similarity queries over large datasets and fit both research and production. Choose based on data size (from in‑memory options for small sets to distributed systems at scale), persistence (do you need durable disk storage or a transient store for prototypes), and use case (lab vs. production). For quick prototyping, Chroma is a common choice — a lightweight in‑memory store; for larger and long‑lived systems, use distributed/cloud vector DBs. In a typical semantic‑search pipeline, documents are first split into meaningful chunks, then embeddings are computed and indexed; on a query, its embedding is computed, nearest chunks are retrieved, and the extracted parts plus the query are fed to an LLM to generate a coherent answer.</p>
<p>Before diving into embeddings and vector DBs, prepare the environment: imports, API keys, and basic config.</p>
<div class="highlight"><pre><span/><code><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">dotenv</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dotenv</span><span class="p">,</span> <span class="n">find_dotenv</span>

<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">'../..'</span><span class="p">)</span>

<span class="n">load_dotenv</span><span class="p">(</span><span class="n">find_dotenv</span><span class="p">())</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">()</span>
</code></pre></div>
<p>Next, load documents and split them into semantically meaningful fragments — this makes data easier to manage and prepares it for embedding creation. We’ll use a series of PDFs (with some “noise” like duplicates) for demonstration:</p>
<div class="highlight"><pre><span/><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain.document_loaders</span><span class="w"> </span><span class="kn">import</span> <span class="n">PyPDFLoader</span>

<span class="n">pdf_document_loaders</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">PyPDFLoader</span><span class="p">(</span><span class="s2">"docs/doc1.pdf"</span><span class="p">),</span>
    <span class="n">PyPDFLoader</span><span class="p">(</span><span class="s2">"docs/doc2.pdf"</span><span class="p">),</span>
    <span class="n">PyPDFLoader</span><span class="p">(</span><span class="s2">"docs/doc3.pdf"</span><span class="p">),</span>
<span class="p">]</span>

<span class="n">loaded_documents_content</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">document_loader</span> <span class="ow">in</span> <span class="n">pdf_document_loaders</span><span class="p">:</span>
    <span class="n">loaded_documents_content</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">document_loader</span><span class="o">.</span><span class="n">load</span><span class="p">())</span>
</code></pre></div>
<p>After loading, split documents into chunks to improve manageability and downstream efficiency:</p>
<div class="highlight"><pre><span/><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain.text_splitter</span><span class="w"> </span><span class="kn">import</span> <span class="n">RecursiveCharacterTextSplitter</span>

<span class="n">document_splitter</span> <span class="o">=</span> <span class="n">RecursiveCharacterTextSplitter</span><span class="p">(</span>
    <span class="n">chunk_size</span><span class="o">=</span><span class="mi">1500</span><span class="p">,</span>
    <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">150</span>
<span class="p">)</span>
<span class="n">document_splits</span> <span class="o">=</span> <span class="n">document_splitter</span><span class="o">.</span><span class="n">split_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
</code></pre></div>
<p>Now compute embeddings for each chunk: turn text into vectors that reflect semantic meaning.</p>
<div class="highlight"><pre><span/><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAIEmbeddings</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">embedding_generator</span> <span class="o">=</span> <span class="n">OpenAIEmbeddings</span><span class="p">()</span>

<span class="n">sentence_examples</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"I like dogs"</span><span class="p">,</span> <span class="s2">"I like canines"</span><span class="p">,</span> <span class="s2">"The weather is ugly outside"</span><span class="p">]</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="p">[</span><span class="n">embedding_generator</span><span class="o">.</span><span class="n">embed_query</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentence_examples</span><span class="p">]</span>

<span class="n">similarity_dog_canine</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">embeddings</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">embeddings</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">similarity_dog_weather</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">embeddings</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">embeddings</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
</code></pre></div>
<p>Index the vectors in a vector store to enable fast similarity search. For demos, Chroma — an in‑memory option — works well:</p>
<div class="highlight"><pre><span/><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain.vectorstores</span><span class="w"> </span><span class="kn">import</span> <span class="n">Chroma</span>

<span class="n">persist_directory</span> <span class="o">=</span> <span class="s1">'chroma_db/'</span>

<span class="c1"># Clear previous database if exists (use Python for cross-platform compatibility)</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">shutil</span>
<span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">persist_directory</span><span class="p">):</span>
    <span class="n">shutil</span><span class="o">.</span><span class="n">rmtree</span><span class="p">(</span><span class="n">persist_directory</span><span class="p">)</span>

<span class="n">vector_database</span> <span class="o">=</span> <span class="n">Chroma</span><span class="o">.</span><span class="n">from_documents</span><span class="p">(</span>
    <span class="n">documents</span><span class="o">=</span><span class="n">document_splits</span><span class="p">,</span>
    <span class="n">embedding</span><span class="o">=</span><span class="n">embedding_generator</span><span class="p">,</span>
    <span class="n">persist_directory</span><span class="o">=</span><span class="n">persist_directory</span>
<span class="p">)</span>
</code></pre></div>
<p>Now perform a similarity search — this is where embeddings + vector DBs shine: quickly selecting the most relevant fragments for a query.</p>
<div class="highlight"><pre><span/><code><span class="n">query</span> <span class="o">=</span> <span class="s2">"Is there an email I can ask for help?"</span>
<span class="n">retrieved_documents</span> <span class="o">=</span> <span class="n">vector_database</span><span class="o">.</span><span class="n">similarity_search</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">retrieved_documents</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">page_content</span><span class="p">)</span>
</code></pre></div>
<p>Finally, consider edge cases and search quality improvements. Even a useful baseline runs into issues: duplicates and irrelevant documents are common problems that degrade results.</p>
<div class="highlight"><pre><span/><code><span class="c1"># Query example illustrating a failure mode</span>
<span class="n">query_matlab</span> <span class="o">=</span> <span class="s2">"What did they say about MATLAB?"</span>

<span class="c1"># Detect duplicate fragments in search results</span>
<span class="n">retrieved_documents_matlab</span> <span class="o">=</span> <span class="n">vector_database</span><span class="o">.</span><span class="n">similarity_search</span><span class="p">(</span><span class="n">query_matlab</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</code></pre></div>
<p>From there, you can apply strategies to mitigate such failures and retrieve fragments that are both relevant and sufficiently diverse. Taken together, embeddings and vector DBs are a powerful pairing for semantic search over large corpora: solid text preparation, thoughtful indexing, and fast nearest‑neighbor querying enable systems that understand complex prompts; analyzing failures and adding techniques further improves robustness and accuracy. For deeper study, see the OpenAI API docs on embedding generation and surveys of vector databases that compare technologies and usage scenarios.</p>
<h2 id="theory-questions">Theory Questions</h2>
<ol>
<li>What is the primary goal of turning text into embeddings?</li>
<li>How do embeddings help measure semantic similarity of words and sentences?</li>
<li>Describe how word embeddings are created and the role of context.</li>
<li>How do embeddings improve semantic search over keyword‑based approaches?</li>
<li>What roles do document and query embeddings play in semantic search?</li>
<li>What is a vector store, and why is it important for efficient search?</li>
<li>What criteria matter when choosing a vector database?</li>
<li>Why is Chroma convenient for prototypes, and what are its limitations?</li>
<li>Describe a semantic‑search pipeline using embeddings and a vector DB.</li>
<li>How does document splitting improve search granularity and relevance?</li>
<li>Why embed chunks, and how does that help retrieval?</li>
<li>Why index the vector store for similarity search?</li>
<li>How is a query processed, and which similarity metrics are used?</li>
<li>How does answer generation improve UX in semantic‑search apps?</li>
<li>What environment setup steps are needed?</li>
<li>Give an example where loading and splitting text are critical to search quality.</li>
<li>How do embeddings “transform” text, and how can you demonstrate vector similarity?</li>
<li>What should you consider when configuring Chroma?</li>
<li>How does similarity search find relevant fragments?</li>
<li>What failures are typical in semantic search, and how can you address them?</li>
</ol>
<h2 id="practical-tasks">Practical Tasks</h2>
<ol>
<li>Implement <code>generate_embeddings</code> that returns a list of “embeddings” for strings (e.g., simulated by string length).</li>
<li>Implement <code>cosine_similarity</code> to compute cosine similarity between two vectors.</li>
<li>Create <code>SimpleVectorStore</code> with <code>add_vector</code> and <code>find_most_similar</code> (cosine‑based).</li>
<li>Load text from a file, split into chunks of a given size (e.g., 500 characters), and print them.</li>
<li>Implement <code>query_processing</code>: generate a query embedding (placeholder), find the nearest chunk in <code>SimpleVectorStore</code>, and print it.</li>
<li>Implement <code>remove_duplicates</code>: return a list without duplicate chunks (exact match or by similarity threshold).</li>
<li>Initialize <code>SimpleVectorStore</code>, add placeholder embeddings, run a semantic search, and print top‑3 results.</li>
<li>Implement <code>embed_and_store_documents</code>: generate placeholder embeddings for chunks, store them in <code>SimpleVectorStore</code>, and return it.</li>
<li>Implement <code>vector_store_persistence</code>: demonstrate saving/loading <code>SimpleVectorStore</code> (serialization/deserialization).</li>
<li>Implement <code>evaluate_search_accuracy</code>: for queries and expected chunks, run search and compute match rate.</li>
</ol>












                
                  
</body>
</html>