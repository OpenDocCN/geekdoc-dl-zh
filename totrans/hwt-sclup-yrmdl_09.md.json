["```py\nimport numpy as np\n\nnum_chips = 16  # we fix 16 as the amount of total model parallelism we do param_size = 70e9  # int8 means 1 byte per param sequence_length = 8192  # can vary this \nhbm_bandwidth = 8.20E+11  # v5e flops = 1.97E+14  # v5e \nparam_size = bytes_per_param * param_count\n\ndef kv_cache_size(bs):\n    return 2 * bs * 128 * 8 * 80\n\ndef min_topology(bytes):\n    return 2 ** np.ceil(np.log2(bytes / 16e9))\n\ndef get_max_batch_size(max_num_chips: int = 16):\n  # for num_chips in topo_sizes:\n  batch_sizes = np.arange(1, 1024, 4)\n  kv_sizes = kv_cache_size(sequence_length * batch_sizes)\n  num_chips = min_topology(kv_sizes + param_size)\n  max_idx = np.where(num_chips <= max_num_chips)[0][-1]\n  return max_idx\n\nmax_idx = get_max_batch_size(num_chips, sequence_length, param_size)  # get the largest batch size that can fit batch_sizes = np.arange(1, 512, 1)[:max_idx]\nkv_sizes = kv_cache_size(sequence_length * batch_sizes)\n\nkv_comms_time = kv_sizes / (num_chips * hbm_bandwidth)\n\nparam_comms_time = param_size / (num_chips * hbm_bandwidth)\nparam_comms_time = np.asarray([param_comms_time] * batch_sizes.shape[0])\n\nflops_time = 2 * param_count * batch_sizes / (num_chips * flops)  # roughly true in a 2ND sense \nmlp_time = np.maximum(flops_time, param_comms_time)\nattn_time = kv_comms_time  # always bandwidth-bound for generate \nlatency = 1000 * (mlp_time + attn_time)\nthroughput = batch_sizes / (latency * num_chips) \n```", "```py\n Austin et al., \"How to Scale Your Model\", Google DeepMind, online, 2025. \n```", "```py\n @article{scaling-book,\n      title = {How to Scale Your Model},\n      author = {Austin, Jacob and Douglas, Sholto and Frostig, Roy and Levskaya, Anselm and Chen, Charlie and Vikram, Sharad\n      and Lebron, Federico and Choy, Peter and Ramasesh, Vinay and Webson, Albert and Pope, Reiner},\n      publisher = {Google DeepMind},\n      howpublished = {Online},\n      note = {Retrieved from https://jax-ml.github.io/scaling-book/},\n      year = {2025}\n    } \n```"]