- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: COT 专栏'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：COT 专栏
- en: 'date: 2024-05-08 11:10:56'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-05-08 11:10:56
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Where Copilots Work
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 共同开发者的工作场所
- en: 来源：[https://every.to/chain-of-thought/where-copilots-work](https://every.to/chain-of-thought/where-copilots-work)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://every.to/chain-of-thought/where-copilots-work](https://every.to/chain-of-thought/where-copilots-work)
- en: 'Sponsored By: Lever'
  id: totrans-6
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 由 Lever 赞助
- en: 'Hire smarter with [Lever](https://www.lever.co/demo/?utm_medium=third_party_email&utm_source=other&utm_campaign=none&utm_content=every_divinations_sponsored_newsletter)—the
    only complete hiring solution that provides modern talent acquisition leaders
    with complete ATS and robust CRM capabilities in one product: LeverTRM'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[Lever](https://www.lever.co/demo/?utm_medium=third_party_email&utm_source=other&utm_campaign=none&utm_content=every_divinations_sponsored_newsletter)招聘更聪明——提供现代人才招聘领导者完整的
    ATS 和强大的 CRM 功能的唯一完整招聘解决方案：LeverTRM
- en: Luke Skywalker had R2-D2’s whistles and beeps. Maverick had Goose. Bertie had
    his butler Jeeves, who shimmered in and out of the room to perform tasks well
    before he’d even been asked to.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 卢克·天行者有 R2-D2 的吹哨声和蜂鸣声。飞行员有古斯。伯蒂有他的管家 Jeeves，他会在甚至没有被要求之前就在房间里闪现来执行任务。
- en: These stories are popular because everyone wants a copilot—a partner who makes
    you better, and who (sometimes) becomes a friend you can lean on when things get
    hard.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这些故事之所以受欢迎，是因为每个人都想要一个共同开发者——一个能让你变得更好的伙伴，以及（有时）成为在困难时可以依靠的朋友。
- en: This sort of thing is exactly what a lot of people in AI are building right
    now.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这种事情正是现在很多人工智能领域的人正在构建的。
- en: GitHub CoPilot is the first large-scale AI use case that has significant traction—reportedly
    writing [40% of the code for developers who use it](https://github.blog/2022-06-21-github-copilot-is-generally-available-to-all-developers/).
    Reid Hoffman thinks there will be a [copilot for every profession](https://fortune.com/2022/12/07/linkedin-founder-reid-hoffman-on-ai-human-amplification/).
    Microsoft is building an [AI copilot into Office](https://techcrunch.com/2023/03/06/microsoft-dynamics-copilot/).
    Diagram is building a [copilot for designers](https://genius.design/). The list
    goes on.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub 共同开发者是第一个有显著影响力的大规模人工智能用例，据说为使用它的开发者编写了[40%的代码](https://github.blog/2022-06-21-github-copilot-is-generally-available-to-all-developers/)。里德·霍夫曼认为，每个职业都会有一个[共同开发者](https://fortune.com/2022/12/07/linkedin-founder-reid-hoffman-on-ai-human-amplification/)。微软正在将一个[人工智能共同开发者集成到
    Office 中](https://techcrunch.com/2023/03/06/microsoft-dynamics-copilot/)。Diagram
    正在为设计师构建一个[共同开发者](https://genius.design/)。名单还在继续。
- en: These systems work like a superpowered autocomplete. They predict what you’re
    about to do, and then offer that to you before you have a chance to do it yourself.
    It saves time and effort.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这些系统就像是超级强大的自动补全。它们预测你即将要做的事情，然后在你自己做之前就将其提供给你。这样能节省时间和精力。
- en: 'If you’re a builder hacking away on side projects, you’re probably thinking
    about building a copilot too. GPT-3 makes this kind of thing pretty easy to pull
    together over a weekend. I know because I’ve been doing it too:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是一个在做副业项目的开发者，你可能正在考虑也要构建一个共同开发者。GPT-3使这种事情在周末能够相当容易地实现。我知道因为我也一直在做：
- en: 'I built a little copilot for my mind. I want it to help make me smarter: to
    make connections between ideas, bring up pieces of supporting evidence for points
    I’m making, and suggest quotes to use as I’m writing.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我为自己建了一个小共同开发者。我希望它能帮助我变得更聪明：在我写作时建立思维之间的联系，为我所做的观点提供支持性证据的片段，并建议我可以使用的引用。
- en: It takes in any chunk of text, and then attempts to complete the chunk using
    quotes it finds in my [Readwise](https://readwise.io/) database.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 它接收任何文本块，然后尝试使用我在[Readwise](https://readwise.io/)数据库中找到的引用来完成这个文本块。
- en: It’s a cool demo, but it isn’t anywhere close to being a usable product yet.
    It doesn’t always pull great quotes for me, and it doesn’t always complete them
    in a way that actually supports the point I’m trying to make. It also doesn’t
    demonstrate sufficient understanding of my writing, or the writing of the authors
    it’s pulling from to be useful.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个很酷的演示，但它离成为一个可用产品还有很长一段距离。它并不总是为我提供很好的引用，也不总是以实际支持我要表达的观点的方式来完成它们。它也没有展示足够的理解我的写作，或者它正在提取的作者的写作，以便有用。
- en: As I wrote in the [End of Organizing](https://every.to/chain-of-thought/the-end-of-organizing),
    I’m quite optimistic about the future of technologies like this. I find myself
    reading fewer and fewer physical books and taking fewer physical notes. I’m increasingly
    confident that every digital highlight I make will be made 10x more useful by
    these tools in the next year or so.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'The question for me and other builders like me, is this: Where can these kinds
    of copilot experiences *actually* delivervalue with the technology that’s available
    *today*? And what are the bottlenecks that need to be resolved to make these useful
    for more use cases?'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take these one at a time.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: ''
  id: totrans-20
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[Lever](https://www.lever.co/demo/?utm_medium=third_party_email&utm_source=other&utm_campaign=none&utm_content=every_divinations_sponsored_newsletter)
    is the leading Talent Acquisition Suite that makes it easy for talent teams to
    reach their hiring goals and to connect companies with top talent.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: With LeverTRM, talent leaders can scale and grow their people pipeline, build
    authentic long-lasting relationships, and source the right people to hire. And,
    thanks to [LeverTRM's](https://www.lever.co/demo/?utm_medium=third_party_email&utm_source=other&utm_campaign=none&utm_content=every_divinations_sponsored_newsletter)
    Analytics, you get customized reports with data visualization, offers completed,
    interview feedback, and much more—so you can make better, more informed, and strategic
    hiring decisions.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Where copilots can be built today
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AI-based copilots are quite useful with out-of-the-box technology in situations
    where small pieces of lightly transformed boilerplate text provide a lot of value.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 'This is particularly true in areas where:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Text can be checked for accuracy quickly with little user effort
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The cost of inaccuracies is low
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Relevant text can be found reliably with [embeddings search](https://platform.openai.com/docs/guides/embeddings)
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: GitHub CoPilot is a great example of this. But other examples are things like
    [grant writing](https://grantable.co/), [contract writing](https://motionize.io/),
    tax prep, many types of [email replies](https://chrome.google.com/webstore/detail/ellie-your-ai-email-assis/mhcnlcilgicfodlpjcacgglchmpoojcp),
    [RFP responses](https://www.userogue.com/), medical recommendations to doctors,
    and more.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: If you’re building a copilot (or thinking of building one), I’ve put together
    a little checklist for you to go through in order to figure out whether it will
    be possible to get good results with today’s technology.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Can you build a copilot for it? A checklist.
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you want to build a copilot for a specific domain using today’s technology
    here’s the list of things you need to check off:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Is there a corpus of relevant text completions to be used by this copilot?
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can relevant text for completions be found reliably with embeddings search over
    this text corpus?
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can those pieces of text, without more context needed, be lightly transformed
    and inserted as an accurate completion?
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些文本片段，无需更多上下文，能够轻松转换并作为准确的完成插入吗？
- en: Can completions be checked for accuracy with little to no user effort?
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成可以在几乎不费吹灰之力的情况下检查其准确性吗？
- en: '**Is there a corpus of relevant text completions to be used by this copilot?**'
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**有一个相关文本完成的语料库可以供这个合作伙伴使用吗？**'
- en: You want your copilot to be smart and not make things up. It should have access
    to some source of knowledge that it can bring to the user when they need it. Ideally,
    this source of knowledge is accurate, up to date, and maybe even personal to the
    user—for example, it might include all of their emails or their company’s internal
    wiki.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 你希望你的合作伙伴聪明，不要凭空想象。它应该能够访问一些知识来源，在用户需要时将其带给用户。理想情况下，这个知识来源应该是准确的、最新的，甚至可能是用户个人的，例如，它可能包括所有他们的电子邮件或他们公司的内部维基。
- en: If you have this, you’re ready to go to the next step.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有这个，你就可以进行下一步了。
- en: '**Can relevant text for completions be found reliably with embeddings search
    over this text corpus?**'
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**可以可靠地通过这个文本语料库进行嵌入搜索以找到相关的文本完成吗？**'
- en: Once you have a knowledge base for your copilot to use, you need the copilot
    to be able to accurately identify chunks of that knowledge base to return to the
    user when they need it. For example, in my copilot for thought demo, I needed
    my copilot to find quotes from my Readwise that were relevant to whatever I was
    currently writing.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了合作伙伴使用的知识库，你需要合作伙伴能够准确地识别这个知识库的片段，当用户需要时将其返回给用户。例如，在我的思考合作伙伴演示中，我需要我的合作伙伴找到我的
    Readwise 中与我当前所写内容相关的引用。
- en: The standard way to do this is to use embeddings search. Embeddings are a condensed
    mathematical representation of a piece of text. Just like latitude and longitude
    can help you tell how close two cities are on a map, embeddings do the same kind
    of thing for text chunks. If you want to know if two pieces of text are similar,
    calculate the embeddings for them and compare them. Text chunks with embeddings
    that are “closer” together are similar.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的标准方法是使用嵌入搜索。嵌入是文本的一种紧凑的数学表示。就像纬度和经度可以帮助你确定地图上两个城市的距离一样，嵌入也可以为文本片段做同样的事情。如果你想知道两个文本片段是否相似，就计算它们的嵌入并进行比较。嵌入在“接近”一起的文本片段是相似的。
- en: Embeddings are useful because when a user is typing something that the copilot
    wants to autocomplete, it can just look through its knowledge base to find pieces
    of text that are “close” to whatever the user is typing.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入很有用，因为当用户正在输入合作伙伴想要自动完成的内容时，它只需浏览其知识库，找到与用户正在输入的内容“接近”的文本片段。
- en: But embeddings aren’t perfect, and it’s where a lot of copilot use cases fail
    for now. Your copilot quality is going to be bounded by your ability to find relevant
    chunks of information in your knowledge base to help the user. If you’re not getting
    relevant results, completion accuracy will suffer.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 但是嵌入不是完美的，这也是目前许多合作伙伴使用案例失败的地方。你的合作伙伴质量将受限于你在知识库中找到相关信息的能力，以帮助用户。如果你没有得到相关的结果，完成的准确性将受到影响。
- en: If you *can* get relevant results, then you can go to the next step.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你*能够*获得相关的结果，那么你可以进行下一步了。
- en: '**Can those pieces of text, without more context needed, be lightly transformed
    and inserted as an accurate completion?**'
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**这些文本片段，无需更多上下文，能够轻松转换并作为准确的完成插入吗？**'
- en: Once you can find the most relevant pieces of information in your knowledge
    base from embeddings search, your copilot is going to need to intelligently package
    them up as a completion for the user.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你可以从嵌入搜索中找到知识库中最相关的信息，你的合作伙伴就需要以智能的方式将它们打包成用户的完成。
- en: This works best if they only need to be slightly transformed before they can
    be suggested. For example, you’re often going to want to rearrange the text so
    that it carries the same information but is rephrased so that it completes the
    user's sentence. This kind of transformation is easy to do with GPT-3, but more
    advanced transformations are harder to do.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方式在它们只需要在建议之前稍微转换时效果最佳。例如，你经常希望重新排列文本，使其传达相同的信息，但重新措辞以完成用户的句子。这种转换在 GPT-3
    中很容易做到，但更高级的转换则更难做到。
- en: '**Can completions be checked for accuracy with little to no user effort?**'
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**完成可以在几乎不费吹灰之力的情况下检查其准确性吗？**'
- en: Once your copilot suggests a completion, it works best if the user knows whether
    or not the completion is accurate without a lot of work. If the user has to spend
    a lot of time figuring out if the completion is accurate or not, they’ll just
    ignore it.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你的合伙人提出一个完成方案，最好让用户知道这个完成方案是否准确，而不需要花费太多功夫。如果用户必须花费大量时间来判断完成方案是否准确，他们就会忽视它。
- en: 'This is one of the big levers for copilots. If you can make it easy to check
    a completion without a lot of work, your copilot can return a lot of wrong answers
    because it doesn’t cost the user much to consider them. I think this is part of
    why GitHub Copilot is successful: You can just run the code to see if it’s right,
    so the computer generates the code and then checks it for you.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这是合伙人的一个重要杠杆之一。如果您能够轻松检查一个完成方案而不需要花费太多精力，那么您的合伙人可以返回许多错误答案，因为用户不需要花费太多精力来考虑它们。我认为这是
    GitHub Copilot 成功的一部分：您只需运行代码来查看它是否正确，因此计算机生成代码然后为您检查它。
- en: Other use cases that require more user input will require correspondingly higher
    rates of accuracy for the user to feel motivated enough to check.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 需要更多用户输入的其他用例将需要相应更高的准确率，以使用户感到有动力进行检查。
- en: What might change this list?
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么可能改变这个列表？
- en: The limits of a copilot are the limits of the AI’s context window. The context
    window is the amount of tokens you can feed into the AI in the prompt, and the
    amount of tokens it can give back in a completion.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 合伙人的限制是人工智能的上下文窗口的限制。上下文窗口是您可以将标记馈送到提示中的数量，以及它可以在完成中返回的标记数量。
- en: Because context windows are limited, you have to use embeddings search to find
    little pieces of information you can feed to your AI for it to generate a copilot
    completion. This means that while context windows are still small, the quality
    of your copilot is bounded by the quality of your embeddings search.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 因为上下文窗口是有限的，所以您必须使用嵌入搜索来找到可以馈送给人工智能以生成合伙人完成的小片段信息。这意味着，在上下文窗口仍然很小的情况下，您的合伙人的质量受到嵌入搜索质量的限制。
- en: GPT-3’s current context window is 4,096 tokens, which is about 3,000 words.
    OpenAI is rumored to soon be releasing a version of its models that have a 32K
    token context window—roughly 8x the current size. This, I think, would be a giant
    step change in the quality of the responses that are returned for copilot use
    cases.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 当前的上下文窗口是 4,096 个标记，大约是 3,000 个单词。有传言说 OpenAI 即将发布一个上下文窗口为 32K 标记的模型版本——大约是当前尺寸的
    8 倍。我认为，这将是对合伙人使用情况返回的响应质量的巨大改变。
- en: You’d be able to return far more information for the AI to reason over and turn
    into a usable response, which would have a direct impact on accuracy.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 您将能够返回更多信息以供人工智能推理，并将其转换为可用的响应，这将直接影响准确性。
- en: The other big limiters here are inference cost, inference speed, embedding cost,
    and access to usable data. I expect cost to go down and speed to go up significantly
    enough that I’m not worried about them as true bottlenecks. But access to usable
    data is a big deal.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的另一个重要限制因素是推理成本、推理速度、嵌入成本和可用数据的访问。我预计成本会下降，速度会显著提高，以至于我不担心它们成为真正的瓶颈。但是可用数据的访问是一件大事。
- en: Right now, I’m using Readwise as my data source. But my completions for my copilot
    would be a lot better if it had access to the books that I am pulling from. The
    average number of tokens in a book is on the order of 80,000 tokens. So in order
    to increase the quality of my responses, I need to figure out how to make that
    data available to the AI, and also clean it so that it’s easy for it to find relevant
    passages.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我正在使用 Readwise 作为我的数据来源。但是，如果我的合伙人可以访问我提取的书籍，我的完成方案会好得多。一本书中的平均标记数量约为 80,000
    个标记。因此，为了提高我的响应质量，我需要想办法让这些数据对人工智能可用，并且对其进行清理，以便它能够找到相关的段落。
- en: Advice for builders
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 建议给建造者
- en: 'If you’re building or investing in this space, my recommendations for creating
    better copilot experiences are as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在构建或投资于这个领域，我对创建更好的合伙人体验的建议如下：
- en: '**Tighten your feedback loops**'
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**加紧您的反馈循环**'
- en: 'You can think of a copilot completion as a sequential chain:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以把一个合伙人完成视为一个连续的链条：
- en: Get user input
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取用户输入
- en: Query for relevant documents
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查询相关文档
- en: Prompt model with documents
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用文档提示模型
- en: Return a result
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回一个结果
- en: As you’re developing a copilot experience, you’ll want to be able to iterate
    as quickly as possible on each of these parts of the chain, with as little code
    as possible. I recommend building tools to help you do this quickly.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在开发助手体验时，你希望尽可能快地迭代链条中的每一部分，并且尽可能少的代码。我建议建立工具来帮助你快速完成这些工作。
- en: 'As I was building my notes copilot, I built a little UI to visualize and quickly
    swamp out each part of the chain:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当我建立我的笔记助手时，我建立了一个小的用户界面来可视化并快速更换链条的每一部分：
- en: This worked well for me, but you should explore your own solutions.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这对我来说效果很好，但你应该探索自己的解决方案。
- en: '**Get creative with embeddings search**'
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**发挥创意进行嵌入搜索**'
- en: For now, the quality of your completions is limited by the quality of your embeddings
    search. Because of this, I’d recommend spending time focusing on increasing the
    quality of your embeddings search.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，你的完成质量受限于你的嵌入式搜索的质量。因此，我建议花时间专注于提高您的嵌入式搜索的质量。
- en: There are many ways to enhance embeddings search to help you get more relevant
    documents. For example, check out [HyDE](https://github.com/texttron/hyde) for
    a creative solution to this problem from the query side. Or, try using GPT-3 to
    summarize the data in your knowledge base to make it easier for embeddings to
    find a usable text chunk.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法可以增强嵌入式搜索，以帮助您获取更相关的文档。例如，请查看[HyDE](https://github.com/texttron/hyde)以从查询方面解决此问题的创意解决方案。或者，尝试使用
    GPT-3 对您的知识库中的数据进行总结，以使嵌入能够找到可用的文本片段更容易。
- en: '**Decrease cost of checking for accuracy**'
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**降低检查准确性的成本**'
- en: 'The other big lever here to create a good experience with existing technology
    is to lower the cost to the user if the accuracy of completions is low. An easy
    one: Before you display anything to the user, use GPT-3 to check if it thinks
    the completion is any good. If not, then don’t display it.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，创建与现有技术一致的良好体验的另一个重要杠杆是降低用户的成本，如果完成的准确性低。一个简单的方法：在向用户显示任何内容之前，使用 GPT-3 检查它是否认为完成是好的。如果不是，那就不要显示它。
- en: 'But there are lots of other ways to do this. For example, make sure completions
    are quite short. Another example: Make sure that all of the context information
    the user would need to check for accuracy is included in the completion—so they
    don’t have to do research or think too hard.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 但是还有很多其他方法可以做到这一点。例如，请确保完成的内容非常简短。另一个例子：确保用户需要检查准确性的所有上下文信息都包含在完成中，这样他们就不必进行研究或思考太多。
- en: Wrapping up
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: 'This is the ideal copilot in my mind:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在我看来，这是理想的助手：
- en: Every time you touch your keyboard it brings to bear your entire archive of
    notes, and everything you’ve ever read, to help you complete your next sentence.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 每次您触摸键盘时，它都会调用您的整个笔记存档和您曾经阅读过的所有内容，以帮助您完成下一句话。
- en: It would help you make connections between ideas, bring up pieces of supporting
    evidence, and suggest quotes to use. It might also bring up writers you love who
    disagree with the point you’re making—so you could change your mind, or sharpen
    your argument in response to theirs.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 它将帮助你在思想之间建立联系，提出支持性证据的片段，并建议使用的引用。它还可能提出你喜欢的作家，他们对你正在提出的观点持不同意见——这样你可以改变主意，或者根据他们的反驳来锋利地调整你的论点。
- en: Ideally, it would do this in a fashion that’s seamless, highly accurate, and
    easily checked. In other words, usually if it completes something it’s making
    a good point, and it’s easy for you to tell if the point is good or not, without
    lots of extra effort.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，它会以一种无缝、高度准确且易于检查的方式完成这些工作。换句话说，通常如果它完成了某事，它是在表达一个好的观点，并且你很容易判断观点是否好，而无需付出太多额外的努力。
- en: This is far from the reality today. If we want to advance these kinds of tools
    beyond just being interesting demos, we’re going to have to build them ourselves.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 目前的情况远非如此。如果我们想要将这些工具推进到不仅仅是有趣的演示，我们必须自己来建立它们。
- en: I hope this post pushes a few of you in that direction. I’ll keep you posted
    as I keep discovering more.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这篇文章能推动一些人朝着这个方向前进。随着我不断发现更多内容，我会继续通知你们。
