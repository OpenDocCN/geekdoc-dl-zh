- en: All About Transformer Inference
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 所有关于Transformer推理的内容
- en: 原文：[https://jax-ml.github.io/scaling-book/inference](https://jax-ml.github.io/scaling-book/inference)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://jax-ml.github.io/scaling-book/inference](https://jax-ml.github.io/scaling-book/inference)
- en: '<d-title>Part 7 of [How To Scale Your Model](/scaling-book) ([Part 6: Training
    LLaMA](../applied-training) | [Part 8: Serving LLaMA](../applied-inference))'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: <d-title>《如何扩展你的模型》第7部分[如何扩展你的模型](/scaling-book) ([第6部分：训练LLaMA](../applied-training)
    | [第8部分：服务LLaMA](../applied-inference))
- en: 'Performing inference on a Transformer can be very different from training.
    Partly this is because inference adds a new factor to consider: latency. In this
    section, we will go all the way from sampling a single new token from a model
    to efficiently scaling a large Transformer across many slices of accelerators
    as part of an inference engine.</d-title>  <d-byline><d-article><d-contents>###
    Contents'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在Transformer上执行推理可能与训练非常不同。部分原因是因为推理增加了一个需要考虑的新因素：延迟。在本节中，我们将从从模型中采样单个新标记开始，到作为推理引擎的一部分，高效地扩展大型Transformer到许多加速器的多个切片。</d-title>  <d-byline><d-article><d-contents>###
    内容
- en: '[The Basics of Transformer Inference](#the-basics-of-transformer-inference)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[Transformer推理的基础](#the-basics-of-transformer-inference)'
- en: '[What do we actually want to optimize?](#what-do-we-actually-want-to-optimize)'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[我们实际上想优化什么？](#what-do-we-actually-want-to-optimize)'
- en: '[Linear operations: what bottlenecks us?](#linear-operations-what-bottlenecks-us)'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[线性操作：什么瓶颈了我们？](#linear-operations-what-bottlenecks-us)'
- en: '[What about attention?](#what-about-attention)'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[关于注意力呢？](#what-about-attention)'
- en: '[Theoretical estimates for LLM latency and throughput](#theoretical-estimates-for-llm-latency-and-throughput)'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LLM延迟和吞吐量的理论估计](#theoretical-estimates-for-llm-latency-and-throughput)'
- en: '[What about memory?](#what-about-memory)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[关于内存呢？](#what-about-memory)'
- en: '[Modeling throughput and latency for LLaMA 2-13B](#modeling-throughput-and-latency-for-llama-2-13b)'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[为LLaMA 2-13B建模吞吐量和延迟](#modeling-throughput-and-latency-for-llama-2-13b)'
- en: '[Tricks for Improving Generation Throughput and Latency](#tricks-for-improving-generation-throughput-and-latency)[Distributing
    Inference Over Multiple Accelerators](#distributing-inference-over-multiple-accelerators)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[提高生成吞吐量和延迟的技巧](#tricks-for-improving-generation-throughput-and-latency)[在多个加速器上分配推理](#distributing-inference-over-multiple-accelerators)'
- en: '[Prefill](#prefill)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[预填充](#prefill)'
- en: '[Generation](#generation)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[生成](#generation)'
- en: '[Sharding the KV cache](#sharding-the-kv-cache)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KV缓存的分片](#sharding-the-kv-cache)'
- en: '[Designing an Effective Inference Engine](#designing-an-effective-inference-engine)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[设计有效的推理引擎](#designing-an-effective-inference-engine)'
- en: '[Continuous batching](#continuous-batching)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[连续批处理](#continuous-batching)'
- en: '[Prefix caching](#prefix-caching)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[前缀缓存](#prefix-caching)'
- en: '[Let''s look at an implementation: JetStream](#let-s-look-at-an-implementation-jetstream)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[让我们看看一个实现：JetStream](#let-s-look-at-an-implementation-jetstream)'
- en: '[Worked Problems](#worked-problems)[Appendix](#appendix)</d-contents>'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[练习题](#worked-problems)[附录](#appendix)</d-contents>'
- en: The Basics of Transformer Inference
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformer推理的基础
- en: So you’ve trained a Transformer, and you want to use it to generate some new
    sequences. *At the end of the day, benchmark scores going up and loss curves going
    down are only proxies for whether something interesting is going to happen once
    the rubber hits the road!*<d-footnote>Historically, you can do a surprising amount
    of research on Transformers without ever touching inference — LLM loss, multiple
    choice benchmarks can be run efficiently without a proper KV cache or generation
    loop implementation. This meant, especially in research codebases, there's often
    a lot of low hanging fruits in the inference codepath.</d-footnote>
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你已经训练了一个Transformer，并希望用它来生成一些新的序列。*最终，基准分数的提升和损失曲线的下降只是对实际应用中是否会发生有趣的事情的代理！*<d-footnote>从历史上看，你可以在不接触推理的情况下对Transformer进行大量的研究——LLM损失、多项选择基准可以在没有适当的KV缓存或生成循环实现的情况下高效运行。这意味着，特别是在研究代码库中，推理代码路径中往往有很多低垂的果实。</d-footnote>
- en: Sampling is conceptually simple. We put a sequence in and our favorite Transformer
    will spit out \(\log p(\text{next token}_i \vert \text{previous tokens})\), i.e.
    log-probabilities for all possible next tokens. We can sample from this distribution
    and obtain a new token. Append this token and repeat this process and we obtain
    a sequence of tokens which is a continuation of the prompt.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 采样在概念上很简单。我们输入一个序列，我们最喜欢的Transformer会输出 \(\log p(\text{next token}_i \vert \text{previous
    tokens})\)，即所有可能下一个标记的对数概率。我们可以从这个分布中进行采样并获得一个新标记。将此标记附加并重复此过程，我们获得一个标记序列，它是提示的延续。
- en: <picture>![](../Images/a514b2077c183b719095be84c5d2a861.png)</picture>
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: <picture>![](../Images/a514b2077c183b719095be84c5d2a861.png)</picture>
- en: '**Figure:** naive sampling from a Transformer. The blue logits give us a distribution
    over the next token that we can sample from. Note that each step re-processes
    the entire prefix, leading to a $\Theta(n^2)$ runtime for the algorithm.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**图示**：从 Transformer 进行朴素采样。蓝色的对数给我们提供了一个关于下一个标记的分布，我们可以从中采样。请注意，每个步骤都会重新处理整个前缀，导致算法的运行时间为
    \(\Theta(n^2)\)。'
- en: We have just described the naive implementation of Transformer sampling, and
    while it works, **we never do it in practice** because we are re-processing the
    entire sequence every time we generate a token. This algorithm is \(O(n^2)\) on
    the FFW and \(O(n^3)\) on the attention mechanism to generate \(n\) tokens!
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚描述了 Transformer 样本的朴素实现，虽然它可行，但我们**从未在实际中这样做**，因为每次生成一个标记时，我们都会重新处理整个序列。这个算法在
    FFW 上是 \(O(n^2)\)，在生成 \(n\) 个标记的注意力机制上是 \(O(n^3)\)！
- en: '**How do we avoid this?** Instead of doing the full forward pass every time,
    it turns out we can save some intermediate activations from each forward pass
    that let us avoid re-processing previous tokens. Specifically, since a given token
    only attends to previous tokens during dot-product attention, we can simply write
    each token’s key and value projections into a new data structure called a **KV
    cache**. Once we’ve saved these key/value projections for past tokens, future
    tokens can simply compute their \(q_i \cdot k_j\) products without performing
    any new FLOPs on the earlier tokens. Amazing!'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们如何避免这种情况**？不是每次都进行完整的正向传递，实际上我们可以从每次正向传递中保存一些中间激活，这样我们就可以避免重新处理之前的标记。具体来说，由于一个给定的标记在点积注意力中只关注之前的标记，我们可以简单地将每个标记的键和值投影写入一个新的数据结构，称为**键值缓存**。一旦我们为过去的标记保存了这些键/值投影，未来的标记就可以简单地计算它们的
    \(q_i \cdot k_j\) 积，而无需在早期标记上执行任何新的浮点运算。太神奇了！'
- en: 'With this in mind, inference has two key parts:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，推理有两个关键部分：
- en: '**Prefill**: Given a long prompt, we process all the tokens in the prompt at
    the same time and save the resulting activations (specifically, the key-value
    projections) in a **“KV cache”**. We also save the logits for the last token.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预填充**：给定一个长的提示，我们同时处理提示中的所有标记，并将生成的激活（特别是键值投影）保存到一个**“键值缓存”**中。我们还将最后一个标记的对数保存起来。'
- en: '**Generation**: Given a KV cache and the previous logits, we incrementally
    sample one token from the logits, feed that token back into the Transformer, and
    produce a new set of logits for the next step. We also append the KV activations
    for that new token to the KV cache. We repeat this until we hit a special `<EOS>`
    token or reach some maximum length limit.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成**：给定一个键值缓存和前一个对数，我们逐步从一个对数中采样一个标记，将该标记反馈到 Transformer 中，并为下一个步骤生成一组新的对数。我们还把新标记的键值激活追加到键值缓存中。我们重复此过程，直到遇到特殊的
    `<EOS>` 标记或达到某个最大长度限制。'
- en: 'Here’s a diagram of sampling with a KV cache:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是使用键值缓存的采样图示：
- en: <picture>![](../Images/2ec9f5ea3c5d41f0d59d2048d68c60f4.png)</picture>
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: <picture>![](../Images/2ec9f5ea3c5d41f0d59d2048d68c60f4.png)</picture>
- en: '**Figure:** diagram of efficient Transformer sampling with a KV cache. **Prefill**
    processes our prompt and saves all the per-token key-value activations in a cache.
    **Generation** takes this cache (and the last-token logits), samples a new token,
    and passes that new token through the model, attending to the KV cache and saving
    the new token''s key-value projections back to the cache. This is an $O(n)$ algorithm
    in the MLP block.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**图示**：使用键值缓存的效率高的 Transformer 样本图。**预填充**处理我们的提示，并将每个标记的键值激活保存到缓存中。**生成**使用这个缓存（以及最后一个标记的对数），采样一个新的标记，并将这个新标记通过模型传递，关注键值缓存，并将新标记的键值投影保存回缓存。这是
    MLP 块中的 \(O(n)\) 算法。'
- en: By sampling with a KV cache, we’ve reduced our time complexity to generate $n$
    tokens to \(O(n)\) on the FFW and \(O(n^2)\) on the attention, since we never
    reprocess a previous token. However, many forward passes are still needed to generate
    a sequence — that’s what’s happening when you query Gemini or ChatGPT and the
    result streams back to you. Every token is (usually) a separate (but partially
    cached) Transformer call to a massive model.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用键值缓存进行采样，我们已经将生成 \(n\) 个标记的时间复杂度降低到 FFW 上的 \(O(n)\) 和注意力机制上的 \(O(n^2)\)，因为我们永远不会重新处理之前的标记。然而，仍然需要许多前向传递来生成一个序列——这就是当你查询
    Gemini 或 ChatGPT 时发生的事情，结果流回到你这里。每个标记（通常）都是一个单独的（但部分缓存的）Transformer 调用到一个大型模型。
- en: We will soon see that **prefill** and **generation** are very different beasts
    —— Transformer inference is two tasks in disguise! Compared to training, the KV
    cache is also a novel and significant source of complexity.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很快就会看到，**预填充**和**生成**是两种非常不同的生物——Transformer推理实际上是两个隐藏的任务！与训练相比，KV缓存也是一个新颖且重要的复杂性来源。
- en: What do we actually want to optimize?
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们实际上想要优化什么？
- en: 'Before we proceed further, it’s worth highlighting one aspect of inference
    that’s totally new: latency. While during training we only care about throughput
    (total tokens processed per second **per chip**), during inference we have to
    worry about how fast we’re producing tokens (both the **Time To First Token (TTFT)**
    and the **per-token latency**). For example:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进一步讨论之前，值得强调推理的一个全新方面：延迟。在训练期间，我们只关心吞吐量（每秒每芯片处理的token总数），但在推理期间，我们必须关注我们生成token的速度（包括**首次token时间（TTFT）**和**每token延迟**）。例如：
- en: '**Offline batch inference** for evals and data generation only cares about
    bulk cost of inference and is blind to the latency of individual samples.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**离线批量推理**仅关注推理的总体成本，对单个样本的延迟视而不见。'
- en: '**Chat interfaces/streaming tasks** need to run cheaply at scale while having
    low TTFT and generating tokens fast enough to exceed human reading speed.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聊天界面/流式任务**需要在规模上以低成本运行，同时具有低TTFT（Token Time To First Token）并快速生成足够的token以超过人类阅读速度。'
- en: '**Edge inference** (e.g. `llama.cpp` on your laptop) only needs to service
    one user at a time at the lowest possible latency, potentially with heavy hardware
    constraints.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**边缘推理**（例如，你的笔记本电脑上的`llama.cpp`）只需要以最低可能的延迟为一次服务一个用户，可能还有重硬件约束。'
- en: Maximizing hardware utilization is still critical and helps with cost and TTFT,
    but unlike training, it does not *necessarily* translate to better experience
    for individual users in all contexts. Many optimizations at the accelerator, systems
    and model architectural level make tradeoffs between latency, throughput, context
    length and even model quality.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最大化硬件利用率仍然至关重要，有助于成本和TTFT，但与训练不同，它并不**必然**会转化为所有场景下单个用户更好的体验。许多在加速器、系统和模型架构层面的优化在延迟、吞吐量、上下文长度甚至模型质量之间做出权衡。
- en: A more granular view of the Transformer
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对Transformer的更细粒度视图
- en: 'So far we’ve mostly treated a Transformer as a stack of feedforward blocks.
    While this is often reasonable from a FLOPs and memory standpoint, it’s not sufficient
    to properly model inference.<d-footnote>One thing you''ll notice throughout this
    section is that inference is much less forgiving than training. We typically have
    far fewer FLOPs, less opportunity for batching, and a much greater sensitivity
    to latency. KV caches dramatically complicate inference as well.</d-footnote>
    As we saw in [Part 4](../transformers), the major components of a Transformer
    forward pass are:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们主要将Transformer视为一系列前馈块。虽然从FLOPs和内存的角度来看，这通常是合理的，但它不足以正确地模拟推理。<d-footnote>在本节中，你会发现推理远不如训练宽容。我们通常有更少的FLOPs，更少的批处理机会，以及对延迟的敏感性更大。KV缓存也极大地复杂化了推理。</d-footnote>正如我们在[第4部分](../transformers)中看到的那样，Transformer前向传递的主要组件包括：
- en: '**A bunch of linear operations**, including the MLP ($W_{in}$, $W_{out}$) and
    the attention QKV projections and output projections ($W_Q$, $W_K$, $W_V$, and
    $W_O$). These all involve reading parameters and a batch of activations from HBM,
    doing some FLOPs, and writing the result back to HBM.'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**一系列线性操作**，包括MLP（$W_{in}$, $W_{out}$）和注意力QKV投影以及输出投影（$W_Q$, $W_K$, $W_V$,
    和 $W_O$）。这些都涉及从HBM读取参数和一批激活，进行一些FLOPs，并将结果写回HBM。'
- en: '**Dot-product attention**. We need to read a batch of key-value projections
    and a batch of query activations from HBM, do a few inner products and some softmax
    operations, and write the attention result back to HBM.'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**点积注意力**。我们需要从HBM读取一批键值投影和一批查询激活，进行一些内积和一些softmax操作，并将注意力结果写回HBM。'
- en: '**Everything else**, including applying layer norms, activation functions,
    tokens sampling, updating KV caches, and positional embeddings. These do take
    some FLOPs, but are dominated by, or fused into, the above.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**其他所有内容**，包括应用层归一化、激活函数、token采样、更新KV缓存和位置嵌入。这些都涉及一些FLOPs，但主要由上述内容主导或融合。'
- en: For the next couple of sections, we’re going to look at each of these in the
    context of prefill and generation and ask what is likely to bottleneck our performance.
    Within a single accelerator, are we compute-bound or memory-bound? We want to
    emphasize how different the answers will be for prefill versus generation.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将从预填充和生成的角度逐一审视这些内容，并探讨什么可能成为我们性能的瓶颈。在单个加速器中，我们是计算受限还是内存受限？我们想要强调预填充与生成之间的答案会有多么不同。
- en: 'Linear operations: what bottlenecks us?'
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线性操作：什么成为我们的瓶颈？
- en: 'All our linear operations are conceptually the same, whether they live in the
    MLP block or attention. Their arithmetic intensity depends on the batch size.
    We did this math in [Section 1](../roofline) but it’s worth repeating. Let’s look
    at a single matrix multiply of a $\text{bf16[B, D]}$ batch by a $\text{bf16[D,
    F]}$ matrix. This could be the big MLP block ($W_\text{in}$ or $W_\text{out}$)
    or one of the smaller attention projections ($W_Q$, $W_K$, $W_V$, $W_O$). To do
    this matmul, we need to load both of these arrays from HBM into the MXU, do the
    multiplicaton, then write the result back to HBM. As before, we have:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的所有线性操作在概念上都是相同的，无论是位于 MLP 块还是注意力中。它们的算术强度取决于批量大小。我们在[第 1 节](../roofline)中进行了这个数学运算，但值得重复。让我们看看单个
    $\text{bf16[B, D]}$ 批量与 $\text{bf16[D, F]}$ 矩阵的矩阵乘法。这可能是一个大的 MLP 块（$W_\text{in}$
    或 $W_\text{out}$）或较小的注意力投影之一（$W_Q$，$W_K$，$W_V$，$W_O$）。为了进行这个矩阵乘法，我们需要将这两个数组从 HBM
    加载到 MXU 中，进行乘法运算，然后将结果写回 HBM。和之前一样，我们有：
- en: \[T_\text{math} = \frac{\text{Computation FLOPs}}{\text{Accelerator FLOPs/s}}
    = \frac{2BDF}{\text{Accelerator FLOPs/s}}\] \[T_\text{comms} = \frac{\text{Communication
    Bytes}}{\text{Bandwidth Bytes/s}} = \frac{2BD + 2FD + 2BF}{\text{Bandwidth Bytes/s}}\]
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: \[T_\text{math} = \frac{\text{计算 FLOPs}}{\text{加速器 FLOPs/s}} = \frac{2BDF}{\text{加速器
    FLOPs/s}}\] \[T_\text{comms} = \frac{\text{通信 字节}}{\text{带宽 字节/s}} = \frac{2BD
    + 2FD + 2BF}{\text{带宽 字节/s}}\]
- en: 'A TPU or GPU can overlap these by loading as it does the compute, so to be
    compute-bound, we need \(T_\text{math} \geq T_\text{comms}\), or:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: TPU 或 GPU 可以在计算的同时加载，因此为了成为计算受限，我们需要 \(T_\text{math} \geq T_\text{comms}\)，或者：
- en: \[\frac{2BDF}{2BD + 2DF + 2BF} \geq \frac{\text{Accelerator FLOPs/s}}{\text{Bandwidth
    Bytes/s}} \underset{\text{TPU v5e}}{=} \frac{1.97E+14}{8.20E+11} = 240\]
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: \[\frac{2BDF}{2BD + 2DF + 2BF} \geq \frac{\text{加速器 FLOPs/s}}{\text{带宽 字节/s}}
    \underset{\text{TPU v5e}}{=} \frac{1.97E+14}{8.20E+11} = 240\]
- en: where the RHS is the arithmetic intensity of our hardware. Now let’s assume
    $D$ and $F$ are very large compared to $B$ (usually our batches are at most 500
    and $D$ and $F > 10k$), we can simplify the denominator by using the fact that
    $\small{2BD + 2DF + 2BF \approxeq 2DF}$ which gives us
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，RHS 是我们硬件的算术强度。现在假设 $D$ 和 $F$ 与 $B$ 相比非常大（通常我们的批量大小最多为 500，而 $D$ 和 $F > 10k$），我们可以通过使用
    $\small{2BD + 2DF + 2BF \approxeq 2DF}$ 的事实来简化分母，这给我们
- en: \[\begin{align*} \frac{2BDF}{2BD + 2DF + 2BF} \approxeq \frac{2BDF}{2DF} \geq
    \frac{\text{Accelerator FLOPs/s}}{\text{Bandwidth Bytes/s}} \\ \underset{\text{TPU
    v5e}}{=} \frac{1.97E+14}{8.20E+11} \implies B \geq 240 = B_{\text{crit}} \end{align*}\]
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{2BDF}{2BD + 2DF + 2BF} \approxeq \frac{2BDF}{2DF} \geq
    \frac{\text{加速器 FLOPs/s}}{\text{带宽 字节/s}} \\ \underset{\text{TPU v5e}}{=} \frac{1.97E+14}{8.20E+11}
    \implies B \geq 240 = B_{\text{crit}} \end{align*}\]
- en: If we quantize our weights or use lower precision FLOPs for the matrix multiplication,
    this critical batch size can change. For instance, if we quantize our weights
    to int8 or fp8, $B_\text{crit}$ decreases by 2x. If we do our FLOPs in int8 or
    fp8, $B_\text{crit}$ increases by 2x. Thus if we let $\beta = \text{bits per param}
    / \text{bits per activation}$ and $\alpha_\text{hbm} = C / W_\text{hbm}$, our
    critical batch size is actually $B_\text{crit} = \beta \alpha_\text{hbm}$.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将权重量化或使用较低精度的 FLOPs 进行矩阵乘法，这个关键批量大小可能会改变。例如，如果我们将权重量化为 int8 或 fp8，$B_\text{crit}$
    将减少 2 倍。如果我们使用 int8 或 fp8 进行 FLOPs，$B_\text{crit}$ 将增加 2 倍。因此，如果我们让 $\beta = \text{每参数位}
    / \text{每激活位}$ 和 $\alpha_\text{hbm} = C / W_\text{hbm}$，我们的关键批量大小实际上是 $B_\text{crit}
    = \beta \alpha_\text{hbm}$。
- en: '**Takeaway:** Transformer matmuls are compute-bound *iff* the per-replica **token**
    batch size is greater than $B_\text{crit} = C / W_\text{hbm} \cdot (\text{bits
    per param} / \text{bits per activation}) = \beta \cdot \alpha_\text{hbm}$. For
    bf16 activations on TPU v5e, this is 240 tokens. For an H100, it is about 280
    tokens.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**要点：** Transformer 矩阵乘法是计算受限的 *iff* 每个副本的 **token** 批量大小大于 $B_\text{crit}
    = C / W_\text{hbm} \cdot (\text{每参数位} / \text{每激活位}) = \beta \cdot \alpha_\text{hbm}$。对于
    TPU v5e 上的 bf16 激活，这是 240 个 token。对于 H100，大约是 280 个 token。'
- en: During training, we’ll have a high intensity during all our matrix multiplications
    because we reuse the same weights over a very large batch. **That high arithmetic
    intensity carries over to prefill, since user prompts are typically hundreds if
    not thousands of tokens long.** As we saw before, the hardware arithmetic intensity
    of a TPUv5e is 240, so if a sequence longer than 240 tokens is fed into a dense
    model running on this hardware at bf16, we would expect to be compute-bound and
    all is well. Prompts shorter than this can technically be batched together to
    achieve higher utilization, but this is typically not necessary.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，由于我们在非常大的批次上重复使用相同的权重，我们将在所有的矩阵乘法中具有高强度。**这种高算术强度会传递到预填充中，因为用户提示通常有数百甚至数千个令牌长。**
    如我们所见，TPUv5e的硬件算术强度为240，因此如果将超过240个令牌的序列输入到运行在bf16上的该硬件上的密集模型，我们预计会处于计算限制状态，一切顺利。比这短的提示在技术上可以一起批处理以实现更高的利用率，但这通常不是必要的。
- en: '**Takeaway:** During prefill, all matrix multiplications are basically always
    compute-bound. Therefore, simply maximizing hardware utilization or MFU (Model
    FLOPs Utilization) is enough to maximize throughput-per-chip (cost) and latency
    (in the form of TTFT). Unless prompts are extremely short, batching at a per-prompt
    level only adds latency for a small improvements in prefill throughput.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结：** 在预填充过程中，所有矩阵乘法基本上总是计算限制的。因此，仅简单地最大化硬件利用率或MFU（模型FLOPs利用率）就足以最大化每芯片的吞吐量（成本）和延迟（以TTFT的形式）。除非提示非常短，否则在提示级别进行批处理只会增加延迟，而只会带来预填充吞吐量的微小改进。'
- en: However, during generation, for each request, we can only do our forward passes
    one token at a time since there’s a sequential dependency between steps! Thus
    we can only (easily) achieve good utilization by batching multiple requests together,
    parallelizing over the batch dimension. We’ll talk about this more later, but
    actually batching many concurrent requests together without affecting latency
    is hard. For that reason, **it is much harder to saturate the hardware FLOPs with
    generation.**
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在生成过程中，对于每个请求，我们只能一次处理一个令牌，因为步骤之间存在顺序依赖性！因此，我们只能通过将多个请求一起批处理，在批次维度上并行化来（容易地）实现良好的利用率。我们稍后会更多地讨论这个问题，但实际上，在不影响延迟的情况下将许多并发请求一起批处理是很困难的。因此，**使用生成来饱和硬件FLOPs要困难得多。**
- en: '**Takeaway:** During generation, the total token batch size must be greater
    than $B_{\text{crit}}$ to be compute-bound on the linear/feed-forward operations
    (240 for bf16 params on TPU v5e). Because generation happens serially, token-by-token,
    this requires us to batch multiple requests together, which is hard!'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结：** 在生成过程中，总令牌批次大小必须大于$B_{\text{crit}}$，以便在TPU v5e上的bf16参数的线性/前馈操作上成为计算限制（240）。因为生成是按顺序逐个令牌发生的，这要求我们将多个请求一起批处理，这很困难！'
- en: '*It’s worth noting just how large this is!* Generate batch size of 240 means
    240 concurrent requests generating at once, and 240 separate KV caches for dense
    models. That means this is difficult to achieve in practice, except in some bulk
    inference settings. In contrast, pushing more than 240 tokens through during a
    prefill is pretty routine, though some care is necessary as sparsity increases.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*值得注意的是，这有多么大！* 生成批次大小为240意味着同时生成240个并发请求，以及为密集模型提供240个单独的KV缓存。这意味着在实践中很难实现，除了在某些批量推理设置中。相比之下，在预填充期间推送超过240个令牌是相当常见的，尽管随着稀疏度的增加需要一些注意。'
- en: '**Note that this exact number will differ on the kind of quantization and hardware.**
    Accelerators often can supply more arithmetic in lower precision. For example,
    if we have int8 parameters but do our computation in bf16, the critical batch
    size drops to 120\. With int8 activations and int8 params, it jumps back up to
    240 since the TPUv5e can supply 400 TOPs/s of int8 x int8.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**请注意，这个确切数字会因量化类型和硬件而异。** 加速器通常可以提供更多低精度的算术运算。例如，如果我们有int8参数但以bf16进行计算，关键批次大小会降至120。使用int8激活和int8参数时，它会跳回到240，因为TPUv5e可以提供400
    TOPs/s的int8 x int8。'
- en: What about attention?
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 那么，关于注意力呢？
- en: 'Things get more complicated when we look at the dot-product attention operation,
    especially since we have to account for KV caches. Let’s look at just one attention
    head with pure multi-headed attention. In a single Flash Attention fusion, we<d-footnote>We''re
    simplifying a fair bit here by ignoring the non-matmul FLOPs in applying the softmax,
    masks etc. They should be overlapped with computation or HBM reads, but it can
    be non-trivial to do on certain TPU generations. Whese details don''t change the
    main message, which is that KV caches are usually memory bound.</d-footnote>:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们查看点积注意力操作时，事情变得更加复杂，特别是因为我们必须考虑 KV 缓存。让我们只看一个具有纯多头注意力的注意力头。在一个单独的 Flash Attention
    融合中，我们<脚注>在这里简化了很多，通过忽略应用 softmax、掩码等时的非矩阵乘法 FLOPs。它们应该与计算或 HBM 读取重叠，但在某些 TPU
    生成中可能很复杂。这些细节不会改变主要信息，即 KV 缓存通常受限于内存。</脚注>：
- en: Read the $Q$ activations of shape $\text{bf16[B, T, D]}$ from HBM.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 HBM 读取形状为 \(\text{bf16[B, T, D]}\) 的 \(Q\) 激活。
- en: Read the $KV$ cache, which is a pair of $\text{bf16[B, S, D]}$ tensors from
    HBM.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 HBM 读取 \(KV\) 缓存，它是一对来自 HBM 的 \(\text{bf16[B, S, D]}\) 张量。
- en: Perform $2BSTD$ FLOPs in the \(QK\) matmul. With Flash Attention, we don’t need
    to write the $\text{bf16[B, S, T]}$ attention matrix back into HBM.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 \(QK\) 矩阵乘法中执行 \(2BSTD\) FLOPs。使用 Flash Attention，我们不需要将 \(\text{bf16[B, S,
    T]}\) 注意力矩阵写回 HBM。
- en: Perform $2BSTD$ in the attention \(AV\) matmul.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在注意力 \(AV\) 矩阵乘法中执行 \(2BSTD\)。
- en: Write the resulting $\text{bf16[B, T, D]}$ tensor back into HBM.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将结果 \(\text{bf16[B, T, D]}\) 张量写回 HBM。
- en: 'Putting it all together, we get:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有内容整合起来，我们得到：
- en: \[\text{Multiheaded Attention Arithmetic Intensity} = \frac{4BSTD}{4BSD + 4BTD}
    = \frac{ST}{S+T}\]
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: \[多头注意力算术强度 = \frac{4BSTD}{4BSD + 4BTD} = \frac{ST}{S+T}\]
- en: For prefill, $S=T$ since we’re doing self-attention, so this simplifies to $T^2
    / 2T = T / 2$. This is great because it means **the arithmetic intensity of attention
    during prefill is $\Theta(T)$**. That means it’s quite easy to be compute-bound
    for attention. As long as our sequence length is fairly large, we’ll be fine!
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于预填充，由于我们正在进行自注意力，所以 \(S=T\)，这简化为 \(T^2 / 2T = T / 2\)。这很好，因为它意味着**预填充期间注意力的算术强度为
    \(\Theta(T)\)**。这意味着注意力很容易成为计算瓶颈。只要我们的序列长度相当大，我们就没有问题！
- en: 'But since generation has a trivial sequence dim, and the $B$ and $D$ dims cancel,
    we can make the approximation:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，由于生成有一个微小的序列维度，且 \(B\) 和 \(D\) 维度相互抵消，我们可以进行如下近似：
- en: \[S \gg T = 1 \implies \frac{ST}{S+T} \approx 1\]
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: \[S \gg T = 1 \implies \frac{ST}{S+T} \approx 1\]
- en: This is bad, since it means we cannot do anything to improve the arithmetic
    intensity of attention during generation. We’re doing a tiny amount of FLOPs while
    loading a massive KV cache. **So we’re basically always memory bandwidth-bound
    during attention!**
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这很糟糕，因为它意味着我们无法提高生成期间注意力的算术强度。我们在加载大量的 KV 缓存时只进行了微量的 FLOPs。**因此，在注意力过程中，我们基本上总是受限于内存带宽！**
- en: '**Takeaway:** during prefill, attention is usually compute bound for any reasonable
    sequence length (roughly $\gt 480$ tokens) while during generation our arithmetic
    intensity is low and constant, so we are always memory bandwidth-bound.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结：**在预填充期间，对于任何合理的序列长度（大致 \(\gt 480\) 个标记），注意力通常受限于计算，而在生成期间我们的算术强度低且恒定，因此我们总是受限于内存带宽。'
- en: '*Why is this, conceptually?* Mainly, we’re compute-bound in linear portions
    of the model because the parameters (the memory bandwidth-heavy components) are
    reused for many batch items. However, every batch item has its own KV cache, so
    a bigger batch size means more KV caches. We will almost *always* be memory bound
    here unless the architecture is adjusted aggressively.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*为什么从概念上来说是这样的？* 主要是因为我们在模型的线性部分受限于计算，因为参数（内存带宽重的组件）被许多批处理项重用。然而，每个批处理项都有自己的
    KV 缓存，因此更大的批处理大小意味着更多的 KV 缓存。除非架构进行了激进调整，否则我们几乎总是受限于内存带宽。'
- en: This also means you will get diminishing returns on throughput from increasing
    batch size once params memory becomes comparable to KV cache memory. The degree
    to which the diminishing returns hurt you depends on the ratio of parameter to
    KV cache bytes for a single sequence, i.e. roughly the ratio $2DF / SHK$. Since
    $HK\approx D$, this roughly depends on the ratio of $F$ to $S$, the sequence length.
    This also depends on architectural modifications that make the KV cache smaller
    (we’ll say more in a moment).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这也意味着，一旦参数内存与 KV 缓存内存相当，增加批量大小的吞吐量将逐渐减少。这种减少的收益程度取决于单个序列的参数到 KV 缓存字节的比率，即大致的比率
    \(2DF / SHK\)。由于 \(HK\approx D\)，这大致取决于 \(F\) 到 \(S\) 的比率，即序列长度。这也取决于使 KV 缓存更小的架构修改（我们稍后会详细介绍）。
- en: Theoretical estimates for LLM latency and throughput
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LLM延迟和吞吐量的理论估计
- en: 'From this math, we can get pretty good bounds on the step time we should aim
    for when optimizing. **(Note: if there is one thing we want to the reader to take
    away from this entire chapter, it’s the following).** For small batch sizes during
    generation (which is common), we can lower-bound our per-step latency by assuming
    we’re memory bandwidth bound in both the attention and MLP blocks:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个数学公式中，我们可以得到在优化时应该追求的步时间的良好界限。（注意：如果我们想让读者从整个章节中带走一点东西，那就是以下内容）。对于生成过程中的小批量大小（这是常见的），我们可以通过假设我们在注意力和MLP块中都是内存带宽受限来降低每步延迟的下限：
- en: \[\begin{equation*} \text{Theoretical Min Step Time} = \frac{\text{Batch Size}
    \times \text{KV Cache Size} + \text{Parameter Size}}{\text{Total Memory Bandwidth}}
    \end{equation*}\]
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation*} \text{理论最小步时间} = \frac{\text{批量大小} \times \text{KV缓存大小}
    + \text{参数大小}}{\text{总内存带宽}} \end{equation*}\]
- en: 'Similarly, for throughput:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，对于吞吐量：
- en: \[\begin{equation*} \text{Theoretical Max Tokens/s} = \frac{\text{Batch Size}
    \times \text{Total Memory Bandwidth}}{\text{Batch Size} \times \text{KV Cache
    Size} + \text{Parameter Size}} \end{equation*}\]
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation*} \text{理论最大标记/秒} = \frac{\text{批量大小} \times \text{总内存带宽}}{\text{批量大小}
    \times \text{KV缓存大小} + \text{参数大小}} \end{equation*}\]
- en: 'Eventually, as our batch size grows, FLOPs begin to dominate parameter loading,
    so in practice we have the more general equation:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，随着批量大小的增加，FLOPs开始主导参数加载，因此在实践中我们有一个更通用的方程：
- en: \[\begin{align} \tiny \text{Theoretical Step Time (General)} = \underbrace{\frac{\text{Batch
    Size} \times \text{KV Cache Size}}{\tiny \text{Total Memory Bandwidth}}}_{\text{Attention
    (always bandwidth-bound)}} + \underbrace{\max\left(\frac{2 \times \text{Batch
    Size} \times \text{Parameter Count}}{\text{Total FLOPs/s}}, \frac{\text{Parameter
    Size}}{\text{Total Memory Bandwidth}}\right)}_{\tiny \text{MLP (can be compute-bound)}}
    \end{align}\]
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align} \tiny \text{理论步时间（通用）} = \underbrace{\frac{\text{批量大小} \times
    \text{KV缓存大小}}{\tiny \text{总内存带宽}}}_{\text{注意（总是带宽受限）}} + \underbrace{\max\left(\frac{2
    \times \text{批量大小} \times \text{参数计数}}{\text{总FLOPs/s}}, \frac{\text{参数大小}}{\text{总内存带宽}}\right)}_{\tiny
    \text{MLP（可能被计算限制）}} \end{align}\]
- en: where the attention component (left) is never compute-bound, and thus doesn’t
    need a FLOPs roofline. These are fairly useful for back-of-the-envelope calculations,
    e.g.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，注意力组件（左侧）永远不会被计算限制，因此不需要FLOPs屋顶线。这些对于估算非常有用，例如。
- en: '**Pop Quiz:** Assume we want to take a generate step with a batch size of 4
    tokens from a 30B parameter dense model on TPU v5e 4x4 slice in int8 with bf16
    FLOPs, 8192 context and 100 kB / token KV caches. What is a reasonable lower bound
    on the latency of this operation? What if we wanted to sample a batch of 256 tokens?'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**快速问答：** 假设我们想要从30B参数稠密模型中生成一个包含4个标记的批量，在TPU v5e 4x4切片上以int8精度和bf16 FLOPs进行，8192个上下文和100
    kB / token的KV缓存。这个操作的延迟的合理下限是多少？如果我们想要采样一个包含256个标记的批量呢？'
- en: <details><summary>Click here for the answer.</summary>
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>点击此处查看答案。</summary>
- en: '**Answer:** in int8, our parameters will use 30e9 bytes and with the given
    specs our KV caches will use `100e3 * 8192 = 819MB` each. We have 16 chips, each
    with `8.1e11` bytes/s of bandwidth and `1.97e14` bf16 FLOPs/s. From the above
    equations, since we have a small batch size, we expect our step time to be at
    least `(4 * 819e6 + 30e9) / (16 * 8.1e11) = 2.5 ms`. At 256 tokens, we’ll be well
    into the compute-bound regime for our MLP blocks, so we have a step time of roughly
    `(256 * 819e6) / (16 * 8.1e11) + (2 * 256 * 30e9) / (16 * 1.97e14) = 21ms`.</details>'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**答案：** 在int8中，我们的参数将使用30e9字节，并且根据给定的规格，我们的KV缓存将使用`100e3 * 8192 = 819MB`。我们有16个芯片，每个芯片有`8.1e11`字节/秒的带宽和`1.97e14`
    bf16 FLOPs/秒。从上述方程中，由于我们的批量大小较小，我们预计我们的步时间至少为`(4 * 819e6 + 30e9) / (16 * 8.1e11)
    = 2.5 ms`。在256个标记时，我们的MLP块将进入计算受限区域，因此我们的步时间大约为`(256 * 819e6) / (16 * 8.1e11)
    + (2 * 256 * 30e9) / (16 * 1.97e14) = 21ms`。</details>'
- en: As you can see, there’s a clear tradeoff between throughput and latency here.
    Small batches are fast but don’t utilize the hardware well. Big batches are slow
    but efficient. Here’s the latency-throughput Pareto frontier calculated for some
    older PaLM models (from the [ESTI paper](https://arxiv.org/pdf/2211.05102)<d-cite
    key="esti">):</d-cite>
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，在这里，吞吐量和延迟之间存在明显的权衡。小批量处理速度快，但无法充分利用硬件。大批量处理速度慢，但效率高。以下是针对一些较旧的PaLM模型计算出的延迟-吞吐量帕累托前沿（来自[ESTI论文](https://arxiv.org/pdf/2211.05102)<d-cite
    key="esti">）：</d-cite>：
- en: <picture>![](../Images/8838f4f4d7d4b6ef78c92d78775fa66d.png)</picture>
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: <picture>![](../Images/8838f4f4d7d4b6ef78c92d78775fa66d.png)</picture>
- en: '**Figure:** Pareto frontier of cost (read: throughput) versus latency for several
    PaLM models. Note how chip count (C) and batch size (B) moves you along the Pareto
    frontier, with the exception of the green dot (C:32 B:16 for PaLM 540B) where
    the available memory prevented the setup from supporting a good batch size and
    caused throughput to suffer. Note how throughput generally tends to flatten around
    after the batch size 240\. int8 weights offers a better latency-throughput pareto
    optimal, but not a better max throughput.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**图示：** 几个 PaLM 模型的成本（即吞吐量）与延迟的帕累托前沿。注意芯片数量（C）和批处理大小（B）如何沿着帕累托前沿移动，除了绿色点（C:32
    B:16 对于 PaLM 540B），在那里可用的内存阻止了设置支持良好的批处理大小，并导致吞吐量下降。注意吞吐量通常在批处理大小 240 之后趋于平坦。int8
    权重提供了更好的延迟-吞吐量帕累托最优，但不是更好的最大吞吐量。'
- en: Not only do we trade off latency and throughput with batch size as knob, we
    may also prefer a larger topology to a smaller one so we can fit larger batches
    if we find ourselves limited by HBM. The [next section](../applied-inference)
    explores this in more detail.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅可以通过批处理大小作为旋钮来权衡延迟和吞吐量，我们还可能更喜欢较大的拓扑结构而不是较小的拓扑结构，这样我们就可以在发现自己受限于 HBM 时适应更大的批处理。下一节
    [下一节](../applied-inference) 将更详细地探讨这一点。
- en: '**Takeaway:** if you care about generation throughput, use the largest per-chip
    batch size possible. Any per-chip batch size above the TPU arithmetic intensity
    ($B_\text{crit}$, usually 120 or 240) will maximize throughput. You may need to
    increase your topology to achieve this. Smaller batch sizes will allow you to
    improve latency at the cost of throughput.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**要点：** 如果您关心生成吞吐量，请使用每个芯片可能的最大批处理大小。任何超过 TPU 算术强度（$B_\text{crit}$，通常为 120
    或 240）的批处理大小都将最大化吞吐量。您可能需要增加您的拓扑结构来实现这一点。较小的批处理大小将允许您在吞吐量牺牲的情况下提高延迟。'
- en: <details><summary>There are some caveats to this from a hardware standpoint.
    Click here for some nits.</summary>
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>从硬件角度来看，对此有一些注意事项。点击此处查看一些细节。</summary>
- en: 'This is all quite theoretical. In practice we often don’t quite see a sharp
    roofline for a few reasons:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切都是相当理论的。在实践中，我们往往由于几个原因而看不到一个尖锐的上限线：
- en: Our assumption that HBM reads will be perfectly overlapped with FLOPs is not
    realistic, since our compiler (XLA) is fallible.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们假设 HBM 读取将完美重叠与 FLOPs 的假设并不现实，因为我们的编译器（XLA）是可能出错的。
- en: For sharded models, XLA also often fails to efficiently overlap the ICI communication
    of our model-sharded matrix multiples with the FLOPs themselves, so we often start
    taking a latency hit on linears over \(\text{BS}=32\).
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于分片模型，XLA 也经常无法有效地重叠模型分片矩阵乘法的 ICI 通信与 FLOPs 本身，因此我们经常在 \(\text{BS}=32\) 的线性上开始遭受延迟损失。
- en: Batch sizes larger than the theoretical roofline will still see some improvement
    in throughput because of imperfect overlapping, but this is a good heuristic.</details>
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批处理大小超过理论上限时，由于不完全重叠，仍会看到吞吐量的某些改进，但这是一个很好的启发式方法。</details>
- en: What about memory?
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于内存呢？
- en: 'We’ve spent some time looking at bandwidth and FLOPs, but not at memory. The
    memory picture looks a lot different at inference time, thanks to our new data
    structure, the KV cache. For this section, let’s pick a real model (LLaMA 2-13B)
    to demonstrate how different things look:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们花了一些时间查看带宽和 FLOPs，但没有查看内存。由于我们新的数据结构 KV 缓存，内存图在推理时看起来大不相同。对于本节，让我们选择一个真实模型（LLaMA
    2-13B）来展示不同的事情看起来如何：
- en: '| hyperparam | value |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | 值 |'
- en: '| --- | --- |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| L (num_layers) | 40 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| L (num_layers) | 40 |'
- en: '| D (d_model) | 5,120 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| D (d_model) | 5,120 |'
- en: '| F (ffw_dimension) | 13,824 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| F (ffw_dimension) | 13,824 |'
- en: '| N (num_heads) | 40 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| N (num_heads) | 40 |'
- en: '| K (num_kv_heads) | 40 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| K (num_kv_heads) | 40 |'
- en: '| H (qkv_dim) | 128 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| H (qkv_dim) | 128 |'
- en: '| V (num_embeddings) | 32,000 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| V (num_embeddings) | 32,000 |'
- en: 'What’s using memory during inference? Well, obviously, our parameters. Counting
    those, we have:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 推理过程中使用了哪些内存？显然，是我们的参数。计算这些参数，我们有：
- en: '| param | formula | size (in bytes) |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 公式 | 大小（以字节为单位） |'
- en: '| --- | --- | --- |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| FFW params | d_model² x ffw_multiplier x 3 (for gelu + out-projection) x
    n_layers | 5,120 x 5,120 x 2.7 x 3 x 40 = **8.5e9** |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| FFW 参数 | d_model² x ffw_multiplier x 3 (for gelu + out-projection) x n_layers
    | 5,120 x 5,120 x 2.7 x 3 x 40 = **8.5e9** |'
- en: '| Vocab params | 2 (input and output embeddings) x n_embeddings x d_model |
    2 x 32,000 x 5,120 = **0.3e9** |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 词汇参数 | 2 (输入和输出嵌入) x n_embeddings x d_model | 2 x 32,000 x 5,120 = **0.3e9**
    |'
- en: '| Attention params | [2 (*q and output*) x d_model x n_heads x d_qkv + 2 (*for
    k and v*) x d_model x n_kv_heads x d_qkv] x n_layers | (2 x 5,120 x 40 x 128 +
    2 x 5,120 x 40 x 128) x 40 = **4.2e9** |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 注意力参数 | [2 (*q 和输出*) x d_model x n_heads x d_qkv + 2 (*for k 和 v*) x d_model
    x n_kv_heads x d_qkv] x n_layers | (2 x 5,120 x 40 x 128 + 2 x 5,120 x 40 x 128)
    x 40 = **4.2e9** |'
- en: Adding these parameters up, we get 8.5e9 + 4.2e9 + 0.3e9 = **13e9 total parameters**,
    just as expected. As we saw in the previous sections, during training we might
    store our parameters in bfloat16 with an optimizer state in float32\. That may
    use around 100GB of memory. That pales in comparison to our gradient checkpoints,
    which can use several TBs.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些参数加起来，我们得到 8.5e9 + 4.2e9 + 0.3e9 = **13e9 总参数**，正如预期的那样。正如我们在前面的章节中看到的，在训练过程中，我们可能会用
    bfloat16 存储我们的参数，并用 float32 的优化器状态。这可能会使用大约 100GB 的内存。这和我们的梯度检查点相比微不足道，梯度检查点可以使用几个
    TB。
- en: '**How is inference different?** During inference, we store one copy of our
    parameters, let’s say in bfloat16\. That uses 26GB — and in practice we can often
    do much better than this with quantization. There’s no optimizer state or gradients
    to keep track of. Because we don’t checkpoint (keep activations around for the
    backwards pass), our activation footprint is negligible for both prefill<d-footnote>Particularly
    thanks to Flash Attention, which avoids materializing our attention matrix</d-footnote>
    and generate. If we prefill 8k tokens, a single activation only uses around `8,192
    x 5,120 x 2 bytes = 80MB` of memory. Longer prefills can be broken down into many
    smaller forward passes, so it’s not a problem for longer contexts either. Generation
    use even fewer tokens than that, so activations are negligible.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**推理有何不同？** 在推理过程中，我们存储参数的一个副本，比如说在 bfloat16 中。这使用了 26GB — 而在实际操作中，我们通常可以通过量化做得更好。没有优化器状态或梯度需要跟踪。因为我们没有检查点（保留激活以供反向传播），所以我们的激活足迹对于预先填充和生成都是微不足道的。如果我们预先填充
    8k 个标记，单个激活仅使用大约 `8,192 x 5,120 x 2 字节 = 80MB` 的内存。更长的预先填充可以被分解成许多更小的正向传递，因此对于较长的上下文也不是问题。生成使用的标记甚至更少，因此激活可以忽略不计。'
- en: '**The main difference is the KV cache**. These are the keys and value projections
    for all past tokens, bounded in size only by the maximum allowed sequence length.
    The total size for \(T\) tokens is'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**主要区别在于 KV 缓存**。这些是所有过去标记的键和值投影，其大小仅受最大允许序列长度的限制。对于 \(T\) 个标记的总大小是'
- en: \[\text{KV cache size} = 2 \cdot \text{bytes per float} \cdot H \cdot K \cdot
    L \cdot T\]
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: \[\text{KV 缓存大小} = 2 \cdot \text{每浮点字节数} \cdot H \cdot K \cdot L \cdot T\]
- en: where \(H\) is the dimension of each head, \(K\) is the number of KV heads,
    \(L\) is the number of layers, and the 2 comes from storing both the keys and
    values.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(H\) 是每个头的维度，\(K\) 是 KV 头的数量，\(L\) 是层数，2 是存储键和值的倍数。
- en: '**This can get big very quickly**, even with modest batch size and context
    lengths. For LLaMA-13B, a KV cache for a single 8192 sequence at bf16 is'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**这会很快变得很大**，即使对于适度的批量和上下文长度也是如此。对于 LLaMA-13B，一个用于单个 8192 序列的 bf16 KV 缓存'
- en: \[8192\ (T) \times 40\ (K) \times 128\ (H) \times 40\ (L) \times 2\ (\text{bytes})
    \times 2 = 6.7 \text{GB}\]
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: \[8192\ (T) \times 40\ (K) \times 128\ (H) \times 40\ (L) \times 2\ (\text{字节})
    \times 2 = 6.7 \text{GB}\]
- en: '**Just 4 of these exceed the memory usage of our parameters!** To be clear,
    LLaMA 2 was not optimized for KV cache size at longer contexts (it isn’t always
    this bad, since usually $K$ is much smaller, as in LLaMA-3), but this is still
    illustrative. We cannot neglect these in memory or latency estimates.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**仅有4个超过了我们参数的内存使用量！** 为了明确起见，LLaMA 2 在较长的上下文中并未针对 KV 缓存大小进行优化（这并不总是这么糟糕，因为通常
    $K$ 要小得多，如在 LLaMA-3 中），但这仍然具有说明性。我们在内存或延迟估计中不能忽视这些因素。'
- en: Modeling throughput and latency for LLaMA 2-13B
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为 LLaMA 2-13B 模型吞吐量和延迟
- en: Let’s see what happens if we try to perform generation perfectly efficiently
    at different batch sizes on 8xTPU v5es, up to the critical batch size (240) derived
    earlier for maximum theoretical throughput.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看在 8xTPU v5es 上尝试以不同的批大小进行完美高效的生成会发生什么，直到之前为最大理论吞吐量推导出的临界批大小（240）。
- en: '| Batch Size | 1 | 8 | 16 | 32 | 64 | 240 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 批大小 | 1 | 8 | 16 | 32 | 64 | 240 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| KV Cache Memory (GiB) | 6.7 | 53.6 | 107.2 | 214.4 | 428.8 | 1608 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| KV 缓存内存 (GiB) | 6.7 | 53.6 | 107.2 | 214.4 | 428.8 | 1608 |'
- en: '| Total Memory (GiB) | 32.7 | 79.6 | 133.2 | 240.4 | 454.8 | 1634 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 总内存 (GiB) | 32.7 | 79.6 | 133.2 | 240.4 | 454.8 | 1634 |'
- en: '| Theoretical Step Time (ms) | 4.98 | 12.13 | 20.30 | 36.65 | 69.33 | 249.09
    |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 理论步骤时间 (ms) | 4.98 | 12.13 | 20.30 | 36.65 | 69.33 | 249.09 |'
- en: '| Theoretical Throughput (tokens/s) | 200.61 | 659.30 | 787.99 | 873.21 | 923.13
    | 963.53 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 理论吞吐量 (tokens/s) | 200.61 | 659.30 | 787.99 | 873.21 | 923.13 | 963.53 |'
- en: 8x TPU v5es gives us 128GiB of HBM, 6.5TiB/s of HBM bandwidth (0.82TiB/s each)
    and 1600TF/s of compute.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 8x TPU v5es 提供了 128GiB 的 HBM，6.5TiB/s 的 HBM 带宽（每个 0.82TiB/s）和 1600TF/s 的计算能力。
- en: For this model, increasing the batch size does give us better throughput, but
    we suffer rapidly diminishing returns. We OOM beyond batch size 16, and need an
    order of magnitude more memory to go near 240\. A bigger topology can improve
    the latency, but we’ve hit a wall on the per chip throughput.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个模型，增加批处理大小确实可以给我们带来更好的吞吐量，但我们会迅速遇到递减的回报。我们超过批处理大小 16 时会 OOM（内存不足），并且需要十倍以上的内存才能接近
    240。更大的拓扑结构可以提高延迟，但我们已经遇到了每块芯片吞吐量的瓶颈。
- en: Let’s say we keep the total number of params the same, but magically make the
    KV cache 5x smaller (say, with 1:5 [GMQA](#tricks-for-improving-generation-throughput-and-latency),
    which means we have 8 KV heads shared over the 40 Q heads — see next section for
    more details).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们保持总参数数量不变，但神奇地使 KV 缓存缩小 5 倍（例如，使用 1:5 [GMQA](#tricks-for-improving-generation-throughput-and-latency)，这意味着我们有
    8 个 KV 头在 40 个 Q 头之间共享——更多细节请见下一节）。
- en: '| Batch Size | 1 | 8 | 16 | 32 | 64 | 240 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 批处理大小 | 1 | 8 | 16 | 32 | 64 | 240 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| KV Cache Memory (GiB) | 1.34 | 10.72 | 21.44 | 42.88 | 85.76 | 321.6 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| KV 缓存内存（GiB） | 1.34 | 10.72 | 21.44 | 42.88 | 85.76 | 321.6 |'
- en: '| Total Memory (GiB) | 27.34 | 36.72 | 47.44 | 68.88 | 111.76 | 347.6 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 总内存（GiB） | 27.34 | 36.72 | 47.44 | 68.88 | 111.76 | 347.6 |'
- en: '| Theoretical Step Time (ms) | 4.17 | 5.60 | 7.23 | 10.50 | 17.04 | 52.99 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 理论步骤时间（ms） | 4.17 | 5.60 | 7.23 | 10.50 | 17.04 | 52.99 |'
- en: '| Theoretical Throughput (tokens/s) | 239.94 | 1,429.19 | 2,212.48 | 3,047.62
    | 3,756.62 | 4,529.34 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 理论吞吐量（token/s） | 239.94 | 1,429.19 | 2,212.48 | 3,047.62 | 3,756.62 | 4,529.34
    |'
- en: With a smaller KV cache, we still have diminishing returns, but the theoretical
    throughput per chip continues to scale up to batch size 240\. We can fit a much
    bigger batch of 64, and latency is also consistently better at all batch sizes.
    The latency, maximum throughput, and maximum batch size all improve dramatically!
    In fact, later LLaMA generations used this exact optimization — LLaMA-3 8B has
    32 query heads and 8 KV heads ([source](https://huggingface.co/MaziyarPanahi/Llama-3-13B-Instruct-v0.1/blob/dfdeb40bdb2c149dfa399ea2be0d56eb120f0831/config.json)).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 使用较小的 KV 缓存，我们仍然有递减的回报，但每块芯片的理论吞吐量仍然可以扩展到批处理大小 240。我们可以容纳更大的 64 批处理，并且所有批处理大小的延迟也始终更好。延迟、最大吞吐量和最大批处理大小都显著提高！实际上，后来的
    LLaMA 代使用了这种精确的优化——LLaMA-3 8B 有 32 个查询头和 8 个 KV 头（[来源](https://huggingface.co/MaziyarPanahi/Llama-3-13B-Instruct-v0.1/blob/dfdeb40bdb2c149dfa399ea2be0d56eb120f0831/config.json)）。
- en: '**Takeaway:** In addition to params, the size of KV cache has a lot of bearing
    over the ultimate inference performance of the model. We want to keep it under
    control with a combination of architectural decisions and runtime optimizations.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结：** 除了参数之外，KV 缓存的大小对模型的最终推理性能有很大影响。我们希望通过结合架构决策和运行时优化来控制它。'
- en: Tricks for Improving Generation Throughput and Latency
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提高生成吞吐量和延迟的技巧
- en: 'Since the original [Attention is All You Need paper](https://arxiv.org/abs/1706.03762),
    many techniques have been developed to make the model more efficient, often targeting
    the KV cache specifically. Generally speaking, a smaller KV cache makes it easier
    to increase batch size and context length of the generation step without hurting
    latency, and makes life easier for the systems surrounding the Transformer (like
    request caching). Ignoring effects on quality, we may see:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 自从原始的 [Attention is All You Need 论文](https://arxiv.org/abs/1706.03762) 以来，已经开发了许多技术来提高模型的效率，通常针对
    KV 缓存进行优化。一般来说，较小的 KV 缓存使得在不影响延迟的情况下更容易增加生成步骤的批处理大小和上下文长度，并且使围绕 Transformer 的系统（如请求缓存）的生活更加容易。忽略对质量的影响，我们可能会看到：
- en: '**Grouped multi-query attention (aka GMQA, GQA):** We can reduce the number
    of KV heads, and share them with many Q heads in the attention mechanism. In the
    extreme case, it is possible to share a single KV head across all Q heads. This
    reduces the KV cache by a factor of the Q:KV ratio over pure MHA, and it has been
    observed that the performance of models is relatively insensitive to this change.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**分组多查询注意力（简称 GMQA，GQA）：** 我们可以减少 KV 头的数量，并与注意力机制中的许多 Q 头共享。在极端情况下，甚至可能将单个
    KV 头跨所有 Q 头共享。这减少了 KV 缓存，相对于纯 MHA，减少了 Q:KV 比例的倍数，并且观察到模型的性能对这种变化相对不敏感。'
- en: <picture>![](../Images/45c47d328aa951d9cbe28b4ae0074615.png)</picture>
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: <picture>![](../Images/45c47d328aa951d9cbe28b4ae0074615.png)</picture>
- en: This also effectively increases the arithmetic intensity of the attention computation
    (see Question 4 in [Section 4](../transformers)).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这也有效地增加了注意力计算的算术强度（参见第 4 节中的问题 4）。
- en: '**Mixing in some local attention layers:** Local attention caps the context
    to a small to moderately sized max length. At training time and prefill time,
    this involves masking the attention matrix to a diagonal strip instead of a triangle.
    This effectively caps the size of the max length of the KV cache for the local
    layers. By mixing in some local layers into the model with some global layers,
    the KV cache is greatly reduced in size at contexts longer than the local window.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**混合一些局部注意力层：** 局部注意力将上下文限制在较小到适中的最大长度。在训练时间和预填充时间，这涉及到将注意力矩阵屏蔽为对角带而不是三角形。这实际上限制了局部层
    KV 缓存的最大长度。通过在模型中混合一些局部层和一些全局层，当上下文长度超过局部窗口时，KV 缓存的大小可以大大减少。'
- en: '**Sharing KVs across layers:** The model can learn to share the same KV caches
    across layers in some pattern. Whilst this does reduce the KV cache size, and
    provide benefits in increasing batch size, caching, offline storage etc. shared
    KV caches may need to be read from HBM multiple times, *so it does not necessarily
    improve the step time.*'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '**跨层共享 KVs：** 模型可以学习以某种模式在层之间共享相同的 KV 缓存。虽然这确实减少了 KV 缓存的大小，并在增加批次大小、缓存、离线存储等方面提供了好处，但共享的
    KV 缓存可能需要多次从 HBM 中读取，*因此并不一定能提高步骤时间。*'
- en: <picture>![](../Images/ce44c6e2af49992e8c38f7acf2faea16.png)</picture>
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: <picture>![](../Images/ce44c6e2af49992e8c38f7acf2faea16.png)</picture>
- en: '**Left:** Multiple layers of pure global attention. **Right:** An example of
    some global/local interleaving pattern with sharing with adjacent layers. Source:
    [Character.ai blog](https://research.character.ai/optimizing-inference/?ref=blog.character.ai).'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**左侧：** 多层纯全局注意力。**右侧：** 与相邻层共享的一些全局/局部交错模式示例。来源：[Character.ai 博客](https://research.character.ai/optimizing-inference/?ref=blog.character.ai)。'
- en: '**Quantization:** Inference is usually less sensitive to the precision of parameters
    and KVs. By quantizing the parameters and KV cache (e.g. to int8, int4, `fp8`
    etc.), we can save on memory bandwidth on both, decrease the batch size required
    to reach the compute roofline and save memory to run at bigger batch sizes. Quantization
    has the added advantage that even if the model was not trained with quantization
    it can often be applied post training.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '**量化：** 推理通常对参数和 KVs 的精度不太敏感。通过量化参数和 KV 缓存（例如，到 int8、int4、`fp8` 等），我们可以在两者上节省内存带宽，减少达到计算屋顶线的所需批次大小，并在更大的批次大小下节省内存。量化还有一个额外的优点，即即使模型没有经过量化训练，通常也可以在训练后应用。'
- en: '**Using ragged HBM reads and Paged Attention:** We allocated 8k of context
    for each KV cache in the calculations above but it is often not necessary to read
    the entire KV cache from memory — requests have a wide range of length distributions
    and don’t use the max context of the model, so we can often implement kernels
    (e.g. Flash Attention variants) that only read the non-padding part of the KV
    cache.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用杂乱无章的 HBM 读取和分页注意力：** 在上面的计算中，我们为每个 KV 缓存分配了 8k 的上下文，但通常没有必要从内存中读取整个 KV
    缓存——请求具有广泛的长度的分布，并且不使用模型的最大上下文，因此我们可以经常实现仅读取 KV 缓存非填充部分的内核（例如，Flash Attention
    变体）。'
- en: Paged Attention <d-cite key="paged">is a refinement upon this that stores KV
    caches in OS-style page tables and mostly avoids padding the KV caches altogether.
    This adds a lot of complexity but means every batch only uses as much memory as
    it needs. This is a runtime optimization, so again it is indifferent to architecture.</d-cite>
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 分页注意力 <d-cite key="paged">是对此的改进，它将 KV 缓存存储在 OS 风格的页面表中，并且几乎完全避免了填充 KV 缓存。这增加了许多复杂性，但意味着每个批次只使用所需的内存量。这是一个运行时优化，因此它对架构没有影响。</d-cite>
- en: <picture>![](../Images/fa96989bb0715a151f7f26096915db5e.png)</picture>
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: <picture>![](../Images/fa96989bb0715a151f7f26096915db5e.png)</picture>
- en: '**Figure:** during generation, a single token (forth) attends to multiple KV
    cache blocks/pages. By paging the KV cache, we avoid loading or storing more memory
    than we need to. Taken from the [PagedAttention paper](https://arxiv.org/pdf/2309.06180).'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '**图示：** 在生成过程中，单个标记（forth）关注多个 KV 缓存块/页面。通过分页 KV 缓存，我们避免了加载或存储比所需的更多内存。摘自 [PagedAttention
    论文](https://arxiv.org/pdf/2309.06180)。'
- en: '**Big Picture:** All told, these KV cache optimizations can reduce KV cache
    sizes by over an order of magnitude compared to a standard MHA Transformer. This
    can lead to an order-of-magnitude improvement in the overall cost of the Transformer.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**总体情况：** 总的来说，这些 KV 缓存优化可以将 KV 缓存大小减少一个数量级以上，与标准 MHA Transformer 相比。这可能导致
    Transformer 的总体成本提高一个数量级。'
- en: Distributing Inference Over Multiple Accelerators
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在多个加速器上分配推理
- en: So far we’ve handwaved how we’re scaling beyond a single chip. Following [Section
    5](../training), let’s explore the different strategies available to us and their
    tradeoffs. As always, we will look at prefill and generation separately.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '到目前为止，我们只是泛泛而谈了如何扩展到单个芯片之外。遵循[第5节](../training)，让我们探讨我们可用的不同策略及其权衡。一如既往，我们将分别查看预填充和生成。 '
- en: Prefill
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预填充
- en: From a roofline standpoint, **prefill is almost identical to training** and
    almost all the same techniques and tradeoffs apply — model (Megatron) parallelism,
    sequence sharding (for sufficiently long context), pipelining, even FSDP are all
    viable! You just have to keep the KVs kicking around so you can do generation
    later. As in training, increasing the number of chips gives us access to more
    FLOPs/s (for potentially lower TTFT), but adds communication overhead (potentially
    reducing throughput per chip).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 从屋顶线角度来看，**预填充几乎与训练相同**，并且几乎所有的技术和权衡都适用——模型（Megatron）并行、序列分片（对于足够长的上下文）、流水线，甚至FSDP都是可行的！你只需要保持KVs活跃，这样你就可以稍后进行生成。就像训练一样，增加芯片的数量为我们提供了更多的FLOPs/s（对于可能更低的TTFT），但增加了通信开销（可能会降低每芯片的吞吐量）。
- en: '**The general rule for sharding prefill:** here’s a general set of rules for
    prefill. We’ll assume we’re doing prefill on a single sequence only (no batch
    dimension):'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**分片预填充的一般规则:** 这里有一套关于预填充的一般规则。我们将假设我们只在一个序列上进行预填充（没有批量维度）：'
- en: '*Model sharding:* We typically do some amount of model parallelism first, up
    to the point we become ICI-bound. As we saw in [Section 5](../training), this
    is around $F / 2200$ for 1 axis (usually around 4-8 way sharding).'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*模型分片:* 我们通常首先进行一定程度的模型并行，直到我们达到ICI界限。正如我们在[第5节](../training)中看到的，对于1个轴来说，这大约是$F
    / 2200$（通常在4-8路分片之间）。'
- en: '*Sequence parallelism:* Beyond this, we do sequence parallelism (like data
    parallelism but sharding across the sequence dimension). While sequence parallelism
    introduces some extra communication in attention, it is typically fairly small
    at longer contexts. As with training, we can overlap the communication and computation
    (using collective matmuls for Megatron and ring attention respectively).'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*序列并行性:* 除了这个之外，我们进行序列并行（类似于数据并行，但是在序列维度上进行分片）。虽然序列并行会在注意力中引入一些额外的通信，但在较长的上下文中通常相当小。与训练一样，我们可以重叠通信和计算（使用Megatron的集体矩阵乘法和环状注意力分别）。'
- en: '**Takeaway:** during prefill, almost any sharding that can work during training
    can work fine. Do model parallelism up to the ICI bound, then do sequence parallelism.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '**要点:** 在预填充期间，几乎任何在训练期间可以工作的分片都可以很好地工作。做到ICI界限内的模型并行，然后进行序列并行。'
- en: Generation
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成
- en: 'Generation is a more complicated beast than prefill. For one thing, it is harder
    to get a large batch size because we need to batch many requests together. Latency
    targets are lower. Together, these mean we are typically more memory-bound and
    more sensitive to communication overhead, which restrict our sharding strategies:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 生成比预填充更复杂。首先，由于我们需要将许多请求一起批处理，因此更难获得大批量。延迟目标更低。这些共同意味着我们通常更受内存限制，并且对通信开销更敏感，这限制了我们的分片策略：
- en: '**FSDP is impossible:** since we are memory-bound in loading our parameters
    and KV caches from HBM to the MXU, we do not want to move them via ICI which is
    orders of magnitudes slower than HBM. *We want to move activations rather than
    weights.* This means methods similar to FSDP are usually completely unviable for
    generation.<d-footnote>Accidentally leaving it on after training is an easy and
    common way to have order of magnitude regressions</d-footnote>'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**FSDP是不可能的:** 由于我们在从HBM到MXU加载我们的参数和KV缓存时受内存限制，我们不希望通过比HBM慢几个数量级的ICI来移动它们。*我们希望移动激活而不是权重。*
    这意味着类似于FSDP的方法通常对于生成来说完全不切实际。《d-footnote>在训练后意外地留下它是一个简单且常见的方法，会导致数量级的退步</d-footnote>'
- en: '**There is no reason to do data parallelism:** pure data parallelism is unhelpful
    because it replicates our parameters and doesn’t help us load parameters faster.
    You’re better off spinning up multiple copies of the model instead.<d-footnote>By
    this we mean, spin up multiple servers with copies of the model at a smaller batch
    size. Data parallelism at the model level is strictly worse.</d-footnote>'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**没有必要进行数据并行:** 纯数据并行是无用的，因为它复制了我们的参数，并不能帮助我们更快地加载参数。你最好启动多个模型的副本。《d-footnote>通过这种方式，我们是指，以较小的批量大小启动多个包含模型副本的服务器。在模型级别上的数据并行是严格更差的。</d-footnote>'
- en: '**No sequence = no sequence sharding.** Good luck sequence sharding.'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**没有序列就没有序列分片。** 祝你好运进行序列分片。'
- en: '*This mostly leaves us with variants of model sharding for dense model generation*.
    As with prefill, the simplest thing we we can do is simple model parallelism (with
    activations fully replicated, weights fully sharded over hidden dimension for
    the MLP) up to 4-8 ways when we become ICI bound. However, since we are often
    memory bandwidth bound, we can actually go beyond this limit to improve latency!'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '*这主要让我们只剩下密集模型生成的模型分片变体。* 与预填充一样，我们能做的最简单的事情是简单的模型并行（对于 MLP，激活完全复制，权重在隐藏维度上完全分片），当达到
    ICI 约束时，可以达到 4-8 种方式。然而，由于我们通常受内存带宽限制，我们可以超过这个限制来提高延迟！'
- en: '**Note on ICI bounds for generation:** during training we want to be compute-bound,
    so our rooflines look at when our ICI comms take longer than our FLOPs. However,
    during generation, if we’re memory bandwidth bound by parameter loading, we can
    increase model sharding beyond this point and improve latency at a minimal throughput
    cost (in terms of tokens/sec/chip). More model sharding gives us more HBM to load
    our weights over, and our FLOPs don’t matter.<d-footnote>In the sense that FLOPs
    time isn''t bottlenecking us, so the thing we need to worry about is ICI time
    exceeding parameter loading time.</d-footnote> Let’s look at how much model parallelism
    we can do before it becomes the bottleneck.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '**关于生成时 ICI 约束的说明：** 在训练过程中，我们希望成为计算受限，因此我们的屋顶线查看当我们的 ICI 通信比 FLOPs 更长时。然而，在生成过程中，如果我们受参数加载的内存带宽限制，我们可以增加模型分片超过这个点，并以最小的吞吐量成本（以每秒每芯片的令牌数为单位）提高延迟。更多的模型分片给我们更多的
    HBM 来加载我们的权重，而我们的 FLOPs 并不重要。<d-footnote>在 FLOPs 时间不是瓶颈的情况下，我们需要担心的是 ICI 时间超过参数加载时间。</d-footnote>
    让我们看看在它成为瓶颈之前我们可以进行多少模型并行。 '
- en: \[\begin{align*}T_\text{HBM comms} = \frac{2DF}{Y \cdot W_\text{hbm}} && T_\text{ICI
    comms} = \frac{2BD}{W_\text{ici}}\end{align*}\] \[T_\text{ICI comms} > T_\text{HBM
    comms} \rightarrow \frac{W_\text{hbm}}{W_\text{ici}} > \frac{F}{Y \cdot B} \rightarrow
    Y > F / (B \cdot \beta)\]
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*}T_\text{HBM comms} = \frac{2DF}{Y \cdot W_\text{hbm}} && T_\text{ICI
    comms} = \frac{2BD}{W_\text{ici}}\end{align*}\] \[T_\text{ICI comms} > T_\text{HBM
    comms} \rightarrow \frac{W_\text{hbm}}{W_\text{ici}} > \frac{F}{Y \cdot B} \rightarrow
    Y > F / (B \cdot \beta)\]
- en: 'where $\beta = W_\text{hbm} / W_\text{ici}$. This number is usually around
    8 for TPU v5e and TPU v6e. That means e.g. if $F$ is 16,384 and $B$ is 32, we
    can in theory do model parallelism up to `16384 / (32 * 8) = 64` ways without
    a meaningful hit in throughput. This assume we can fully shard our KV caches 64-ways
    which is difficult: we discuss this below.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\beta = W_\text{hbm} / W_\text{ici}$。这个数字对于 TPU v5e 和 TPU v6e 通常在 8 左右。这意味着例如，如果
    $F$ 是 16,384，$B$ 是 32，理论上我们可以做到 `16384 / (32 * 8) = 64` 种方式的模型并行，而不会对吞吐量产生有意义的影响。这假设我们可以完全以
    64 方式分片我们的 KV 缓存，这是困难的：我们下面会讨论这个问题。
- en: For the attention layer, we also model shard attention \(W_Q\) and \(W_O\) over
    heads Megatron style. The KV weights are quite small, and replicating them is
    often cheaper than sharding beyond $K$-way sharding.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 对于注意力层，我们也以 Megatron 风格对注意力 \(W_Q\) 和 \(W_O\) 进行模型分片。KV 权重相当小，复制它们通常比超过 K 方分片更便宜。
- en: '**Takeaway:** our only options during generation are variants of model parallelism.
    We aim to move activations instead of KV caches or parameters, which are larger.
    When our batch size is large, we do model parallelism up to the FLOPs-ICI bound
    ($F / \alpha$). When our batch size is smaller, we can improve latency by model
    sharding more (at a modest throughput cost). When we want to model shard more
    ways than we have KV heads, we can shard our KVs along the batch dimension as
    well.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结：** 在生成过程中，我们只有模型并行的变体可以选择。我们的目标是移动激活，而不是 KV 缓存或参数，因为它们更大。当我们的批量大小很大时，我们做到
    FLOPs-ICI 约束的模型并行（$F / \alpha$）。当我们的批量大小较小时，我们可以通过模型分片来提高延迟（以适度的吞吐量成本为代价）。当我们想要比我们有的
    KV 头更多的模型分片方式时，我们也可以沿着批量维度对 KVs 进行分片。'
- en: Sharding the KV cache
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分片 KV 缓存
- en: '**We also have an additional data structure that needs to be sharded — the
    KV cache.** Again, we almost always prefer to avoid replicating the cache, since
    it is the primary source of attention latency. To do this, we first Megatron-shard
    the KVs along the head dimension. This is limited to $K$-way sharding, so for
    models with a small number of heads, we shard the head dimension as much as possible
    and then shard along the batch dimension, i.e. $\text{KV}[2, B_Z, S, K_Y, H]$.
    This means the KV cache is completely distributed.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们还有一个需要分片的数据结构——KV 缓存。** 同样，我们几乎总是倾向于避免复制缓存，因为它通常是关注延迟的主要来源。为了做到这一点，我们首先沿着头维度对
    KVs 进行 Megatron 分片。这限制于 K 方分片，因此对于头数较少的模型，我们尽可能地对头维度进行分片，然后沿着批量维度进行分片，即 $\text{KV}[2,
    B_Z, S, K_Y, H]$。这意味着 KV 缓存是完全分布的。'
- en: <picture>![](../Images/0c51aaca7f765fae304e0e61cee06ffb.png)</picture>
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: <picture>![](../Images/0c51aaca7f765fae304e0e61cee06ffb.png)</picture>
- en: '**Figure:** comparison of the attention mechanism with (a) Multi head attention
    with pure model sharding and (b) Multiquery attention with batch sharding of the
    KV cache. Notice how we need two extra AllToAlls to shift the activations from
    model sharding to batch sharding, so they can act on the KV caches.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '**图：** 与（a）纯模型分片的多头注意力和（b）KV缓存批分片的多查询注意力的比较。注意我们为什么需要两个额外的AllToAll来将激活从模型分片转移到批分片，以便它们可以作用于KV缓存。'
- en: The cost of this is two AllToAlls every attention layer — one to shift the Q
    activations to the batch sharding so we can compute attention with batch sharding,
    and one to shift the batch sharded attention output back to pure model sharded.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这个成本是每个注意力层两个AllToAll——一个将Q激活移至批分片，以便我们可以使用批分片计算注意力，另一个将批分片注意力输出移回纯模型分片。
- en: <details><summary>Here’s the full algorithm!</summary>
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>这里是完整的算法！</summary>
- en: Here we’ll write out the full attention algorithm with model parallelism over
    both $Y$ and $Z$. I apologize for using $K$ for both the key tensor and the KV
    head dimension. Let $M=N/K$.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将写出在$Y$和$Z$上具有模型并行性的完整注意力算法。我为同时使用$K$作为键张量和KV头维度而道歉。设$M=N/K$。
- en: X[B, D] = … (existing activations, unsharded from previous layer)
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: X[B, D] = … (现有激活，来自前一层的未分片)
- en: K[B[Z], S, K[Y], H], V[B[Z], S, K, H] = … (existing KV cache, batch sharded)
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: K[B[Z], S, K[Y], H], V[B[Z], S, K, H] = … (现有的KV缓存，批分片)
- en: Q[B, N[YZ], H] = X[B, D] * W[Q][D, N[YZ], H]
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Q[B, N[YZ], H] = X[B, D] * W[Q][D, N[YZ], H]
- en: Q[B[Z], N[Y], H] = **AllToAll**[Z->B](Q[B, N[YZ], H])
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Q[B[Z], N[Y], H] = **AllToAll**[Z->B](Q[B, N[YZ], H])
- en: Q[B[Z], K[Y], M, H] = **Reshape**(Q[B[Z], N[Y], H])
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Q[B[Z], K[Y], M, H] = **Reshape**(Q[B[Z], N[Y], H])
- en: O[B[Z], S, K[Y], M] = Q[B[Z], K[Y], M, H] *[H] K[B[Z], S, K[Y], H]
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: O[B[Z], S, K[Y], M] = Q[B[Z], K[Y], M, H] *[H] K[B[Z], S, K[Y], H]
- en: O[B[Z], S, K, M] = **Softmax**[S](O[B[Z], S, K[Y]])
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: O[B[Z], S, K, M] = **Softmax**[S](O[B[Z], S, K[Y]])
- en: O[B[Z], K[Y], M, H] = O[B[Z], S, K, M] *[S] V[B[Z], S, K[Y], H]
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: O[B[Z], K[Y], M, H] = O[B[Z], S, K, M] *[S] V[B[Z], S, K[Y], H]
- en: O[B, K[Y], M[Z], H] = **AllToAll**[Z->M](O[B[Z], K[Y], M, H])
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: O[B, K[Y], M[Z], H] = **AllToAll**[Z->M](O[B[Z], K[Y], M, H])
- en: O[B, N[YZ], H] = **Reshape**(O[B, K[Y], M[Z], H])
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: O[B, N[YZ], H] = **Reshape**(O[B, K[Y], M[Z], H])
- en: X[B, D] {U[YZ]} = W[O][N[YZ], H, D] *[N,H] O[B, N[YZ], H]
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: X[B, D] {U[YZ]} = W[O][N[YZ], H, D] *[N,H] O[B, N[YZ], H]
- en: X[B, D] = **AllReduce**(X[B, D] { U[YZ]})
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: X[B, D] = **AllReduce**(X[B, D] { U[YZ]})
- en: This is pretty complicated but you can see generally how it works. The new comms
    are modestly expensive since they operate on our small activations, while in return
    we save a huge amount of memory bandwidth loading the KVs (which are stationary).</details>
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这相当复杂，但你可以大致了解它是如何工作的。新的通信成本相对较高，因为它们操作的是我们的小激活，而作为回报，我们节省了大量内存带宽来加载KVs（它们是静态的）。</details>
- en: '**Sequence sharding:** If the batch size is too small, or the context is long,
    we can sequence shard the KV cache. Again, we pay a collective cost in accumulating
    the attention across shards here. First we need to AllGather the Q activations,
    and then accumulate the KVs in a similar fashion to Flash Attention.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**序列分片：** 如果批大小太小，或者上下文很长，我们可以对KV缓存进行序列分片。同样，我们在跨分片累积注意力时支付了集体成本。首先，我们需要AllGather
    Q激活，然后以类似于Flash Attention的方式累积KVs。'
- en: Designing an Effective Inference Engine
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设计一个有效的推理引擎
- en: So far we’ve looked at how to optimize and shard the individual prefill and
    generate operations efficiently in isolation. To actually use them effectively,
    we need to design an inference engine which can feed these two operations at a
    point of our choosing on the latency/throughput Pareto frontier.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经探讨了如何有效地优化和分片单个预填充和生成操作。为了实际有效地使用它们，我们需要设计一个推理引擎，该引擎可以在我们选择的延迟/吞吐量Pareto前沿点上提供这两个操作。
- en: 'The simplest method is simply to run a batch of prefill, then a batch of generations:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的方法是简单地运行一批预填充，然后运行一批生成：
- en: <picture>![](../Images/1681163b20fda8015319344c7e1add77.png)</picture>
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: <picture>![](../Images/1681163b20fda8015319344c7e1add77.png)</picture>
- en: '**Figure:** in the simplest setup, requests are aggregated, and the server
    alternates between running a batch of prefills and calling the generate function
    until completion for all sequences.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '**图：** 在最简单的设置中，请求被聚合，服务器在运行一批预填充和调用生成函数之间交替，直到所有序列完成。'
- en: 'This is easy to implement and is the first inference setup in most codebases,
    but it has multiple drawbacks:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这很容易实现，并且是大多数代码库中的第一个推理设置，但它有多个缺点：
- en: '**Latency is terrible.** We couple the prefill and generate batch size. Time
    to first token (TTFT) is terrible at big prefill batch sizes — you need to finish
    all prefills before any users can see any tokens. Generate throughput is terrible
    at small batch sizes.'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**延迟非常糟糕。**我们将预填充和生成批量大小结合起来。在大的预填充批量大小下，首次令牌时间（TTFT）非常糟糕——你需要完成所有预填充，然后用户才能看到任何令牌。在小的批量大小下，生成吞吐量也非常糟糕。'
- en: '**We block shorter generations on longer ones.** Many sequences will finish
    before others, leaving empty batch slots during generation, hurting generate throughput
    further. The problem exacerbates as batch size and generation length increases.'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**我们阻止较短的生成在较长的生成上执行。**许多序列会在其他序列之前完成，在生成过程中留下空批处理槽位，进一步损害生成吞吐量。随着批量大小和生成长度的增加，这个问题会加剧。'
- en: '**Prefills are padded.** Prefills are padded to the longest sequence and we
    waste a lot of compute. There are solutions for this, but historically XLA made
    it quite difficult to skip these FLOPs. Again this becomes worse the bigger the
    batch size and prefill sequence length.'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预填充被填充。**预填充被填充到最长的序列，我们浪费了很多计算资源。对此有解决方案，但历史上XLA使跳过这些FLOPs变得相当困难。同样，随着批量大小和预填充序列长度的增加，这个问题会变得更糟。'
- en: '**We’re forced to share a sharding between prefill and generation.** Both prefill
    and generate live on the same slice, which means we use the same topology and
    shardings (unless you keep two copies of the weights) for both and is generally
    unhelpful for performance e.g. generate wants a lot more model sharding.'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**我们被迫在预填充和生成之间共享一个分片。**预填充和生成都生活在同一个切片上，这意味着我们为两者使用相同的拓扑和分片（除非你保留两个权重的副本），并且通常对性能没有帮助，例如生成需要更多的模型分片。'
- en: Therefore this method is only recommended for edge applications (which usually
    only cares about serving a single user and using hardware with less FLOPs/byte)
    and rapid iteration early in the lifecycle of a Transformer codebase (due to its
    simplicity).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这种方法仅适用于边缘应用（通常只关注为单个用户提供服务并使用具有较少FLOPs/byte的硬件）以及Transformer代码库生命周期早期的快速迭代（由于其简单性）。
- en: 'A slightly better approach involves performing prefill at batch size 1 (where
    it is compute-bound but has reasonable latency) but batch multiple requests together
    during generation:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 一种稍微好一点的方法是在批量大小为1时执行预填充（此时它是计算受限但具有合理的延迟），但在生成期间将多个请求一起批量处理：
- en: <picture>![](../Images/aa60bc14d625c68e5ef6eee341fa1e0f.png)</picture>
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: <picture>![](../Images/aa60bc14d625c68e5ef6eee341fa1e0f.png)</picture>
- en: This will avoid wasted TTFT from batched prefill while keeping generation throughput
    high. We call this an **interleaved** configuration, since we “interleave” prefill
    and generation steps. This is very powerful for bulk generation applications like
    evaluations where throughput is the main goal. The orchestrator can be configured
    to prioritise prefill the moment any generation slots open up, ensuring high utilisation
    even for very large generation batch sizes. We can also avoid padding our prefill
    to the maximum length, since it isn’t batched with another request.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这将避免批量预填充时的TTFT浪费，同时保持生成吞吐量高。我们称这种配置为**交错**配置，因为我们“交错”预填充和生成步骤。这对于像评估这样的批量生成应用非常强大，因为吞吐量是主要目标。协调器可以配置为在任何生成槽位打开时优先预填充，确保即使对于非常大的生成批量大小也能实现高利用率。我们还可以避免将预填充填充到最大长度，因为它不会与另一个请求批量处理。
- en: The main disadvantage is that when the server is performing a prefill, the generation
    of all other requests pauses since all the compute resources will be consumed
    by the prefill. User A whose response is busy decoding will be blocked by user
    B whose prefill is occurring. This means even though TTFT has improved, the token
    generation will be jittery and slow on average, which is not a good user experience
    for many applications — other user’s prefills are on the critical path of the
    overall latency of a request.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 主要缺点是，当服务器正在执行预填充时，所有其他请求的生成都会暂停，因为所有计算资源都将被预填充消耗。忙于解码响应的用户A会被正在发生预填充的用户B阻塞。这意味着尽管TTFT有所改善，但平均而言，令牌生成将会有所波动且速度较慢，这对许多应用来说并不是一个好的用户体验——其他用户的预填充是请求整体延迟的关键路径。
- en: To get around this, we separate decode and prefill. While Transformer inference
    can be done on one server, it is often better from a latency standpoint to execute
    the two different tasks on two sets of TPUs/GPUs. Prefill servers generate KV
    caches that get sent across the network to the generate servers, which batch multiple
    caches together and generate tokens for each of them. We call this **“disaggregated”**
    serving.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们将解码和预填充分开。虽然Transformer推理可以在一个服务器上完成，但从延迟的角度来看，通常最好在两套TPU/GPU上执行两个不同的任务。预填充服务器生成键值缓存，并将其发送到网络中的生成服务器，生成服务器将多个缓存一起批处理并为每个缓存生成令牌。我们称这种服务为**“解耦”**服务。
- en: <picture>![](../Images/1d26de9d1f547b73e58d92d037a1d09d.png)</picture>
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: <picture>![](../Images/1d26de9d1f547b73e58d92d037a1d09d.png)</picture>
- en: 'This provides a few advantages:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这提供了一些优势：
- en: '**Low latency at scale**: A user’s request never blocks on another user’s,
    except if there is insufficient prefill capacity. The request should be immediately
    prefilled, then sent to the generation server, then immediately slotted into the
    generation buffer. If we expect many concurrent requests to come in, we can scale
    the number of prefill servers independently from the number of generate servers
    so users are not left in the prefill queue for an extended period of time.'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**低延迟扩展性**：用户的请求永远不会因为其他用户的请求而阻塞，除非预填充容量不足。请求应立即预填充，然后发送到生成服务器，然后立即放入生成缓冲区。如果我们预计会有许多并发请求到来，我们可以独立于生成服务器的数量扩展预填充服务器的数量，这样用户就不会在预填充队列中等待过长时间。'
- en: '**Specialization:** Quite often, the latency-optimal parameter sharding strategy/hardware
    topology for prefill and generate is quite different (for instance, more model
    parallelism is useful for generate but not prefill). Constraining the two operations
    to use the same sharding hurts the performance of both, and having two sets of
    weights uses memory. Also, by moving prefill onto its own server, it doesn’t need
    to hold any KV caches except the one it’s currently processing. That means we
    have a lot more memory free for history caching (see the next section) or optimizing
    prefill latency.'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**专业化**：通常，预填充和生成的延迟最优参数划分策略/硬件拓扑结构相当不同（例如，对于生成来说，更多的模型并行性是有用的，但不适用于预填充）。限制两个操作使用相同的划分会损害两者的性能，并且使用两套权重会占用内存。此外，通过将预填充移动到自己的服务器上，它不需要保留任何除了当前正在处理的键值缓存以外的缓存。这意味着我们有更多的内存可用于历史缓存（见下一节）或优化预填充延迟。'
- en: One downside is that the KV cache now needs to be shifted across the network.
    This is typically acceptable but again provides a motivation for reducing KV cache
    size.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 一个缺点是现在需要将键值缓存通过网络传输。这通常是可接受的，但再次为减少键值缓存大小提供了动机。
- en: '**Takeaway:** for latency-sensitive, high-throughput serving, we typically
    have to separate prefill and generation into separate servers, with prefill operating
    at batch 1 and generation batching many concurrent requests together.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '**要点**：对于对延迟敏感、高吞吐量服务，我们通常需要将预填充和生成分别部署到不同的服务器上，预填充在批次1运行，生成将多个并发请求一起批处理。'
- en: Continuous batching
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 持续批处理
- en: 'Problem (2) above motivates the concept of **continuous batching**. We optimize
    and compile:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的问题（2）促使了**持续批处理**的概念。我们优化和编译：
- en: A number of prefill functions with variable context lengths and inserts it into
    some KV buffer, some maximum batch size and context length/number of pages.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将具有可变上下文长度的多个预填充函数插入到某些键值缓冲区中，一些最大批量和上下文长度/页数。
- en: A generate function which takes in the KV cache, and performs the generation
    step for all currently active requests.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个生成函数，它接收键值缓存，并为所有当前活跃的请求执行生成步骤。
- en: We then combine these functions with an orchestrator which queues the incoming
    requests, calls prefill and generate depending on the available generate slots,
    handles history caching (see next section) and streams the tokens out.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们然后将这些函数与一个编排器结合，编排器负责排队处理传入的请求，根据可用的生成槽位调用预填充和生成，处理历史缓存（见下一节）并将令牌流出。
- en: <picture>![](../Images/508a488a240bed38b9b3d9d75c3363ef.png)</picture>
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: <picture>![](../Images/508a488a240bed38b9b3d9d75c3363ef.png)</picture>
- en: Prefix caching
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 前缀缓存
- en: 'Since prefill is expensive and compute-bound (giving us less headroom), one
    of the best ways to reduce its cost is to do less of it. Because LLMs are autoregressive,
    the queries [“I”, “like”, “dogs”] and [“I”, “like”, “cats”] produce KV caches
    that are identical in the first two tokens. What this means is that, in principle,
    if we compute the “I like dogs” cache first and then the “I like cats” cache,
    we only need to do 1 / 3 of the compute. We can save most of the work by reusing
    the cache. This is particularly powerful in a few specific cases:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 由于预填充成本高昂且计算密集（给我们留下较少的余量），降低其成本的最佳方法之一是减少预填充的操作。因为大型语言模型（LLMs）是自回归的，所以查询“我喜欢狗”和“我喜欢猫”产生的
    KV 缓存在前两个标记上是相同的。这意味着，原则上，如果我们首先计算“我喜欢狗”的缓存，然后计算“我喜欢猫”的缓存，我们只需要进行 1/3 的计算。我们可以通过重用缓存来节省大部分工作。这在一些特定情况下尤其强大：
- en: '**Chatbots**: most chatbot conversations involve a back-and-forth dialog that
    strictly appends to itself. This means if we can save the KV caches from each
    dialog turn, we can skip computation for all but the newest tokens.'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**聊天机器人**：大多数聊天机器人对话涉及严格追加的来回对话。这意味着如果我们能保存每个对话回合的 KV 缓存，我们就可以跳过除了最新标记之外的所有计算。'
- en: '**Few-shot prompting**: if we have any kind of few-shot prompt, this can be
    saved and reused for free. System instructions often have this form as well.'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**少样本提示**：如果我们有任何类型的少样本提示，这可以免费保存和重用。系统指令通常也有这种形式。'
- en: 'The only reason this is hard to do is memory constraints. As we’ve seen, KV
    caches are big (often many GB), and for caching to be useful we need to keep them
    around until a follow-up query arrives. Typically, any unused HBM on the prefill
    servers can be used for a local caching system. Furthermore, accelerators usually
    have a lot of memory on their CPU hosts (e.g. a 8xTPUv5e server has 128GiB of
    HBM, but around 450GiB of Host DRAM). This memory is much slower than HBM — too
    slow to do generation steps usually — but is fast enough for a cache read. In
    practice:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这之所以难以实现，仅仅是因为内存限制。正如我们所见，KV 缓存很大（通常有多个 GB），为了使缓存有用，我们需要在后续查询到达之前将其保留。通常，任何预填充服务器上未使用的
    HBM 都可以用于本地缓存系统。此外，加速器通常在其 CPU 主机上拥有大量内存（例如，一个 8xTPUv5e 服务器有 128GiB 的 HBM，但大约有
    450GiB 的主机 DRAM）。这种内存比 HBM 慢得多——通常太慢以至于无法进行生成步骤——但对于缓存读取来说足够快。在实践中：
- en: Because the KV cache is local to the set of TPUs that handled the initial request,
    we need some form of affinity routing to ensure follow-up queries arrive at the
    same replica. This can cause issues with load balancing.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于 KV 缓存位于处理初始请求的 TPUs 集合中，我们需要某种形式的亲和路由来确保后续查询到达相同的副本。这可能会引起负载均衡问题。
- en: A smaller KV cache is helpful (again) — it enables us to save more KV caches
    in the same amount of space, and reduce read times.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 较小的 KV 缓存是有帮助的（再次）——它使我们能够在相同的空间内保存更多的 KV 缓存，并减少读取时间。
- en: The KV cache and their lookups can be stored quite naturally in a tree or trie.
    Evictions can happen on an LRU basis.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KV 缓存及其查找可以非常自然地存储在树或字典树中。淘汰操作可以基于最近最少使用（LRU）原则进行。
- en: <picture>![](../Images/2e461a95afcd3fcf09063c6cc8c6e63b.png)</picture>
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: <picture>![](../Images/2e461a95afcd3fcf09063c6cc8c6e63b.png)</picture>
- en: '**Figure:** KV prefix cache implemented as an LRU trie. We can avoid duplicating
    KV memory by sharing prefixes. Source: [Character.ai blog](https://research.character.ai/optimizing-inference/?ref=blog.character.ai).'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '**图**：将 KV 前缀缓存实现为 LRU 字典树。我们可以通过共享前缀来避免重复 KV 内存。来源：[Character.ai 博客](https://research.character.ai/optimizing-inference/?ref=blog.character.ai)。'
- en: 'Let’s look at an implementation: JetStream'
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 让我们看看一个实现示例：JetStream
- en: Google has open-sourced a library that implements this logic called [JetStream](https://github.com/google/JetStream).
    The server has a set of “prefill engines” and “generate engines”, usually on different
    TPU slices, which are orchestrated by a single controller. Prefill happens in
    the “[prefill thread](https://github.com/AI-Hypercomputer/JetStream/blob/c0f83127c16d7861cacc560303a28404c6cbb24c/jetstream/core/orchestrator.py#L499)”,
    while generation happens in the “[generate thread](https://github.com/AI-Hypercomputer/JetStream/blob/c0f83127c16d7861cacc560303a28404c6cbb24c/jetstream/core/orchestrator.py#L629)”.
    We also have a “[transfer thread](https://github.com/AI-Hypercomputer/JetStream/blob/c0f83127c16d7861cacc560303a28404c6cbb24c/jetstream/core/orchestrator.py#L592)”
    that orchestrates copying the KV caches from the prefill to generate slices.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: Google 已经开源了一个实现此逻辑的库，称为 [JetStream](https://github.com/google/JetStream)。服务器有一组“预填充引擎”和“生成引擎”，通常位于不同的
    TPU 切片上，由单个控制器进行编排。预填充发生在“[预填充线程](https://github.com/AI-Hypercomputer/JetStream/blob/c0f83127c16d7861cacc560303a28404c6cbb24c/jetstream/core/orchestrator.py#L499)”中，而生成发生在“[生成线程](https://github.com/AI-Hypercomputer/JetStream/blob/c0f83127c16d7861cacc560303a28404c6cbb24c/jetstream/core/orchestrator.py#L629)”中。我们还有一个“[传输线程](https://github.com/AI-Hypercomputer/JetStream/blob/c0f83127c16d7861cacc560303a28404c6cbb24c/jetstream/core/orchestrator.py#L592)”来编排将
    KV 缓存从预填充复制到生成切片。
- en: 'The Engine interface (implemented [here](https://github.com/google/JetStream/blob/445f1aa8e857d0a09d72618e365daf80723bdf4c/jetstream/engine/engine_api.py#L138))
    is a generic interface that any LLM must provide. The key methods are:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 引擎接口（在[此处](https://github.com/google/JetStream/blob/445f1aa8e857d0a09d72618e365daf80723bdf4c/jetstream/engine/engine_api.py#L138)实现）是一个通用的接口，任何大型语言模型都必须提供。关键方法包括：
- en: '**prefill:** takes a set of input tokens and generates a KV cache.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预填充操作**：接受一组输入标记并生成一个 KV 缓存。'
- en: '**insert:** takes a KV cache and inserts it into the batch of KV caches that
    generate is generating from.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**插入操作**：接受一个 KV 缓存并将其插入正在生成的 KV 缓存批次中。'
- en: '**generate:** takes a set of batched KV caches and generates one token per
    batch entry, appending a single token’s KV cache to the decode state for each
    token.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成操作**：接受一组批处理的 KV 缓存并为每个批次条目生成一个标记，将单个标记的 KV 缓存附加到每个标记的解码状态中。'
- en: We also have a PyTorch version of JetStream available [here](https://github.com/google/jetstream-pytorch).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也提供了 JetStream 的 PyTorch 版本，[在此处](https://github.com/google/jetstream-pytorch)可以找到。
- en: Worked Problems
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作问题
- en: 'I’m going to invent a new model based on LLaMA-2 13B for this section. Here
    are the details:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我将基于 LLaMA-2 13B 为这一部分发明一个新的模型。以下是详细信息：
- en: '| hyperparam | value |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| hyperparam | value |'
- en: '| --- | --- |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| L (num_layers) | 64 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| L (num_layers) | 64 |'
- en: '| D (d_model) | 4,096 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| D (d_model) | 4,096 |'
- en: '| F (ffw_dimension) | 16,384 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| F (ffw_dimension) | 16,384 |'
- en: '| N (num_heads) | 32 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| N (num_heads) | 32 |'
- en: '| K (num_kv_heads) | 8 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| K (num_kv_heads) | 8 |'
- en: '| H (qkv_dim) | 256 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| H (qkv_dim) | 256 |'
- en: '| V (num_embeddings) | 32,128 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| V (num_embeddings) | 32,128 |'
- en: '**Question 1:** How many parameters does the above model have? How large are
    its KV caches per token in int8? *You can assume we share the input and output
    projection matrices.*'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 1**：上述模型有多少个参数？每个标记的 KV 缓存以 int8 为单位有多大？*你可以假设我们共享输入和输出投影矩阵。*'
- en: <details><summary>Click here for the answer.</summary>
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>点击此处查看答案。</summary>
- en: '**Parameter count:**'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '**参数数量**：'
- en: 'MLP parameter count: $L * D * F * 3$'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLP 参数数量：$L * D * F * 3$
- en: 'Attention parameter count: $L * 2 * D * H * (N + K)$'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力参数数量：$L * 2 * D * H * (N + K)$
- en: 'Vocabulary parameter: $D * V$ (since we share these matrices)'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词汇参数：$D * V$（因为我们共享这些矩阵）
- en: Our total parameter count is thus $L * D * (3F + 2H * (N + K)) + D * V$. Plugging
    in the numbers above, we have `64 * 4096 * (3*16384 + 2 * 256 * (32 + 8)) + 4096
    * 32128 = 18.4e9`. Thus, this model has about 18.4 billion parameters.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的总参数数量为 $L * D * (3F + 2H * (N + K)) + D * V$。将上面的数字代入，我们得到 `64 * 4096
    * (3*16384 + 2 * 256 * (32 + 8)) + 4096 * 32128 = 18.4e9`。因此，这个模型大约有 184 亿个参数。
- en: The KV caches are $2 * L * K * H$ per token in int8, which is `2 * 64 * 8 *
    256 = 262kB` per token.</details>
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: KV 缓存的大小为每个标记 $2 * L * K * H$，以 int8 为单位，每个标记为 `2 * 64 * 8 * 256 = 262kB`。</details>
- en: '**Question 2:** Say we want to serve this model on a TPUv5e 4x4 slice and can
    fully shard our KV cache over this topology. What’s the largest batch size we
    can fit, assuming we use int8 for everything and want to support 128k sequences?
    What if we dropped the number of KV heads to 1?'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 2**：如果我们想在 TPUv5e 4x4 切片上运行此模型，并且可以完全在这个拓扑上分割我们的 KV 缓存，那么我们能够适应的最大批次大小是多少？如果我们把
    KV 头的数量减少到 1 呢？'
- en: <details><summary>Click here for the answer.</summary>
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>点击此处查看答案。</summary>
- en: Our KV caches have size $2 \cdot L \cdot K \cdot H$ per token in int8, or `2
    * 64 * 8 * 256 = 262kB`. For 128k sequences, this means `262e3 * 128e3 = 33.5GB`
    per batch entry. Since each TPU has 16GB of HBM, including our parameters, the
    largest batch size we can fit is `(16 * 16e9 - 18.4e9) / 33.5e9 = 7`. If we had
    $K=1$, we would have 8 times this, aka about 56.</details>
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们KV缓存的大小为每个token的$2 \cdot L \cdot K \cdot H$，在int8中，或者`2 * 64 * 8 * 256 = 262kB`。对于128k序列，这意味着每个批次条目为`262e3
    * 128e3 = 33.5GB`。由于每个TPU有16GB的HBM，包括我们的参数，我们可以适应的最大批次大小是`(16 * 16e9 - 18.4e9)
    / 33.5e9 = 7`。如果我们有$K=1$，那么我们将有8倍于此，即大约56。</details>
- en: '**Question 3:** How long does it take to load all the parameters into the MXU
    from HBM assuming they’re fully sharded on a TPU v5e 4x4 slice? Assume int8 parameters.
    *This is a good lower bound on the per-step latency.*'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题3**：假设在TPU v5e 4x4切片上完全分片，将所有参数从HBM加载到MXU需要多长时间？假设参数为int8。*这是一个很好的每步延迟的下限。*'
- en: <details><summary>Click here for the answer.</summary>
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>点击此处查看答案。</summary>
- en: We have a total of 18.4B parameters, or 18.4e9 bytes in int8\. We have 8.1e11
    HBM bandwidth per chip, so it will take roughly `18e9 / (8.1e11 * 16) = 1.3ms`
    assuming we can fully use our HBM bandwidth.</details>
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们总共有18.4B参数，或者18.4e9字节在int8中。我们每个芯片有8.1e11 HBM带宽，所以如果我们能完全使用我们的HBM带宽，大约需要`18e9
    / (8.1e11 * 16) = 1.3ms`。</details>
- en: '**Question 4:** Let’s say we want to serve this model on a TPUv5e 4x4 slice
    using int8 FLOPs and parameters/activations. How would we shard it for both prefill
    and decode? *Hint: maybe answer these questions first:*'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题4**：假设我们想在TPUv5e 4x4切片上使用int8 FLOPs和参数/激活来提供这个模型。我们将如何为预填充和解码进行分片？*提示：也许先回答这些问题：*'
- en: What does ICI look like on a 4x4?
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在4x4上ICI看起来像什么？
- en: What’s the roofline bound on tensor parallelism?
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 张量并行性的roofline界限是什么？
- en: How can we shard the KV caches?
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何分片KV缓存？
- en: For this sharding, what is the rough per-step latency for generation?
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种分片，生成的大致每步延迟是多少？
- en: '**Question 5:** Let’s pretend the above model is actually an MoE. An MoE model
    is effectively a dense model with E copies of the FFW block. Each token passes
    through k of the FFW blocks and these `k` are averaged to produce the output.
    Let’s use `E=16` and `k=2` with the above settings.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题5**：让我们假设上面的模型实际上是一个MoE。MoE模型实际上是一个具有E个FFW块的密集模型。每个token通过k个FFW块，这些`k`个块被平均以产生输出。让我们使用`E=16`和`k=2`的上述设置。'
- en: How many total and activated parameters does it have? *Activated means used
    by any given token.*
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它有多少总参数和激活参数？*激活参数是指任何给定token使用的参数。*
- en: What batch size is needed to become FLOPs bound on TPU v5e?
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 需要多少批次大小才能在TPU v5e上成为FLOPs限制？
- en: How large are its KV caches per token?
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个token的KV缓存有多大？
- en: How many FLOPs are involved in a forward pass with T tokens?
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在一个包含T个token的前向传递中涉及多少FLOPs？
- en: <details><summary>Click here for the answer.</summary>
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>点击此处查看答案。</summary>
- en: (1) As an MoE, each MLP block now has $3 * E * D * F$ parameters, an increase
    of $E$ over the dense variant. Thus it now has $L * D * (3EF + 2H * (N + K)) +
    D * V$ or `64 * 4096 * (3*16*16384 + 2 * 256 * (32 + 8)) + 4096 * 32128 = 212e9`
    total parameters, an increase of about 12x. For activated parameters, we have
    $k$ rather than $E$ activated parameters, for a total of `64 * 4096 * (3*2*16384
    + 2 * 256 * (32 + 8)) + 4096 * 32128 = 31.2e9`, an increase of less than 2x over
    the dense variant.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 作为MoE，每个MLP块现在有$3 * E * D * F$个参数，比密集版本增加了$E$。因此，它现在有$L * D * (3EF + 2H
    * (N + K)) + D * V$或`64 * 4096 * (3*16*16384 + 2 * 256 * (32 + 8)) + 4096 * 32128
    = 212e9`个总参数，比密集版本增加了约12倍。对于激活参数，我们有$k$而不是$E$个激活参数，总共有`64 * 4096 * (3*2*16384
    + 2 * 256 * (32 + 8)) + 4096 * 32128 = 31.2e9`，比密集版本增加了不到2倍。
- en: (2) Because we have $E$ times more parameters for only $k$ times more FLOPs,
    our HBM roofline increases by a factor of $E/k$. That means on a TPU v5e we need
    about `240 * (16 / 2) = 1920` tokens.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 因为我们只有$k$倍更多的FLOPs就有$E$倍更多的参数，我们的HBM roofline增加了$E/k$倍。这意味着在TPU v5e上我们需要大约`240
    * (16 / 2) = 1920`个token。
- en: (3) The KV cache size stays the same as the MoE character doesn’t change anything
    about the attention mechanism.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: (3) KV缓存的大小与MoE字符保持一致，因为MoE字符没有改变注意力机制。
- en: (4) This is still $2ND$ where $D$ is the activated parameter count. Thus this
    is $2 * \text{31.2e9} * T$.</details>
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: (4) 这仍然是$2ND$，其中$D$是激活参数的数量。因此，这是$2 * \text{31.2e9} * T$。</details>
- en: '**Question 6:** With MoEs, we can do “expert sharding”, where we split our
    experts across one axis of our mesh. In our standard notation, our first FFW weight
    has shape `[E, D, F]` and we shard it as [E[Z], D[X], F[Y]] where `X` is only
    used during training as our FSDP dimension. Let’s say we want to do inference
    on a TPU v5e:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题6**：使用MoEs，我们可以进行“专家分片”，即在网格的一个轴上分割我们的专家。按照我们的标准符号，我们的第一个FFW权重形状为 `[E,
    D, F]`，我们将其分片为 [E[Z], D[X], F[Y]]，其中`X`仅在训练期间作为我们的FSDP维度使用。假设我们想在TPU v5e上进行推理：'
- en: What’s the HBM weight loading time for the above model on a TPU v5e 8x16 slice
    with Y=8, Z=16? How much free HBM is available per TPU?
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于TPU v5e 8x16切片上上述模型，Y=8，Z=16的HBM权重加载时间是多少？每个TPU有多少空闲HBM？
- en: What is the smallest slice we could fit our model on?
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们能将模型放入最小的切片中吗？
- en: '**Question 7 [2D model sharding]:** Here we’ll work through the math of what
    the [ESTI paper](https://arxiv.org/pdf/2211.05102) calls 2D weight-stationary
    sharding. We describe this briefly in Appendix B, but try doing this problem first
    to see if you can work out the math. The basic idea of 2D weight stationary sharding
    is to shard our weights along both the $D$ and $F$ axes so that each chunk is
    roughly square. This reduces the comms load and allows us to scale slightly farther.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题7 [2D模型分片]**：我们将通过[ESTI论文](https://arxiv.org/pdf/2211.05102)中称为2D权重静态分片的数学问题。我们在附录B中简要描述了这一点，但先尝试解决这个问题，看看你是否能解决数学问题。2D权重静态分片的基本思想是沿着$D$和$F$轴分片我们的权重，使得每个块大致为正方形。这减少了通信负载，并允许我们稍微扩展一些。'
- en: 'Here’s the algorithm for 2D weight stationary:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 这是2D权重静态分片的算法：
- en: In[B, D[X]] = **AllGather**[YZ](In[B, D[XYZ]])
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: In[B, D[X]] = **AllGather**[YZ](In[B, D[XYZ]])
- en: Tmp[B, F[YZ]] {U.X} = In[B, D[X]] *[D] W[in][D[X], F[YZ]]
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Tmp[B, F[YZ]] {U.X} = In[B, D[X]] *[D] W[in][D[X], F[YZ]]
- en: Tmp[B, F[YZ]] = **AllReduce**[X](Tmp[B, F[YZ]] {U.X})
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Tmp[B, F[YZ]] = **AllReduce**[X](Tmp[B, F[YZ]] {U.X})
- en: Out[B, D[X]] {U.YZ} = Tmp[B, F[YZ]] *[F] W2[F[YZ], D[X]]
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Out[B, D[X]] {U.YZ} = Tmp[B, F[YZ]] *[F] W2[F[YZ], D[X]]
- en: Out[B, D[XYZ]] = **ReduceScatter**[YZ](Out[B, D[X]] {U.YZ})
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Out[B, D[XYZ]] = **ReduceScatter**[YZ](Out[B, D[X]] {U.YZ})
- en: Your goal is to work out $T_\text{math}$ and $T_\text{comms}$ for this algorithm
    and find when it will outperform traditional 3D model sharding?
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 你的目标是计算出这个算法的$T_\text{math}$和$T_\text{comms}$，并找出何时它会优于传统的3D模型分片？
- en: <details><summary>Click here for the answer!</summary>
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>点击此处查看答案！</summary>
- en: Let’s work out $T_\text{math}$ and $T_\text{comms}$. All our FLOPs are fully
    sharded so as before we have $T_\text{math} = 4BDF / (N \cdot C)$ but our comms
    are now
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算出$T_\text{math}$和$T_\text{comms}$。我们所有的FLOPs都是完全分片的，所以和之前一样，我们有$T_\text{math}
    = 4BDF / (N \cdot C)$，但我们的通信现在是
- en: \[\begin{align*} T_\text{2D comms} = \frac{2BD}{2X \cdot W_\text{ici}} + \frac{4BF}{YZ
    \cdot W_\text{ici}} + \frac{2BD}{2X \cdot W_\text{ici}} = \frac{2BD}{X \cdot W_\text{ici}}
    + \frac{4BF}{YZ \cdot W_\text{ici}} \end{align*}\]
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} T_\text{2D comms} = \frac{2BD}{2X \cdot W_\text{ici}} + \frac{4BF}{YZ
    \cdot W_\text{ici}} + \frac{2BD}{2X \cdot W_\text{ici}} = \frac{2BD}{X \cdot W_\text{ici}}
    + \frac{4BF}{YZ \cdot W_\text{ici}} \end{align*}\]
- en: where we note that the AllReduce is twice as expensive and we scale our comms
    by the number of axes over which each operation is performed. Assuming we have
    freedom to choose our topology and assuming $F=4D$ (as in LLaMA-2), we claim (by
    some basic calculus) that the optimal values for $X$, $Y$, and $Z$ are $X = \sqrt{N
    / 8}$, $YZ = \sqrt{8N}$ so the total communication is
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到AllReduce的成本是两倍，并且我们通过每个操作执行的轴的数量来缩放我们的通信。假设我们有选择拓扑的自由，并且假设$F=4D$（如LLaMA-2中所示），我们声称（通过一些基本的微积分）$X$、$Y$和$Z$的最佳值分别是$X
    = \sqrt{N / 8}$，$YZ = \sqrt{8N}$，因此总的通信量是
- en: \[T_\text{2D comms} = \frac{2B}{W_\text{ici}} \left(\frac{D}{X} + \frac{8D}{YZ}\right)
    = \frac{\sqrt{128} BD}{\sqrt{N} \cdot W_\text{ici}} \approx \frac{11.3 BD}{\sqrt{N}
    \cdot W_\text{ici}}\]
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: \[T_\text{2D comms} = \frac{2B}{W_\text{ici}} \left(\frac{D}{X} + \frac{8D}{YZ}\right)
    = \frac{\sqrt{128} BD}{\sqrt{N} \cdot W_\text{ici}} \approx \frac{11.3 BD}{\sqrt{N}
    \cdot W_\text{ici}}\]
- en: Firstly, copying from above, normal 1D model parallelism would have $T_\text{model
    parallel comms} = 4BD / (3 \cdot W_\text{ici})$, so when are the new comms smaller?
    We have
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，从上面复制，正常的1D模型并行会有$T_\text{model parallel comms} = 4BD / (3 \cdot W_\text{ici})$，那么何时新的通信会更小？我们有
- en: \[\begin{align*} T_\text{model parallel comms} > T_\text{2D comms} \iff \frac{4BD}{3
    \cdot W_\text{ici}} > \frac{\sqrt{128} BD}{\sqrt{N} \cdot W_\text{ici}} \\ \iff
    N > 128 \cdot \left(\frac{3}{4}\right)^2 = 81 \end{align*}\]
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} T_\text{model parallel comms} > T_\text{2D comms} \iff \frac{4BD}{3
    \cdot W_\text{ici}} > \frac{\sqrt{128} BD}{\sqrt{N} \cdot W_\text{ici}} \\ \iff
    N > 128 \cdot \left(\frac{3}{4}\right)^2 = 81 \end{align*}\]
- en: For a general $F$, we claim this condition is
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一般的$F$，我们声称这个条件是
- en: \[N > 32 \cdot \left(\frac{F}{D}\right) \cdot \left(\frac{3}{4}\right)^2\]
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: \[N > 32 \cdot \left(\frac{F}{D}\right) \cdot \left(\frac{3}{4}\right)^2\]
- en: So that tells us if we have more than 81 chips, we’re better off using this
    new scheme. Now this is a slightly weird result because we’ve historically found
    ourselves ICI bound at around ~20 way tensor parallelism. But here, even if we’re
    communication-bound, our total communication continues to decrease with the number
    of total chips! What this tells us is that we can continuous to increase our chips,
    increase our batch size, do more parameter scaling, and see reduced latency.</details>
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这告诉我们，如果我们有超过81个芯片，我们最好使用这个新方案。现在这是一个有点奇怪的结果，因为我们历史上在约20倍张量并行时发现自己处于ICI瓶颈。但在这里，即使我们处于通信瓶颈，我们的总通信量仍然随着总芯片数的增加而持续减少！这告诉我们，我们可以继续增加芯片，增加批量大小，进行更多的参数缩放，并看到延迟的降低。</details>
- en: That’s all for Part 7! For Part 8, with a look at how we might serve LLaMA 3
    on TPUs, click [here](../applied-inference).
  id: totrans-304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第七部分的内容到此结束！要了解如何在TPU上运行LLaMA 3，请点击[这里](../applied-inference)。
- en: Appendix
  id: totrans-305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: 'Appendix A: How real is the batch size > 240 rule?'
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录A：批量大小>240规则的真实性如何？
- en: The simple rule we provided above, that our batch size must be greater than
    240 tokens to be compute-bound, is roughly true but ignores some ability of the
    TPU to prefetch the weights while other operations are not using all available
    HBM, like when doing inter-device communication.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 我们上面提供的简单规则，即我们的批量大小必须大于240个token才能成为计算瓶颈，大致上是正确的，但它忽略了TPU在执行其他操作（如设备间通信）时预取权重的能力。
- en: Here’s an empirical plot of layer time (in microseconds) for a small Transformer
    with d[model] 8192, d[ff] 32768, and only 2 matmuls per layer. This comes from
    [this Colab notebook](https://colab.sandbox.google.com/drive/1_6krERgtolH7hbUIo7ewAMLlbA4fqEF8?usp=sharing).
    You’ll see that step time increases very slowly up until around batch 240, and
    then increases linearly.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个小型Transformer的层时间（以微秒为单位）的经验性图表，其d[model]为8192，d[ff]为32768，并且每层只有2个matmuls。这来自[这个Colab笔记本](https://colab.sandbox.google.com/drive/1_6krERgtolH7hbUIo7ewAMLlbA4fqEF8?usp=sharing)。您会看到，步骤时间在约240个批次之前增长非常缓慢，然后线性增长。
- en: <picture>![](../Images/e72ff0ae288f12ea126cefef7d9b0fa9.png)</picture>
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: <picture>![](../Images/e72ff0ae288f12ea126cefef7d9b0fa9.png)</picture>
- en: Here’s the actual throughput in tokens / us. This makes the argument fairly
    clearly. Since our layer is about 600M parameters sharded 4 ways here, we’d expect
    a latency of roughly 365us at minimum.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是实际的吞吐量（以token/μs为单位）。这使得论点相当清晰。由于我们的层大约有600M个参数，分片4次，我们预计至少的延迟为365μs。
- en: <picture>![](../Images/a5a8f07ebed03d5c12afbef591eef5b7.png)</picture>
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: <picture>![](../Images/a5a8f07ebed03d5c12afbef591eef5b7.png)</picture>
- en: So at least in this model, we do in fact see throughput increase until about
    BS240 per data parallel shard.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，至少在这个模型中，我们确实看到吞吐量在约BS240每个数据并行分片时增加。
- en: 'Appendix B: 2D Weight Stationary sharding'
  id: totrans-313
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录B：二维权重静态分片
- en: As the topology grows, if we have access to higher dimensional meshes (like
    that of TPUs) it is possible to refine this further with “**2D Weight Sharding”**.
    By introducing a second sharding axis. We call this “**2D Weight Stationary**”,
    and was described in more detail in the [Efficiently Scaling Transformer Inference
    paper](https://arxiv.org/abs/2211.05102).
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 随着拓扑的增长，如果我们能够访问更高维度的网格（如TPU的网格），则可以使用“**二维权重分片**”进一步细化。通过引入第二个分片轴。我们称之为“**二维权重静态**”，这在《Efficiently
    Scaling Transformer Inference》论文中有更详细的描述。[Efficiently Scaling Transformer Inference](https://arxiv.org/abs/2211.05102)。
- en: Because we’re only sharding the hidden \(F\) dimension in Megatron, it can become
    significantly smaller than \(E\) (the \(d_\text{model}\) dimension) once the number
    of chips grows large with 1D sharding. This means at larger batch sizes, it can
    be more economical to perform a portion of the collectives over the hidden dimension
    after the first layer of the MLP is applied.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在Megatron中只分片隐藏的\(F\)维度，一旦芯片数量随着1D分片增长而变得很大，它就可以显著小于\(E\)（\(d_\text{model}\)维度）。这意味着在更大的批量大小下，在MLP的第一层应用之后在隐藏维度上执行部分集体操作可能更加经济。
- en: <picture>![](../Images/d9711b60a15e135573d6707a74847a42.png)</picture>
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: <picture>![](../Images/d9711b60a15e135573d6707a74847a42.png)</picture>
- en: 'This figure shows:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 此图显示了：
- en: 1D weight-stationary sharding, a.k.a. Pure Megatron sharding, where activations
    are fully replicated after AllGather, and weights are fully sharded over the hidden
    F dimension.
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一维权重静态分片，也称为纯Megatron分片，其中激活在AllGather之后完全复制，权重在隐藏的F维度上完全分片。
- en: 2D weight stationary sharding, where weights are sharded over both the hidden
    F and reduction E dimension, and activations are sharded over the E dimension.
    We perform an AllGather on the (yz) axis before the first layer, then ReduceScatter
    on the (x) axis.
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2D权重静态分片，其中权重在隐藏F和减少E维度上分片，激活在E维度上分片。我们在第一层之前在(yz)轴上进行AllGather，然后在(x)轴上进行ReduceScatter。
- en: For the attention layer, Megatron style sharding is also relatively simple for
    smaller numbers of chips. However, Megatron happens over the \(n_\text{heads}\)
    dimension, which puts a limit on the amount of sharding that is possible. Modifying
    the 2D sharding with for (instead of sharding the hidden, we shard the \(n_\text{heads}\)
    dimension), we gain the ability to scale further.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 对于注意力层，Megatron风格的分片对于较小的芯片数量来说也比较简单。然而，Megatron是在\(n_\text{heads}\)维度上进行的，这限制了可能进行的分片数量。通过修改2D分片（而不是分片隐藏层，而是分片\(n_\text{heads}\)维度），我们获得了进一步扩展的能力。
- en: 'Appendix C: Latency bound communications'
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录C：延迟限制通信
- en: As a recap, in [Section 3](../sharding) we derived the amount of time it takes
    to perform an AllGather into a tensor of size B on each TPU, over X chips on a
    1D ring links of full duplex bandwidth of WICI and latency Tmin.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 作为回顾，在[第3节](../sharding)中，我们推导了在每个TPU上执行AllGather到大小为B的张量所需的时间，在1D全双工带宽为WICI和延迟Tmin的环链上的X个芯片上。
- en: \[T_{total} = \max\left(\frac{T_{min} \cdot |X|}{2}, \frac{B}{W_{ICI}}\right)\]
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: \[T_{total} = \max\left(\frac{T_{min} \cdot |X|}{2}, \frac{B}{W_{ICI}}\right)\]
- en: For large B, the wall clock stays relatively constant because as you add more
    chips to the system, you simultaneously scale the amount of data movement necessary
    to perform the operation and the total bandwidth available.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大的B，墙钟时间相对恒定，因为当你向系统中添加更多芯片时，你同时增加了执行操作所需的数据移动量以及总带宽。
- en: <picture>![](../Images/f03965598db4a6903bb29ab3fd3f29bf.png)</picture>
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](../Images/f03965598db4a6903bb29ab3fd3f29bf.png)'
- en: Because of the relatively low amounts of data being moved during latency optimized
    inference, collectives on activations are often bound by the latency term (especially
    for small batch sizes). One can visualise the latency quite easily, by counting
    the number of hops we need to complete before it is completed.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在延迟优化推理过程中移动的数据量相对较低，激活上的集体操作通常受延迟项的限制（特别是对于小批量大小）。可以通过计算完成前所需的跳数来很容易地可视化延迟。
- en: 'On TPUs, if the tensor size-dependent part of communication is less than 1
    microsecond per hop (a hop is communication between two adjacent devices) we can
    be bottlenecked by the fixed overhead of actually dispatching the collective.
    With `4.5e10` unidirectional ICI bandwidth, ICI communication becomes latency
    bound when: \((\text{bytes} / n_\text{shards}) / 4.5e10 < 1e-6\). For 8-way Megatron
    sharding, this is when `buffer_size < 360kB`. **This actually is not that small
    during inference:** with `BS=16` and `D=8192` in int8, our activations will use
    `16*8192=131kB`, so we’re already latency bound.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在TPU上，如果通信的张量大小相关部分小于每跳1微秒（跳是指两个相邻设备之间的通信），我们可能会因为实际调度集体操作的固定开销而成为瓶颈。在`4.5e10`单向ICI带宽下，当：\((\text{bytes}
    / n_\text{shards}) / 4.5e10 < 1e-6\)时，ICI通信变为延迟限制。对于8路Megatron分片，这是当`buffer_size
    < 360kB`时。**实际上，在推理过程中并不算小**：在`BS=16`和`D=8192`的int8中，我们的激活将使用`16*8192=131kB`，因此我们已经达到延迟限制。
- en: '**Takeaway:** our comms become latency bound when \(\text{total bytes} < W_{ICI}
    \times 1e-6\). For instance, with model parallelism over \(Y\), we become bound
    in int8 when \(Y > BD / 45,000\).'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '**要点**：当\(\text{total bytes} < W_{ICI} \times 1e-6\)时，我们的通信变为延迟限制。例如，当模型并行超过\(Y\)时，我们在int8中成为限制，当\(Y
    > BD / 45,000\)时。'
- en: There’s a parallel to be drawn here with the compute roofline — we are incurring
    the fixed cost of some small operations (latency for comms, memory bandwidth for
    matmuls).
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 这里可以与计算屋顶线进行类比——我们正在承担一些小操作（通信的延迟，矩阵乘法的内存带宽）的固定成本。
- en: 'Appendix D: Speculative Sampling'
  id: totrans-330
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录D：推测采样
- en: When we *really* care about end to end latency, there is one extra trick we
    can employ called speculative sampling<d-cite key="spec1"><d-cite key="spec2">.
    As a recap, we usually generate tokens from a large Transformer one by one:</d-cite></d-cite>
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们**真正**关心端到端延迟时，我们可以使用一个额外的技巧，称为推测采样<sup><a id="spec1"></a><a id="spec2"></a></sup>。作为回顾，我们通常逐个从大型Transformer生成标记：</sup>
- en: <picture>![](../Images/d2c88f2b82fc249eed187209f4f7a3d2.png)</picture>
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](../Images/d2c88f2b82fc249eed187209f4f7a3d2.png)'
- en: 'With speculative sampling, we use a smaller, cheaper model to generate tokens
    and then check the result with the big model. This is easiest to understand with
    *greedy decoding*:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 使用推测性采样时，我们使用一个较小、较便宜的小型模型生成标记，然后使用大模型检查结果。这可以通过**贪婪解码**来最容易理解：
- en: <picture>![](../Images/bcb8d3bf69526010b4291cbcb26c5eef.png)</picture>
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: <图片>![](../Images/bcb8d3bf69526010b4291cbcb26c5eef.png)</图片>
- en: We sample greedily from some smaller, cheaper model. Ideally we use a model
    trained to match the larger model, e.g. by distillation, but it could be as simple
    as simply using n-grams or token matching a small corpus of text.
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从一些较小、较便宜的小型模型中进行贪婪采样。理想情况下，我们使用训练以匹配较大模型的模型，例如通过蒸馏，但它可能只是简单地使用n-gram或与小型文本语料库进行标记匹配。
- en: After we’ve generated K tokens, we use the big model to compute the next-token
    logits for all the tokens we’ve generated so far.
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们生成K个标记后，我们使用大模型计算到目前为止生成的所有标记的下一个标记的logits。
- en: Since we’re decoding greedily, we can just check if the token generated by the
    smaller model has the highest probability of all possible tokens. If one of the
    tokens is wrong, we take the longest correct prefix and replace the first wrong
    token with the correct token, then go back to (1). If all the tokens are correct,
    we can use the last correct logit to sample an extra token before going back to
    (1).
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们是贪婪解码，我们只需检查较小模型生成的标记是否是所有可能标记中概率最高的。如果其中一个标记是错误的，我们取最长的正确前缀，并用正确的标记替换第一个错误的标记，然后回到（1）。如果所有标记都是正确的，我们可以在回到（1）之前使用最后一个正确的logit来采样一个额外的标记。
- en: '**Why is this a latency win?** This scheme still requires us to do the FLOPs-equivalent
    of one forward pass through the big model for every token, but because we can
    batch a bunch of tokens together, we can do all these FLOPs in one forward pass
    and take advantage of the fact that we’re *not* *compute-bound* to score more
    tokens for free.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么这会降低延迟？** 这个方案仍然要求我们对每个标记进行一次大模型的FLOPs等效的前向传递，但由于我们可以将大量标记一起批处理，我们可以在一次前向传递中完成所有这些FLOPs，并利用我们**不是**计算受限的事实来免费为更多标记评分。'
- en: Every accepted token becomes more expensive in terms of FLOPs on average (since
    some will be rejected, and we have to call a draft model), but we wring more FLOPs
    out of the hardware, and the small model is cheap, so we win overall. We also
    share KV cache loads across multiple steps, so **speculative decoding can also
    be a throughput win for long context.** Since everything has been checked by the
    big model, we don’t change the sampling distribution at all (though the exact
    trajectory will differ for non-greedy).
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 每个被接受的标记在平均FLOPs（因为一些会被拒绝，我们不得不调用一个草稿模型）方面都会变得更贵，但我们从硬件中榨取了更多的FLOPs，小型模型又很便宜，所以总体上我们是赢家。我们还跨多个步骤共享KV缓存加载，因此**推测性解码也可以为长上下文带来吞吐量的提升**。由于一切都已经由大模型检查过，我们根本不需要改变采样分布（尽管非贪婪的情况下，确切的轨迹会有所不同）。
- en: Traditionally, speculative decoding relies on the existence of a smaller model
    with a similar sampling distribution to the target model, e.g. LLaMA-2 2B for
    LLaMA-2 70B, which often doesn’t exist. Even when this is available, the smaller
    drafter can still be too expensive if the acceptance rate is low. Instead, it
    can be helpful to embed a drafter within the main model, for instance by adding
    a dedicated drafter head to one of the later layers of the base model<d-cite key="eagle"><d-cite
    key="medusa"><d-cite key="DeepSeek3">. Because this head shares most of its parameters
    with the main model, it’s faster to run and matches the sampling distribution
    more closely.</d-cite></d-cite></d-cite>
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，推测性解码依赖于存在一个与目标模型具有相似采样分布的小型模型，例如LLaMA-2 2B用于LLaMA-2 70B，这通常是不存在的。即使这种模型可用，如果接受率低，较小的草稿模型仍然可能过于昂贵。相反，将草稿模型嵌入到主模型中可能有所帮助，例如通过向基础模型的后几层添加一个专门的草稿头<参考文献
    key="eagle"><参考文献 key="medusa"><参考文献 key="DeepSeek3">。因为这个头与主模型共享大多数参数，所以运行速度更快，并且更接近采样分布。</参考文献></参考文献></参考文献>
- en: For normal autoregressive sampling the token/s is the same as the step time.
    We are still beholden to the theoretical minimum step time according to the Arithmetic
    Intensity section here (in fact, Speculative Sampling step times are usually quite
    a bit slower than normal autoregressive sampling, but because we get more than
    1 token out per step on average we can get much better tokens/s).
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 对于正常的自回归采样，标记/步与步时间相同。我们仍然受制于这里算术强度部分的理论最小步时间（实际上，推测性采样的步时间通常比正常的自回归采样慢得多，但因为我们平均每步可以得到超过1个标记，我们可以得到更好的标记/秒）。
- en: <picture>![](../Images/a191201792c3a62420edbb823dd16d51.png)</picture>
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: <picture>![](../Images/a191201792c3a62420edbb823dd16d51.png)</picture>
- en: '**Figure:** this figure shows the per-step latency and speculation success
    rate for Chinchilla (a 70B model from DeepMind) with a 4B parameter drafter (small
    model). For XSum (a natural language dataset), the ideal amount of speculation
    is about 3-4 tokens ahead, while HumanEval (a coding dataset) is more predictable
    and sees wins from more aggressive speculation.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '**图示：** 这张图显示了 Chinchilla（DeepMind 的一个 70B 模型）与一个 4B 参数的 drafter（小型模型）的每步延迟和投机成功率。对于
    XSum（一个自然语言数据集），理想的投机量大约是 3-4 个令牌，而 HumanEval（一个编码数据集）则更可预测，并且从更激进的投机中看到收益。'
- en: '**How does this work for non-greedy decoding?** This is a bit more complicated,
    but essentially boils down to a Metropolis-Hastings inspired algorithm where we
    have \(P_{\text{draft model}}(\text{chosen token})\) and \(P_{\text{target model}}(\text{chosen
    token})\) derived from the logits, and reject the chosen token probabilistically
    if the ratio of these probabilities is smaller than some threshold.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '**非贪婪解码是如何工作的？** 这要复杂一些，但本质上归结为一个受 Metropolis-Hastings 启发的算法，其中我们有了从 logits
    推导出的 \(P_{\text{draft model}}(\text{chosen token})\) 和 \(P_{\text{target model}}(\text{chosen
    token})\)，如果这些概率的比率小于某个阈值，则概率性地拒绝所选的令牌。'
- en: These [two](https://arxiv.org/abs/2211.17192) [papers](https://arxiv.org/abs/2302.01318)
    derived this concurrently and have good examples of how this works in practice.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 这两篇[论文](https://arxiv.org/abs/2211.17192) [和这篇](https://arxiv.org/abs/2302.01318)
    同时推导出这一结论，并提供了很好的实际应用示例。
- en: '**Takeaway:** Speculative sampling is yet another powerful lever for trading
    throughput for better per token latency. However, in the scenario where batch
    size is limited (e.g. small hardware footprint or large KV caches), it becomes
    a win-win.</d-article>  <d-appendix><d-footnote-list><d-citation-list>### Miscellaneous'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '**要点总结：** 投机采样是另一种强大的杠杆，可以在吞吐量和每令牌延迟之间进行权衡。然而，在批大小受限的情况下（例如，小硬件占用或大 KV 缓存），它成为一种双赢的局面。</d-article>  <d-appendix><d-footnote-list><d-citation-list>###
    杂项'
- en: ^*Work done at Google DeepMind, now at MatX.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: ^*在 Google DeepMind 的工作，现在在 MatX。
- en: Citation
  id: totrans-348
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 引用
- en: 'For attribution in academic contexts, please cite this work as:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在学术环境中进行归属时，请引用以下工作：
- en: '[PRE0]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'or as a BibTeX entry:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 或者作为一个 BibTeX 条目：
- en: '[PRE1]</d-citation-list></d-footnote-list></d-appendix> <d-bibliography src="/scaling-book/assets/bibliography/main.bib"></d-bibliography></d-byline>'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE1]</d-citation-list></d-footnote-list></d-appendix> <d-bibliography src="/scaling-book/assets/bibliography/main.bib"></d-bibliography></d-byline>'
