<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><h2 class="h2" id="ch06"><span class="ash">6</span></h2>
<h2 class="h2a">Customizing Embeddings and Model Architectures</h2>
<h3 class="h3" id="ch06lev1sec1">Introduction</h3>
<p>Two full chapters of prompt engineering equipped us with the knowledge of how to effectively interact with (prompt) LLMs, acknowledging their immense potential as well as their limitations and biases. We have also fine-tuned models both open and closed source to expand on an LLM’s pre-training to better solve our own specific tasks. We have even seen a full case study of how semantic search and embedding spaces can help us retrieve relevant information from a dataset with speed and ease.</p>
<p>To further broaden our horizons, we will utilize lessons learned from earlier chapters and dive into the world of fine-tuning embedding models and customizing pre-trained LLM architectures to unlock even greater potential in our LLM implementations. By refining the very foundations of these models, we can cater to specific business use cases and foster improved performance.</p>
<p>Foundation models, while impressive on their own, can be adapted and optimized to suit a variety of tasks through minor to major tweaks in their architectures. This customization enables us to address unique challenges and tailor LLMs to specific business requirements. The underlying embeddings form the basis for these customizations, as they are responsible for capturing the semantic relationships between data points and can significantly impact the success of various tasks.</p>
<p>Recalling our semantic search example, we identified that the original embeddings from OpenAI were designed to preserve semantic similarity, but the bi-encoder was further tuned to cater to asymmetric semantic search, matching short queries with longer passages. In this chapter, we will expand upon this concept, exploring techniques to train a bi-encoder that can effectively capture other business use cases. By doing so, we will uncover the potential of customizing embeddings and model architectures to create even more powerful and versatile LLM applications.</p>
<h3 class="h3" id="ch06lev1sec2">Case Study – Building a Recommendation System</h3>
<p>The majority of this chapter will explore the role of embeddings and model architectures in designing a recommendation engine using a real-world dataset as our case study. Our objective is to highlight the importance of customizing embeddings and model architectures in achieving better performance and results tailored to specific use cases.</p>
<h4 class="h4" id="ch06lev2sec1">Setting Up the Problem and the Data</h4>
<p>To demonstrate the power of customized embeddings, we will be using the MyAnimeList 2020 dataset, which can be accessed on Kaggle at the following link: MyAnimeList 2020 Dataset. This dataset contains information about anime titles, ratings (from 1-10), and user preferences, offering a rich source of data to build a recommendation engine. <a href="ch06.html#ch06fig01">Figure 6.1</a> shows a snippet of the dataset on the Kaggle page:</p>
<div class="group">
<div class="image" id="ch06fig01"><img src="graphics/06fig01.jpg" alt="Images" width="978" height="717"/></div>
<p class="fig-caption"><strong>Figure 6.1</strong> <em>The MyAnimeList database is one of the largest datasets we have worked with to date. It can be found on Kaggle and it has tens of millions of rows of ratings and thousands of anime titles complete with dense text features describing each anime title.</em></p>
</div>
<p>To ensure a fair evaluation of our recommendation engine, we will divide the dataset into separate training and testing sets. This process allows us to train our model on one portion of the data and evaluate its performance on a separate, unseen portion, thereby providing an unbiased assessment of its effectiveness. <a href="ch06.html#list6_1">Listing 6.1</a> shows a snippet of our code to load the anime titles and split them into a train and test split.</p>
<p class="ex-caption" id="list6_1"><strong>Listing 6.1</strong> <em>Loading and splitting our anime data</em></p>
<div class="pre-box">
<pre><code># Load the anime titles with genres, synopsis, producers, etc
# there are 16,206 titles
pre_merged_anime = pd.read_csv('../data/anime/pre_merged_anime.csv')

# Load the ratings given by users who have **completed** an anime
# there are 57,633,278 ratings!
rating_complete = pd.read_csv('../data/anime/rating_complete.csv')

import numpy as np

# split the ratings into an 90/10 train/test split
rating_complete_train, rating_complete_test = \
              np.split(rating_complete.sample(frac=1, random_state=42), 
                       [int(.9*len(rating_complete))])</code></pre>
</div>
<p>With our data loaded up and split, let’s take some time to better define what we are actually trying to solve.</p>
<h4 class="h4" id="ch06lev2sec2">Defining the Problem of Recommendation</h4>
<p>Developing an effective recommendation system is, to put it mildly, a complex task. Human behavior and preferences can be intricate and difficult to predict (understatement of the millennium). The challenge lies in understanding and predicting what users will find appealing or interesting, which is influenced by a multitude of factors.</p>
<p>Recommendation systems need to take into account both user features and item features to generate personalized suggestions. User features can include demographic information like age, browsing history, and past item interactions (which will be the focus of our work in this chapter), while item features can encompass characteristics like genre, price, or popularity. However, these factors alone may not paint the complete picture, as human mood and context also play a significant role in shaping preferences. For instance, a user's interest in a particular item might change depending on their current emotional state or the time of day.</p>
<p>Striking the right balance between exploration and pattern exploitation is also important in recommendation systems. By pattern exploitation, I’m referring to a system recommending items that it is confident the user will like based on their past preferences or are just simply similar to things they have interacted with before, while exploration involves suggesting items that the user might not have considered before. Striking this balance ensures that users continue to discover new content while still receiving recommendations that align with their interests. We will consider both of these factors.</p>
<p>Defining the problem of recommendation is a multifaceted challenge that requires considering various factors such as user and item features, human mood, the number of recommendations to optimize, and the balance between exploration and exploitation. Given all of this, let’s dive in!</p>
<h5 class="h5" id="ch06lev3sec1">Content versus Collaborative Recommendations</h5>
<p>Recommendation engines can be broadly categorized into two main approaches: content-based and collaborative filtering. <strong>Content-based recommendations</strong> focus on the attributes of the items being recommended, utilizing item features to suggest similar content to users based on their past interactions. In contrast, <strong>collaborative filtering</strong> capitalizes on the preferences and behavior of users, generating recommendations by identifying patterns among users with similar interests or tastes.</p>
<p>In content-based recommendations, the system extracts relevant features from items, such as genre, keywords, or themes, to build a profile for each user. This profile helps the system understand the user's preferences and suggest items with similar characteristics. For instance, if a user has previously enjoyed action-packed anime titles, the content-based recommendation engine would suggest other anime series with similar action elements.</p>
<p>On the other hand, collaborative filtering can be further divided into user-based and item-based approaches. User-based collaborative filtering finds users with similar preferences and recommends items that those users have liked or interacted with. Item-based collaborative filtering, instead, focuses on finding items that are similar to those the user has previously liked, based on the interactions of other users. In both cases, the underlying principle is to leverage the wisdom of the crowd to make personalized recommendations.</p>
<p>In our case study, we plan to fine-tune a bi-encoder (like the one we saw in <a href="ch02.html#ch02">chapter 2</a>) to generate embeddings for anime features. Our goal is to minimize the cosine similarity loss in such a way that the similarity between embeddings reflects how common it is that users like both animes.</p>
<p>In fine-tuning a bi-encoder our goal is to create a recommendation system that can effectively identify similar anime titles based on the preferences of promoters and <strong>not</strong> just because they are semantically similar. <a href="ch06.html#ch06fig02">Figure 6.2</a> shows what this might look like. The resulting embeddings will enable our model to make recommendations that are more likely to align with the tastes of users who are enthusiastic about the content.</p>
<div class="group">
<div class="image" id="ch06fig02"><img src="graphics/06fig02.jpg" alt="Images" width="498" height="555"/></div>
<p class="fig-caption"><strong>Figure 6.2</strong> <em>Embedders are generally pre-trained to place embedded data near each other if they are semantically similar. In our case, we want an embedder that places embedded data near each other if they are similar in terms of <strong>user-preferences</strong>.</em></p>
</div>
<p>In terms of recommendation techniques, our approach combines elements of both content-based and collaborative recommendations. We leverage content-based aspects by using the features of each anime as input to the bi-encoder. At the same time, we incorporate collaborative filtering by considering the Jaccard score of promoters, which is based on the preferences and behavior of users. This hybrid approach allows us to take advantage of the strengths of both techniques to create a more effective recommendation system.</p>
<p>I got lost here tracking exactly what we’re trying to do. Maybe explaining how we’re going to construct this embedder, and how it will combine collaborative filtering and semantic similarity would be helpful. I realized later that we’re trying this model on the collaborative filtering as a label.</p>
<p>To summarize, our plan is to:</p>
<p class="numbera">1. Define/construct a text embedding model, either using them as is, or fine-tuning them on user-preference data</p>
<p class="numbera">2. Define a hybrid approach of collaborative filtering (using the jaccard score to define user/anime similarities) and content filtering (semantic similarity of anime titles by way of descriptions, etc) that will influence our user-preference data structure as well as how we score recommendations given to us by the pipeline</p>
<p class="numbera">3. Fine-tune open-source LLMs on a training set of user-preference data</p>
<p class="numbera">4. Run our system on a testing set of user preference data to decide which embedder was responsible for the best anime title recommendations</p>
<h4 class="h4" id="ch06lev2sec3">A 10,000 Foot View of Our Recommendation System</h4>
<p>Our recommendation process will generate personalized anime recommendations for a given user based on their past ratings. Here's an explanation of the steps in our recommendation engine:</p>
<p class="numbera">1. <strong>Input</strong>: The input for the recommendation engine is a user ID and an integer k (example 3).</p>
<p class="numbera">2. <strong>Identify highly-rated animes</strong>: For each anime title that the user has rated as a 9 or 10 (a promoting score on the NPS scale), identify k other relevant animes by finding nearest matches in the anime’s embedding space. From these, we consider both how often an anime was recommended and how high the resulting cosine score was in the embedding space to take the top k results for the user. <a href="ch06.html#ch06fig03">Figure 6.3</a> outlines this process. The pseudo code would look like:</p>
<div class="group">
<div class="image" id="ch06fig03"><img src="graphics/06fig03.jpg" alt="Images" width="1006" height="544"/></div>
<p class="fig-caption"><strong>Figure 6.3</strong> <em>Step 2 takes in the user and finds k animes <strong>for each</strong> user-promoted (gave a score of 9 or 10) anime. For example if the user promoted 4 animes (6345, 4245, 249, 120) and we set k=3, the system will first retrieve 12 semantically similar animes (3 per promoted animes with duplicates allowed) and then de-duplicate any animes that came up multiple times by weighing it slightly more than the original cosine scores. We then take the top k unique recommended anime titles considering both cosine scores to promoted animes and how often occurred in the original list of 12.</em></p>
</div>
<pre class="pre1"><code>given: user, k=3
promoted_animes = all anime titles that the user gave a score of 9 or a 10

relevant_animes = []
for each promoted_anime in promoted_animes:
    add k animes to relevant_animes with the highest cosine similiarty to promoted_anime along with the cosine score


# relevant_animes should now have k * (however many animes were in promoted_animes)

# Calculate a weighted score of each unique relevant anime given how many times it appears in the list and it’s similarity to promoted animes

final_relevant_animes = the top k animes with the highest weighted cosine/occurrence score</code></pre>
<p>The github has the full code to run this step with examples too! For example, given k=3 and user id <code>205282</code>, the result of step two would result in the following dictionary where each key represents a different embedding model used and the values are anime title ids and corresponding cosine similarity scores to promoted titles the user liked:</p>
<pre class="pre"><code>final_relevant_animes = {
  'text-embedding-ada-002': { '6351': 0.921, '1723': 0.908, '2167': 0.905 },
  'paraphrase-distilroberta-base-v1': { '17835': 0.594, '33970': 0.589,  '1723': 0.586 }
}</code></pre>
<p class="numbera">3. <strong>Score relevant animes</strong>: For each of the relevant animes identified in the previous step, if the anime is not present in the testing set for that user, ignore it. If we have a user rating for the anime in the testing set, we assign a score to the recommended anime given the NPS-inspired rules:</p>
<p class="number-bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> If the rating in the testing set for the user and the recommended anime was 9 or 10, the anime is considered a “Promoter” and the system receives +1 points.</p>
<p class="number-bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> If the rating is 7 or 8, the anime is considered “Passive” and receives 0 points.</p>
<p class="number-bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> If the rating is between 1 and 6, the anime is considered a “Detractor” and receives -1 point.</p>
<p>The final output of this recommendation engine is a ranked list of top N (depending on how many we wish to show the user) animes that are most likely to be enjoyed by the user and a score of how well the system did given a testing ground truth set. <a href="ch06.html#ch06fig04">Figure 6.4</a> shows this entire process at a high level.</p>
<div class="group">
<div class="image" id="ch06fig04"><img src="graphics/06fig04.jpg" alt="Images" width="1034" height="432"/></div>
<p class="fig-caption"><strong>Figure 6.4</strong> <em>The overall recommendation process involves using an embedder to retrieve similar animes from a user’s already promoted titles. It then assigns a score to the recommendations given if they were present in the testing set of ratings.</em></p>
</div>
<h4 class="h4" id="ch06lev2sec4">Generating a custom description field to compare items</h4>
<p>To compare different anime titles and generate recommendations more effectively, we will create our own custom generated description field that incorporates several relevant features from the dataset (Shown in <a href="ch06.html#ch06fig05">Figure 6.5</a>). This approach offers several advantages and enables us to capture a more comprehensive context of each anime title, resulting in a richer and more nuanced representation of the content.</p>
<div class="group">
<div class="image" id="ch06fig05"><img src="graphics/06fig05.jpg" alt="Images" width="419" height="572"/></div>
<p class="fig-caption"><strong>Figure 6.5</strong> <em>Our custom generated description of each anime combines many raw features including the title, genre list, synopsis, producers, and more. This approach can be contrary to how many developers think because instead of generating a structured, tabular dataset, we are deliberately creating natural text representation of our anime titles and we will let our LLM-based embedders capture that in a vector (tabular) form.</em></p>
</div>
<p>By combining multiple features, such as plot summaries, character descriptions, and genres, we can create a multidimensional representation of each anime title which allows our model to consider a broader range of information when comparing titles and identifying similarities, leading to more accurate and meaningful recommendations.</p>
<p>Incorporating various features from the dataset into a single description field can also aid in overcoming potential limitations in the dataset, such as missing or incomplete data. By leveraging the collective strength of multiple features, we ensure that our model has access to a more robust and diverse set of information and mitigates the effect of individual titles missing pieces of information.</p>
<p>In addition, using a custom generated description field enables our model to adapt to different user preferences more effectively. Some users may prioritize plot elements, while others may be more interested in genres or mediums (TV series vs movies). By capturing a wide range of features in our description field, we can cater to a diverse set of user preferences and deliver personalized recommendations that align with individual tastes.</p>
<p>Overall, This approach of creating our own custom description field from several individual fields ultimately should result in a recommendation engine that delivers more accurate and relevant content suggestions. <a href="ch06.html#list6_2">Listing 6.2</a> provides a snippet of the code used to generate these descriptions.</p>
<p class="ex-caption" id="list6_2"><strong>Listing 6.2</strong> <em>Generating custom descriptions from multiple anime fields</em></p>
<div class="pre-box">
<pre><code>def clean_text(text):
    # Remove non-printable characters
    text = ''.join(filter(lambda x: x in string.printable, text))
    # Replace multiple whitespace characters with a single space
    text = re.sub(r'\s{2,}', ' ', text).strip()
    return text.strip()


def get_anime_description(anime_row):
    """
    Generates a custom description for an anime title based on various features from the input data.

    :param anime_row: A row from the MyAnimeList dataset containing relevant anime information.
    :return: A formatted string containing a custom description of the anime.
    """

...
    description = (
        f"{anime_row['Name']} is a {anime_type}.\n"
... #  NOTE I omitting over a dozen other rows here for brevity
        f"Its genres are {anime_row['Genres']}\n"
    )
    return clean_text(description)

# Create a new column in our merged anime dataframe for our new descriptions
pre_merged_anime['generated_description'] = pre_merged_anime.apply(get_anime_description, axis=1)</code></pre>
</div>
<h4 class="h4" id="ch06lev2sec5">Setting a Baseline with Foundation Embedders</h4>
<p>Before customizing our embeddings, we will establish a baseline performance using two foundation embedders: OpenAI's powerful Ada-002 embedder and a small open-source bi-encoder based on a distilled RoBERTa model. These pre-trained models offer a starting point for comparison, helping us to quantify the improvements achieved through customization. We will start with these two models and eventually work our way up to comparing four different embedders – 1 closed-sourced and 3 open-sourced.</p>
<h4 class="h4" id="ch06lev2sec6">Preparing our fine-tuning data</h4>
<p>To attempt to create a robust recommendation engine, we will fine-tune open-source embedders using the Sentence Transformers library. We will begin by calculating the Jaccard Similarity between promoted animes from the training set.</p>
<p><strong>Jaccard similarity</strong> is a simple method to measure the similarity between two sets of data based on the number of elements they share. The Jaccard similarity is calculated by dividing the number of elements that both groups have in common by the total number of distinct elements in both groups combined.</p>
<p>Let's say we have two anime shows, Anime A and Anime B.</p>
<p>Suppose we have the following people who like these shows:</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> People who like Anime A: Alice, Bob, Carol, David</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> People who like Anime B: Bob, Carol, Ethan, Frank</p>
<p>To calculate the Jaccard similarity, we first find the people who like both Anime A and Anime B. In this case, it's Bob and Carol.</p>
<p>Next, we find the total number of distinct people who like either Anime A or Anime B. Here, we have Alice, Bob, Carol, David, Ethan, and Frank.</p>
<p>Now, we can calculate the Jaccard similarity by dividing the number of common elements (2, as Bob and Carol like both shows) by the total number of distinct elements (6, as there are 6 unique people in total).</p>
<p class="center">Jaccard similarity = 2/6 = 1/3 ≈ 0.33</p>
<p>So, the Jaccard similarity between Anime A and Anime B, based on the people who like them, is about 0.33 or 33%. This means that 33% of the distinct people who like either show have similar tastes in anime, as they enjoy both Anime A and Anime B. <a href="ch06.html#ch06fig06">Figure 6.6</a> shows another example.</p>
<div class="group">
<div class="image" id="ch06fig06"><img src="graphics/06fig06.jpg" alt="Images" width="984" height="491"/></div>
<p class="fig-caption"><strong>Figure 6.6</strong> <em>To convert our raw ratings into pairs of animes with associated scores, we will consider every pair of anime titles and compute the jaccard score between promoting users.</em></p>
</div>
<p>We will apply this logic to calculate the Jaccard similarity for every pair of animes using a training set of the ratings DataFrame and only keep scores above a certain threshold as “positive examples” (label of 1) and the rest will be considered “negative” (label of 0).</p>
<p>Important Note: We are free to label any anime pairs with a label between -1 and 1 but I am only using 0 and 1 because I’m only using promoting scores to create my data so it’s not fair to say that if the jaccard score between animes is low then the users totally disagree on the anime. That’s not necessarily true! If I expanded this case study I would want to explicitly label animes as -1 if and only if users were genuinely rating them opposite (most users who promote one are detractors of the other).</p>
<p>Once we have jaccard scores for anime ides, we will need to convert them into tuples of anime descriptions and the cosine label (in our case either 0 or 1) and then we are ready to update our open-source embedders and experiment with different token windows (shown in <a href="ch06.html#ch06fig07">Figure 6.7</a>).</p>
<div class="group">
<div class="image" id="ch06fig07"><img src="graphics/06fig07.jpg" alt="Images" width="514" height="945"/></div>
<p class="fig-caption"><strong>Figure 6.7</strong> <em>Jaccard scores are converted into cosine labels and then fed into our bi-encoder so it may attempt to learn patterns between the generated anime descriptions and how users co-like the titles.</em></p>
</div>
<p>Once we have Jaccard similarities between anime pairs, we can convert these to labels for our bi-encoder with a simple rule. In our case, if the score is above 0.3, then we label the pair as positive (label 1) and if the label is &lt; 0.1, we label is as “negative” (label 0).</p>
<h5 class="h5" id="ch06lev3sec2">Adjusting Model Architectures</h5>
<p>When working with open-source embedders, we have so much more flexibility to change things around if we need to. For example, the open source model I want to use was pre-trained with the ability to only take in 128 tokens at a time and truncate anything longer than that. <a href="ch06.html#ch06fig08">Figure 6.8</a> shows the histogram of the token lengths for our generated anime descriptions which clearly shows that we have many descriptions that are over 128 tokens, some in the 600 range!</p>
<div class="group">
<div class="image" id="ch06fig08"><img src="graphics/06fig08.jpg" alt="Images" width="954" height="603"/></div>
<p class="fig-caption"><strong>Figure 6.8</strong> <em>We have several animes that, after tokenizing, are hundreds of tokens long, some have over 600 tokens.</em></p>
</div>
<p>In <a href="ch06.html#list6_3">Listing 6.3</a>, we change the input sequence length to be 384 instead of 128.</p>
<p class="ex-caption" id="list6_3"><strong>Listing 6.3</strong> <em>Modifying an open-source bi-encoder’s max sequence length</em></p>
<div class="pre-box">
<pre><code>from sentence_transformers import SentenceTransformer

# Load a pre-trained SBERT model
model = SentenceTransformer('paraphrase-distilroberta-base-v1')
model.max_seq_length = 384     # Truncate long documents to 384 tokens
model</code></pre>
</div>
<p>Why 384? Well:</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> The histogram of token lengths shows that 384 would capture most of our animes in their entirely and would truncate the rest</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> 384 = 256 + 128, the sum of 2 binary numbers and we like binary numbers -Modern hardware components, especially GPUs, are designed to perform optimally with binary numbers so they can split up workloads evenly.</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> Why not 512 then to capture more training data? I still want to be conservative here. The more I increase this max token window size, the more data I would need to train the system because we are adding parameters to our model and therefore there is more to learn. It will also take more time and compute resources to load, run, and update the larger model.</p>
<p class="bullet-sub">• For what it’s worth, I did initially try this process with an embedding size of 512 and got worse results while taking about 20% longer on my machine.</p>
<p>To be explicit, anytime we alter an original pre-trained foundation model in any capacity the model must learn something from scratch. In this case, the model will learn, from scratch, how text longer than 128 tokens can be formatted and how to assign attention scores across a longer text span. It can be difficult to make these model architecture adjustments but often well worth the effort in terms of performance! In our case, changing the max input length to 384 is only the starting line because this model now has to learn about text longer than 128 tokens.</p>
<p>With modified bi-encoder architectures, data prepped and ready to go, we are ready to fine-tune!</p>
<h4 class="h4" id="ch06lev2sec7">Fine-tuning Open-Source Embedders Using Sentence Transformers</h4>
<p>It’s time to fine-tune our open-source embedders using Sentence Transformers. Sentence Transformers as a reminder is a library built on top of the Hugging Face Transformers library.</p>
<p>We create a custom training loop using the Sentence Transformers library shown in <a href="ch06.html#list6_4">Listing 6.4</a>. We use the provided training and evaluation functionalities of the library, such as the 'fit()' method for training and the 'evaluate()' method for validation.</p>
<p>Before we begin the fine-tuning process, we need to decide on several hyperparameters, such as learning rate, batch size, and the number of training epochs. I have experimented with various hyperparameter settings to find a good combination that leads to optimal model performance. I will dedicate all of <a href="ch08.html#ch08">chapter 8</a> to discussing dozens of open-source fine-tuning hyper parameters so if you are looking for a deeper discussion on how I came to these numbers, please refer to <a href="ch08.html#ch08">Chapter 8</a>.</p>
<p>We gauge how well the model learned by checking the change in the cosine similarity which jumped up to the high .8, .9s! That’s great.</p>
<p class="ex-caption" id="list6_4"><strong>Listing 6.4</strong> <em>Fine-tuning a bi-encoder</em></p>
<div class="pre-box">
<pre><code># Create a DataLoader for the examples
train_dataloader = DataLoader(
    train_examples,
    batch_size=16,
    shuffle=True
)

...

# Create a DataLoader for the validation examples
val_dataloader = DataLoader(
    all_examples_val,
    batch_size=16,
    shuffle=True
)

# Use the CosineSimilarityLoss from Sentence Transformers
loss = losses.CosineSimilarityLoss(model=model)

# Set the number of epochs for training
num_epochs = 5

# Calculate warmup steps using 10% of the training data
warmup_steps = int(len(train_dataloader) * num_epochs * 0.1)

# Create the evaluator using validation data
evaluator = evaluation.EmbeddingSimilarityEvaluator(
    val_sentences1,  # List of first anime descriptions in each pair from validation data
    val_sentences2,  # List of second anime descriptions in each pair from validation data
    val_scores       # List of corresponding cosine similarity labels for validation data
)

# get initial metrics
model.evaluate(evaluator)  # initial embedding similarity score: <strong>0.0202</strong>


# Configure the training process
model.fit(
    # Set the training objective with the train dataloader and loss function
    train_objectives=[(train_dataloader, loss)],
    epochs=num_epochs,  # set the number of epochs
    warmup_steps=warmup_steps,  # Set the warmup steps
    evaluator=evaluator,  # Set the evaluator for validation during training
    output_path="anime_encoder"  # Set the output path for saving the fine-tuned model
)

# get final metrics
model.evaluate(evaluator)  # final embedding similarity score:   <strong>0.8628</strong></code></pre>
</div>
<p>With our fine-tuned bi-encoder, we can generate embeddings for new anime descriptions and compare them with the embeddings of our existing anime database. By calculating the cosine similarity between the embeddings, we can recommend animes that are most similar to the user's preferences.</p>
<p>It’s worth noting that once we go through the process of fine-tuning a single custom embedder using our user preference data, we can then pretty easily swap out different models with similar architectures and run the same code, rapidly expanding our universe of embedder options. For this case study, I also fine-tuned another LLM called <code>all-mpnet-base-v2</code> which (at the time of writing) is regarded as a very good open-source embedder for semantic search and clustering purposes. It is a bi-encoder as well so we can simply swap out references to our Roberta model with mpnet and change virtually no code (see the github for the complete case study).</p>
<h4 class="h4" id="ch06lev2sec8">Summary of Results</h4>
<p>In the course of this case study we:</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> Generated a custom anime description field using several raw fields from the original dataset</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> Created training data for a bi-encoder from user-anime ratings using a combination of NPS/Jaccard scoring and our generated descriptions</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> Modified an open source architecture to accept a larger token window to account for our longer description field.</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> Fine-tuned two bi-encoders with our training data to attempt to create a model that mapped our descriptions to an embedding space more aligned to our user’s preferences</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> Defined an evaluation system using NPS-scoring to reward a promoted recommendation (the user giving it a 9 or a 10 in the testing) and punishing detracted titles (users giving it a 1-6 in the testing set)</p>
<p>We had four candidates for our embedders:</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> <code><strong>text-embedding-002</strong></code> - OpenAI’s recommended embedder for all use-cases, mostly optimized for semantic similarity</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> <code><strong>paraphrase-distilroberta-base-v1</strong></code> - An open source model pre-trained to summarize short pieces of text</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> <code><strong>anime_encoder</strong></code> - The distilroberta model with a modified 384 token window and fine-tuned on our user preference data</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> <code><strong>anime_encoder_bigger</strong></code> - A larger open source model (<code>all-mpnet-base-v2</code>) that was pre-trained with a token window size of 512 which I further fine-tuned on our user preference data, same as <code>anime_encoder</code>.</p>
<p><a href="ch06.html#ch06fig09">Figure 6.9</a> shows our final results for our four embedder candidates across lengthening recommendation windows (how many recommendations we show the user).</p>
<div class="group">
<div class="image" id="ch06fig09"><img src="graphics/06fig09.jpg" alt="Images" width="1025" height="554"/></div>
<p class="fig-caption"><strong>Figure 6.9</strong> <em>Our larger open-source model (anime_encoder_bigger) consistently outperforms OpenAI’s embedder in recommending anime titles to our users based on historical preferences.</em></p>
</div>
<p>Each tick on the x axis here represents showing the user a list of that many anime titles and the y axis is a aggregated score for the embedder using the scoring system outlined before where we also further reward the model if a correct recommendation was placed closer to the front of the list and likeways punish it more if it recommends something that the user is a detractor for closer to the beginning of the list.</p>
<p>Some interesting takeaways:</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> The best performing model is our larger fine-tuned model and it consistently outperforms OpenAI’s embedder!</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> The fine-tuned distilroberta model (anime_encoder) underperforms it’s pre-trained cousin who can only take in 128 tokens at a time. This is most likely because:</p>
<p class="bullet-sub"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> The model doesn’t have enough parameters in it’s attention layersto capture the recommendation problem well and its non fine-tuned cousin is simply relying on recommending semantically similar titles.</p>
<p class="bullet-sub"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> The model might require more than 384 tokens to capture all possible relationships.</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> All models start to degrade in performance as it is expected to recommend more and more titles which is fair. The more titles anything recommends, the less confident it will be as it goes down the list.</p>
<h5 class="h5" id="ch06lev3sec3">Exploring Exploration</h5>
<p>Earlier I mentioned that a recommendation system’s level of “exploration” can be defined as how often it recommends something that the user may not have watched before. We didn’t take any explicit measures to try and encourage exploration but it is still worth seeing how our embedders stack up. <a href="ch06.html#ch06fig10">Figure 6.10</a> shows a graph of the raw number of animes recommended to all of the users in our test dataset.</p>
<div class="group">
<div class="image" id="ch06fig10"><img src="graphics/06fig10.jpg" alt="Images" width="892" height="719"/></div>
<p class="fig-caption"><strong>Figure 6.10</strong> <em>Comparing how many unique animes were recommended during the course of the testing process.</em></p>
</div>
<p>OpenAI’s ada and our bigger encoder gave out more recommendations than the two other options but OpenAI clearly seems to be in the lead of diversity of unique animes recommended. This could be a sign (not proof) that our users are not that explorative and tend to gravitate towards the same animes and our fine-tuned bi-encoder is picking up on that and delivering fewer unique results. It could also simply be that the OpenAI ada embedder was trained on such a diverse set of data and is so large in terms of parameters that it is simply better than our fine-tuned model at delivering consistently favored animes at scale.</p>
<p>To answer these questions and more, we would want to continue our research by, for example we could:</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> Try new open sourced models and closed sourced models</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> Design new metrics for quality assurance to test our embedders on a more holistic scale</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> Calculate new training datasets that use other metrics like correlation coefficients instead of jaccard</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> Toggle recommendation system hyper parameters like K. We only ever considered grabbing the first K=3 animes for each promoted anime but what if we let that number vary as well?</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> Run some pre-training on blogs and wikis about anime recommendation and theory so the model has some latent access to information about how to consider recommendations</p>
<p>Honestly that last one is a bit pie in the sky and would really work best if we could also combine that with some chain of thought prompting on a different LLM but still, this is a big question and sometimes that means we need big ideas and big answers. So I leave it to you now; go have big ideas!</p>
<h3 class="h3" id="ch06lev1sec3">Conclusion</h3>
<p>In this chapter, I showed you the process of fine-tuning open-source embedding models for a specific use case, which in our case, was generating high-quality anime recommendations based on users' historical preferences. By comparing the performance of our customized models with that of OpenAI's embedder, we observed that a fine-tuned model could consistently outperform OpenAI's embedder.</p>
<p>Customizing embedding models and their architectures for specialized tasks can lead to improved performance and provide a viable alternative to closed-source models, especially when access to labeled data and resources for experimentation is available.</p>
<p>I hope that the success of our fine-tuned model in recommending anime titles serves as a testament to the power and flexibility that open-source models offer, paving the way for further exploration, experimentation, and application in whatever tasks you might have.</p>
</div>
</div>
</body></html>