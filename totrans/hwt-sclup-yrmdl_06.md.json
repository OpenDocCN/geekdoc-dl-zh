["```py\nbatch_size = 32\nd_model = 128\nd_ff = 4 * d_model\n\nnum_layers = len(jax.devices())\n\nkey = jax.random.PRNGKey(0)\n\n# Pretend each layer is just a single matmul. x = jax.random.normal(key, (batch_size, d_model))\nweights = jax.random.normal(key, (num_layers, d_model, d_model))\n\ndef layer_fn(x, weight):\n  return x @ weight\n\n# Assume we have num_layers == num_pipeline_stages intermediates = [x]\nfor i in range(num_layers):\n  x = layer_fn(x, weights[i])\n  intermediates.append(x)\n\n  if i != num_layers - 1:\n    x = jax.device_put(x, jax.devices()[i+1])\n\ndef loss_fn(batch):\n  return jnp.mean(batch ** 2)  # make up some fake loss function \nloss, dx = jax.value_and_grad(loss_fn)(x)\n\nfor i in range(0, num_layers, -1):\n  _, f_vjp = jax.vjp(layer_fn, intermediates[i + 1], weights[i])\n  dx, dw = f_vjp(dx)  # compute the jvp dx @ J(L)(x[i], W[i])\n  weights[i] = weights[i] - 0.01 * dw  # update our weights \n  if i != 0:\n    dx = jax.device_put(dx, jax.devices()[i-1]) \n```", "```py\n Austin et al., \"How to Scale Your Model\", Google DeepMind, online, 2025. \n```", "```py\n @article{scaling-book,\n      title = {How to Scale Your Model},\n      author = {Austin, Jacob and Douglas, Sholto and Frostig, Roy and Levskaya, Anselm and Chen, Charlie and Vikram, Sharad\n      and Lebron, Federico and Choy, Peter and Ramasesh, Vinay and Webson, Albert and Pope, Reiner},\n      publisher = {Google DeepMind},\n      howpublished = {Online},\n      note = {Retrieved from https://jax-ml.github.io/scaling-book/},\n      year = {2025}\n    } \n```"]