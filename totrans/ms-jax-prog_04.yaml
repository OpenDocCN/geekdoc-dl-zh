- en: 'Part 2: Deep Learning with Jax'
  id: totrans-0
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: 第二部分：使用Jax进行深度学习
- en: '* * *'
  id: totrans-1
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '* * *'
- en: Welcome to the second part of our Jax journey, where we'll embark on the exciting
    realm of deep learning. This part will equip you with the knowledge and tools
    to construct deep learning models using Jax's functional programming paradigm,
    automatic differentiation capabilities, and optimizers.
  id: totrans-2
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 欢迎来到我们Jax之旅的第二部分，我们将踏上深度学习的激动人心领域。本部分将为您提供使用Jax的函数式编程范式、自动微分能力和优化器构建深度学习模型的知识和工具。
- en: We'll begin by delving into the fundamentals of neural networks, grasping the
    concepts of activation functions, backpropagation, optimization, and diverse network
    architectures. This foundation will pave the way for crafting your own neural
    networks using Jax.
  id: totrans-3
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 我们将从深入探讨神经网络的基础开始，理解激活函数、反向传播、优化方法和多样的网络架构概念。这一基础将为您使用Jax构建自己的神经网络铺平道路。
- en: Next, we'll explore regularization techniques, such as dropout, batch normalization,
    and early stopping, to combat overfitting and enhance the generalization performance
    of your models. Additionally, we'll delve into hyperparameter tuning to optimize
    your models' performance.
  id: totrans-4
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨正则化技术，例如dropout、批标准化和早停，以应对过拟合并提升模型的泛化性能。此外，我们将深入研究超参数调优，以优化模型的性能。
- en: With these essential concepts firmly grasped, we'll venture into the practical
    applications of deep learning, building image classification models, natural language
    processing models for text classification and sentiment analysis, and generative
    models.
  id: totrans-5
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 扎实掌握这些基本概念后，我们将进入深度学习的实际应用领域，构建图像分类模型、文本分类和情感分析的自然语言处理模型，以及生成模型。
- en: Let's go!!
  id: totrans-6
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 让我们开始吧！
