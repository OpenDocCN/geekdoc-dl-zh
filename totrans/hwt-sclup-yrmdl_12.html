<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Conclusions and Further Reading</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Conclusions and Further Reading</h1>
<blockquote>原文：<a href="https://jax-ml.github.io/scaling-book/conclusion">https://jax-ml.github.io/scaling-book/conclusion</a></blockquote> <d-title>  <p>Part 11 of <a href="/scaling-book">How To Scale Your Model</a> (<a href="../jax-stuff">Part 10: JAX</a> | <a href="../gpus">Part 12: GPUs</a>)</p> <p>Thank you for reading! Here we'll include a few more references for further study.</p> </d-title> <d-byline/> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#acknowledgments">Acknowledgments</a> </div> <div> <a href="#further-reading">Further Reading</a> </div> <div> <a href="#feedback">Feedback</a> </div> </nav> </d-contents> <p><strong>Thank you for reading this set of essays and congratulations on making it all the way to the end.</strong> Before we conclude, a few acknowledgments:</p> <h2 id="acknowledgments">Acknowledgments</h2> <p>This document represents a significant collective investment from many people at Google DeepMind, who we’d like to briefly acknowledge!</p> <ul> <li>James Bradbury, Reiner Pope, and Blake Hechtman originally derived many of the ideas in this manuscript, and were early to understanding the systems view of the Transformer.</li> <li>Sholto Douglas wrote the first version of this doc and is responsible for kicking off the project. He is more than anyone responsible for the overall narrative of this doc.</li> <li>Jacob Austin led the work of transforming this first version from rough notes into a more polished and comprehensive artifact. He did much of the work of editing, formatting, and releasing this document, and coordinated contributions from other authors.</li> <li>Most of the figures and animations were made by Anselm Levskaya and Charlie Chen.</li> <li>Charlie Chen wrote the inference section and drew many of the inference figures.</li> <li>Roy Frostig helped with publication, editing, and many other steps of the journey.</li> </ul> <p>We’d also like to thank many others gave critical feedback throughout the process, in particular Zak Stone, Nikhil Sethi, Caitlin Stanton, Alex Dimitriev, Sridhar Lakshmanamurthy, Albert Magyar, Diwakar Gupta, Jeff Dean, Corry Wang, Matt Johnson, Peter Hawkins, and many others. Thanks to Ruiqi Gao for help with the HTML formatting.</p> <p><strong>Thank you all!</strong></p> <p class="announce">Before you go, you might also enjoy reading the new <a href="../gpus">Section 12</a> on NVIDIA GPUs!</p> <h2 id="further-reading">Further Reading</h2> <p>There is a bunch of related writing, including the following:</p> <ul> <li> <a href="https://henryhmko.github.io/posts/tpu/tpu.html" rel="external nofollow noopener" target="_blank"><strong>TPU Deep Dive</strong></a>: a wonderful in-depth look at the TPU architecture in the spirit of this book.</li> <li> <a href="https://fleetwood.dev/posts/domain-specific-architectures" rel="external nofollow noopener" target="_blank"><strong>Domain specific architectures for AI inference</strong></a>: a hardware and model deep dive in the spirit of this book.</li> <li> <a href="https://dl.acm.org/doi/pdf/10.1145/3360307" rel="external nofollow noopener" target="_blank"><strong>A Domain-Specific Supercomputer for Training Deep Neural Networks</strong></a>: one of the OG TPU paper, this has a lot of great details about the Google TPU program not covered here.</li> <li> <a href="https://horace.io/brrr_intro.html" rel="external nofollow noopener" target="_blank"><strong>Making Deep Learning Go Brrrr From First Principles</strong></a>: a more GPU and PyTorch-focused tutorial on LLM rooflines and performance engineering.</li> <li> <a href="https://jax.readthedocs.io/en/latest/pallas/tpu/details.html" rel="external nofollow noopener" target="_blank"><strong>Writing TPU Kernels with Pallas</strong></a>: increasingly, TPU programming involves writing custom kernels in Pallas. This series discusses how to write kernels and many lower level TPU details that aren’t mentioned here.</li> <li> <a href="https://siboehm.com/articles/22/CUDA-MMM" rel="external nofollow noopener" target="_blank"><strong>How to Optimize a CUDA Matmul Kernel for cuBLAS-like Performance: a Worklog</strong></a>: while GPU and CUDA specific, this is an excellent blog post showing how to optimize a matmul kernel in CUDA. This might be a good deep dive into how TPUs and GPUs are different.</li> <li> <a href="https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html" rel="external nofollow noopener" target="_blank"><strong>Distributed arrays and automatic parallelization</strong></a>: this is a really nice guide to parallelism APIs in JAX and is a good way to learn how to actually implement some of the ideas we’ve discussed here.</li> <li> <a href="https://github.com/rwitten/HighPerfLLMs2024" rel="external nofollow noopener" target="_blank"><strong>Rafi Witten’s High Performance LLMs 2024 Class</strong></a>: our former colleague Rafi gave a great course on TPU performance engineering and the slides are all on GitHub. This covers a bunch of things in more depth than we do here.</li> <li> <a href="https://arxiv.org/abs/2211.05102" rel="external nofollow noopener" target="_blank"><strong>[2211.05102] Efficiently Scaling Transformer Inference</strong></a>: a detailed paper on the mathematics of Transformer inference. This is the inspiration for a lot of this document.</li> <li> <a href="https://huggingface.co/spaces/nanotron/ultrascale-playbook" rel="external nofollow noopener" target="_blank"><strong>Huggingface Ultra-Scale Playbook</strong></a>: something of a GPU analog to this book, this talks more at depth about how PyTorch implements parallelism techniques and memory-saving techniques during training.</li> <li> <a href="https://kipp.ly/transformer-inference-arithmetic/" rel="external nofollow noopener" target="_blank"><strong>Transformer Inference Arithmetic</strong></a>: a blog with many of the same ideas as this book and some excellent illustrations.</li> <li> <a href="https://stanford-cs336.github.io/spring2025/index.html#coursework" rel="external nofollow noopener" target="_blank"><strong>Stanford CS336 Slides and Videos</strong></a>: a fantastic Stanford course covering many details of LLM training and serving, with some useful exercises. Assignments 1 and 2 are particularly relevant.</li> <li> <a href="https://github.com/stas00/ml-engineering" rel="external nofollow noopener" target="_blank"><strong>Stas Bekman’s ML Engineering Handbook</strong></a>: a highly practical guide to ML infrastructure, covering topics not addressed in this book like how to negotiate with cloud providers, cluster management, and empirical measurements of GPU throughput.</li> </ul> <p>There remains a lot of room for comprehensive writing in this area, so we hope this manuscript encourages more of it! We also believe that this is a fruitful area to study and research. In many cases, it can be done even without having many hardware accelerators on hand.</p> <h2 id="feedback">Feedback</h2> <p>Please leave comments or questions so that we can improve this further. You can reach our corresponding author, Jacob Austin, at jacobaustin123 [at] gmail [dot] com, or suggest edits by posting issues, pull requests, or discussions <a href="https://github.com/jax-ml/scaling-book" rel="external nofollow noopener" target="_blank">on GitHub</a>.</p> </d-article> <d-appendix> <d-footnote-list/> <d-citation-list/> <div class="base-grid appendix-entry"> <h3 style="grid-column: 0;">Miscellaneous</h3> <p class="author-footnote" style="grid-column: text;"><sup>*</sup>Work done at Google DeepMind, now at MatX.</p> </div> <div class="base-grid appendix-entry"> <h3 style="grid-column: 0;">Citation</h3> <p class="author-footnote">For attribution in academic contexts, please cite this work as:</p> <div class="author-footnote"> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c">Austin et al., "How to Scale Your Model", Google DeepMind, online, 2025.</span>
</code></pre></div></div> </div> <p class="author-footnote">or as a BibTeX entry:</p> <div class="author-footnote"> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="nc">@article</span><span class="p">{</span><span class="nl">scaling-book</span><span class="p">,</span>
      <span class="na">title</span> <span class="p">=</span> <span class="s">{How to Scale Your Model}</span><span class="p">,</span>
      <span class="na">author</span> <span class="p">=</span> <span class="s">{Austin, Jacob and Douglas, Sholto and Frostig, Roy and Levskaya, Anselm and Chen, Charlie and Vikram, Sharad
      and Lebron, Federico and Choy, Peter and Ramasesh, Vinay and Webson, Albert and Pope, Reiner}</span><span class="p">,</span>
      <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Google DeepMind}</span><span class="p">,</span>
      <span class="na">howpublished</span> <span class="p">=</span> <span class="s">{Online}</span><span class="p">,</span>
      <span class="na">note</span> <span class="p">=</span> <span class="s">{Retrieved from https://jax-ml.github.io/scaling-book/}</span><span class="p">,</span>
      <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
    <span class="p">}</span>
</code></pre></div></div> </div> </div> </d-appendix> <d-bibliography src="/scaling-book/assets/bibliography/"/>      
</body>
</html>