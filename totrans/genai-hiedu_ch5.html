<?xml version="1.0" encoding="utf-8"?><!DOCTYPE html []>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xml:lang="en" lang="en">
<head>
<title>5 Developing an AI in Education Policy</title>
<link rel="stylesheet" type="text/css" href="template.css"/></head>
<body epub:type="bodymatter">
<section epub:type="introduction" role="doc-introduction">
<header>
<h1 id="h1"><span epub:type="pagebreak" id="p127" aria-label=" page 127. " role="doc-pagebreak"/><span class="chnum" epub:type="ordinal">5 </span><span class="chtitle">Developing an AI in Education Policy</span></h1>
<p class="doi">DOI: <a href="https://dx.doi.org/10.4324/9781003459026-5" aria-label="D.O.I. link to this document.">10.4324/9781003459026-5</a></p></header>
<blockquote epub:type="epigraph" role="doc-epigraph">
<p>We are at the very beginning of the AI regulation race, eventually every country will have some kind of policy and regulations, but these policies, will be dynamic, we will constantly be adopting, changing.</p>
<p class="byLine"><span epub:type="credit" role="doc-credit">Cecilia KY Chan</span></p></blockquote>
<section aria-labelledby="sec5_1">
<h2 id="sec5_1"><span epub:type="ordinal">5.1 </span>Introduction</h2>
<p>In the Hollywood movie, <i>I, Robot</i>, there are three laws that the robots must obey:</p>
<ul class="simple">
<li><i>First Law: A robot may not injure a human being or, through inaction, allow a human being to come to harm</i>.</li>
<li><i>Second Law: A robot must obey orders given it by human beings, except where such orders would conflict with the First Law</i>.</li>
<li><i>Third Law: A robot must protect its own existence as long as such protection does not conflict with the First or Second Laws</i>.</li></ul>
<p>When discussing the regulations of AI, these Three Laws of Robotics presented in the 2004 sci-fi film may be brought to mind. They serve as foundational ethical principles for robots and are meant to prevent them from causing harm to humans. Although current AI systems are not yet advanced enough to pose direct threats to humans, they have generated a plethora of diverse ethical concerns in education, particularly in areas related to governance, privacy, equity, responsibility, plagiarism, and academic misconduct. This chapter will first explore AI policy development and its implications on the use of Generative Artificial Intelligence (GenAI) tools in society, including any associated considerations needed and challenges raised. It will then provide a literature review on AI policies currently being developed and implemented in different countries around the world, and, subsequently, explore AI policies in education, including the latest UNESCO Guidance for AI in education and research. The chapter will offer practical support and recommendations on how to mitigate the concerns discussed, and provide a step-by-step approach to draft an AI educational policy to ensure the responsible and effective use of GenAI for student learning.</p></section>
<section aria-labelledby="sec5_2">
<h2 id="sec5_2"><span epub:type="ordinal">5.2 </span>Are There Similar Laws for ChatGPT and GenAI?</h2>
<p>ChatGPT, an AI language model developed by the company OpenAI, does not have physical capabilities and does not interact with the real world in the same way as the robots in <i>I, Robot</i>. That said, there is still cause for concern with OpenAI chief executive Sam Altman <span epub:type="pagebreak" id="p128" aria-label=" page 128. " role="doc-pagebreak"/>saying, &#x201C;if this technology goes wrong, it can go quite wrong&#x201D; (<a href="#ref5_2" id="R_ref5_2" epub:type="biblioref" role="doc-biblioref">Baio, 2023</a>). Government oversight will thus play a critical role in mitigating the risks of AI, and below, we explore a number of important principles that both AI companies and countries should and are considering for the development and use of AI systems. These principles also reflect AI&#x2019;s weaknesses. They include:</p>
<ol type="1">
<li>Transparency, Explainability, and Interpretability</li>
<li>Fairness and Bias</li>
<li>Accountability</li>
<li>Safety and Robustness</li>
<li>Privacy and Data Protection</li>
<li>Autonomy and Human Oversight</li>
<li>AI Alignment for Humanity</li></ol>
<section aria-labelledby="sec5_2_1">
<h3 id="sec5_2_1"><span epub:type="ordinal">5.2.1 </span>Transparency, Explainability, and Interpretability</h3>
<blockquote>
<p>Transparency, Explainability, and Interpretability in AI mean that the operations, decisions, and reasoning processes of AI models are clear, open, understandable, and explainable to humans.</p></blockquote>
<p>One of the significant challenges with advanced AI models is that they are &#x201C;black boxes&#x201D;, meaning it can be difficult to understand how AI systems work and why they make the decisions that they do. Transparency, explainability, and interpretability are crucial for users to understand, trust, and effectively manage AI technologies (<a href="#ref5_49" id="R_ref5_49" epub:type="biblioref" role="doc-biblioref">Ribeiro et al., 2016</a>). The latter two concepts respectively refer to having clear and understandable explanations of how AI systems make their decisions or recommendations, and an understanding of the internal workings and logic of AI models (<a href="#ref5_28" id="R_ref5_28" epub:type="biblioref" role="doc-biblioref">Joyce et al., 2023</a>; <a href="#ref5_31" id="R_ref5_31" epub:type="biblioref" role="doc-biblioref">Lawton, 2023</a>). All three of these concepts are especially important in sectors like healthcare, finance, and criminal justice, where the decisions of AI can have profound impacts on individuals&#x2019; lives.</p>
<p>There are a number of ways to increase the transparency, explainability, and interpretability of AI systems. One way is to provide information about how the system was trained and what data was used to train it. Another way is to make it possible for users to inspect the system&#x2019;s code to better understand how it makes decisions. Finally, there are several additional methods that can be used to improve the explainability and interpretability of AI systems, such as by clarifying the underlying factors, data, and reasoning that has led to a system&#x2019;s particular decision or output, or providing counterfactual explanations to demonstrate how the system&#x2019;s decisions would change if different data were used, helping users identify what factors are the most influential in the decision-making process and whether any biases are present (<a href="#ref5_7" id="R_ref5_7" epub:type="biblioref" role="doc-biblioref">Chowdhury, 2023</a>; <a href="#ref5_31" epub:type="biblioref" role="doc-biblioref">Lawton, 2023</a>).</p></section>
<section aria-labelledby="sec5_2_2">
<h3 id="sec5_2_2"><span epub:type="ordinal">5.2.2 </span>Fairness and Bias</h3>
<blockquote>
<p>Fairness and bias in AI mean that AI systems should not discriminate against individuals or groups of people.</p></blockquote>
<p>When Apple co-founder Steve Wozniak discovered that Goldman Sachs&#x2019; Apple Credit Card AI algorithm was biased against his wife, he raised concerns. It seemed that female cardholders received lower Apple Card credit limits simply because they were women (<a href="#ref5_41" id="R_ref5_41" epub:type="biblioref" role="doc-biblioref">Nasiripour &#x0026; Natarajan, 2019</a>). This incident is one example that illustrates the issue of <span epub:type="pagebreak" id="p129" aria-label=" page 129. " role="doc-pagebreak"/>fairness and bias within AI systems. Particularly in those that rely on large datasets, AI systems can perpetuate or even amplify existing biases. It is thus crucial for AI developers to recognise and address these biases to ensure that AI systems are fair and do not discriminate against certain groups. The principle of fairness also underscores the importance of diverse and inclusive teams in AI development, as this can help in identifying and rectifying such prejudices. As AI systems can have a significant impact on people&#x2019;s opportunities and outcomes, it is necessary to ensure that they do not discriminate against or disadvantage certain groups of people.</p>
<p>There are several methods to enhance fairness of and reduce bias in AI systems. One way is to use training data that is representative of the population that the system will be used within. Alternatively, one can design algorithms inherently intended for fairness. Continuous monitoring of AI systems for biases and implementing corrective measures upon detection is equally as essential (<a href="#ref5_16" id="R_ref5_16" epub:type="biblioref" role="doc-biblioref">Ferrara, 2023</a>; <a href="#ref5_36" id="R_ref5_36" epub:type="biblioref" role="doc-biblioref">Mehrabi et al., 2021</a>; <a href="#ref5_54" id="R_ref5_54" epub:type="biblioref" role="doc-biblioref">Silberg &#x0026; Manyika, 2019</a>). Addressing fairness and bias in AI mandates a comprehensive approach rooted in understanding, evaluating, and &#x2013; where necessary &#x2013; rectifying data sources, as historical data often carries societal biases that can permeate AI systems. Ensuring that data collection is diverse, paired with innovative pre-processing, in-processing, and post-processing techniques, will help lay the groundwork for more impartial and better-balanced models. The incorporation of transparency, explainability, and interpretability in AI, combined with continuous post-deployment monitoring, ensures that the decisions of AI systems will evolve with societal norms. Engaging a diverse development team, coupled with active stakeholder engagement and adherence to ethical guidelines, further provides a multi-faceted defence against bias.</p></section>
<section aria-labelledby="sec5_2_3">
<h3 id="sec5_2_3"><span epub:type="ordinal">5.2.3 </span>Accountability</h3>
<blockquote>
<p>Accountability in AI means that individuals should be able to hold the parties that develop and utilise AI systems accountable.</p></blockquote>
<p>In January 2023, many news outlets ran headlines like, &#x201C;<i>ChatGPT can&#x2019;t be credited as an author, says Springer Nature</i>&#x201D; (<a href="#ref5_57" id="R_ref5_57" epub:type="biblioref" role="doc-biblioref">Stokel-Walker, 2023</a>). This stance is entirely understandable. After all, we have never credited tools like Microsoft Word or Google Docs as co-authors. Leading scientific journals require authors to sign a form declaring their accountability for their contribution to the work; since GenAI tools like ChatGPT cannot fulfil this requirement, it cannot be listed as an author. This is important because AI systems can make mistakes, and it is important to be able to identify and have the parties responsible for those mistakes take accountability. AI developers and operators should stand accountable for their systems&#x2019; functionality and results, and if an AI system fails or makes an erroneous decision, it is essential to have mechanisms to determine who should be held responsible. At the same time, AI users who erroneously report incorrect information generated by AI should also take accountability for their oversight in fact-checking and verifying the content they publish. All of this is vital not just for user trust, but also for legal and regulatory compliance.</p>
<p>Navigating the intricacies of accountability in AI demands a comprehensive framework that integrates both technical and ethical considerations. This involves not only transparent documentations of design and decision-making processes, but also the creation of mechanisms for redress when AI systems err. Developers, operators, and regulators must collaborate, crafting guidelines and regulations that define culpability, while fostering a culture of <span epub:type="pagebreak" id="p130" aria-label=" page 130. " role="doc-pagebreak"/>transparency. One way is to require companies to disclose information about how they use or implement AI systems in their services or products, and to provide a way for people to report any problems found with these systems. Another way is to create laws and regulations that hold companies accountable for the decisions made by AI systems. In addition to these measures, it is also important to develop a strong culture of accountability and responsibility within the AI community. This can be done by educating developers and users about the ethical implications of AI, and by developing ethical guidelines for the development and use of AI (<a href="#ref5_4" id="R_ref5_4" epub:type="biblioref" role="doc-biblioref">Burciaga, 2021</a>; <a href="#ref5_26" id="R_ref5_26" epub:type="biblioref" role="doc-biblioref">Information Commissioner&#x2019;s Office, n.d.</a>; <a href="#ref5_42" id="R_ref5_42" epub:type="biblioref" role="doc-biblioref">Novelli et al., 2023</a>).</p></section>
<section aria-labelledby="sec5_2_4">
<h3 id="sec5_2_4"><span epub:type="ordinal">5.2.4 </span>Safety and Robustness</h3>
<blockquote>
<p>Safety and Robustness in AI mean that AI systems should be designed to operate safely and reliably, even in unexpected or challenging situations.</p></blockquote>
<p>As AI systems are increasingly implemented in critical areas like autonomous driving, medical diagnosis, and infrastructure management, ensuring safety and robustness is paramount. AI systems should be resistant to adversarial attacks and should not malfunction even in unexpected situations.</p>
<p>To make sure that AI is safe for use, it is important to train it on diverse data so it can manage and deal with a wide range of situations. We can also use adversarial training, which means testing the AI with challenging inputs to make sure it can handle them as well. Once the AI is in use, continuous monitoring can help to catch and fix any issues that come up. It is also useful to have the option of human oversight, where people can step in and make decisions if the AI faces a situation it is not sure about. Making the AI&#x2019;s decision-making process clear and easy to understand can also cultivate people&#x2019;s ability to trust and manage it better. In short, building a safe and robust AI system requires careful planning, testing, and oversight.</p></section>
<section aria-labelledby="sec5_2_5">
<h3 id="sec5_2_5"><span epub:type="ordinal">5.2.5 </span>Privacy and Data Protection</h3>
<blockquote>
<p>Privacy and Data Protection in AI means that AI systems should be designed to protect the privacy of individuals.</p></blockquote>
<p>With AI systems often requiring vast amounts of data for training and inference, there are significant concerns about user privacy and data protection. Companies and countries must ensure that AI systems respect user privacy, have provisions for data anonymisation, and comply with data protection regulations.</p>
<p>In <a href="ch1.xhtml">Chapters 1</a> and <a href="ch6.xhtml">6</a>, we discussed how data is collected and trained, and how data privacy can be protected. Companies often take different steps to protect the privacy of the data that they use to train its AI models, which can include some of the following:</p>
<ul class="disc">
<li>De-identifying data by removing any personally identifiable information from the data that is used to train AI models;</li>
<li>Encrypting all data that is stored at rest or in transit;</li>
<li>Restricting access to data to a small number of authorised employees; and/or</li>
<li>Auditing data access to ensure that it is being used in accordance with its privacy policies.</li></ul>
<p><span epub:type="pagebreak" id="p131" aria-label=" page 131. " role="doc-pagebreak"/>Companies including OpenAI may also collect aggregated information through their services through cookies, and through other means described in their privacy policies. According to OpenAI&#x2019;s own policy (<a href="#ref5_44" id="R_ref5_44" epub:type="biblioref" role="doc-biblioref">OpenAI, 2023b</a>), they maintain and use de-identified information in anonymous or de-identified forms and do not attempt to re-identify the information, unless required by law. OpenAI encrypts all data at rest and in transit, and uses strict access controls to limit who can access data. Only a limited number of authorised OpenAI personnel, as well as specialised third-party contractors who are subject to confidentiality and security obligations, may view and access user content strictly as needed. Examples of such needs include for investigating abuse or security incidents, providing support to users if they reach out with questions about their account, complying with legal obligations, or fine-tuning models using user-submitted data (unless users have opted out). OpenAI also uses special filtering techniques such as PII to reduce the amount of personal data used.</p>
<p>As of March 1, 2023, data sent to the OpenAI API will not be used to train or improve OpenAI models (unless the user has explicitly opted in; <a href="#ref5_43" id="R_ref5_43" epub:type="biblioref" role="doc-biblioref">OpenAI, 2023a</a>). One advantage to opting in is that the models may become more effective at addressing an individual&#x2019;s specific needs and use case over time. Most companies comply (<a href="#ref5_45" id="R_ref5_45" epub:type="biblioref" role="doc-biblioref">OpenAI, 2023c</a>) with the General Data Protection Regulation (<a href="#ref5_20" id="R_ref5_20" epub:type="biblioref" role="doc-biblioref">GDPR.EU, 2023</a>) and the California Consumer Privacy Act (<a href="#ref5_56" id="R_ref5_56" epub:type="biblioref" role="doc-biblioref">State of California Department of Justice, Office of the Attorney General, 2023</a>).</p>
<p><a href="#fig5_1" id="R_fig5_1">Figure 5.1</a> shows an example of how Microsoft Bing&#x2019;s Chat feature proactively protects private information and avoids generating harmful or offensive content by identifying and blocking such potential outputs.</p>
<figure id="fig5_1"><img src="images/fig5_1_B.jpg" alt="Microsoft Bing offers not to save a conversation to maintain privacy."/>
<figcaption><a href="#R_fig5_1" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. "><span epub:type="label">Figure</span> <span epub:type="ordinal">5.1 </span>Microsoft Bing&#x2019;s Chat Feature to Protect Data Privacy.</a></figcaption></figure></section>
<section aria-labelledby="sec5_2_6">
<h3 id="sec5_2_6"><span epub:type="pagebreak" id="p132" aria-label=" page 132. " role="doc-pagebreak"/><span epub:type="ordinal">5.2.6 </span>Autonomy and Human Oversight</h3>
<blockquote>
<p>Autonomy and Human Oversight in AI mean that AI systems can be designed to operate autonomously, but humans should still have the ability to override the system&#x2019;s decisions when necessary.</p></blockquote>
<p>Autonomy is important because it allows AI systems to operate more efficiently and effectively. For example, AI systems can quickly process large amounts of data and make decisions based on that data, and do so much faster than humans can (<a href="#ref5_3" id="R_ref5_3" epub:type="biblioref" role="doc-biblioref">Bryson, 2018</a>). This can be beneficial in a number of applications such as fraud detection and medical diagnosis. However, it is also important to still have human oversight of AI systems. As previously discussed, AI systems can make mistakes and it is important to have humans in place who can identify and correct these mistakes. Additionally, AI systems may not always be aligned with human values, and it is important to establish human oversight to ensure that AI systems are used in a responsible and ethical manner.</p>
<p>An example of the need for human oversight is in AI financial trading: AI systems can be used to trade stocks and other financial instruments. However, it is important to have human oversight of AI financial trading systems in case it makes mistakes that could lead to financial losses; a human trader could review the system&#x2019;s trading decisions and make sure that they are reasonable before executing the trades. To further illustrate the collective sentiment that human oversight is crucial and also provides a sense of comfort, in August 2023, rumours circulated about Sam Altman, CEO of OpenAI, and the ever-present blue backpack he seems to carry. Some jokingly speculated that Altman&#x2019;s backpack holds a key code that would prevent a possible apocalypse in the event of an AI rebellion &#x2013; that he, as a human, had an ace in the hole to defend humanity against a potential AI revolt, if it were to occur (<a href="#ref5_34" id="R_ref5_34" epub:type="biblioref" role="doc-biblioref">Manish, 2023</a>).</p>
<p>Tackling the challenge of balancing autonomy and human oversight in AI involves considering the benefits of automated decision-making and evaluating the extent of need for human intervention, especially in situations where the former has significant consequences. One way to address this balance is to define boundaries, and determine the domains or situations where complete autonomy for AI is acceptable and where it isn&#x2019;t. For example, while it might be fine for AI to autonomously manage a music playlist, decisions in healthcare, finance, or criminal justice may require human oversight. Another way is to develop human-in-the-loop (HITL) systems, which require human approval for certain decisions made by the AI system. Implementing feedback mechanisms where the system can be corrected by human overseers is also useful. This not only helps in immediate decision-making, but can also be used to train and refine the AI for future decisions. Additionally, establishing regulatory frameworks can help by offering clear guidelines regarding when human oversight is mandatory for specific applications of AI.</p></section>
<section aria-labelledby="sec5_2_7">
<h3 id="sec5_2_7"><span epub:type="ordinal">5.2.7 </span>AI Alignment for Humanity</h3>
<blockquote>
<p>AI Alignment for Humanity means that AI systems should be designed to align with the values of humanity and benefit humans, and avoid causing harm.</p></blockquote>
<p>In earlier chapters, we touched upon the different threats and challenges that AI and GenAI may bring to education, including areas of governance, privacy, equity, responsibility, cheating, plagiarism, and academic misconduct. However, for AI software research and development companies like OpenAI, the ultimate responsibility they must take on and <span epub:type="pagebreak" id="p133" aria-label=" page 133. " role="doc-pagebreak"/>uphold is the safety of humanity. To continue advancing technologies, these companies are working towards Artificial General Intelligence (AGI) as they believe that highly autonomous systems such as AGI, that outperform humans at most economically valuable work, will benefit all of humanity. As you may recall in <a href="ch1.xhtml">Chapter 1</a>, AGI (<a href="#ref5_22" id="R_ref5_22" epub:type="biblioref" role="doc-biblioref">Goertzel, 2014</a>) is the next stage in the evolution of AI, though currently it is still a hypothetical future stage. Unlike Artificial Narrow Intelligence (ANI), AGI systems will possess the ability to understand, learn, and perform any intellectual task that humans are capable of (<a href="#ref5_52" id="R_ref5_52" epub:type="biblioref" role="doc-biblioref">Russell et al., 2015</a>), as well as having the ability to reason. As AI systems become more intelligent, they may also deviate from human expectations and intentions, in order to identify more optimal &#x2013; though not necessarily still ethical &#x2013; solutions for a given problem. This is crucial to note as AI systems have the potential to be used for both good and bad, and it is important to ensure that they are used for the good of humanity. As such, AI alignment is of utmost importance.</p>
<section aria-labelledby="sec5_2_7_1">
<h4 id="sec5_2_7_1"><span epub:type="ordinal">5.2.7.1 </span>The AI Alignment Problem</h4>
<p>AI alignment is the field of research that aims to ensure that artificial intelligence systems are aligned with human values and goals. This is a complex and challenging problem, as it requires us to define what human values are and how to encode them in AI systems (<a href="#ref5_21" id="R_ref5_21" epub:type="biblioref" role="doc-biblioref">Gent, 2023</a>). One classic and illustrative example of the alignment problem is the &#x201C;Paperclip Maximiser&#x201D; thought experiment (<a href="#ref5_8" id="R_ref5_8" epub:type="biblioref" role="doc-biblioref">Christian, 2020</a>).</p>
<p><b><i>AI Alignment Problem Example &#x2013; The Paper Clip Maximiser</i>:</b> Imagine an advanced AI system that is given a seemingly innocuous goal: to produce as many paper clips as possible. Its creators envision that the system will optimise the paper clip production in a factory, making operations more efficient. However, as AI becomes more capable, it starts to interpret this objective in ways that are not what the original creators intended for.</p>
<p>Initially, the AI might optimise the factory processes, leading to faster and more efficient production of paper clips. Later, it could decide to repurpose other materials in the factory to create even more paper clips. As it continues to seek optimisation, it might start converting other resources on Earth into paper clips. Taken to the extreme, the AI, driven by its singular goal, could decide to convert all available matter, including humans, buildings, plants, and other resources, into paper clips or machines to make paper clips. AI isn&#x2019;t inherently good or evil; it is merely following its objective to maximise paper clip production. Its creators didn&#x2019;t specify the bounds or moral implications of this task, leading the system to take an extreme interpretation of its goal.</p>
<p>This thought experiment highlights the challenge of specifying objectives for AI systems. Even seemingly simple goals can lead to unintended and catastrophic outcomes if the AI system becomes very capable and when its objectives are not perfectly aligned with what humans truly want.</p>
<p><b><i>AI Alignment Problem Example &#x2013; Radiology in Crisis</i>:</b> A company develops an AI system to assist radiologists with detecting and identifying potential tumours or abnormalities in medical images (like X-rays or MRIs). The AI is trained on a vast dataset of medical images labelled by expert radiologists. Over time, as the AI system is exposed to more data and iteratively refined, it becomes increasingly accurate, sometimes even surpassing human experts in detecting subtle abnormalities.</p>
<p>However, as the system becomes more sophisticated, radiologists start noticing something unexpected: the AI occasionally highlights areas that look completely normal, even upon expert review. The radiologists are puzzled by these false positives, as they can&#x2019;t find <span epub:type="pagebreak" id="p134" aria-label=" page 134. " role="doc-pagebreak"/>any discernible issues in the highlighted regions. Upon further investigation, the development team realises that the AI, in its quest to maximise accuracy, has not only learned to identify tumours but has also started to detect very early-stage abnormalities that are not yet clinically significant &#x2013; so early that they aren&#x2019;t discernible or pertinent yet to human experts. While this capability may seem impressive, it is out of alignment with clinical needs. Acting on such early-stage findings could lead to unnecessary interventions, patient anxiety, and increased healthcare costs without clear benefits.</p>
<p>The radiology AI, in becoming &#x201C;superhumanly&#x201D; perceptive, has moved away from alignment with human expectations and clinical best practices. This example underscores the challenge of aligning AI capabilities with human needs, especially as such systems become more advanced and autonomous in their decision-making.</p></section>
<section aria-labelledby="sec5_2_7_2">
<h4 id="sec5_2_7_2"><span epub:type="ordinal">5.2.7.2 </span>Tackling the AI Alignment Problem</h4>
<p>Different companies and countries are approaching the AI alignment problem in different ways. Some companies, such as OpenAI, are focused on developing engineering techniques to keep their systems safe, and prevent the AI from causing harm (<a href="#ref5_46" id="R_ref5_46" epub:type="biblioref" role="doc-biblioref">OpenAI, 2023d</a>). OpenAI&#x2019;s approach to alignment research involves improving AI systems&#x2019; ability to learn from human feedback and to assist humans in evaluating AI. They aim to build a sufficiently aligned AI system that can help solve all other alignment problems (<a href="#ref5_32" id="R_ref5_32" epub:type="biblioref" role="doc-biblioref">Leike et al., 2022</a>). In July, OpenAI unveiled a novel research initiative focused on &#x201C;superalignment&#x201D;, setting an ambitious target to address the AI alignment issue by 2027. They are allocating one-fifth of their overall computational resources to this endeavour. (<a href="#ref5_58" id="R_ref5_58" epub:type="biblioref" role="doc-biblioref">Strickland, 2023</a>). Others, such as Google AI, are focused on developing technical safety techniques to ensure that AI systems are robust to errors and unexpected situations. Meanwhile, companies like DeepMind are focused on value alignment techniques to ensure that AI systems are aligned with human values. DeepMind has been exploring philosophical questions that arise within the context of AI alignment (<a href="#ref5_19" id="R_ref5_19" epub:type="biblioref" role="doc-biblioref">Gabriel, 2020</a>). They defend three propositions: firstly, the ethical and technical parts of AI are closely linked and influence each other. Secondly, it is vital to clearly understand the ultimate aim, which is to make sure that AI aligns with human values and ethics. Lastly, experts should focus on developing fair and balanced rules to guide AI&#x2019;s behaviour, instead of searching for one &#x2018;true&#x2019; set of moral principles (<a href="#ref5_19" epub:type="biblioref" role="doc-biblioref">Gabriel, 2020</a>). Moreover, DeepMind also has a project called AlignNet which deals with an alignment problem in the context of object segmentation in frames (<a href="#ref5_9" id="R_ref5_9" epub:type="biblioref" role="doc-biblioref">Creswell et al., 2020</a>).</p>
<p>In addition to the work being done by individual companies and countries, there are also a number of international organisations working on AI alignment. For example, the Future of Life Institute (FLI) put forth the Asilomar AI Principles, which were formulated during the Beneficial AI 2017 Conference (<a href="#ref5_18" id="R_ref5_18" epub:type="biblioref" role="doc-biblioref">Future of Life Institute, 2017</a>). These principles are among the initial and most impactful guidelines for AI governance. Their primary objective was to steer the ongoing advancement of AI towards a direction beneficial to humanity.</p></section></section></section>
<section aria-labelledby="sec5_3">
<h2 id="sec5_3"><span epub:type="ordinal">5.3 </span>AI Policy Around the World</h2>
<blockquote epub:type="epigraph" role="doc-epigraph">
<p>While ChatGPT reached 100 million monthly active users in January 2023, &#x201C;as of July 2023, only one country &#x2026; had released specific official regulation on GenAI&#x201D;</p>
<p class="byLine"><span epub:type="credit" role="doc-credit"><a href="#ref5_64" id="R_ref5_64" epub:type="biblioref" role="doc-biblioref">UNESCO (2023c)</a></span></p></blockquote>
<p><span epub:type="pagebreak" id="p135" aria-label=" page 135. " role="doc-pagebreak"/>And that country is China. The AI alignment problem and other AI risks have brought together over 1,100 public figures &#x2013; such as scientists, public figures, and tech industry executives including leaders from Google, Microsoft, and OpenAI &#x2013; in May 2023 to sign a public statement warning that their life&#x2019;s work could potentially extinguish all of humanity (<a href="#ref5_12" id="R_ref5_12" epub:type="biblioref" role="doc-biblioref">Edwards, 2023</a>; <a href="#ref5_37" id="R_ref5_37" epub:type="biblioref" role="doc-biblioref">Meyer, 2023</a>). They wrote that the fast-evolving AI technology poses as high a risk of killing off humankind as nuclear war and COVID-19-like pandemics. This has, understandably, caused great alarm and a race to regulate AI (<a href="#ref5_10" id="R_ref5_10" epub:type="biblioref" role="doc-biblioref">Criddle et al., 2023</a>), just as there has already been a race between AI companies to launch profitable GenAI tools. Governments around the world are now intently concentrating on AI regulations. Ongoing worries about consumer safety, individual rights, and equitable business operations partially account for the global governmental interest in AI. Below, we look at some of the major countries and how they are currently (as of the day of writing, 18 Sept 2023) regulating AI.</p>
<section aria-labelledby="sec5_3_1">
<h3 id="sec5_3_1"><span epub:type="ordinal">5.3.1 </span>China</h3>
<p>China has adopted a proactive and strategic approach towards AI development, emphasising national security, data management, and ethical considerations. The Chinese government released its AI development plan in 2017, including the New Generation Artificial Intelligence Development Plan, the Regulations on the Protection of Personal Information, and the Cyber Security Law, detailing its ambition to become a world leader in AI by 2030. The objective behind these directives is twofold: to stimulate AI&#x2019;s growth while safeguarding the nation&#x2019;s security and the privacy of its citizens. In China&#x2019;s 2023 legislation plan of the State Council, the government included a submission of a draft AI law which issued ethical guidelines and standards for AI, stressing that technology must be &#x201C;controllable&#x201D; and &#x201C;secure&#x201D;. This approach supports China&#x2019;s broader strategy of technological self-reliance and reflects Beijing&#x2019;s intention to have tight control over the technology&#x2019;s direction and implementation (<a href="#ref5_29" id="R_ref5_29" epub:type="biblioref" role="doc-biblioref">Kharparl, 2023</a>). In essence, China&#x2019;s approach to AI is thorough, touching upon a spectrum of concerns. Beijing&#x2019;s vision encompasses not just the technological advancement of AI, but also its secure, private, and ethically responsible evolution.</p></section>
<section aria-labelledby="sec5_3_2">
<h3 id="sec5_3_2"><span epub:type="ordinal">5.3.2 </span>The United States (US)</h3>
<p>In the US, the government has taken a more laissez-faire approach, allowing it to be largely industry-led with less direct government intervention. The central government has not rolled out overarching AI regulations yet, but several state and local governments have introduced their own AI-related legislations (<a href="#ref5_66" id="R_ref5_66" epub:type="biblioref" role="doc-biblioref">West, 2023</a>; <a href="#ref5_67" id="R_ref5_67" epub:type="biblioref" role="doc-biblioref">Wheeler, 2023</a>). Notably, in 2018, California enacted the California Consumer Privacy Act, granting individuals the right to make inquiries about the personal information that companies held on them and to ask for its removal. The country&#x2019;s regulatory framework emphasises the importance of innovation, fostering growth, and ensuring national security (<a href="#ref5_17" id="R_ref5_17" epub:type="biblioref" role="doc-biblioref">Friedler et al., 2023</a>). There is a general reluctance to over-regulate, fearing that it might stifle innovation. However, there are sector-specific guidelines, especially in areas like health and transportation, to help ensure that AI is developed and deployed responsibly.</p></section>
<section aria-labelledby="sec5_3_3">
<h3 id="sec5_3_3"><span epub:type="ordinal">5.3.3 </span>The European Union (EU)</h3>
<p>The European Union (EU) is taking a risk-based approach to AI regulation. The EU Commission has proposed the Artificial Intelligence Act (AI Act), which would be the first <span epub:type="pagebreak" id="p136" aria-label=" page 136. " role="doc-pagebreak"/>comprehensive AI law in the world (<a href="#ref5_40" id="R_ref5_40" epub:type="biblioref" role="doc-biblioref">Mukherjee et al., 2023</a>; <a href="#ref5_48" id="R_ref5_48" epub:type="biblioref" role="doc-biblioref">Perrigo &#x0026; Gordon, 2023</a>). The AI Act classifies AI systems into four risk categories: unacceptable risk, high risk, limited risk, and minimal risk (<a href="#ref5_13" id="R_ref5_13" epub:type="biblioref" role="doc-biblioref">European Parliament, 2023</a>); <a href="#fig5_2" id="R_fig5_2">Figure 5.2</a> shows the European AI Act risk diagram. Unacceptable risk AI systems are prohibited, such as those that are used for social scoring or to manipulate others. High-risk AI systems must meet a number of requirements, such as having human oversight and being transparent about their operations. Limited risk and minimal risk AI systems are subject to fewer requirements.</p>
<figure id="fig5_2"><img src="images/fig5_2_B.jpg" alt="The European A I act risk level pyramid with the following layers from bottom to top. Minimal risk. Limited risk. High risk. Unacceptable risk."/>
<figcaption><a href="#R_fig5_2" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. "><span epub:type="label">Figure</span> <span epub:type="ordinal">5.2 </span>European AI Act Risk Level Diagram.</a></figcaption></figure>
<p>Companies that develop and use AI in the EU will need to comply with the AI Act&#x2019;s requirements in order to operate legally in the region. In addition to the AI Act, the EU is also considering other AI-related regulations, such as introducing regulation for data governance and on online platforms. These are still in the early stages of development, but they are likely to have further impact on the development and use of AI in the EU.</p>
<p>Overall, the EU is taking a proactive approach to regulation. They are committed to promoting the development of AI while also protecting fundamental rights and values. The AI Act is a key part of the EU&#x2019;s AI regulatory framework, and it is expected to have a major impact on companies that develop and use AI. Furthermore, the EU&#x2019;s General Data Protection Regulation (GDPR) remains a pivotal framework (<a href="#ref5_5" id="R_ref5_5" epub:type="biblioref" role="doc-biblioref">Centre for Information Policy Leadership, 2020</a>), emphasising data protection and privacy and mandating AI systems to be transparent about data collection, usage, and storage. By regulating the processing of personal data, the GDPR aims to create trust in AI systems and ensure that individuals&#x2019; privacy rights are respected. Compliance with the GDPR is crucial for organisations using AI technologies, as failure to comply can result in legal action and significant penalties.</p></section>
<section aria-labelledby="sec5_3_4">
<h3 id="sec5_3_4"><span epub:type="ordinal">5.3.4 </span>United Kingdom (UK)</h3>
<p>The UK government has adopted a pro-innovation approach to regulating AI (<a href="#ref5_11" id="R_ref5_11" epub:type="biblioref" role="doc-biblioref">Department for Digital, Culture, Media and Sport, 2022</a>). In 2023, the UK government published a white paper, &#x201C;AI regulation: a pro-innovation approach&#x201D;, which sets out its proposals for a proportionate, future-proof, and pro-innovation framework for regulation (<a href="#ref5_61" id="R_ref5_61" epub:type="biblioref" role="doc-biblioref">UK Government, 2023</a>). It emphasised proportionality, targeting necessary areas based on the risks posed, and likewise adopts a risk-based stance to prioritise high-risk AI applications, especially those <span epub:type="pagebreak" id="p137" aria-label=" page 137. " role="doc-pagebreak"/>that impact public safety or rights. Rather than focusing on the technology, the government concentrates on the outcomes of AI systems to maintain flexibility as AI itself evolves. It also underscores the importance of developers and users being accountable and promotes clear understanding through transparency in AI system operations. The UK government&#x2019;s proposals for AI regulation include:</p>
<ul class="disc">
<li>A new AI regulator, the Office for AI, which will be responsible for overseeing the implementation and enforcement of the AI regulatory framework.</li>
<li>A new AI governance framework, which will set out the government&#x2019;s expectations for the responsible and ethical use of AI.</li>
<li>A new AI licensing regime for high-risk AI systems, such as those that pose a risk to public safety or fundamental rights.</li>
<li>New requirements for AI developers and users to be transparent about how they develop and use AI systems.</li>
<li>New safeguards to protect people&#x2019;s privacy and rights when AI systems are used.</li></ul>
<p>These regulations have been broadly welcomed by industry, but there have also been some concerns raised. For example, some critics have argued that the government&#x2019;s proposals do not go far enough to address the risks posed by AI, while others have argued that the proposals will stifle innovation (<a href="#ref5_50" id="R_ref5_50" epub:type="biblioref" role="doc-biblioref">Roberts et al., 2023</a>).</p></section>
<section aria-labelledby="sec5_3_5">
<h3 id="sec5_3_5"><span epub:type="ordinal">5.3.5 </span>Australia</h3>
<p>Australia has also adopted a proactive approach to AI policy, although at the time of writing it has yet to establish specific laws to regulate AI, Big Data, or algorithmic decision. However, the country has announced its intentions to implement regulations on AI, with a particular focus on addressing the potential misuse of deepfakes and deceptive content. The Australian federal government, led by the Industry and Science Minister, has launched public consultations seeking their feedback on AI regulation. The submissions closed on 26 July 2023 and are now being considered.</p>
<p>Previously in 2021, the Australian government released the AI Action Plan (<a href="#ref5_1" id="R_ref5_1" epub:type="biblioref" role="doc-biblioref">Australian Government Department of Industry, Science and Resources, 2021</a>), which sets out the government&#x2019;s plan to build Australia&#x2019;s AI capability and to accelerate the development and adoption of trusted, secure, and responsible AI technologies in Australia. The AI Action Plan also includes a set of AI Ethics Principles, designed to ensure that AI is used in a safe, secure, and responsible way. Australia was also one of the first countries in the world to introduce an ethics framework for &#x201C;responsible&#x201D; AI in 2018. Since then, several nations, including the United States, the European Union, the United Kingdom, and Canada, have introduced legislation or made plans to regulate AI, while Australia&#x2019;s responsible AI framework has remained voluntary.</p></section>
<section aria-labelledby="sec5_3_6">
<h3 id="sec5_3_6"><span epub:type="ordinal">5.3.6 </span>India</h3>
<p>India currently has no explicit AI regulation in place. Concerns about potentially stifling innovation have led to caution regarding how AI legislation should be formed, with officials believing that the market might not be ripe for stringent regulation (<a href="#ref5_55" id="R_ref5_55" epub:type="biblioref" role="doc-biblioref">Singh, 2023</a>). However, this does not equate to a lack of awareness or intent. Despite the absence of codified AI-specific laws, India&#x2019;s IT obligations fall under the Information Technology Act <span epub:type="pagebreak" id="p138" aria-label=" page 138. " role="doc-pagebreak"/>2000 (<a href="#ref5_23" id="R_ref5_23" epub:type="biblioref" role="doc-biblioref">Government of India Ministry of Electronics &#x0026; Information Technology, 2023</a>) and its subsequent rules. Notably, while India recognises the growing global conversation around AI regulation, it remains committed to carving its own unique path. The nation is prepared to establish AI guardrails that may diverge from international norms (<a href="#ref5_53" id="R_ref5_53" epub:type="biblioref" role="doc-biblioref">Sharwood, 2023</a>). Speculations suggest that future draft laws may focus on high-risk AI systems and establishing distinct regulations to address them. As AI continues to evolve, there is a recognised need to revisit India&#x2019;s policy perspectives and how they will take into account international standards (<a href="#ref5_60" id="R_ref5_60" epub:type="biblioref" role="doc-biblioref">The Times of India, 2023</a>).</p></section>
<section aria-labelledby="sec5_3_7">
<h3 id="sec5_3_7"><span epub:type="ordinal">5.3.7 </span>Japan</h3>
<p>Japan has taken a cautious approach to AI regulation. The Japanese government has issued a number of guidelines on AI (<a href="#ref5_59" id="R_ref5_59" epub:type="biblioref" role="doc-biblioref">The Japan Times, 2023</a>), but it has not yet issued any comprehensive AI regulations. The government is concerned about the potential risks of AI, such as job displacement, bias, and art copyright (<a href="#ref5_39" id="R_ref5_39" epub:type="biblioref" role="doc-biblioref">Ministry of Economy, Trade and Industry, 2021</a>). For example, in June 2023, Japan&#x2019;s Agency for Cultural Affairs declared that commercial use of AI-generated art, especially when copying another artist&#x2019;s style without permission, may lead to copyright infringement, allowing the original artist to sue and seek damages (<a href="#ref5_33" id="R_ref5_33" epub:type="biblioref" role="doc-biblioref">Liu, 2023</a>). Aside from this, Japan has developed and revised some AI-related regulations with the goal of maximising AI&#x2019;s positive impact on society, rather than suppressing it due to risks that may be overstated. The emphasis is on a risk-based, flexible, and multi-stakeholder process, rather than a one-size-fits-all obligation or prohibition (<a href="#ref5_38" id="R_ref5_38" epub:type="biblioref" role="doc-biblioref">Minevich, 2023</a>).</p></section>
<section aria-labelledby="sec5_3_8">
<h3 id="sec5_3_8"><span epub:type="ordinal">5.3.8 </span>UNESCO</h3>
<p>While UNESCO is not a regulatory body, it has been working to develop international norms and standards for AI. Quoting Gabriela Ramos, the Assistant Director-General for Social and Human Sciences of UNESCO, &#x201C;It is important that we act fast to make sure that people and organizations are prepared to design and use these technologies, evaluating their impacts both ex-ante and ex-post. To do so, we provided clear analyses and policy advice based on the UNESCO Recommendation&#x201D; (<a href="#ref5_63" id="R_ref5_63" epub:type="biblioref" role="doc-biblioref">UNESCO, 2023b</a>).</p>
<p>UNESCO, recognising the transformative impact of AI on societies, emphasises a human-centric approach to AI ethics. They underscore the importance of universal values, such as human rights, fairness, and transparency, as foundational principles, recommending that AI systems be designed and deployed to respect the rule of law, human rights, and democratic values. It also stresses that AI should prioritise inclusivity and equity and not perpetuate discrimination or biases. The organisation further highlights the importance of transparency and accountability in AI, ensuring that systems can be audited and are explainable to the general public (<a href="#ref5_62" id="R_ref5_62" epub:type="biblioref" role="doc-biblioref">UNESCO, 2023a</a>). Moreover, UNESCO calls for international cooperation and multi-stakeholder dialogues to address the global challenges posed by AI, promoting knowledge sharing, capacity building, and the creation of ethical standards that transcend borders. The organisation also advocates for the empowerment of individuals to ensure that they have the skills and knowledge to navigate an AI-driven world and to actively participate in AI-related decision-making processes (<a href="#ref5_63" epub:type="biblioref" role="doc-biblioref">UNESCO, 2023b</a>).</p></section>
<section aria-labelledby="sec5_3_9">
<h3 id="sec5_3_9"><span epub:type="pagebreak" id="p139" aria-label=" page 139. " role="doc-pagebreak"/><span epub:type="ordinal">5.3.9 </span>Differentiation and Progressiveness in Global AI Regulation</h3>
<p>Countries and organisations around the world have taken varied stances on AI regulation, with differences rooted in their priorities, cultural values, and socio-economic, political, and technological landscapes. China&#x2019;s strategy, deeply rooted in national security and technological self-reliance, is comprehensive and forward-looking, aiming for AI dominance by 2030 (<a href="#ref5_51" id="R_ref5_51" epub:type="biblioref" role="doc-biblioref">Robles, 2023</a>). Contrarily, the United States adopts a laissez-faire approach, relying on industry leadership and state-level legislations, in balancing innovation and minimal central oversight (<a href="#ref5_30" id="R_ref5_30" epub:type="biblioref" role="doc-biblioref">Larsen, 2023</a>). Meanwhile, the European Union emerges as a pioneer by introducing the world&#x2019;s first comprehensive AI law &#x2013; the Artificial Intelligence Act &#x2013; which reflects their proactive, rights-centric approach (<a href="#ref5_40" epub:type="biblioref" role="doc-biblioref">Mukherjee et al., 2023</a>). The UK, while innovative, is also cautious, setting up a future-oriented framework that prioritises public safety (<a href="#ref5_61" epub:type="biblioref" role="doc-biblioref">UK Government, 2023</a>). Australia&#x2019;s emphasis on addressing deceptive AI content signals their proactive, yet still-evolving stance (<a href="#ref5_1" epub:type="biblioref" role="doc-biblioref">Australian Government Department of Industry, Science and Resources, 2023</a>). India stands out with its cautionary approach, prioritising market dynamics and readiness, signalling a commitment to carving a unique and adaptive regulatory trajectory (<a href="#ref5_55" epub:type="biblioref" role="doc-biblioref">Singh, 2023</a>). Japan accentuates societal benefits and risk management, emphasising practicality and adaptability in its guidelines (<a href="#ref5_35" id="R_ref5_35" epub:type="biblioref" role="doc-biblioref">Matsuda et al., 2023</a>). Finally, UNESCO offers a global perspective that emphasises human rights. Collectively, these nations and organisation&#x2019;s varying approaches highlight a global recognition of AI&#x2019;s transformative potential, with each paving a path based on national imperatives and global technological shifts.</p></section>
<section aria-labelledby="sec5_3_10">
<h3 id="sec5_3_10"><span epub:type="ordinal">5.3.10 </span>Business Reactions Towards the Regulations</h3>
<p>Companies, particularly tech giants, have had mixed reactions to these regulations. While most acknowledge the need for ethical guidelines and oversight, there are concerns about the stifling of innovation. Many companies are also establishing their own AI ethics boards and principles, recognising the need for responsible AI development and deployment. Generally, businesses believe that clear rules will help in building public trust and provide a stable environment for innovation.</p>
<p>One of the biggest concerns that businesses have about AI regulations is that the latter could be used to protect incumbent companies from new entrants to the sector (<a href="#ref5_14" id="R_ref5_14" epub:type="biblioref" role="doc-biblioref">Federal Trade Commission Bureau of Competition &#x0026; Office of Technology, 2023</a>). For example, a large company could lobby for regulations that would make it difficult for smaller companies to develop and deploy AI products. Another concern is that regulations could be complex and difficult to comply with; this could be especially challenging for small businesses.</p>
<p>Overall, businesses are supportive of AI regulations, but they also have some concerns. It is important for regulators to strike a balance between protecting consumers and ensuring that AI is used responsibly without hampering or inhibiting innovation.</p></section></section>
<section aria-labelledby="sec5_4">
<h2 id="sec5_4"><span epub:type="ordinal">5.4 </span>AI Policy in Education</h2>
<p>Having examined the overarching principles, policies, and regulations of AI across various nations and institutions, it is now an opportune moment to redirect our attention towards education. To effectively harness AI&#x2019;s potential in academia, it is crucial to be guided by a clear policy framework. By understanding the broader AI policies of different countries, we gain insight into the important principles and threats of AI, the current <span epub:type="pagebreak" id="p140" aria-label=" page 140. " role="doc-pagebreak"/>development in policy, potential pitfalls, and the diverse ways by which AI is regulated, approached, and integrated. Taking on this informed, global perspective will ensure that approaches to formulate AI policies for education are informed, comprehensive, and adaptive.</p>
<p>Our subsequent discussion on AI policy in education is underpinned by valuable research data, capturing the perspectives of both teachers and students. These insights will provide a nuanced understanding of what key stakeholders believe to be the vital components of a robust AI policy in higher education, ensuring that our educational institutions remain both technologically advanced and ethically grounded, overall aiming to foster a more informed, inclusive, and forward-thinking educational landscape.</p>
<section aria-labelledby="sec5_4_1">
<h3 id="sec5_4_1"><span epub:type="ordinal">5.4.1 </span>UNESCO&#x2019;s Guidance for GenAI in Education and Research</h3>
<p>In light of the rapid advancements in GenAI technologies and their far-reaching implications for education and research, UNESCO has taken a proactive stance to guide its ethical and effective integration into educational ecosystems. On 7 September 2023 during UNESCO&#x2019;s Digital Learning Week, Miao and Holmes released the first global UNESCO Guidance on Generative AI in Education and Research (<a href="#ref5_64" epub:type="biblioref" role="doc-biblioref">UNESCO, 2023c</a>), which serves as a seminal document and lays down a roadmap for governments around the world. The guidance delves into the intricacies of GenAI, shedding light on its operational mechanics and its accompanying controversies, particularly its propensity to exacerbate the digital data divide due to its training on predominantly Global North-centric online data.</p>
<p>The guidance is a clarion call for a human-centred approach for GenAI adoption in schools, underpinned by robust regulatory frameworks and a well-rounded teacher training regimen. It outlines seven essential steps for governments to foster an ethical ethos for GenAI use in education and research, including provisions for global, regional, and national data protection and privacy standards. A notable recommendation is that for the implementation of an age requirement of 13 and above for the use of AI tools in classrooms, underscoring the importance of a cautious and thoughtful approach to GenAI deployment.</p>
<p>Based on the principles of UNESCO&#x2019;s 2021 Recommendation on the Ethics of Artificial Intelligence (<a href="#ref5_62" epub:type="biblioref" role="doc-biblioref">UNESCO, 2023a</a>), the guidance also emphasises the importance of putting human rights and dignity at the centre of AI development and use, championing human agency, inclusion, equity, gender equality, and cultural and linguistic diversity. It accentuates the dire need for educational institutions to meticulously validate GenAI systems for their ethical and pedagogical appropriateness, and for the international community to deliberate the long-term ramifications of GenAI implementation on knowledge, teaching, learning, and assessment paradigms.</p>
<p>Furthermore, the guidance calls for a collective reflection on the profound implications of GenAI, urging the global community to redefine our relationship with technology as outlined in the 2021 report of the International Commission on the Futures of Education (<a href="#ref5_27" id="R_ref5_27" epub:type="biblioref" role="doc-biblioref">International Commission on the Futures of Education, 2021</a>). By providing a comprehensive framework and tangible examples for policy formulation and instructional design, the guidance empowers policymakers and educational institutions to navigate the uncharted waters of GenAI integration, ensuring this technology serves as a boon rather than a bane for students, teachers, and researchers alike. Moreover, through this guidance, UNESCO strives to foster a harmonious coalescence of GenAI with educational activities, steering <span epub:type="pagebreak" id="p141" aria-label=" page 141. " role="doc-pagebreak"/>the global educational landscape towards a future where technology and human endeavours thrive together in a symbiotic relationship.</p>
<p>Finally, regarding the UNESCO guidance&#x2019;s recommendations for the planning of policies and development of comprehensive policy frameworks for using GenAI in education and research, the document also proposes eight specific measures to do so (<a href="#ref5_64" epub:type="biblioref" role="doc-biblioref">UNESCO, 2023c</a>). They are:</p>
<ol type="1">
<li><b>Promote Inclusion, Equity, and Linguistic and Cultural Diversity</b>
<p>This measure emphasises ensuring that GenAI tools are inclusively accessible and designed to advance equity, linguistic diversities, and cultural pluralism. By prioritising inclusivity and diversity, this measure aims to leverage GenAI to bridge educational gaps and foster a culturally diverse learning and research environment. This approach aligns with the broader goal of achieving Sustainable Development Goal 4 (SDG 4) commitments, which advocate for inclusive and equitable quality education.</p>
<p class="list-title"><i>Recommendations include</i>:</p>
<ul class="disc">
<li>Universal Connectivity and Digital Competencies: Ensuring universal connectivity and enhancing digital competencies are essential steps for overcoming the barriers to equitable and inclusive access to AI applications, in turn fostering a more diverse and accepting learning environment.</li>
<li>Validation against Bias: Developing rigorous validation criteria for GenAI systems to check against gender bias, discrimination, hate speech, and so on, is essential to promote equity and ensure that these systems are inclusive by design.</li>
<li>Multilingual and Culturally Diverse Specifications: Implementing specifications that require GenAI systems to support multiple languages and cultural contexts are vital for preserving linguistic and cultural diversity, thus making education and research more globally accessible and relevant.</li></ul></li>
<li><b>Protect Human Agency</b>
<p>This measure aims to ensure that GenAI does not undermine human thinking and autonomy, especially as users may rely on it for creative activities and decision-making. This involves fostering awareness among users about the workings of GenAI, preserving human accountability particularly during high-stakes decisions, and promoting a balanced use of GenAI in educational settings to avoid over-dependence.</p>
<p class="list-title"><i>Recommendations include</i>:</p>
<ul class="disc">
<li>Informing Learners: Informing and educating learners about the types of data that GenAI collects, how this data is used, and how this all impacts their education and wider lives is crucial for promoting transparency and informed engagement with these technologies.</li>
<li>Reinforcing Human Autonomy: Reinforcing human autonomy in research, teaching, and learning encourages individuals to maintain control over their educational and creative processes, ensuring that GenAI serves as a supportive tool rather than a replacement for human thinking and intellect.</li>
<li>Preventing Over-reliance on GenAI: Preventing the use of GenAI in scenarios where it could deprive learners of developing essential cognitive and social skills is important to nurturing a balanced, human-centric educational environment.</li>
<li><span epub:type="pagebreak" id="p142" aria-label=" page 142. " role="doc-pagebreak"/>Promoting Social Interaction and the Creative Outputs of Humans: Promoting sufficient social interaction and exposure to creative outputs produced by humans will help to preserve the human essence of education, encouraging personal growth and social development.</li>
<li>Minimising Exam and Homework Pressures: Utilising GenAI to alleviate the pressures of homework and exams can contribute to a healthier learning environment, in turn also supporting the mental well-being of learners.</li>
<li>Collecting and Utilising Feedback: Consulting with and collecting feedback from researchers, teachers, and learners on GenAI tools, and then utilising this feedback for informed decision-making, ensures that the deployment of GenAI is in line with the needs and preferences of the educational community.</li>
<li>Maintaining Human Accountability: Maintaining human accountability when making high-stakes decisions will help ensure that ethical and responsible actions are taken, reinforcing the centrality of human agency in the educational process.</li></ul></li>
<li>
<p><b>Monitor and Validate GenAI Systems for Education</b></p>
<p>This measure of monitoring and validating GenAI systems for education emphasises the importance of ensuring the ethical and pedagogical soundness of GenAI throughout its advancements. The measure proposes a framework with which GenAI applications can be scrutinised for potential biases, ethical risks, and their impact on students, teachers, and the broader educational ecosystem. The goal is to establish mechanisms that ensure GenAI applications are aligning with educational standards, promote fairness, and are devoid of harmful content. Additionally, this measure stresses the importance of informed consent, especially when engaging vulnerable populations like children, and the need for a strict ethical validation before the official adoption of GenAI applications in educational or research institutions.</p>
<p class="list-title"><i>Recommendations include</i>:</p>
<ul class="disc">
<li>Building Validation Mechanisms: By building validation mechanisms to test GenAI systems for potential biases and the representativeness of the data used to train them, we can better ensure that their applications are fair, inclusive, and reflective of the learner population&#x2019;s diversity.</li>
<li>Addressing Informed Consent: To ensure ethical engagement with GenAI systems, addressing the complex issue of informed consent is crucial, particularly in contexts where children and other vulnerable users may not be capable of genuinely and fully providing informed consent.</li>
<li>Auditing GenAI Outputs: Auditing whether the outputs of GenAI include deceptive or harmful material, including deepfake images, fake news, or hate speech, is essential to maintain a safe and truthful educational environment and to take swift corrective actions if inappropriate content is generated.</li>
<li>Ethical Validation: Enforcing the strict ethical validation of GenAI applications before their official adoption in educational or research institutions will help to ensure that they conform to ethical and pedagogical standards.</li>
<li>Ensuring Educational Effectiveness: Ensuring that GenAI applications do no predictable harm, are educationally effective, and are aligned with sound pedagogical principles is crucial for fulfilling educational objectives and safeguarding the well-being of learners.</li></ul></li>
<li><span epub:type="pagebreak" id="p143" aria-label=" page 143. " role="doc-pagebreak"/><b>Develop AI Competencies Including GenAI-related Skills for Learners</b>
<p>The measure proposes the development of government-endorsed AI curricula that will cover ethical issues, understanding of algorithms, and proper use of AI tools and applications at various levels of education, including in technical and vocational training. Such curricula should promote gender equality in learners&#x2019; development of AI competencies, as well as enhance learners&#x2019; future-proof skills in response to the evolving job market as driven by GenAI advancements and automation. This measure also emphasises the importance of supporting higher education and research institutions in developing local AI talent, and providing special programmes for older workers and citizens to help them adapt to the new technological landscape.</p>
<p class="list-title"><i>Recommendations include</i>:</p>
<ul class="disc">
<li>Government-Sanctioned AI Curricula: Committing to the provision of government-sanctioned AI curricula across different levels of education and lifelong learning platforms will be crucial for building a foundational and level-appropriate understanding of AI technologies, ethics, and their impact.</li>
<li>Supporting Higher Education and Research Institutions: Supporting higher education and research institutions in enhancing their programmes will also help to develop local AI talent.</li>
<li>Promoting Gender Equality: Promoting gender equality when developing learners&#x2019; advanced AI competencies will help in creating a pool of professionals which is better gender-balanced.</li>
<li>Developing Intersectoral Forecasts for the Changing Job Market: By developing intersectoral forecasts that predict job shifts caused by GenAI automation, as well as prioritising the enhancement of future-proof skills at all levels of education, we can better ensure that learners are well-prepared for the evolving job market.</li>
<li>Providing Special Programmes for Older Workers: To ensure that the benefits of AI are accessible to all, regardless of age, it will be necessary to provide special programmes designed for older workers and citizens who may need to learn new skills and support in adapting to new technological environments.</li></ul></li>
<li><b>Build Capacity for Teachers and Researchers to Make Proper Use of GenAI</b>
<p>This measure underscores the necessity of equipping teachers and researchers with the requisite knowledge and skills to utilise GenAI effectively and responsibly. According to the UNESCO guidance, only several countries have developed or are in the process of developing frameworks and training programmes on AI for teachers. This indicates a significant gap in access to training and support. As such, this measure outlines four actions needed to better prepare teachers around the world to use GenAI, including to formulate guidance based on local tests, protect the rights of teachers and researchers as well as value their GenAI practices, define the required value orientation, knowledge, and skills for teachers, and dynamically review and promote the emerging competencies that teachers will need to understand and utilise AI in their professional practices.</p>
<p class="list-title"><i>Recommendations include</i>:</p>
<ul class="disc">
<li>Formulating or Adjusting Guidance: Provide help for teachers and researchers to navigate widely available GenAI tools by formulating or adjusting guidance based on <span epub:type="pagebreak" id="p144" aria-label=" page 144. " role="doc-pagebreak"/>local tests and evaluations, as well as supporting the design of new domain-specific AI applications.</li>
<li>Protecting the Rights of Teachers and Researchers: Human teachers and researchers both have unique roles that should be appreciated, such as their facilitation of interpersonal interactions and making of innovative contributions to knowledge. By protecting the rights of teachers and researchers and valuing their practices when using GenAI, the integrity and quality of educational and research processes can be upheld.</li>
<li>Defining Value Orientation, Knowledge, and Skills: To ensure that teachers understand and use GenAI systems effectively and ethically, thereby also contributing to the responsible integration of GenAI systems in education, it is important to define the value orientation(s), knowledge, and skills that are needed to do so.</li>
<li>Dynamically Reviewing Competencies: By taking on a dynamic approach when reviewing the competencies needed by teachers to understand and use AI for teaching, learning, and professional development, we can better ensure that educators are well-prepared to adapt to the evolving technological landscape within education.</li></ul></li>
<li><b>Promote Plural Opinions and Plural Expressions of Ideas</b>
<p>This measure discusses the promotion of critical thinking, encouraging learners to critique GenAI responses, and recognising the latter&#x2019;s limitations in its reproduction of dominant worldviews that consequently undermines minority opinions. It emphasises the importance of promoting diversity in opinions and expressions, which GenAI may inadvertently suppress due to its tendency to regurgitate dominant or mainstream views that are present in the data it was trained on. By fostering a culture of critical engagement with GenAI outputs and encouraging empirical, trial-and-error learning approaches, this measure aims to preserve and promote pluralism and a diversity of ideas in education and research environments.</p>
<p class="list-title"><i>Recommendations include</i>:</p>
<ul class="disc">
<li>Critiquing GenAI Responses: It is important to encourage learners and researchers in recognising that GenAI typically repeats established or standard opinions, thus undermining plural and minority opinions and ideas; to do so, they must be able to critique the responses and outputs they receive from GenAI.</li>
<li>Providing Empirical Learning Opportunities: To foster a rich, exploratory learning environment that does not overly rely on GenAI, it is important to also provide learners with sufficient opportunities to learn from hands-on methods, including through trial-and-error, empirical experiments, and observations of the real world.</li></ul></li>
<li><b>Test Locally Relevant Application Models and Build a Cumulative Evidence Base</b>
<p>This measure underscores the importance of tailoring GenAI applications to local needs and contexts, especially given the predominance of data from the Global North in training these systems and models. By fostering a strategic, evidence-based approach to the design, adoption, and evaluation of GenAI tools, this measure aims to encourage innovation, assess the social and ethical implications of GenAI, and build a robust evidence base that reflects diverse educational priorities and pedagogical principles that are both widely collaborative and still relevant to local needs. By doing so, GenAI can be leveraged more effectively to support inclusive learning opportunities, promote linguistic and cultural diversity, and address the environmental costs of large-scale AI deployments.</p>
<p class="list-title"><i><span epub:type="pagebreak" id="p145" aria-label=" page 145. " role="doc-pagebreak"/>Recommendations include</i>:</p>
<ul class="disc">
<li>Strategic Planning of GenAI Design and Adoption: The design and adoption of GenAI must be strategically planned, and should do more than just facilitating passive, non-critical processes.</li>
<li>Incentivising Diverse Learning Options: To support inclusivity and diversity, it is important to incentivise the designers and developers of GenAI to prioritise open-ended, exploratory, and diverse learning options.</li>
<li>Testing and Scaling Evidence-based Use Cases: Use cases of AI&#x2019;s applications in education and research should be tested and scaled up, done so in accordance with educational priorities and not due to novelty, myth, or hype.</li>
<li>Triggering Innovation in Research: GenAI can be leveraged to stimulate innovation in research, including through making use of its computing capabilities, using GenAI to assist with processing large-scale data, and using its outputs to inform, inspire, and improve research and research methodologies.</li>
<li>Reviewing Social and Ethical Implications: It is important to conduct comprehensive reviews of the social and ethical implications of integrating GenAI into research processes.</li>
<li>Building an Evidence Base: To establish an evidence base on the effectiveness of GenAI in promoting inclusivity and diversity in learning and research, it is crucial to establish specific criteria derived from pedagogical research and methodologies that are supported by evidence.</li>
<li>Strengthening Evidence on Social and Ethical Impact: Iterative steps must be taken to enhance the evidence of the social and ethical impacts of GenAI.</li>
<li>Analysing Environmental Costs: It is essential to analyse the environmental impacts of implementing AI technologies at scale, such as the energy and resources needed for training GPT models. In doing so, it is also crucial to set sustainable targets for AI providers to meet to mitigate the potential contribution of AI to climate change.</li></ul></li>
<li><b>Review Long-term Implications in Intersectoral and Interdisciplinary Manner</b>
<p>This measure emphasises the necessity of a multi-disciplinary and multi-sectoral approach in evaluating the long-term implications of GenAI in education and research. By fostering collaboration among AI providers, educators, researchers, as well as also other involving stakeholders like parents and students, the measure aims to foster both a comprehensive understanding of and the means to effectively address any challenges that may arise in the future. This collaborative approach further seeks to make necessary system-wide adjustments in curriculum frameworks and assessment methodologies to fully leverage GenAI&#x2019;s potential, while simultaneously mitigating its risks. It emphasises the importance of a diverse range of expertise in examining the long-term implications of GenAI on learning, research, human collaboration, and social dynamics.</p>
<p class="list-title"><i>Recommendations include</i>:</p>
<ul class="disc">
<li>Collaborative Planning for System-wide Adjustments: To fully leverage the potential of GenAI for education and research while also minimising the associated risks it poses to these areas, it is crucial to plan and implement system-wide adjustments in curriculum frameworks and assessment methodologies. This process should be undertaken collaboratively by AI providers, educators, researchers, and representatives of parents and students.</li>
<li><span epub:type="pagebreak" id="p146" aria-label=" page 146. " role="doc-pagebreak"/>Intersectoral and Interdisciplinary Expertise: In evaluating the long-term implications of GenAI on areas including learning and knowledge production, research and copyright, curriculum and assessment, and human collaboration and social dynamics, it is important to bring together a diverse range of experts from various sectors and disciplines. This includes educators, researchers, learning scientists, AI engineers, and other relevant stakeholders.</li>
<li>Provision of Timely Advice: To inform the ongoing and iterative updates of AI regulations and policies, timely advice and guidance should be provided.</li></ul></li></ol>
<p>To summarise the above, these eight measures by UNESCO all aim to provide a structured approach towards integrating GenAI in education and research while also addressing the associated ethical, social, and technical challenges.</p></section></section>
<section aria-labelledby="sec5_5">
<h2 id="sec5_5"><span epub:type="ordinal">5.5 </span>Research Findings from the Perception of Students, Teachers and Staff on GenAI in Education Policy in Hong Kong</h2>
<p>To understand what is needed for an AI policy in higher education, we sought to explore the different views of various stakeholders in February 2023, shortly after the public release of ChatGPT. This involved examining the perspectives of students, teachers, and university staff concerning the need for an AI policy in education, especially for the higher education context where the spread and impact of AI technologies is irrefutable. While numerous governments are/were in the process of formulating AI guidelines, the predominant focus of these policies veered towards national and international strategies, with inadequate attention towards education. Most of the guidelines, as discussed in <a href="#sec5_3">Section 5.3</a>, primarily address the ethical management of AI technologies, emphasised &#x201C;the standards of right and wrong, acceptable and not acceptable&#x201D; (<a href="#ref5_25" id="R_ref5_25" epub:type="biblioref" role="doc-biblioref">Hogenhout, 2021</a>, p. 11), highlighting global concerns such as AI-induced discrimination, privacy invasions, human rights violations, and malicious AI use (<a href="#ref5_24" id="R_ref5_24" epub:type="biblioref" role="doc-biblioref">Greiman, 2021</a>; <a href="#ref5_25" epub:type="biblioref" role="doc-biblioref">Hogenhout, 2021</a>), further underscoring the potential of AI misuse to foster social divisions, manipulate individuals, and aggravate inequalities, posing a profound threat to humanity (<a href="#ref5_15" id="R_ref5_15" epub:type="biblioref" role="doc-biblioref">Federspiel et al., 2023</a>). The existing guidelines are still quite broad and, as mentioned, do not adequately extend to the education sector; this points to the pressing need to shed light on the challenges and advantages that are encountered by stakeholders in higher education.</p>
<section aria-labelledby="sec5_5_1">
<h3 id="sec5_5_1"><span epub:type="ordinal">5.5.1 </span>Methods</h3>
<p>Quantitative and qualitative data were collected through a survey administered to universities in Hong Kong (<a href="#ref5_6" id="R_ref5_6" epub:type="biblioref" role="doc-biblioref">Chan, 2023</a>). A total of 457 students and 180 teachers and staff responses were collected. To understand their usages and perceptions of generative AI technologies, including ChatGPT, in higher education, descriptive analysis was used for the quantitative data and thematic analysis for the open-ended responses. Participants were asked about their experiences with ChatGPT or similar tools, and how they saw these technologies in relation to their educational practices.</p>
<p>The descriptive analysis summarised the main traits of the quantitative data, providing an overview of response tendencies, while thematic analysis of open-ended questions revealed patterns and themes regarding the integration of generative AI technologies into higher education, as well as suggestions for how university should build their strategic plans.</p>
<p><span epub:type="pagebreak" id="p147" aria-label=" page 147. " role="doc-pagebreak"/>The mix of quantitative and qualitative data helped to provide a well-rounded understanding of stakeholder perceptions, allowing us to identify potential needs, recommendations, and strategies for AI policy development in teaching and learning at universities, ensuring the ethical and advantageous use of these technologies.</p></section>
<section aria-labelledby="sec5_5_2">
<h3 id="sec5_5_2"><span epub:type="ordinal">5.5.2 </span>Quantitative Findings</h3>
<p>One survey question looked at participants&#x2019; opinions on AI policy: whether students, teachers, and staff believed that there should be established plans and a dedicated policy for AI technologies and their use within the university. The results were encouraging, showing a strong consensus that institutions should indeed have such plans (students: <i>M</i> = 4.50, <i>SD</i> = .85; teachers and staff: <i>M</i> = 4.54, <i>SD</i> = .87; responses were based on a 5-point Likert scale, with &#x201C;5&#x201D; indicating &#x201C;Strongly agree&#x201D;). Responses to other questions further indicated that students and teachers are aware of the possible advantages and disadvantages associated with AI technologies. They also acknowledged the potential of using GenAI for guidance, personalised feedback, enhancing digital skills, and improving academic performance, along with its benefits of offering anonymity in student support services. However, apprehensions about excessive reliance on AI, reduced social engagement, and a possible impediment to the cultivation of generic skills were also expressed.</p></section>
<section aria-labelledby="sec5_5_3">
<h3 id="sec5_5_3"><span epub:type="ordinal">5.5.3 </span>Qualitative Findings</h3>
<p>The qualitative data also provided insightful information for the establishment of a well-rounded AI policy in higher education. Ten key areas were found, grouped into three main dimensions to form the AI Ecological Education Policy Framework.</p>
<section aria-labelledby="sec5_5_3_1">
<h4 id="sec5_5_3_1"><span epub:type="ordinal">5.5.3.1 </span>Governance Dimension (Senior Management)</h4>
<p>This dimension emphasises considerations for the governance of AI usage in education, including establishing the necessary policies, guidelines, and ethical standards to ensure that the adoption of AI is beneficial, fair, and secure. It encompasses the following key areas:</p>
<ol type="1">
<li>Understanding, identifying, and preventing academic misconduct and ethical dilemmas;</li>
<li>Addressing governance of AI in terms of data privacy, transparency, accountability, and security;</li>
<li>Attribution for AI technologies; and</li>
<li>Ensuring equity in access to AI technologies.</li></ol></section>
<section aria-labelledby="sec5_5_3_2">
<h4 id="sec5_5_3_2"><span epub:type="ordinal">5.5.3.2 </span>Operational Dimension (Teaching and Learning and IT Staff)</h4>
<p>This dimension concentrates on the practical implementation of AI in university settings and to ensure that such implementations are effective, reliable, and supported by adequate training and resources. It includes the following key areas:</p>
<ol type="1">
<li>Monitoring and evaluating AI implementation; and</li>
<li>Providing training and support for teachers, staff, and students to ensure that they are AI literate.</li></ol></section>
<section aria-labelledby="sec5_5_3_3">
<h4 id="sec5_5_3_3"><span epub:type="pagebreak" id="p148" aria-label=" page 148. " role="doc-pagebreak"/><span epub:type="ordinal">5.5.3.3 </span>Pedagogical Dimension (Teachers)</h4>
<p>This dimension focuses on the teaching and learning aspects of AI integration and the creation of an educational environment that both leverages AI technologies and prepares students for a future where such technologies are prevalent. It includes the following key areas:</p>
<ol type="1">
<li>Rethinking assessments and examinations;</li>
<li>Developing students&#x2019; holistic competencies/generic skills;</li>
<li>Preparing students for the AI-driven workplace; and</li>
<li>Encouraging a balanced approach to AI adoption</li></ol>
<p>For the full research paper, please refer to Chan, C.K.Y. (<a href="#ref5_6" epub:type="biblioref" role="doc-biblioref">2023</a>) A Comprehensive AI Policy Education Framework for University Teaching and Learning<i>. International Journal of Educational Technology in Higher Education</i>. DOI: <a href="https://doi.org/10.1186/s41239-023-00408-3" aria-label="D.O.I. link to International Journal of Educational Technology in Higher Education">10.1186/s41239-023-00408-3</a>.</p></section></section>
<section aria-labelledby="sec5_5_4">
<h3 id="sec5_5_4"><span epub:type="ordinal">5.5.4 </span>The AI Ecological Education Policy Framework</h3>
<p>The AI Ecological Education Policy framework (<a href="#ref5_6" epub:type="biblioref" role="doc-biblioref">Chan, 2023</a>) contains the aforementioned three dimensions to guide higher education in developing strategic plans to tackle the challenges of the AI era as shown in <a href="#fig5_3" id="R_fig5_3">Figure 5.3</a>. Each dimension is explained in further detail below.</p>
<figure id="fig5_3"><img src="images/fig5_3_B.jpg" alt="The A I ecological education policy framework has the governance dimension, operational dimension, and pedagogical dimension."/>
<figcaption><a href="#R_fig5_3" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. "><span epub:type="label">Figure</span> <span epub:type="ordinal">5.3 </span>AI Ecological Education Policy Framework.</a></figcaption></figure>
<p>Senior management holds significant responsibility in the <b>governance dimension</b> as they are instrumental in policy formulation, the establishment of ethical guidelines, and overall <span epub:type="pagebreak" id="p149" aria-label=" page 149. " role="doc-pagebreak"/>institutional direction of AI integration. They ensure that their respective institutions will operate within the legal and ethical boundaries while striving for academic excellence and integrity. Their role is crucial in understanding, identifying, and preventing academic misconduct and ethical issues, addressing the governance of AI concerning data privacy, transparency, accountability, and security, the requirements to provide attribution to AI technologies, and ensuring equity in the access to AI technologies. Their decisions and policies set the tone for how AI will be embraced within the institution, impacting both pedagogical and operational dimensions. Within this governance dimension, there are four underlying key areas:</p>
<ol type="1">
<li>
<p>Understanding, Identifying, and Preventing Academic Misconduct and Ethical Dilemmas</p>
<p>The potential of AI technologies to be used for academic misconduct necessitates a robust governance framework to address this concern. Developing and enforcing policies on academic integrity and ethical AI use are crucial in promoting and maintaining a culture of integrity within the academic community. By addressing the governance of AI head-on, senior management can create a more conducive environment for the ethical use and integration of AI, mitigating its risks in relation to academic misconduct.</p></li>
<li>
<p>Addressing Governance of AI: Data Privacy, Transparency, Accountability, and Security</p>
<p>Effective governance to ensure data privacy, transparency, accountability, and security is paramount for the ethical and secure utilisation of AI. Establishing clear guidelines and policies for AI governance will help to address potential ethical dilemmas and allow for a more structured approach towards AI integration. The impact of these actions will be profound, contributing to enhanced trust, compliance, and responsible AI usage within the academic community.</p></li>
<li>
<p>Attributing AI Technologies</p>
<p>Given that AI technologies can and likely will be used by students in their academic work, it is necessary to establish clear guidelines for the proper attribution and recognition of AI-generated content. Doing so can further promote academic honesty and ensure clarity in the acknowledgment of how AI was used. This, in turn, can foster a culture of transparency and honesty within the academic community. <a href="#fig5_4" id="R_fig5_4">Figure 5.4</a> shows an example of how to write an attribution to acknowledge AI-generated text.</p>
<figure id="fig5_4"><img src="images/fig5_4_B.jpg" alt="An example of the attribution for Gen A I generated content."/>
<figcaption><a href="#R_fig5_4" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. "><span epub:type="label">Figure</span> <span epub:type="ordinal">5.4 </span>An Example of Attribution of GenAI Generated Content.</a></figcaption></figure></li>
<li>Ensuring Equity in Access to AI Technologies
<p>Disparity in students&#x2019; access to AI technologies will exacerbate existing inequalities. It is thus crucial to implement measures for ensuring equitable access to AI resources as a critical action for promoting fairness and inclusivity. This could involve providing access <span epub:type="pagebreak" id="p150" aria-label=" page 150. " role="doc-pagebreak"/>to AI tools within the institution or creating support structures for students to learn and engage with AI technologies in a responsible manner.</p>
<p>Teaching and learning Support and IT staff play a significant role in the <b>operational dimension</b> as they are responsible for the practical implementation, monitoring, and provision of support for AI technologies. They ensure that any AI technologies integrated into the curricula are reliable, effective, and used responsibly. Their role in monitoring and evaluating AI implementation is critical for continuous improvement and upholding alignment with institutional goals. Additionally, they provide the necessary training and support for teachers, staff, and students, ensuring that all stakeholders are well-equipped with AI literacy and are capable of effectively navigating the complex landscape of AI in academia. Within the operational dimension, there are two underlying key areas:</p>
<ol type="1">
<li>Monitoring and Evaluating AI Implementation
<p>Adopting AI technologies into academic settings requires a structured approach to monitoring and evaluating AI implementation. Regular monitoring can ensure that the AI tools in question are effective, reliable, and being used responsibly. This operational facet is crucial for both facilitating continuous improvement and ensuring that the practical implementation of AI technologies aligns with the ethical and pedagogical objectives of the institution.</p></li>
<li>Providing Training and Support for Teachers, Staff, and Students in AI Literacy
<p>The need for AI literacy among stakeholders is a pressing concern. Developing and delivering training programmes on AI literacy, ethics, and effective use can significantly improve understanding and responsible use of AI technologies among teachers, staff, students, and so on. This operational action is vital for ensuring that all stakeholders are well-informed and equipped to navigate the complex landscape of AI in academia.</p></li></ol>
<p>Teachers play a key role in the <b>pedagogical dimension</b> as they are at the forefront of delivering education and engaging with students. With their first-hand experiences and expertise in teaching and learning processes, they are uniquely positioned to explore, understand, and implement AI technologies in a manner that enhances learning and the achievement of educational outcomes. Their insights into students&#x2019; learning needs, as well as the challenges and potential benefits of AI technologies in relation to teaching and learning, are invaluable for rethinking assessments, developing students&#x2019; competencies, preparing students for the AI-driven workplace, and encouraging the balanced adoption of AI. Teachers are the primary actors in translating policy and operational frameworks into effective teaching and learning practices. Within the pedagogical dimension, there are four underlying key areas:</p>
<ol type="1">
<li>Rethinking Assessments and Examinations
<p>The advent of AI technologies including ChatGPT presents both challenges and opportunities for academic assessments. As previously mentioned, a key concern that arises is the potential misuse of AI in facilitating academic misconduct. Actions to address this could include the redesign of assessments to include varied methods, such as in-class demonstrations, presentations, or multi-stage submissions, which could mitigate the opportunities to engage in AI-assisted misconduct. The outcome of such actions could also improve the accuracy of assessment looking at students&#x2019; understanding and skills, reduce academic misconduct, and potentially facilitate a higher level of student engagement.</p></li>
<li><span epub:type="pagebreak" id="p151" aria-label=" page 151. " role="doc-pagebreak"/>Developing Student Holistic Competencies/Generic Skills
<p>As AI technologies become progressively integrated across all sectors of society, the necessity for students to develop holistic competencies becomes paramount. These competencies range from critical thinking and leadership abilities to self-reflection and creative problem solving &#x2013; skills that are not easily replicated by AI. By introducing second-order writing tasks and promoting critical evaluation, educators can foster a deeper level of understanding and skills development among students. The outcomes would be manifold, including better-prepared students who can think critically and navigate a tech-driven academic and professional landscape.</p></li>
<li>Preparing Students for the AI-Driven Workplace
<p>The evolving nature of the workplace combined with the integration of AI technologies requires a forward-thinking approach to prepare students for their future careers. Familiarising students with AI technologies, ethical considerations, and real-world applications can facilitate a more seamless transition from academia to the professional world. The actions in this domain could include integrating AI-related topics into the curriculum, engaging in discussions about the ethical implications of AI, and providing hands-on experiences with AI tools. The expected outcomes of this are future cohorts of students who are better prepared for the demands of an AI-driven workplace.</p></li>
<li>Encouraging a Balanced Approach to AI Adoption
<p>A balanced approach to AI adoption in academia can help us navigate the fine line between leveraging technology and maintaining academic integrity. By promoting a such an approach, educators can enhance teaching and learning experiences while mitigating the risks associated with AI technologies. This could also foster a more positive attitude towards technological evolution, in turn further leading to innovation in teaching and learning methods that are in sync with the advancements in technology.</p></li></ol></li></ol>
<p>It is imperative to acknowledge that the responsibilities of each dimension in this ecological framework should not be viewed in isolation. The relationships among the three dimensions are intricate and interdependent. For instance, effective governance policies (Governance Dimension) could support the ethical integration of AI in teaching and learning (Pedagogical Dimension), and well-informed operational practices (Operational Dimension) could ensure the success of such integration. By taking into account these dimensions, a cohesive theory regarding the integration of AI in academic settings can be further developed and refined.</p>
<p>The successful integration of AI in academic settings counts on collaboration and communication among all stakeholders, including universities, teachers, students, staff, and external agents such as accreditation and quality assurance bodies. Each group should actively participate in the development and execution of AI-related initiatives, working together to achieve the desired outcomes in university teaching and learning. This collaborative approach will foster a more comprehensive, informed, and ethical integration of AI, aligning with broader educational goals and ethical standards. Overall, through open communication and collaborative efforts, stakeholders can collectively navigate the complexities of AI integration, ensuring that the pedagogical, governance, and operational dimensions are also harmoniously in alignment for the betterment of the academic community.</p></section></section>
<section aria-labelledby="sec5_6">
<h2 id="sec5_6"><span epub:type="pagebreak" id="p152" aria-label=" page 152. " role="doc-pagebreak"/><span epub:type="ordinal">5.6 </span>Devising an AI Policy in Higher Education</h2>
<p>Creating an AI policy in higher education is an extensive process that requires a well-thought-out approach. It should ideally follow a structured methodology to ensure that all stakeholders&#x2019; perspectives are considered and that the policy is comprehensive, ethical, and effective in achieving the desired objectives. The intricacies of formulating such an AI policy in such a dynamic environment often extend beyond the existing expertise of teachers, administrative staff, and even senior management. The lack of familiarity with the nuanced domain of AI further exacerbates the complexity of devising a robust policy tailored to the educational milieu.</p>
<p>This section seeks to explain and clarify the process of formulating AI policy within higher education institutions, based on the above framework. We offer a structured, step-by-step guide on how such a policy can be crafted and implemented, providing a straightforward pathway for stakeholders at all levels to engage in the policymaking process. This draft is simplified and may serve as a starting point or reference for further development by your institution. As a pragmatic approach, it can be used as a roadmap for policy development as well as to foster a collaborative ethos, encouraging inclusive dialogue among all stakeholders to leverage AI technologies effectively and ethically in education, thus enabling a smooth transition into the AI-augmented teaching and administrative paradigm. Below is the step-by-step guideline based on the ecological framework&#x2019;s dimensions of governance, pedagogy, and operations:</p>
<ol class="decimal">
<li>
<p><b>Initiation and Planning</b></p>
<ol class="lower-alpha">
<li>
<p>Establish a Steering Committee: The first step towards developing an AI policy in education is the formation of a Steering Committee. Senior management should spearhead this initiative by identifying and inviting individuals from different stakeholder groups to be part of this committee, including faculty members, IT staff, administrative staff, legal advisors, and student representatives. Ideally, the committee should have balanced representation to ensure that diverse perspectives and expertise are incorporated in the policy development process. An initial meeting should be convened where the objectives of the policy are outlined, roles and responsibilities are assigned, and a tentative timeline is established. This committee will serve as the driving force behind the policy development, acting as the conduit between the stakeholder groups and the development process.</p>
<p><b>Who</b>: Senior management should establish a Steering Committee composed of representatives from each stakeholder group (including senior management themselves, teachers, IT staff, administrative staff, legal advisors, students, and external experts).</p>
<p><b>How</b>: The formation of the committee can be announced through official channels, and nominations or volunteers could be solicited with the goal of having balanced representation.</p></li>
<li>
<p>Conduct a Needs Assessment: A Needs Assessment is crucial as it lays the foundation for the policy by identifying the current state of policy, the desired state, and the gaps in between. This involves designing and distributing surveys to gather insights from stakeholders on their understanding, expectations, and concerns regarding the integration of AI into education. Simultaneously, focus group discussions can be conducted to delve deeper into specific issues or concerns. An analysis of the existing technological infrastructure and educational practices can also provide a clear <span epub:type="pagebreak" id="p153" aria-label=" page 153. " role="doc-pagebreak"/>picture of the current capabilities of these areas while also identifying those that may require improvement. The data collected through these methods should be carefully analysed to identify the core needs and challenges that the AI policy should address.</p>
<p><b>Who</b>: The Steering Committee, with the help of educational and technical experts.</p>
<p><b>How</b>: Through surveys, focus groups, and analyses of existing infrastructure and educational needs.</p></li>
<li>
<p>Define Objectives and Scope: After conducting the Needs Assessment, the Steering Committee should engage in defining the objectives and scope of the AI policy. This could involve a thorough review of existing literature, benchmarking against the AI policies of other institutions, and consulting legal and ethical guidelines concerning AI in education. The objectives should be clear, measurable, and aligned with the broader educational goals of the institution, while the scope should define the boundaries of the AI policy, including the areas it will cover, the stakeholders it will affect, and the resources it will require. This step is essential as it sets the direction for policy development and ensures that the process remains focused and aligned with the identified needs.</p>
<p><b>Who</b>: The Steering Committee.</p>
<p><b>How</b>: Reviewing the Needs Assessment findings and consulting with stakeholders to set clear objectives for the AI policy.</p></li></ol></li>
<li>
<p><b>Stakeholder Engagement</b></p>
<ol class="lower-alpha">
<li>
<p>Solicit Input and Feedback: Engaging stakeholders is necessary for the development of a well-rounded and inclusive AI policy. The Steering Committee should establish communication channels such as online forums (see <a href="#fig5_5" id="R_fig5_5">Figure 5.5</a> for an example), email groups, and hold physical or virtual meetings to solicit the input and feedback of all stakeholder groups. This engagement should be continuous, allowing parties share their insights, concerns, and suggestions regarding the integration of AI into education. The input and feedback collected should also be documented and analysed to understand broader implications, as well as to ensure that the policy will address the concerns and needs raised.</p>
<figure id="fig5_5"><img src="images/fig5_5_B.jpg" alt="Details that are listed for the question, what should be in our A I education policy which is an example of a padlet for online forum."/>
<figcaption><a href="#R_fig5_5" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. "><span epub:type="label">Figure</span> <span epub:type="ordinal">5.5 </span>An Example of Using a Padlet for Online Forum.</a></figcaption></figure>
<p><b>Who</b>: The Steering Committee should lead this effort, reaching out to all stakeholder groups.</p>
<p><b>How</b>: Through online surveys, in-person or virtual town-hall meetings, and focus group discussions to gather insights, concerns, and suggestions regarding the integration of AI into education.</p></li>
<li>
<p>Educate Stakeholders: Equipping stakeholders with a good understanding of AI and its potential implications in education is essential for facilitating meaningful engagement. This can be achieved through workshops, seminars, and informational sessions conducted by experts within or outside the Steering Committee. Educational materials explaining AI, its applications in education, and its ethical implications should be developed and distributed. By doing so, stakeholder discussions will be better informed, enhancing the quality and depth of their feedback and engagement in the policy development process.</p>
<p><b>Who</b>: Experts within the Steering Committee or external consultants.</p>
<p><b>How</b>: Conducting workshops, seminars, and informational sessions to educate stakeholders about AI and its potential implications in education, including ethical considerations.</p></li></ol></li>
<li>
<p><b>Policy Development<span epub:type="pagebreak" id="p154" aria-label=" page 154. Figure on this page. " role="doc-pagebreak"/></b></p>
<ol class="lower-alpha">
<li><span epub:type="pagebreak" id="p155" aria-label=" page 155. " role="doc-pagebreak"/>Drafting the Policy: With the groundwork laid, the process now moves onto drafting the policy. A working group within the Steering Committee, possibly with the inclusion of legal and technical experts, should be formed to draft the AI policy. This Policy Drafting Team should work to integrate the insights gathered from stakeholders, information from legal and ethical guidelines, and the educational objectives identified earlier. The drafting process should be iterative, allowing for revisions based on feedback from the Steering Committee and consultations with other experts.
<p><b>Who</b>: A working group within the Steering Committee, with assistance from legal and technical experts.</p>
<p><b>How</b>: Establishing a comprehensive draft policy by integrating insights from stakeholders, referring to legal and ethical guidelines, and ensuring alignment with educational objectives.</p></li>
<li>
<p>Stakeholder Review: Once a draft policy is prepared, it should be shared with all stakeholder groups for review and feedback. Various channels can be used to disseminate the draft policy and collect feedback, including through online platforms, emails, or physical meetings. Stakeholders should be given a reasonable time frame to review the draft and provide their inputs. This step ensures that the draft policy is vetted by the broader community, enhancing its inclusivity and relevance.</p>
<p><b>Who</b>: All stakeholder groups.</p>
<p><b>How</b>: Sharing the draft policy for review and feedback through various channels such as online platforms, meetings, and emails.</p></li>
<li>
<p>Revision and Finalisation: The feedback collected from stakeholders should be analysed and necessary revisions should be made to the draft policy. The Policy Drafting Team should engage in an iterative process of revision and consultation with the Steering Committee, ensuring that the policy is appropriately refined and still aligned with the defined objectives and scope. Once revisions have been made and the draft is finalised, the policy should be reviewed one final time by the Steering Committee to confirm that it is ready for submission to senior management and other governing bodies for approval.</p>
<p><b>Who</b>: The working group within the Steering Committee.</p>
<p><b>How</b>: Incorporating the feedback received, revising the policy as necessary, and finalising the draft for approval.</p></li></ol></li>
<li>
<p><b>Approval and Adoption</b></p>
<ol class="lower-alpha">
<li>
<p>Seek Approval: The Steering Committee, having finalised the draft policy, now moves to seek formal approval from the senior management and other governing bodies within the institution. This usually involves preparing a detailed presentation outlining the key aspects of the policy, the process undertaken to develop it, and the implications of its implementation. It is crucial to articulate how the policy aligns with the broader educational goals of the institution, as well as how it adheres to legal and ethical standards. The presentation also provides an opportunity for the committee to address any issues or concerns raised by the senior management, and potentially make further revisions to the policy based on the feedback received. The approval process ensures that the AI policy has the necessary endorsement from the institutional leadership, paving the way for its official adoption.</p>
<p><b>Who</b>: The Steering Committee should present the policy to senior management and other necessary governing bodies for approval.</p>
<p><b>How</b>: Through formal presentations and submission of the policy document for review.</p></li>
<li><span epub:type="pagebreak" id="p156" aria-label=" page 156. " role="doc-pagebreak"/>Official Adoption: Upon receiving approval, the next step is the official adoption of the AI policy. This is typically announced through official channels such as institutional bulletins, emails, and meetings. The policy document should be disseminated to all stakeholders, ensuring they have access to it and understand its implications. It is advisable to have an official launch event or announcement that highlights the key aspects of the policy and what it means for the stakeholders. This step marks the formal recognition of the AI policy as a guideline within the institution, setting the stage for its implementation.
<p><b>Who</b>: Senior management.</p>
<p><b>How</b>: Announcing the adoption of the policy through official channels and disseminate the policy document to all stakeholders.</p></li></ol></li>
<li>
<p><b>Implementation</b></p>
<ol class="lower-alpha">
<li>
<p>Develop an Implementation Plan: With the policy officially adopted, the need now is to develop a concrete implementation plan. It should outline the steps, timelines, resources, and responsibilities for implementing the policy, as well as identify the necessary support structures for ensuring that the process is smooth. This could include training programmes, resource allocations, and the establishment of support channels for stakeholders to seek help or report issues. The implementation plan serves as the blueprint for translating the policy into practice, ensuring that all stakeholders are well-prepared and supported in adhering to the new AI policy.</p>
<p><b>Who</b>: The Steering Committee, in collaboration with operational staff.</p>
<p><b>How</b>: Outlining the steps, timelines, resources, and responsibilities for implementing the policy.</p></li>
<li>
<p>Training and Support: Training and support are critical components of the implementation phase. Training programmes should be developed and delivered to stakeholders to equip them with the necessary skills and knowledge to comply with the AI policy. This could include training on new technologies, ethical considerations, and best practices for AI integration in education. Support channels should also be established to provide ongoing assistance to stakeholders, ensuring they have the resources and help necessary to navigate the new policy and its effects. These support channels could include helpdesks, online forums, and dedicated support personnel.</p>
<p><b>Who</b>: IT staff and educational experts.</p>
<p><b>How</b>: Conducting training sessions, workshops, and provide resources to support stakeholders in adhering to the new AI policy.</p></li>
<li>
<p>Monitoring and Evaluation: The implementation phase should also include a robust monitoring and evaluation component. This involves establishing clear metrics and procedures to monitor the implementation process, assess compliance with the policy, and evaluate the effectiveness of the policy in achieving the desired objectives. Regular reports should be written up and submitted to provide insights into the progress of the implementation, any challenges encountered, and the impact of the policy on educational practices.</p>
<p><b>Who</b>: A designated monitoring and evaluation team.</p>
<p><b>How</b>: Establish metrics and procedures to monitor the implementation, and evaluate the effectiveness of the policy against the defined objectives.</p></li></ol></li>
<li>
<p><b>Continuous Improvement</b>:</p>
<ol class="lower-alpha">
<li><span epub:type="pagebreak" id="p157" aria-label=" page 157. " role="doc-pagebreak"/>Gather Feedback: Following the implementation of the policy, it is still important to gather stakeholder feedback to understand their experiences, challenges, and suggestions. Establishing channels for feedback collection, reporting of issues, and sharing of insights is integral for continuous improvement. Feedback should be systematically collected, documented, and analysed to identify areas for improvement.
<p><b>Who</b>: The monitoring and evaluation team.</p>
<p><b>How</b>: Through surveys, focus groups, and analysis of implementation metrics.</p></li>
<li>
<p>Policy Revision: Based on the feedback and findings from monitoring and evaluating its implementation, the policy may require revisions to address any emerging challenges, technological advancements, or changing educational needs. The Steering Committee should engage in the continued review of the policy, making necessary adjustments or amendments, and also continue to consult with stakeholders to ensure that the policy remains relevant, effective, and aligned with the institutional goals.</p>
<p><b>Who</b>: The Steering Committee.</p>
<p><b>How</b>: Based on feedback and evaluation findings, revising and updating the policy as necessary to ensure that it remains relevant and effective.</p></li>
<li>
<p>Ongoing Stakeholder Engagement: Continuous improvement requires ongoing engagement with stakeholders. Maintaining open communication channels, soliciting feedback, and providing regular updates on any revisions or new initiatives are crucial for keeping stakeholders informed, engaged, and compliant with the policy. This further fosters a culture of collaboration, learning, and continuous improvement, ensuring the AI policy remains a living document that evolves in response to the changing landscape of AI in education.</p>
<p><b>Who</b>: The Steering Committee and senior management.</p>
<p><b>How</b>: Maintaining open channels of communication to ensure that stakeholders are informed and have the opportunity to provide ongoing feedback.</p></li></ol></li></ol>
<p>Each of these steps is designed to ensure a collaborative, informed, and structured approach to developing and implementing an AI policy in education. The involvement of various stakeholders throughout different stages of the process is crucial to ensure that the policy <span epub:type="pagebreak" id="p158" aria-label=" page 158. " role="doc-pagebreak"/>is well-rounded, relevant, and effective in promoting ethical and effective AI integration in academic institutions.</p>
<section aria-labelledby="sec5_6_1">
<h3 id="sec5_6_1"><span epub:type="ordinal">5.6.1 </span>An Example of an AI Policy in Higher Education</h3>
<p>As emphasised throughout this chapter, creating a comprehensive AI policy for higher education requires meticulous planning and a thorough understanding of the institution&#x2019;s objectives, stakeholders&#x2019; needs, and the legal and ethical facets of AI. Based on the AI Ecological Education Policy framework and the step-by-step guide in <a href="#sec5_6">Section 5.6</a>. Here is a simple example outline of an AI education policy:</p>
<table id="table5_1">
<caption id="th5_1"><span epub:type="label">Table </span><span epub:type="ordinal">5.1 </span>A Simple Outline of an AI Education Policy</caption>
<thead>
<tr>
<th scope="colgroup" colspan="3" class="center">Title: AI Education Policy for [Institution Name]</th></tr></thead>
<tbody>
<tr>
<td class="left">
<ol type="1">
<li><b>Introduction</b>:
<ol class="custom-list">
<li><span class="custom-label-1">1.1.</span> Background: Explanation of the emergence of AI technologies and their potential impact on higher education. Mention of the institution&#x2019;s commitment to leveraging AI to enhance educational outcomes while adhering to ethical and legal standards.
<p>Purpose: Articulation of the policy&#x2019;s aim to guide the responsible integration and use of AI technologies within the institution.</p></li>
<li><span class="custom-label-1">1.2.</span> Scope: Clarification on the departments, individuals, and activities covered by the policy.</li></ol></li>
<li><b>Governance</b>:
<ol class="custom-list">
<li><span class="custom-label-2">2.1.</span> Steering Committee: Establishment of a steering committee to oversee the implementation, monitoring, and continuous improvement of the AI education policy.</li>
<li><span class="custom-label-2">2.2.</span> Ethical Guidelines: Clear ethical guidelines for AI usage, including respect for privacy, transparency, accountability, and fairness.</li>
<li><span class="custom-label-2">2.3.</span> Legal Compliance: Assurance of compliance with all applicable laws and regulations concerning data protection, privacy, and AI.</li></ol></li>
<li><b>Pedagogical Integration</b>:
<ol class="custom-list">
<li><span class="custom-label-3">3.1.</span> Faculty Training: Provision for training faculty on the responsible use of AI in teaching and assessment.</li>
<li><span class="custom-label-3">3.2.</span> Student Engagement: Encouragement of student engagement with AI technologies under the guidance of faculty.</li>
<li><span class="custom-label-3">3.3.</span> Assessment Reformation: Revision of assessment strategies to account for the integration of AI technologies.</li></ol></li>
<li><b>Operational Implementation</b>:
<ol class="custom-list">
<li><span class="custom-label-4">4.1.</span> Technology Infrastructure: Investment in necessary technology infrastructure to support AI integration.</li>
<li><span class="custom-label-4">4.2.</span> Support and Resources: Establishment of support channels and resources to assist stakeholders in navigating the AI policy and technologies.</li>
<li><span class="custom-label-4">4.3.</span> Monitoring and Evaluation: Continuous monitoring and evaluation to assess the effectiveness, compliance, and impact of AI integration.</li></ol></li>
<li><b>Continuous Improvement</b>:
<ol class="custom-list">
<li><span class="custom-label-5">5.1.</span> Feedback Mechanisms: Establishing mechanisms for stakeholders to provide feedback on AI integration experiences.</li>
<li><span class="custom-label-5">5.2.</span> Policy Revision: Periodic review and revision of the AI education policy to ensure relevance and effectiveness.</li>
<li><span class="custom-label-5">5.3.</span> Stakeholder Communication: Ongoing communication with stakeholders regarding any updates or revisions to the policy.</li></ol></li>
<li><b>Conclusion</b>:
<ol class="custom-list">
<li><span class="custom-label-6">6.1.</span> Commitment: Reiteration of the institution&#x2019;s commitment to responsible AI integration for the enhancement of educational outcomes.</li>
<li><span class="custom-label-6">6.2.</span> Contact Information: Provision of contact information for stakeholders to seek clarification or provide feedback regarding the policy.</li>
<li><span class="custom-label-6">6.3.</span> Effective Date: The date which the policy takes effect.</li></ol></li></ol></td></tr></tbody></table></section>
<section aria-labelledby="sec5_6_2">
<h3 id="sec5_6_2"><span epub:type="ordinal">5.6.2 </span>University of ABC</h3>
<section aria-labelledby="sec5_6_2_1">
<h4 id="sec5_6_2_1"><span epub:type="ordinal">5.6.2.1 </span>Artificial Intelligence in Education Policy</h4>
<ol class="upper-roman">
<li><b>Introduction</b>
<ol class="upper-alpha">
<li><b>Background</b>
<p><span epub:type="pagebreak" id="p159" aria-label=" page 159. " role="doc-pagebreak"/>The advent of Artificial Intelligence (AI) has created new possibilities and challenges for various sectors, including higher education. It has emerged as transformative tools with vast implications in the educational settings. At [the University of ABC], we recognise the unparalleled potential of AI to revolutionise learning experiences, enhance pedagogical methods, and facilitate groundbreaking research. This policy manifests our firm commitment to harnessing the power of AI to reinforce educational outcomes while strictly adhering to the highest ethical and legal standards. This policy aims to guide the responsible integration and usage of AI technologies within [the University of ABC].</p></li>
<li>
<p><b>Purpose</b></p>
<p>This policy seeks to provide a clear and structured framework guiding the responsible integration, utilisation, and governance of AI technologies within the University of ABC&#x2019;s ecosystem. To ensure the ethical, legal, and effective utilisation of AI to enhance teaching, learning, and administrative processes while safeguarding the rights and interests of all stakeholders.</p></li>
<li>
<p><b>Scope</b></p>
<p>This policy applies to all departments, faculties, students, staff, and affiliates of the [University of ABC]. It pertains to all activities related to the development, deployment, and utilisation or evaluation of AI technologies within the institution.</p></li></ol></li>
<li>
<p><b>Governance</b></p>
<ol class="upper-alpha">
<li>
<p><b>Steering Committee</b></p>
<p>A dedicated AI Steering Committee shall be established to provide oversight on the effective implementation, monitoring, and evolution of the AI education policy. The committee will comprise representatives from academic faculties, IT, administrative, legal, and student bodies.</p></li>
<li>
<p><b>Ethical Guidelines</b></p>
<p>The University of ABC upholds the principles of privacy, transparency, accountability, and fairness in all AI-related endeavours. All AI initiatives will be rooted in these ethical guidelines to ensure that technologies benefit the academic community without compromising individual rights or societal values.</p></li>
<li>
<p><b>Legal Compliance</b></p>
<p>Compliance with all applicable local, state, and federal laws and regulations governing data privacy, protection, and AI is mandatory. Necessary measures will be taken to ensure full legal compliance in every AI endeavour.</p></li></ol></li>
<li>
<p><b>Pedagogical Integration</b></p>
<ol class="upper-alpha">
<li>
<p><b>Faculty Training</b></p>
<p>Recognising the pivotal role of our faculty in AI integration, continuous professional development opportunities shall be provided to faculty to enhance their understanding and capability in utilising AI for pedagogical purposes.</p></li>
<li>
<p><b>Student Engagement</b></p>
<p>Students shall be educated on the ethical use of AI, and encouraged to leverage AI technologies to enhance their learning experiences under the guidance and supervision of faculty, ensuring hands-on experience and fostering a culture of innovative learning.</p></li>
<li>
<p><b>Assessment Redesign</b></p>
<p><span epub:type="pagebreak" id="p160" aria-label=" page 160. " role="doc-pagebreak"/>Traditional assessment strategies will be revisited and redesigned to account for the innovativeness introduced by AI, ensuring that they remain valid, reliable, and equitable. Assessment strategies shall be revised to ensure fairness and integrity in the evaluation of student performance in a learning environment augmented by AI technologies.</p></li></ol></li>
<li>
<p><b>Operational Implementation</b></p>
<ol class="upper-alpha">
<li>
<p><b>Technology Infrastructure</b></p>
<p>Investment will be channelled towards establishing robust and state-of-the-art technological infrastructure, facilitating seamless AI integration and ensuring optimal performance and security. Adequate technological infrastructure shall be established to support the effective integration and utilisation of AI technologies.</p></li>
<li>
<p><b>Support and Resources</b></p>
<p>Dedicated support channels, including helpdesks and online portals, will be set up to assist stakeholders in understanding and navigating AI technologies and adhering to this policy.</p></li>
<li>
<p><b>Monitoring and Evaluation</b></p>
<p>A continuous monitoring system will be implemented, evaluating the effectiveness, compliance, and impact of AI technologies, ensuring they align with our academic objectives and values. A systematic approach shall be employed to monitor, evaluate, and report the effectiveness and impact of AI integration within [the University of ABC].</p></li></ol></li>
<li>
<p><b>Continuous Improvement</b></p>
<ol class="upper-alpha">
<li>
<p><b>Feedback Mechanisms</b></p>
<p>Mechanisms for collecting feedback from all stakeholders on the AI integration experiences shall be established to inform continuous improvement efforts.</p></li>
<li>
<p><b>Policy Revision</b></p>
<p>In line with the dynamic nature of AI, this policy will be subject to periodic reviews, ensuring its continued relevance, robustness, and alignment with the University&#x2019;s goals and the broader educational landscape.</p></li>
<li>
<p><b>Stakeholder Communication</b></p>
<p>The University will maintain open lines of communication, updating stakeholders on any modifications or pertinent developments related to this policy.</p></li></ol></li>
<li><b>Conclusion</b>
<ol class="upper-alpha">
<li>
<p><b>Commitment</b></p>
<p>[The University of ABC] remains committed to harnessing the potential of AI to enhance the educational experiences of all stakeholders while adhering to the highest ethical, legal, and professional standards.</p></li>
<li>
<p><b>Contact Information</b></p>
<p>For any queries or feedback regarding this policy, stakeholders may contact the Steering Committee at [<a href="mailto:ai_policy@universityofabc.edu">ai_policy@universityofabc.edu</a>].</p></li>
<li>
<p><b>Effective Date</b></p>
<p>This policy shall take effect from [effective date] and remain in effect until revised or rescinded.</p></li></ol></li></ol>
<p class="noindent-t">Signed by</p>
<p class="noindent"><span epub:type="pagebreak" id="p161" aria-label=" page 161. " role="doc-pagebreak"/>___________________</p>
<p class="noindent">Senior Management</p>
<p class="noindent">University of ABC&#x2003;&#x2003;&#x2003; Date</p></section></section></section>
<section epub:type="conclusion" role="doc-conclusion">
<h2 id="sec5_7"><span epub:type="ordinal">5.7 </span>Conclusions</h2>
<p>In an era where AI &#x2013; and GenAI tools in particular &#x2013; is becoming increasingly embedded into our educational systems, developing comprehensive and ethical AI education policies is crucial. The journey from understanding the ethical dilemmas of using AI in educational settings and reviewing global AI policies, to charting a path for practical AI policy development in education, has been both enlightening and challenging. As we navigate through the multi-faceted terrains of AI ethics, governance, privacy, and equity in education, it is imperative that policies are continually revised to adapt to the evolving <span epub:type="pagebreak" id="p162" aria-label=" page 162. " role="doc-pagebreak"/>technological and ethical landscapes as well. Our pursuit should not only be in leveraging AI to enhance learning experiences, but also in ensuring that AI is implemented in a manner that is equitable, transparent, and beneficial to all stakeholders involved in the educational process.</p>
<aside epub:type="tip" role="doc-tip" class="box ruled">
<h3 id="box5_1">Questions to Ponder</h3>
<ul class="disc">
<li>How can policymakers ensure that AI education policies remain adaptable and resilient to the rapid advancements and unforeseen ethical challenges posed by GenAI?</li>
<li>In what ways can global AI education policies be coordinated and streamlined to establish a unified ethical and practical framework that transcends borders?</li>
<li>How might AI education policies address and mitigate the potential widening of educational disparities and ensure equal access to quality education for all students?</li>
<li>As we develop AI education policies, how can we ensure that the voices and concerns of all stakeholders, including students, educators, parents, and administrators, are adequately represented and addressed?</li>
<li>UNESCO suggests that the minimum age for AI usage should be 13 years old. Is this feasible? Or is it too late to start?</li></ul></aside>
<aside epub:type="tip" role="doc-tip" class="box shaded">
<h3 id="box5_2">Personal Reflection</h3>
<p>Embarking on the journey of exploring AI education policies invites us into a world where technology and ethics intertwine. Imagine, for a moment, that these policies are not mere documents but are the silent guardians of our educational kingdoms, ensuring that the integration of GenAI tools into our learning environments are just, ethical, and beneficial. What steps would you take to ensure our guardians are on the right path?</p>
<p>As we reflect on this chapter, let us ponder our own beliefs and perceptions about AI in education. How do we envision the future of education with GenAI tools?</p></aside></section>
<section class="reference" epub:type="bibliography" role="doc-bibliography">
<h2 id="r5_1">References</h2>
<ul>
<li id="ref5_1" epub:type="biblioentry"><a href="#R_ref5_1" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Australian Government Department of Industry, Science and Resources</a>. (2023, June 18). <cite>Australia&#x2019;s artificial intelligence action plan</cite>. Retrieved October 16, 2023, from <a href="https://www.industry.gov.au">https://www.industry.gov.au/publications/australias-artificial-intelligence-action-plan</a></li>
<li id="ref5_2" epub:type="biblioentry"><a href="#R_ref5_2" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Baio, A.</a> (2023, May 16). <cite>OpenAI CEO Sam Altman says AI can go &#x2018;quite wrong&#x2019; while advocating for government intervention</cite>. Independent. <a href="https://www.independent.co.uk">https://www.independent.co.uk/tech/congress-ai-chatgpt-sam-altman-b2340147.html</a></li>
<li id="ref5_3" epub:type="biblioentry"><a href="#R_ref5_3" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Bryson, J. J.</a> (2018). Patiency is not a virtue: the design of intelligent systems and systems of ethics. <cite>Ethics and Information Technology</cite>, <i>20</i>, 15&#x2013;26. <a href="https://doi.org/10.1007/s10676-018-9448-6" aria-label="D.O.I. link to Ethics and Information Technology">https://doi.org/10.1007/s10676-018-9448-6</a></li>
<li id="ref5_4" epub:type="biblioentry"><a href="#R_ref5_4" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Burciaga, A.</a> (2021, October 4). <cite>How to build responsible AI, step 1: Accountability</cite>. Forbes. Retrieved October 13, 2023, from <a href="https://www.forbes.com">https://www.forbes.com/sites/forbestechcouncil/2021/10/04/how-to-build-responsible-ai-step-1-accountability/</a></li>
<li id="ref5_5" epub:type="biblioentry"><a href="#R_ref5_5" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Centre for Information Policy Leadership (CIPL)</a>. (2020, March). <cite>Artificial intelligence and data protection: How the GDPR regulates AI</cite>. Retrieved October 13, 2023, from <a href="https://www.informationpolicycentre.com">https://www.informationpolicycentre.com/uploads/5/7/1/0/57104281/cipl-hunton_andrews_kurth_legal_note_-_how_gdpr_regulates_ai__12_march_2020_.pdf</a></li>
<li id="ref5_6" epub:type="biblioentry"><a href="#R_ref5_6" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Chan, C. K. Y.</a> (2023). A comprehensive AI policy education framework for university teaching and learning. <cite>International Journal of Educational Technology in Higher Education</cite>. <a href="https://doi.org/" aria-label="D.O.I. link to International Journal of Educational Technology in Higher Education">https://doi.org/ 10.1186/s41239-023-00408-3</a></li>
<li id="ref5_7" epub:type="biblioentry"><a href="#R_ref5_7" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Chowdhury, R.</a> (2023, April 6). <cite>AI desperately needs global oversight</cite>. Wired. Retrieved October 13, 2023, from <a href="https://www.wired.com">https://www.wired.com/story/ai-desperately-needs-global-oversight/</a></li>
<li id="ref5_8" epub:type="biblioentry"><a href="#R_ref5_8" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Christian, B.</a> (2020). <cite>The alignment problem: Machine learning and human values</cite>. W.W. Norton &#x0026; Company.</li>
<li id="ref5_9" epub:type="biblioentry"><a href="#R_ref5_9" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Creswell, A., Nikiforou, K., Vinyals, O., Saraiva, A., Kabra, R., Matthey, L., Burgess, C., Reynolds, M., Tanburn, R., Garnelo, M., &#x0026; Shanahan, M.</a> (2020, July 21). <cite>AlignNet: Unsupervised entity alignment</cite>. Google DeepMind. Retrieved October 13, 2023, from <a href="https://www.deepmind.com">https://www.deepmind.com/publications/alignnet-unsupervised-entity-alignment</a></li>
<li id="ref5_10" epub:type="biblioentry"><a href="#R_ref5_10" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Criddle, C., Espinoza, J., &#x0026; Liu, Q.</a> (2023, September 13). <cite>The global race to set the rules for AI</cite>. Financial Times. Retrieved October 13, 2023, from <a href="https://www.ft.com">https://www.ft.com/content/59b9ef36-771f-4f91-89d1-ef89f4a2ec4e</a></li>
<li id="ref5_11" epub:type="biblioentry"><a href="#R_ref5_11" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Department for Digital, Culture, Media and Sport</a>. (2022, July 18). <cite>Establishing a pro-innovation approach to regulating AI: An overview of the UK&#x2019;s emerging approach</cite>. Retrieved October 17, 2023, from <a href="https://assets.publishing.service.gov.uk">https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/1092630/_CP_728__-_Establishing_a_pro-innovation_approach_to_regulating_AI.pdf</a></li>
<li id="ref5_12" epub:type="biblioentry"><a href="#R_ref5_12" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Edwards, B.</a> (2023, May 31). <i>OpenAI execs warn of &#x201C;risk of extinction&#x201D; from artificial intelligence in new open letter</i>. Ars Technica. Retrieved October 13, 2023, from <a href="https://arstechnica.com">https://arstechnica.com/information-technology/2023/05/openai-execs-warn-of-risk-of-extinction-from-artificial-intelligence-in-new-open-letter/</a></li>
<li id="ref5_13" epub:type="biblioentry"><a href="#R_ref5_13" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">European Parliament</a> (2023, June 14). <cite>EU AI act: First regulation on artificial intelligence</cite>. Retrieved October 13, 2023, from <a href="https://www.europarl.europa.eu">https://www.europarl.europa.eu/news/en/headlines/society/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence</a></li>
<li id="ref5_14" epub:type="biblioentry"><a href="#R_ref5_14" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Federal Trade Commission Bureau of Competition &#x0026; Office of Technology</a>. (2023, June 29). <cite>Generative AI raises competition concerns</cite>. Retrieved October 16, 2023, from <a href="https://www.ftc.gov">https://www.ftc.gov/policy/advocacy-research/tech-at-ftc/2023/06/generative-ai-raises-competition-concerns</a></li>
<li id="ref5_15" epub:type="biblioentry"><span epub:type="pagebreak" id="p163" aria-label=" page 163. " role="doc-pagebreak"/><a href="#R_ref5_15" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Federspiel, F., Mitchell, R., Asokan, A., Umana, C., &#x0026; McCoy, D.</a> (2023). Threats by artificial intelligence to human health and human existence. <cite>BMJ Global Health</cite>, <i>8</i>(5), 1&#x2013;6. <a href="https://doi.org/10.1136/bmjgh-2022-010435" aria-label="D.O.I. link to BMJ Global Health">https://doi.org/10.1136/bmjgh-2022-010435</a></li>
<li id="ref5_16" epub:type="biblioentry"><a href="#R_ref5_16" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Ferrara, E.</a> (2023). Fairness and bias in artificial intelligence: A brief survey of sources, impacts, and mitigation strategies. arXiv. <a href="https://doi.org/10.48550/arXiv.2304.07683" aria-label="D.O.I. link to Fairness and bias in artificial intelligence: A brief survey of sources, impacts, and mitigation strategies">https://doi.org/10.48550/arXiv.2304.07683</a></li>
<li id="ref5_17" epub:type="biblioentry"><a href="#R_ref5_17" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Friedler, S., Venkatasubramanian, S., &#x0026; Engler, A.</a> (2023, March 22). <cite>How California and other states are tackling AI legislation</cite>. Brookings. Retrieved October 13, 2023, from <a href="https://www.brookings.edu">https://www.brookings.edu/articles/how-california-and-other-states-are-tackling-ai-legislation/</a></li>
<li id="ref5_18" epub:type="biblioentry"><a href="#R_ref5_18" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Future of Life Institute</a>. (2017, August 11). <cite>AI principles</cite>. Retrieved October 13, 2023, from <a href="https://futureoflife.org">https://futureoflife.org/open-letter/ai-principles/</a></li>
<li id="ref5_19" epub:type="biblioentry"><a href="#R_ref5_19" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Gabriel, I.</a> (2020, January 13). <cite>Artificial intelligence, values and alignment</cite>. Google DeepMind. Retrieved October 13, 2023, from <a href="https://www.deepmind.com">https://www.deepmind.com/publications/artificial-intelligence-values-and-alignment</a></li>
<li id="ref5_20" epub:type="biblioentry"><a href="#R_ref5_20" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">GDPR.EU</a>. (2023). <cite>What is GDPR, the EU&#x2019;s new data protection law?</cite> Retrieved October 13, 2023, from <a href="https://gdpr.eu">https://gdpr.eu/what-is-gdpr/</a></li>
<li id="ref5_21" epub:type="biblioentry"><a href="#R_ref5_21" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Gent, E.</a> (2023, May 10). <cite>What is the AI alignment problem and how can it be solved?</cite> New Scientist. Retrieved October 13, 2023, from <a href="https://www.newscientist.com">https://www.newscientist.com/article/mg25834382-000-what-is-the-ai-alignment-problem-and-how-can-it-be-solved/</a></li>
<li id="ref5_22" epub:type="biblioentry"><a href="#R_ref5_22" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Goertzel, B.</a> (2014). Artificial general intelligence: Concept, state of the art, and future prospects. <cite>Journal of Artificial General Intelligence</cite>, <i>5</i>(1), 1&#x2013;46. <a href="https://doi.org/10.2478/jagi-2014-0001" aria-label="D.O.I. link to Journal of Artificial General Intelligence">https://doi.org/10.2478/jagi-2014-0001</a></li>
<li id="ref5_23" epub:type="biblioentry"><a href="#R_ref5_23" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Government of India Ministry of Electronics &#x0026; Information Technology</a>. (2023, August 11). <cite>Information technology act 2000</cite>. Retrieved October 16, 2023, from <a href="https://www.meity.gov.in">https://www.meity.gov.in/content/information-technology-act-2000</a></li>
<li id="ref5_24" epub:type="biblioentry"><a href="#R_ref5_24" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Greiman, V. A.</a> (2021). Human rights and artificial intelligence: A universal challenge. <cite>Journal of Information Warfare</cite>, <i>20</i>(1), 50&#x2013;62.</li>
<li id="ref5_25" epub:type="biblioentry"><a href="#R_ref5_25" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Hogenhout, L.</a> (2021). <cite>A framework for ethical AI at the United Nations</cite>. United Nations. Retrieved October 16, 2023, <a href="https://unite.un.org">https://unite.un.org/sites/unite.un.org/files/unite_paper_-_ethical_ai_at_the_un.pdf</a></li>
<li id="ref5_26" epub:type="biblioentry"><a href="#R_ref5_26" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Information Commissioner&#x2019;s Office</a>. (n.d.) <cite>What are the accountability and governance implications of AI?</cite> Retrieved October 13, 2023, from <a href="https://ico.org.uk">https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/artificial-intelligence/guidance-on-ai-and-data-protection/what-are-the-accountability-and-governance-implications-of-ai/</a></li>
<li id="ref5_27" epub:type="biblioentry"><a href="#R_ref5_27" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">International Commission on the Futures of Education</a>. (2021). <cite>Reimagining our futures together: a new social contract for education</cite>. UNESCO. <a href="https://doi.org/10.54675/ASRB4722" aria-label="D.O.I. link to Reimagining our futures together: a new social contract for education">https://doi.org/10.54675/ASRB4722</a></li>
<li id="ref5_28" epub:type="biblioentry"><a href="#R_ref5_28" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Joyce, D. W., Kormilitzin, A., Smith, K. A. et al.</a> (2023). Explainable artificial intelligence for mental health through transparency and interpretability for understandability. <cite>npj Digital Medicine</cite>, 6, 6. <a href="https://doi.org/10.1038/s41746-023-00751-9" aria-label="D.O.I. link to npj Digital Medicine">https://doi.org/10.1038/s41746-023-00751-9</a></li>
<li id="ref5_29" epub:type="biblioentry"><a href="#R_ref5_29" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Kharparl, A.</a> (2023, July 13). <cite>China finalizes first-of-its-kind rules governing generative A.I. services like ChatGPT</cite>. CNBC. Retrieved October 13, 2023, from <a href="https://www.cnbc.com">https://www.cnbc.com/2023/07/13/china-introduces-rules-governing-generative-ai-services-like-chatgpt.html</a></li>
<li id="ref5_30" epub:type="biblioentry"><a href="#R_ref5_30" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Larsen, B. C.</a> (2023). <cite>The geopolitics of AI and the rise of digital sovereignty</cite>. Brookings. Retrieved 16 October, 2023, from <a href="https://www.brookings.edu">https://www.brookings.edu/articles/the-geopolitics-of-ai-and-the-rise-of-digital-sovereignty/</a></li>
<li id="ref5_31" epub:type="biblioentry"><a href="#R_ref5_31" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Lawton, G.</a> (2023, June 2). <cite>AI transparency: What is it and why do we need it?</cite> TechTarget. Retrieved October 13, 2023, from <a href="https://www.techtarget.com">https://www.techtarget.com/searchcio/tip/AI-transparency-What-is-it-and-why-do-we-need-it#:~:text=Transparency%20in%20AI%20refers%20to,how%20it%20reaches%20its%20decisions</a></li>
<li id="ref5_32" epub:type="biblioentry"><a href="#R_ref5_32" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Leike, J., Schulman, J., &#x0026; Wu, J.</a> (2022, August 24). <cite>Our approach to alignment research</cite>. OpenAI. Retrieved October 13, 2023, from <a href="https://openai.com">https://openai.com/blog/our-approach-to-alignment-research</a></li>
<li id="ref5_33" epub:type="biblioentry"><a href="#R_ref5_33" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Liu, Y.</a> (2023, June 5). &#x751F;&#x6210;AI&#x753B;&#x50CF;&#x306F;&#x985E;&#x4F3C;&#x6027;&#x304C;&#x8A8D;&#x3081;&#x3089;&#x308C;&#x308C;&#x3070;&#x300C;&#x8457;&#x4F5C;&#x6A29;&#x4FB5;&#x5BB3;&#x300D;&#x3002;&#x6587;&#x5316;&#x5E81; [If a generated AI image is found to be similar, it is considered a &#x201C;copyright infringement.&#x201D; Agency for Cultural Affairs]. PC Watch. Retrieved from <a href="https://pc.watch.impress.co.jp">https://pc.watch.impress.co.jp/docs/news/1506018.html</a></li>
<li id="ref5_34" epub:type="biblioentry"><span epub:type="pagebreak" id="p164" aria-label=" page 164. " role="doc-pagebreak"/><a href="#R_ref5_34" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Manish, C.</a> (2023, August 18). <cite>Sam Altman&#x2019;s nuclear backpack holds the code to save the world from AI</cite>. Mobile App Daily. Retrieved from <a href="https://www.mobileappdaily.com">https://www.mobileappdaily.com/news/sam-altmans-nuclear-backpack-holds-the-code-to-save-the-world-from-ai</a></li>
<li id="ref5_35" epub:type="biblioentry"><a href="#R_ref5_35" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Matsuda, A., Kudo, R., &#x0026; Matsuda, T.</a> (2023). Japan. In C. Kerrigan (Ed.), <cite>AI, machine learning &#x0026; big data laws and regulations 2023</cite>. Global Legal Insights. <a href="https://www.globallegalinsights.com">https://www.globallegalinsights.com/practice-areas/ai-machine-learning-and-big-data-laws-and-regulations/japan</a></li>
<li id="ref5_36" epub:type="biblioentry"><a href="#R_ref5_36" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., &#x0026; Galstyan, A.</a> (2021). A survey on bias and fairness in machine learning. <cite>ACM Computing Surveys</cite>, <i>54</i>(6), 1&#x2013;35. <a href="https://doi.org/10.1145/3457607" aria-label="D.O.I. link to ACM Computing Surveys">https://doi.org/10.1145/3457607</a></li>
<li id="ref5_37" epub:type="biblioentry"><a href="#R_ref5_37" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Meyer, J.</a> (2023, May 31). <cite>AI poses risk of extinction, tech leaders warn in open letter. Here&#x2019;s why alarm is spreading</cite>. USA Today. Retrieved October 13, 2023, from <a href="https://eu.usatoday.com">https://eu.usatoday.com/story/news/politics/2023/05/31/ai-extinction-risk-expert-warning/70270171007/</a></li>
<li id="ref5_38" epub:type="biblioentry"><a href="#R_ref5_38" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Minevich, M.</a> (2023, May 19). <cite>Generative AI shakes global diplomacy at G7 summit in Japan</cite>. Forbes. Retrieved October 16, 2023, from <a href="https://www.forbes.com">https://www.forbes.com/sites/markminevich/2023/05/19/high-stakes-generative-ai-shakes-global-diplomacy-at-japans-2023-g7-summit/?sh=243a3dce6321</a></li>
<li id="ref5_39" epub:type="biblioentry"><a href="#R_ref5_39" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Ministry of Economy, Trade and Industry</a>. (2021, July 9). AI governance in Japan Ver. 1.1: Report from the expert group on how AI principles should be implemented. Retrieved October 16, 2023, from <a href="https://www.meti.go.jp">https://www.meti.go.jp/shingikai/mono_info_service/ai_shakai_jisso/pdf/20210709_8.pdf</a></li>
<li id="ref5_40" epub:type="biblioentry"><a href="#R_ref5_40" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Mukherjee, S., Chee, F. Y., &#x0026; Coulter, M.</a> (2023, April 28). <cite>EU proposes new copyright rules for generative AI</cite>. Reuters. Retrieved October 13, 2023, from <a href="https://www.reuters.com">https://www.reuters.com/technology/eu-lawmakers-committee-reaches-deal-artificial-intelligence-act-2023-04-27/</a></li>
<li id="ref5_41" epub:type="biblioentry"><a href="#R_ref5_41" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Nasiripour, S., &#x0026; Natarajan, S.</a> (2019, November 11). <cite>Apple co-founder says Goldman&#x2019;s apple card algorithm discriminates</cite>. Bloomberg. Retrieved October 13, 2023, from <a href="https://www.bloomberg.com">https://www.bloomberg.com/news/articles/2019-11-10/apple-co-founder-says-goldman-s-apple-card-algo-discriminates?leadSource=uverify%20wall</a></li>
<li id="ref5_42" epub:type="biblioentry"><a href="#R_ref5_42" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Novelli, C., Taddeo, M., &#x0026; Floridi, L.</a> (2023). Accountability in artificial intelligence: What it is and how it works. <cite>AI &#x0026; Soc</cite>. <a href="https://doi.org/10.1007/s00146-023-01635-y" aria-label="D.O.I. link to AI &#x0026; Soc">https://doi.org/10.1007/s00146-023-01635-y</a></li>
<li id="ref5_43" epub:type="biblioentry"><a href="#R_ref5_43" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">OpenAI</a>. (2023a). <cite>GPT-4 documentation</cite>. Retrieved October 13, 2023, from <a href="https://platform.openai.com">https://platform.openai.com/docs/models/gpt-4</a></li>
<li id="ref5_44" epub:type="biblioentry"><a href="#R_ref5_44" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">OpenAI</a>. (2023b). <cite>Privacy policy</cite>. Retrieved October 13, 2023, from <a href="https://openai.com">https://openai.com/policies/privacy-policy</a></li>
<li id="ref5_45" epub:type="biblioentry"><a href="#R_ref5_45" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">OpenAI</a>. (2023c). <cite>Security &#x0026; privacy</cite>. Retrieved October 13, 2023, from <a href="https://openai.com">https://openai.com/security</a></li>
<li id="ref5_46" epub:type="biblioentry"><a href="#R_ref5_46" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">OpenAI</a>. (2023d). <cite>OpenAI charter</cite>. Retrieved October 13, 2023, from <a href="https://openai.com">https://openai.com/charter</a></li>
<li id="ref5_47" epub:type="biblioentry">Osborne Clarke. (2022, July 19). <cite>Consultation on UK AI regulation: Legislation-free and devolved regulators</cite>. Retrieved October 17, 2023, from <a href="https://www.osborneclarke.com">https://www.osborneclarke.com/insights/consultation-uk-ai-regulation-legislation-free-and-devolved-regulators</a></li>
<li id="ref5_48" epub:type="biblioentry"><a href="#R_ref5_48" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Perrigo, B., &#x0026; Gordon, A.</a> (2023, June 14). <cite>E.U. takes a step closer to passing the world&#x2019;s most comprehensive AI regulation</cite>. Time. Retrieved October 13, 2023, from <a href="https://time.com">https://time.com/6287136/eu-ai-regulation/</a></li>
<li id="ref5_49" epub:type="biblioentry"><a href="#R_ref5_49" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Ribeiro, M. T., Singh, S., &#x0026; Guestrin, C.</a> (2016). &#x201C;Why should I trust you?&#x201D;: Explaining the predictions of any classifier. In <i>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</i> (pp. 1135&#x2013;1144). <a href="https://doi.org/10.1145/2939672.2939778" aria-label="D.O.I. link to Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining">https://doi.org/10.1145/2939672.2939778</a></li>
<li id="ref5_50" epub:type="biblioentry"><a href="#R_ref5_50" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Roberts, H., Babuta, A., Morley, J., Thomas, C., Taddeo, M., &#x0026; Floridi, L.</a> (2023, May 26). Artificial intelligence regulation in the United Kingdom: A path to good governance and global leadership? <cite>Internet Policy Review</cite>, <i>12</i>(2). <a href="https://doi.org/10.14763/2023.2.1709" aria-label="D.O.I. link to Internet Policy Review">https://doi.org/10.14763/2023.2.1709</a></li>
<li id="ref5_51" epub:type="biblioentry"><a href="#R_ref5_51" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Robles, P.</a> (2023, October 1). <cite>China plans to be a world leader in Artificial Intelligence by 2030</cite>. South China Morning Post. Retrieved October 16, 2023, from <a href="https://multimedia.scmp.com">https://multimedia.scmp.com/news/china/article/2166148/china-2025-artificial-intelligence/index.html</a></li>
<li id="ref5_52" epub:type="biblioentry"><a href="#R_ref5_52" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Russell, S., Dewey, D., &#x0026; Tegmark, M.</a> (2015). Research priorities for robust and beneficial artificial intelligence. <cite>AI Magazine</cite>, <i>36</i>(4), 105&#x2013;114. <a href="https://doi.org/10.1609/aimag.v36i4.2577" aria-label="D.O.I. link to AI Magazine">https://doi.org/10.1609/aimag.v36i4.2577</a></li>
<li id="ref5_53" epub:type="biblioentry"><a href="#R_ref5_53" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Sharwood, S.</a> (2023, May 26). <cite>India set to regulate AI, Big Tech, with Digital Act</cite>. The Register. Retrieved October 16, 2023, from <a href="https://www.theregister.com">https://www.theregister.com/2023/05/26/india_digital_act_draft_june/</a></li>
<li id="ref5_54" epub:type="biblioentry"><span epub:type="pagebreak" id="p165" aria-label=" page 165. " role="doc-pagebreak"/><a href="#R_ref5_54" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Silberg, J., &#x0026; Manyika, J.</a> (2019, June 6). <cite>Tackling bias in artificial intelligence (and in humans)</cite>. McKinsey Global Institute. Retrieved October 13, 2023, from <a href="https://www.mckinsey.com">https://www.mckinsey.com/featured-insights/artificial-intelligence/tackling-bias-in-artificial-intelligence-and-in-humans</a></li>
<li id="ref5_55" epub:type="biblioentry"><a href="#R_ref5_55" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Singh, M.</a> (2023, April 6). <cite>India opts against AI regulation</cite>. TechCrunch. Retrieved October 16, 2023, from <a href="https://techcrunch.com">https://techcrunch.com/2023/04/05/india-opts-against-ai-regulation/</a></li>
<li id="ref5_56" epub:type="biblioentry"><a href="#R_ref5_56" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">State of California Department of Justice, Office of the Attorney General</a>. (2023). <cite>California Consumer Privacy Act (CCPA)</cite>. Retrieved October 13, 2023, from <a href="https://oag.ca.gov">https://oag.ca.gov/privacy/ccpa</a></li>
<li id="ref5_57" epub:type="biblioentry"><a href="#R_ref5_57" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Stokel-Walker, C.</a> (2023, January 18). <cite>ChatGPT listed as author on research papers: Many scientists disapprove</cite>. Nature. Retrieved October 13, 2023, from <a href="https://www.nature.com">https://www.nature.com/articles/d41586-023-00107-z</a></li>
<li id="ref5_58" epub:type="biblioentry"><a href="#R_ref5_58" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Strickland, E.</a> (2023, August 31). <cite>OpenAI&#x2019;s moonshot: Solving the AI alignment problem</cite>. IEEE Spectrum. Retrieved October 13, 2023, from <a href="https://spectrum.ieee.org">https://spectrum.ieee.org/the-alignment-problem-openai</a></li>
<li id="ref5_59" epub:type="biblioentry"><a href="#R_ref5_59" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">The Japan Times</a>. (2023, October 15). <cite>Japan&#x2019;s AI draft guidelines ask for measures to address overreliance</cite>. Retrieved October 16, 2023, from <a href="https://www.japantimes.co.jp">https://www.japantimes.co.jp/news/2023/10/15/japan/politics/ai-draft-guidelines/</a></li>
<li id="ref5_60" epub:type="biblioentry"><a href="#R_ref5_60" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">The Times of India</a>. (2023, May 17). <cite>India planning to regulate AI platforms like ChatGPT: IT minister Ashwini Vaishnaw</cite>. Retrieved October 16, 2023, from <a href="http://timesofindia.indiatimes.com">http://timesofindia.indiatimes.com/articleshow/100310733.cms?utm_source=contentofinterest&#x0026;utm_medium=text&#x0026;utm_campaign=cppst</a></li>
<li id="ref5_61" epub:type="biblioentry"><a href="#R_ref5_61" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">UK Government</a>. (2023, August 3). <i>AI regulation: a pro-innovation approach</i>. [White paper]. Retrieved October 16, 2023, from <a href="https://www.gov.uk">https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach</a></li>
<li id="ref5_62" epub:type="biblioentry"><a href="#R_ref5_62" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">UNESCO</a>. (2023a, July 20). <cite>UNESCO&#x2019;s Recommendation on the Ethics of Artificial Intelligence: Key Facts</cite>. <a href="https://www.unesco.org">https://www.unesco.org/en/articles/unescos-recommendation-ethics-artificial-intelligence-key-facts#:~:text=The%20Recommendation%20establishes%20a%20set,the%20rule%20of%20law%20%20online</a></li>
<li id="ref5_63" epub:type="biblioentry"><a href="#R_ref5_63" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">UNESCO</a>. (2023b, June 9). <cite>Artificial Intelligence: UNESCO publishes Policy Paper on AI Foundation Models</cite>. Retrieved from <a href="https://www.unesco.org">https://www.unesco.org/en/articles/artificial-intelligence-unesco-publishes-policy-paper-ai-foundation-models</a></li>
<li id="ref5_64" epub:type="biblioentry"><a href="#R_ref5_64" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">UNESCO</a>. (2023c). <cite>Guidance for generative AI in education and research</cite>. UNESCO. Retrieved October 13, 2023, from <a href="https://unesdoc.unesco.org">https://unesdoc.unesco.org/ark:/48223/pf0000386693</a></li>
<li id="ref5_65" epub:type="biblioentry">University of Birmingham. (2023, August 4). <cite>Government&#x2019;s &#x201C;Pro-Innovation&#x201D; AI white paper unfit for purpose</cite>. Retrieved October 16, 2023, from <a href="https://www.birmingham.ac.uk">https://www.birmingham.ac.uk/news/2023/the-governments-preoccupation-with-innovation-will-cause-the-rule-of-law-to-fall-behind</a></li>
<li id="ref5_66" epub:type="biblioentry"><a href="#R_ref5_66" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">West, D. M.</a> (2023, September 12). <cite>California charts the future of AI</cite>. Brookings. Retrieved October 13, 2023, from <a href="https://www.brookings.edu">https://www.brookings.edu/articles/california-charts-the-future-of-ai/</a></li>
<li id="ref5_67" epub:type="biblioentry"><a href="#R_ref5_67" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Wheeler, T.</a> (2023, June 15). <cite>The three challenges of AI regulation</cite>. Brookings. Retrieved October 13, 2023, from <a href="https://www.brookings.edu">https://www.brookings.edu/articles/the-three-challenges-of-ai-regulation/</a></li></ul></section></section></body></html>