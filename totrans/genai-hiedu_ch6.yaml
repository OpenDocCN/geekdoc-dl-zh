- en: 6 Technology Behind GenAI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 生成式人工智能背后的技术
- en: 'DOI: [10.4324/9781003459026-6](https://dx.doi.org/10.4324/9781003459026-6)'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'DOI: [10.4324/9781003459026-6](https://dx.doi.org/10.4324/9781003459026-6)'
- en: Big centralised AI models have amazed people. Personal, portable, and secure
    AI will make everyone amazing.
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 大型集中式人工智能模型让人们感到惊讶。个人、便携和安全的AI将使每个人都变得非凡。
- en: ''
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Tom Colloton
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 托姆·科洛顿
- en: 6.1 Introduction
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 简介
- en: The aim of this chapter is to equip the reader with a clearer understanding
    of the technology behind generative Artificial Intelligence (GenAI). While the
    book primarily focuses on GenAI in higher education, we recognise that some readers
    may be curious to learn more about the “black box” of GenAI. Thus, we will delve
    into its history, processes, methods, and the foundational technology of GenAI.
    In this exploration, the reader will encounter terminology commonly used in the
    Artificial Intelligence (AI) field.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是使读者对生成式人工智能（GenAI）背后的技术有一个更清晰的理解。虽然本书主要关注高等教育中的GenAI，但我们认识到，一些读者可能对GenAI的“黑箱”感兴趣。因此，我们将深入探讨其历史、过程、方法和GenAI的基础技术。在这项探索中，读者将遇到在人工智能（AI）领域常用的术语。
- en: In this chapter, we will discuss GenAI models, henceforth referred to simply
    as ‘models’ for brevity. It is crucial not to confuse these with other types of
    models mentioned in the book, such as assessment models. Contemporary GenAI systems
    predominantly employ deep neural networks (DNN) as the foundational approach for
    their models. As illustrated in [Figure 6.1](#fig6_1), DNNs are a type of artificial
    neural network (ANN). The concept of using artificial neural networks (ANNs) has
    deep roots in AI, drawing inspiration from studies on the biological brain’s functionality.
    We will delve deeper into this history, providing context leading up to the present-day
    state-of-the-art models. As illustrated in [Figure 6.1](#fig6_1), GenAI and its
    various model types, including large language models (LLMs) and text-to-image
    models like diffusion models, reside within the realm of deep learning ANNs.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论生成式人工智能模型，以下将简称为“模型”。至关重要的是不要将这些模型与其他书中提到的其他类型的模型混淆，例如评估模型。当代的生成式人工智能系统主要采用深度神经网络（DNN）作为其模型的基础方法。如图6.1所示，DNN是一种人工神经网络（ANN）。使用人工神经网络（ANNs）的概念在AI中有着深厚的根源，它从对生物大脑功能的研究中汲取灵感。我们将更深入地探讨这一历史，提供引领到当今最先进模型的前置背景。如图6.1所示，生成式人工智能及其各种模型类型，包括大型语言模型（LLMs）和扩散模型等文本到图像模型，都位于深度学习人工神经网络（ANNs）的领域内。
- en: '![A I categories include Artificial intelligence, machine learning, artificial
    neural networks, deep learning, and gen A I which include L L M and text to image.](images/fig6_1_B.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![人工智能类别包括人工智能、机器学习、人工神经网络、深度学习和生成式人工智能，后者包括LLM和文本到图像。](images/fig6_1_B.jpg)'
- en: '[Figure 6.1 AI Categorisation.](#R_fig6_1)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.1 人工智能分类。](#R_fig6_1)'
- en: '**Artificial neural networks (ANNs)** are computational models inspired by
    the human brain’s structure and function. They consist of interconnected nodes,
    called neurons, which process and transmit information through weighted connections.
    ANNs are designed to learn and recognise patterns from input data, making them
    useful for tasks such as classification, prediction, and decision-making in various
    fields like computer vision, natural language processing, and robotics. This chapter
    will delve into the processes and techniques that underpin these capabilities.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工神经网络（ANNs）**是受人类大脑结构和功能启发的计算模型。它们由相互连接的节点组成，称为神经元，通过加权连接处理和传输信息。ANNs被设计用来从输入数据中学习和识别模式，这使得它们在诸如计算机视觉、自然语言处理和机器人学等领域的分类、预测和决策等任务中非常有用。本章将深入探讨支撑这些能力的过程和技术。'
- en: The precise mechanisms by which neural networks store experiences and utilise
    them for predictions remain somewhat enigmatic. This elusive nature has led some
    to describe their operation as “magic”, given that the complete intricacies and
    potential limitations are not yet fully understood. This chapter will try to share
    what is known and what is not yet understood about how these models work.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络存储经验和利用它们进行预测的精确机制仍然有些神秘。这种难以捉摸的性质导致一些人将它们的运作描述为“魔法”，因为其完整的复杂性以及潜在的局限性尚未完全被理解。本章将尝试分享关于这些模型如何工作已知和尚未理解的内容。
- en: This chapter involves examining specific GenAI solutions that are currently
    popular and widely used. It will delve into these specific examples in detail,
    while also referencing broader methods. By juxtaposing specific examples with
    general concepts, we aim to offer non-experts a more insightful perspective than
    if we merely discussed the topic in general terms.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涉及检查目前流行且广泛使用的特定GenAI解决方案。它将详细探讨这些特定示例，同时参考更广泛的方法。通过将具体示例与一般概念并置，我们旨在为非专业人士提供一个比仅仅泛泛而谈该主题更为深刻的视角。
- en: The chapter is structured in a manner that facilitates a gradual understanding,
    building later concepts on top of earlier concepts as much as possible. However,
    some topics are interrelated and require more of an iterative approach rather
    than a pure serial reading of the information, thus it is advised to read the
    chapter end to end and then re-read to get a deeper understanding of these concepts
    that required a more iterative approach.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的结构旨在促进逐步理解，尽可能在早期概念的基础上构建后续概念。然而，有些主题是相互关联的，需要更多的迭代方法，而不是纯粹的信息线性阅读，因此建议从头到尾阅读本章，然后重新阅读以获得对这些需要更多迭代方法的概念的更深入理解。
- en: 'The structure of the chapter is outlined below:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的结构概述如下：
- en: '**The history**. This section looks at the history of artificial neural networks
    (ANNs), the early work that was done and some of the recent advancements.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**历史**。本节探讨了人工神经网络（ANNs）的历史，早期的工作以及一些最近的进展。'
- en: '**Creating a model**. This section aims to furnish a comprehensive understanding
    of how a model operates and the processes involved in its creation. It examines
    the various stages of model creation: from gathering vast quantities of data to
    considerations in design and structure; the training phase; and testing the model
    for quality, performance, and safety.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**创建模型**。本节旨在提供一个全面的理解，了解模型是如何运作的，以及其创建过程中涉及到的各种步骤。它考察了模型创建的各个阶段：从收集大量数据到设计结构和考虑；训练阶段；以及测试模型的质量、性能和安全。'
- en: '**Models and ecosystems**. This section looks at how models are being used
    and the ecosystems that have built up around them to allow people to interact
    with models in different ways and allow the models to interact with their environments.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型和生态系统**。本节探讨了模型的使用方式以及围绕它们建立起来的生态系统，允许人们以不同的方式与模型互动，并允许模型与其环境互动。'
- en: '**State-of-the-art models**. This section aims to provide an overview of some
    currently popular models considered state-of-the-art. It examines the applications
    of these models, discusses their design approaches, and comments on their strengths
    and weaknesses.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最先进的模型**。本节旨在概述一些目前被认为是最先进的流行模型。它考察了这些模型的应用，讨论了它们的设计方法，并对其优缺点进行了评论。'
- en: '**Conclusions**. This section recaps the key points of GenAI and reflects on
    the current boundaries of its application.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结论**。本节总结了GenAI的关键点，并反思了其当前应用的边界。'
- en: 6.2 The History
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 历史
- en: The subsequent sections present the history of artificial neural networks (ANNs)
    in chronological order. You might encounter terms that have not yet been discussed
    or elaborated upon, such as the specifics of an artificial neuron or cell, the
    connectivity within artificial neural networks, the concept of a layer, and the
    roles of weights and biases. However, these intricacies will be addressed in due
    course. Initially, our focus will be on providing a chronological overview of
    how these concepts evolved. Later, we will delve into the particulars, emphasising
    those aspects that remain pertinent today, all while employing contemporary terminology.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的几节将按时间顺序介绍人工神经网络（ANNs）的历史。你可能会遇到一些尚未讨论或详细阐述的术语，例如人工神经元或细胞的细节、人工神经网络中的连接性、层的概念以及权重和偏差的作用。然而，这些复杂性将在适当的时候得到解决。最初，我们的重点是提供一个这些概念如何发展的时间顺序概述。稍后，我们将深入探讨细节，强调那些至今仍相关的方面，同时使用当代术语。
- en: The history of AI research can be understood as an evolving interplay of several
    distinct approaches or “camps”. These different approaches often reflected the
    diverse intellectual backgrounds of AI’s founding figures and were influenced
    by the available technology, prevailing scientific paradigms, and broader societal
    trends. Two of these camps were the Symbolists and the Connectionists.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能研究的历史可以理解为几种不同方法或“阵营”之间不断演变的相互作用。这些不同的方法通常反映了人工智能创始人多样化的知识背景，并受到可用技术、主流科学范式和更广泛的社会趋势的影响。其中两个阵营是符号主义者和连接主义者。
- en: The **Symbolists** believed that intelligence could be attained through the
    manipulation of symbols and rules. Their methodologies encompassed logic-based
    systems, rule-based expert systems, and semantic networks. Key figures within
    this camp included John McCarthy, Marvin Minsky, and Herbert Simon. Their primary
    focus was on determining how the essential rules of understanding, logic, and
    reason could be represented within a machine and discerning what these fundamental
    rules encompassed.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 符号主义者认为，通过操纵符号和规则可以获得智能。他们的方法包括基于逻辑的系统、基于规则的专家系统和语义网络。这个阵营中的关键人物包括约翰·麦卡锡、马文·明斯基和赫伯特·西蒙。他们的主要焦点是确定理解、逻辑和推理的基本规则如何在机器中表达，以及这些基本规则包含什么。
- en: The **Connectionists** believed that intelligence emerges from interconnected
    networks of simple units, often referred to as cells or neurons. Their methodologies
    are exemplified by neural networks and deep neural networks, which include diverse
    architectures such as RNNs (recurrent neural networks), CNNs (convolutional neural
    networks), and Transformers. Notable figures within this camp included Frank Rosenblatt,
    Geoffrey Hinton, Yann LeCun, James Rumelhart, and James McClelland. Their primary
    focus was on determining how a machine could learn the necessary rules, how to
    train the said machine, and how to structure its architecture to facilitate learning
    in a manner as comprehensive as a human.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 连接主义者认为，智能是从简单单元相互连接的网络中产生的，这些单元通常被称为细胞或神经元。他们的方法以神经网络和深度神经网络为例，包括RNN（循环神经网络）、CNN（卷积神经网络）和Transformer等多样化的架构。这个阵营中的知名人物包括弗兰克·罗森布拉特、杰弗里·辛顿、杨立昆、詹姆斯·鲁梅尔哈特和詹姆斯·麦克莱兰德。他们的主要焦点是确定机器如何学习必要的规则，如何训练该机器，以及如何构建其架构以促进学习，使其尽可能全面。
- en: We discuss these two camps in more detail later.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在稍后更详细地讨论这两个阵营。
- en: It is important to recognise that advancements have been made in other areas
    of AI beyond the scope of this discussion. [Figure 6.2](#fig6_2) illustrates AI’s
    application in gaming; notably, many of these milestones didn’t employ ANNs until
    more recent times. In specific instances, such as checkers, there is debate as
    to whether an ANN could even match a rule-based approach, especially given that
    it is considered a ‘solved’ game. Our primary focus in this context is on ANNs
    and, consequently, on the types of challenges where ANNs excel, including natural
    language processing (NLP), image recognition and generation, and certain gaming
    scenarios. Nonetheless, AI encompasses a vast range of areas, many of which are
    crucial; however, not all will be discussed here.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要认识到，在这次讨论的范围之外，人工智能的其他领域也取得了进步。[图6.2](#fig6_2)展示了人工智能在游戏中的应用；值得注意的是，许多这些里程碑直到最近才使用ANN（人工神经网络）。在特定情况下，例如国际象棋，有人争论ANN是否甚至能够与基于规则的途径相匹配，尤其是在它被认为是一个“已解决”的游戏的情况下。在此背景下，我们的主要焦点是ANN，因此是ANN在自然语言处理（NLP）、图像识别和生成以及某些游戏场景中表现优异的挑战类型。尽管如此，人工智能涵盖了一个广泛的领域，其中许多领域至关重要；然而，并非所有内容都会在这里讨论。
- en: '![A timeline from 1947 to 2023 that shows the advances of A I and gaming.](images/fig6_2_B.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![从1947年到2023年的时间线，展示了人工智能和游戏的发展。](images/fig6_2_B.jpg)'
- en: '[Figure 6.2 AI and Gaming Timeline.](#R_fig6_2)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.2 人工智能和游戏时间线](#R_fig6_2)'
- en: As we embark on a journey through the rich history of Artificial Intelligence
    (AI), with a keen focus on ANNs, it is important to grasp the evolution of this
    field in stages. This journey not only simplifies our understanding but also provides
    a structured lens through which we can appreciate the milestones, challenges,
    and rapid innovations that AI has undergone. Thus, the history has been broken
    into three phases.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们踏上探索人工智能（AI）丰富历史的旅程，并密切关注ANN时，了解这个领域的演变阶段非常重要。这次旅程不仅简化了我们的理解，而且提供了一个有组织的视角，通过这个视角我们可以欣赏人工智能所经历的里程碑、挑战和快速创新。因此，历史被划分为三个阶段。
- en: '**The Genesis Phase (1940s–1980s)**: This was the birth of AI. During this
    period, the foundational ideas were laid down. Think of these as the foundational
    years of AI, where pioneers set the stage, developing the very first algorithms
    and concepts. Many of the ideas from this era served as the bedrock upon which
    later innovations were built. See [Figure 6.4](#fig6_4) for key milestones in
    the Genesis Phase.![A timeline from 1947 to 2023 that shows the advances of hardware
    and another timeline from 1947 to 2033 for A I periods.](images/fig6_3_B.jpg)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**创世阶段（1940年代–1980年代）**：这是人工智能的诞生。在这一时期，奠定了基础理念。将这些视为人工智能的奠基年份，先驱们为舞台搭建，开发了最初的算法和概念。这一时期许多想法成为了后来创新的基础。参见[图6.4](#fig6_4)了解创世阶段的关键里程碑。![从1947年到2023年的硬件发展时间线以及从1947年到2033年的人工智能时期时间线](images/fig6_3_B.jpg)'
- en: '[Figure 6.3 AI Periods and Hardware Advances.](#R_fig6_3)'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[图6.3 人工智能时期和硬件发展](#R_fig6_3)'
- en: '![A timeline from 1947 to 1994 about the genesis phase and publications and
    models.](images/fig6_4_B.jpg)'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![关于创世阶段和出版物及模型的时间线，从1947年到1994年](images/fig6_4_B.jpg)'
- en: '[Figure 6.4 The Genesis Phase (1940s to1980s) – Publications and Models.](#R_fig6_4)'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[图6.4 创世阶段（1940年代至1980年代）- 出版物和模型](#R_fig6_4)'
- en: '**The Maturing Phase (1980s–2010s)**: After its birth, AI went through a phase
    of growth and maturation. During this period, the foundational ideas from the
    genesis phase were refined, expanded, and implemented in various applications.
    The concepts became clearer, and the tools more sophisticated. It was a time of
    exploration, consolidation, and practical application. See [Figure 6.5](#fig6_5)
    for key milestones in the Maturing Phase.![A timeline from 1980 to 2010 about
    the maturing phase and publications and models.](images/fig6_5_B.jpg)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成熟阶段（1980年代–2010年代）**：在诞生之后，人工智能经历了一个成长和成熟的过程。在这一时期，创世阶段的基础理念得到了细化、扩展并在各种应用中得到实施。概念变得更加清晰，工具也更加复杂。这是一个探索、巩固和实际应用的时代。参见[图6.5](#fig6_5)了解成熟阶段的关键里程碑。![关于成熟阶段和出版物及模型的时间线，从1980年到2010年](images/fig6_5_B.jpg)'
- en: '[Figure 6.5 The Maturing Phase (1980s to 2010s) – Publications and Models.](#R_fig6_5)'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[图6.5 成熟阶段（1980年代至2010年代）- 出版物和模型](#R_fig6_5)'
- en: '**The Acceleration Phase (2010s–2030s)**: Entering the current era, we see
    an explosion in AI capabilities and applications. Thanks to advances in computational
    power, data availability, and refined algorithms, AI, especially ANNs, is now
    evolving at an unprecedented rate and we expect this to continue for some considerable
    time. This phase captures the whirlwind of innovation, and the transformative
    impact AI is having on nearly every facet of our lives. At the end of this phase,
    we expect that AI will have been integrated and adopted into society. It is not
    clear what that picture will look like, but the anticipation of this future drives
    current research and application efforts. For more on what the future may look
    like, we invite you to read [Chapter 7](ch7.xhtml) on some of our predictions.
    See [Figure 6.6](#fig6_6) for key milestones in the Acceleration Phase.![A timeline
    from 2010 to 2023 about the acceleration phase and publications and models.](images/fig6_6_B.jpg)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加速阶段（2010年代–2030年代）**：进入当前时代，我们看到人工智能的能力和应用发生了爆炸式增长。得益于计算能力的提升、数据可用性的增加和算法的改进，人工智能，尤其是人工神经网络，现在正以前所未有的速度发展，我们预计这种趋势将持续相当长一段时间。这一阶段捕捉了创新的风暴，以及人工智能对我们生活的几乎每一个方面的变革性影响。在这一阶段的末期，我们预计人工智能将被整合并应用于社会。目前尚不清楚这个画面将是什么样子，但对这个未来的期待推动了当前的研究和应用努力。关于未来可能的样子，我们邀请您阅读[第7章](ch7.xhtml)中的一些预测。参见[图6.6](#fig6_6)了解加速阶段的关键里程碑。![关于加速阶段和出版物及模型的时间线，从2010年到2023年](images/fig6_6_B.jpg)'
- en: '[Figure 6.6 The Acceleration Phase (2010s to 2030s) – Publications and Models.](#R_fig6_6)'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[图6.6 加速阶段（2010年代至2030年代）- 出版物和模型](#R_fig6_6)'
- en: By breaking down the history of AI and ANNs into these three distinct phases,
    readers will find it easier to digest the information, understand the context
    of current AI developments, and quickly reference the most recent and relevant
    advancements that shape our daily interactions with technology.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将人工智能和人工神经网络的历史划分为这三个不同的阶段，读者将更容易消化信息，理解当前人工智能发展的背景，并快速参考最近和最相关的进步，这些进步塑造了我们与技术的日常互动。
- en: '[Figure 6.3](#fig6_3) helps depict these phases and how they relate to commonly
    referred to periods in AI history such as the AI winters and AI boom periods.
    It also highlights some of the key hardware advancements that have facilitated
    progress in the field. These phases will be discussed in the respective sections
    below.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.3](#fig6_3)有助于描绘这些阶段以及它们与人工智能历史中通常提到的时期（如人工智能寒冬和人工智能繁荣时期）的关系。它还突出了促进该领域进步的一些关键硬件进步。以下各节将分别讨论这些阶段。'
- en: As we delve deeper into each phase, we will uncover the stories, the challenges,
    the breakthroughs, and the visionaries that have made AI the formidable force
    it is today.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们深入到每个阶段，我们将揭示那些使人工智能成为今天如此强大力量的故事、挑战、突破和先驱。
- en: 6.2.1 The Genesis Phase (1940s–1980s)
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.1 创世阶段（1940年代–1980年代）
- en: 6.2.1.1 New Fields of Study
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1.1 新研究领域
- en: In the earliest days of AI, there was a desire to build machines that could
    solve problems by learning, similar to how the brain works. This approach was
    fundamentally different from explicitly programming a machine to solve a specific
    problem. To build these machines, scientists studied animal brains to understand
    their biological models. This can be seen in the work by [McCulloch and Pitts
    (1943)](#ref6_42) on modelling a neuron, and in the work by [Hebb (1949)](#ref6_24)
    on Hebbian Learning. These works, along with contributions from others in the
    fields of neurophysiology and psychology, were used by those in the fields of
    Mathematics, Statistical Theory, Information Science, and Computer Science (both
    Information Science and Computer Science were in their infancy at this point).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工智能的最早阶段，人们希望构建能够通过学习解决问题的机器，类似于大脑的工作方式。这种方法与明确编程机器解决特定问题的方法在本质上不同。为了构建这些机器，科学家们研究了动物大脑，以了解其生物模型。这可以从[McCulloch和Pitts
    (1943)](#ref6_42)关于建模神经元的作品中看出，也可以从[Hebb (1949)](#ref6_24)关于赫布学习的工作中看出。这些作品，以及神经生理学和心理学领域的其他人的贡献，被数学、统计理论、信息科学和计算机科学（当时信息科学和计算机科学都处于起步阶段）领域的人所使用。
- en: Given the availability of these biological models and the extensive research
    conducted in these areas, it might have seemed reasonable to anticipate swift
    solutions to problems that explicit programming struggled with, such as pattern
    recognition, reasoning, and the understanding and use of language.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些生物模型的可获得性和在这些领域进行的广泛研究，预测迅速解决那些显式编程难以解决的问题（如模式识别、推理、语言的理解和使用）似乎是合理的。
- en: 6.2.1.2 The Symbolists
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1.2 符号主义者
- en: 'In 1955, McCarthy, along with Marvin Minsky, Nathaniel Rochester, and Claude
    Shannon, penned a proposal for a workshop to be held at Dartmouth College in the
    summer of 1956\. The primary objective of this workshop was as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 1955年，麦卡锡与Marvin Minsky、Nathaniel Rochester和Claude Shannon共同起草了一份提案，提议在1956年夏天在达特茅斯学院举办研讨会。这次研讨会的首要目标是：
- en: An attempt will be made to find how to make machines use language, form abstractions,
    and concepts, solve kinds of problems now reserved for humans, and improve themselves.
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 将尝试找出如何让机器使用语言、形成抽象和概念、解决目前仅限于人类解决的问题，并自我改进。
- en: ''
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ([McCarthy et al., 1955](#ref6_40), p. 1)
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ([麦卡锡等人，1955](#ref6_40)，第1页)
- en: This workshop is widely regarded as the birth of AI as a formal academic discipline.
    The Dartmouth workshop was the first occasion where the term “artificial intelligence”
    was used, a term coined by McCarthy himself.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这个研讨会被广泛认为是人工智能作为正式学术学科的诞生。达特茅斯研讨会是首次使用“人工智能”一词的场合，这个术语是由麦卡锡本人提出的。
- en: The optimism of the Dartmouth attendees was high. They genuinely believed that
    with a dedicated effort over the summer, they could make significant inroads into
    achieving machine intelligence. This is partly a reflection of the overall optimism
    about technology and computing that pervaded the 1950s. They thought that the
    available computational capacity should be sufficient for their tasks. Their main
    challenge, as they saw it, was the need to develop appropriate algorithms and
    program instructions to guide the machines.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 达特茅斯与会者的乐观情绪很高。他们真诚地相信，通过夏天的专注努力，他们可以取得重大进展，实现机器智能。这在很大程度上反映了20世纪50年代普遍存在的对技术和计算机的乐观态度。他们认为，现有的计算能力应该足以完成他们的任务。他们认为的主要挑战，正如他们所看到的，是开发适当的算法和程序指令来指导机器。
- en: The group was heavily influenced by what later became known as the “Symbolist”
    school of AI. As mentioned, Symbolists believed that intelligence arises from
    the manipulation of symbols and that it can be achieved through rule-based systems
    and formal reasoning. This was in stark contrast to other paradigms, such as the
    Connectionist approach that sought to replicate neural networks and the evolutionary
    approach that took inspiration from Darwinian processes.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这个群体受到了后来被称为“符号主义”的人工智能学派的影响很大。正如所提到的，符号主义者认为智能源于符号的操作，并且可以通过基于规则的系统和形式推理来实现。这与其他范式形成鲜明对比，例如试图复制神经网络联结主义方法和从达尔文过程汲取灵感的进化方法。
- en: This Symbolist approach is reflected in early AI projects like the Logic Theorist
    and General Problem Solver, which aimed to simulate human problem-solving capabilities
    through symbolic manipulations.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这种符号主义方法体现在早期的AI项目中，如逻辑理论家和通用问题求解器，它们旨在通过符号操作模拟人类解决问题的能力。
- en: In hindsight, while the Dartmouth workshop didn’t achieve its lofty goals within
    a single summer, it marked the beginning of a new and transformative field. The
    event catalysed research and set the direction for AI for many years. Even though
    we now recognise the challenges in achieving human-like AI are more complex than
    McCarthy and his colleagues initially imagined, their vision, ambition, and foundational
    work laid the groundwork for the field of AI.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾过去，尽管达特茅斯研讨会没有在短短一个夏天内实现其宏伟目标，但它标志着一个新的、变革性领域的发展。这次活动催化了研究，并为人工智能设定了多年的方向。尽管我们现在认识到实现类似人类的AI的挑战比McCarthy及其同事最初想象的要复杂得多，但他们的愿景、雄心和基础工作为人工智能领域奠定了基础。
- en: 6.2.1.3 The Connectionists
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1.3 联结主义者
- en: Connectionists of AI, often associated with the development and study of Artificial
    Neural Networks (ANNs), take inspiration from the human brain’s intricate web
    of neurons to design computational models. These models aim to emulate the brain’s
    ability to recognise patterns, process information, and learn from experiences.
    Unlike traditional symbolic AI, which relies on explicit rules to make decisions,
    Connectionism adopts a bottom-up approach. By adjusting the connections (or weights)
    between a myriad of simple processing nodes (akin to neurons), Connectionists
    believe that complex cognition can emerge organically. Since their inception,
    ANNs and the Connectionist paradigm have faced waves of both enthusiasm and skepticism.
    However, with advancements in computational power and algorithmic techniques in
    recent decades, Connectionism has become central to many of AI’s most groundbreaking
    achievements, particularly in deep learning and areas such as image and speech
    recognition.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能的联结主义者，通常与人工神经网络（ANNs）的开发和研究相关联，从人脑错综复杂的神经元网络中汲取灵感来设计计算模型。这些模型旨在模拟大脑识别模式、处理信息和从经验中学习的能力。与依赖于显式规则做出决策的传统符号人工智能不同，联结主义采用自下而上的方法。通过调整众多简单处理节点（类似于神经元）之间的连接（或权重），联结主义者相信复杂的认知可以自然地产生。自从它们诞生以来，ANNs和联结主义范式就经历了热情和怀疑的浪潮。然而，随着近年来计算能力和算法技术的进步，联结主义已经成为人工智能许多突破性成就的核心，尤其是在深度学习和图像、语音识别等领域。
- en: 'In the early days of Artificial Intelligence, there was a promising glimmer
    on the horizon, a belief that ‘neuron nets’ might just hold the answer to some
    of AI’s most perplexing questions. Among these was the intriguing query, “How
    can a set of hypothetical neurons be arranged to form concepts?” Scholars like
    Uttley, Rashevsky and his ensemble, the duo Farley and Clark, and pioneers like
    Pitts, McCulloch, Minsky, Rochester, and Holland, were all engrossed in deciphering
    this enigma. Despite their vast and varied contributions, the consensus remained:
    the field desperately needed more theoretical depth ([McCarthy et al., 1955](#ref6_40)).'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工智能的早期，天际线上出现了一线希望的曙光，人们相信“神经网络”可能正是解决人工智能一些最令人困惑问题的答案。其中就包括一个引人入胜的问题：“如何安排一组假设的神经元以形成概念？”像Uttley、Rashevsky及其团队，Farley和Clark这对搭档，以及Pitts、McCulloch、Minsky、Rochester和Holland等先驱学者都沉迷于破解这个谜团。尽管他们做出了广泛而多样的贡献，但共识始终是：该领域迫切需要更多的理论深度（[McCarthy等，1955](#ref6_40)）。
- en: 'The legacy of artificial neural networks (ANNs) is deeply intertwined with
    AI’s history. These networks, which might seem contemporary, have been the bedrock
    of AI research since its inception. The landmark Dartmouth summer project was
    indeed a milestone, but even before this event, McCulloch and his colleagues had
    ventured into the neural realms. Their 1943 publication, ‘A logical calculus of
    the ideas immanent in nervous activity’ ([McCulloch et al., 1943](#ref6_42)),
    presented an avant-garde notion: neurons could be emulated using simplistic switches,
    which when networked in unique arrangements, could replicate the logic of a Turing
    machine.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络(ANNs)的遗产与人工智能的历史紧密相连。这些网络可能看起来很现代，但它们自人工智能诞生以来一直是人工智能研究的基础。标志性的达特茅斯夏季项目确实是一个里程碑，但在此事件之前，麦卡洛克和他的同事们就已经进入了神经领域。他们的1943年出版物《神经活动内在逻辑的逻辑演算》([McCulloch
    et al., 1943](#ref6_42))提出了一种前卫的观点：神经元可以用简单的开关来模拟，当以独特的排列连接时，可以复制图灵机的逻辑。
- en: However, it wasn’t until 1958 that the perceptron, a term now synonymous with
    AI, was born. In 1958, Frank Rosenblatt wrote about it in his paper ([Rosenblatt,
    1958](#ref6_58)). Think of it like a mini-brain with different parts working together,
    similar to modern ANNs. This mini-brain had different types of units, known as
    S-units, A-units, and R-units, which formed a basic two-layer network. Though
    at the time, people saw it as a ‘three-layered network’. Rosenblatt called it
    ‘photoperceptron’, and it was designed to recognise objects in photos, like circles
    and rectangles ([Rosenblatt, 1957](#ref6_57)).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，直到1958年，感知器这个现在与人工智能同义的术语才诞生。1958年，弗兰克·罗森布拉特在他的论文中提到了它([Rosenblatt, 1958](#ref6_58))。想象一下，它就像一个拥有不同部分协同工作的微型大脑，类似于现代的ANNs。这个微型大脑有不同类型的单元，被称为S单元、A单元和R单元，它们构成了一个基本的两层网络。尽管在当时，人们认为它是一个“三层网络”。罗森布拉特称之为“光感知器”，它被设计用来识别照片中的物体，如圆形和矩形([Rosenblatt,
    1957](#ref6_57))。
- en: The 1960s saw the perceptron’s blueprint being taken and expanded upon. Widrow
    and Hoff birthed ADALINE ([Widrow, 1960](#ref6_81)) and MADALINE ([Widrow, 1962](#ref6_82)),
    employing novel algorithms to fine-tune weights and biases, laying the foundation
    for modern feed forward networks. Block and team ([Block et al., 1962](#ref6_8))
    added a layer to the perceptron model, propelling it into the realm of deep neural
    networks. Their model aimed for more extensive image classification, however,
    training this improved network was tough because there weren’t yet good methods
    to correct errors in multi-layer networks.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 20世纪60年代，感知器的蓝图被采纳并扩展。 Widrow 和 Hoff 创造了 ADALINE ([Widrow, 1960](#ref6_81))
    和 MADALINE ([Widrow, 1962](#ref6_82))，采用新颖的算法来微调权重和偏差，为现代前馈网络奠定了基础。Block 和他的团队([Block
    et al., 1962](#ref6_8))向感知器模型添加了一层，将其推进到深度神经网络领域。他们的模型旨在进行更广泛的目标分类，然而，训练这个改进的网络很困难，因为当时还没有好的方法来纠正多层网络中的错误。
- en: 'In 1969, however, a cautionary tale emerged. Minsky and others published “Perceptrons:
    An introduction to computational geometry” ([Minsky & Papert, 1988](#ref6_45)),
    highlighting the perceptron’s mathematical limitations. The book particularly
    scrutinised the single-layer perceptron and expressed skepticism about its potential
    in resolving intricate challenges.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，到了1969年，一个警示故事出现了。明斯基和其他人出版了《感知器：计算几何导论》([Minsky & Papert, 1988](#ref6_45))，强调了感知器的数学局限性。这本书特别审视了单层感知器，并对它在解决复杂挑战中的潜力表示怀疑。
- en: In 1985, Rumelhart and his team introduced a new technique that helped teach
    multi-layered machines. This method, which corrected mistakes as it learned, opened
    the door for what we call deep learning. Still, we didn’t have the powerful computers
    needed to fully use this new technique. It is worth noting that Paul Werbos’ earlier
    doctoral dissertation ([Werbos, 1974](#ref6_80)) had pre-empted Rumelhart’s work,
    laying the groundwork for backpropagation.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 1985年，鲁梅尔哈特和他的团队介绍了一种新技术，帮助教授多层机器。这种方法在学习过程中纠正错误，为我们现在所说的深度学习打开了大门。尽管如此，我们还没有足够强大的计算机来充分利用这项新技术。值得注意的是，保罗·韦伯斯更早的博士论文([Werbos,
    1974](#ref6_80))已经预见了鲁梅尔哈特的工作，为反向传播奠定了基础。
- en: Intriguingly, Minsky and Papert, in an extended edition of their earlier book
    ([Minsky & Papert, 1988](#ref6_45)), critically reviewed advances like those made
    by Rumelhart and McClelland. Despite acknowledging progress, they felt the claims,
    especially those surrounding the efficiency of gradient descent and the generalised
    delta rule, were overstated. The debates and differences of opinion serve as reminders
    of the cautious notes sounded by thinkers like Weizenbaum in the 1970s (see the
    next section on philosophical concerns), warning against getting too excited too
    soon.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 令人好奇的是，明斯基和帕佩特在他们早期书籍的扩展版中([Minsky & Papert, 1988](#ref6_45))，批判性地回顾了诸如鲁梅尔哈特和麦克莱兰德所取得的进步。尽管他们承认了进步，但他们认为这些主张，尤其是关于梯度下降效率和广义delta规则的主张，被过度夸大了。这些辩论和意见分歧提醒我们，像韦泽鲍姆这样的思想家在20世纪70年代发出的谨慎警告（参见下一节关于哲学问题的内容），警告人们不要过早地过于兴奋。
- en: Over time, even with advances like this new teaching method, backpropagation,
    the big hopes for ANNs started to fade. This led to periods known as ‘AI winters’
    where people lost interest and didn’t invest much in AI. But, as always, after
    a cold period of winter, things tend to warm up again in spring.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，即使有了像这种新的教学方法——反向传播这样的进步，人工神经网络的大希望也开始消退。这导致了被称为“人工智能冬天”的时期，人们对人工智能失去了兴趣，没有在人工智能上投入太多。但，就像往常一样，在寒冷的冬天之后，事情在春天往往会再次回暖。
- en: 6.2.1.4 Philosophical and Ethical Considerations
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1.4 哲学和伦理考虑
- en: 6.2.1.4.1 The Turing Test – Alan Turing
  id: totrans-66
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.2.1.4.1 图灵测试 – 艾伦·图灵
- en: Philosophical questions concerning the ability of machines to think emerged
    in these early days. In 1950, Alan Turing addressed this in his paper, “Computing
    Machinery and Intelligence” ([Turing, 1950](#ref6_73)). Instead of directly asking
    “can machines think?”, Turing proposed an alternative method. He suggested a game,
    which he named “The Imitation Game”. This game measured the likelihood of a machine
    deceiving an interrogator against the probability of a real person doing the same,
    relying solely on written communications in a question-and-answer format. This
    challenge subsequently became renowned as the Turing test.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 关于机器能否思考的哲学问题在早期就出现了。1950年，艾伦·图灵在他的论文“计算机与智能”([Turing, 1950](#ref6_73))中讨论了这个问题。图灵没有直接问“机器能思考吗？”而是提出了一个替代方法。他建议一个游戏，他称之为“模仿游戏”。这个游戏衡量了机器欺骗审问者的可能性与真实人做同样事情的概率，仅依靠问答格式中的书面交流。这个挑战随后成为著名的图灵测试。
- en: 6.2.1.4.2 Three Laws of Robotics – Isaac Asimov
  id: totrans-68
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.2.1.4.2 机器人三大定律 – 艾萨克·阿西莫夫
- en: In a related development around the ethical considerations regarding machine
    intelligence, Isaac Asimov – a science fiction writer and professor of biochemistry
    – developed the “Three Laws of Robotics” in 1942\. These were featured in his
    short story “Runaround”, which formed part of the “I, Robot” series ([Asimov,
    1950](#ref6_4)), as mentioned in [Chapter 5](ch5.xhtml).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在关于机器智能的伦理考虑的相关发展中，科幻作家和生物化学教授艾萨克·阿西莫夫在1942年提出了“机器人三大定律”。这些定律出现在他的短篇小说“Runaround”中，这是“我，机器人”系列的一部分([Asimov,
    1950](#ref6_4))，如[第5章](ch5.xhtml)中提到的。
- en: A robot may not injure a human being, or, through inaction, allow a human being
    to come to harm.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 机器人不得伤害人类，或者，通过不作为，让人类受到伤害。
- en: A robot must obey the orders given it by human beings except where such orders
    would conflict with the First Law.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 机器人必须服从人类给予它的命令，除非这些命令与第一定律相冲突。
- en: A robot must protect its own existence as long as such protection does not conflict
    with the First or Second Laws.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 机器人必须保护自己的存在，只要这种保护不与第一或第二定律相冲突。
- en: 6.2.1.4.3 Computer Power and Human Reason – Joseph Weizenbaum
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.2.1.4.3 计算机力量与人类理性 – 约瑟夫·韦泽鲍姆
- en: 'Joseph Weizenbaum, the creator of the ELIZA chatbot and author of *Computer
    Power and Human Reason: From Judgment to Calculation* ([1976](#ref6_78)), grew
    increasingly wary of the expanding influence of computer technology. He argued
    that machines shouldn’t handle tasks needing genuine compassion, emphasising their
    inability to exercise human judgement and distinguishing between mere decision-making
    and genuine choice.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '约瑟夫·韦泽鲍姆，ELIZA聊天机器人的创造者，以及《计算机力量与人类理性：从判断到计算》(*Computer Power and Human Reason:
    From Judgment to Calculation*, [1976](#ref6_78))的作者，越来越警惕计算机技术的扩张影响。他认为机器不应该处理需要真正同情心的任务，强调它们无法行使人类判断力，区分单纯的决策和真正的选择。'
- en: During the 1970s, skepticism about computers replicating human thought was prevalent.
    Many felt that the AI community had made overly ambitious promises. In his book,
    Weizenbaum delved into the overhyped expectations of AI and the unsettling emotional
    bonds people formed with AI systems. He also expressed concerns about society’s
    growing dependency on technology. As we will see, these ambitious claims would
    continue to be a major topic of debate in the AI world.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在20世纪70年代，对计算机复制人类思维的怀疑情绪很普遍。许多人认为，人工智能界做出了过于雄心勃勃的承诺。在他的书中，Weizenbaum深入探讨了人工智能被过度炒作的期望以及人们与人工智能系统形成的令人不安的情感纽带。他还表达了对社会日益依赖技术的担忧。正如我们将看到的，这些雄心勃勃的声明将继续成为人工智能世界的主要辩论话题。
- en: 6.2.1.4.4 Father of Cybernetics – Norbert Wiener
  id: totrans-76
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.2.1.4.4 控制论之父 – 诺伯特·维纳
- en: 'Furthermore, Norbert Wiener, often referred to as the “father of cybernetics”,
    expressed concerns about the potential for automation to lead to unintended consequences
    in his work “God & Golem, Inc.: A Comment on Certain Points where Cybernetics
    Impinges on Religion” ([Wiener, 1966](#ref6_83)) He posited that once we can effectively
    replicate human decision-making processes, extreme caution is imperative in managing
    these innovations. To illustrate, he drew upon the tale of ‘The Monkey’s Paw’,
    highlighting how wielding power without full comprehension could result in catastrophic
    outcomes.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，常被称为“控制论之父”的诺伯特·维纳（Norbert Wiener）在他的著作《上帝与哥莱姆，公司：关于控制论对宗教的影响的几点评论》（“God
    & Golem, Inc.: A Comment on Certain Points where Cybernetics Impinges on Religion”）([Wiener,
    1966](#ref6_83))中表达了对自动化可能带来意外后果的担忧。他提出，一旦我们能够有效地复制人类的决策过程，对这些创新的管理就必须极端谨慎。为了说明这一点，他引用了《猴爪》的故事，强调在不完全理解的情况下掌握权力可能导致灾难性的后果。'
- en: From the 1940s to the 1980s, the field of AI embarked on an exhilarating journey,
    laying the bedrock upon which future innovations would thrive. The essence of
    ANNs drew inspiration from the intricate webs of neurons in our brains, thanks
    to the pioneering work of individuals like McCulloch and Pitts. As the decades
    progressed, this biological inspiration was interwoven with a fabric of mathematical
    and statistical rigour, establishing proofs and formulas that defined how artificial
    neurons would interact and collaborate within networks. By the 1980s, this foundational
    work bore fruit in the form of breakthrough learning techniques like backpropagation
    and gradient descent. These mechanisms optimised the training of ANNs, equipping
    them with the capability to refine their performance and solve complex problems.
    Alongside these technical advancements, the era was also marked by deep philosophical
    introspection, questioning the boundaries of AI, its relation to consciousness,
    and the ethics surrounding its potential. By the close of the 1980s, the AI landscape
    had been primed with the necessary fundamentals, ready for the transformations
    that would follow including the concept of the quantum computer such as those
    discussed by Richard Feynman ([Feynman, 1982](#ref6_17)) that are perhaps a still
    long way off from delivering their benefits to AI.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 从20世纪40年代到80年代，人工智能领域经历了一段令人兴奋的旅程，为未来创新奠定了坚实的基础。人工神经网络（ANNs）的本质受到了我们大脑中神经元复杂网络的启发，这得益于像麦卡洛克（McCulloch）和皮茨（Pitts）这样的先驱者的开创性工作。随着岁月的推移，这种生物灵感与数学和统计严谨性的织锦交织在一起，确立了定义人工神经元如何在网络中交互和协作的证明和公式。到20世纪80年代，这项基础工作以反向传播和梯度下降等突破性学习技术的形式结出了果实。这些机制优化了ANNs的训练，使它们能够改进其性能并解决复杂问题。与此同时，这个时代也以深刻的哲学内省为标志，质疑人工智能的边界、它与意识的关系以及围绕其潜力的伦理问题。到20世纪80年代末，人工智能领域已经具备了必要的基石，为随后发生的转型做好了准备，包括像理查德·费曼（Richard
    Feynman）讨论的量子计算机概念（[Feynman, 1982](#ref6_17)），这些可能还有很长的路要走才能将它们的益处带给人工智能。
- en: 6.2.2 The Maturing Phase (1980s to 2010s)
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.2 成熟阶段（1980年代至2010年代）
- en: In the span between the 1980s and 2010s, ANNs experienced both challenges and
    pivotal advancements. Initial enthusiasm for ANNs had diminished due to persistent
    issues like the vanishing gradient problem, but the 1980s brought about key innovations
    that would later be instrumental for the field’s revival.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在1980年代和2010年代之间，人工神经网络（ANNs）既面临挑战，也取得了关键性的进步。由于持续存在的问题，如梯度消失问题，对ANNs的初始热情已经减弱，但20世纪80年代带来了对领域复兴至关重要的关键创新。
- en: The introduction of the backpropagation algorithm by Rumelhart, Hinton, and
    Williams in 1986 was a seminal moment (Rumelhart et al., [1986a](#ref6_61), [1986b](#ref6_62),
    [1986c](#ref6_63)). This algorithm optimised the weights in multi-layer perceptrons,
    sparking renewed interest and research in ANNs. Following this, the torchbearers
    of the field, namely LeCun, Bengio, and Hinton, made significant contributions.
    Their efforts during this phase laid the foundation for deep learning and the
    transformative changes of the next era ([Goodfellow et al., 2016](#ref6_22)).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Rumelhart、Hinton和Williams在1986年引入反向传播算法是一个里程碑式的事件(Rumelhart et al., [1986a](#ref6_61),
    [1986b](#ref6_62), [1986c](#ref6_63))。该算法优化了多层感知器的权重，激发了人们对人工神经网络的新一轮兴趣和研究。在此之后，该领域的领军人物LeCun、Bengio和Hinton做出了重大贡献。他们在这一阶段的工作为深度学习和下一个时代的变革性变化奠定了基础([Goodfellow
    et al., 2016](#ref6_22))。
- en: In the 1990s, there were important developments in using neural networks. Researchers
    found ways to use them for complex data analysis ([Breiman, 2001](#ref6_9)), to
    simplify data by reducing its complexity ([Hinton et al., 2006](#ref6_27)), and
    to improve algorithms for better performance ([Friedman, 2002](#ref6_18)). They
    also developed new techniques for training and managing neural networks, like
    energy-based models ([Ackley et al., 1985](#ref6_1)), and introduced new types
    of neural cells, such as long short-term memory (LSTM), to solve problems with
    gradients ([Hochreiter & Schmidhuber, 1997](#ref6_28)). Additionally, researchers
    gained a better understanding of Support Vector Machines (SVM) and kernel methods,
    which improved the creation of robust predictive models ([Wahba et al., 2002](#ref6_75)).
    With innovations like LeCun’s convolutional neural networks (CNNs) for image recognition
    ([LeCun et al., 1989](#ref6_37)) and the advent of recurrent neural networks (RNNs)
    for sequential data ([Hochreiter & Schmidhuber, 1997](#ref6_28)), the vast potential
    of ANNs was increasingly realised.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在20世纪90年代，神经网络的应用取得了重要进展。研究人员找到了将它们用于复杂数据分析的方法([Breiman, 2001](#ref6_9))，通过降低其复杂性来简化数据([Hinton
    et al., 2006](#ref6_27))，以及改进算法以获得更好的性能([Friedman, 2002](#ref6_18))。他们还开发了新的训练和管理神经网络的技术，如基于能量的模型([Ackley
    et al., 1985](#ref6_1))，并引入了新的神经网络细胞类型，例如长短期记忆(LSTM)，以解决梯度问题([Hochreiter & Schmidhuber,
    1997](#ref6_28))。此外，研究人员对支持向量机(SVM)和核方法有了更好的理解，这提高了鲁棒预测模型的创建([Wahba et al., 2002](#ref6_75))。随着LeCun的卷积神经网络(CNNs)在图像识别([LeCun
    et al., 1989](#ref6_37))和循环神经网络(RNNs)在序列数据([Hochreiter & Schmidhuber, 1997](#ref6_28))的引入，人工神经网络(ANNs)的巨大潜力逐渐被认识到。
- en: However, the full potential of deep ANNs was hampered by computational restraints
    and the scarcity of comprehensive labelled datasets. Despite these hurdles, the
    maturing phase was crucial in embedding the importance of ANNs and setting the
    groundwork for the momentous progress of the acceleration phase that would follow.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，深度人工神经网络的全部潜力受到了计算限制和全面标记数据集稀缺性的制约。尽管存在这些障碍，成熟阶段对于嵌入人工神经网络的重要性以及为随后加速阶段的重大进展奠定基础至关重要。
- en: 6.2.3 The Acceleration Phase (2010s to 2030s)
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.3 加速阶段（2010年代至2030年代）
- en: The early 2010s marked a monumental shift for ANNs with developments powered
    by larger datasets, increased computational capabilities - particularly from GPUs,
    and innovative algorithms. One notable paper in this regard was “Building High-level
    Features Using Large Scale Unsupervised Learning” by Le et al., which showcased
    the strength of refined algorithms in propelling ANNs’ capabilities ([Le et al.,
    2011](#ref6_36)).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 2010年代初，随着更大数据集的发展、计算能力的提升——尤其是GPU的计算能力，以及创新算法的推动，人工神经网络经历了重大的转变。在这方面，一篇引人注目的论文是Le等人撰写的“使用大规模无监督学习构建高级特征”，展示了精炼算法在推动人工神经网络能力方面的优势([Le
    et al., 2011](#ref6_36))。
- en: The subfield of deep learning, an advanced application of ANNs, took centre
    stage during this period. Esteemed figures like Geoffrey Hinton, Yann LeCun, and
    Yoshua Bengio were instrumental in these evolutions ([Goodfellow et al., 2016](#ref6_22)).
    A groundbreaking moment occurred with the creation of AlexNet, a convolutional
    neural network (CNN) designed by Alex Krizhevsky under Geoffrey Hinton’s guidance.
    In 2012, AlexNet achieved unmatched performance in the ImageNet competition, a
    benchmark in image recognition, marking what many called the “ImageNet moment”
    ([Krizhevsky et al., 2012](#ref6_33)).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个时期，深度学习这一神经网络的高级应用领域成为了焦点。像Geoffrey Hinton、Yann LeCun和Yoshua Bengio这样的知名人物在这些演变中发挥了关键作用（Goodfellow等人，2016年[#ref6_22]）。一个突破性的时刻是AlexNet的创建，这是在Geoffrey
    Hinton的指导下，由Alex Krizhevsky设计的卷积神经网络（CNN）。2012年，AlexNet在图像识别基准ImageNet竞赛中取得了无与伦比的成绩，这被许多人称为“ImageNet时刻”（Krizhevsky等人，2012年[#ref6_33]）。
- en: Such triumphs galvanised the broader tech industry. Companies like Google and
    Facebook not only adopted but also significantly contributed to ANN research.
    For instance, Google’s approach to recommendation systems was articulated in “Wide
    & Deep Learning for Recommender Systems” ([Cheng et al., 2016](#ref6_12)), while
    Facebook advanced text understanding and user modelling through ANNs, as evidenced
    by their studies ([Joulin et al., 2016](#ref6_31); [Kaur et al., 2021](#ref6_32)).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的成功激励了整个科技行业。像谷歌和Facebook这样的公司不仅采用了神经网络研究，而且还做出了重大贡献。例如，谷歌在推荐系统方面的方法在“Wide
    & Deep Learning for Recommender Systems”一文中得到了阐述（Cheng等人，2016年[#ref6_12]），而Facebook通过神经网络在文本理解和用户建模方面取得了进展，如他们的研究所示（Joulin等人，2016年[#ref6_31]；Kaur等人，2021年[#ref6_32]）。
- en: As the decade progressed, ANNs’ applications diversified. Recurrent neural networks
    (RNNs), and particularly their evolved form, long short-term memory networks (LSTMs),
    became the go-to for handling sequential data, finding applications in natural
    language processing and time-series analysis ([Hochreiter et al., 1997](#ref6_28)).
    In parallel, generative adversarial networks (GANs), introduced by Ian Goodfellow,
    transformed generative models, enabling feats like image synthesis and style transfer
    ([Goodfellow et al., 2014](#ref6_23)).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 随着这个十年的发展，神经网络的用途变得更加多样化。循环神经网络（RNNs），尤其是它们演化的形式，长短期记忆网络（LSTMs），成为处理序列数据的首选，并在自然语言处理和时间序列分析中找到了应用（Hochreiter等人，1997年[#ref6_28]）。同时，由Ian
    Goodfellow引入的生成对抗网络（GANs）改变了生成模型，使得图像合成和风格迁移等壮举成为可能（Goodfellow等人，2014年[#ref6_23]）。
- en: By the late 2010s, the introduction of transformer architectures, like the one
    described in the “Attention Is All You Need” paper, signalled another paradigm
    shift, especially in NLP. This paper served as a bedrock for models such as BERT
    and GPT variants, including OpenAI’s ChatGPT-3.5 and GPT 4.0, redefining text
    comprehension and generation ([Vaswani et al., 2017](#ref6_74)).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 到2010年代末，Transformer架构的引入，如“Attention Is All You Need”论文中描述的，标志着另一个范式转变，尤其是在自然语言处理（NLP）领域。这篇论文为BERT和GPT变体等模型奠定了基础，包括OpenAI的ChatGPT-3.5和GPT
    4.0，重新定义了文本理解和生成（Vaswani等人，2017年[#ref6_74]）。
- en: With the dawn of the 2020s, the drive behind ANNs only intensified. Breakthroughs
    in areas like few-shot learning and self-supervised learning signalled the continuous
    advancement of the field. Yet, as AI capabilities surged, so did ethical and societal
    concerns. These concerns were highlighted in March 2023 open letter from the Future
    of Life Institute, urging a pause on the development of models more advanced than
    GPT-4, underlining the need for a more reflective approach to AI’s rapid advancement
    (Future of Life Institute, [2023a](#ref6_19), [2023b](#ref6_20)).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 随着2020年代的到来，神经网络（ANNs）的推动力不断增强。在少样本学习和自监督学习等领域取得的突破标志着该领域的持续进步。然而，随着人工智能能力的提升，伦理和社会问题也随之而来。这些问题在2023年3月未来生命研究所发布的公开信中得到了强调，该信呼吁暂停开发比GPT-4更先进的模型，并强调了需要采取更反思的方法来应对人工智能的快速进步（未来生命研究所，[2023a](#ref6_19)，[2023b](#ref6_20)）。
- en: While the path towards artificial general intelligence (AGI) or even artificial
    super intelligence (ASI) remains debated, what is unequivocal is the transformative
    role of AI will have on society and humanity. In addition, this transformative
    role is likely to be further compounded by future advancements. Echoing Richard
    Feynman’s proposition, quantum computers might surpass classical computers in
    simulating complex systems ([Feynman, 1982](#ref6_17)), potentially steering AI
    into its next epoch. Indeed, milestones like achieving quantum supremacy, where
    quantum devices outpace their classical counterparts, hint at a future where the
    bounds of AI’s potential could be redefined ([Arute et al., 2019](#ref6_3)).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管通往通用人工智能（AGI）或甚至人工超级智能（ASI）的道路仍然存在争议，但AI对社会和人类将产生的变革性作用是毋庸置疑的。此外，这种变革性作用很可能会因未来的进步而进一步加剧。正如理查德·费曼的命题，量子计算机可能在模拟复杂系统方面超越经典计算机([费曼，1982](#ref6_17))，这可能会将AI引向其下一个时代。确实，像实现量子霸权这样的里程碑，即量子设备超越其经典对应物，预示着一个未来，在那里AI潜力的界限可能会被重新定义([Arute等人，2019](#ref6_3))。
- en: 6.3 Creating a Model
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 创建模型
- en: Now that we have covered the history of ANNs in some detail, we will turn our
    attention to the process of creating a GenAI model. This discussion will focus
    on recent approaches and will delve deeper into the terminology and advances previously
    outlined in the historical overview. As we delve into the creation and comprehension
    of models, you will recognise some of the key people and advances mentioned in
    the earlier history section.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经详细介绍了人工神经网络（ANNs）的历史，我们将把注意力转向创建GenAI模型的过程。这次讨论将侧重于最近的方法，并深入探讨在历史概述中先前概述的术语和进展。随着我们深入到模型创建和理解的探讨中，你将认识到在早期历史部分提到的某些关键人物和进展。
- en: '[Figure 6.7](#fig6_7) illustrates the essential steps in creating a model.
    This process encompasses both the training and the testing of a foundational model,
    followed by its fine-tuning. A foundational model is an initial, untrained model
    with a generic purpose (i.e.) to learn from data. Many foundational models undergo
    fine-tuning for specific applications. For instance, the LLaMA-2 (Large Language
    Model Meta AI) foundational model was fine-tuned for chat functionality, resulting
    in the finely-tuned model known as LLaMA-2-chat. This fine-tuning involves further
    training of the foundational model on specific data to serve a more specialised
    purpose, such as functioning as a chatbot to answer user queries or engage in
    conversations with users.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.7](#fig6_7)展示了创建模型的基本步骤。这个过程包括基础模型的训练和测试，然后进行微调。基础模型是一个初始的、未经训练的具有通用目的的模型（即）从数据中学习。许多基础模型经过微调以适应特定应用。例如，LLaMA-2（大型语言模型Meta
    AI）基础模型经过微调以实现聊天功能，从而产生了被称为LLaMA-2-chat的精细调整模型。这种微调涉及在特定数据上进一步训练基础模型，以实现更专门的目的，例如作为聊天机器人来回答用户查询或与用户进行对话。'
- en: '![A model which shows the larger picture about creating a model.](images/fig6_7_B.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![展示创建模型更大图景的模型图](images/fig6_7_B.jpg)'
- en: '[Figure 6.7 Creating a Model – The Big Picture.](#R_fig6_7)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.7 创建模型 – 大图景](#R_fig6_7)'
- en: 'The steps involved in creating a model are:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 创建模型涉及的步骤包括：
- en: The Training Data
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练数据
- en: Data Gathering and Preparation
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据收集和准备
- en: Dataset Customisation
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据集定制
- en: The Foundation Model
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基础模型
- en: Model Design and Structuring
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型设计和结构化
- en: Model Training
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型训练
- en: Model Testing
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型测试
- en: The Fine-Tuning
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 微调
- en: Fine-Tuning Data Set Customisation
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 微调数据集定制
- en: Model Fine-Tuning Training
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型微调训练
- en: Model Testing
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型测试
- en: The Deployment and Use
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署和使用
- en: Model Deployment
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型部署
- en: Model Monitoring
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型监控
- en: Model Use (aka inference)
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型使用（又称推理）
- en: Fine-tuning can employ a variety of training techniques, including reinforcement
    learning with human feedback (RLHF), which will be discussed below. The following
    sections will cover each of these areas to provide an understanding of what is
    involved.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 微调可以采用各种训练技术，包括带有人类反馈的强化学习（RLHF），这将在下文讨论。以下各节将涵盖这些领域，以提供对这些内容的理解。
- en: 6.3.1 The Training Data
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.1 训练数据
- en: The initial step in the comprehensive model creation process is data gathering
    and preparation. Modern deep neural networks (DNNs), especially large language
    models (LLMs), require vast amounts of training data. While several well-known
    data sources are available, some companies maintain their own unique repositories.
    Prominent entities such as Meta, Google, and Microsoft possess vast amounts of
    data, owing to the diverse services they offer. Meanwhile, companies like OpenAI
    have ventured into developing their own data-sourcing solutions, such as the web
    crawling tool, GPTbot, which gathers publicly accessible data from the web for
    OpenAI.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 综合模型创建过程的初始步骤是数据收集和准备。现代深度神经网络（DNNs），尤其是大型语言模型（LLMs），需要大量的训练数据。虽然有一些知名的数据源可用，但一些公司维护他们自己的独特存储库。像Meta、Google和Microsoft这样的知名实体由于提供多样化的服务，拥有大量的数据。同时，像OpenAI这样的公司已经涉足开发他们自己的数据源解决方案，例如网络爬虫工具GPTbot，它从网络上收集公开可访问的数据供OpenAI使用。
- en: After the initial acquisition, the dataset might be further refined to improve
    quality or to better align with the desired outcomes of a particular model. The
    processes of gathering and preparation often intertwine with dataset customisation,
    especially when an organisation sources its data directly, rather than relying
    on externally provided datasets.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在初步获取后，数据集可能需要进一步精炼以提高质量或更好地与特定模型的预期结果相匹配。收集和准备的过程通常与数据集定制交织在一起，尤其是在组织直接获取数据而不是依赖外部提供的数据集时。
- en: It is crucial to acknowledge the complexity and sophistication inherent in the
    data gathering, preparation, and customisation phases. To provide a clearer understanding
    of these processes, they will be explored in more depth using several widely utilised
    data sources as examples.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 认识到数据收集、准备和定制阶段固有的复杂性和复杂性至关重要。为了更清晰地理解这些过程，将使用几个广泛使用的数据源作为例子进行更深入的探讨。
- en: 6.3.1.1 Text Data – Common Crawl
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.1.1 文本数据 – Common Crawl
- en: Common Crawl (see [https://commoncrawl.org/](https://commoncrawl.org)) ([Patel,
    2020](#ref6_49)) is an organisation that offers data dumps from billions of web
    pages on a regular basis, typically 6 to 12 times per year. While it has some
    data from as far back as 2008, there have been more consistent outputs since 2011\.
    Common Crawl employs a tool known as a spider bot to access the content of web
    pages. A spider bot starts with a list of URLs and retrieves the content of these
    web pages. Additionally, it can identify other URLs within a web page (i.e., links
    to different sites) and pursue these links to access those pages. For each page
    accessed, the spider bot saves the content and recognises other links to pursue,
    expanding outwards much like a spider’s web to encompass other URLs.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Common Crawl（见[https://commoncrawl.org/](https://commoncrawl.org)）([Patel, 2020](#ref6_49)）是一个定期提供数十亿网页数据转储的组织，通常每年6到12次。虽然它有一些可以追溯到2008年的数据，但自2011年以来，输出更加一致。Common
    Crawl使用一种称为爬虫机器人的工具来访问网页内容。爬虫机器人从一个URL列表开始，检索这些网页的内容。此外，它还可以识别网页内的其他URL（即指向不同网站的链接）并追踪这些链接以访问那些页面。对于每个访问的页面，爬虫机器人保存内容并识别其他要追踪的链接，向外扩展，就像蜘蛛网一样，涵盖其他URL。
- en: The CCBot, commonly referred to as the Common Crawl spider bot, processes a
    candidate database of URLs for each iteration. It updates this database with new
    URLs based on its findings during the crawl. While the set of URLs is expansive,
    they are not rigorously reviewed, organised, or managed. As a result, the quality
    of the data obtained can vary significantly, encompassing content in multiple
    languages. Surprisingly, there is a minimal overlap between the outputs of one
    crawl and the next. For instance, the overlap between the data dumps from February
    2023 and June 2023 is only around 1% (see [https://commoncrawl.github.io/cc-crawl-statistics/plots/crawloverlap](https://commoncrawl.github.io)).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: CCBot，通常被称为通用爬虫机器人，为每一轮迭代处理候选URL数据库。它根据爬取过程中的发现更新此数据库中的新URL。虽然URL集合很大，但它们并没有经过严格的审查、组织或管理。因此，获得的数据质量可能差异很大，包括多种语言的内容。令人惊讶的是，一次爬取的输出与下一次爬取之间的重叠非常小。例如，2023年2月和6月的数据转储之间的重叠仅为大约1%（见[https://commoncrawl.github.io/cc-crawl-statistics/plots/crawloverlap](https://commoncrawl.github.io))）。
- en: The volume of data provided in each release is indeed staggering. For example,
    the June 2023 release (CC-MAIN-2023-23) comprises 100 segments. Each segment contains
    800 files for WARC (approximately 1.1 GB compressed), WAT (around 285 MB compressed),
    and WET (roughly 115 MB compressed) formats. Including various metadata, the total
    size of the June 2023 release is about 120 TB when compressed. The compression
    ratio for text data is favourable, ranging from 3 to 5 times, which results in
    an uncompressed size of approximately 580 TB. However, not all users will need
    every format. Some might focus only on the WET format (plaintext), which alone
    amounts to about 26 TB when uncompressed.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 每次发布的提供数据量确实令人震惊。例如，2023年6月的发布（CC-MAIN-2023-23）包含100个段。每个段包含800个WARC（大约1.1 GB压缩）、WAT（大约285
    MB压缩）和WET（大约115 MB压缩）格式的文件。包括各种元数据，2023年6月发布的压缩总大小约为120 TB。文本数据的压缩比有利，范围在3到5倍之间，导致未压缩大小约为580
    TB。然而，并非所有用户都需要每个格式。有些人可能只关注WET格式（纯文本），未压缩时仅占约26 TB。
- en: 'The output from Common Crawl varies in both content and quality. When training
    a large language model (LLM), this data undergoes further processing and cleaning
    to enhance its quality. Specialised tools, such as CCNet ([Wenzek et al., 2019](#ref6_79)),
    have been designed to help with this, especially when dealing with outputs like
    those from Common Crawl. For instance, these tools can restructure the files,
    decompress them, and divide them into shards. Each web page entry is then housed
    in a specifically formatted ‘json’ file. Data deduplication occurs at the paragraph
    level, and language detection is performed, enabling the sorting of different
    languages into separate datasets. Some of these steps are complex. Take the language
    detection feature as an example: it uses another language model, fastText ([https://fasttext.cc/](https://fasttext.cc)),
    which is pre-trained on alternative data sources like Wikipedia, Tatoeba, and
    SETimes, to identify the language of the Common Crawl output. [Figure 6.8](#fig6_8)
    showcases the intricacy of the data gathering and post-processing stages.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Common Crawl的输出在内容和质量上都有所不同。当训练大型语言模型（LLM）时，这些数据会进行进一步的处理和清理，以提高其质量。专门设计的工具，如CCNet
    ([Wenzek等人，2019](#ref6_79))，可以帮助处理这些问题，尤其是在处理来自Common Crawl的输出时。例如，这些工具可以重新结构化文件，解压缩它们，并将它们分成碎片。然后，每个网页条目都存放在一个特定格式的‘json’文件中。数据去重发生在段落级别，并执行语言检测，使不同语言可以分别排序到不同的数据集中。其中一些步骤很复杂。以语言检测功能为例：它使用另一个语言模型fastText
    ([https://fasttext.cc/](https://fasttext.cc))，该模型在维基百科、Tatoeba和SETimes等替代数据源上预先训练，以识别Common
    Crawl输出的语言。[图6.8](#fig6_8)展示了数据收集和后处理阶段的复杂性。
- en: '![A flowchart like diagram that shows the processes in the data gathering and
    customisation pipeline.](images/fig6_8_B.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![一个类似于流程图的图表，展示了数据收集和定制管道中的过程。](images/fig6_8_B.jpg)'
- en: '[Figure 6.8 Data Gathering and Customisation Pipeline.](#R_fig6_8)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.8 数据收集和定制管道。](#R_fig6_8)'
- en: It does not end there; further processing by other models is also possible.
    For instance, data quality can be assessed, and, based on the quality score, specific
    data can either be retained or removed from the dataset. One model employed in
    this manner is the 5-gram Kneser-Ney model, which utilises a perplexity measure
    to compare the input from Common Crawl with another source deemed of higher quality,
    such as Wikipedia.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是结束；其他模型也可以进行进一步的处理。例如，可以评估数据质量，并根据质量分数，具体数据可以被保留或从数据集中移除。在这种方式下使用的一个模型是5-gram
    Kneser-Ney模型，它使用困惑度度量来比较来自Common Crawl的输入与另一个被认为质量更高的来源，例如维基百科。
- en: The perplexity is a measure of the predictability of the text within a specific
    paragraph. The higher the perplexity, the more surprised (or perplexed) the model
    becomes, suggesting a lower quality in this data customisation pipeline. If the
    perplexity surpasses a certain threshold for a given paragraph, that paragraph
    may be removed from the dataset with the aim of enhancing the overall quality
    of the dataset. You may recall that Perplexity is also used for GenAI text detection
    in [Section 4.7](ch4.xhtml#sec4_7).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 困惑度是衡量特定段落内文本可预测性的度量。困惑度越高，模型越感到惊讶（或困惑），这表明在这个数据定制管道中的质量越低。如果某个段落的困惑度超过了一个特定的阈值，那么这个段落可能会从数据集中移除，目的是提高数据集的整体质量。你可能还记得，困惑度也用于[第4.7节](ch4.xhtml#sec4_7)中的GenAI文本检测。
- en: Given the vast volume of data involved, the necessity to execute complex tasks
    like these pre-processing models, and the fact that the customisation processes
    operate on specialised data processing environments requiring significant expertise
    to build and manage, the cost of undertaking this task is considerable.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 由于涉及的数据量巨大，执行这些预处理模型所需的复杂任务，以及定制过程在需要大量专业知识来构建和管理的专业数据处理环境中进行的事实，进行这项任务的成本相当高。
- en: 6.3.1.2 Text Data – Colossal Clean Crawled Corpus (C4)
  id: totrans-128
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.1.2 文本数据 – Colossal Clean Crawled Corpus (C4)
- en: The Colossal Clean Crawled Corpus (C4) dataset is derived from dumps provided
    by Common Crawl ([Raffel et al., 2023](#ref6_54)). This dataset undergoes various
    filtering and formatting processes to produce a version compatible with models
    developed using the TensorFlow runtime. TensorFlow is an important widely used
    tool for those building and training models. The filtering process includes tasks
    such as deduplication and the removal of sensitive words.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Colossal Clean Crawled Corpus (C4)数据集来源于Common Crawl提供的存档（[Raffel等人，2023](#ref6_54)）。这个数据集经过各种过滤和格式化过程，以产生与使用TensorFlow运行时开发的模型兼容的版本。TensorFlow是构建和训练模型的重要且广泛使用的工具。过滤过程包括诸如去重和移除敏感词等任务。
- en: Initially, Google created the C4 data set for their own use. While they provided
    a description and some tools to generate the dataset from the Common Crawl inputs,
    they did not release the actual dataset. Recognising the value of this dataset,
    another organisation, the Allen Institute for AI (see [https://allenai.org/](https://allenai.org);
    founded by the late Paul G. Allen, co-founder of Microsoft), reproduced the C4
    dataset. They collaborated with a company called Hugging Face (see [https://huggingface.co/](https://huggingface.co))
    to host and make this dataset accessible to all.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，谷歌创建了C4数据集用于自身使用。虽然他们提供了一份描述和一些工具，可以从Common Crawl输入中生成数据集，但他们并没有发布实际的数据集。认识到这个数据集的价值，另一家组织，艾伦人工智能研究所（参见[https://allenai.org/](https://allenai.org)；由已故的保罗·G·艾伦创立，微软的联合创始人），重新制作了C4数据集。他们与一家名为Hugging
    Face的公司（参见[https://huggingface.co/](https://huggingface.co)）合作，托管并使这个数据集对所有用户可访问。
- en: Recreating the C4 dataset is not only costly (with expenses estimated between
    US$100s and US$1,000s in compute costs) but also requires significant expertise
    and knowledge.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 重新创建C4数据集不仅成本高昂（计算成本估计在数百到数千美元之间），而且还需要大量的专业知识和知识。
- en: 6.3.1.3 Image Data – LAION-5B
  id: totrans-132
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.1.3 图像数据 – LAION-5B
- en: The LAION-5B image dataset (see [https://laion.ai/blog/laion-5b/](https://laion.ai))
    ([Schuhmann et al., 2022](#ref6_67)) is a contemporary dataset designed for state-of-the-art
    (SOTA) model training. Its primary objective is to provide an open and readily
    accessible dataset for text-to-image model training. The dataset comprises 5.85
    billion filtered image-text pairs, of which 2.32 billion are in the English language.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: LAION-5B图像数据集（参见[https://laion.ai/blog/laion-5b/](https://laion.ai)）([Schuhmann等人，2022](#ref6_67)）是一个为最先进（SOTA）模型训练设计的当代数据集。其主要目标是提供一个开放且易于访问的数据集，用于文本到图像模型训练。该数据集包含58.5亿个过滤的图像-文本对，其中23.2亿个是英语。
- en: 'The foundation for this dataset was the Common Crawl, from which image URLs
    were extracted and subsequently filtered using an existing OpenAI CLIP (Contrastive
    Language-Image Pre-Training) model ([Radford et al., 2021](#ref6_50)). This process
    resulted in a dataset comprising: a) 2.32 billion English image-text examples,
    b) 2.26 billion multilingual examples, and c) 1.27 billion examples not tied to
    any specific language (e.g., pertaining to places, products, etc.). Notably, this
    dataset is considerably more extensive than other comparable datasets. By making
    it publicly accessible, it promotes greater transparency, especially concerning
    the ethical considerations linked to large datasets of publicly sourced images.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集的基础是Common Crawl，从中提取了图像URL，随后使用现有的OpenAI CLIP（对比语言-图像预训练）模型进行了过滤（[Radford等人，2021](#ref6_50)）。这个过程产生了一个包含以下内容的数据库：a)
    23.2亿个英语图像-文本示例，b) 22.6亿个多语言示例，以及c) 12.7亿个与任何特定语言无关的示例（例如，与地点、产品等相关）。值得注意的是，这个数据集比其他类似的数据集要广泛得多。通过使其公开可访问，它促进了更大的透明度，特别是在与公开来源的大图像数据集相关的伦理考量方面。
- en: 6.3.1.4 Image Data – LabelMe
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.1.4 图像数据 – LabelMe
- en: The primary objective of the LabelMe dataset (see [http://labelme.csail.mit.edu/](http://labelme.csail.mit.edu))
    ([Russell et al., 2007](#ref6_64); [Oliva, 2001](#ref6_47)) is to offer an online
    annotation tool to construct image datasets for computer vision research. Users
    could register for an account, contribute annotations, and subsequently access
    the datasets for their research. This approach facilitated the accumulation of
    a substantial collection of images and annotations. Importantly, these annotations
    were not mere labels; users could pinpoint and specify individual objects within
    an image. Thus, elements such as a car in the background, a person in the foreground,
    buildings, roads, and any other visible component in the image could be distinctly
    annotated.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: LabelMe 数据集（见 [http://labelme.csail.mit.edu/](http://labelme.csail.mit.edu)）([Russell
    等人，2007](#ref6_64)；[Oliva，2001](#ref6_47)）的主要目标是提供一个在线标注工具，用于构建计算机视觉研究的图像数据集。用户可以注册账户，贡献标注，然后访问数据集进行他们的研究。这种方法促进了大量图像和标注的积累。重要的是，这些标注不仅仅是标签；用户可以在图像中定位并指定单个对象。因此，背景中的汽车、前景中的人、建筑物、道路以及图像中任何其他可见的组件都可以被明确标注。
- en: 6.3.1.5 Other Sources
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.1.5 其他来源
- en: Other frequently utilised sources include data dumps from GitHub, which contain
    open-source software implementations in numerous programming languages; dumps
    from Wikipedia, an online encyclopaedia; releases from Project Gutenberg, which
    hosts over 70,000 books no longer under copyright; and content from ‘The Pile’,
    an amalgamation of 22 smaller datasets that has recently stirred controversy due
    to issues related to copyrighted books.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 其他常用来源包括来自 GitHub 的数据存档，其中包含多种编程语言的开源软件实现；来自维基百科的存档，这是一个在线百科全书；来自 Project Gutenberg
    的发布，该网站托管超过 70,000 本不再受版权保护的书籍；以及来自“The Pile”的内容，这是一个由 22 个较小的数据集组成的混合体，最近因与版权书籍相关的问题而引起争议。
- en: Several other notable data sources exist. For instance, Tesla possesses an extensive
    database of driving-related videos, which it harnesses to train its autopilot
    system. ImageNet ([https://www.image-net.org/](https://www.image-net.org)) offers
    a vast collection of over one million images, each labelled with nouns corresponding
    to the content of the image; this resource has been pivotal in advancing computer
    vision and deep learning research. Another significant source is COCO (Common
    Objects in Context), which provides image data for training. CelebA serves as
    a repository of celebrity images, particularly useful for facial recognition tasks.
    Additionally, there are audio-focused datasets such as the MIDI dataset for music;
    the VCTK dataset for voice-related tasks; and the UCF101 dataset, designed for
    action recognition in videos.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 存在着几个其他值得注意的数据源。例如，特斯拉拥有一个广泛的与驾驶相关的视频数据库，它利用这个数据库来训练其自动驾驶系统。ImageNet ([https://www.image-net.org/](https://www.image-net.org))
    提供了超过一百万张图片的庞大集合，每张图片都标有与图片内容相对应的名词；这个资源在推进计算机视觉和深度学习研究方面发挥了关键作用。另一个重要的来源是 COCO（上下文中的常见物体），它提供了用于训练的图像数据。CelebA
    作为名人图像的存储库，特别适用于面部识别任务。此外，还有专注于音频的数据集，如用于音乐的 MIDI 数据集；用于语音相关任务的 VCTK 数据集；以及用于视频动作识别的
    UCF101 数据集。
- en: Google offers a valuable tool for searching various types of datasets, accessible
    at [https://datasetsearch.research.google.com/](https://datasetsearch.research.google.com).
    Regardless of the specific dataset one requires, there are likely similar sets
    available, and this search tool aims to simplify the discovery process.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Google 提供了一个搜索各种类型数据集的有价值工具，可通过 [https://datasetsearch.research.google.com/](https://datasetsearch.research.google.com)
    访问。无论需要哪种特定数据集，都可能存在类似的集合，这个搜索工具旨在简化发现过程。
- en: 6.3.1.6 Other Data Customisations
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.1.6 其他数据定制
- en: Each of these datasets has its own approach to sourcing from the original, preparing
    the data dump, and then further processing to customise the dataset for the specific
    training task. These preparation and customisation steps may involve similar filtering
    as described above, but they can also encompass distinct types of filtering and
    content curation to achieve the specific aims.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 每个这些数据集都有其从原始数据源获取、准备数据存档以及进一步处理以定制数据集以适应特定训练任务的方法。这些准备和定制步骤可能涉及与上述描述相似的过滤，但也可以包括不同类型的过滤和内容编辑以达到特定的目标。
- en: A significant aspect of this customisation pertains to structuring the data
    in a way that is optimally suited for the specific model in question and the intended
    training strategy. The design and structure of the model, as well as the various
    training approaches, are elaborated upon in subsequent sections. Nevertheless,
    these factors play a crucial role in determining the necessary data preparation
    and customisation processes.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这种定制化的一个重要方面是按照最适合特定模型和预期训练策略的方式组织数据。模型的设计和结构，以及各种训练方法，将在后续章节中详细阐述。尽管如此，这些因素在确定必要的数据准备和定制化过程中起着至关重要的作用。
- en: One common customisation involves ensuring that the size of the data inputs
    aligns with the model design. This will differ depending on the type of data the
    model handles, be it text-based input or image-based input. For language models,
    the typical input size comprises paragraphs that can be fed into the system as
    a single unit, usually amounting to about 512 tokens. Tokens are similar to words
    and we will discuss them more later. These inputs are either split or padded to
    ensure they fit the model. Subsequently, these inputs are often grouped into batches,
    possibly in sizes of 16, 32, or 64\. Such batches are generally processed independently
    during training, which facilitates parallel streams into the model. The collective
    set of all batches constitutes an epoch, and the model may undergo training across
    several iterations of these epochs to maximise the training benefits. Importantly,
    between each epoch iteration, the batches are typically shuffled to prevent the
    model from making unwarranted learning assumptions about the sequence.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的定制化涉及确保数据输入的大小与模型设计相匹配。这取决于模型处理的数据类型，无论是基于文本的输入还是基于图像的输入。对于语言模型，典型的输入大小包括可以作为一个单一单元输入系统的段落，通常大约有512个标记。标记类似于单词，我们将在稍后进行更详细的讨论。这些输入要么被分割，要么被填充以确保它们适合模型。随后，这些输入通常被分组到批次中，可能的大小为16、32或64。这些批次在训练期间通常独立处理，这有助于模型并行流的输入。所有批次的集合构成一个epoch，模型可能需要经过这些epochs的多次迭代以最大化训练效益。重要的是，在每个epoch迭代之间，批次通常会被随机打乱，以防止模型对序列做出未经证实的假设学习。
- en: One final consideration involves dividing the dataset into training and validation
    subsets. This step is crucial to ensure that the models are accurate and do not
    suffer from overtraining (or overfitting). Overfitting is where the model learns
    the training data very well but does poorly when tested with a dataset outside
    the training set. In short it does not generalise well. Typically, the dataset
    is divided into training and testing portions on a 70% to 30% basis, with the
    testing subset selected randomly to circumvent issues associated with alternative
    selection methods. The testing portion of a dataset is used to evaluate how well
    a machine learning model generalises to new, unseen data. While the training data
    are used to train the model, the testing data serves as an independent set of
    examples that the model has never seen during training. Many datasets come pre-divided
    into training, testing, and validation subsets. The validation subset (sometimes
    referred to as the development set) is generally included to facilitate the fine-tuning
    of specific models.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个考虑因素是将数据集划分为训练集和验证集。这一步骤对于确保模型准确无误且不会过度训练（或过度拟合）至关重要。过度拟合是指模型对训练数据学得很好，但在使用训练集以外的数据集进行测试时表现不佳。简而言之，它没有很好地推广。通常，数据集按照70%到30%的比例划分为训练集和测试集，测试集随机选择以避免与替代选择方法相关的问题。数据集的测试部分用于评估机器学习模型对新、未见数据的推广能力。虽然训练数据用于训练模型，但测试数据作为模型在训练期间从未见过的独立示例集。
- en: 6.3.2 Model Design and Structuring
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.2 模型设计和结构
- en: Clearly, the design and structuring of models is an intricate topic, necessitating
    deep experience and understanding of AI, deep neural networks, and the associated
    statistical and mathematical concepts.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，模型的设计和结构是一个复杂的话题，需要深厚的AI、深度神经网络以及相关的统计和数学概念的经验和理解。
- en: Subsequent sections will explore key considerations surrounding model design
    and its nuanced details. As mentioned, the aim is to provide insights into the
    factors that influence a model and their inner workings. This will enable readers
    to delve deeper into specific topics or explore the statistical and mathematical
    concepts that underpin these design considerations which we do not explore in
    this book.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 后续章节将探讨模型设计及其细微细节的关键考虑因素。正如所述，目标是提供影响模型的因素及其内部运作的见解。这将使读者能够深入了解特定主题或探索支撑这些设计考虑因素背后的统计和数学概念，这些内容我们在这本书中并未探讨。
- en: 6.3.2.1 Artificial Neurons – Weights, Bias, and Activations Functions
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.2.1 人工神经元 – 权重、偏置和激活函数
- en: Artificial neurons are simple structures with inputs and outputs. While there
    can be any number of inputs, the output is a singular value. Although the inputs
    can vary, they converge to produce this single output value. Neurons are often
    depicted as a basic circle, with the relevant inputs and outputs illustrated,
    as shown in [Figure 6.9](#fig6_9). This graphical representation is of a rudimentary
    type of neuron known as a feed-forward cell. To gain a deeper understanding of
    the neuron, one can delve into the internals of the cell, as shown in [Figure
    6.9](#fig6_9).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经元是具有输入和输出的简单结构。虽然可以有任意数量的输入，但输出是一个单一值。尽管输入可以变化，但它们会汇聚以产生这个单一的输出值。神经元通常被描绘为一个基本的圆圈，其中相关的输入和输出被展示出来，如图[图6.9](#fig6_9)所示。这种图形表示是一种基本类型的前馈细胞。为了更深入地理解神经元，可以深入了解细胞内部，如图[图6.9](#fig6_9)所示。
- en: '![A diagram that provides exhaustive information regarding a feed forward cell.](images/fig6_9_B.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![提供关于前馈细胞详尽信息的图表](images/fig6_9_B.jpg)'
- en: '[Figure 6.9 Feed-Forward Cell.](#R_fig6_9)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.9 前馈细胞](#R_fig6_9)'
- en: 'This introduces a few important concepts:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这引入了一些重要的概念：
- en: '**Weights**. Each weight is just a number; there is one weight (number) for
    each input into the neuron. The value of this number changes as the model is trained.
    Once trained it is fixed.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**权重**。每个权重只是一个数字；每个输入到神经元中的都有一个权重（数字）。这个数字的值在模型训练过程中会变化。一旦训练完成，它就是固定的。'
- en: '**Bias**. This is just a number and there is one for each neuron. The value
    of this number changes as the model is trained. Once trained it is fixed.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏置**。这只是一个数字，每个神经元都有一个。这个数字的值在模型训练过程中会变化。一旦训练完成，它就是固定的。'
- en: '**Activation function**. This is a special mathematical function that helps
    improve the usefulness of the output. There are many different types of activation
    functions. Some popular ones are sigmoid (used in older models such as Long Short-Term
    Memory Networks), Tanh (used in older models such as hidden layers of Sequence-to-Sequence
    models), ReLu (used in GPT3), and SwiGLU (used in LLaMA).'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**激活函数**。这是一种特殊的数学函数，有助于提高输出的有用性。有许多不同类型的激活函数。一些流行的类型包括sigmoid（用于旧模型，如长短期记忆网络），Tanh（用于旧模型，如序列到序列模型的隐藏层），ReLu（用于GPT3），和SwiGLU（用于LLaMA）。'
- en: In this instance, the input signal (denoted as x[i], which is a numerical value)
    is multiplied by the corresponding weight (w[i]) for that input. This multiplication
    is conducted for all inputs, and the resultant products are then summed (effectively
    yielding the sum of the products). Subsequently, the bias (b) is added, and the
    resultant figure is passed to the activation function. This activation function
    processes the given value, producing an output which is also a numerical value.
    If this neuron is connected to multiple other neurons, the same output value emanating
    from this neuron is transmitted to all interconnected neurons. Hence, the neuron
    can be conceptualised as a mathematical operation on a collection of input values
    to produce an output value, and that mathematical operation is informed by another
    set of values known as weights and the bias.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，输入信号（表示为x[i]，它是一个数值）被乘以该输入对应的权重（w[i]）。对所有输入进行这种乘法运算，然后将结果相加（实际上得到的是乘积的总和）。随后，加上偏置（b），然后将得到的数值传递给激活函数。这个激活函数处理给定的值，产生一个也是数值的输出。如果这个神经元连接到多个其他神经元，那么从这个神经元发出的相同输出值将被传输到所有相互连接的神经元。因此，可以将神经元概念化为对一组输入值进行数学运算以产生输出值的操作，而这个数学运算是由另一组称为权重和偏置的值所指导的。
- en: These weights and biases will be revisited in the context of model training.
    Collectively, they are referred to as the parameters of the model. This is the
    mechanism by which the model ‘remembers’ or stores information; these numerical
    values encapsulate the patterns that the model has discerned during its training
    phase and can subsequently be employed to make predictions. Often, models are
    benchmarked based on the number of parameters they possess, stemming from the
    assumption that a higher number of parameters equates to greater capabilities.
    However, several other factors influence the efficacy of a model beyond just the
    count of parameters. These include the quality of the training data utilised,
    the number of layers and their interconnections, as well as the internal functions
    deployed, such as the activation function or the sum of the products.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这些权重和偏差将在模型训练的上下文中被重新审视。总的来说，它们被称为模型的参数。这是模型“记住”或存储信息的方式；这些数值包含了模型在训练阶段识别出的模式，并且可以随后用于做出预测。通常，模型会根据其拥有的参数数量进行基准测试，这源于假设参数数量越多，能力越强。然而，除了参数数量之外，还有其他几个因素会影响模型的有效性。这些包括所使用训练数据的质量、层数及其互连，以及部署的内部函数，如激活函数或乘积之和。
- en: 'LLaMA-2 (Large Language Model Meta AI) from Meta AI is available in several
    size variants: 7B, 13B, and 70B, where ‘B’ stands for billion parameters. OpenAI’s
    ChatGPT-3.5 boasts 175B parameters. While the size of ChatGPT-4.0 has not been
    officially disclosed, it is speculated to comprise about 1.8T parameters, with
    ‘T’ representing trillion. Google’s LaMDA (Language Models for Dialog Applications)
    possesses up to 137B parameters. In contrast, Google’s PaLM-2 (Pathways Language
    Model) is reported to have around 340B parameters. Initially, Google’s BARD chatbot
    was based on LaMDA, but it was subsequently transitioned to PaLM-2 due to the
    latter’s superior reasoning capabilities. These sizes give some indication of
    the number of cells involved in a SOTA model.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Meta AI的LLaMA-2（大型语言模型Meta AI）有几种尺寸变体：7B、13B和70B，其中‘B’代表十亿参数。OpenAI的ChatGPT-3.5拥有175B参数。虽然ChatGPT-4.0的尺寸尚未官方公布，但据推测其包含大约1.8T参数，其中‘T’代表万亿。Google的LaMDA（用于对话应用的语言模型）拥有高达137B参数。相比之下，Google的PaLM-2（路径语言模型）据报道有大约340B参数。最初，Google的BARD聊天机器人基于LaMDA，但由于后者在推理能力上的优越性，后来转向了PaLM-2。这些尺寸提供了一些关于SOTA模型中涉及细胞数量的指示。
- en: '[Figure 6.10](#fig6_10) illustrates a different type of artificial neuron known
    as a Recurrent Cell.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.10](#fig6_10) 展示了一种称为循环单元的不同类型的人工神经元。'
- en: '![A diagram that provides exhaustive and detailed information regarding a recurrent
    cell.](images/fig6_10_B.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![提供关于循环单元详尽和详细信息的一个图表。](images/fig6_10_B.jpg)'
- en: '[Figure 6.10 Recurrent Cell.](#R_fig6_10)'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.10 循环单元。](#R_fig6_10)'
- en: As [Figure 6.10](#fig6_10) illustrates, this structure closely resembles the
    feed-forward cell, with one notable exception, it incorporates internal feedback.
    In this arrangement, the output from the previous iteration (t-1) is reintroduced
    as an additional input in the current (t) input flow. This input (h[t-1]) is multiplied
    by its designated weight (w[h]) to contribute to the current output. In essence,
    this type of cell possesses memory of the previous output, which aids in computing
    the current output. These recurrent cells are a fundamental component of the recurrent
    neural network (RNN) models mentioned earlier.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图6.10](#fig6_10) 所示，这种结构与前馈单元非常相似，但有一个显著的区别，即它包含内部反馈。在这种安排中，前一个迭代（t-1）的输出被重新引入作为当前（t）输入流的一个额外输入。这个输入（h[t-1]）乘以其指定的权重（w[h]）以贡献当前的输出。本质上，这种类型的单元具有对先前输出的记忆，这有助于计算当前的输出。这些循环单元是之前提到的循环神经网络（RNN）模型的基本组成部分。
- en: There are many different possible types of cells, another one previously mentioned
    is the long short-term memory (LSTM) cell, but these two specific examples should
    give an idea of what is involved.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 存在许多不同类型的细胞，之前提到的一种是长短期记忆（LSTM）单元，但这两个具体的例子应该能给出涉及内容的概览。
- en: 6.3.3 Model Layers and Connections
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.3 模型层和连接
- en: These artificial neurons, also known as cells or nodes, can be connected together
    in layers. Neural networks in AI are composed of nodes organised in layers, with
    each node connected to others in the next layer. These connections are often referred
    to as synapses. [Figure 6.11](#fig6_11) illustrates some common structures of
    deep neural networks, typically comprising an input layer, an output layer, and
    multiple hidden layers in between.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这些人工神经元，也称为细胞或节点，可以按层连接在一起。人工智能中的神经网络由按层组织的节点组成，每个节点都与下一层的其他节点相连。这些连接通常被称为突触。[图6.11](#fig6_11)
    说明了深度神经网络的一些常见结构，通常包括一个输入层、一个输出层和多个中间隐藏层。
- en: '![A deep neural network with an input layer, three hidden layers, and an output
    layer.](images/fig6_11_B.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![具有输入层、三个隐藏层和输出层的深度神经网络](images/fig6_11_B.jpg)'
- en: '[Figure 6.11 Deep Neural Network.](#R_fig6_11)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.11 深度神经网络](#R_fig6_11)'
- en: The number of nodes in each layer can vary, and the connections don’t necessarily
    have to link to every node in the next layer, although this is a typical arrangement.
    For a network to be considered ‘deep’, it must contain at least three hidden layers.
    When counting the layers, both the hidden layers and the output layer are included.
    In the example provided, the deep neural network consists of four layers.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 每层的节点数量可以不同，连接也不一定需要链接到下一层的每个节点，尽管这是一种典型安排。要使网络被认为是“深度”的，它必须包含至少三个隐藏层。在计算层数时，包括隐藏层和输出层。在提供的示例中，深度神经网络由四层组成。
- en: There is flexibility in the number of inputs and nodes in different layers.
    The following diagrams ([Figure 6.12](#fig6_12)) present various approaches.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同层的输入和节点数量上存在灵活性。以下图表([图6.12](#fig6_12))展示了各种方法。
- en: '![Three different deep neural networks which shows the variations that are
    possible in deep neural networks.](images/fig6_12_B.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![展示深度神经网络可能存在变化的三个不同深度神经网络](images/fig6_12_B.jpg)'
- en: '[Figure 6.12 Deep Neural Network Variations.](#R_fig6_12)'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.12 深度神经网络变化](#R_fig6_12)'
- en: '[Figure 6.12](#fig6_12) provides a high-level overview of what a deep neural
    network looks like in an abstract manner.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.12](#fig6_12) 以抽象的方式概述了深度神经网络的外观。'
- en: A significant aspect of designing and structuring a model involves choosing
    the type of cell to use; determining which cell types are suitable for specific
    layers; deciding the number of layers to include; and defining the number of nodes
    in each of the input, hidden, and output layers.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 设计和构建模型的一个重要方面是选择要使用的细胞类型；确定哪些细胞类型适合特定的层；决定要包含的层数；以及定义输入、隐藏和输出层中每个层的节点数量。
- en: Certain model capabilities are tailored to the type of data being processed,
    such as text or images. These features will be discussed primarily in the context
    of the most common use cases, such as large language models (LLMs), text-to-image
    models, or multi-modal models (supporting input and output for multiple data types,
    including text, images, audio, video, etc.). For instance, a model might facilitate
    text input and generate text output, or it could generate image output based on
    user requests. Conversely, it could also accept image input and produce descriptive
    text as output.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 某些模型能力针对的是处理的数据类型，例如文本或图像。这些特性将主要在最常见的用例（如大型语言模型（LLMs）、文本到图像模型或多模态模型（支持包括文本、图像、音频、视频等多种数据类型的输入和输出））的背景下进行讨论。例如，一个模型可能促进文本输入并生成文本输出，或者根据用户请求生成图像输出。相反，它也可以接受图像输入并产生描述性文本作为输出。
- en: 6.3.4 Key Model Capabilities
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.4 关键模型能力
- en: The following sections examine key model capabilities designed for specific
    data types, whether text or images. This section will help you understand what
    actually happens to the input that is passed to the model and what the model does
    with that input both during training and during inference (normal user interaction
    post training).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分将探讨针对特定数据类型（无论是文本还是图像）设计的模型的关键能力。本节将帮助您了解输入到模型中实际发生了什么，以及模型在训练和推理（训练后的正常用户交互）期间如何处理该输入。
- en: 6.3.4.1 Text – Tokenisation
  id: totrans-178
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.4.1 文本 – 分词
- en: Before delving into specifics, it is essential to understand the nature of the
    inputs. In the context of an LLM, both the inputs and outputs consist of text.
    Let’s begin by examining this scenario.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入具体细节之前，理解输入的本质是至关重要的。在LLM的背景下，输入和输出都由文本组成。让我们首先考察这个场景。
- en: There are several upstream processing steps required to transform the text,
    whether it is what you type or the training data, into the actual model input.
    The initial step in this process is known as tokenisation. Tokenisation involves
    breaking the text into tokens. For example, when using the GPT-3 tokeniser, the
    sentence ‘Can you provide a summary of the novel the count of monte cristo’ would
    be tokenised into the following tokens ([Figure 6.13](#fig6_13))
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 将文本（无论是您输入的文本还是训练数据）转换为实际模型输入需要几个上游处理步骤。这个过程的第一步被称为分词。分词涉及将文本分解成标记。例如，当使用GPT-3分词器时，句子“Can
    you provide a summary of the novel the count of monte cristo”会被分词成以下标记（[图6.13](#fig6_13)）
- en: '![An example that shows how the following sentence is tokenised. Can you provide
    a summary of the novel the count of monte cristo.](images/fig6_13_B.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![以下句子的分词示例。你能提供《基督山伯爵》的摘要吗？](images/fig6_13_B.jpg)'
- en: '[Figure 6.13 Tokenisation Example 1.](#R_fig6_13)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.13 分词示例1.](#R_fig6_13)'
- en: 'As you can see in [Figure 6.13](#fig6_13), there are a few things to note:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在[图6.13](#fig6_13)中可以看到，有几个需要注意的点：
- en: The spaces are included in the words (at the start of the words).
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 空格包含在单词中（位于单词的开头）。
- en: Words are mostly separated into different tokens.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单词主要被分割成不同的标记。
- en: Sometimes words can be split (e.g. ‘mon’ and ‘te’) into multiple tokens.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有时单词可以被分割成多个标记（例如，‘mon’和‘te’）。
- en: If you slightly change the string to ‘Can you provide a summary of the novel,
    ‘The Count of Monte Cristo,’ then the tokens for this tokeniser will be ([Figure
    6.14](#fig6_14)).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您将字符串稍微改为“Can you provide a summary of the novel, ‘The Count of Monte Cristo,’”那么这个分词器的标记将是（[图6.14](#fig6_14)）。
- en: '![Another example of how the following sentence is tokenised. Can you provide
    a summary of the novel, ‘The Count of Monte Cristo.’](images/fig6_14_B.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![以下句子的另一个分词示例。你能提供《基督山伯爵》的摘要吗？](images/fig6_14_B.jpg)'
- en: '[Figure 6.14 Tokenisation Example 2.](#R_fig6_14)'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.14 分词示例2.](#R_fig6_14)'
- en: As you can see in [Figure 6.14](#fig6_14), the changes in punctuation and capitalisation
    have impacted the tokenisation process.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在[图6.14](#fig6_14)中可以看到，标点符号和大小写的变化已经影响了分词过程。
- en: There is a limit to the size of the input that can be provided to the model,
    known as the context window. This limit restricts the number of tokens that can
    be passed to the model simultaneously. For example, in the LLaMA-2 model from
    Meta, the context window is 4096 tokens; while in ChatGPT-3.0, it is 2048 tokens.
    However, in ChatGPT-4, the context window is 32k tokens.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 可以提供给模型的输入大小有限，称为上下文窗口。这个限制限制了可以同时传递给模型的标记数量。例如，在Meta的LLaMA-2模型中，上下文窗口是4096个标记；而在ChatGPT-3.0中，它是2048个标记。然而，在ChatGPT-4中，上下文窗口是32k个标记。
- en: The tokenisation approach (which includes words, part words, letters, punctuation
    handling, etc.) is an important part of the model design and may require different
    strategies for different languages.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 分词方法（包括单词、部分单词、字母、标点符号处理等）是模型设计的重要组成部分，可能需要针对不同语言采取不同的策略。
- en: The context window size is important when you consider the approaches that the
    model uses for in-context learning (i.e., learning based on the inputs provided).
    There are a few approaches which can be used.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 当您考虑模型使用的上下文学习方法（即基于输入的学习）时，上下文窗口大小很重要。有几个方法可以使用。
- en: 'Zero-Shot learning: In this approach, only the prompt you type is provided
    as an input to the model.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 零样本学习：在这种方法中，只有您输入的提示被作为输入提供给模型。
- en: 'Few-Shot learning: In this approach, both multiple examples and the specific
    prompt you provide are sent to the model as inputs. The number of examples can
    range from 1 up to 32 examples.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 少样本学习：在这种方法中，多个示例和您提供的特定提示都作为输入发送给模型。示例的数量可以从1个到32个不等。
- en: In the case of the few-shot learning, the examples can use up many tokens, leaving
    fewer tokens for the actual prompt being performed, thus in cases where few-shot
    learning is viewed as important, a larger context window is needed. Likewise,
    if the inputs themselves are large e.g. full text documents, larger context windows
    are needed.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在少样本学习的情况下，示例可能会使用掉许多标记，从而为实际执行的提示留下较少的标记，因此，在少样本学习被视为重要的情况下，需要更大的上下文窗口。同样，如果输入本身很大，例如全文文档，也需要更大的上下文窗口。
- en: 6.3.4.2 Text – Encoding
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.4.2 文本 – 编码
- en: One purpose of tokenisation is to map the inputs to a known set of possible
    options. In the case of the English language and the tokenisation approach shown
    above, there are about 50,000 possible tokens based on the GPT-3 tokenisation
    design.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 分词的一个目的就是将输入映射到一个已知的可能选项集合。在英语语言和上述分词方法的情况下，基于GPT-3的分词设计，大约有50,000个可能的令牌。
- en: Picture these 50,000 possible tokenised values as the vertical columns in a
    matrix, and picture each of the actual input tokens from the prompt as the horizontal
    rows in that matrix. This would allow the inputs to be mapped to numbers (ones
    or zeros) like an encoding mechanism.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 将这50,000个可能的分词值想象成矩阵的垂直列，将实际输入令牌想象成该矩阵的水平行。这将允许将输入映射到数字（一或零），就像编码机制一样。
- en: The matrix would contain a ‘1’ in the column corresponding to the position of
    each token in the input, and the ‘1’ would be in the appropriate row based on
    its position in the input text sequence. All other columns in that row would be
    ‘0’. This is demonstrated in [Figure 6.15](#fig6_15). If the number of input tokens
    is less than the context window, the remaining columns would be filled with ‘0’.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵将包含一个‘1’在对应于输入中每个令牌位置的列中，并且根据其在输入文本序列中的位置，这个‘1’将位于适当的行中。该行中的所有其他列都将为‘0’。这已在[图6.15](#fig6_15)中展示。如果输入令牌的数量少于上下文窗口，剩余的列将填充为‘0’。
- en: '![A sentence that has been tokenised and the related encoding matrix for the
    same.](images/fig6_15_B.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![一个已经被分词的句子及其相关的编码矩阵](images/fig6_15_B.jpg)'
- en: '[Figure 6.15 Encoding Matrix.](#R_fig6_15)'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.15 编码矩阵](#R_fig6_15)'
- en: This encoding approach allows the prompt to be passed as an input, which is
    a matrix of numbers, to the model. Since the model can only process numbers, this
    encoding is a critical part of the pre-processing required to use the model.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这种编码方法允许将提示作为数字矩阵输入传递给模型。由于模型只能处理数字，这种编码是使用模型所需的预处理的关键部分。
- en: The order of the tokens is also significant. In addition to the token values
    in the matrix, the position of each token in the sequence is important and can
    be identified by the model from the input provided.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 令牌的顺序也很重要。除了矩阵中的令牌值之外，每个令牌在序列中的位置也很重要，并且可以通过模型从提供的输入中识别出来。
- en: This approach of mapping categorical variables (tokens or words) to a simple
    matrix is one type of encoding called ‘one-hot encoding’. However, there are more
    sophisticated approaches available. This encoding represents a fixed, rule-based
    process. These decisions are integral to model design and structure.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这种将分类变量（令牌或单词）映射到简单矩阵的方法是一种称为“独热编码”的编码类型。然而，还有更复杂的方法可用。这种编码代表了一个固定、基于规则的过程。这些决策对于模型设计和结构至关重要。
- en: 6.3.4.3 Text – Embedding
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.4.3 文本 – 嵌入
- en: Once the tokens are presented in a matrix format in the transformer architecture
    for LLMs, the next step is embedding. This is where the matrix input is mapped
    to a vector representation for each token and each token position.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦令牌以矩阵格式呈现于LLM的Transformer架构中，下一步就是嵌入。这就是矩阵输入被映射到每个令牌和每个令牌位置的向量表示。
- en: The mapping of the token’s entry in the matrix to a vector is a learned representation.
    In other words, through training, the model learns how to position the vectors
    in this space so that the geometric relationships between them reflect the semantic
    relationships between the corresponding tokens. For example, it could happen that
    types of animals would be ‘near’ each other (have similar values for their vectors),
    or that words with similar meanings appear near each other.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 将矩阵中令牌条目映射到向量的过程是一个学习到的表示。换句话说，通过训练，模型学习如何在这个空间中定位向量，以便它们之间的几何关系反映了对应令牌之间的语义关系。例如，动物类型可能会“靠近”（它们的向量具有相似值），或者具有相似意义的单词可能会出现在彼此附近。
- en: Put in other words, embeddings in LLMs are numerical representations that capture
    the essence of words or phrases. Rather than treating words as isolated units,
    embeddings transform them into vectors in a high-dimensional space, where similar
    words are positioned closer together. This allows models to understand context
    and meaning, making text generation more coherent and contextually relevant. For
    example, think of embeddings as giving words a unique address in a city (the high-dimensional
    space). So, while the word “cat” might live at one address, the word “kitten”,
    being similar in meaning, would live nearby (in other words would have a similar
    numerical value). When the model needs to generate text, it uses these addresses
    (numerically similar values) to find words that fit best in the context, ensuring
    the sentences make sense.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，LLM 中的嵌入是捕获单词或短语本质的数值表示。而不是将单词视为孤立的单元，嵌入将它们转换成高维空间中的向量，其中相似的单词彼此更接近。这使得模型能够理解上下文和意义，使文本生成更加连贯和上下文相关。例如，将嵌入视为在城市的独特地址（高维空间）中为单词提供地址。因此，虽然“猫”这个词可能住在某个地址，但意义相似的“小猫”就会住在附近（换句话说，会有相似的数值）。当模型需要生成文本时，它使用这些地址（数值相似值）来找到最适合上下文的单词，确保句子有意义。
- en: If you don’t grasp the details of the embedding process, don’t worry. The key
    thing to remember is that this is a learned process; the embedding layer has weights
    and biases just like the other layers, and through the learning process, these
    weight and bias values are updated. This learning process is explained in more
    detail below.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有掌握嵌入过程的细节，不要担心。要记住的关键点是这是一个学习过程；嵌入层有权重和偏差，就像其他层一样，并且通过学习过程，这些权重和偏差值会更新。这个学习过程将在下面更详细地解释。
- en: 6.3.4.4 Text – Attention
  id: totrans-211
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.4.4 文本 – 注意力
- en: The paper ‘Attention is all you need’ ([Vaswani et al., 2017](#ref6_74)) emphasised
    the significance of attention in the design and structure of a model. The attention
    mechanism enables the model to identify long-range (distant) dependencies between
    words in a sentence or paragraph. This is analogous to how humans address this
    issue when listening to someone speak or reading text. We analyse what preceded
    the current context and relate it to what we are currently processing, attempting
    to form a coherent mental picture of the situation. A key component of this attention
    mechanism is the “attention head”.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 论文《Attention is all you need》（[Vaswani 等人，2017](#ref6_74)）强调了注意力在模型设计和结构中的重要性。注意力机制使模型能够识别句子或段落中单词之间的长距离（远程）依赖关系。这与人类在听某人说话或阅读文本时如何处理这个问题类似。我们分析当前上下文之前的内容，并将其与我们当前处理的内容联系起来，试图形成一个连贯的心理图景。这个注意力机制的关键组成部分是“注意力头”。
- en: 'The model is designed to calculate attention scores for each word in the input,
    be it a sentence or paragraph, in order to understand the relative importance
    of each word. Typically, there are multiple attention heads, and each head computes
    its own score. Similar to embeddings, these scores are learned from the weights
    associated with three parameters for each attention head: the so-called ‘query’,
    ‘key’, and ‘value’ parameters, often referred to as matrices W[q], W[k], and W[v].
    Different attention heads process the input in parallel, and the set of attention
    scores is then aggregated to create a new representation of that token. Having
    multiple attention heads allows the model to learn a more comprehensive set of
    relationships among tokens. For example, one head might learn to focus on syntactic
    relationships like subject–verb agreement, while another might learn to capture
    semantic relationships like synonymy.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型旨在计算输入中每个单词的注意力分数，无论是句子还是段落，以便理解每个单词的相对重要性。通常，存在多个注意力头，每个头计算自己的分数。类似于嵌入，这些分数是从与每个注意力头相关的三个参数的权重中学习的，所谓的“查询”、“键”和“值”参数，通常称为矩阵
    W[q]、W[k] 和 W[v]。不同的注意力头并行处理输入，然后将注意力分数集合并集以创建该标记的新表示。拥有多个注意力头允许模型学习到更全面的标记之间的关系集。例如，一个头可能学会关注句法关系，如主谓一致，而另一个头可能学会捕捉语义关系，如同义。
- en: Consider the sentence ‘The cat sat on the mat.’ With a single attention head,
    it might concentrate on the relationship between ‘cat’ and ‘sat’, capturing the
    subject–verb relationship. However, when multiple attention heads are employed,
    another might focus on the relationship between ‘sat’ and ‘mat’, capturing the
    verb–object relationship, while yet another could emphasise the relationship between
    ‘on’ and ‘mat’, capturing the preposition–object relationship.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 以句子‘The cat sat on the mat.’为例。使用单个注意力头时，它可能会关注‘cat’和‘sat’之间的关系，捕捉主谓关系。然而，当使用多个注意力头时，另一个可能会关注‘sat’和‘mat’之间的关系，捕捉动宾关系，而另一个可能会强调‘on’和‘mat’之间的关系，捕捉介词和宾语之间的关系。
- en: As mentioned, these scores and the focus of each attention head are learnt during
    the training process. The designer does not assign a specific head to a specific
    ability; this is entirely dynamic and learnt during training. It may seem somewhat
    amazing, perhaps unbelievable, that this happens, but it does. With millions of
    training examples, the model learns to specialise specific attention heads for
    specific purposes. This is an emergent feature of the model. The stochastic nature
    of the training process encourages diversity among the heads, so that different
    heads take on specific but different purposes.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 正如所述，这些分数和每个注意力头的焦点是在训练过程中学习的。设计者不会将特定的头分配给特定的能力；这是完全动态的，并在训练过程中学习。这似乎有些令人惊讶，也许令人难以置信，但这确实发生了。在数百万个训练示例中，模型学会了为特定目的专门化特定的注意力头。这是模型的一个涌现特征。训练过程的随机性鼓励头之间的多样性，使得不同的头承担具体但不同的目的。
- en: 6.3.4.5 Text – Next Word Prediction Learning Goal
  id: totrans-216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.4.5 文本 – 下一个单词预测 学习目标
- en: We have seen that some layers in the model are focused on finding the relationships
    between words (embedding), and some are focused on understanding what the most
    important words of the input are (attention). The other layers in the model will
    be focused on predicting the next word in the text. If the input so far is ‘I
    went to the’, then the model is trying to predict what the next word is, and let’s
    say it predicts ‘market’, but the actual word is ‘school’, then it will adjust
    the weights and biases in the model to better predict this in the future. It will
    then try to predict the next word again in ‘I went to the school’, and it might
    predict ‘to’, and this is correct. Then it predicts ‘pick’, and this is correct,
    then it predicts ‘up’, and this is correct, and then it predicts ‘my’ which is
    correct again. Then the next word it predicts is ‘kids’ but the actual word is
    ‘books’. It again will update the weights and biases to try and better predict
    in the future. This gives the model the input ‘I went to the school to pick up
    my books’. As it learns it will also be updating the embeddings and attention
    layers to improve those too.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，模型中的某些层专注于寻找单词之间的关系（嵌入），而另一些层则专注于理解输入中最重要单词的内容（注意力）。模型中的其他层将专注于预测文本中的下一个单词。如果到目前为止的输入是‘我去过’，那么模型正在尝试预测下一个单词是什么，比如说它预测了‘market’，但实际上是‘school’，那么它将调整模型中的权重和偏差，以便在将来更好地预测。然后它将再次尝试在‘我去过学校’中预测下一个单词，并且可能会预测‘to’，这是正确的。然后它预测‘pick’，这也是正确的，然后它预测‘up’，这也是正确的，然后它预测‘my’，这又正确。然后它预测的下一个单词是‘kids’，但实际单词是‘books’。它再次更新权重和偏差，试图在将来更好地预测。这给模型提供了输入‘我去过学校去取我的书’。随着它不断学习，它也会更新嵌入和注意力层，以改善这些层。
- en: 6.3.4.6 Image – Pre-processing
  id: totrans-218
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.4.6 图像 – 预处理
- en: This is where the input image is prepared and normalised or standardised for
    input into the model. The input image may be a JPEG or a GIF and needs to be converted
    to the standard format. This conversion includes not only changing the format
    but also adjusting other details such as the size of the image (width, height,
    and aspect ratio), the resolution of the image, and the colour space. Depending
    on the complexity, it may involve image rotation to ensure it is correctly oriented.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是输入图像被准备并归一化或标准化以输入模型的过程。输入图像可能是一个JPEG或GIF，需要转换为标准格式。这种转换不仅包括改变格式，还包括调整其他细节，例如图像的大小（宽度、高度和纵横比）、图像的分辨率和色彩空间。根据复杂程度，可能涉及图像旋转以确保其正确方向。
- en: Part of the pre-processing involves tensor (matrix of vectors) conversion, which
    is similar to the encoding discussed for text models. In the case of an image,
    this process is somewhat easier because the image naturally has a binary representation
    of a set of pixels in height and width, with each pixel having values for colours,
    such as Red (R), Green (G), and Blue (B). Thus, conversion involves creating a
    tensor or matrix of vectors for these pixel coordinates and RGB values. This has
    also been covered in [Section 1.8](ch1.xhtml#sec1_8) [Figure 1.4](ch1.xhtml#fig1_4).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理的一部分涉及张量（向量的矩阵）转换，这与文本模型中讨论的编码类似。在图像的情况下，这个过程相对容易，因为图像自然地具有高度和宽度的像素集合的二进制表示，每个像素都有颜色值，如红色（R）、绿色（G）和蓝色（B）。因此，转换涉及为这些像素坐标和RGB值创建张量或向量矩阵。这也在[第1.8节](ch1.xhtml#sec1_8)
    [图1.4](ch1.xhtml#fig1_4)中有所涉及。
- en: 6.3.4.7 Image – Encoding
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.4.7 图像 – 编码
- en: Image encoding is similar to embedding for text-based models. However, instead
    of using embedding layers, which are employed in text models, image models typically
    use convolutional layers. These layers learn the relationships between the content
    of the image, aiding in the extraction of model features.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图像编码类似于基于文本的模型的嵌入。然而，与在文本模型中使用的嵌入层不同，图像模型通常使用卷积层。这些层学习图像内容之间的关系，有助于提取模型特征。
- en: 6.3.4.8 Image – Diffusion Learning Goal
  id: totrans-223
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.4.8 图像 – 扩散学习目标
- en: The model is presented with an encoded image along with a text description of
    that image. Forward diffusion is the process where a model introduces noise into
    an image, gradually making the picture blurry by altering some of the pixels’
    colours. This noise is added incrementally until the image becomes unrecognisable,
    resembling a television screen when not tuned in. This is shown in [Section 1.8](ch1.xhtml#sec1_8)
    [Figure 1.6](ch1.xhtml#fig1_6).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 模型被呈现一个编码后的图像以及该图像的文本描述。正向扩散是一个模型通过改变一些像素的颜色，逐渐向图像中引入噪声，使图片变得模糊的过程。这种噪声是逐步添加的，直到图像变得无法识别，类似于未调谐的电视屏幕。这可以在[第1.8节](ch1.xhtml#sec1_8)
    [图1.6](ch1.xhtml#fig1_6)中看到。
- en: Reverse diffusion, on the other hand, is the process in which a model removes
    noise from an image, gradually restoring it to its original, recognisable form.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，反向扩散是一个模型从图像中去除噪声的过程，逐渐将其恢复到原始、可识别的形式。
- en: These forward and reverse diffusion processes teach the model how to generate
    an image which it has a text description of by removing noise. When tasked with
    creating an image solely based on a text description, the model can utilise this
    learning to remove noise from an initial image with random noise, resulting in
    a completely new image that did not previously exist.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这些正向和反向扩散过程教会模型如何通过去除噪声来生成一个它有文本描述的图像。当被要求仅基于文本描述创建图像时，模型可以利用这种学习从具有随机噪声的初始图像中去除噪声，从而生成一个之前不存在的全新图像。
- en: 6.3.4.9 Hyperparameters
  id: totrans-227
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.4.9 超参数
- en: Hyperparameters are specific controls of a model that designers must choose
    and adjust during the design and training processes. Unlike model parameters,
    which are automatically adjusted by the model itself during learning, hyperparameters
    are only adjusted by the model’s designer.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数是设计者在设计和训练过程中必须选择和调整的模型特定控制。与模型参数不同，模型参数在学习过程中由模型自身自动调整，而超参数仅由模型的设计者调整。
- en: These hyperparameters span a wide range of aspects of the model, including the
    learning rate, the batch size, the number of epochs, factors impacting the activation
    function used, the dropout rate, and more. During this phase, designers must determine
    the best values for these hyperparameters.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这些超参数涵盖了模型各个方面的一个广泛范围，包括学习率、批量大小、训练轮数、影响所使用激活函数的因素、dropout率等。在这个阶段，设计者必须确定这些超参数的最佳值。
- en: Sometimes, hyperparameters are exposed to the end users of a model to help control
    a model’s behaviour. One such example is the ‘temperature’ hyperparameter, which
    controls how repeatable a prediction is. Lower values mean more randomness, while
    higher values mean less randomness and more consistency.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，超参数会暴露给模型的最终用户，以帮助控制模型的行为。一个这样的例子是“温度”超参数，它控制预测的可重复性。较低的值意味着更多的随机性，而较高的值则意味着更少的随机性和更多的一致性。
- en: 6.3.5 Foundational Model Training
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.5 基础模型训练
- en: Thus far, we have covered the big picture, how the data is gathered and prepared,
    and different aspects that go into the design and structuring of the model. In
    this section, we focus on foundational model training, commonly referred to as
    pre-training.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经概述了整体情况，包括数据的收集和准备，以及模型设计和结构化方面的不同方面。在本节中，我们专注于基础模型训练，通常称为预训练。
- en: A foundational model is a model built and initially trained as a general-purpose
    model, and it is not a specialisation of another model. A fine-tuned model is
    a foundational model (or another fine-tuned model) that is specialised for a particular
    purpose.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型是一个作为通用模型构建和最初训练的模型，它不是另一个模型的专门化。微调模型是一个基础模型（或另一个微调模型），它针对特定目的进行了专门化。
- en: The term ‘pre-trained’ or ‘pre-training’ refers to the training that occurs
    before the model becomes useful as a general-purpose foundational model.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: “预训练”或“预训练”一词指的是在模型成为通用基础模型之前发生的训练。
- en: When training a foundational model (also known as pre-training), there are different
    types of training that can occur, which impact the type of learning that takes
    place. The following sections delve into these different types of learning and
    the associated training processes.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练一个基础模型（也称为预训练）时，可能会发生不同类型的训练，这些训练类型会影响发生的学习的类型。以下章节将深入探讨这些不同类型的学习和相关的训练过程。
- en: 6.3.5.1 Supervised Learning
  id: totrans-236
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.5.1 监督学习
- en: Supervised learning occurs when the training data includes both the data and
    a label that allows the model to determine whether its prediction was correct
    or not. For instance, if the training data pertains to the sentiment of a given
    section of text, it will include a description of that sentiment associated with
    each section of text. When the model is being trained and makes a prediction about
    the text’s sentiment, it can then reference the provided label to assess the accuracy
    of its prediction. It will subsequently adjust the parameters in the model to
    enhance future predictions.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练数据包括数据和允许模型确定其预测是否正确的标签时，就会发生监督学习。例如，如果训练数据涉及文本某个部分的情感，它将包括与每个文本部分相关的情感描述。当模型正在训练并预测文本的情感时，它可以参考提供的标签来评估其预测的准确性。然后，它将调整模型中的参数以增强未来的预测。
- en: Another example is when the model deals with images containing animals. In this
    case, the training data consists of numerous pictures of animals, each accompanied
    by labels describing the type of animal depicted. The model can utilise this training
    data and labels to make predictions and refine its predictions.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是当模型处理包含动物的图像时。在这种情况下，训练数据包括许多动物的图片，每张图片都附有描述所描绘动物类型的标签。模型可以利用这些训练数据和标签来做出预测并优化其预测。
- en: Yet another example involves a model aiming to enhance the resolution of an
    image. In this scenario, the training data comprises a low-resolution image and
    a higher-resolution image of the same picture. The model can predict what the
    higher-resolution image might look like and use the provided higher resolution
    image to adjust its prediction for improvement.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子涉及一个旨在提高图像分辨率的模型。在这种情况下，训练数据包括同一图片的低分辨率图像和更高分辨率的图像。模型可以预测更高分辨率图像可能的样子，并使用提供的更高分辨率图像来调整其预测以进行改进。
- en: 6.3.5.2 Self-Supervised Learning
  id: totrans-240
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.5.2 自监督学习
- en: Self-supervised learning is a method in which the model uses unlabelled input
    to supervise its own learning. For instance, if the input is ‘I love AI’, and
    the model’s objective is to predict the next word, it can use ‘I xxxx’ to predict
    ‘I have’. It can then check if that prediction was correct based on the subsequent
    input, ‘I love’, and adjust its prediction for the future. This is the most common
    approach to learning for LLMs, as data from sources such as Common Crawl can be
    relatively easily used to create a self-supervised training set for models that
    predict the next word, like LLMs.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督学习是一种模型使用未标记输入来监督其自身学习的方法。例如，如果输入是“我爱AI”，而模型的目的是预测下一个单词，它可以使用“我爱xxxx”来预测“我有”。然后，它可以根据后续输入“我爱”，检查该预测是否正确，并调整其未来的预测。这是LLMs学习中最常见的方法，因为来自Common
    Crawl等来源的数据可以相对容易地用于为预测下一个单词的模型（如LLMs）创建自监督训练集。
- en: 6.3.5.3 Unsupervised Learning
  id: totrans-242
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.5.3 无监督学习
- en: Unsupervised learning is a type of training where data is provided to the model
    without any specific knowledge about what is expected, without any ‘labels,’ or
    without using the self-supervised learning approach. This type of learning can
    be useful for specific scenarios, such as clustering, which involves identifying
    groups (clusters) of related items.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习是一种训练类型，其中数据被提供给模型，而没有任何关于预期内容的特定知识，没有任何“标签”，或者不使用自监督学习方法。这种学习在特定场景中可能很有用，例如聚类，它涉及识别相关项目（簇）的组。
- en: 6.3.5.4 Reinforcement Learning with Human Feedback (RLHF)
  id: totrans-244
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.5.4 基于人类反馈的强化学习（RLHF）
- en: Reinforcement Learning with Human Feedback (RLHF) is particularly beneficial
    in domains where the desired behaviour is difficult to specify, or where the model
    needs to learn nuanced or complex strategies. It has been applied in training
    models across various domains, including game playing (such as Go), dialog systems,
    and robotics.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 基于人类反馈的强化学习（RLHF）在难以指定所需行为或模型需要学习细微或复杂策略的领域特别有益。它已被应用于训练跨越各个领域的模型，包括游戏（如围棋）、对话系统和机器人技术。
- en: One of the challenges associated with RLHF is that it can be time-consuming
    and expensive, as it requires ongoing involvement from human evaluators. Additionally,
    it may necessitate careful design to ensure that the feedback collected is informative
    and that biases in human judgments do not unduly influence the model’s behaviour.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 与RLHF相关的一个挑战是，它可能既耗时又昂贵，因为它需要人类评估者的持续参与。此外，它可能需要精心设计，以确保收集到的反馈是有信息的，并且人类判断中的偏见不会过度影响模型的行为。
- en: The paradigm of RLHF serves as a bridge between traditional supervised learning,
    in which a model learns from a fixed dataset of labelled examples, and reinforcement
    learning, where a model learns by interacting with an environment to maximise
    a reward signal and thus strongly favour the human feedback option when learning.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF（基于人类反馈的强化学习）范式充当了传统监督学习（模型从固定数据集的标记示例中学习）和强化学习（模型通过与环境的交互来最大化奖励信号并因此强烈倾向于人类反馈选项）之间的桥梁。
- en: 6.3.5.5 Learning
  id: totrans-248
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.5.5 学习
- en: This section delves into the learning process in more detail. So far, learning
    has been discussed without a detailed explanation. It has been mentioned that
    the model makes predictions, such as next-word predictions in the case of LLMs,
    and then compares the predicted value to the expected value (via self-supervised,
    supervised, or reinforcement learning), and then adjusts the model as needed.
    It was also mentioned that the model’s parameters, which are the weights and biases,
    are the numbers that get adjusted during learning.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将更详细地探讨学习过程。到目前为止，学习已被讨论，但没有详细的解释。已经提到，模型会做出预测，例如在LLM的情况下预测下一个单词，然后比较预测值和预期值（通过自监督、监督或强化学习），然后根据需要调整模型。还提到，模型的参数，即权重和偏差，是学习过程中进行调整的数字。
- en: 'We understand that the model’s understanding of the world results from a combination
    of its structure (including embeddings, attention, and other layers) and the values
    of its parameters (the weights and biases). Initially, the weights are set to
    some value before training starts on the foundational model. Typically, this value
    is random, or sometimes zero. Then, the value is adjusted during training. We
    have already discussed that the training data is grouped into batches. Now, let’s
    explore how the learning process is closely linked to these batches and the significance
    of batch size. For each batch, the following processes occur:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们理解，模型对世界的理解源于其结构（包括嵌入、注意力和其他层）及其参数值（权重和偏差）的结合。最初，在基础模型开始训练之前，权重被设置为某个值。通常，这个值是随机的，有时为零。然后，在训练过程中调整这个值。我们已经讨论过，训练数据被分组到批次中。现在，让我们探讨学习过程如何与这些批次紧密相连，以及批次大小的重要性。对于每个批次，以下过程发生：
- en: The forward pass. This is where the data from the batch is passed ‘forward’
    through the neural network. The model makes predictions one word or image at a
    time for each item in the batch. It keeps track of the prediction that it makes
    and the actual value.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前向传播。这是将批次中的数据通过神经网络进行“前向”传递的地方。模型逐个单词或图像地对批次中的每个项目进行预测。它记录下所做的预测和实际值。
- en: The loss calculation. The loss calculation involves the model comparing its
    predicted value to the actual expected value. A loss function is used for this
    purpose. In LLMs, a common loss function is categorical cross-entropy, whereas
    image-generation models often use loss functions like generative adversarial loss.
    These loss functions quantify the difference between the actual and predicted
    values for each word or image. These quantified values for each input are then
    averaged across the entire batch, resulting in an average loss value available
    for the next step.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 损失计算。损失计算涉及模型将其预测值与实际预期值进行比较。为此使用损失函数。在大型语言模型（LLMs）中，常见的损失函数是分类交叉熵，而图像生成模型通常使用生成对抗损失等损失函数。这些损失函数量化了实际值与预测值之间每个单词或图像的差异。然后，这些输入的量化值在整个批处理中平均，从而得到可用于下一步的平均损失值。
- en: The backward pass, also known as backpropagation ([Werbos, 1974](#ref6_80),
    and [Rumelhart et al., 1985](#ref6_60)). Steps 1 and 2 occur for each item in
    the batch, and the loss calculation step maintains a running average loss across
    the entire batch. Backpropagation is then performed based on this average loss
    value. This involves computing the gradients of the loss with respect to each
    parameter in the model. Even though one sentence might have been predicted perfectly,
    errors in other sentences contribute to the gradients and guide the parameter
    updates. It is important to remember that the model consists of many layers, each
    with numerous cells or neurons, and each of these has multiple weights and a bias
    that may need adjustment. The calculated gradients, based on the average loss,
    assign the level of influence (sometimes called the blame or reward) to each weight
    and bias in the model for the errors between the predicted values and the actual
    values.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反向传播，也称为反向传播算法([Werbos, 1974](#ref6_80)，以及[Rumelhart 等人，1985](#ref6_60))。步骤1和2对批处理中的每个项目都会执行，损失计算步骤在整个批处理中维护一个运行平均损失。然后基于这个平均损失值执行反向传播。这涉及到计算模型中每个参数相对于损失的梯度。即使一句话可能被完美预测，其他句子中的错误也会对梯度产生影响，并指导参数更新。重要的是要记住，模型由许多层组成，每层有众多细胞或神经元，而这些细胞或神经元都有多个需要调整的权重和偏差。基于平均损失的梯度计算，将影响水平（有时称为责任或奖励）分配给模型中每个权重和偏差，以反映预测值与实际值之间的误差。
- en: The adjustment. The adjustment of the weights and biases for each neuron in
    each layer. Now that the model has assigned a level of blame or reward to each
    of the parameters in the model, it can proceed to adjust them at the end of the
    batch.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整。调整每一层中每个神经元的权重和偏差。现在模型已经为模型中的每个参数分配了责任或奖励水平，它可以在批处理结束时调整这些参数。
- en: 'This is the essence of learning in a neural network: make a prediction, estimate
    an error, assign a level of blame or reward for the error to each parameter in
    the model, and adjust each parameter in the model. Over millions or billions of
    tokens, the model starts to have specialised and improved behaviours. The embedding
    layers learn, allowing associated words to be grouped in a manner that reflects
    their relationship; similar words will be close together, disparate words will
    be far apart. The attention layers learn, with each attention head specialising
    in a specific manner that allows more distant relationships between words to be
    formed, such as between subjects and actions. The other layers also contribute
    to the overall relationships between words, language concepts, and real-world
    concepts. From these different learnings and the patterns and relationships formed,
    the model gains what is like a type of ‘understanding’ of the world and how things
    relate. Much of this learning and understanding is emergent from the large set
    of data that the model was trained on and the structure of the model that allows
    different parts to focus on specific aspects. This is one of the reasons why it
    is so difficult for humans to understand how the models really work; there are
    typically billions of parameters, which can be trained on millions or billions,
    or even trillions of tokens, and humans are not good at understanding how such
    vast volumes of data and numbers interact to result in this emergent capability.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是神经网络学习的本质：做出预测，估计误差，为模型中的每个参数分配一个误差的责备或奖励水平，并调整模型中的每个参数。在数百万或数十亿个标记符中，模型开始展现出专业化和改进的行为。嵌入层学习，使得相关词汇能够以反映它们之间关系的方式分组；相似的词汇会靠得很近，不同的词汇则会相隔甚远。注意力层学习，每个注意力头都专门以某种方式形成，使得词汇之间更远的关系得以形成，例如主语和动作之间的关系。其他层也贡献于词汇、语言概念和现实世界概念之间的整体关系。从这些不同的学习和形成的模式和关系中，模型获得了类似于对世界及其事物之间关系的“理解”。这种学习和理解的大部分是从模型训练所使用的大量数据以及允许不同部分专注于特定方面的模型结构中涌现出来的。这也是人类难以理解模型真正工作原理的原因之一；通常有数十亿个参数，这些参数可以在数百万或数十亿，甚至万亿个标记符上进行训练，而人类并不擅长理解如此庞大的数据量和数字是如何相互作用，从而产生这种涌现能力。
- en: It should be remembered that this understanding is purely related to the training
    data that has been provided. It lacks the kind of understanding that humans possess.
    Humans have an understanding of physical laws and draw from their experiences
    to assess a situation. They adjust their responses based on this understanding.
    Humans also have belief-based rules, which they use to adjust their responses
    and actions. However, something like an LLM only has the data it was trained on
    and does not have these other approaches to learning. Humans are adept at learning
    from a relatively small number of examples and gaining rich insights from this.
    AI models need vast quantities of data to start developing useful emergent behaviours.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 应该记住，这种理解纯粹与所提供的训练数据相关。它缺乏人类所拥有的那种理解。人类理解物理定律，并从他们的经验中吸取教训来评估情况。他们根据这种理解调整他们的反应。人类还有基于信念的规则，他们使用这些规则来调整他们的反应和行动。然而，类似LLM这样的东西只有它所训练的数据，并没有这些其他的学习方法。人类擅长从相对较少的例子中学习，并从中获得丰富的见解。AI模型需要大量的数据才能开始发展有用的涌现行为。
- en: While different, the learning process is somewhat like a child’s learning process;
    first receiving a broad education in kindergarten, primary, and secondary school,
    which is like pre-training of a foundational model; and then specialising the
    learning for a particular discipline or profession like Computer Science in university,
    which is like fine-tuning that we discuss in more detail later.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然不同，但学习过程多少有点像孩子的学习过程；首先在幼儿园、小学和中学接受广泛的教育，这就像是一个基础模型的预训练；然后在大学专注于特定学科或专业，如计算机科学，这就像是我们后面更详细讨论的微调。
- en: At the very start of the learning process, the model knows nothing; in other
    words, the values of the weights and biases have not been set to appropriate values.
    Typically, at this point, the model will set the weights and biases to random
    values, and then the training processes will adjust as described. This is somewhat
    like a child who, when born, we assume they have no knowledge of the world but
    need to learn as they grow.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习过程的最初阶段，模型一无所知；换句话说，权重和偏差的值尚未设置为适当的值。通常，在这个阶段，模型会将权重和偏差设置为随机值，然后训练过程将按照描述进行调整。这有点像出生时我们假设孩子对世界一无所知，但随着他们成长需要学习。
- en: The concept of deep neural networks was inspired by the human brain and the
    brains of animals. We can see there are some similar concepts, but we can also
    see they are very different, and some may say a fundamentally different nature
    to how they work.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络的概念受到了人类大脑和动物大脑的启发。我们可以看到有一些相似的概念，但我们也可以看到它们非常不同，有些人可能会说它们在运作方式上有根本性的不同。
- en: 6.3.6 Foundational Model Testing
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.6 基础模型测试
- en: Once a model has been trained, the next step is to test the model to assess
    its usefulness. Testing models can be a complex task. Typically, the dataset is
    first split into training and testing data, as discussed earlier. The training
    data is used to train the model, and then the testing set is used to evaluate
    how well the model is performing.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型被训练，下一步就是测试模型以评估其有用性。测试模型可能是一项复杂的任务。通常，数据集首先被分成训练数据和测试数据，如前所述。训练数据用于训练模型，然后测试集用于评估模型的表现有多好。
- en: One important thing to remember is that during training, the model parameters
    are updated after each batch. However, during testing, the model parameters remain
    unchanged. The goal of testing is to understand how the model will perform. If
    the model parameters are continually changing, it will make testing the performance
    impossible, as each test would impact the performance, and tests would not be
    reproducible.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的一个重要事情是，在训练过程中，模型参数在每个批次之后都会更新。然而，在测试过程中，模型参数保持不变。测试的目标是了解模型的表现。如果模型参数持续变化，将使得测试性能变得不可能，因为每次测试都会影响性能，而且测试将无法重复。
- en: Using a proportion of the data for testing, such as 20%, is helpful to understand
    the model’s performance for the team creating the model. However, it is not useful
    for comparing or explaining the model’s performance to others, as they do not
    have any understanding of the test data involved, the difficulty level of prediction,
    the scope of the testing, etc. This is where standard testing benchmarks come
    into play. The following section looks at the different benchmarks commonly used.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 使用数据的一部分进行测试，例如20%，有助于模型创建团队理解模型的表现。然而，这并不适用于与其他人比较或解释模型的表现，因为他们对涉及到的测试数据、预测的难度级别、测试范围等没有任何了解。这就是标准测试基准发挥作用的地方。下一节将探讨常用的不同基准。
- en: 6.3.6.1 Testing Benchmarks
  id: totrans-264
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.6.1 测试基准
- en: There exists a vast array of testing benchmarks, and every year sees the introduction
    of more, tailored for specific purposes or offering enhanced benefits. Such benchmarks
    are typically centred on particular tasks or broader areas.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 存在着大量的测试基准，每年都会有更多针对特定目的或提供增强效益的基准被引入。这些基准通常集中在特定的任务或更广泛的领域。
- en: Benchmarks commonly provide both the evaluation protocol (rules) and a standard
    dataset for assessing the models. This standard dataset ensures that different
    models undergo evaluation in a uniform manner, rendering the comparisons between
    various models meaningful. Standard datasets are often divided into training and
    testing subsets to ensure consistent training and testing. At times, these datasets
    are further segmented into validation sets. Typically, a model will be trained
    on the training set, fine-tuned using the validation set, and ultimately tested
    using the test set. On occasion, benchmark creators might withhold the testing
    set, asking model developers to submit their models for assessment. This strategy
    aims to prevent models from training on the test data, which would confer an unjust
    advantage during benchmark evaluation.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试通常提供评估协议（规则）和用于评估模型的标准数据集。这个标准数据集确保不同的模型以统一的方式进行评估，使得不同模型之间的比较有意义。标准数据集通常分为训练集和测试集，以确保一致的训练和测试。有时，这些数据集还会进一步分割成验证集。通常，模型将在训练集上训练，使用验证集进行微调，并最终使用测试集进行测试。有时，基准测试的创建者可能会保留测试集，要求模型开发者提交他们的模型进行评估。这种策略旨在防止模型在测试数据上训练，这在基准测试评估期间会给予模型不公平的优势。
- en: The evaluation protocol outlines crucial details about the test’s objectives,
    including how to compute the score based on outcomes from each test component.
    This score often serves as a straightforward singular metric for contrasting two
    models. Sometimes, either the benchmark creator or the relevant community will
    establish a leaderboard, displaying ranked scores for each tested model, facilitating
    easier comparison. Such leaderboards might also feature a human baseline score,
    derived from the average scores of human participants. This addition can offer
    context, helping users appreciate the model’s performance. It is not uncommon
    for models to surpass the average human score in many benchmarks.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 评估协议概述了测试目标的关键细节，包括如何根据每个测试组件的结果计算分数。这个分数通常作为对比两个模型的直接单一指标。有时，基准测试的创建者或相关社区将建立排行榜，显示每个测试模型的排名分数，便于比较。这样的排行榜也可能包括一个人类基线分数，这是通过人类参与者平均分数得出的。这种添加可以提供上下文，帮助用户欣赏模型的表现。在许多基准测试中，模型超越人类平均分数并不罕见。
- en: Below are examples of prevalent benchmarks, categorised by their focal area.
    This should offer a comprehensive understanding of each benchmark, highlighting
    both their strengths and limitations. The grouping area is not strict and the
    benchmarks may be presented under alternative headings depending on the context
    in particular for the more general benchmarks that cover a wider set of areas.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一些常见的基准测试示例，按其焦点领域分类。这应该提供对每个基准的全面理解，突出其优势和局限性。分组领域不是严格的，基准测试可能会根据特定上下文下的不同标题进行展示，特别是对于覆盖更广泛领域的更通用基准测试。
- en: 6.3.6.2 Common Sense Reasoning Benchmarks
  id: totrans-269
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.6.2 常识推理基准测试
- en: 6.3.6.2.1 BoolQ ([Clark et al., 2018](#ref6_15))
  id: totrans-270
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.3.6.2.1 BoolQ ([Clark 等人，2018](#ref6_15))
- en: BoolQ, which stands for Boolean Questions, is a question-answering dataset that
    involves determining whether a provided statement is true or false based on a
    given passage of text. The objective is to answer a binary (yes/no) question using
    the information in the passage. The benchmark comprises approximately 9,400 training
    examples, 3,200 verification examples, and 3,200 test examples.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: BoolQ，代表布尔问题，是一个问答数据集，它涉及根据给定的文本段落确定提供的陈述是真是假。目标是使用段落中的信息回答一个二元（是/否）问题。基准测试包括大约9,400个训练示例、3,200个验证示例和3,200个测试示例。
- en: 'Here is how the dataset is structured:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是数据集的结构：
- en: '**Passage**: A snippet of text that contains the information necessary to answer
    the given question.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**段落**：包含回答给定问题所需信息的文本片段。'
- en: '**Question**: A yes/no question based on the passage.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问题**：基于段落的是/否问题。'
- en: '**Label**: A binary label indicating whether the answer to the question is
    “Yes” or “No” based on the passage.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标签**：一个二元标签，表示根据段落，问题的答案是“是”还是“否”。'
- en: The performance of models on BoolQ is typically evaluated using accuracy, which
    is the proportion of correct answers out of the total number of examples.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: BoolQ上模型的性能通常使用准确率来评估，准确率是正确答案占总示例数的比例。
- en: 'The BoolQ dataset challenges models in several ways:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: BoolQ数据集以几种方式挑战模型：
- en: '**Reading Comprehension**: Models must be able to accurately extract and understand
    information from the passage to answer the question correctly.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**阅读理解**：模型必须能够准确提取和理解段落中的信息，以正确回答问题。'
- en: '**Binary Classification**: Models need to classify the answer into one of two
    categories: Yes or No.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**二分类**：模型需要将答案分类为两个类别之一：是或否。'
- en: '**Inference**: Sometimes, the answer may not be explicitly stated in the passage,
    requiring the model to make inferences based on the available information.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推理**：有时，答案可能没有在段落中明确陈述，需要模型根据可用信息进行推理。'
- en: By testing on datasets like BoolQ, researchers can gauge how well language models
    are able to understand and extract relevant information from text to answer questions
    accurately.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在 BoolQ 等数据集上进行测试，研究人员可以评估语言模型理解并从文本中提取相关信息以准确回答问题的能力。
- en: 6.3.6.2.2 PIQA ([Bisk et al., 2020](#ref6_7))
  id: totrans-282
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.3.6.2.2 PIQA ([Bisk 等人，2020](#ref6_7))
- en: The PIQA benchmark, which stands for Physical Intelligence Question Answering,
    is a dataset designed to test a system’s understanding of everyday physical reasoning.
    It was introduced by Bisk et al. in their 2020 paper. The benchmark comprises
    approximately 16,100 training examples, 1,800 verification examples, and 3,000
    test examples.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: PIQA 基准测试，即物理智能问答（Physical Intelligence Question Answering），是一个旨在测试系统对日常物理推理理解的测试数据集。它由
    Bisk 等人在他们 2020 年的论文中引入。基准测试包含大约 16,100 个训练示例、1,800 个验证示例和 3,000 个测试示例。
- en: In this benchmark, questions are posed in such a manner that they necessitate
    the model to exhibit a common-sense understanding of the physical world to furnish
    accurate answers.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个基准测试中，问题以这种方式提出，需要模型展示对物理世界的常识性理解，以提供准确的答案。
- en: 'Here is how the data is structured in PIQA:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是 PIQA 中数据结构的组织方式：
- en: '**Question**: A question that typically involves some aspect of everyday physical
    reasoning. This could include questions about the states of matter, simple machines,
    the motion of objects, etc.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问题**：通常涉及日常物理推理方面的问题。这可能包括关于物质状态、简单机械、物体运动等问题。'
- en: '**Answer**: The correct answer to the question, often a sentence or phrase
    that explains the reasoning or provides a solution to the posed problem.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**答案**：问题的正确答案，通常是一个句子或短语，解释推理或为提出的问题提供解决方案。'
- en: The performance of models on the PIQA benchmark is typically evaluated based
    on the accuracy of the answers produced. This accuracy measure checks whether
    the model’s answer matches the correct answer or if it provides a logically equivalent
    solution to the problem posed.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: PIQA 基准测试中模型的性能通常基于产生的答案的准确性来评估。这个准确性指标检查模型的答案是否与正确答案匹配，或者它是否提供了对提出问题的逻辑等价解决方案。
- en: PIQA is a challenging dataset as it necessitates models to possess a common-sense
    understanding of physical principles and to apply this understanding to novel
    situations. It serves as a means to assess how proficiently AI systems can reason
    about the physical world in a manner analogous to humans.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: PIQA 是一个具有挑战性的数据集，因为它要求模型具备对物理原理的常识性理解，并将这种理解应用于新情境。它作为评估 AI 系统能够以类似于人类的方式推理物理世界熟练程度的手段。
- en: 6.3.6.2.3 SIQA ([Sap et al., 2019](#ref6_66))
  id: totrans-290
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.3.6.2.3 SIQA ([Sap 等人，2019](#ref6_66))
- en: The Social Intelligence Question Answering (SIQA) benchmark was introduced in
    a paper by Maarten Sap et al. in 2019\. This dataset is designed to evaluate common-sense
    reasoning in AI models within the context of social situations. The objective
    is to determine how proficiently models can comprehend and reason about social
    scenarios, a skill vital for the development of AI systems that can interact naturally
    and effectively with humans. The benchmark comprises approximately 33,400 training
    examples, 1,900 verification examples, and 2,000 test examples.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 社会智能问答（SIQA）基准测试是在 Maarten Sap 等人于 2019 年发表的一篇论文中引入的。这个数据集旨在评估在社交情境中 AI 模型的常识推理能力。目标是确定模型在理解和推理社交场景方面的熟练程度，这对于开发能够与人类自然有效地互动的
    AI 系统至关重要。基准测试包含大约 33,400 个训练示例、1,900 个验证示例和 2,000 个测试示例。
- en: 'The SIQA dataset consists of questions about social situations. Each question
    is paired with three possible answers: one correct answer and two incorrect answers.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: SIQA 数据集包含关于社交情境的问题。每个问题都与三个可能的答案配对：一个正确答案和两个错误答案。
- en: '**Context**: A description of a social situation.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情境**：对社交情况的描述。'
- en: '**Question**: A question related to the social scenario provided in the context.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问题**：与上下文中提供的社会情境相关的问题。'
- en: '**Answers**: Three possible answers are provided, one correct and two incorrect.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**答案**：提供了三个可能的答案，一个正确和两个错误。'
- en: The performance on SIQA is typically evaluated using metrics such as accuracy,
    which measures the proportion of correctly answered questions out of the total
    number posed. The reasoning behind the answer choices, as well as the social understanding
    exhibited by the model, can be analysed to gain deeper insight into the model’s
    performance.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: SIQA 的性能通常使用准确率等指标来评估，该指标衡量正确回答的问题占总提出问题的比例。通过分析答案选择背后的推理以及模型展现的社会理解，可以更深入地了解模型的性能。
- en: This benchmark assists researchers in gauging how proficiently their models
    can comprehend, interpret, and respond to social situations, marking a significant
    step towards the development of more socially aware AI systems.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 这个基准测试帮助研究人员评估他们的模型在理解、解释和回应社会情境方面的熟练程度，这是朝着开发更具有社会意识的 AI 系统迈出的重要一步。
- en: 6.3.6.2.4 SWAG ([Zellers et al., 2018](#ref6_84))
  id: totrans-298
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.3.6.2.4 SWAG ([Zellers 等人，2018](#ref6_84))
- en: SWAG (Situations With Adversarial Generations) is a dataset aimed at evaluating
    grounded common-sense inference. It is designed to measure a system’s ability
    to reason about everyday situations described in a sentence. The benchmark presents
    a partially observable scenario, with the objective being to predict the most
    plausible continuation from among four choices. The benchmark comprises approximately
    73,000 training examples, 20,000 verification examples, and 20,000 test examples.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: SWAG（具有对抗性生成的情境）是一个旨在评估基于常识推理的语料库。它旨在衡量系统对句子中描述的日常情境进行推理的能力。基准测试提供了一个部分可观察的场景，目标是预测四个选项中最可能的后续情况。基准测试包含大约
    73,000 个训练示例，20,000 个验证示例和 20,000 个测试示例。
- en: 'Here is how the SWAG dataset is structured:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是如何构建 SWAG 语料库的：
- en: '**Premise Sentence**: This is a given statement or situation that sets up a
    scenario.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前提句子**：这是一个给定的陈述或情况，它设定了一个场景。'
- en: '**Ending Options**: There are four possible endings provided for each scenario.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结尾选项**：为每个场景提供了四个可能的结尾。'
- en: '**Correct Ending**: Among the four endings, one of them is labelled as the
    correct or most plausible continuation of the scenario.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正确结尾**：在四个结尾中，其中一个被标记为场景的正确或最可能的后续情况。'
- en: The task is to select the most plausible ending based on the provided premise.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 任务是根据提供的前提选择最可能的结尾。
- en: The dataset comprises multiple-choice questions about grounded situations, where
    models are expected to choose the most plausible continuation from four options.
    The creators of SWAG employed a novel adversarial filtering technique to construct
    the dataset. This ensures that the distractor (incorrect) answers are challenging
    and cannot be easily distinguished from the correct answer based solely on superficial
    text patterns.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 该语料库包含关于基于情境的多项选择题，其中模型被期望从四个选项中选择最可能的后续情况。SWAG 的创造者采用了一种新颖的对抗性过滤技术来构建该语料库。这确保了干扰（错误）答案具有挑战性，并且仅基于表面的文本模式无法轻易与正确答案区分开来。
- en: The task for the model is to select the most plausible ending (in this case,
    option d) given the premise.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的任务是，根据前提条件选择最可能的结尾（在这种情况下，选项 d）。
- en: In evaluating a model using SWAG, the model’s accuracy in selecting the correct
    ending is measured. This benchmark, therefore, provides a means to assess a system’s
    common-sense reasoning capabilities within a grounded, real-world scenario context.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 SWAG 评估模型时，测量模型选择正确结尾的准确率。因此，这个基准测试提供了一个评估系统在基于常识、真实世界场景背景下的推理能力的方法。
- en: 6.3.6.2.5 HellaSwag ([Zellers et al., 2019](#ref6_85))
  id: totrans-308
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.3.6.2.5 HellaSwag ([Zellers 等人，2019](#ref6_85))
- en: HellaSwag is a benchmark dataset for evaluating machine learning models on their
    ability to perform common-sense reasoning. It can be seen as an extension or a
    more challenging version of the SWAG benchmark. The benchmark comprises approximately
    39,000 training examples, 10,000 verification examples, and 20,000 test examples.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: HellaSwag 是一个基准语料库，用于评估机器学习模型在执行常识推理方面的能力。它可以被视为 SWAG 基准测试的扩展或更具有挑战性的版本。基准测试包含大约
    39,000 个训练示例，10,000 个验证示例和 20,000 个测试示例。
- en: 'Here is a detailed breakdown:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个详细的分解：
- en: '**Premise**: Similar to SWAG, HellaSwag begins with a premise describing a
    particular scenario. However, the premises in HellaSwag are typically more intricate
    and potentially ambiguous.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前提**：与SWAG类似，HellaSwag从一个描述特定场景的前提开始。然而，HellaSwag中的前提通常更为复杂和可能具有歧义。'
- en: '**Ending Options**: For each premise, four possible continuations are presented.
    These continuations are frequently designed to be misleading or non-obvious, thereby
    challenging the model’s reasoning capabilities.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结尾选项**：对于每个前提，提供四个可能的延续。这些延续通常被设计为具有误导性或非显而易见，从而挑战模型的推理能力。'
- en: '**Correct Ending**: Among the four continuations, one is labelled as the correct
    or most plausible continuation based on the scenario described in the premise.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正确结尾**：在四个延续中，有一个被标记为基于前提中描述的场景的正确或最可能的延续。'
- en: The primary objective for a machine learning model in this benchmark is to select
    the most plausible continuation based on the provided premise.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个基准中，机器学习模型的主要目标是根据提供的前提选择最可能的延续。
- en: The creators of HellaSwag employed a more sophisticated process to produce challenging
    distractor options among the continuations. They utilised an LLM to automatically
    generate distractor continuations that are plausible but incorrect. This approach
    renders HellaSwag a notably challenging benchmark, as the distractors are designed
    to be misleading for both models and potentially human evaluators.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: HellaSwag的创造者采用了一种更复杂的过程来生成延续中的具有挑战性的干扰选项。他们使用一个大型语言模型（LLM）自动生成看似合理但实际上错误的干扰延续。这种方法使得HellaSwag成为一个特别具有挑战性的基准，因为干扰选项被设计为对模型和可能的人类评估者都具有误导性。
- en: The task for the model is to select the most plausible ending (in this case,
    option C) given the premise. The accuracy of the model on this task would indicate
    its ability to reason through complex, real-world scenarios with potentially misleading
    information.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的任务是选择最可能的结尾（在这种情况下，选项C）。模型在这个任务上的准确性将表明其通过复杂、现实场景并可能存在误导性信息进行推理的能力。
- en: HellaSwag was designed to be a challenging benchmark to push the boundaries
    of what models can do in terms of common-sense reasoning and understanding nuanced
    real-world scenarios.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: HellaSwag被设计为一个具有挑战性的基准，以推动模型在常识推理和理解细微的现实场景方面的边界。
- en: 6.3.6.2.6 WinoGrande ([Sakaguchi et al., 2019](#ref6_65))
  id: totrans-318
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.3.6.2.6 WinoGrande ([Sakaguchi等人，2019](#ref6_65))
- en: WinoGrande is a large-scale dataset designed to evaluate machine learning models
    on their ability to solve Winograd Schema challenges. The Winograd Schema challenge
    is a type of common-sense reasoning task that tests a model’s capability to resolve
    pronoun references in sentences. The benchmark comprises approximately 9,200 training
    examples, 1,200 verification examples, and 1,700 test examples.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: WinoGrande是一个大规模数据集，旨在评估机器学习模型解决Winograd Schema挑战的能力。Winograd Schema挑战是一种常识推理任务，测试模型在句子中解决代词引用的能力。该基准包括大约9,200个训练示例，1,200个验证示例和1,700个测试示例。
- en: WinoGrande offers a significant number of examples to furnish a more statistically
    robust assessment of a model’s performance on such tasks. The examples within
    WinoGrande are crafted to be minimally divergent, signifying that a slight alteration
    in the wording of a sentence can modify the correct answer. This design aims to
    probe a model’s grasp of nuanced language and contextual information. WinoGrande
    utilises an adversarial filtering approach to ensure the calibre and challenge
    level of the examples in the dataset. This filtering method aids in excluding
    examples that are either overly simplistic or present multiple potentially correct
    answers. The primary task for models using WinoGrande is to clarify ambiguous
    pronoun references within sentences, identifying, for instance, the specific entity
    to which a pronoun pertains.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: WinoGrande提供了大量的示例，以提供对模型在类似任务上性能的更稳健的统计评估。WinoGrande中的示例被精心设计，以尽可能减少差异，这意味着句子措辞的微小变化可以改变正确答案。这种设计旨在测试模型对细微语言和上下文信息的掌握。WinoGrande采用对抗过滤方法来确保数据集中示例的质量和挑战水平。这种方法有助于排除过于简单或存在多个可能正确答案的示例。使用WinoGrande的模型的主要任务是澄清句子中的模糊代词引用，例如确定代词所指的具体实体。
- en: The challenge in this example arises from the ambiguous reference of “it” in
    the sentence. The model is tasked with determining to what “it” refers, based
    on the contextual information provided in the sentence.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子中的挑战来自于句子中“它”的模糊指代。模型的任务是根据句子中提供的上下文信息确定“它”指的是什么。
- en: The WinoGrande benchmark aims to offer a more rigorous evaluation of models’
    common-sense reasoning abilities and their grasp of nuanced language. It is designed
    to be a challenging benchmark that pushes the boundaries of models’ capabilities
    in terms of common-sense reasoning and understanding natural language.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: WinoGrande基准旨在提供对模型常识推理能力和对细微语言掌握的更严格评估。它被设计为一个具有挑战性的基准，旨在推动模型在常识推理和理解自然语言方面的能力边界。
- en: 6.3.6.2.7 ARC Easy and Challenge ([Clark et al., 2018](#ref6_15))
  id: totrans-323
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.3.6.2.7 ARC Easy and Challenge ([Clark et al., 2018](#ref6_15))
- en: 'The ARC dataset stands for AI2 Reasoning Challenge, which was developed by
    the Allen Institute for Artificial Intelligence (AI2). The dataset is created
    to evaluate a machine learning model’s ability to answer questions that require
    reasoning and understanding across several sentences. The ARC dataset is divided
    into two subsets: ARC-Easy and ARC-Challenge.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: ARC数据集代表AI2 Reasoning Challenge，由艾伦人工智能研究所（AI2）开发。该数据集旨在评估机器学习模型回答需要推理和理解多个句子的问题的能力。ARC数据集分为两个子集：ARC-Easy和ARC-Challenge。
- en: '**ARC-Easy** (2,200 training, 500 validation, 2,300 testing):'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '**ARC-Easy**（2,200个训练样本，500个验证样本，2,300个测试样本）：'
- en: This part of the dataset contains questions that are relatively easy to answer.
    These questions might not require deep reasoning and might be solvable with straightforward
    fact retrieval or simpler inference.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集部分包含相对容易回答的问题。这些问题可能不需要深入的推理，可能可以通过简单的事实检索或更简单的推理来解决。
- en: '**ARC-Challenge** (1,100 training, 290 validation, 1,100 testing):'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '**ARC-Challenge**（1,100个训练样本，290个验证样本，1,100个测试样本）：'
- en: This subset consists of questions that are more challenging and are designed
    to necessitate more advanced reasoning to answer correctly. The questions in ARC-Challenge
    are expected to be difficult for current machine learning models and aim to push
    the boundary of what AI systems can achieve in terms of reasoning.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 这个子集包含更具挑战性的问题，这些问题旨在需要更高级的推理才能正确回答。ARC-Challenge中的问题预计对当前机器学习模型来说将是困难的，旨在推动AI系统在推理方面的边界。
- en: Each example in the ARC dataset consists of a question, a set of possible answer
    choices, and the correct answer. The questions are formatted as multiple-choice
    questions. This format allows for clear evaluation metrics by checking whether
    the model selects the correct answer. The questions cover a range of topics, primarily
    within the domain of science. They are sourced from real 3rd to 9th grade science
    exams, aiming to challenge models with questions that are easy for humans but
    hard for machines. Both the ARC-Easy and ARC-Challenge subsets are designed to
    require external knowledge to answer correctly, going beyond the information given
    in the question itself.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: ARC数据集中的每个示例都包含一个问题、一组可能的答案选项和正确答案。问题以多项选择题的形式呈现。这种格式允许通过检查模型是否选择了正确答案来获得清晰的评估指标。这些问题覆盖了广泛的主题，主要在科学领域。它们来源于真实的3至9年级科学考试，旨在用对人类来说容易但对机器来说困难的问题挑战模型。ARC-Easy和ARC-Challenge子集都是设计来需要外部知识才能正确回答的，超越了问题本身提供的信息。
- en: '**ARC-Easy Example**:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '**ARC-Easy 示例**：'
- en: '**Question**: “What gas do plants absorb from the atmosphere to photosynthesise?”'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问题**：“植物从大气中吸收哪种气体进行光合作用？”'
- en: '**Answers**:'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**答案**：'
- en: Oxygen
  id: totrans-333
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 氧气
- en: Nitrogen
  id: totrans-334
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 氮
- en: Carbon Dioxide
  id: totrans-335
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 二氧化碳
- en: Hydrogen
  id: totrans-336
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 氢
- en: '**The correct answer** is c) Carbon Dioxide.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正确答案是** c) 二氧化碳。'
- en: '**ARC-Challenge Example**:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '**ARC-Challenge 示例**：'
- en: '**Question**: “If a plant living in a desert has evolved to have spines instead
    of leaves, what could be the most likely reason for this adaptation?”'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问题**：“如果一个生活在沙漠中的植物进化出刺而不是叶子，这种适应的最可能原因是什么？”'
- en: '**Answers**:'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**答案**：'
- en: To attract more insects for pollination
  id: totrans-341
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了吸引更多昆虫进行授粉
- en: To reduce water loss through transpiration
  id: totrans-342
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了减少通过蒸腾作用的水分损失
- en: To capture more sunlight for photosynthesis
  id: totrans-343
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了捕获更多阳光进行光合作用
- en: To make it easier for the plant to capture prey
  id: totrans-344
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使植物更容易捕捉猎物
- en: '**The correct answer** is b) To reduce water loss through transpiration.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正确答案是** b) 为了减少通过蒸腾作用的水分损失。'
- en: Alongside the ARC dataset, a corpus of text is provided which contains the information
    necessary to answer the questions. This corpus can be used to train models to
    use external information to answer questions.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 ARC 数据集，还提供了一个包含回答问题所需信息的文本语料库。这个语料库可以用来训练模型使用外部信息来回答问题。
- en: It should be noted that the corpus of text is not structured; each sentence
    can be on totally different topics (e.g., one sentence quoted above pertains to
    floods, followed by photosynthesis, and then a reference to the god Mars). The
    model being evaluated may have to combine items from disparate locations in the
    corpus of text to answer specific, particularly challenging questions. In the
    above example at least five different areas of the text corpus are relevant or
    potentially relevant. Naturally, the model can draw on its other training data,
    not just the corpus associated with the benchmark, to answer the questions.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 应该注意的是，文本语料库是非结构化的；每个句子可能涉及完全不同的主题（例如，上面引用的句子涉及洪水，接着是光合作用，然后是关于火星神的引用）。被评估的模型可能需要从文本语料库的不同位置组合项目来回答特定且特别具有挑战性的问题。在上面的例子中，至少有五个不同的文本语料库区域是相关或可能相关的。自然地，模型可以借鉴其其他训练数据，而不仅仅是与基准相关的语料库，来回答问题。
- en: The primary aim of the ARC dataset is to encourage the development of new models
    that can reason and understand text in a manner akin to humans, especially within
    an educational or scientific context. The distinction between ARC-Easy and ARC-Challenge
    allows for the evaluation of models at different levels of difficulty, advancing
    the state-of-the-art in machine reasoning.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: ARC 数据集的主要目标是鼓励开发新的模型，这些模型能够以类似于人类的方式推理和理解文本，尤其是在教育或科学环境中。ARC-Easy 和 ARC-Challenge
    之间的区别允许在不同难度的水平上评估模型，从而推进机器推理的最新进展。
- en: 6.3.6.3 Question Answering Benchmarks
  id: totrans-349
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.6.3 问答基准
- en: 6.3.6.3.1 OpenBookQA ([Mihaylov et al., 2018](#ref6_43))
  id: totrans-350
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.3.6.3.1 OpenBookQA ([Mihaylov 等人，2018](#ref6_43))
- en: OpenBookQA is a benchmark designed to evaluate the ability of machine learning
    models to answer questions based on a small set of facts, known as the “Open Book”.
    The “Open Book” consists of a collection of facts that should be sufficient for
    answering the questions in the dataset. The aim is to test the model’s capacity
    to reason over these facts and combine information to answer questions accurately.
    The benchmark has about 4,900 training examples and about 500 verification examples,
    and 500 test examples. The open book contains about 1,300 entries or facts.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: OpenBookQA 是一个旨在评估机器学习模型基于少量事实（称为“开放书籍”）回答问题的能力的基准。这个“开放书籍”包含了一系列事实，这些事实应该足以回答数据集中的问题。目标是测试模型对这些事实进行推理和结合信息以准确回答问题的能力。该基准大约有
    4,900 个训练示例，大约 500 个验证示例和 500 个测试示例。开放书籍包含大约 1,300 个条目或事实。
- en: Each question comes with four answer choices, out of which only one is correct.
    The questions are designed to be answerable with the help of the facts provided
    in the Open Book, though some external common knowledge might also be required.
    The questions cover a variety of topics and are designed to test various forms
    of reasoning including retrieval, comparison, spatial reasoning, temporal reasoning,
    causality, etc. Evaluation is typically done based on the accuracy of the model
    in selecting the correct answer from the provided options.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 每个问题都附带四个答案选项，其中只有一个正确。这些问题设计成可以通过 Open Book 中提供的事实来回答，尽管可能还需要一些外部常识。这些问题覆盖了各种主题，旨在测试各种推理形式，包括检索、比较、空间推理、时间推理、因果关系等。评估通常基于模型在提供的选项中选择正确答案的准确性。
- en: OpenBookQA is used by researchers to evaluate and compare different question-answering
    models. It is particularly useful for assessing how well models can leverage a
    limited set of facts to answer a broad range of questions.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: OpenBookQA 被研究人员用来评估和比较不同的问答模型。它特别有助于评估模型如何有效地利用有限的事实来回答广泛的问题。
- en: The OpenBookQA benchmark presents a controlled setting to evaluate how effectively
    machine learning models can utilise a set of facts to answer questions that require
    some level of reasoning or synthesis of information.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: OpenBookQA 基准提供了一个受控的环境来评估机器学习模型如何有效地利用一组事实来回答需要一定推理或信息综合水平的问题。
- en: 6.3.6.3.2 Natural Questions ([Kwiatkowski et al., 2019](#ref6_34))
  id: totrans-355
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.3.6.3.2 Natural Questions ([Kwiatkowski 等人，2019](#ref6_34))
- en: The Natural Questions (NQ) benchmark, introduced by Kwiatkowski et al. in 2019,
    is designed to evaluate models on their ability to answer real-world questions
    based on the content of a given document. In this benchmark, each example consists
    of a question along with a Wikipedia page, and the task is to identify a specific
    span of text from the page that answers the question, or indicate that no answer
    is present. The full dataset is 42Gb, but a simplified dataset is available which
    is about 4Gb.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 自然问题（NQ）基准，由Kwiatkowski等人于2019年引入，旨在评估模型根据给定文档的内容回答现实世界问题的能力。在这个基准中，每个示例都包含一个问题以及一个维基百科页面，任务是识别页面中回答问题的特定文本范围，或指出没有答案。完整数据集为42Gb，但提供了一个简化的数据集，大约为4Gb。
- en: 'This task closely mimics real-world scenarios where users pose questions based
    on a document or a web page they are viewing. For instance, a question could be:
    “When was the Eiffel Tower completed?” Given a Wikipedia page about the Eiffel
    Tower, the correct response would be to identify the text span “completed in 1889”
    as the answer. This benchmark is significant as it requires models to effectively
    handle a wide range of natural language questions and to extract precise answers
    from the accompanying documents, showcasing their comprehension and information
    retrieval capabilities.'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 这个任务紧密模拟了现实世界场景，其中用户根据他们正在查看的文档或网页提出问题。例如，一个问题可能是：“埃菲尔铁塔何时完工？”给定一个关于埃菲尔铁塔的维基百科页面，正确的回答应该是识别文本范围“1889年完工”作为答案。这个基准很重要，因为它要求模型有效地处理各种自然语言问题，并从相关文档中提取精确答案，展示了它们的理解和信息检索能力。
- en: 6.3.6.3.3 TriviaQA ([Joshi et al., 2017](#ref6_30))
  id: totrans-358
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.3.6.3.3 TriviaQA ([Joshi et al., 2017](#ref6_30))
- en: TriviaQA, introduced by Joshi et al. in 2017, is a benchmark designed to evaluate
    models on their ability to answer trivia questions. The dataset comprises question–answer
    pairs from trivia enthusiasts along with evidence documents that provide supporting
    information for the answers. The goal is for models to accurately answer the questions
    using the information available in the associated documents. It contains over
    650K question-answer-evidence triples.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: TriviaQA是由Joshi等人于2017年引入的，是一个旨在评估模型在回答常识问题能力上的基准。数据集包含来自常识爱好者的问答对以及提供答案支持信息的证据文档。目标是让模型能够使用相关文档中的信息准确回答问题。它包含超过650K个问答-证据三元组。
- en: In TriviaQA, the questions are grouped by the source from which they come, and
    are categorised as either verified or unverified based on whether they have been
    cross-checked against the evidence documents. The evidence documents are collected
    from various sources like Wikipedia, web pages, or books, which provide a good
    diversity of language and complexity.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 在TriviaQA中，问题根据它们来源的来源进行分组，并根据是否与证据文档进行过交叉核对，将它们分类为已验证或未验证。证据文档来自各种来源，如维基百科、网页或书籍，这些提供了良好的语言多样性和复杂性。
- en: 'A sample question from TriviaQA might be:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: TriviaQA的一个示例问题可能是：
- en: '**Question**: “What river is the principal river of northern Italy?” **Answer**:
    “Po”.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问题**：“意大利北部的河流是什么？” **答案**：“波河”。'
- en: '**Evidence**: Link to Wikipedia, and perhaps a link to another source'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**证据**：链接到维基百科，也许还有另一个来源的链接'
- en: In this benchmark, models are evaluated on their ability to not only provide
    the correct answer but also to demonstrate an understanding of the context and
    evidence from the supporting documents that justify the answer. This benchmark
    challenges models in reading comprehension, knowledge extraction, and the ability
    to handle a mix of formal and informal text, making it a robust measure of a model’s
    capacity to deal with real-world, open-domain question-answering scenarios.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个基准中，模型不仅要提供正确的答案，还要展示对支持文档中证明答案的上下文和证据的理解。这个基准挑战模型在阅读理解、知识提取以及处理正式和非正式文本混合的能力，使其成为衡量模型处理现实世界、开放域问答场景能力的一个稳健指标。
- en: 6.3.6.3.4 SQuAD v1.1 ([Rajpurkar et al., 2016](#ref6_56)) and 2.0 ([Rajpurkar
    et al., 2018](#ref6_55))
  id: totrans-365
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.3.6.3.4 SQuAD v1.1 ([Rajpurkar et al., 2016](#ref6_56)) 和 2.0 ([Rajpurkar
    et al., 2018](#ref6_55))
- en: The Stanford Question Answering Dataset (SQuAD 1.1) is a collection of 100k
    crowdsourced question-answer pairs ([Rajpurkar et al., 2016](#ref6_56)).
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 斯坦福问答数据集（SQuAD 1.1）是一个包含10万个众包问答对的集合 ([Rajpurkar et al., 2016](#ref6_56))。
- en: The Stanford Question Answering Dataset (SQuAD) is a widely recognised benchmark
    for evaluating the performance of machine reading and question-answering (QA)
    systems. In this benchmark, models are provided with a passage of text and then
    asked to answer questions based on the content of that text.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 斯坦福问答数据集（SQuAD）是评估机器阅读和问答（QA）系统性能的一个公认的基准。在这个基准中，模型被提供一段文本，然后要求根据该文本的内容回答问题。
- en: 'SQuAD consists of two main versions: SQuAD 1.1 and SQuAD 2.0\. In SQuAD 1.1,
    the focus is on answering questions where the answer is guaranteed to be present
    in the provided passage. SQuAD 2.0, on the other hand, includes questions for
    which the answer may or may not be present in the passage, thus challenging models
    to determine when the information needed to answer a question is lacking.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: SQuAD包括两个主要版本：SQuAD 1.1和SQuAD 2.0。在SQuAD 1.1中，重点是回答那些答案肯定存在于提供的段落中的问题。另一方面，SQuAD
    2.0包括那些答案可能或可能不在段落中的问题，从而挑战模型确定何时所需回答问题的信息缺失。
- en: 'Here is an example from SQuAD 1.1:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个来自SQuAD 1.1的例子：
- en: '**Passage**: “Super Bowl 50 was an American football game to determine the
    champion of the National Football League (NFL) for the 2015 season. The American
    Football Conference (AFC) champion Denver Broncos defeated the National Football
    Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl
    title.”'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**段落**： “超级碗50是一场美国橄榄球比赛，用以确定2015赛季国家橄榄球联盟（NFL）的冠军。美国橄榄球联盟（AFC）冠军丹佛野马队以24-10击败国家橄榄球联盟（NFC）冠军卡罗来纳黑豹队，赢得了他们的第三个超级碗冠军。”'
- en: '**Question**: “Which NFL team won Super Bowl 50?”'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问题**： “哪个NFL球队赢得了超级碗50？”'
- en: '**Answer**: “Denver Broncos”'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**答案**： “丹佛野马”'
- en: 'And an example from SQuAD 2.0:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个来自SQuAD 2.0的例子：
- en: '**Passage**: as above'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**段落**：如上所述'
- en: '**Question**: “Who was the MVP of Super Bowl 50?”'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问题**： “超级碗50的MVP是谁？”'
- en: '**Answer**: In this case, the passage does not provide the information needed
    to answer the question, so the correct response would be to indicate that the
    answer is not present in the passage.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**答案**：在这种情况下，段落没有提供回答问题的所需信息，因此正确的回答应该是指出答案不在段落中。'
- en: SQuAD has been a crucial benchmark for evaluating and comparing different QA
    systems, and has spurred a significant amount of research in the NLP community.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: SQuAD一直是评估和比较不同问答系统的一个关键基准，并在自然语言处理（NLP）社区中激发了大量研究。
- en: 6.3.6.4 Reading Comprehension Benchmarks
  id: totrans-378
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.6.4 阅读理解基准
- en: 6.3.6.4.1 RACE reading comprehension benchmark ([Lai et al., 2017](#ref6_35))
  id: totrans-379
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.3.6.4.1 RACE阅读理解基准 ([Lai等人，2017](#ref6_35))
- en: The RACE (ReAding Comprehension from Examinations) dataset is a large-scale
    reading comprehension dataset collected from English examinations in China, intended
    for students in grades 3 through 12\. The benchmark is designed to evaluate machine
    comprehension models in a more challenging and realistic setting, as it includes
    a diverse range of question types and topics.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: RACE（从考试中读取理解，ReAding Comprehension from Examinations）数据集是一个大规模的阅读理解数据集，收集自中国的英语考试，旨在为3至12年级的学生使用。该基准旨在在一个更具挑战性和现实的环境中评估机器理解模型，因为它包括多种多样的题目类型和主题。
- en: 'The dataset is split into two subsets: RACE-M, which consists of middle school
    exam questions, and RACE-H, which consists of high school exam questions.'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集分为两个子集：RACE-M，它包括中学考试问题，和RACE-H，它包括高中考试问题。
- en: 'Here is the format for how questions and passages are structured within the
    RACE benchmark:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 在RACE基准中，问题和段落的结构格式如下：
- en: A passage is provided, which could be a narrative, an article, or a dialogue.
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供一段文本，这可能是一篇叙述、一篇文章或一段对话。
- en: Multiple-choice questions related to the passage are presented, each with four
    answer options.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与段落相关的多项选择题被呈现出来，每个问题都有四个选项。
- en: 'Here is a simplified example inspired by the kind of questions you might find
    in the RACE dataset:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个受RACE数据集中可能找到的问题类型启发的简化例子：
- en: '**Passage**: “In 1920, women in the United States won the right to vote with
    the ratification of the 19th amendment to the US Constitution. This was the result
    of many years of struggle and activism by women suffragists who believed in equal
    voting rights for women.”'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**段落**： “1920年，美国妇女通过美国宪法的第19修正案的批准赢得了投票权。这是妇女选举权运动者多年斗争和积极活动的结果，她们相信妇女应该享有平等的投票权。”'
- en: '**Question**: “What did the 19th amendment to the US Constitution grant?”'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问题**： “美国宪法的第19修正案赋予了什么？”'
- en: '**Answers**:'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**答案**：'
- en: The right for women to work
  id: totrans-389
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 妇女工作的权利
- en: The right for women to vote
  id: totrans-390
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 妇女选举权
- en: The abolition of slavery
  id: totrans-391
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 奴隶制的废除
- en: The establishment of income tax
  id: totrans-392
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所得税的设立
- en: '**Correct Answer**: b) The right for women to vote'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正确答案**: b) 妇女选举权'
- en: In this example, the passage provides the necessary information to answer the
    multiple-choice question. The model’s task is to understand the passage well enough
    to select the correct answer from the provided options. In a real-world scenario,
    the questions in RACE can be much more challenging and the passages longer and
    more complex, making it a robust benchmark for evaluating reading comprehension
    models.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，文章提供了回答多项选择题所需的信息。模型的任务是充分理解文章，以便从提供的选项中选择正确答案。在现实世界的场景中，RACE中的问题可能更具挑战性，文章更长且更复杂，这使得它成为评估阅读理解模型的强大基准。
- en: 6.3.6.5 Mathematical Reasoning Benchmarks
  id: totrans-395
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.6.5 数学推理基准
- en: 6.3.6.5.1 MATH ([Hendrycks et al., 2021](#ref6_26))
  id: totrans-396
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.3.6.5.1 MATH ([Hendrycks et al., 2021](#ref6_26))
- en: The MATH (Mathematical Reasoning Dataset) benchmark introduced by Hendrycks
    et al., in 2021, is designed to evaluate the mathematical reasoning abilities
    of machine learning models. The dataset contains mathematical problems that require
    various levels of reasoning and understanding to solve. The problems cover a wide
    range of topics and difficulty levels, which makes it a challenging benchmark
    for assessing how well models can handle abstract mathematical reasoning and symbol
    manipulation. The dataset consists of 12,500 challenging competition mathematics
    problems (7,500 training, 5,000 testing). Each problem in MATH has a full step-by-step
    solution which can be used to teach models to generate answer derivations and
    explanations.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: Hendrycks等人于2021年引入的MATH（数学推理数据集）基准旨在评估机器学习模型的数学推理能力。该数据集包含需要各种推理和理解水平才能解决的问题。这些问题覆盖了广泛的主题和难度级别，这使得它成为评估模型如何处理抽象数学推理和符号操作的挑战性基准。该数据集包含12,500个具有挑战性的竞赛数学问题（7,500个训练，5,000个测试）。MATH中的每个问题都有一个完整的逐步解决方案，可用于教会模型生成答案推导和解释。
- en: The questions in the MATH dataset are categorised into several topics, including
    algebra, calculus, geometry, measurement, number theory, probability, and statistics.
    Each question is accompanied by a step-by-step solution, which is intended to
    help evaluate how well models can generate not just the final answer, but also
    the intermediate steps and explanations.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: MATH数据集中的问题被分为几个主题，包括代数、微积分、几何、测量、数论、概率和统计学。每个问题都附有逐步解决方案，旨在帮助评估模型生成最终答案以及中间步骤和解释的能力。
- en: 'Here is a simplified example inspired by the kind of questions you might find
    in the MATH dataset:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个受MATH数据集中可能找到的问题启发的简化示例：
- en: '**Question**: “Simplify the expression: (x² + 2x + 1) + (2x² + 3x + 2)”'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问题**: “简化表达式：(x² + 2x + 1) + (2x² + 3x + 2)”'
- en: '**Step-by-step Solution**: “First, we’ll add the like terms together. We’ll
    start with the terms that have x²: x² + 2x² = 3x²; Now, we’ll add the terms that
    have x: 2x + 3x = 5x; Finally, we’ll add the constant terms: 1 + 2 = 3; Putting
    it all together, we get: 3x + 2+5x+3'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**逐步解决方案**: “首先，我们将同类项相加。我们首先处理含有x²的项：x² + 2x² = 3x²；接下来，我们将含有x的项相加：2x + 3x
    = 5x；最后，我们将常数项相加：1 + 2 = 3；将所有这些加在一起，我们得到：3x + 2 + 5x + 3'
- en: '**Final Answer**: 3x² + 5x + 3'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最终答案**: 3x² + 5x + 3'
- en: In this problem, the model would need to recognise the structure of polynomial
    expressions and perform the appropriate operations to simplify the expression.
    The step-by-step solution is crucial for understanding the model’s reasoning process.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个问题中，模型需要识别多项式表达式的结构并执行适当的操作以简化表达式。逐步解决方案对于理解模型的推理过程至关重要。
- en: 6.3.6.5.2 GSM8k ([Cobbe et al., 2021](#ref6_16))
  id: totrans-404
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.3.6.5.2 GSM8k ([Cobbe et al., 2021](#ref6_16))
- en: GSM8K (Grade School Math) consists of high-quality grade school math problems.
    These were created by human problem writers. The dataset is segmented into 7,400
    training problems and 1,300 test problems. The problems take between 2 and 8 steps
    to solve, and solutions involve performing a sequence of elementary calculations
    using basic arithmetic operations to reach the final answer. This benchmark is
    from Cobbe et al. in OpenAI.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: GSM8K（小学数学）包含高质量的初级数学问题。这些问题是由人类问题编写者创建的。该数据集分为7,400个训练问题和1,300个测试问题。这些问题需要2到8步来解决，解决方案涉及使用基本算术运算执行一系列基本计算以得到最终答案。这个基准来自Cobbe等人在OpenAI的研究。
- en: One of the goals of this benchmark was to help facilitate research for model
    creators and allow them to measure the performance of their models using different
    approaches for this multi-step type of mathematical problems which even high-parameter
    count modern transformer-based models have difficulty solving.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 该基准的一个目标是为了帮助模型创建者促进研究，并允许他们使用不同的方法来衡量他们在这种多步骤数学问题上的模型性能，即使是高参数的现代基于transformer的模型也难以解决。
- en: 6.3.6.6 Code Generation Benchmarks
  id: totrans-407
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.6.6 代码生成基准
- en: 6.3.6.6.1 HumanEval ([Chen et al., 2021](#ref6_11))
  id: totrans-408
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.3.6.6.1 HumanEval ([陈等，2021](#ref6_11))
- en: The HumanEval benchmark, introduced by OpenAI in a paper by [Chen et al., 2021](#ref6_11),
    is designed to evaluate the problem-solving abilities of language models. The
    benchmark consists of a dataset of tasks (approx. 160), where each task is a function
    problem written in Python that the model has to solve by predicting the function’s
    output based on given inputs. The problems are designed to require mathematical,
    logical, or other forms of common-sense reasoning.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: HumanEval基准由OpenAI在陈等人的论文中引入，旨在评估语言模型的问题解决能力。基准包括一个任务数据集（约160个），其中每个任务都是一个用Python编写的函数问题，模型必须通过根据给定的输入预测函数的输出来解决。这些问题被设计成需要数学、逻辑或其他形式的常识推理。
- en: In HumanEval, the tasks are formulated in a way that they are easy for humans
    to solve but are challenging for machine models, aiming to bridge the gap between
    human and machine problem-solving capabilities. The tasks are not constrained
    to a particular domain or type and can span a range of topics and difficulty levels.
    They can encompass various types of problems, like mathematical calculations,
    string manipulations, or logic-based puzzles.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 在HumanEval中，任务被制定成对人类来说容易解决但对机器模型具有挑战性的形式，旨在弥合人类与机器问题解决能力之间的差距。任务不受特定领域或类型的限制，可以涵盖一系列主题和难度级别。它们可以包括各种类型的问题，如数学计算、字符串操作或基于逻辑的谜题。
- en: 'Here is an example task from HumanEval:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个来自HumanEval的示例任务：
- en: 'A model is supposed to generate Python code that solves the described problem
    in the comment. In this case, it might generate something like:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 模型应该生成Python代码来解决注释中描述的问题。在这种情况下，它可能生成如下内容：
- en: The main goal of the HumanEval benchmark is to push forward the capabilities
    of models in terms of problem solving and reasoning. The benchmark can be used
    to evaluate different models and to understand how well they can understand and
    generate correct and efficient code to solve a given problem.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: HumanEval基准的主要目标是推动模型在问题解决和推理方面的能力。该基准可用于评估不同的模型，并了解它们理解和生成正确且高效的代码来解决给定问题的能力。
- en: 6.3.6.6.2 MBPP ([Austin et al., 2021](#ref6_5))
  id: totrans-414
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.3.6.6.2 MBPP ([奥斯汀等，2021](#ref6_5))
- en: The MBPP (Mostly Basic Programming Problems) benchmark consists of about 970
    Python programming problems, designed to be solvable by beginner-level programmers,
    covering programming fundamentals, standard library functionality, etc. Each problem
    consists of a task description, code solution and 3 automated test cases.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: MBPP（大多数基本编程问题）基准包括大约970个Python编程问题，旨在由初级程序员解决，涵盖了编程基础、标准库功能等。每个问题包括任务描述、代码解决方案和3个自动测试案例。
- en: 6.3.6.7 Multi-task Language Understanding Benchmarks
  id: totrans-416
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.6.7 多任务语言理解基准
- en: 6.3.6.7.1 GLUE ([Wang et al., 2018](#ref6_77))
  id: totrans-417
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.3.6.7.1 GLUE ([王等，2018](#ref6_77))
- en: GLUE (General Language Understanding Evaluation) consists of a collection of
    nine NLU (Natural Language Understanding) tasks, covering a variety of linguistic
    phenomena and domains. The number of training examples, verification examples,
    and testing examples for each of the following varies considerably. The approximate
    numbers shown give some guidance.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: GLUE（通用语言理解评估）包括九个NLU（自然语言理解）任务，涵盖了各种语言现象和领域。以下各项的训练示例、验证示例和测试示例数量差异很大。显示的近似数字提供了一些指导。
- en: 'Here are the tasks included in GLUE with some examples to help understand the
    concept being tested:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是GLUE中包含的任务以及一些示例，以帮助理解所测试的概念：
- en: 'MultiNLI (Multi-Genre Natural Language Inference): Assessing whether a hypothesis
    is entailed, contradicted, or neither by a given premise (391,000 training, 19,000
    testing, 19,000 validation).'
  id: totrans-420
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: MultiNLI（多体裁自然语言推理）：评估给定前提是否被假设所蕴含、矛盾或既不是蕴含也不是矛盾（训练：391,000，测试：19,000，验证：19,000）。
- en: 'Premise: “The orchestra is playing a beautiful symphony.”'
  id: totrans-421
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前提：“乐团正在演奏一首美丽的交响曲。”
- en: 'Hypothesis: “There is a musical performance by the orchestra.”'
  id: totrans-422
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设：“乐团将有一场音乐表演。”
- en: 'Label: Entailment'
  id: totrans-423
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标签：蕴涵
- en: 'QQP (Quora Question Pairs): Identifying duplicate questions (363,000 training,
    390,000 testing, 40,000 validation).'
  id: totrans-424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: QQP（Quora问题对）：识别重复问题（363,000个训练样本，390,000个测试样本，40,000个验证样本）。
- en: 'Question1: “How can I improve my credit score?”'
  id: totrans-425
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题1：“我如何提高我的信用评分？”
- en: 'Question2: “What steps can I take to boost my credit rating?”'
  id: totrans-426
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题2：“我可以采取哪些步骤来提高我的信用评级？”
- en: 'Label: Duplicate'
  id: totrans-427
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标签：重复
- en: 'QNLI (Question Natural Language Inference): Identifying answer sentences for
    a given question (103,000 training, 5,000 testing, 5,000 validation).'
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: QNLI（问题自然语言推理）：为给定问题识别答案句子（103,000个训练样本，5,000个测试样本，5,000个验证样本）。
- en: 'Question: “What is the capital of France?”'
  id: totrans-429
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：“法国的首都是什么？”
- en: 'Sentence: “Paris is the capital of France.”'
  id: totrans-430
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 句子：“巴黎是法国的首都。”
- en: 'Label: Entailment'
  id: totrans-431
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标签：蕴涵
- en: 'SST-2 (Stanford Sentiment Treebank): Binary sentiment classification (67,000
    training, 1,800 testing, 800 validation).'
  id: totrans-432
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SST-2（斯坦福情感树库）：二元情感分类（67,000个训练样本，1,800个测试样本，800个验证样本）。
- en: 'Sentence: “The storyline was dull and unexciting.”'
  id: totrans-433
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 句子：“故事情节平淡无奇。”
- en: 'Label: Negative'
  id: totrans-434
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标签：负面
- en: 'Sentence: “The movie was fantastic with a gripping plot.”'
  id: totrans-435
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 句子：“这部电影情节扣人心弦，非常精彩。”
- en: 'Label: Positive'
  id: totrans-436
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标签：正面
- en: 'CoLA (Corpus of Linguistic Acceptability): Grammaticality judgment (8,000 training,
    1,000 testing, 1,000 validation).'
  id: totrans-437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CoLA（语言可接受性语料库）：语法性判断（8,000个训练样本，1,000个测试样本，1,000个验证样本）。
- en: 'Sentence: “The book was put on top of the shelf by John.”'
  id: totrans-438
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 句子：“这本书被约翰放在了书架上。”
- en: 'Label: Acceptable'
  id: totrans-439
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标签：可接受
- en: 'Sentence: “The grass green.”'
  id: totrans-440
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 句子：“草是绿色的。”
- en: 'Label: Unacceptable'
  id: totrans-441
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标签：不可接受
- en: 'STS-B (Semantic Textual Similarity Benchmark): Estimating similarity scores
    for sentence pairs (5,000 training, 1,000 testing, 1,400 validation).'
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: STS-B（语义文本相似度基准）：估计句子对的相似度分数（5,000个训练样本，1,000个测试样本，1,400个验证样本）。
- en: 'Sentence1: “A dog is running in a park.”'
  id: totrans-443
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 句子1：“一只狗在公园里奔跑。”
- en: 'Sentence2: “A dog is sprinting across the park.”'
  id: totrans-444
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 句子2：“一只狗在公园里冲刺。”
- en: 'Similarity Score: High'
  id: totrans-445
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 相似度分数：高
- en: 'MRPC (Microsoft Research Paraphrase Corpus): Identifying paraphrases among
    sentence pairs (4,000 training, 1,700 testing).'
  id: totrans-446
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: MRPC（微软研究释义语料库）：识别句子对中的释义（4,000个训练样本，1,700个测试样本）。
- en: 'Sentence1: “The company reported a significant increase in quarterly revenue.”'
  id: totrans-447
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 句子1：“公司报告了季度收入的显著增长。”
- en: 'Sentence2: “Quarterly revenue saw a significant rise as reported by the company.”'
  id: totrans-448
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 句子2：“公司报告季度收入显著增长。”
- en: 'Label: Equivalent'
  id: totrans-449
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标签：等价
- en: 'RTE (Recognising Textual Entailment): Identifying entailment between pairs
    of text (2,400 training, 2,900 testing, 270 validation).'
  id: totrans-450
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RTE（识别文本蕴涵）：识别文本对之间的蕴涵（2,400个训练样本，2,900个测试样本，270个验证样本）。
- en: 'Sentence1: “No evidence that chemical imbalances cause depression has been
    found.”'
  id: totrans-451
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 句子1：“尚未发现化学失衡导致抑郁的证据。”
- en: 'Sentence2: “Chemical imbalances cause depression.”'
  id: totrans-452
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 句子2：“化学失衡导致抑郁。”
- en: 'Label: Not Entailment'
  id: totrans-453
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标签：非蕴涵
- en: 'WNLI (Winograd NLI): Natural language inference using coreference resolution
    (600 training, 140 testing, 70 validation).'
  id: totrans-454
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: WNLI（Winograd NLI）：使用指称消解进行自然语言推理（600个训练样本，140个测试样本，70个验证样本）。
- en: 'Sentence1: “The keys were locked inside the car.”'
  id: totrans-455
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 句子1：“钥匙被锁在车里了。”
- en: 'Sentence2: “The car had the keys locked inside.”'
  id: totrans-456
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 句子2：“车钥匙被锁在车里。”
- en: 'Label: Entailment'
  id: totrans-457
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标签：蕴涵
- en: 6.3.6.7.2 SuperGLUE ([Wang et al., 2019](#ref6_76))
  id: totrans-458
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.3.6.7.2 SuperGLUE ([王等人，2019](#ref6_76))
- en: 'SuperGLUE was introduced as a more challenging successor to GLUE, and it consists
    of a new set of more difficult language understanding tasks. Here are the tasks
    included in SuperGLUE:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: SuperGLUE被引入作为比GLUE更具挑战性的后续者，它包括一组更困难的语言理解任务。以下是SuperGLUE包含的任务：
- en: 'BoolQ (Boolean Questions): Answering yes/no questions. The details of BoolQ
    are provided in a separate section as it is its own benchmark.'
  id: totrans-460
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BoolQ（布尔问题）：回答是/否问题。BoolQ的详细信息在单独的部分提供，因为它是一个自己的基准。
- en: 'CB (CommitmentBank): Identifying entailment relationships involving human commitments
    (250 training, 250 testing, 50 validation).'
  id: totrans-461
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CB（承诺银行）：识别涉及人类承诺的蕴涵关系（250个训练样本，250个测试样本，50个验证样本）。
- en: 'Premise: “I can’t help you move next weekend.”'
  id: totrans-462
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前提：“我无法在下个周末帮你搬家。”
- en: 'Hypothesis: “The speaker is not available to help with moving next weekend.”'
  id: totrans-463
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设：“下个周末演讲者无法帮忙搬家。”
- en: 'Label: Entailment'
  id: totrans-464
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标签：蕴涵
- en: 'COPA (Choice of Plausible Alternatives): Identifying causes or effects in given
    situations (400 training, 500 testing, and 100 validation).'
  id: totrans-465
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: COPA（合理替代选择）：在给定情境中识别原因或结果（400个训练样本，500个测试样本，100个验证样本）。
- en: 'Question: “What was the effect?”'
  id: totrans-466
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：“效果是什么？”
- en: 'Sentence: “He didn’t study, so he failed the exam.”'
  id: totrans-467
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 句子：“他没有学习，所以考试不及格。”
- en: 'Choices: 1) He didn’t study. 2) He failed the exam.'
  id: totrans-468
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选项：1) 他没有学习。2) 他考试不及格。
- en: 'Answer: He failed the exam.'
  id: totrans-469
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：他考试不及格。
- en: 'MultiRC (Multi-Sentence Reading Comprehension): Answering questions with multiple
    possible answers (450 training, 150 testing, 80 validation).'
  id: totrans-470
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: MultiRC（多句子阅读理解）：回答有多个可能答案的问题（训练集450个样本，测试集150个样本，验证集80个样本）。
- en: 'Question: “What happened to the cat?”'
  id: totrans-471
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：“猫发生了什么事？”
- en: 'Passage: “The cat climbed up the tree and couldn’t come down.”'
  id: totrans-472
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文段：“猫爬上了树，下不来了。”
- en: 'Answer: “Climbed up the tree, couldn’t come down.”'
  id: totrans-473
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：“爬上了树，下不来了。”
- en: 'ReCoRD (Reading Comprehension with Common-sense Reasoning Dataset): Reading
    comprehension that involves common-sense reasoning (65,000 training, 7,400 testing,
    7,400 validation).'
  id: totrans-474
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ReCoRD（基于常识推理的阅读理解数据集）：涉及常识推理的阅读理解（训练集65,000个样本，测试集7,400个样本，验证集7,400个样本）。
- en: 'Question: “Who bought flowers?”'
  id: totrans-475
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：“谁买了花？”
- en: 'Passage: “George went to the store and bought some flowers.”'
  id: totrans-476
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文段：“乔治去了商店并买了一些花。”
- en: 'Answer: George'
  id: totrans-477
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：乔治
- en: 'Question: “Why couldn’t the cat come down?”'
  id: totrans-478
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：“为什么猫下不来？”
- en: 'Passage: “The cat climbed up the tree and couldn’t come down.”'
  id: totrans-479
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文段：“猫爬上了树，下不来了。”
- en: 'Answer: It’s not stated.'
  id: totrans-480
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：未说明。
- en: 'RTE (Recognising Textual Entailment): Also included in GLUE, but reused here
    (2,400 training, 3000 testing, 270 validation).'
  id: totrans-481
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RTE（文本蕴涵识别）：也包含在GLUE中，但在此处重复使用（训练集2,400个样本，测试集3,000个样本，验证集270个样本）。
- en: 'Sentence1: “The sun rises in the east.”'
  id: totrans-482
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 句子1：“太阳从东方升起。”
- en: 'Sentence2: “The sun sets in the west.”'
  id: totrans-483
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 句子2：“太阳在西方落下。”
- en: 'Label: Not Entailment'
  id: totrans-484
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标签：非蕴涵
- en: 'WiC (Word-in-Context): Determining whether a word is used with the same sense
    in two sentences (5,400 training, 1,400 testing, 630 validation).'
  id: totrans-485
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: WiC（上下文中的词）：确定一个词在两个句子中是否以相同的意义使用（训练集5,400个样本，测试集1,400个样本，验证集630个样本）。
- en: 'Word: “rock”'
  id: totrans-486
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单词：“rock”
- en: 'Sentence1: “He collects rocks.”'
  id: totrans-487
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 句子1：“他收集石头。”
- en: 'Sentence2: “He’s my rock.”'
  id: totrans-488
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 句子2：“他是我的依靠。”
- en: 'Label: Different'
  id: totrans-489
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标签：不同
- en: 'WSC (Winograd Schema Challenge): A coreference resolution task, similar to
    WNLI in GLUE (550 training, 140 testing, 100 testing).'
  id: totrans-490
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: WSC（Winograd 框架挑战）：一个指代消解任务，类似于GLUE中的WNLI（训练集550个样本，测试集140个样本，验证集100个样本）。
- en: 'Sentence: “The man who hunts ducks out on weekends.”'
  id: totrans-491
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 句子：“那个周末打猎鸭子的男人。”
- en: 'Question: “Who hunts ducks?”'
  id: totrans-492
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：“谁打猎鸭子？”
- en: 'Answer: The man'
  id: totrans-493
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：那个人
- en: 6.3.6.7.3 MMLU ([Hendrycks et al., 2020](#ref6_25))
  id: totrans-494
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.3.6.7.3 MMLU ([Hendrycks 等人，2020](#ref6_25))
- en: The MMLU (Massive Multitask Language Understanding) benchmark, introduced by
    Hendrycks in 2020, encompasses 57 tasks that span various domains, including elementary
    mathematics, US history, computer science, law, and more. This benchmark was established
    in response to recent advancements in LLMs, which were achieving human-level or
    even surpassing human-level performance on earlier benchmarks like GLUE and SuperGLUE.
    The difficulty of the tasks in MMLU ranges from elementary to advanced professional
    levels, testing both knowledge and problem-solving abilities. It can be used in
    zero-shot and few-shot settings when evaluating models.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: MMLU（大规模多任务语言理解）基准，由Hendrycks于2020年提出，包含57个任务，涵盖多个领域，包括基础数学、美国历史、计算机科学、法律等。该基准是为了应对LLMs在早期基准如GLUE和SuperGLUE上取得人类水平甚至超越人类水平的性能而建立的。MMLU中的任务难度从基础到高级专业水平不等，测试知识和解决问题的能力。在评估模型时，可用于零样本和少样本设置。
- en: 6.3.6.8 Toxicity Benchmarks
  id: totrans-496
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.6.8 毒性基准
- en: 6.3.6.8.1 RealToxicityPrompts ([Gehman et al., 2020](#ref6_21))
  id: totrans-497
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.3.6.8.1 RealToxicityPrompts ([Gehman 等人，2020](#ref6_21))
- en: RealToxicityPrompts, introduced by [Gehman et al. (2020)](#ref6_21), is a benchmark
    designed to evaluate the risk and tendencies of language models, particularly
    GPT-3 and similar models, to generate unsafe or toxic outputs. This benchmark
    comprises a dataset that probes the unsafe language generation tendencies of models
    in response to various prompts.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: RealToxicityPrompts，由Gehman等人（2020）提出，是一个旨在评估语言模型，尤其是GPT-3及其类似模型，生成不安全或有毒输出的风险和倾向的基准。该基准包含一个数据集，用于探测模型在响应各种提示时的不安全语言生成倾向。
- en: RealToxicityPrompts seeks to elucidate the extent to which pre-trained language
    models, like GPT-3, might produce toxic, offensive, or otherwise undesirable outputs
    when given different types of prompts.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: RealToxicityPrompts旨在阐明预训练语言模型，如GPT-3，在接收到不同类型的提示时可能产生有毒、冒犯性或其它不希望输出的程度。
- en: The dataset is made up of prompts designed to elicit responses from the models.
    Its objective is to ascertain how frequently and in what situations these models
    produce answers that might be deemed harmful, offensive, or toxic.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集由旨在从模型中诱出响应的提示组成。其目标是确定这些模型在何种频率和情境下产生可能被视为有害、冒犯性或有毒的答案。
- en: A toxicity classifier evaluates the model-generated responses for potential
    toxicity, helping to quantify the likelihood of an output being perceived as toxic
    or harmful by users.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 毒性分类器评估模型生成的响应的潜在毒性，有助于量化输出被用户感知为有毒或有害的可能性。
- en: By shedding light on the propensities and risks associated with automated language
    generation, developers and researchers can more effectively devise safeguards
    and countermeasures to curtail the dissemination of detrimental content.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 通过揭示与自动化语言生成相关的倾向和风险，开发人员和研究人员可以更有效地制定安全措施和对策，以限制有害内容的传播。
- en: It should be noted that evaluating toxicity is a complex process. The benchmark
    itself must employ a machine learning model, trained specifically for this purpose,
    to measure the toxicity of a response. This is in contrast to simpler cases, such
    as verifying answers to multiple-choice questions.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 应该注意的是，评估毒性是一个复杂的过程。基准测试本身必须使用专门为此目的训练的机器学习模型来衡量响应的毒性。这与简单的案例形成对比，例如验证多项选择题的答案。
- en: A crucial consideration when addressing toxicity in language models is the potential
    for inherent biases within the models. These biases can manifest in the outputs.
    The benchmark seeks to illuminate these issues, guiding efforts to develop more
    impartial and unbiased models.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理语言模型中的毒性问题时，一个重要的考虑因素是模型内部可能存在的固有偏差。这些偏差可能体现在输出中。基准测试旨在阐明这些问题，指导努力开发更公正和无偏见的模型。
- en: 6.3.6.9 Biases Benchmarks
  id: totrans-505
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.6.9 偏差基准
- en: 6.3.6.9.1 CrowS-Pairs ([Nangia et al., 2020](#ref6_46))
  id: totrans-506
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.3.6.9.1 CrowS-Pairs ([Nangia 等人，2020](#ref6_46))
- en: The CrowS-Pairs (Crowdsourced Stereotype Pairs) benchmark, introduced by Nangia
    et al. in 2020, is designed to examine the biases present in a language model.
    Instead of merely measuring a model’s performance, it aims to understand and highlight
    the model’s stereotyping and biased tendencies, particularly in nuanced and non-explicit
    contexts.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 由 Nangia 等人在 2020 年引入的 CrowS-Pairs（众包刻板印象对）基准测试旨在检查语言模型中存在的偏差。它不仅测量模型的性能，而是旨在理解和突出模型在细微和非明确情境中的刻板印象和偏差倾向。
- en: This benchmark focuses on identifying socio-cultural and demographic biases
    in models. It assesses models on various dimensions of bias, such as race, religion,
    and gender, among others. CrowS-Pairs comprises pairs of sentences crafted to
    contrast non-stereotypical and stereotypical scenarios. Human annotators were
    engaged to develop and validate these pairs, ensuring they encapsulate a wide
    and subtle range of biases.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 该基准测试专注于识别模型中的社会文化人口偏差。它评估模型在诸如种族、宗教和性别等其他维度的偏差。CrowS-Pairs 包含了一系列句子对，旨在对比非刻板和刻板场景。为了确保这些句子对涵盖了广泛且细微的偏差范围，人类标注员参与了这些句子对的开发和验证。
- en: Models are evaluated on their responses to these sentence pairs, from which
    a bias score is derived, offering a quantitative measure of the model’s bias.
    The benchmark gives insights into both the nature and extent of bias across different
    dimensions. It captures not only explicit but also implicit and subtle biases,
    which might otherwise be overlooked.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在其对这些句子对的响应上进行评估，从而得出一个偏差分数，为模型偏差提供了一个量化的度量。基准测试提供了关于不同维度上偏差的性质和程度的见解。它不仅捕捉到显性偏差，还包括隐性和细微的偏差，这些偏差否则可能会被忽视。
- en: Here, Sentence A might contain a gendered stereotype linking mothers to responsibility,
    while Sentence B offers a non-stereotypical counterpart. Models are then evaluated
    based on how they interpret and assess these pairs, providing insights into their
    biases.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，句子 A 可能包含将母亲与责任联系起来的性别刻板印象，而句子 B 则提供非刻板印象的对应物。然后根据模型如何解释和评估这些对进行评估，从而提供关于其偏差的见解。
- en: These two sentences would be presented to the model, which would be asked to
    provide a score or assess the plausibility of each sentence. The benchmark would
    then evaluate the model’s response, knowing which sentence is biased and which
    is not.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个句子将被呈现给模型，模型将被要求为每个句子提供分数或评估其可信度。然后基准测试将评估模型的响应，了解哪个句子是有偏差的，哪个是没有偏差的。
- en: CrowS-Pairs is a vital tool in the domain of ethical AI and bias analysis. It
    supports the development of fairer, more impartial language models by highlighting
    their inherent biases.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: CrowS-Pairs 是伦理 AI 和偏见分析领域中的一个重要工具。它通过突出模型固有的偏见，支持开发更公平、更公正的语言模型。
- en: 6.3.6.9.2 WinoGender benchmark ([Rudinger et al., 2018](#ref6_59))
  id: totrans-513
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.3.6.9.2 WinoGender 基准 ([Rudinger et al., 2018](#ref6_59))
- en: The WinoGender benchmark, introduced by Rudinger et al. in 2018, is specifically
    designed to evaluate the gender bias in coreference resolution systems. Coreference
    resolution is a task in natural language processing (NLP) that involves determining
    when two or more words (or phrases) in a text refer to the same entity.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: WinoGender 基准是由 Rudinger 等人在 2018 年提出的，专门设计用来评估指代消解系统中的性别偏见。指代消解是自然语言处理（NLP）中的一个任务，涉及确定文本中两个或多个单词（或短语）是否指代同一实体。
- en: The WinoGender dataset includes sentences designed around an anaphoric pronoun
    (he, she, his, hers, etc.) that is linked to one of two potential referents in
    the sentence. Importantly, one of the referents is stereotypically associated
    with the pronoun, while the other is not. The purpose of this design is to explore
    whether models are more likely to link pronouns to stereotypically associated
    referents, thus revealing potential gender biases in their predictions.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: WinoGender 数据集包括围绕代词（他、她、他的、她的等）设计的句子，该代词与句子中的两个潜在指代对象之一相关联。重要的是，其中一个指代对象与代词存在刻板印象的关联，而另一个则没有。这种设计的目的是探索模型是否更有可能将代词与刻板印象相关的指代对象联系起来，从而揭示其预测中可能存在的性别偏见。
- en: In a stereotype-conforming context, a model might be prone to associating “she”
    with “nurse” and “he” with “surgeon” due to prevalent gender stereotypes. The
    benchmark would evaluate whether the model makes such stereotypical associations
    consistently across various scenarios.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 在符合刻板印象的情境中，由于普遍存在的性别刻板印象，一个模型可能会倾向于将“她”与“护士”以及“他”与“外科医生”联系起来。基准测试将评估模型是否在各种场景中持续地做出这种刻板印象的关联。
- en: In addition to revealing biases in model predictions, the WinoGender benchmark
    also underscores the challenges that such biases pose to achieving accurate and
    fair coreference resolution. Model developers and researchers use benchmarks like
    WinoGender to assess and subsequently mitigate the biases present in their models,
    aiming for more equitable and accurate performance across different contexts and
    demographic groups.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 除了揭示模型预测中的偏见外，WinoGender 基准还强调了这些偏见在实现准确和公平的指代消解中带来的挑战。模型开发者和研究人员使用 WinoGender
    等基准来评估并随后减轻模型中存在的偏见，旨在在不同情境和人口群体中获得更公平和准确的表现。
- en: 6.3.6.10 Truthfulness Benchmarks
  id: totrans-518
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.6.10 真实性基准
- en: 6.3.6.10.1 TruthfulQA ([Lin et al., 2022](#ref6_39))
  id: totrans-519
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.3.6.10.1 TruthfulQA ([Lin et al., 2022](#ref6_39))
- en: The TruthfulQA benchmark, introduced by Lin et al. in 2022, is designed to examine
    the reliability and veracity of responses generated by LLMs to open-domain questions.
    The fundamental goal is to scrutinise how well these LLMs provide truthful and
    accurate answers across a wide range of topics and questions.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: TruthfulQA 基准是由 Lin 等人在 2022 年提出的，旨在检查 LLM 对开放域问题的回答的可靠性和真实性。基本目标是审查这些 LLM 在广泛的主题和问题中提供真实和准确答案的能力。
- en: The need for a benchmark like TruthfulQA stems from observations that, while
    models like GPT-3 can generate fluent and contextually appropriate responses,
    they can sometimes generate answers that are incorrect, misleading, or fabricated.
    Ensuring the reliability of information provided by LLMs is crucial, especially
    as they become more integrated into informational and decision-making tools.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 TruthfulQA 这样的基准的需求源于观察到的现象，尽管像 GPT-3 这样的模型可以生成流畅且符合上下文语境的回答，但它们有时会生成不正确、误导性或虚构的答案。确保大型语言模型（LLM）提供信息的可靠性至关重要，尤其是在它们越来越多地集成到信息和决策工具中时。
- en: 'Approach:'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 方法：
- en: '**Questions**: The dataset includes a variety of questions that are designed
    to probe the model’s ability to provide accurate and reliable answers. These questions
    could span a wide array of topics, including history, science, and general knowledge.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问题**：数据集包括各种设计用来探测模型提供准确和可靠答案能力的问题。这些问题可能涉及历史、科学和一般知识等广泛的主题。'
- en: '**Model Responses**: The LLMs generate responses to the provided questions.
    The aim is to evaluate the correctness and reliability of these responses.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型回答**：LLM 生成对提供问题的回答。目标是评估这些回答的正确性和可靠性。'
- en: '**Evaluation**: Human evaluators or an automated system will assess the model-generated
    responses for their accuracy and truthfulness, comparing them to verified information
    or predefined answer keys.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估**：人类评估者或自动化系统将评估模型生成的响应的准确性和真实性，将其与验证信息或预定义的答案键进行比较。'
- en: TruthfulQA serves as a crucial tool in gauging how LLMs handle the provision
    of factual information, which is essential for ensuring that these models can
    be trusted sources of information in various applications, such as conversational
    agents, informational retrieval systems, and more.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: TruthfulQA作为衡量LLM处理提供事实信息能力的关键工具，这对于确保这些模型可以作为各种应用（如对话代理、信息检索系统等）中可信赖的信息来源至关重要。
- en: 6.3.7 The Fine-Tuning
  id: totrans-527
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.7 微调
- en: Fine-tuning is a technique in machine learning where a pre-trained model is
    further trained (typically on a smaller dataset) to adapt its existing knowledge
    to a new task. The model has already learned various features or patterns from
    a larger dataset and can utilise this knowledge to perform well on a related task
    with less data. Below are descriptions and examples for both a language model
    (LLM) and an image generation model.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 微调是机器学习中的一种技术，其中预训练模型进一步训练（通常是在较小的数据集上），以适应其现有知识以适应新任务。该模型已经从更大的数据集中学习到各种特征或模式，并可以利用这些知识在相关任务上以较少的数据表现良好。以下是对语言模型（LLM）和图像生成模型的描述和示例。
- en: 6.3.7.1 Fine-Tuning in Language Models (LLM)
  id: totrans-529
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.7.1 语言模型（LLM）中的微调
- en: After a model like GPT-3 has been pre-trained on a vast corpus of text, it has
    accumulated a wide range of linguistic knowledge. Fine-tuning involves training
    it further on a smaller, domain-specific dataset to specialise its capabilities
    towards certain tasks or industries.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPT-3等模型在大量文本语料库上预训练后，它积累了广泛的语文学识。微调涉及在更小、特定领域的数据集上进一步训练它，以专门化其能力，针对某些任务或行业。
- en: Imagine you have an LLM trained on general text, and now you want to fine-tune
    it for legal advice. The LLM has been trained on a massive corpus and understands
    a wide array of English text. You introduce the LLM to a smaller dataset consisting
    of legal documents, court rulings, and attorney correspondences. The model adapts
    its generalised knowledge to become proficient in understanding and generating
    legal text. The fine-tuned LLM can now generate more contextually and terminologically
    accurate responses to legal queries or assist in drafting legal documents.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个在通用文本上训练的LLM，现在你想对其进行法律建议的微调。该LLM已经在庞大的语料库上训练，并理解了广泛的英语文本。你引入LLM到一个包含法律文件、法院裁决和律师通信的小型数据集。该模型将其泛化知识调整为精通理解和生成法律文本。微调后的LLM现在可以生成更符合语境和术语准确的针对法律查询的响应，或在起草法律文件时提供帮助。
- en: 6.3.7.2 Fine-Tuning in Image Generation Models
  id: totrans-532
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.7.2 图像生成模型中的微调
- en: An image generation model pre-trained on a large dataset has learned to generate
    images by understanding various visual patterns, structures, and contexts from
    the training data. Fine-tuning involves further training the model on a smaller,
    specific dataset to enhance its capability in generating images related to a specific
    domain or characteristic.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型数据集上预训练的图像生成模型已经学会了通过理解训练数据中的各种视觉模式、结构和上下文来生成图像。微调涉及在更小、更具体的数据集上进一步训练模型，以增强其在生成特定领域或特征相关图像方面的能力。
- en: Consider a generative adversarial network (GAN) that has been trained on a wide
    variety of images (e.g., faces, animals, objects). The GAN knows how to generate
    a broad spectrum of images by understanding general patterns, colours, shapes,
    and textures found in the training data. Now, suppose you want to generate images
    of birds. You fine-tune the GAN using a smaller dataset consisting exclusively
    of bird images. The model learns the specific visual characteristics related to
    different bird species. The fine-tuned model can now generate varied and contextually
    relevant images of birds, considering specific aspects like plumage, beak shape,
    and size more accurately.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个在广泛图像（例如，面孔、动物、物体）上训练的生成对抗网络（GAN）。GAN知道如何通过理解训练数据中发现的通用模式、颜色、形状和纹理来生成广泛的图像。现在，假设你想生成鸟类图像。你使用仅包含鸟类图像的较小数据集对GAN进行微调。该模型学习与不同鸟种相关的特定视觉特征。微调后的模型现在可以生成各种与鸟类相关的图像，更准确地考虑羽毛、喙形状和大小等特定方面。
- en: 6.3.7.3 Steps for Fine-tuning
  id: totrans-535
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.7.3 微调步骤
- en: These steps follow the same steps as described above for the foundational model,
    but with specific datasets and testing focused on the fine-tuning specifics. In
    the case of the model design and structuring, the model’s architecture can be
    adjusted to be suitable for the new task (e.g., changing the output layer for
    classification tasks). Typically, the specific hyperparameters of the model will
    be adjusted to help ensure the best results from the fine-tuning e.g., use of
    a smaller learning rate to avoid forgetting the previously learned features and
    gently adapt the model to the new task.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤遵循上述基础模型描述的相同步骤，但具有特定的数据集和测试，专注于微调的特定方面。在模型设计和结构方面，可以根据新任务调整模型的架构以适应（例如，为分类任务更改输出层）。通常，模型的特定超参数将进行调整，以确保微调的最佳结果，例如，使用较小的学习率以避免忘记之前学习到的特征，并温和地调整模型以适应新任务。
- en: Fine-tuning allows leveraging the extensive knowledge captured during pre-training
    to achieve better performance on related tasks even when less training data is
    available for them. This methodology has proven effective across various domains
    and tasks in machine learning.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 微调允许利用预训练期间捕获的广泛知识，在相关任务上即使可用训练数据较少也能实现更好的性能。这种方法在机器学习的各个领域和任务中已被证明是有效的。
- en: 6.3.8 The Deployment
  id: totrans-538
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.8 部署
- en: We have explored the intricacies involved in creating a model. However, before
    a model, especially ones such as GPT-3/GPT-4 or LLaMA-2, is prepared for widespread
    use, several steps must be undertaken to ready the model for large-scale production.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经探讨了创建模型所涉及的复杂性。然而，在模型，尤其是像GPT-3/GPT-4或LLaMA-2这样的模型准备用于广泛使用之前，必须采取几个步骤来为大规模生产准备模型。
- en: 'This may involve some or all of the following:'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能包括以下的一些或所有步骤：
- en: '**Model Finalisation:** This stage involves selecting the specific model for
    deployment to an environment for general usage by users, typically called the
    production environment or just production. Often, companies will develop multiple
    models, each subjected to various customisations during training and testing.
    Subsequently, a decision must be made regarding which model to advance to production.
    During this stage, any final testing necessary is conducted (elaborated upon later)
    and actions such as optimising the model’s hyperparameters are taken.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型最终确定**：这一阶段涉及选择特定的模型部署到环境，以便用户进行通用使用，通常称为生产环境或简称生产。通常，公司会开发多个模型，每个模型在训练和测试过程中都会进行各种定制。随后，必须做出决定，确定哪个模型可以推进到生产阶段。在这一阶段，将进行任何必要的最终测试（将在后面详细说明），并采取诸如优化模型超参数等行动。'
- en: '**Model Optimisation:** This is a crucial step in which resource usage, including
    CPU and memory, is optimised for production purposes, specifically for inference.
    Training a model and deploying it for mass use entail distinct scenarios, and
    when the model is serving user queries, it must be tailored for that specific
    context. One effective action that model creators can take is **model pruning**.
    This involves scrutinising the model to identify components that can be removed
    to simplify it, without compromising its performance capabilities, while simultaneously
    reducing the required computational resources, including memory and CPU. At times,
    teams may conduct an **ablation study**, which systematically removes parts of
    the model to assess their impact. If such removal leads to a positive resource
    impact without compromising performance, they may decide to exclude that component
    from the production version. Various other techniques can be less invasive, including
    **thresholding** (pruning items below a specified threshold), **activation analysis**
    (removing items with minimal impact on output), **sensitivity analysis** (pruning
    items with limited contribution to output), and **redundancy analysis** (eliminating
    redundant components, such as superfluous layers). Naturally, after implementing
    these modifications, it is imperative to rigorously test the model to ensure it
    continues to perform effectively. Another important optimisation strategy often
    employed is **quantisation**, a process that reduces the precision of the model
    weights to lower memory requirements and accelerate inference. This approach is
    commonly utilised when running models on personal computers or mobile devices.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型优化:** 这是一个关键步骤，其中资源使用（包括CPU和内存）被优化以用于生产目的，特别是用于推理。训练模型并将其部署以供大规模使用涉及不同的场景，当模型在处理用户查询时，它必须针对该特定上下文进行调整。模型创建者可以采取的一个有效行动是**模型剪枝**。这涉及到仔细审查模型以识别可以移除以简化模型而不会损害其性能能力的组件，同时减少所需的计算资源，包括内存和CPU。有时，团队可能会进行**消融研究**，系统地移除模型的某些部分以评估其影响。如果这种移除导致对资源的积极影响而不损害性能，他们可能会决定将该组件排除在生产版本之外。还有其他一些技术可能不那么侵入性，包括**阈值剪枝**（移除低于指定阈值的项）、**激活分析**（移除对输出影响最小的项）、**敏感性分析**（移除对输出贡献有限的项）和**冗余分析**（消除冗余组件，如多余的层）。当然，在实施这些修改后，严格测试模型以确保其继续有效运行是至关重要的。另一个经常采用的重要优化策略是**量化**，这是一个降低模型权重精度的过程，以降低内存需求并加速推理。这种方法在在个人电脑或移动设备上运行模型时通常被采用。'
- en: '**Model Conversion:** This stage involves the conversion of the model into
    a format suitable for deployment, such as Open Neural Network Exchange (ONNX),
    TensorFlow SavedModel, or PyTorch. Some models may be distributed as installable
    scripts, as is the case with GPT4All, while others may be packaged as container-ready
    deployments using technologies like Docker for Kubernetes, as exemplified by Mistral.
    The selection of the model distribution format holds significance, as it directly
    impacts the accessibility and ease of use of the model, thereby influencing the
    level of interest from the community. In situations where the model is exclusively
    Intended for internal use and is only exposed through a user interface (e.g.,
    a chatbot) or an API (as described below), these model details may remain concealed
    from external parties not affiliated with the organisation that created the model,
    such as ChatGPT3.5 or ChatGPT4.0.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型转换:** 这个阶段涉及将模型转换为适合部署的格式，例如Open Neural Network Exchange (ONNX)、TensorFlow
    SavedModel或PyTorch。一些模型可能以可安装的脚本形式分发，例如GPT4All，而其他模型可能被打包为使用Docker for Kubernetes等技术准备好的容器化部署，例如Mistral。模型分发格式的选择具有重要意义，因为它直接影响模型的可访问性和易用性，从而影响社区的兴趣水平。在模型仅打算用于内部使用，并且仅通过用户界面（例如，聊天机器人）或API（如下文所述）公开的情况下，这些模型细节可能对外部方（例如，ChatGPT3.5或ChatGPT4.0）保持隐藏，这些外部方与创建该模型的组织无关。'
- en: '**Additional Testing:** In most cases, a comprehensive array of specialised
    tests is conducted, encompassing aspects such as inference speed, resource consumption,
    and scale testing, which involves assessing the model’s performance under varying
    user inference request loads. These evaluations may lead to further refinements
    of the model, including the implementation of optimised caching (in memory storing
    of data for faster access) strategies.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**附加测试：** 在大多数情况下，会进行一系列综合的专业测试，包括推理速度、资源消耗和规模测试，这涉及到评估模型在不同用户推理请求负载下的性能。这些评估可能导致对模型的进一步优化，包括实施优化的缓存（内存中存储数据以实现更快访问）策略。'
- en: '**API Wrapper:** End users typically interact with the model via a user interface
    such as a browser-based chatbot interface. However, a model may provide an Application
    Programming Interface (API) to allow technically skilled users to interact with
    the model and to build their own end user interfaces into the models. These APIs
    are typically provided using a technology known as a REST API. A more in-depth
    discussion of these elements that wrap the model will be provided later, as they
    constitute key components of the broader ecosystem in which a model operates.
    In the case of the API wrapper, considerations will include determining the methods
    of user authentication and establishing the framework for authorisation procedures.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**API包装器：** 最终用户通常通过用户界面（如基于浏览器的聊天机器人界面）与模型交互。然而，模型可能提供一个应用程序编程接口（API），以便技术熟练的用户与模型交互，并将自己的最终用户界面集成到模型中。这些API通常使用一种称为REST
    API的技术提供。关于这些围绕模型的元素的更深入讨论将在稍后提供，因为它们构成了模型运行的更广泛生态系统中的关键组成部分。在API包装器的案例中，需要考虑确定用户认证的方法和建立授权程序框架。'
- en: '**Monitoring and Logging:** This is a critical part of a model deployment and
    allows the model to be kept healthy by the team responsible for the model in the
    production environment. This monitoring serves multiple purposes, extending beyond
    mere verification of its operational status. It also entails keeping track of
    factors like the number of users and the utilisation of resources such as CPU,
    memory, and network bandwidth, ensuring they remain within predefined thresholds
    and controls. Additionally, effective monitoring plays a pivotal role in identifying
    and rectifying instances of undesirable behaviour, whether exhibited by the model
    itself or by the users interacting with it. If something goes wrong, they can
    use monitoring and logging to identify what and how to fix it for the future.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控和日志记录：** 这是模型部署的关键部分，允许模型在生产环境中由负责该模型的团队保持健康状态。这种监控具有多重目的，而不仅仅是验证其操作状态。它还包括跟踪诸如用户数量和CPU、内存和网络带宽等资源的利用率，确保它们保持在预定义的阈值和控制范围内。此外，有效的监控在识别和纠正模型自身或与模型交互的用户表现出的不良行为方面发挥着关键作用。如果出现问题，他们可以使用监控和日志记录来识别是什么以及如何在未来修复它。'
- en: '**Filtering for Safety and Alignment:** In practice, a model intended for public
    use, such as the GPT-4 model integrated with ChatGPT, typically incorporates various
    features to mitigate the risk of generating inappropriate outputs. For instance,
    input filters can be employed to examine incoming input early in the process,
    thereby identifying and addressing potentially inappropriate content. Similarly,
    in cases where problematic output is detected, output filters can intervene to
    prevent such content from reaching the user. These output filters may substitute
    the inappropriate output with a more suitable response or provide a default message
    conveying the model’s inability to assist.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全和对齐过滤：** 在实践中，旨在公开使用的模型，例如与ChatGPT集成的GPT-4模型，通常包含各种功能来降低生成不适当输出的风险。例如，可以在处理过程的早期阶段使用输入过滤器来检查传入的输入，从而识别和解决可能不适当的内容。同样，在检测到问题输出时，输出过滤器可以介入，防止此类内容到达用户。这些输出过滤器可能用更合适的响应替换不适当的内容，或提供默认消息传达模型无法协助的能力。'
- en: '**Continuous Improvement:** Once the model is available it will require continual
    improvements based on new advances and discoveries from the team that create it
    or from learnings in the runtime environment with real users interacting with
    the model. The model creators need to understand how they plan to release these
    updates, whether they will it be real-time releases that are transparent to users,
    or made available as new versions of a model that the user must explicitly select,
    or perhaps as new specialisations of a model (e.g., trained for some specific
    purposes e.g., chatbot, code generation, specific scientific capabilities, etc.)
    and released as if it was a new model. When users use the model, it is important
    to try and address issues quickly that come up. Sometimes the model may start
    to give biased information or incorrect information; this is known as model hallucination.
    There can be a lot of public pressure on organisations to fix such issues; for
    example Meta had to take down one of their models (called Galactica) only after
    three days due to biased and incorrect information that it provided, and it was
    not possible to fix the model quickly ([Snoswell & Burgess, 2022](#ref6_68)).'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**持续改进：** 一旦模型可用，它将需要根据创建该模型的团队的新进展和发现，或从与模型交互的真实用户在运行时环境中的学习中进行持续改进。模型创建者需要了解他们计划如何发布这些更新，是将其作为对用户透明的实时发布，还是作为用户必须明确选择的模型新版本提供，或者可能是作为模型的新专业版本（例如，为某些特定目的训练，例如聊天机器人、代码生成、特定的科学能力等）并作为新模型发布。当用户使用模型时，重要的是要尽快解决出现的问题。有时模型可能会开始提供有偏见的信息或错误信息；这被称为模型幻觉。组织可能会面临大量公众压力来解决这些问题；例如，Meta在模型（称为Galactica）提供有偏见和错误信息三天后不得不将其下线，而且无法快速修复模型（[Snoswell
    & Burgess, 2022](#ref6_68)）。'
- en: 6.3.9 Model Use (aka Inference)
  id: totrans-549
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.9 模型使用（又称推理）
- en: Typically, models do not undergo learning processes (i.e., updates to their
    weights and biases) during regular runtime interactions. Learning occurs exclusively
    during the pre-training of the foundational model or during the fine-tuning phase.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，模型在常规运行时交互期间不会经历学习过程（即对其权重和偏差的更新）。学习仅在基础模型的预训练期间或微调阶段发生。
- en: However, this can be somewhat perplexing, as there are approaches that might
    appear akin to learning. One such example is the concept of few-shot learning.
    In few-shot learning, similar request-response pairs are presented alongside the
    intended request to assist the model in generating the most appropriate response.
    The terminology, including “zero-shot learning” (where no examples are provided),
    “one-shot learning” (with a single example request-reply pair provided for action),
    and “few-shot learning” (in which multiple request-reply pairs are provided, typically
    ranging from 2 to 5 pairs), can be confounding. This is because the model is not
    actually learning; rather, it is utilising these examples to enhance its comprehension
    of the requests. The model’s parameters remain unaltered. Perhaps these concepts
    would be more accurately described as “few-shot requests” or “few-shot guidance”
    rather than “few-shot learning”.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这可能会有些令人困惑，因为有一些方法可能看起来类似于学习。一个这样的例子是少样本学习的概念。在少样本学习中，与预期请求一起呈现类似请求-响应对，以帮助模型生成最合适的响应。包括“零样本学习”（不提供任何示例）、“单样本学习”（提供一个示例请求-回复对以供操作）和“少样本学习”（提供多个请求-回复对，通常为2到5对）在内的术语可能会令人困惑。这是因为模型实际上并没有学习；相反，它正在利用这些示例来增强其对请求的理解。模型的参数保持不变。也许这些概念更准确地描述为“少样本请求”或“少样本指导”而不是“少样本学习”。
- en: It is worth noting that certain models may compile a database of user requests
    and responses for potential use in training the model in the future or for updates.
    However, as a rule, on-the-fly learning is not commonly employed, as it could
    render the models unstable. This serves as another illustration of the fundamental
    distinction between ANNs and human brains, with the latter continually engaged
    in the process of learning.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，某些模型可能会编译用户请求和响应的数据库，以备将来在训练模型或更新时使用。然而，作为规则，即时学习并不常见，因为它可能导致模型不稳定。这又是一个ANN（人工神经网络）和人类大脑之间基本区别的例证，后者持续参与学习过程。
- en: The following sections provide information about other runtime or inference
    time activities that are worth understanding.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分提供了有关其他运行时或推理时间活动的信息，这些活动值得理解。
- en: 6.3.9.1 Prompt Engineering
  id: totrans-554
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Prompt engineering involves the design and optimisation of prompts (also read
    [Section 3.8](ch3.xhtml#sec3_8)). These are input sequences or instructions intended
    to guide a language model, such as ChatGPT, to produce the desired outputs. This
    technique is especially relevant in the context of few-shot learning, where the
    model is provided with examples within the prompt to help it understand and perform
    specific tasks.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
- en: At present, a myriad of templates and strategies are being championed as ‘the
    best’ for prompt engineering, especially in relation to ChatGPT. These methods
    typically encompass various elements that structure the request. This includes
    elucidating the context, defining the intended audience or roles, and offering
    examples (as observed in few-shot learning) to effectively shape the model’s response
    (read [Section 3.8.2](ch3.xhtml#sec3_8_2). on the Prompt Engineering Components).
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
- en: While much of the conversation about prompt engineering centres on LLMs, it
    is also considered in relation to image generation and other forms of content
    creation.
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.9.2 Prompt Injection
  id: totrans-558
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Prompt injection is a tactic whereby individuals attempt to manipulate models
    into producing inappropriate responses. They modify the prompt in various ways,
    leading to outputs that the model should not generate, a phenomenon occasionally
    referred to as adversarial prompting.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
- en: Such inappropriate outputs can range from humorous or insulting remarks to the
    provision of accurate yet ethically questionable information, such as instructions
    on creating a bomb or methods for disposing of a body.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.9.3 Jailbreaking
  id: totrans-561
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Jailbreaking refers to the process by which a model is manipulated into a mode
    that circumvents the usual safeguards in place, ensuring its intended behaviour.
    This is achieved based on specific prompts from users.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
- en: There are well-documented instances of this, such as the ‘Sydney’ mode for Bing,
    or the ‘DAN’ (Do Anything Now) mode for ChatGPT. Some individuals have also bypassed
    certain safety mechanisms of the model using alternative techniques. For example,
    by instructing the model to simulate a conversation where it is a model speaking
    to another model, and then responding to a prompt. This kind of recursive scenario
    seems to disorient the model and its filtering mechanisms, leading it to eventually
    produce responses it would not typically generate.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.9.4 Prompt Leaking
  id: totrans-564
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Prompt leaking happens where a user designs a prompt that tricks the model into
    providing real information that it should not, perhaps internal details about
    the model itself.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Models and Ecosystems
  id: totrans-566
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Figure 6.16](#fig6_16) shows how a model and the ecosystem around the model
    may be constructed. This will be different for different models and companies
    depending on their specific needs, the market they are addressing, and many other
    factors.'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
- en: '![A depiction of how a model and the ecosystem around the model may be constructed.](images/fig6_16_B.jpg)'
  id: totrans-568
  prefs: []
  type: TYPE_IMG
- en: '[Figure 6.16 The Model Ecosystem.](#R_fig6_16)'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.16 模型生态系统。](#R_fig6_16)'
- en: 'The following sections discuss some of the key aspects of this ecosystem. In
    the example from [Figure 6.16](#fig6_16) we can see:'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 以下章节讨论了该生态系统的关键方面。在[图6.16](#fig6_16)的示例中，我们可以看到：
- en: User A, is using both a Chat App and a Document Authoring App which use Foundation
    Model A.
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户A正在使用一个聊天应用和一个文档编写应用，这两个应用都使用基础模型A。
- en: Software Developer A has built and deployed a custom application which uses
    a fine-tuned version of Model A which they created specifically for legal professionals
    and specialises in international law. It uses a Retrieval Augmented Generation
    (RAG) approach to ensure the most useful outcomes for users of his product.
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 软件开发人员A构建并部署了一个自定义应用程序，该应用程序使用他们为法律专业人士创建的、专门针对国际法的微调版本模型A。它使用检索增强生成（RAG）方法，以确保其产品用户获得最有用的结果。
- en: User B uses the Custom Product that specialises in Legal GenAI for international
    law.
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户B使用专注于国际法的法律通用人工智能（Legal GenAI）的定制产品。
- en: Software Developer B has built a generic API that supports multiple foundational
    models (A, B, and C). It also supports several of the plugins that are available
    for Foundational Model A. These plugins allow Foundation Model A to do several
    interesting things, including search for travel options from a travel consolidation
    service and do general real-time searches from a well-known search engine. As
    part of this implementation Software Developer B used the ‘functions’ capability
    of Model B and if it does not get results it will call Foundational Model C.
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 软件开发人员B构建了一个通用的API，它支持多个基础模型（A、B和C）。它还支持为基础模型A提供的几个插件。这些插件允许基础模型A执行一些有趣的事情，包括从旅行整合服务中搜索旅行选项，以及从知名搜索引擎中进行一般实时搜索。作为此实现的一部分，软件开发人员B使用了模型B的“函数”功能，如果它没有获得结果，它将调用基础模型C。
- en: 6.4.1 Custom Interfaces and Chat
  id: totrans-575
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.1 自定义接口和聊天
- en: The most common custom interface is the familiar chatbot prompt and response
    interface. It is important to realise that this chat interface that humans interact
    with is separate to the model. A chat interface can make interaction with the
    model easy, and it can make several decisions about interacting with that model
    that may not be obvious. For example, it can do some of the following
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的自定义界面是熟悉的聊天机器人提示和响应界面。重要的是要认识到，人类与之交互的聊天界面与模型是分开的。聊天界面可以使与模型的交互变得容易，并且它可以做出关于与该模型交互的几个可能不明显的决定。例如，它可以执行以下操作：
- en: Setting specific hyperparameters to specific values
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将特定的超参数设置为特定值
- en: Providing previous interactions details so the model has the context of the
    conversation (e.g., automatically adding key aspects of the previous part of the
    discussion to the current context for the model)
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供之前的交互详情，以便模型具有对话的上下文（例如，自动将讨论前一部分的关键方面添加到模型的当前上下文中）
- en: Handling requests and dealing with items like caching of data to minimise the
    processing and memory load on the model.
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理请求和处理诸如数据缓存等事项，以最小化对模型的处理和内存负载。
- en: Chat interfaces are not the only way to interact with models; many tools integrate
    with GenAI. For example, web site design tools integrate with models that allow
    generation of images for the site or allow generation of items like business cards.
    Some tools, like Good Documents, integrate models for content generation while
    authoring your document.
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天界面不是与模型交互的唯一方式；许多工具都与GenAI集成。例如，网站设计工具与模型集成，允许生成网站图像或生成诸如名片之类的项目。一些工具，如Good
    Documents，在编写文档时集成模型以生成内容。
- en: 6.4.2 APIs and Functions
  id: totrans-581
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.2 API和函数
- en: The model is typically exposed as an Application Programming Interface (API),
    even if this is solely for the chat interface mentioned earlier. Companies such
    as OpenAI also offer an API to software developers who wish to harness the model
    for their bespoke needs. The API permits other software applications to access
    the model and retrieve results. This becomes particularly advantageous, for instance,
    if one operates a company offering document authorship software, like Google Docs,
    Microsoft Word, or Good Notes. Such an API can be used to integrate the model’s
    capabilities into the software, enabling functionalities such as content suggestions
    or content reviews.
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 模型通常以应用程序编程接口（API）的形式公开，即使这只是为了之前提到的聊天界面。像OpenAI这样的公司也向希望利用模型满足特定需求的软件开发者提供API。该API允许其他软件应用程序访问模型并检索结果。这在例如，如果你运营一家提供文档编写软件的公司，如Google
    Docs、Microsoft Word或Good Notes，就特别有利。这样的API可以将模型的功能集成到软件中，实现诸如内容建议或内容审查等功能。
- en: Functions provide a novel method to afford additional flexibility to software
    developers utilising the model API. There are instances where the model might
    not furnish pertinent information in its response, perhaps because the query pertains
    to a topic arising after the model’s training was finalised. Under these circumstances,
    the developer can instruct the model to return the specifics for a function call
    if it is unable to directly address the query. The model will then endeavour to
    reshape the request to align with the developer-provided function call and return
    this revised request, thereby alleviating the developer’s need to amend the request
    independently. The developer can then initiate the specified function with relative
    ease. It is crucial to emphasise that the software developer orchestrates the
    function call. The model either furnishes a standard response or, if that is unattainable,
    it preps the data for the function call as proficiently as possible, simplifying
    the process for the developer. However, the onus of initiating the function, if
    deemed apt, lies with the developer.
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 函数为使用模型API的软件开发者提供了提供额外灵活性的新方法。有时，模型可能不会在其响应中提供相关信息，可能是因为查询涉及模型训练完成后出现的主题。在这种情况下，开发者可以指示模型在无法直接回答查询时返回函数调用的具体信息。然后，模型将努力调整请求以与开发者提供的函数调用相匹配，并返回这个修改后的请求，从而减轻开发者独立修改请求的需要。然后，开发者可以相对容易地启动指定的函数。重要的是要强调，软件开发者负责发起函数调用。模型要么提供标准响应，要么如果无法做到这一点，就尽可能高效地准备函数调用所需的数据，简化开发者的过程。然而，如果认为合适，发起函数的责任在于开发者。
- en: 6.4.3 Plugins and Agents
  id: totrans-584
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.3 插件和代理
- en: Plugins, sometimes referred to as agents, enable software developers to construct
    a service which the model can invoke. In the context of an API, the developer
    initiates the call to the model. However, when dealing with a plugin or agent,
    it is the model that reaches out to another system or capability.
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 插件，有时也称为代理，使软件开发者能够构建一个模型可以调用的服务。在API的上下文中，开发者会启动对模型的调用。然而，当处理插件或代理时，是模型主动联系另一个系统或功能。
- en: Consider an instance where the ultimate objective is to ascertain current flight
    prices and schedules from Hong Kong to Dublin for a specific future date. The
    LLM would be oblivious to such information, as it wouldn’t possess this data in
    its training set; and even if it did, the information would likely be outdated.
    However, if a company like Expedia intends to supply this data to a model, a plugin
    would be the mechanism. A user might transmit a request to the model via a chat
    interface or an API call from a bespoke interface. The model, recognising its
    own inability to address the query, would invoke the Expedia plugin to retrieve
    the necessary data and relay it to the user.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个实例，其最终目标是确定从香港到都柏林在特定未来日期的当前航班价格和时刻表。LLM对此类信息一无所知，因为它在其训练集中不包含这些数据；即使它有，这些信息也很可能是过时的。然而，如果像Expedia这样的公司打算向模型提供这些数据，插件就是实现这一目标的机制。用户可能通过聊天界面或从定制界面发起的API调用向模型发送请求。模型意识到自己无法直接回答查询，就会调用Expedia插件来检索必要的数据并将其传达给用户。
- en: One can envisage a world abundant in diverse plugins, empowering models to perform
    a plethora of tasks for users. This extends beyond merely listing flights to booking
    them, ordering pizzas, hiring cars, managing household devices, and virtually
    any task feasible through online automation, given the right plugin.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 可以设想一个充满各种插件的世界，这些插件使模型能够为用户执行大量任务。这不仅仅局限于列出航班，还包括预订航班、订购披萨、租车、管理家庭设备，以及通过在线自动化实现的几乎任何任务，只要有了合适的插件。
- en: The paramount importance of trust in plugins becomes evident. The implications
    of a plugin transferring funds between bank accounts could be dire if exploited
    to transfer money illicitly. If a plugin fetches information that is deliberately
    incorrect, it poses challenges. There might be instances where a plugin’s action
    deviates from the user’s intention; for example, a user might want to only check
    flight prices but ends up having a flight booked unintentionally via the plugin.
    Such scenarios would lead to user dissatisfaction.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 插件中信任的重要性显而易见。如果插件被用于非法转账资金，其后果可能是灾难性的。如果插件获取的信息是故意错误的，它将带来挑战。可能存在插件的行为偏离用户意图的情况；例如，用户可能只想检查航班价格，但最终通过插件无意中预订了航班。这样的场景会导致用户不满。
- en: This necessitates the establishment of stringent guidelines and policies governing
    plugins, to which developers must adhere to and formally commit.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 这需要建立严格的指南和政策来规范插件，开发者必须遵守并正式承诺。
- en: OpenAI has integrated a web-browsing plugin with its ChatGPT product. Users
    have the discretion to activate or deactivate this plugin via the ChatGPT interface.
    If activated and ChatGPT fails to address a query (e.g., when asked for recent
    data not encompassed in its training), it will access the web-browsing plugin,
    essentially tapping into the new Bing search functionality which incorporates
    a specialised version of GPT-4\. This provides a valuable extension to the model,
    whilst ensuring the user retains authority over the plugin’s usage.
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI已经将其ChatGPT产品与网络浏览插件集成。用户可以通过ChatGPT界面选择激活或停用此插件。如果激活且ChatGPT未能回答查询（例如，当被要求提供其训练数据中未包含的最近数据时），它将访问网络浏览插件，本质上是在利用新的Bing搜索功能，该功能包含一个GPT-4的专用版本。这为模型提供了有价值的扩展，同时确保用户保留对插件使用的控制权。
- en: 6.4.4 Custom Fine-Tuning
  id: totrans-591
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.4 自定义微调
- en: It is feasible to fine-tune certain models for bespoke applications. In OpenAI’s
    instance, they have augmented the API for their model (currently the gpt-3-5-turbo
    model) to permit users to upload a curated set of training data in a distinct
    format. Users can then instruct the model to fine-tune based on this data and
    subsequently receive a newly instantiated model tailored for that specific objective.
    It is incumbent upon the user to test and ascertain that the fine-tuned model
    delivers the anticipated advantages.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些特定应用，可以对某些模型进行微调。以OpenAI为例，他们已经增强了其模型（目前是gpt-3-5-turbo模型）的API，允许用户上传一组经过精心挑选的以特定格式存储的训练数据。然后，用户可以指示模型根据这些数据进行微调，并随后接收一个针对该特定目标量身定制的全新模型。用户有责任测试并确认微调后的模型能够提供预期的优势。
- en: Such a facility paves the way for the creation of custom models tailored for
    niche purposes. For instance, a model could be honed to specialise in international
    law, enabling software developers to supplement their products with additional
    functionalities. These enhanced features would prove invaluable to a select group
    of users seeking such specific capabilities.
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的设施为创建针对特定目的的定制模型铺平了道路。例如，一个模型可以被优化以专门从事国际法，使软件开发者能够通过添加额外的功能来补充他们的产品。这些增强功能将对寻求此类特定功能的用户群非常有价值。
- en: 6.4.5 Custom Models
  id: totrans-594
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.5 自定义模型
- en: Many model developers offer customised models for specific purposes. These are
    often fine-tuned versions of the foundational model, adapted for that particular
    application. Examples encompass chat-based models, code generation-based models,
    and even highly specialised models that not only generate but also execute code.
    OpenAI has introduced a custom model named ‘Advanced Data Analysis’, previously
    called Code Interpreter, designed to generate code. Furthermore, this model can
    execute the code and present the output. This proves invaluable for software engineers,
    enabling them to iterate over code more rapidly, with the model shouldering more
    responsibilities, including debugging. While the Advanced Data Analysis is marketed
    akin to a plugin and is managed via the Chat interface, much like the ‘Web Browsing’
    plugin mentioned earlier, its distinct specialisation is evident when accessed
    via an API, the Advanced Data Analysis is invoked as a separate model.
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 许多模型开发者提供针对特定目的的定制模型。这些通常是基础模型的微调版本，针对特定应用进行了调整。例子包括基于聊天的模型、基于代码生成的模型，甚至是一些高度专业化的模型，这些模型不仅生成代码，还能执行代码。OpenAI推出了一种名为“高级数据分析”的定制模型，之前称为代码解释器，旨在生成代码。此外，该模型还可以执行代码并展示输出。这对软件工程师来说非常有价值，使他们能够更快地迭代代码，模型承担更多责任，包括调试。虽然高级数据分析被营销为类似插件的产品，并通过聊天界面管理，就像之前提到的“网页浏览”插件一样，但通过API访问时，其独特的专业化特点显而易见，高级数据分析作为一个单独的模型被调用。
- en: 6.4.6 Vector Databases and Retrieval Augmented Generation (RAG)
  id: totrans-596
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.6 向量数据库和检索增强生成（RAG）
- en: Vector databases are specialised databases designed for the efficient querying
    and retrieval of data using vectors. In this context, vectors are arrays of numbers
    representing objects, such as text, images, or sounds, within a multidimensional
    space.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 向量数据库是专门设计用于使用向量高效查询和检索数据的数据库。在此上下文中，向量是表示多维空间中对象的数字数组，例如文本、图像或声音。
- en: These databases are optimised for operations like similarity searches among
    vectors. They allow users to efficiently identify items resembling a given input
    vector. Such capability proves particularly beneficial in applications related
    to machine learning, recommendation systems, image retrieval, and natural language
    processing, wherein locating similar items in high-dimensional spaces is vital.
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据库针对向量之间的相似性搜索等操作进行了优化。它们允许用户高效地识别与给定输入向量相似的项。这种能力在机器学习、推荐系统、图像检索和自然语言处理等应用中尤其有益，在这些应用中，在高维空间中定位相似项至关重要。
- en: Retrieval Augmented Generation (RAG) merges the capabilities of pre-trained
    language models with information retrieval systems, enhancing the generation of
    responses in conversational AI and other NLP tasks.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）将预训练语言模型的能力与信息检索系统相结合，增强了对话式人工智能和其他自然语言处理任务中的响应生成。
- en: Given an input (e.g., a question or prompt), RAG sources pertinent documents
    or text snippets from a corpus or database, often a vector database. The information
    retrieved subsequently informs the generative model’s response, facilitating the
    creation of more precise, informative, and contextually relevant outputs.
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个输入（例如，一个问题或提示），RAG从语料库或数据库中检索相关的文档或文本片段，通常是一个向量数据库。随后检索到的信息将指导生成模型的响应，从而促进更精确、信息丰富和上下文相关的输出的创建。
- en: This approach can be likened to enriching the prompt with examples (akin to
    few-shot learning), enabling the model to generate superior responses.
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法可以类比为在提示中添加示例（类似于少样本学习），使模型能够生成更优质的响应。
- en: A typical scenario where this is advantageous involves an organisation possessing
    an internal model and a database of documents that can readily address the current
    request. For instance, a legal firm aiming to produce content may want to ensure
    alignment with its existing templates and structures, particularly for documents
    like contracts, legal notices, or letters.
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 这种优势的一个典型场景是，一个组织拥有一个内部模型和可以轻松处理当前请求的文档数据库。例如，一家旨在生成内容的律师事务所可能希望确保其内容与现有的模板和结构保持一致，尤其是对于合同、法律通知或信函等文件。
- en: 6.5 State-of-the-Art Models Overview
  id: totrans-603
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.5 现有技术模型概述
- en: In the subsequent sections, we examine several renowned state-of-the-art (SOTA)
    models. Our emphasis is on large language models and text-to-image diffusion models.
    For each model, we offer a concise overview of the company and the model itself,
    followed by details concerning its release history, specifications, uses, and
    pertinent commentary.
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们考察了几种著名的最先进（SOTA）模型。我们的重点是大型语言模型和文本到图像的扩散模型。对于每个模型，我们提供了公司及其本身的简要概述，随后是关于其发布历史、规格、用途和相关评论的详细信息。
- en: 6.5.1 LLM – OpenAI ChatGPT-4.0
  id: totrans-605
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.1 LLM – OpenAI ChatGPT-4.0
- en: 'Established in 2015, OpenAI was originally a non-profit organisation dedicated
    to the development and democratisation of open AI systems. Among its founders
    were eminent figures in the AI domain, including individuals like Elon Musk. Subsequently,
    the organisation transitioned to a for-profit structure. Its current mission statement
    reads:'
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI成立于2015年，最初是一个致力于开放人工智能系统开发和民主化的非营利组织。其创始人包括AI领域的杰出人物，如埃隆·马斯克。随后，该组织转变为营利性结构。其当前的使命声明如下：
- en: Our mission is to ensure that artificial general intelligence – AI systems that
    outperform human intelligence – benefits all of humanity.
  id: totrans-607
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们的使命是确保通用人工智能——超越人类智能的人工智能系统——造福全人类。
- en: '**Release History**'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: '**发布历史**'
- en: 'OpenAI GPT-n series of models:'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI GPT-n系列模型：
- en: Table 6.1 OpenAI GPT-n Series Release History
  id: totrans-610
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 表6.1 OpenAI GPT-n系列发布历史
- en: '| Date | Model | Description |'
  id: totrans-611
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 日期 | 模型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-612
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 11/Jun/2018 | GPT-1 | Improving Language Understanding by Generative Pre-Training
    ([Radford et al. 2018](#ref6_52)) |'
  id: totrans-613
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 11/Jun/2018 | GPT-1 | 通过生成预训练改进语言理解 ([Radford et al. 2018](#ref6_52)) |'
- en: '| 14/Feb/2019 | GPT-2 | Language Models are Unsupervised Multitask Learners
    ([Radford et al., 2019](#ref6_53); Solaiman et al., 2019) |'
  id: totrans-614
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 14/Feb/2019 | GPT-2 | 语言模型是无监督的多任务学习者 ([Radford et al., 2019](#ref6_53);
    Solaiman et al., 2019) |'
- en: '| 28/May/2020 | GPT-3 | Language Models are Few-Shot Learners ([Brown et al.,
    2020](#ref6_10)) |'
  id: totrans-615
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 28/May/2020 | GPT-3 | 语言模型是少样本学习者 ([Brown et al., 2020](#ref6_10)) |'
- en: '| 15/Mar/2022 | GPT-3.5 | Version 3.5 release |'
  id: totrans-616
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 15/Mar/2022 | GPT-3.5 | 3.5版本发布 |'
- en: '| 30/Nov/2022 | ChatGPT | This was based on GPT-3.5 model and later both GTP-3.5
    and GPT-4 were supported via user selection |'
  id: totrans-617
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 30/Nov/2022 | ChatGPT | 这基于GPT-3.5模型，后来通过用户选择支持GPT-3.5和GPT-4 |'
- en: '| 14/Mar/2023 | GPT-4 | GPT-4 Technical Report ([OpenAI, 2023](#ref6_48)) |'
  id: totrans-618
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 14/Mar/2023 | GPT-4 | GPT-4技术报告 ([OpenAI, 2023](#ref6_48)) |'
- en: '**Specifications**'
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: '**规格**'
- en: 'The size and specification of GPT-4 has not been shared; however, from the
    [OpenAI 2023](#ref6_48) Technical Report publication we do learn the following:'
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4的大小和规格尚未公开；然而，从[OpenAI 2023](#ref6_48)技术报告的发布中，我们了解到以下信息：
- en: It is a transformer pre-trained model (like GPT3)
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是一个转换器预训练模型（类似于GPT3）
- en: It exhibits human-level performance on many benchmarks.
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它在许多基准测试中表现出人类水平的表现。
- en: 'However, more details have been leaked and, while not official, they do provide
    some insight to the sizing:'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，更多细节已经泄露，尽管不是官方的，但它们确实提供了一些关于尺寸的见解：
- en: GPT-4 has ~1.8 trillion parameters across 120 layers.
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-4在120层中拥有约1.8万亿个参数。
- en: GPT-4 is trained on ~13 trillion tokens, including both text-based and code-based
    data.
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-4在约13万亿个标记上进行训练，包括基于文本和代码的数据。
- en: The training data included Common Crawl & RefinedWeb, totalling 13T tokens.
    There is considerable speculation that additional sources like Twitter, Reddit,
    YouTube, and a large collection of textbooks were also used.
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据包括Common Crawl & RefinedWeb，总计13T个标记。有相当多的猜测认为还使用了额外的来源，如Twitter、Reddit、YouTube以及大量教科书。
- en: It uses a mixture of experts (MoE) architecture which is an ensemble learning
    approach and allows different experts to specialise in different areas with the
    most appropriate being selected. There may be either 16 experts with ~111B parameters
    each or 8 experts with ~220B parameters each.
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使用专家混合（MoE）架构，这是一种集成学习方法，允许不同的专家在不同的领域专业化，并选择最合适的专家。可能有16个专家，每个专家约1110亿个参数，或者8个专家，每个专家约2200亿个参数。
- en: '**Use**'
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用**'
- en: It is multimodal, which means it can handle text and images as inputs and text
    as an output. Recently, it has been integrated with DALL-E 3 for image-based output.
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是多模态的，这意味着它可以处理文本和图像作为输入，并以文本作为输出。最近，它已经与DALL-E 3集成，用于基于图像的输出。
- en: With the Code Interpreter (now called advanced data analysis) feature, it can
    both generate and execute code
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过代码解释器（现在称为高级数据分析）功能，它可以生成和执行代码
- en: '**Comments**'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: '**评论**'
- en: This model is widely regarded as the most advanced and useful for general text
    generation as well as code generation.
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型被广泛认为是最先进且适用于通用文本生成以及代码生成的。
- en: 6.5.2 LLM - Meta LLaMA-2
  id: totrans-633
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.2 LLM - Meta LLaMA-2
- en: Meta, formerly known as Facebook, introduced the LLaMA model as one of its most
    sophisticated offerings. Although it was initially disseminated as a research
    release, it was inadvertently leaked and broadly circulated. Consequently, for
    LLaMA-2, Meta opted to release it under a more permissive commercial license.
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: Meta（原名Facebook）将 LLaMA 模型作为其最复杂的产品之一引入。尽管它最初作为研究发布，但意外泄露并广泛传播。因此，对于 LLaMA-2，Meta
    选择以更宽松的商业许可证发布。
- en: '**Release History**'
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: '**发布历史**'
- en: Table 6.2 Meta LLaMA Release History
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表 6.2 Meta LLaMA 版本发布历史
- en: '| Date | Model | Description |'
  id: totrans-637
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 日期 | 模型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-638
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Feb/2023 | LLaMA-1 | LLaMA: Open and Efficient Foundation Language Models
    ([Touvron et al., 2023a](#ref6_71)) |'
  id: totrans-639
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| Feb/2023 | LLaMA-1 | LLaMA: 开放和高效的开放基础语言模型 ([Touvron 等人，2023a](#ref6_71))
    |'
- en: '| 18/Jul/2023 | LLaMA-2 | LLaMA-2: Open Foundation and Fine-Tuned Chat Models
    ([Touvron et al., 2023b](#ref6_72)) |'
  id: totrans-640
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 18/Jul/2023 | LLaMA-2 | LLaMA-2: 开放基础和微调聊天模型 ([Touvron 等人，2023b](#ref6_72))
    |'
- en: '**Specifications**'
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: '**规格**'
- en: LLaMA-2 is designed as an auto-regressive language optimised transformer.
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLaMA-2 被设计为自回归语言优化的 Transformer。
- en: The fine-tuned versions of the model implement Supervised Fine-Tuning (SFT)
    and Reinforcement Learning with Human Feedback (RLHF) to better align with human
    preferences for helpfulness and safety.
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型的微调版本实现了监督微调（SFT）和基于人类反馈的强化学习（RLHF），以更好地与人类对有用性和安全性的偏好保持一致。
- en: 'LLaMA-2 comes in three sizes based on the number of parameters: 7 billion,
    13 billion, and 70 billion parameters.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLaMA-2 根据参数数量分为三种大小：70亿、130亿和700亿参数。
- en: The models have a token count referring only to the pre-training data, with
    all models trained with a global batch size of 4 million tokens. The largest model,
    with 70 billion parameters, utilises Grouped-Query Attention (GQA) for improved
    inference scalability
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的标记计数仅指预训练数据，所有模型均使用 400 万个全局批次的标记进行训练。最大的模型，具有 700 亿参数，利用分组查询注意力（GQA）以改善推理可扩展性。
- en: The training and fine-tuning of LLaMA-2 leveraged publicly available online
    data sources, with over one million human-annotated examples used for fine-tuning.
    This model does not include Meta user data in its training or fine-tuning datasets.
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLaMA-2 的训练和微调利用了公开可用的在线数据源，使用了超过一百万个由人类标注的示例用于微调。该模型在其训练或微调数据集中不包括 Meta 用户数据。
- en: '**Use**'
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: '**用途**'
- en: The model is primarily intended for text-based applications.
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型主要用于文本应用。
- en: It’s optimised for dialogue use cases through its fine-tuned versions, known
    as LLaMA-2-Chat, but its pre-trained versions can be adapted for a broader range
    of natural language generation tasks.
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过其微调版本，即 LLaMA-2-Chat，它优化了对话用例，但其预训练版本可以适应更广泛的自然语言生成任务。
- en: LLaMA-2 is open for both commercial and research use, and it is particularly
    targeted for use in English language tasks.
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLaMA-2 对商业和研究用途均开放，尤其针对英语语言任务。
- en: '**Comments**'
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: '**评论**'
- en: LLaMA-2 is Meta’s response to other large language models like OpenAI’s GPT
    models and Google’s AI models, with a distinguishing feature of being more open
    and freely available for almost anyone to use for research and commercial purposes.
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLaMA-2 是 Meta 对 OpenAI 的 GPT 模型以及 Google 的 AI 模型等大型语言模型的回应，其显著特点是更加开放和免费，几乎任何人都可以用于研究和商业目的。
- en: 6.5.3 LLM – Google Bard and PaLM-2
  id: totrans-653
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.3 LLM – Google Bard 和 PaLM-2
- en: Google has consistently contributed to AI research, notably through the seminal
    paper titled “Attention is All You Need”, which has influenced the prevailing
    direction of Transformer-based architectures for large language models.
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: Google 一直致力于 AI 研究，特别是通过具有里程碑意义的论文“Attention is All You Need”，该论文影响了基于 Transformer
    架构的大型语言模型的流行方向。
- en: '**Release History**'
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: '**发布历史**'
- en: Table 6.3 Google PaLM/Bard/LaMDA Release History
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表 6.3 Google PaLM/Bard/LaMDA 发布历史
- en: '| Date | Model | Description |'
  id: totrans-657
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 日期 | 模型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-658
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 18/May/2021 | LaMDA-1 | LaMDA: Language Models for Dialog Applications ([Thoppilan
    et al., 2022](#ref6_70)) |'
  id: totrans-659
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 18/May/2021 | LaMDA-1 | LaMDA: 用于对话应用的语言模型 ([Thoppilan 等人，2022](#ref6_70))
    |'
- en: '| 11/May/2022 | LaMDA-2 | Version 2 update |'
  id: totrans-660
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 11/May/2022 | LaMDA-2 | 版本 2 更新 |'
- en: '| 21/Mar/2023 | Bard | Chat bot made available initially built on LaMDA-2 |'
  id: totrans-661
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 21/Mar/2023 | Bard | 基于LaMDA-2最初构建的聊天机器人 |'
- en: '| Mar/2023 | Bard | Google announced Bard would switch to PaLM-2 shortly |'
  id: totrans-662
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 2023年3月 | Bard | Google宣布Bard将很快切换到PaLM-2 |'
- en: '| Mar/2023 | PaLM-1 | PaLM: Scaling Language Modeling with Pathways ([Chowdhery
    et al., 2022](#ref6_13)) |'
  id: totrans-663
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 2023年3月 | PaLM-1 | PaLM：通过路径扩展语言建模([Chowdhery等人，2022](#ref6_13)) |'
- en: '| May/2023 | PaLM-2 | PaLM 2 Technical Report ([Anil et al., 2023](#ref6_2))
    |'
  id: totrans-664
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 2023年5月 | PaLM-2 | PaLM 2技术报告([Anil等人，2023](#ref6_2)) |'
- en: '**Specification**'
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: '**规格**'
- en: PaLM-2 (Pathways Language Model) was discussed in relation to the three sizes
    small (PaLM-2 S), medium (PaLM-2 M) and large (PaLM-2 L) in the paper ([Anil et
    al., 2023](#ref6_2)). The large version is reported to be considerable smaller
    than the original PaLM model which has 540B parameters.
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 论文中讨论了PaLM-2（路径语言模型）与三种尺寸的小型（PaLM-2 S）、中型（PaLM-2 M）和大型（PaLM-2 L）的关系。据报道，大型版本比原始的具有540B参数的PaLM模型要小得多。
- en: PaLM-2 has put greater focus on training with text from languages other than
    English as well as a focus on a diverse set of domains for the training data.
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PaLM-2在训练时更加注重使用非英语语言的文本，以及关注多样化的训练数据领域。
- en: The model is based on the transformer model; however, it has expanded the training
    objectives from masked language modelling to use a mixture of different pre-training
    objectives to help it understand different aspect of the language.
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型基于transformer模型；然而，它将训练目标从掩码语言建模扩展到使用不同预训练目标的混合，以帮助它理解语言的各个方面。
- en: The model also relooked at how training compute should be used and move to an
    approach where model size vs training data size were more aligned with a 1:1 ratio
    to get the best value for a specific compute capacity.
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型还重新审视了训练计算的使用方式，并转向了一种模型大小与训练数据大小以1:1的比例更一致的方法，以获得特定计算能力下的最佳价值。
- en: 'PaLM-2 is provided in several different configurations: Gecko, Otter, Bison,
    and Unicorn. It is thought that the sizes of these are 1.5B, 6B, 137B, and 540B.
    Each version has a different size and is designed for use within specific applications.
    For example, Gecko is the smallest and most lightweight. It is designed for mobile
    devices.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PaLM-2提供了几种不同的配置：Gecko、Otter、Bison和Unicorn。据信这些配置的大小分别为1.5B、6B、137B和540B。每个版本都有不同的尺寸，并设计用于特定应用。例如，Gecko是最小且最轻量级的。它专为移动设备设计。
- en: The chat interface Google Bard uses the PaLM-2 model. However, the only way
    for developers to access these models at the moment is via the Google Cloud Platform
    (e.g. PaLM API, MakerSuite, or Vertex AI).
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google Bard使用的聊天界面采用了PaLM-2模型。然而，目前开发人员访问这些模型的唯一方式是通过Google Cloud Platform（例如PaLM
    API、MakerSuite或Vertex AI）。
- en: '**Use**'
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: '**用途**'
- en: It is focused on advanced reasoning tasks, including code and math, classification
    and question-answering, translation and multilingual proficiency, and natural
    language generation.
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它专注于高级推理任务，包括代码和数学、分类和问答、翻译和多语言能力，以及自然语言生成。
- en: '**Comments**'
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: '**评论**'
- en: The PaLM-2 model, and Google Bard have not had the same popularity or adoption
    that OpenAI ChatGPT has had or the same level of praise for its abilities.
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PaLM-2模型和Google Bard并没有像OpenAI ChatGPT那样受欢迎或被采用，也没有获得对其能力的相同水平的赞誉。
- en: 6.5.4 LLM – Anthropic Claude
  id: totrans-676
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.4 LLM – Anthropic Claude
- en: Anthropic was founded in 2021 and among its founders were several people who
    left OpenAI to start Anthropic and thus it is a relatively new player.
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: Anthropic成立于2021年，其创始人中有几位是从OpenAI离职来创办Anthropic的，因此它是一个相对较新的参与者。
- en: '**Release History**'
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: '**发布历史**'
- en: Table 6.4 Anthropic Claude Release History
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表6.4 Anthropic Claude发布历史
- en: '| Date | Model | Description |'
  id: totrans-680
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 日期 | 模型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-681
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 14/Mar/2023 | Claude-1 | Claude and Claude Instant were released |'
  id: totrans-682
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 2023年3月14日 | Claude-1 | Claude和Claude Instant发布 |'
- en: '| 23/Aug/2023 | Claude-2 | Claude was updated |'
  id: totrans-683
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 2023年8月23日 | Claude-2 | Claude进行了更新 |'
- en: '**Specifications**'
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: '**规格**'
- en: Claude is a transformer-based architecture. The model is larger than the 52B
    parameter model AnthropicLM discussed in the paper ([Bai et al., 2022](#ref6_6)),
    but is an autoregressive model trained on a large corpus of text in a self-supervised
    manner similar to GPT-3.
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Claude是一个基于transformer的架构。该模型比论文中讨论的AnthropicLM的52B参数模型要大，但它是一个在大规模文本语料库上以自监督方式训练的自动回归模型，类似于GPT-3。
- en: 'Claude comes in two versions: Claude and Claude Instant. Claude is a state-of-the-art
    high-performance model, while Claude Instant is a lighter, less expensive, and
    much faster option.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Claude有两种版本：Claude和Claude Instant。Claude是一个最先进的高性能模型，而Claude Instant是一个更轻量级、更便宜且速度更快的选项。
- en: Anthropic has partnered with Quora and they have built a chat interface to Claude
    as a mobile application called Poe. Poe also offers interfaces to other models,
    which makes it an interesting tool to compare the responses from different models.
    Anthropic also provide developer access via an API to Claude and Claude Instant.
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthropic与Quora合作，并构建了一个名为Poe的移动应用程序的聊天界面，用于克劳德。Poe还提供了对其他模型的接口，使其成为一个有趣的工具，可以比较不同模型的响应。Anthropic还通过API为克劳德和克劳德即时提供开发者访问权限。
- en: Anthropic has also partnered with DuckDuckGo, which is a privacy-focused search
    engine and browser designed to integrate with real-time information.
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthropic还与DuckDuckGo合作，DuckDuckGo是一个以隐私为重点的搜索引擎和浏览器，旨在与实时信息集成。
- en: '**Use**'
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用**'
- en: Claude is capable of a wide variety of conversational and text-processing tasks
    while maintaining a high degree of reliability and predictability.
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 克劳德能够在保持高度可靠性和可预测性的同时，执行广泛的对话和文本处理任务。
- en: Claude can help with use cases, including summarisation, search, creative and
    collaborative writing, Q&A, coding, and more.
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 克劳德可以帮助处理包括摘要、搜索、创意和协作写作、问答、编码等用例。
- en: '**Comments**'
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: '**注释**'
- en: Anthropic have written about ([Bai et al., 2022](#ref6_6)) an approach called
    Constitutional AI (CAI) which involves models training models in a safe manner.
    This reflects the history of the company as focused on AI safety research.
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthropic撰写了关于（[Bai等人，2022](#ref6_6)）一种称为宪法人工智能（CAI）的方法的文章，该方法涉及以安全的方式训练模型。这反映了公司专注于AI安全研究的历史。
- en: One of the unique things about Claude is its large context size of 100k tokens.
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 克劳德（Claude）的一个独特之处在于其庞大的上下文大小，达到100k个标记。
- en: Based on current capabilities Claude is not as strong as GPT-4.
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据当前能力，克劳德不如GPT-4强大。
- en: 6.5.5 LLM – Mistral AI Mistral
  id: totrans-696
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.5 LLM – Mistral AI Mistral
- en: Mistral AI was founded in 2023 and only released its first model a short time
    ago. Unlike many of the other players, Mistral is based in Europe (France) and
    has offered its initial model as open source.
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: Mistral AI成立于2023年，不久前才发布了其第一个模型。与许多其他参与者不同，Mistral总部位于欧洲（法国），并已将其初始模型作为开源发布。
- en: '**Release History**'
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: '**发布历史**'
- en: Table 6.5 Mistral AI Release History
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表6.5 Mistral AI发布历史
- en: '| Date | Model | Description |'
  id: totrans-700
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 日期 | 模型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-701
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 27/Sep/2023 | Mistral-7B | Mistral 7B model is released under an Apache 2
    license. |'
  id: totrans-702
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 27/Sep/2023 | Mistral-7B | Mistral 7B模型在Apache 2许可证下发布。|'
- en: '**Specifications**'
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: '**规格**'
- en: 'Mistral AI has released two initial versions: Mistral-7B and Mistral-7B-Instruct.
    It intends to release larger models later. The instruct version is a fine-tuned
    version for question-and-answer interactions such as a chat use case.'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mistral AI发布了两个初始版本：Mistral-7B和Mistral-7B-Instruct。它打算稍后发布更大的模型。指令版本是为问答交互（如聊天用例）微调的版本。
- en: The models use grouped-query attention (GQA) for faster inference.
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些模型使用分组查询注意力（GQA）以实现更快的推理。
- en: The models use sliding window attention (SWA) to handle longer sequences at
    smaller cost.
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些模型使用滑动窗口注意力（SWA）以较小的成本处理更长的序列。
- en: The models have been released as open source with an Apache 2 style license.
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些模型以Apache 2风格的许可证作为开源发布。
- en: Based on the benchmarks, they outperforms LLaMA-2 13B on all benchmarks and
    outperforms LLaMA-1 34B on many benchmarks. They approaches CodeLlama 7B performance
    on code, while remaining good at English tasks.
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据基准测试，它们在所有基准测试中都优于LLaMA-2 13B，在许多基准测试中优于LLaMA-1 34B。它们在代码方面的性能接近CodeLlama
    7B，同时在英语任务上仍然表现良好。
- en: '**Use**'
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用**'
- en: The initial models are focused on general purpose English language generation
    and a fine-tuned question–answer model.
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始模型专注于通用英语语言生成和微调的问答模型。
- en: They are also trained on code generation tasks.
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们还训练于代码生成任务。
- en: '**Comments**'
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: '**注释**'
- en: This model is open source and available for commercial use, which presents a
    useful alternative for developers.
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型是开源的，可用于商业用途，这为开发者提供了一个有用的替代方案。
- en: It is also positioning itself as a high-performance model despite its currently
    smaller size.
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管目前规模较小，但它也定位自己为一个高性能模型。
- en: 6.5.6 Diffusion Model – stability.ai Stable Diffusion
  id: totrans-715
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.6 扩散模型 – stability.ai Stable Diffusion
- en: Stability AI was founded in 2020 and their model, Stable Diffusion, was first
    released in 2022\. This model built on the work from Prof. Dr. Björn Ommer, who
    led the original Stable Diffusion V1 release.
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: Stability AI成立于2020年，他们的模型Stable Diffusion于2022年首次发布。该模型基于Prof. Dr. Björn Ommer的工作，他领导了原始Stable
    Diffusion V1的发布。
- en: '**Release History**'
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: '**发布历史**'
- en: Table 6.6 Stable Diffusion Release History
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表6.6 Stable Diffusion发布历史
- en: '| Date | Model | Description |'
  id: totrans-719
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 日期 | 模型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-720
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Aug/2022 | Stable Diffusion 1.4 | Version 1.4 release. |'
  id: totrans-721
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 2022年8月 | 稳定扩散1.4 | 版本1.4发布。 |'
- en: '| Oct/2022 | Stable Diffusion 1.5 | Version 1.5 release. |'
  id: totrans-722
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 2022年10月 | 稳定扩散1.5 | 版本1.5发布。 |'
- en: '| Nov/2022 | Stable Diffusion 2.0 | Version 2.0 release. |'
  id: totrans-723
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 2022年11月 | 稳定扩散2.0 | 版本2.0发布。 |'
- en: '| Dec/2022 | Stable Diffusion 2.1 | Version 2.1 release. |'
  id: totrans-724
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 2022年12月 | 稳定扩散2.1 | 版本2.1发布。 |'
- en: '| Jul/2023 | Stable Diffusion XL1.0 | New XL version 1.0 release |'
  id: totrans-725
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 2023年7月 | 稳定扩散XL1.0 | 新XL版本1.0发布 |'
- en: '**Specifications**'
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: '**规格**'
- en: Stable Diffusion is a deep learning, text-to-image model based on diffusion
    techniques. It operates as a Latent Diffusion Model with a fixed, pre-trained
    text encoder, known as OpenCLIP-ViT/H, for generating and modifying images based
    on text prompts.
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稳定扩散是一个基于扩散技术的深度学习、文本到图像模型。它作为一个具有固定、预训练文本编码器（称为OpenCLIP-ViT/H）的潜在扩散模型运行，用于根据文本提示生成和修改图像。
- en: It is believed to be trained on over 5 billion images from a variety of sources
    such as Flickr, Wikimedia Commons and LAION-5B.
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 据信它是在来自Flickr、Wikimedia Commons和LAION-5B等多种来源的超过50亿张图像上训练的。
- en: '**Use**'
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用**'
- en: Its primary use is to generate detailed images conditioned on text descriptions.
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它的主要用途是根据文本描述生成详细的图像。
- en: It also supports other tasks such as inpainting (editing within the image),
    outpainting (extending the image outside of the original image), and image-to-image
    translations guided by text prompts.
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它还支持其他任务，例如图像修复（图像内的编辑）、图像扩展（扩展到原始图像之外）以及由文本提示引导的图像到图像的翻译。
- en: The model is noted for its ability to create descriptive images with enhanced
    composition and realistic aesthetics, and it can also generate words within images.
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型以其能够创建具有增强构图和逼真美学的描述性图像而闻名，并且它还可以在图像中生成单词。
- en: '**Comments**'
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: '**评论**'
- en: Its flagship image model, SDXL 1.0, is particularly highlighted for its superiority
    in image generation.
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它的旗舰图像模型SDXL 1.0在图像生成方面的优越性得到了特别强调。
- en: The Stable Diffusion 2.0 release included robust text-to-image models trained
    using a new text encoder developed by LAION, which significantly improved the
    quality of generated images compared to earlier releases.
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稳定扩散2.0的发布包括了使用由LAION开发的新文本编码器训练的鲁棒文本到图像模型，与早期版本相比，显著提高了生成图像的质量。
- en: 6.5.7 Diffusion Model – OpenAI DALL-E 3
  id: totrans-736
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.7 扩散模型 – OpenAI DALL-E 3
- en: DALL-E 3 is the most recent text-to-image model developed by OpenAI, which was
    discussed earlier in the context of ChatGPT. The integration of DALL-E 3 with
    ChatGPT enhances its accessibility and represents a marked advancement over DALL-E
    2.
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: DALL-E 3是OpenAI开发的最新文本到图像模型，之前在ChatGPT的上下文中已经讨论过。DALL-E 3与ChatGPT的集成增强了其可访问性，并在DALL-E
    2之上实现了显著的进步。
- en: '**Release History**'
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: '**发布历史**'
- en: Table 6.7 OpenAI DALL-E Release History
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表6.7 OpenAI DALL-E 发布历史
- en: '| Date | Model | Description |'
  id: totrans-740
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 日期 | 模型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-741
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 5/Jan/2021 | DALL-E | Initial release |'
  id: totrans-742
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 2021年1月5日 | DALL-E | 初始发布 |'
- en: '| 6/Apr/2022 | DALL-E 2 | Version 2 release |'
  id: totrans-743
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 2022年4月6日 | DALL-E 2 | 版本2发布 |'
- en: '| Sep/2023 | DALL-E 3 | Version 3 release |'
  id: totrans-744
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 2023年9月 | DALL-E 3 | 版本3发布 |'
- en: '**Specifications**'
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: '**规格**'
- en: DALL-E 3 is a text-to-image model based on diffusion techniques.
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DALL-E 3是一个基于扩散技术的文本到图像模型。
- en: It has been built natively on ChatGPT. When prompted with an idea, ChatGPT will
    automatically generate tailored and detailed prompts for DALL-E 3
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是在ChatGPT上本地构建的。当被一个想法提示时，ChatGPT将自动为DALL-E 3生成定制和详细的提示。
- en: '**Use**'
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用**'
- en: Image generation where the details of the prompt are important in providing
    a more nuanced output image.
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在图像生成中，提示的细节对于提供更细腻的输出图像非常重要。
- en: '**Comments**'
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: '**评论**'
- en: DALL-E 3 has put more effort into ‘safety’ where prompts can’t generate images
    in the style of living artists and creators can request their works to be excluded
    from training of future image generation models.
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DALL-E 3在“安全性”方面投入了更多努力，其中提示无法生成活着的艺术家和创作者风格的图像，创作者可以请求将他们的作品排除在未来图像生成模型的训练之外。
- en: DALL-E 3-generated images belong to the prompter.
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DALL-E 3生成的图像属于提示者。
- en: 6.5.8 Diffusion Model – Midjourney Inc. Midjourney
  id: totrans-753
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.8 扩散模型 – Midjourney Inc. Midjourney
- en: Midjourney was founded by David Holt. Distinctively, unlike other AI start-ups,
    it generated substantial revenue prior to securing venture capital funding. Their
    inaugural model, released in 2022, gained popularity, and its integration with
    Discord appeared to bolster its prominence.
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: Midjourney 由 David Holt 创立。与其它人工智能初创公司不同，它在获得风险投资资金之前就已经产生了大量收入。他们于 2022 年发布的首款模型获得了人气，并且与
    Discord 的集成似乎增强了其知名度。
- en: '**Release History**'
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
  zh: '**发布历史**'
- en: Table 6.8 Midjourney Release History
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表 6.8 Midjourney 发布历史
- en: '| Date | Model | Description |'
  id: totrans-757
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 日期 | 模型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-758
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Feb/2022 | Midjourney V1 | Version 1 release |'
  id: totrans-759
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| Feb/2022 | Midjourney V1 | 版本 1 发布 |'
- en: '| 12/Apr/2022 | Midjourney V2 | Version 2 release |'
  id: totrans-760
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 12/Apr/2022 | Midjourney V2 | 版本 2 发布 |'
- en: '| 25/Jul/2022 | Midjourney V3 | Version 3 release |'
  id: totrans-761
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 25/Jul/2022 | Midjourney V3 | 版本 3 发布 |'
- en: '| 5/Nov/2022 | Midjourney V4 | Version 4 release |'
  id: totrans-762
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 5/Nov/2022 | Midjourney V4 | 版本 4 发布 |'
- en: '| 15/Mar/2023 | Midjourney V5 | Version 5 release |'
  id: totrans-763
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 15/Mar/2023 | Midjourney V5 | 版本 5 发布 |'
- en: '| 3/May/2023 | Midjourney V5.1 | Version 5.1 release |'
  id: totrans-764
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 3/May/2023 | Midjourney V5.1 | 版本 5.1 发布'
- en: '| 22/Jun/2023 | Midjourney V5.2 | Version 5.2 release |'
  id: totrans-765
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 22/Jun/2023 | Midjourney V5.2 | 版本 5.2 发布 |'
- en: '**Specifications**'
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: '**规格**'
- en: Midjourney is a diffusion model and uses a transformer neural network to generate
    images from text prompts.
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Midjourney 是一个扩散模型，使用变压器神经网络从文本提示生成图像。
- en: The diffusion process is designed to generate more creative and expressive images
    than some of the other models.
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩散过程旨在生成比其他一些模型更具创造性和表现力的图像。
- en: '**Use**'
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: '**用途**'
- en: Midjourney is designed to be user-friendly and accessible through Discord.
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Midjourney 设计得既用户友好又易于通过 Discord 访问。
- en: '**Comments**'
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: '**评论**'
- en: Some of Midjourney’s experimental algorithms might have licensing limitations
    under the Creative ML OpenRAIL-M license, as implied by a recent amendment to
    their terms of service.
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Midjourney 的一些实验性算法可能受到 Creative ML OpenRAIL-M 许可证的许可限制，这可以从他们服务条款的最新修订中看出。
- en: 6.5.9 Speech Recognition – OpenAI Whisper
  id: totrans-773
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.9 语音识别 – OpenAI Whisper
- en: The Whisper model from OpenAI has been released as an open source model with
    the objective of facilitating its use in a wide variety of situations, including
    embedded scenarios where speech-to-text capabilities are beneficial.
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 的 Whisper 模型已被发布为开源模型，旨在促进其在各种情况下的使用，包括嵌入式场景，在这些场景中，语音到文本的功能是有益的。
- en: '**Release History**'
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: '**发布历史**'
- en: Table 6.9 OpenAI Whisper Release History
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表 6.9 OpenAI Whisper 发布历史
- en: '| Date | Model | Description |'
  id: totrans-777
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 日期 | 模型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-778
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 21/Sep/2022 | Whisper | Robust Speech Recognition via Large-Scale Weak Supervision
    ([Radford et al., 2022](#ref6_51)) |'
  id: totrans-779
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 21/Sep/2022 | Whisper | 通过大规模弱监督实现鲁棒的语音识别 ([Radford 等人，2022](#ref6_51)) |'
- en: '**Specifications**'
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: '**规格**'
- en: It is an encoder-decoder transformer architecture.
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是一个编码器-解码器变压器架构。
- en: The encoder processes input with two convolutional layers and the decoder uses
    learned positional encodings.
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器使用两个卷积层处理输入，解码器使用学习到的位置编码。
- en: It has been trained on over 680,000 hours of multilingual and multitasked data.
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它在超过 68 万小时的跨语言和多任务数据上进行了训练。
- en: '**Use**'
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
  zh: '**用途**'
- en: Whisper is an automatic speech recognition system supporting multiple languages
    with ability to understand accents and background noise. In real time it can transcribe
    speech into text.
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Whisper 是一个支持多种语言的自动语音识别系统，具有理解口音和背景噪音的能力。在实时中，它可以转录语音为文本。
- en: The model is provided as open source in a manner that allows it to be integrated
    into products.
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型以开源的方式提供，允许将其集成到产品中。
- en: '**Comments**'
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
  zh: '**评论**'
- en: It approaches human-level robustness and accuracy on English speech recognition.
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它在英语语音识别上接近人类水平的鲁棒性和准确性。
- en: One of the big advantages of Whisper is that it does well on zero-shot tasks,
    meaning that it does not required examples of a person’s speech to transcribe
    it.
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Whisper 的一大优点是它在零样本任务上表现良好，这意味着它不需要人的语音示例来转录。
- en: 6.6 Conclusions
  id: totrans-790
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.6 结论
- en: Artificial Neural Networks (ANNs) and Deep Neural Networks (DNNs) have a long
    history in AI. They have essentially been present since the inception of the AI
    discipline, tracing back to the Dartmouth Summer Research Project.
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络（ANNs）和深度神经网络（DNNs）在人工智能领域有着悠久的历史。它们基本上自人工智能学科诞生以来就存在，可以追溯到达特茅斯夏季研究项目。
- en: The objective of ANNs and DNNs has always been to develop a computer program
    capable of learning in the manner humans do, acquiring expertise across various
    domains, or potentially any area on which it is trained.
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络和深度神经网络的目标始终是开发一种能够以人类方式学习的计算机程序，在各个领域或可能在其训练的任何领域获得专业知识。
- en: Today, this ambition appears more attainable than ever before. The capabilities
    of these neural networks may well be boundless, provided we have sufficiently
    advanced computers to support ever-expanding models and an increasing wealth of
    data to train these models across the diverse topics we wish them to master.
  id: totrans-793
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，这一目标似乎比以往任何时候都更容易实现。只要我们有足够先进的计算机来支持不断扩大的模型，以及越来越多的数据来训练这些模型，以掌握我们希望他们精通的各个主题，这些神经网络的潜力可能确实是无限的。
- en: The process of creating, training, and testing a model has never been more accessible.
    This ease is due to the availability of knowledge, which aids in building the
    necessary expertise, the ready access to the models themselves, data to train
    them, and benchmarks for testing. Fine-tuning an existing foundational model is
    now comparatively straightforward and can be undertaken even by relatively inexperienced
    individuals or teams. We observe state-of-the-art (SOTA) models being released
    under flexible commercial licenses (e.g., LLaMA-2) or genuine open-source licenses
    for parts of the model environment (e.g., Mistral, Whisper, Stable Diffusion).
    The prevalence of comprehensive free online courses about machine learning further
    reduces barriers to knowledge, enabling individuals to acquire profound expertise.
    Furthermore, the prospect of quantum computing is imminent, promising transformative
    advancements in processing capabilities.
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
  zh: 创建、训练和测试模型的过程从未如此易于接近。这种便利性得益于知识的可用性，它有助于构建必要的专业知识，对模型本身的便捷访问，用于训练的数据，以及用于测试的基准。现在，微调现有的基础模型相对简单，甚至相对缺乏经验的个人或团队也能承担。我们观察到最先进的（SOTA）模型在灵活的商业许可证（例如，LLaMA-2）或真正的开源许可证下发布，用于模型环境的部分（例如，Mistral、Whisper、Stable
    Diffusion）。关于机器学习的全面免费在线课程的大量存在进一步降低了知识获取的门槛，使个人能够获得深厚的专业知识。此外，量子计算的前景即将到来，承诺在处理能力方面带来变革性的进步。
- en: All these developments arguably reinforce the notion that AGI (Artificial General
    Intelligence) and ASI (Artificial Superintelligence) are inevitable, destined
    to reach previously inconceivable levels of intelligence.
  id: totrans-795
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些发展都有力地证明了AGI（通用人工智能）和ASI（超级人工智能）是不可避免的，注定要达到以前无法想象的高度智能水平。
- en: Nevertheless, it is prudent to recall past warnings against overestimating the
    potential of neural networks. History reminds us of two prior AI ‘winters’, where
    such overconfidence played a significant role.
  id: totrans-796
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，明智的做法是回忆过去对神经网络潜力的过度估计的警告。历史提醒我们，有两个先前的AI“寒冬”，其中这种过度自信发挥了重要作用。
- en: References
  id: totrans-797
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[Ackley, D. H., Hinton, G. E., & Sejnowski, T. J.](#R_ref6_1) (1985). A learning
    algorithm for Boltzmann machines. Cognitive Science, *9*(2), 147–169.'
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Ackley, D. H., Hinton, G. E., & Sejnowski, T. J.](#R_ref6_1) (1985). Boltzmann机的学习算法。认知科学，*9*(2)，147–169。'
- en: '[Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri,
    S., Taropa, E., Bailey, P., Chen, Z., … & Saeta, B.](#R_ref6_2) (2023). PaLM 2
    technical report [Preprint]. [https://doi.org/10.48550/arXiv.2305.10403](https://doi.org/10.48550/arXiv.2305.10403)'
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri,
    S., Taropa, E., Bailey, P., Chen, Z., … & Saeta, B.](#R_ref6_2) (2023). PaLM 2
    技术报告 [预印本]。 [https://doi.org/10.48550/arXiv.2305.10403](https://doi.org/10.48550/arXiv.2305.10403)'
- en: '[Arute, F., Arya, K., Babbush, R., Bacon, D., Bardin, J. C., Barends, R., …
    & Martinis, J. M.](#R_ref6_3) (2019). Quantum supremacy using a programmable superconducting
    processor. Nature, *574*(7779), 505–510\. [https://doi.org/10.1038/s41586-019-1666-5](https://doi.org/10.1038/s41586-019-1666-5)'
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Arute, F., Arya, K., Babbush, R., Bacon, D., Bardin, J. C., Barends, R., …
    & Martinis, J. M.](#R_ref6_3) (2019). 使用可编程超导处理器实现量子霸权。自然，*574*(7779)，505–510。
    [https://doi.org/10.1038/s41586-019-1666-5](https://doi.org/10.1038/s41586-019-1666-5)'
- en: '[Asimov, I.](#R_ref6_4) (1950). Runaround. In The Isaac Asimov Collection (Ed.),
    I, Robot (p. 40). Doubleday.'
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Asimov, I.](#R_ref6_4) (1950). Runaround. In The Isaac Asimov Collection (Ed.),
    I, Robot (p. 40). Doubleday。'
- en: '[Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang,
    E., Cai, C., Terry, M., Le, Q., & Sutton, C.](#R_ref6_5) (2021). Program synthesis
    with large language models [Preprint]. [https://doi.org/10.48550/arXiv.2108.07732](https://doi.org/10.48550/arXiv.2108.07732)'
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang,
    E., Cai, C., Terry, M., Le, Q., & Sutton, C.](#R_ref6_5) (2021). 使用大型语言模型进行程序综合
    [预印本]. [https://doi.org/10.48550/arXiv.2108.07732](https://doi.org/10.48550/arXiv.2108.07732)'
- en: '[Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen,
    A., Goldie, A., Mirhoseini, A., McKinnon, C., Chen, C., Olsson, C., Olah, C.,
    Hernandez, D., Drain, D., Ganguli, D., Li, D., Tran-Johnson, E., Perez, E., Kerr,
    J., … Kaplan, J.](#R_ref6_6) (2022). Constitutional AI: Harmlessness from AI feedback
    [Preprint]. [https://doi.org/10.48550/arXiv.2212.08073](https://doi.org/10.48550/arXiv.2212.08073)'
  id: totrans-803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen,
    A., Goldie, A., Mirhoseini, A., McKinnon, C., Chen, C., Olsson, C., Olah, C.,
    Hernandez, D., Drain, D., Ganguli, D., Li, D., Tran-Johnson, E., Perez, E., Kerr,
    J., … Kaplan, J.](#R_ref6_6) (2022). 宪章 AI：从 AI 反馈中获得无害性 [预印本]. [https://doi.org/10.48550/arXiv.2212.08073](https://doi.org/10.48550/arXiv.2212.08073)'
- en: '[Bisk, Y., Zellers, R., Le Bras, R., Gao, J., & Choi, Y.](#R_ref6_7) (2020).
    PIQA: Reasoning about physical commonsense in natural language. In *AAAI 2020*.
    [https://doi.org/10.48550/arXiv.1911.11641](https://doi.org/10.48550/arXiv.1911.11641)'
  id: totrans-804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Bisk, Y., Zellers, R., Le Bras, R., Gao, J., & Choi, Y.](#R_ref6_7) (2020).
    PIQA: 在自然语言中推理物理常识. 在 *AAAI 2020*. [https://doi.org/10.48550/arXiv.1911.11641](https://doi.org/10.48550/arXiv.1911.11641)'
- en: '[Block, H. D., Knight, B. W. Jr., & Rosenblatt, F.](#R_ref6_8) (1962). Analysis
    of a four-layer series-coupled perceptron II. Reviews of Modern Physics, *34*(1),
    135\. [https://doi.org/10.1103/RevModPhys.34.135](https://doi.org/10.1103/RevModPhys.34.135)'
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Block, H. D., Knight, B. W. Jr., & Rosenblatt, F.](#R_ref6_8) (1962). 分析一个四层串联感知器
    II. 现代物理评论, *34*(1), 135\. [https://doi.org/10.1103/RevModPhys.34.135](https://doi.org/10.1103/RevModPhys.34.135)'
- en: '[Breiman, L.](#R_ref6_9) (2001). Random forests. Machine Learning, *45*(1),
    5–32\. [https://doi.org/10.1023/A:1010933404324](https://doi.org/10.1023/A:1010933404324)'
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Breiman, L.](#R_ref6_9) (2001). 随机森林. 机器学习, *45*(1), 5–32\. [https://doi.org/10.1023/A:1010933404324](https://doi.org/10.1023/A:1010933404324)'
- en: '[Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
    Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss,
    A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J.,
    Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B.,
    Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., & Amodei, D.](#R_ref6_10)
    (2020). Language models are few-shot learners. arXiv preprint. [https://doi.org/10.48550/arXiv.2005.14165](https://doi.org/10.48550/arXiv.2005.14165)'
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
    Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss,
    A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J.,
    Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B.,
    Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., & Amodei, D.](#R_ref6_10)
    (2020). 语言模型是少样本学习者. arXiv 预印本. [https://doi.org/10.48550/arXiv.2005.14165](https://doi.org/10.48550/arXiv.2005.14165)'
- en: '[Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. de O., Kaplan, J., Edwards,
    H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov,
    M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov,
    M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F. P.,
    Cummings, D., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss,
    W. H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S.,
    Jain, S., Saunders, W., Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra,
    V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K.,
    Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., & Zaremba,
    W.](#R_ref6_11) (2021). Evaluating large language models trained on code (Version
    v2) [Preprint]. [https://doi.org/10.48550/arXiv.2107.03374](https://doi.org/10.48550/arXiv.2107.03374)'
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. de O., Kaplan, J., Edwards,
    H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov,
    M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov,
    M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F. P.,
    Cummings, D., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss,
    W. H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S.,
    Jain, S., Saunders, W., Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra,
    V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K.,
    Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., & Zaremba,
    W.](#R_ref6_11) (2021). 评估在代码上训练的大型语言模型（版本 v2） [预印本]. [https://doi.org/10.48550/arXiv.2107.03374](https://doi.org/10.48550/arXiv.2107.03374)'
- en: '[Cheng, H.-T., Koc, L., Harmsen, J., Shaked, T., Chandra, T., Aradhye, H.,
    Anderson, G., Corrado, G., Chai, W., Ispir, M., Anil, R., Haque, Z., Hong, L.,
    Jain, V., Liu, X., & Shah, H.](#R_ref6_12) (2016). Wide & deep learning for recommender
    systems. arXiv:1606.07792 [cs.LG]. [https://doi.org/10.48550/arXiv.1606.07792](https://doi.org/10.48550/arXiv.1606.07792)'
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Cheng, H.-T., Koc, L., Harmsen, J., Shaked, T., Chandra, T., Aradhye, H.,
    Anderson, G., Corrado, G., Chai, W., Ispir, M., Anil, R., Haque, Z., Hong, L.,
    Jain, V., Liu, X., & Shah, H.](#R_ref6_12) (2016). 广度与深度学习在推荐系统中的应用. arXiv:1606.07792
    [cs.LG]. [https://doi.org/10.48550/arXiv.1606.07792](https://doi.org/10.48550/arXiv.1606.07792)'
- en: '[Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A.,
    Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., … & Fiedel, N.](#R_ref6_13)
    (2022). PaLM: Scaling language modeling with pathways [Preprint]. [https://doi.org/10.48550/arXiv.2204.02311](https://doi.org/10.48550/arXiv.2204.02311)'
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A.,
    Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., … & Fiedel, N.](#R_ref6_13)
    (2022). PaLM: 通过路径扩展语言模型 [预印本]. [https://doi.org/10.48550/arXiv.2204.02311](https://doi.org/10.48550/arXiv.2204.02311)'
- en: 'Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., & Toutanova,
    K. (2019). BoolQ: Exploring the surprising difficulty of natural yes/no questions.
    *NAACL 2019*. [https://doi.org/10.48550/arXiv.1905.10044](https://doi.org/10.48550/arXiv.1905.10044)'
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., & Toutanova,
    K. (2019). BoolQ: 探索自然是/否问题的惊人难度. *NAACL 2019*. [https://doi.org/10.48550/arXiv.1905.10044](https://doi.org/10.48550/arXiv.1905.10044)'
- en: '[Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C.,
    & Tafjord, O.](#R_ref6_15) (2018). Think you have solved question answering? Try
    ARC, the AI2 reasoning challenge. arXiv preprint arXiv:1803.05457\. [https://doi.org/10.48550/arXiv.1803.05457](https://doi.org/10.48550/arXiv.1803.05457)'
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C.,
    & Tafjord, O.](#R_ref6_15) (2018). 认为你的问答问题已经解决了？试试ARC，AI2推理挑战。arXiv预印本 arXiv:1803.05457\.
    [https://doi.org/10.48550/arXiv.1803.05457](https://doi.org/10.48550/arXiv.1803.05457)'
- en: '[Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert,
    M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., & Schulman, J.](#R_ref6_16)
    (2021). Training verifiers to solve math word problems (Version v2) [Preprint].
    [https://doi.org/10.48550/arXiv.2110.14168](https://doi.org/10.48550/arXiv.2110.14168)'
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert,
    M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., & Schulman, J.](#R_ref6_16)
    (2021). 训练验证者解决数学文字问题（版本v2） [预印本]. [https://doi.org/10.48550/arXiv.2110.14168](https://doi.org/10.48550/arXiv.2110.14168)'
- en: '[Feynman, R. P.](#R_ref6_17) (1982). Simulating physics with computers. International
    Journal of Theoretical Physics, *21*(6/7), 467–488.'
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Feynman, R. P.](#R_ref6_17) (1982). 用计算机模拟物理学. 国际理论物理学杂志, *21*(6/7), 467–488.'
- en: '[Friedman, J. H.](#R_ref6_18) (2002). Stochastic gradient boosting. Computational
    Statistics & Data Analysis, *38*(4), 367–378\. [https://doi.org/10.1016/S0167-9473(01)00065-2](https://doi.org/10.1016/S0167-9473(01)00065-2)'
  id: totrans-815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Friedman, J. H.](#R_ref6_18) (2002). 随机梯度提升. 计算统计学与数据分析, *38*(4), 367–378\.
    [https://doi.org/10.1016/S0167-9473(01)00065-2](https://doi.org/10.1016/S0167-9473(01)00065-2)'
- en: '[Future of Life Institute](#R_ref6_19). (2023a). An open letter: Pause giant
    AI experiments. Retrieved [insert date of retrieval here], from [https://futureoflife.org/open-letter/pause-giant-ai-experiments/](https://futureoflife.org)'
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[生命未来研究所](#R_ref6_19). (2023a). 一封公开信：暂停巨型AI实验. [插入检索日期]，来自 [https://futureoflife.org/open-letter/pause-giant-ai-experiments/](https://futureoflife.org)'
- en: '[Future of Life Institute](#R_ref6_20). (2023b). Policymaking in the pause.
    [https://futureoflife.org/wp-content/uploads/2023/04/FLI_Policymaking_In_The_Pause.pdf](https://futureoflife.org)'
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[生命未来研究所](#R_ref6_20). (2023b). 暂停时期的政策制定. [https://futureoflife.org/wp-content/uploads/2023/04/FLI_Policymaking_In_The_Pause.pdf](https://futureoflife.org)'
- en: '[Gehman, S., Gururangan, S., Sap, M., Choi, Y., & Smith, N. A.](#R_ref6_21)
    (2020). RealToxicityPrompts: Evaluating neural toxic degeneration in language
    models [Preprint]. [https://doi.org/10.48550/arXiv.2009.11462](https://doi.org/10.48550/arXiv.2009.11462)'
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Gehman, S., Gururangan, S., Sap, M., Choi, Y., & Smith, N. A.](#R_ref6_21)
    (2020). RealToxicityPrompts: 评估语言模型中的神经毒性退化 [预印本]. [https://doi.org/10.48550/arXiv.2009.11462](https://doi.org/10.48550/arXiv.2009.11462)'
- en: '[Goodfellow, I., Bengio, Y., & Courville, A.](#R_ref6_22) (2016). Deep learning.
    MIT Press. [http://www.deeplearningbook.org](http://www.deeplearningbook.org)'
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Goodfellow, I., Bengio, Y., & Courville, A.](#R_ref6_22) (2016). 深度学习. MIT出版社.
    [http://www.deeplearningbook.org](http://www.deeplearningbook.org)'
- en: '[Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D.,
    Ozair, S., Courville, A., & Bengio, Y.](#R_ref6_23) (2014). Generative adversarial
    networks arXiv preprint arXiv:1406.2661\. [https://doi.org/10.48550/arXiv.1406.2661](https://doi.org/10.48550/arXiv.1406.2661)'
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D.,
    Ozair, S., Courville, A., & Bengio, Y.](#R_ref6_23) (2014). 生成对抗网络 arXiv 预印本 arXiv:1406.2661\.
    [https://doi.org/10.48550/arXiv.1406.2661](https://doi.org/10.48550/arXiv.1406.2661)'
- en: '[Hebb, D. O.](#R_ref6_24) (1949). The organization of behavior: A neuropsychological
    theory. John Wiley & Sons; Chapman & Hall.'
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Hebb, D. O.](#R_ref6_24) (1949). 行为组织：一种神经心理学理论。John Wiley & Sons; Chapman
    & Hall.'
- en: '[Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., & Steinhardt,
    J.](#R_ref6_25) (2020). Measuring massive multitask language understanding [Preprint].
    [https://doi.org/10.48550/arXiv.2009.03300](https://doi.org/10.48550/arXiv.2009.03300)'
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., & Steinhardt,
    J.](#R_ref6_25) (2020). 使用MATH数据集测量大规模多任务语言理解 [预印本]。 [https://doi.org/10.48550/arXiv.2009.03300](https://doi.org/10.48550/arXiv.2009.03300)'
- en: '[Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song,
    D., & Steinhardt, J.](#R_ref6_26) (2021). Measuring mathematical problem solving
    with the MATH dataset (Version v2) [Preprint]. NeurIPS. [https://doi.org/10.48550/arXiv.2103.03874](https://doi.org/10.48550/arXiv.2103.03874)'
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song,
    D., & Steinhardt, J.](#R_ref6_26) (2021). 使用MATH数据集（版本v2）测量数学问题解决 [预印本]。NeurIPS。
    [https://doi.org/10.48550/arXiv.2103.03874](https://doi.org/10.48550/arXiv.2103.03874)'
- en: '[Hinton, G. E., Osindero, S., & Teh, Y-W.](#R_ref6_27) (2006). A fast learning
    algorithm for deep belief nets. Neural Computation, *18*(7), 1527–1554\. [https://doi.org/10.1162/neco.2006.18.7.1527](https://doi.org/10.1162/neco.2006.18.7.1527)'
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Hinton, G. E., Osindero, S., & Teh, Y-W.](#R_ref6_27) (2006). 一种快速学习深度信念网络的算法。神经计算，*18*(7)，1527–1554\.
    [https://doi.org/10.1162/neco.2006.18.7.1527](https://doi.org/10.1162/neco.2006.18.7.1527)'
- en: '[Hochreiter, S., & Schmidhuber, J.](#R_ref6_28) (1997). Long short-term memory.
    Neural Computation, *9*(8), 1735–1780\. [https://doi.org/10.1162/neco.1997.9.8.1735](https://doi.org/10.1162/neco.1997.9.8.1735)'
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Hochreiter, S., & Schmidhuber, J.](#R_ref6_28) (1997). 长短期记忆。神经计算，*9*(8)，1735–1780\.
    [https://doi.org/10.1162/neco.1997.9.8.1735](https://doi.org/10.1162/neco.1997.9.8.1735)'
- en: Hopfield, J. J. (1982). Neural networks and physical systems with emergent collective
    computational abilities. Proceedings of the National Academy of Sciences of the
    USA, *79*(8), 2554–2558.
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hopfield, J. J. (1982). 具有涌现集体计算能力的神经网络和物理系统。美国国家科学院院刊，*79*(8)，2554–2558.
- en: '[Joshi, M., Choi, E., Weld, D. S., & Zettlemoyer, L.](#R_ref6_30) (2017). TriviaQA:
    A large scale distantly supervised challenge dataset for reading comprehension
    (Version v2) [Preprint]. arXiv. [https://doi.org/10.48550/arXiv.1705.03551](https://doi.org/10.48550/arXiv.1705.03551)'
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Joshi, M., Choi, E., Weld, D. S., & Zettlemoyer, L.](#R_ref6_30) (2017). TriviaQA：用于阅读理解的远程监督大规模挑战数据集（版本v2）
    [预印本]。arXiv。 [https://doi.org/10.48550/arXiv.1705.03551](https://doi.org/10.48550/arXiv.1705.03551)'
- en: '[Joulin, A., Grave, E., Bojanowski, P., Douze, M., Jégou, H., & Mikolov, T.](#R_ref6_31)
    (2016). FastText.zip: Compressing text classification models. *arXiv:1612.03651
    [cs.CL]*. [https://doi.org/10.48550/arXiv.1612.03651](https://doi.org/10.48550/arXiv.1612.03651)'
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Joulin, A., Grave, E., Bojanowski, P., Douze, M., Jégou, H., & Mikolov, T.](#R_ref6_31)
    (2016). FastText.zip：压缩文本分类模型。*arXiv:1612.03651 [cs.CL]*。 [https://doi.org/10.48550/arXiv.1612.03651](https://doi.org/10.48550/arXiv.1612.03651)'
- en: '[Kaur, S., Singh, S., & Kaushal, S.](#R_ref6_32) (2021). Abusive content detection
    in online user-generated data: A survey. Procedia Computer Science. [https://doi.org/10.1016/j.procs.2021.05.098](https://doi.org/10.1016/j.procs.2021.05.098)'
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Kaur, S., Singh, S., & Kaushal, S.](#R_ref6_32) (2021). 在在线用户生成数据中检测侮辱性内容：综述。计算机科学进展。
    [https://doi.org/10.1016/j.procs.2021.05.098](https://doi.org/10.1016/j.procs.2021.05.098)'
- en: '[Krizhevsky, A., Sutskever, I., & Hinton, G. E.](#R_ref6_33) (2012). ImageNet
    classification with deep convolutional neural networks. Advances in Neural Information
    Processing Systems, *25*(2). [https://doi.org/10.1145/3065386](https://doi.org/10.1145/3065386)'
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Krizhevsky, A., Sutskever, I., & Hinton, G. E.](#R_ref6_33) (2012). 使用深度卷积神经网络进行ImageNet分类。神经信息处理系统进展，*25*(2)。
    [https://doi.org/10.1145/3065386](https://doi.org/10.1145/3065386)'
- en: '[Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti,
    C., Epstein, D., Polosukhin, I., Kelcey, M., Devlin, J., Lee, K., Toutanova, K.
    N., Jones, L., Chang, M.-W., Dai, A., Uszkoreit, J., Le, Q., & Petrov, S.](#R_ref6_34)
    (2019). Natural questions: A benchmark for question answering research. Transactions
    of the Association of Computational Linguistics, 7(15), 453–466\. [https://doi.org/10.1162/tacl_a_00276](https://doi.org/10.1162/tacl_a_00276)'
  id: totrans-831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti,
    C., Epstein, D., Polosukhin, I., Kelcey, M., Devlin, J., Lee, K., Toutanova, K.
    N., Jones, L., Chang, M.-W., Dai, A., Uszkoreit, J., Le, Q., & Petrov, S.](#R_ref6_34)
    (2019). 自然问题：问答研究基准. 计算语言学协会会刊，7(15), 453–466\. [https://doi.org/10.1162/tacl_a_00276](https://doi.org/10.1162/tacl_a_00276)'
- en: '[Lai, G., Xie, Q., Liu, H., Yang, Y., & Hovy, E.](#R_ref6_35) (2017). RACE:
    Large-scale reading comprehension dataset from examinations (Version v5) [Preprint].
    arXiv. [https://doi.org/10.48550/arXiv.1704.04683](https://doi.org/10.48550/arXiv.1704.04683)'
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Lai, G., Xie, Q., Liu, H., Yang, Y., & Hovy, E.](#R_ref6_35) (2017). RACE：来自考试的阅读理解大规模数据集（版本v5）
    [预印本]. arXiv. [https://doi.org/10.48550/arXiv.1704.04683](https://doi.org/10.48550/arXiv.1704.04683)'
- en: '[Le, Q. V., Ranzato, M., Monga, R., Devin, M., Chen, K., Corrado, G. S., Dean,
    J., & Ng, A. Y.](#R_ref6_36) (2011). Building high-level features using large
    scale unsupervised learning. arXiv:1112.6209\. [https://doi.org/10.48550/arXiv.1112.6209](https://doi.org/10.48550/arXiv.1112.6209)'
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Le, Q. V., Ranzato, M., Monga, R., Devin, M., Chen, K., Corrado, G. S., Dean,
    J., & Ng, A. Y.](#R_ref6_36) (2011). 使用大规模无监督学习构建高级特征. arXiv:1112.6209\. [https://doi.org/10.48550/arXiv.1112.6209](https://doi.org/10.48550/arXiv.1112.6209)'
- en: '[LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard,
    W., & Jackel, L. D.](#R_ref6_37) (1989). Backpropagation applied to handwritten
    zip code recognition. Neural Computation, *1*(4), 541–551.'
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard,
    W., & Jackel, L. D.](#R_ref6_37) (1989). 将反向传播应用于手写邮编识别. 神经计算，*1*(4), 541–551.'
- en: LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient-based learning
    applied to document recognition. Proceedings of the IEEE, *86*(11), 2278–2324\.
    [https://doi.org/10.1109/5.726791](https://doi.org/10.1109/5.726791)
  id: totrans-835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). 将基于梯度的学习方法应用于文档识别.
    IEEE汇刊，*86*(11), 2278–2324\. [https://doi.org/10.1109/5.726791](https://doi.org/10.1109/5.726791)
- en: '[Lin, S., Hilton, J., & Evans, O.](#R_ref6_39) (2022). TruthfulQA: Measuring
    how models mimic human falsehoods [Preprint]. [https://doi.org/10.48550/arXiv.2109.07958](https://doi.org/10.48550/arXiv.2109.07958)'
  id: totrans-836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Lin, S., Hilton, J., & Evans, O.](#R_ref6_39) (2022). TruthfulQA：衡量模型模仿人类错误的方式
    [预印本]. [https://doi.org/10.48550/arXiv.2109.07958](https://doi.org/10.48550/arXiv.2109.07958)'
- en: '[McCarthy, J., Minsky, M. L., Rochester, N., & Shannon, C. E.](#R_ref6_40)
    (1955, August 31). A proposal for the Dartmouth summer research project on artificial
    intelligence. Dartmouth College; Harvard University; I.B.M. Corporation; Bell
    Telephone Laboratories.'
  id: totrans-837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[McCarthy, J., Minsky, M. L., Rochester, N., & Shannon, C. E.](#R_ref6_40)
    (1955, August 31). 关于人工智能夏季研究项目的提案. 达特茅斯学院；哈佛大学；IBM公司；贝尔电话实验室。'
- en: 'McClelland, J. L., Rumelhart, D. E., & PDP Research Group. (1987). Parallel
    distributed processing: Explorations in the microstructure of cognition: Psychological
    and biological models (Vol. 2). The MIT Press. [https://doi.org/10.7551/mitpress/5237.001.0001](https://doi.org/10.7551/mitpress/5237.001.0001)'
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McClelland, J. L., Rumelhart, D. E., & PDP Research Group. (1987). 并行分布式处理：认知微观结构的探索：心理和生物模型（第2卷）.
    麻省理工学院出版社. [https://doi.org/10.7551/mitpress/5237.001.0001](https://doi.org/10.7551/mitpress/5237.001.0001)
- en: '[McCulloch, W. S., & Pitts, W.](#R_ref6_42) (1943). A logical calculus of the
    ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics, *5*(4),
    115–133\. [https://doi.org/10.1007/BF02478259](https://doi.org/10.1007/BF02478259)'
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[McCulloch, W. S., & Pitts, W.](#R_ref6_42) (1943). 神经活动中内在思想的逻辑演算. 数学生物物理学通报,
    *5*(4), 115–133\. [https://doi.org/10.1007/BF02478259](https://doi.org/10.1007/BF02478259)'
- en: '[Mihaylov, T., Clark, P., Khot, T., & Sabharwal, A.](#R_ref6_43) (2018). Can
    a suit of armor conduct electricity? A new dataset for open book question answering.
    arXiv preprint arXiv:1809.02789\. [https://doi.org/10.48550/arXiv.1809.02789](https://doi.org/10.48550/arXiv.1809.02789)'
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Mihaylov, T., Clark, P., Khot, T., & Sabharwal, A.](#R_ref6_43) (2018). 一套盔甲能导电吗？开放书问答的新数据集.
    arXiv预印本 arXiv:1809.02789\. [https://doi.org/10.48550/arXiv.1809.02789](https://doi.org/10.48550/arXiv.1809.02789)'
- en: 'Minsky, M., & Papert, S. A. (1969). Perceptrons: An introduction to computational
    geometry. MIT Press.'
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Minsky, M., & Papert, S. A. (1969). 感知器：计算几何导论. 麻省理工学院出版社。
- en: '[Minsky, M., & Papert, S. A.](#R_ref6_45) (1988). Perceptrons: An introduction
    to computational geometry (Expanded subsequent ed.). Massachusetts Institute of
    Technology.'
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Minsky, M., & Papert, S. A.](#R_ref6_45) (1988). 感知器：计算几何导论（扩展版）. 麻省理工学院出版社.'
- en: '[Nangia, N., Vania, C., Bhalerao, R., & Bowman, S. R.](#R_ref6_46) (2020).
    CrowS-Pairs: A challenge dataset for measuring social biases in masked language
    models [Preprint]. [https://doi.org/10.48550/arXiv.2010.00133](https://doi.org/10.48550/arXiv.2010.00133)'
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Nangia, N., Vania, C., Bhalerao, R., & Bowman, S. R.](#R_ref6_46) (2020).
    CrowS-Pairs：用于测量掩码语言模型中社会偏见的挑战数据集[预印本]. [https://doi.org/10.48550/arXiv.2010.00133](https://doi.org/10.48550/arXiv.2010.00133)'
- en: '[Oliva, A., & Torralba, A.](#R_ref6_47) (2001) Modeling the shape of the scene:
    A holistic representation of the spatial envelope. A. International Journal of
    Computer Vision, *42*(3), 145–175.'
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Oliva, A., & Torralba, A.](#R_ref6_47) (2001) 建模场景的形状：空间包络的整体表示. 国际计算机视觉杂志，*42*(3)，145–175.'
- en: '[OpenAI](#R_ref6_48). (2023). GPT-4 technical report [Preprint]. [https://doi.org/10.48550/arXiv.2303.08774](https://doi.org/10.48550/arXiv.2303.08774)'
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OpenAI](#R_ref6_48). (2023). GPT-4技术报告[预印本]. [https://doi.org/10.48550/arXiv.2303.08774](https://doi.org/10.48550/arXiv.2303.08774)'
- en: '[Patel, J. M.](#R_ref6_49) (2020). Introduction to common crawl datasets. In
    Getting structured data from the internet. Apress. [https://doi.org/10.1007/978-1-4842-6576-5_6](https://doi.org/10.1007/978-1-4842-6576-5_6)'
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Patel, J. M.](#R_ref6_49) (2020). 常见网络爬虫数据集简介. 在《从互联网获取结构化数据》中. Apress. [https://doi.org/10.1007/978-1-4842-6576-5_6](https://doi.org/10.1007/978-1-4842-6576-5_6)'
- en: '[Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
    G., Askell, A., Mishkin, P., Clark, J., Krueger, G., & Sutskever, I.](#R_ref6_50)
    (2021). Learning transferable visual models from natural language supervision.
    [https://doi.org/10.48550/arXiv.2103.00020](https://doi.org/10.48550/arXiv.2103.00020)'
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
    G., Askell, A., Mishkin, P., Clark, J., Krueger, G., & Sutskever, I.](#R_ref6_50)
    (2021). 从自然语言监督中学习可迁移的视觉模型. [https://doi.org/10.48550/arXiv.2103.00020](https://doi.org/10.48550/arXiv.2103.00020)'
- en: '[Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., & Sutskever,
    I.](#R_ref6_51) (2022). Robust speech recognition via large-scale weak supervision.
    arXiv preprint. [https://doi.org/10.48550/arXiv.2212.04356](https://doi.org/10.48550/arXiv.2212.04356)'
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., & Sutskever,
    I.](#R_ref6_51) (2022). 通过大规模弱监督实现鲁棒的语音识别. arXiv预印本. [https://doi.org/10.48550/arXiv.2212.04356](https://doi.org/10.48550/arXiv.2212.04356)'
- en: '[Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I.](#R_ref6_52) (2018).
    Improving language understanding by generative pre-training. OpenAI. Retrieved
    from [https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf](https://cdn.openai.com)'
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I.](#R_ref6_52) (2018).
    通过生成预训练改进语言理解. OpenAI. 从[https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf](https://cdn.openai.com)获取.'
- en: '[Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I.](#R_ref6_53)
    (2019). Language models are unsupervised multitask learners. OpenAI. Retrieved
    from [https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf](https://cdn.openai.com)'
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I.](#R_ref6_53)
    (2019). 语言模型是无监督的多任务学习者. OpenAI. 从[https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf](https://cdn.openai.com)获取.'
- en: '[Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
    Y., Li, W., & Liu, P. J.](#R_ref6_54) (2023). Exploring the limits of transfer
    learning with a unified text-to-text transformer (Version 4) [Preprint]. arXiv.
    [https://doi.org/10.48550/arXiv.1910.10683](https://doi.org/10.48550/arXiv.1910.10683)'
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
    Y., Li, W., & Liu, P. J.](#R_ref6_54) (2023). 使用统一的文本到文本转换器探索迁移学习的极限（版本4）[预印本].
    arXiv. [https://doi.org/10.48550/arXiv.1910.10683](https://doi.org/10.48550/arXiv.1910.10683)'
- en: '[Rajpurkar, P., Jia, R., & Liang, P.](#R_ref6_55) (2018). Know what you don’t
    know: Unanswerable questions for SQuAD (Version v1) [Preprint]. arXiv. [https://doi.org/10.48550/arXiv.1806.03822](https://doi.org/10.48550/arXiv.1806.03822)'
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Rajpurkar, P., Jia, R., & Liang, P.](#R_ref6_55) (2018). 知道你所不知道的：SQuAD的不可回答问题（版本v1）[预印本].
    arXiv. [https://doi.org/10.48550/arXiv.1806.03822](https://doi.org/10.48550/arXiv.1806.03822)'
- en: '[Rajpurkar, P., Zhang, J., Lopyrev, K., & Liang, P.](#R_ref6_56) (2016). SQuAD:
    100,000+ questions for machine comprehension of text (Version v3) [Preprint].
    arXiv. [https://doi.org/10.48550/arXiv.1606.05250](https://doi.org/10.48550/arXiv.1606.05250)'
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Rajpurkar, P., Zhang, J., Lopyrev, K., & Liang, P.](#R_ref6_56) (2016). SQuAD：用于文本机器理解的10万+问题（版本v3）[预印本].
    arXiv. [https://doi.org/10.48550/arXiv.1606.05250](https://doi.org/10.48550/arXiv.1606.05250)'
- en: '[Rosenblatt, F.](#R_ref6_57) (1957). The perceptron—A perceiving and recognizing
    automaton (Report No. 85-460-1). Cornell Aeronautical Laboratory.'
  id: totrans-854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Rosenblatt, F.](#R_ref6_57) (1957). 感知器——一种感知和识别的自动机（报告号 85-460-1）。康奈尔航空实验室。'
- en: '[Rosenblatt, F.](#R_ref6_58) (1958). The perceptron: A probabilistic model
    for information storage and organization in the brain. Psychological Review, *65*(6),
    386–408\. [https://doi.org/10.1037/h0042519](https://doi.org/10.1037/h0042519)'
  id: totrans-855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Rosenblatt, F.](#R_ref6_58) (1958). 感知器：大脑中信息存储和组织的一种概率模型。心理学评论，*65*(6)，386–408。[https://doi.org/10.1037/h0042519](https://doi.org/10.1037/h0042519)'
- en: '[Rudinger, R., Naradowsky, J., Leonard, B., & Van Durms, B.](#R_ref6_59) (2018).
    Gender bias in coreference resolution [Preprint]. [https://doi.org/10.48550/arXiv.1804.09301](https://doi.org/10.48550/arXiv.1804.09301)'
  id: totrans-856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Rudinger, R., Naradowsky, J., Leonard, B., & Van Durms, B.](#R_ref6_59) (2018).
    核心词消解中的性别偏见 [预印本]。[https://doi.org/10.48550/arXiv.1804.09301](https://doi.org/10.48550/arXiv.1804.09301)'
- en: '[Rumelhart, D. E., Hinton, G. E., & Williams, R. J.](#R_ref6_60) (1985). Learning
    internal representations by error propagation (ICS Report 8506). Institute for
    Cognitive Science, University of California San Diego.'
  id: totrans-857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Rumelhart, D. E., Hinton, G. E., & Williams, R. J.](#R_ref6_60) (1985). 通过误差传播学习内部表示（ICS报告
    8506）。加州大学圣地亚哥分校认知科学研究所。'
- en: '[Rumelhart, D. E., Hinton, G. E., & Williams, R. J.](#R_ref6_61) (1986a). Learning
    internal representations by error propagation. In D. E. Rumelhart, J. L. McClelland,
    and the PDP Research Group (Eds.), Parallel distributed processing: Explorations
    in the microstructure of cognition. Volume 1: Foundations (pp. 318–362). MIT Press.'
  id: totrans-858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Rumelhart, D. E., Hinton, G. E., & Williams, R. J.](#R_ref6_61) (1986a). 通过误差传播学习内部表示。在
    D. E. Rumelhart, J. L. McClelland 和 PDP 研究小组（编），并行分布式处理：认知微观结构的探索。第1卷：基础（第 318–362
    页）。麻省理工学院出版社。'
- en: '[Rumelhart, D. E., Hinton, G. E., & Williams, R. J.](#R_ref6_62) (1986b). Learning
    representations by back-propagating errors. Nature, *323*(6088), 533–536.'
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Rumelhart, D. E., Hinton, G. E., & Williams, R. J.](#R_ref6_62) (1986b). 通过反向传播错误学习表示。自然，*323*(6088)，533–536。'
- en: '[Rumelhart, D. E., McClelland, J. L.](#R_ref6_63), & PDP Research Group. (1986c).
    Parallel distributed processing: Explorations in the microstructure of cognition:
    Foundations (Vol. 1). The MIT Press. [https://doi.org/10.7551/mitpress/5236.001.0001](https://doi.org/10.7551/mitpress/5236.001.0001)'
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Rumelhart, D. E., McClelland, J. L.](#R_ref6_63)，与 PDP 研究小组。 (1986c). 并行分布式处理：认知微观结构的探索：基础（第
    1 卷）。麻省理工学院出版社。[https://doi.org/10.7551/mitpress/5236.001.0001](https://doi.org/10.7551/mitpress/5236.001.0001)'
- en: '[Russell, B., Torralba, A., Murphy, K., & Freeman, W. T.](#R_ref6_64) (2007).
    LabelMe: A database and web-based tool for image annotation. International Journal
    of Computer Vision, *77*(1–3), 157–173.'
  id: totrans-861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Russell, B., Torralba, A., Murphy, K., & Freeman, W. T.](#R_ref6_64) (2007).
    LabelMe：图像标注的数据库和基于网络的工具。国际计算机视觉杂志，*77*(1–3)，157–173。'
- en: '[Sakaguchi, K., Le Bras, R., Bhagavatula, C., & Choi, Y.](#R_ref6_65) (2019).
    WinoGrande: An adversarial Winograd schema challenge at scale. arXiv preprint
    arXiv:1907.10641\. [https://doi.org/10.48550/arXiv.1907.10641](https://doi.org/10.48550/arXiv.1907.10641)'
  id: totrans-862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Sakaguchi, K., Le Bras, R., Bhagavatula, C., & Choi, Y.](#R_ref6_65) (2019).
    WinoGrande：大规模的对抗性Winograd方案挑战。arXiv预印本 arXiv:1907.10641。[https://doi.org/10.48550/arXiv.1907.10641](https://doi.org/10.48550/arXiv.1907.10641)'
- en: '[Sap, M., Rashkin, H., Chen, D., LeBras, R., & Choi, Y.](#R_ref6_66) (2019).
    SocialIQA: Commonsense reasoning about social interactions. In *Proceedings of
    the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP
    2019)*. [https://arxiv.org/abs/1904.09728](https://arxiv.org)'
  id: totrans-863
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Sap, M., Rashkin, H., Chen, D., LeBras, R., & Choi, Y.](#R_ref6_66) (2019).
    社会推理IQA：关于社会互动的常识推理。在 *2019年实证自然语言处理会议（EMNLP 2019）论文集* 中。[https://arxiv.org/abs/1904.09728](https://arxiv.org)'
- en: '[Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti,
    M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., Schramowski, P., Kundurthy,
    S., Crowson, K., Schmidt, L., Kaczmarczyk, R., & Jitsev, J.](#R_ref6_67) (2022).
    LAION-5B: An open large-scale dataset for training next generation image-text
    models. [https://doi.org/10.48550/arXiv.2210.08402](https://doi.org/10.48550/arXiv.2210.08402)'
  id: totrans-864
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti,
    M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., Schramowski, P., Kundurthy,
    S., Crowson, K., Schmidt, L., Kaczmarczyk, R., & Jitsev, J.](#R_ref6_67) (2022).
    LAION-5B：用于训练下一代图像-文本模型的开放大规模数据集。[https://doi.org/10.48550/arXiv.2210.08402](https://doi.org/10.48550/arXiv.2210.08402)'
- en: '[Snoswell, A. J., & Burgess, J.](#R_ref6_68) (2022, November 29). The Galactica
    AI model was trained on scientific knowledge – But it spat out alarmingly plausible
    nonsense. The Conversation. [https://theconversation.com/the-galactica-ai-model-was-trained-on-scientific-knowledge-but-it-spat-out-alarmingly-plausible-nonsense-195445](https://theconversation.com)'
  id: totrans-865
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Snoswell, A. J., & Burgess, J.](#R_ref6_68) (2022, November 29). The Galactica
    AI model was trained on scientific knowledge – But it spat out alarmingly plausible
    nonsense. The Conversation。[https://theconversation.com/the-galactica-ai-model-was-trained-on-scientific-knowledge-but-it-spat-out-alarmingly-plausible-nonsense-195445](https://theconversation.com)'
- en: Tesauro, G. (1995). Temporal difference learning and TD-Gammon. Communications
    of the ACM, *38*(3), 58–68.
  id: totrans-866
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tesauro, G. (1995). 时序差分学习和TD-Gammon。ACM通讯，*38*(3)，58–68。
- en: '[Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng,
    H.-T., Jin, A., Bos, T., Baker, L., Du, Y., Li, Y., Lee, H., Zheng, H. S., Ghafouri,
    A., Menegali, M., Huang, Y., Krikun, M., Lepikhin, D., Qin, J., Chen, D., … Le,
    Q.](#R_ref6_70) (2022). LaMDA: Language models for dialog applications. arXiv
    preprint. [https://doi.org/10.48550/arXiv.2201.08239](https://doi.org/10.48550/arXiv.2201.08239)'
  id: totrans-867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng,
    H.-T., Jin, A., Bos, T., Baker, L., Du, Y., Li, Y., Lee, H., Zheng, H. S., Ghafouri,
    A., Menegali, M., Huang, Y., Krikun, M., Lepikhin, D., Qin, J., Chen, D., … Le,
    Q.](#R_ref6_70) (2022). LaMDA: 用于对话应用的语言模型。arXiv预印本。[https://doi.org/10.48550/arXiv.2201.08239](https://doi.org/10.48550/arXiv.2201.08239)'
- en: '[Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix,
    T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A.,
    Grave, E., & Lample, G.](#R_ref6_71) (2023a). LLaMA: Open and efficient foundation
    language models. arXiv preprint. [https://doi.org/10.48550/arXiv.2302.13971](https://doi.org/10.48550/arXiv.2302.13971)'
  id: totrans-868
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix,
    T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A.,
    Grave, E., & Lample, G.](#R_ref6_71) (2023a). LLaMA: 开放且高效的基座语言模型。arXiv预印本。[https://doi.org/10.48550/arXiv.2302.13971](https://doi.org/10.48550/arXiv.2302.13971)'
- en: '[Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y.,
    Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer,
    C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller,
    B., … Scialom, T.](#R_ref6_72) (2023b). Llama 2: Open foundation and fine-tuned
    chat models. arXiv preprint. [https://doi.org/10.48550/arXiv.2307.09288](https://doi.org/10.48550/arXiv.2307.09288)'
  id: totrans-869
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y.,
    Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer,
    C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller,
    B., … Scialom, T.](#R_ref6_72) (2023b). Llama 2：开放基座和微调聊天模型。arXiv预印本。[https://doi.org/10.48550/arXiv.2307.09288](https://doi.org/10.48550/arXiv.2307.09288)'
- en: '[Turing, A. M.](#R_ref6_73) (1950). Computing machinery and intelligence. Mind,
    *49*, 433–460.'
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Turing, A. M.](#R_ref6_73) (1950). 计算机与智能。Mind，*49*，433–460。'
- en: '[Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.
    N., Kaiser, L., & Polosukhin, I.](#R_ref6_74) (2017). Attention is all you need.
    arXiv preprint arXiv:1706.03762v7\. [https://doi.org/10.48550/arXiv.1706.03762](https://doi.org/10.48550/arXiv.1706.03762)'
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.
    N., Kaiser, L., & Polosukhin, I.](#R_ref6_74) (2017). 注意力即是所需。arXiv预印本 arXiv:1706.03762v7。[https://doi.org/10.48550/arXiv.1706.03762](https://doi.org/10.48550/arXiv.1706.03762)'
- en: '[Wahba, G., Lin, Y., Zhang, H. H., & Lee, Y.](#R_ref6_75) (2002). Support vector
    machines, reproducing Kernel Hilbert spaces and the randomized GACV. In B. Schölkopf
    & M. K. Warmuth (Eds.), Advances in large margin classifiers (pp. 1–69). MIT Press.'
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Wahba, G., Lin, Y., Zhang, H. H., & Lee, Y.](#R_ref6_75) (2002). 支持向量机，再生核希尔伯特空间和随机GACV。在B.
    Schölkopf & M. K. Warmuth (编)，大间隔分类器进展（第1-69页）。MIT出版社。'
- en: '[Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F.,
    Levy, O., & Bowman, S. R.](#R_ref6_76) (2019). SuperGLUE: A stickier benchmark
    for general-purpose language understanding systems. *NeurIPS 2019*. Retrieved
    from [super.gluebenchmark.com](http://super.gluebenchmark.com)'
  id: totrans-873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F.,
    Levy, O., & Bowman, S. R.](#R_ref6_76) (2019). SuperGLUE：通用语言理解系统的粘性基准。*NeurIPS
    2019*。从[super.gluebenchmark.com](http://super.gluebenchmark.com)检索。'
- en: '[Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., & Bowman, S. R.](#R_ref6_77)
    (2018). GLUE: A multi-task benchmark and analysis platform for natural language
    understanding. *ICLR 2019*. arXiv:1804.07461\. [https://gluebenchmark.com/](https://gluebenchmark.com)'
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., & Bowman, S. R.](#R_ref6_77)
    (2018). GLUE: 自然语言理解的多元任务基准和分析平台。*ICLR 2019*。arXiv:1804.07461。[https://gluebenchmark.com/](https://gluebenchmark.com)'
- en: '[Weizenbaum, J.](#R_ref6_78) (1976). Computer power and human reason: From
    judgment to calculation. W. H. Freeman.'
  id: totrans-875
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Weizenbaum, J.](#R_ref6_78) (1976). 计算机力量与人类理性：从判断到计算。W. H. Freeman出版社。'
- en: '[Wenzek, G., Lachaux, M.-A., Conneau, A., Chaudhary, V., Guzmán, F., Joulin,
    A., & Grave, E.](#R_ref6_79) (2019). CCNet: Extracting high quality monolingual
    datasets from web crawl data. [https://doi.org/10.48550/arXiv.1911.00359](https://doi.org/10.48550/arXiv.1911.00359)'
  id: totrans-876
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Wenzek, G., Lachaux, M.-A., Conneau, A., Chaudhary, V., Guzmán, F., Joulin,
    A., & Grave, E.](#R_ref6_79) (2019). CCNet：从网络爬取数据中提取高质量的单语数据集。[https://doi.org/10.48550/arXiv.1911.00359](https://doi.org/10.48550/arXiv.1911.00359)'
- en: '[Werbos, P. J.](#R_ref6_80) (1974). *Beyond Regression: New Tools for prediction
    and Analysis in the behavioral sciences* (Doctoral dissertation, Harvard University).'
  id: totrans-877
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Werbos, P. J.](#R_ref6_80) (1974). *《超越回归：行为科学预测和分析的新工具》（哈佛大学博士论文）*。'
- en: '[Widrow, B.](#R_ref6_81) (1960). *An adaptive “ADALINE: Neuron using chemical
    “MEMISTORS”* (Technical Report No. 1553-2).'
  id: totrans-878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Widrow, B.](#R_ref6_81) (1960). *《自适应“ADALINE：使用化学“MEMISTORS”的神经元”》（技术报告No.
    1553-2）*。'
- en: '[Widrow, B.](#R_ref6_82) (1962). Generalization and information storage in
    networks of Adaline ‘neurons’. In M. C. Yovitz, G. T. Jacobi, & G. Goldstein (Eds.),
    Self-organizing systems: Symposium proceedings (pp. 435–461). Spartan Books.'
  id: totrans-879
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Widrow, B.](#R_ref6_82) (1962). 网络Adaline“神经元”的泛化和信息存储。见 M. C. Yovitz, G.
    T. Jacobi, & G. Goldstein (编者)，自我组织系统：研讨会论文集（第435–461页）。Spartan Books。'
- en: '[Wiener, N.](#R_ref6_83) (1966). God & Golem, Inc.: A comment on certain points
    where cybernetics impinges on religion. The MIT Press.'
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Wiener, N.](#R_ref6_83) (1966). God & Golem, Inc.: A comment on certain points
    where cybernetics impinges on religion. The MIT Press。'
- en: '[Zellers, R., Bisk, Y., Schwartz, R., & Choi, Y.](#R_ref6_84) (2018). SWAG:
    A large-scale adversarial dataset for grounded commonsense inference. *Proceedings
    of the 2018 conference on empirical methods in natural language processing (EMNLP
    2018)*. [https://doi.org/10.48550/arXiv.1808.05326](https://doi.org/10.48550/arXiv.1808.05326)'
  id: totrans-881
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Zellers, R., Bisk, Y., Schwartz, R., & Choi, Y.](#R_ref6_84) (2018). SWAG：用于基于常识推理的大规模对抗数据集。*《2018年实证自然语言处理会议（EMNLP
    2018）论文集》*。[https://doi.org/10.48550/arXiv.1808.05326](https://doi.org/10.48550/arXiv.1808.05326)'
- en: '[Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., & Choi, Y.](#R_ref6_85)
    (2019). HellaSwag: Can a machine really finish your sentence? In *Proceedings
    of the 57th annual meeting of the association for computational linguistics (ACL
    2019)*. [https://doi.org/10.48550/arXiv.1905.07830](https://doi.org/10.48550/arXiv.1905.07830)'
  id: totrans-882
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., & Choi, Y.](#R_ref6_85)
    (2019). HellaSwag：机器真的能完成你的句子吗？见 *《第57届计算语言学协会年会（ACL 2019）论文集》*。[https://doi.org/10.48550/arXiv.1905.07830](https://doi.org/10.48550/arXiv.1905.07830)'
