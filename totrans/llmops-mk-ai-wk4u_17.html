<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>2.6 RAG Systems — Techniques for QA</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>2.6 RAG Systems — Techniques for QA</h1>
<blockquote>原文：<a href="https://boramorka.github.io/LLM-Book/en/CHAPTER-2/2.6%20RAG%20%E2%80%94%20Techniques%20for%20QA/">https://boramorka.github.io/LLM-Book/en/CHAPTER-2/2.6%20RAG%20%E2%80%94%20Techniques%20for%20QA/</a></blockquote>
                
                  


  
  



<p>Retrieval‑Augmented Generation (RAG) combines retrieval and generation, changing how we work with large corpora to build accurate QA systems and chatbots. A critical stage is feeding retrieved documents to the model along with the original query to generate an answer. After relevant materials are retrieved, they must be synthesized into a coherent answer that blends the content with the query’s context and leverages the model’s capabilities. The overall flow is simple: the system accepts a question; retrieves relevant fragments from a vector store; then feeds the retrieved content together with the question into an LLM to form an answer. By default, you can send all retrieved parts into context, but context‑window limits often lead to strategies like MapReduce, Refine, or Map‑Rerank — they aggregate or iteratively refine answers across many documents.</p>
<p>Before using an LLM for QA, ensure the environment is set up: imports, API keys, model versions, and so on.</p>
<div class="highlight"><pre><span/><code><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">dotenv</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dotenv</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">datetime</span>

<span class="c1"># Load environment variables and configure the OpenAI API key</span>
<span class="n">load_dotenv</span><span class="p">()</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">()</span>

<span class="c1"># Configure LLM versioning</span>
<span class="n">current_date</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span><span class="o">.</span><span class="n">date</span><span class="p">()</span>
<span class="n">llm_name</span> <span class="o">=</span> <span class="s2">"gpt-3.5-turbo"</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Using LLM version: </span><span class="si">{</span><span class="n">llm_name</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</code></pre></div>
<p>Next, retrieve documents relevant to the query from a vector database (VectorDB), where embeddings are stored.</p>
<div class="highlight"><pre><span/><code><span class="c1"># Import the vector store and embedding generator</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.vectorstores</span><span class="w"> </span><span class="kn">import</span> <span class="n">Chroma</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAIEmbeddings</span>

<span class="c1"># Directory where the vector database persists its data</span>
<span class="n">documents_storage_directory</span> <span class="o">=</span> <span class="s1">'docs/chroma/'</span>

<span class="c1"># Initialize the embedding generator using OpenAI embeddings</span>
<span class="n">embeddings_generator</span> <span class="o">=</span> <span class="n">OpenAIEmbeddings</span><span class="p">()</span>

<span class="c1"># Initialize the vector database with the persistence directory and embedding function</span>
<span class="n">vector_database</span> <span class="o">=</span> <span class="n">Chroma</span><span class="p">(</span><span class="n">persist_directory</span><span class="o">=</span><span class="n">documents_storage_directory</span><span class="p">,</span> <span class="n">embedding_function</span><span class="o">=</span><span class="n">embeddings_generator</span><span class="p">)</span>

<span class="c1"># Show the current number of documents in the vector database</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Documents in VectorDB: </span><span class="si">{</span><span class="n">vector_database</span><span class="o">.</span><span class="n">_collection</span><span class="o">.</span><span class="n">count</span><span class="p">()</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</code></pre></div>
<p><code>RetrievalQA</code> combines retrieval and generation: the LLM answers based on retrieved documents. First, initialize the language model,</p>
<div class="highlight"><pre><span/><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatOpenAI</span>

<span class="c1"># Initialize the chat model with the selected LLM</span>
<span class="n">language_model</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">llm_name</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div>
<p>then configure the RetrievalQA chain with a custom prompt,</p>
<div class="highlight"><pre><span/><code><span class="c1"># Import required LangChain modules</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.chains</span><span class="w"> </span><span class="kn">import</span> <span class="n">RetrievalQA</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.prompts</span><span class="w"> </span><span class="kn">import</span> <span class="n">PromptTemplate</span>

<span class="c1"># Create a custom prompt template to guide the LLM to use the provided context effectively</span>
<span class="n">custom_prompt_template</span> <span class="o">=</span> <span class="s2">"""To better assist with the inquiry, consider the details provided below as your reference...</span>
<span class="si">{context}</span>
<span class="s2">Inquiry: </span><span class="si">{question}</span>
<span class="s2">Insightful Response:"""</span>

<span class="c1"># Initialize the RetrievalQA chain with the custom prompt</span>
<span class="n">a_question_answering_chain</span> <span class="o">=</span> <span class="n">RetrievalQA</span><span class="o">.</span><span class="n">from_chain_type</span><span class="p">(</span>
    <span class="n">language_model</span><span class="p">,</span>
    <span class="n">retriever</span><span class="o">=</span><span class="n">vector_database</span><span class="o">.</span><span class="n">as_retriever</span><span class="p">(),</span>
    <span class="n">return_source_documents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">chain_type_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">"prompt"</span><span class="p">:</span> <span class="n">PromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="n">custom_prompt_template</span><span class="p">)}</span>
<span class="p">)</span>
</code></pre></div>
<p>and check the answer on a simple query.</p>
<div class="highlight"><pre><span/><code><span class="c1"># Provide a sample query</span>
<span class="n">query</span> <span class="o">=</span> <span class="s2">"Is probability a class topic?"</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">a_question_answering_chain</span><span class="p">({</span><span class="s2">"query"</span><span class="p">:</span> <span class="n">query</span><span class="p">})</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Answer:"</span><span class="p">,</span> <span class="n">response</span><span class="p">[</span><span class="s2">"result"</span><span class="p">])</span>
</code></pre></div>
<p>Next come advanced QA chain types. MapReduce and Refine help work around context‑window limits when handling many documents: MapReduce aggregates in parallel, while Refine improves the answer sequentially.</p>
<div class="highlight"><pre><span/><code><span class="c1"># Configure a QA chain to use MapReduce, aggregating answers from multiple documents</span>
<span class="n">question_answering_chain_map_reduce</span> <span class="o">=</span> <span class="n">RetrievalQA</span><span class="o">.</span><span class="n">from_chain_type</span><span class="p">(</span>
    <span class="n">language_model</span><span class="p">,</span>
    <span class="n">retriever</span><span class="o">=</span><span class="n">vector_database</span><span class="o">.</span><span class="n">as_retriever</span><span class="p">(),</span>
    <span class="n">chain_type</span><span class="o">=</span><span class="s2">"map_reduce"</span>
<span class="p">)</span>

<span class="c1"># Run MapReduce with the user query</span>
<span class="n">response_map_reduce</span> <span class="o">=</span> <span class="n">question_answering_chain_map_reduce</span><span class="p">({</span><span class="s2">"query"</span><span class="p">:</span> <span class="n">query</span><span class="p">})</span>

<span class="c1"># Show the aggregated answer</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"MapReduce answer:"</span><span class="p">,</span> <span class="n">response_map_reduce</span><span class="p">[</span><span class="s2">"result"</span><span class="p">])</span>

<span class="c1"># Configure a QA chain to use Refine, which iteratively improves the answer</span>
<span class="n">question_answering_chain_refine</span> <span class="o">=</span> <span class="n">RetrievalQA</span><span class="o">.</span><span class="n">from_chain_type</span><span class="p">(</span>
    <span class="n">language_model</span><span class="p">,</span>
    <span class="n">retriever</span><span class="o">=</span><span class="n">vector_database</span><span class="o">.</span><span class="n">as_retriever</span><span class="p">(),</span>
    <span class="n">chain_type</span><span class="o">=</span><span class="s2">"refine"</span>
<span class="p">)</span>

<span class="c1"># Run Refine with the same user query</span>
<span class="n">response_refine</span> <span class="o">=</span> <span class="n">question_answering_chain_refine</span><span class="p">({</span><span class="s2">"query"</span><span class="p">:</span> <span class="n">query</span><span class="p">})</span>

<span class="c1"># Show the refined answer</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Refine answer:"</span><span class="p">,</span> <span class="n">response_refine</span><span class="p">[</span><span class="s2">"result"</span><span class="p">])</span>
</code></pre></div>
<p>In practice, consider: choose between MapReduce and Refine based on the task (the former for fast aggregation from many sources; the latter for higher accuracy and iterative improvement); in distributed systems, performance depends on network latency and serialization; effectiveness varies with data, so experiment.</p>
<p>One notable limitation of RetrievalQA is the lack of dialogue history, which degrades handling of follow‑up questions. Demonstration of the limitation:</p>
<div class="highlight"><pre><span/><code><span class="c1"># Import a QA chain from a hypothetical library</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">some_library</span><span class="w"> </span><span class="kn">import</span> <span class="n">question_answering_chain</span> <span class="k">as</span> <span class="n">qa_chain</span>

<span class="c1"># Define an initial question related to course content</span>
<span class="n">initial_question_about_course_content</span> <span class="o">=</span> <span class="s2">"Does the curriculum cover probability theory?"</span>
<span class="c1"># Generate an answer to the initial question</span>
<span class="n">response_to_initial_question</span> <span class="o">=</span> <span class="n">qa_chain</span><span class="p">({</span><span class="s2">"query"</span><span class="p">:</span> <span class="n">initial_question_about_course_content</span><span class="p">})</span>

<span class="c1"># Define a follow‑up question without explicitly preserving conversation context</span>
<span class="n">follow_up_question_about_prerequisites</span> <span class="o">=</span> <span class="s2">"Why are those prerequisites important?"</span>
<span class="c1"># Generate an answer to the follow‑up question</span>
<span class="n">response_to_follow_up_question</span> <span class="o">=</span> <span class="n">qa_chain</span><span class="p">({</span><span class="s2">"query"</span><span class="p">:</span> <span class="n">follow_up_question_about_prerequisites</span><span class="p">})</span>

<span class="c1"># Display both answers — initial and follow‑up</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Answer to the initial question:"</span><span class="p">,</span> <span class="n">response_to_initial_question</span><span class="p">[</span><span class="s2">"result"</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Answer to the follow‑up question:"</span><span class="p">,</span> <span class="n">response_to_follow_up_question</span><span class="p">[</span><span class="s2">"result"</span><span class="p">])</span>
</code></pre></div>
<p>This underscores the importance of integrating conversation memory into RAG systems.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Advanced QA techniques in RAG deliver more dynamic and accurate answers. A careful <code>RetrievalQA</code> implementation and handling of its limitations enable building systems capable of substantive dialogue with users.</p>
<h2 id="further-reading">Further Reading</h2>
<ul>
<li>Explore the latest advances in LLMs and their impact on RAG.</li>
<li>Investigate strategies for integrating conversation memory into RAG frameworks.</li>
</ul>
<p>This chapter provides a foundation for understanding and practicing advanced QA techniques in RAG and for further innovation in AI interactions.</p>
<h2 id="theory-questions">Theory Questions</h2>
<ol>
<li>Name the three stages of QA in RAG.</li>
<li>What are context‑window limits, and how do MapReduce/Refine help work around them?</li>
<li>Why is a vector database (VectorDB) needed for retrieval in RAG?</li>
<li>How does <code>RetrievalQA</code> combine retrieval and generation?</li>
<li>Compare the MapReduce and Refine approaches.</li>
<li>Which practical factors matter in distributed systems (network latency, serialization)?</li>
<li>Why is it important to experiment with both approaches?</li>
<li>How does missing dialogue history affect handling of follow‑up questions?</li>
<li>Why integrate conversation memory into RAG?</li>
<li>What should be studied next to deepen RAG expertise?</li>
</ol>
<h2 id="practical-tasks">Practical Tasks</h2>
<ol>
<li>Initialize a vector DB (Chroma + OpenAIEmbeddings) and print the number of documents it contains.</li>
<li>Configure <code>RetrievalQA</code> with a custom prompt, specifying the model and the data storage directory.</li>
<li>Demonstrate <code>MapReduce</code> and <code>Refine</code> on a single query and print the resulting answers.</li>
<li>Simulate a follow‑up question without preserving dialogue context to show the <code>RetrievalQA</code> limitation.</li>
</ol>












                
                  
</body>
</html>