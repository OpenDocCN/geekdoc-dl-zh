<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>14  Training with luz</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>14  Training with luz</h1>
<blockquote>原文：<a href="https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/training_with_luz.html">https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/training_with_luz.html</a></blockquote>

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">

</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>At this point in the book, you know how to train a neural network. Truth be told, though, there’s some cognitive effort involved in having to remember the right execution order of steps like <code>optimizer$zero_grad()</code>, <code>loss$backward()</code>, and <code>optimizer$step()</code>. Also, in more complex scenarios than our running example, the list of things to actively remember gets longer.</p>
<p>One thing we haven’t talked about yet, for example, is how to handle the usual three stages of machine learning: training, validation, and testing. Another is the question of data flow between <em>devices</em> (CPU and GPU, if you have one). Both topics necessitate additional code to be introduced to the training loop. Writing this code can be tedious, and creates a potential for mistakes.</p>
<p>You can see exactly what I’m referring to in the appendix at the end of this chapter. But now, I want to focus on the remedy: a high-level, easy-to-use, concise way of organizing and instrumenting the training process, contributed by a package built on top of <code>torch</code>: <code>luz</code>.</p>
<section id="que-haya-luz---que-haja-luz---let-there-be-light" class="level2" data-number="14.1">
<h2 data-number="14.1" class="anchored" data-anchor-id="que-haya-luz---que-haja-luz---let-there-be-light"><span class="header-section-number">14.1</span> Que haya luz - Que haja luz - Let there be light</h2>
<p>A <em>torch</em> already brings some light, but sometimes in life, there is no <em>too bright</em>. <code>luz</code> was designed to make deep learning with <code>torch</code> as effortless as possible, while at the same time allowing for easy customization. In this chapter, we focus on the overall process; examples of customization will appear in later chapters.</p>
<p>For ease of comparison, we take our running example, and add a third version, now using <code>luz</code>. First, we “just” directly port the example; then, we adapt it to a more realistic scenario. In that scenario, we</p>
<ul>
<li><p>make use of separate training, validation, and test sets;</p></li>
<li><p>have <code>luz</code> compute <em>metrics</em> during training/validation;</p></li>
<li><p>illustrate the use of <em>callbacks</em> to perform custom actions or dynamically change hyper-parameters during training; and</p></li>
<li><p>explain what is going on with the aforementioned <em>devices</em>.</p></li>
</ul>
</section>
<section id="porting-the-toy-example" class="level2" data-number="14.2">
<h2 data-number="14.2" class="anchored" data-anchor-id="porting-the-toy-example"><span class="header-section-number">14.2</span> Porting the toy example</h2>
<section id="data" class="level3" data-number="14.2.1">
<h3 data-number="14.2.1" class="anchored" data-anchor-id="data"><span class="header-section-number">14.2.1</span> Data</h3>
<p><code>luz</code> does not just substantially transform the code required to train a neural network; it also adds flexibility on the data side of things. In addition to a reference to a <code>dataloader()</code>, its <code>fit()</code> method accepts <code>dataset()</code>s, tensors, and even R objects, as we’ll be able to verify soon.</p>
<p>We start by generating an R matrix and a vector, as before. This time though, we also wrap them in a <code>tensor_dataset()</code>, and instantiate a <code>dataloader()</code>. Instead of just 100, we now generate 1000 observations.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"/><span class="fu">library</span>(torch)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"/><span class="fu">library</span>(luz)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"/></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"/><span class="co"># input dimensionality (number of input features)</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"/>d_in <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"/><span class="co"># number of observations in training set</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"/>n <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"/></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"/>x <span class="ot">&lt;-</span> <span class="fu">torch_randn</span>(n, d_in)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"/>coefs <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.2</span>, <span class="sc">-</span><span class="fl">1.3</span>, <span class="sc">-</span><span class="fl">0.5</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"/>y <span class="ot">&lt;-</span> x<span class="sc">$</span><span class="fu">matmul</span>(coefs)<span class="sc">$</span><span class="fu">unsqueeze</span>(<span class="dv">2</span>) <span class="sc">+</span> <span class="fu">torch_randn</span>(n, <span class="dv">1</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"/></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"/>ds <span class="ot">&lt;-</span> <span class="fu">tensor_dataset</span>(x, y)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"/></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"/>dl <span class="ot">&lt;-</span> <span class="fu">dataloader</span>(ds, <span class="at">batch_size =</span> <span class="dv">100</span>, <span class="at">shuffle =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
</section>
<section id="model" class="level3" data-number="14.2.2">
<h3 data-number="14.2.2" class="anchored" data-anchor-id="model"><span class="header-section-number">14.2.2</span> Model</h3>
<p>To use <code>luz</code>, no changes are needed to the model definition. Note, though, that we just <em>define</em> the model architecture; we never actually <em>instantiate</em> a model object ourselves.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"/><span class="co"># dimensionality of hidden layer</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"/>d_hidden <span class="ot">&lt;-</span> <span class="dv">32</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"/><span class="co"># output dimensionality (number of predicted features)</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"/>d_out <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"/></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"/>net <span class="ot">&lt;-</span> <span class="fu">nn_module</span>(</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"/>  <span class="at">initialize =</span> <span class="cf">function</span>(d_in, d_hidden, d_out) {</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"/>    self<span class="sc">$</span>net <span class="ot">&lt;-</span> <span class="fu">nn_sequential</span>(</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"/>      <span class="fu">nn_linear</span>(d_in, d_hidden),</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"/>      <span class="fu">nn_relu</span>(),</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"/>      <span class="fu">nn_linear</span>(d_hidden, d_out)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"/>    )</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"/>  },</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"/>  <span class="at">forward =</span> <span class="cf">function</span>(x) {</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"/>    self<span class="sc">$</span><span class="fu">net</span>(x)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"/>  }</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"/>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
</section>
<section id="training" class="level3" data-number="14.2.3">
<h3 data-number="14.2.3" class="anchored" data-anchor-id="training"><span class="header-section-number">14.2.3</span> Training</h3>
<p>To train the model, we don’t write loops anymore. <code>luz</code> replaces the familiar <em>iterative</em> style by a <em>declarative</em> one: You tell <code>luz</code> what you want to happen, and like a docile sorcerer’s apprentice, it sets in motion the machinery.</p>
<p>Concretely, instruction happens in two – required – calls.</p>
<ol type="1">
<li>In <code>setup()</code>, you specify the loss function and the optimizer to use.</li>
<li>In <code>fit()</code>, you pass reference(s) to the training (and optionally, validation) data, as well as the number of epochs to train for.</li>
</ol>
<p>If the model is configurable – meaning, it accepts arguments to <code>initialize()</code> – a third method comes into play: <code>set_hparams()</code>, to be called in-between the other two. (That’s <code>hparams</code> for hyper-parameters.) Using this mechanism, you can easily experiment with, for example, different layer sizes, or other factors suspected to affect performance.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"/>fitted <span class="ot">&lt;-</span> net <span class="sc">%&gt;%</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"/>  <span class="fu">setup</span>(<span class="at">loss =</span> <span class="fu">nn_mse_loss</span>(), <span class="at">optimizer =</span> optim_adam) <span class="sc">%&gt;%</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"/>  <span class="fu">set_hparams</span>(</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"/>    <span class="at">d_in =</span> d_in,</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"/>    <span class="at">d_hidden =</span> d_hidden, <span class="at">d_out =</span> d_out</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"/>  ) <span class="sc">%&gt;%</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"/>  <span class="fu">fit</span>(dl, <span class="at">epochs =</span> <span class="dv">200</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>Running this code, you should see output approximately like this:</p>
<pre><code>Epoch 1/200
Train metrics: Loss: 3.0343                                                                               
Epoch 2/200
Train metrics: Loss: 2.5387                                                                               
Epoch 3/200
Train metrics: Loss: 2.2758                                                                               
...
...
Epoch 198/200
Train metrics: Loss: 0.891                                                                                
Epoch 199/200
Train metrics: Loss: 0.8879                                                                               
Epoch 200/200
Train metrics: Loss: 0.9036 </code></pre>
<p>Above, what we passed to <code>fit()</code> was the <code>dataloader()</code>. Let’s check that referencing the <code>dataset()</code> would have been just as fine:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"/>fitted <span class="ot">&lt;-</span> net <span class="sc">%&gt;%</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"/>  <span class="fu">setup</span>(<span class="at">loss =</span> <span class="fu">nn_mse_loss</span>(), <span class="at">optimizer =</span> optim_adam) <span class="sc">%&gt;%</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"/>  <span class="fu">set_hparams</span>(</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"/>    <span class="at">d_in =</span> d_in,</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"/>    <span class="at">d_hidden =</span> d_hidden, <span class="at">d_out =</span> d_out</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"/>  ) <span class="sc">%&gt;%</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"/>  <span class="fu">fit</span>(ds, <span class="at">epochs =</span> <span class="dv">200</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>Or even, <code>torch</code> tensors:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"/>fitted <span class="ot">&lt;-</span> net <span class="sc">%&gt;%</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"/>  <span class="fu">setup</span>(<span class="at">loss =</span> <span class="fu">nn_mse_loss</span>(), <span class="at">optimizer =</span> optim_adam) <span class="sc">%&gt;%</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"/>  <span class="fu">set_hparams</span>(</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"/>    <span class="at">d_in =</span> d_in,</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"/>    <span class="at">d_hidden =</span> d_hidden, <span class="at">d_out =</span> d_out</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"/>  ) <span class="sc">%&gt;%</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"/>  <span class="fu">fit</span>(<span class="fu">list</span>(x, y), <span class="at">epochs =</span> <span class="dv">200</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>And finally, R objects, which can be convenient when we aren’t already working with tensors.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"/>fitted <span class="ot">&lt;-</span> net <span class="sc">%&gt;%</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"/>  <span class="fu">setup</span>(<span class="at">loss =</span> <span class="fu">nn_mse_loss</span>(), <span class="at">optimizer =</span> optim_adam) <span class="sc">%&gt;%</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"/>  <span class="fu">set_hparams</span>(</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"/>    <span class="at">d_in =</span> d_in,</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"/>    <span class="at">d_hidden =</span> d_hidden, <span class="at">d_out =</span> d_out</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"/>  ) <span class="sc">%&gt;%</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"/>  <span class="fu">fit</span>(<span class="fu">list</span>(<span class="fu">as.matrix</span>(x), <span class="fu">as.matrix</span>(y)), <span class="at">epochs =</span> <span class="dv">200</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>In the following sections, we’ll always be working with <code>dataloader()</code>s; but in some cases those “shortcuts” may come in handy.</p>
<p>Next, we extend the toy example, illustrating how to address more complex requirements.</p>
</section>
</section>
<section id="a-more-realistic-scenario" class="level2" data-number="14.3">
<h2 data-number="14.3" class="anchored" data-anchor-id="a-more-realistic-scenario"><span class="header-section-number">14.3</span> A more realistic scenario</h2>
<section id="integrating-training-validation-and-test" class="level3" data-number="14.3.1">
<h3 data-number="14.3.1" class="anchored" data-anchor-id="integrating-training-validation-and-test"><span class="header-section-number">14.3.1</span> Integrating training, validation, and test</h3>
<p>In deep learning, training and validation phases are interleaved. Every epoch of training is followed by an epoch of validation. Importantly, the data used in both phases have to be strictly disjoint.</p>
<p>In each training phase, gradients are computed and weights are changed; during validation, none of that happens. Why have a validation set, then? If, for each epoch, we compute task-relevant metrics for both partitions, we can see if we are <em>overfitting</em> to the training data: that is, drawing conclusions based on training sample specifics not descriptive of the overall population we want to model. All we have to do is two things: instruct <code>luz</code> to compute a suitable metric, and pass it an additional <code>dataloader</code> pointing to the validation data.</p>
<p>The former is done in <code>setup()</code>, and for a regression task, common choices are mean squared or mean absolute error (MSE or MAE, resp.). As we’re already using MSE as our loss, let’s choose MAE for a metric:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"/>fitted <span class="ot">&lt;-</span> net <span class="sc">%&gt;%</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"/>  <span class="fu">setup</span>(</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"/>    <span class="at">loss =</span> <span class="fu">nn_mse_loss</span>(),</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"/>    <span class="at">optimizer =</span> optim_adam,</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"/>    <span class="at">metrics =</span> <span class="fu">list</span>(<span class="fu">luz_metric_mae</span>())</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"/>  ) <span class="sc">%&gt;%</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"/>  <span class="fu">fit</span>(...)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>The validation <code>dataloader</code> is passed in <code>fit()</code> – but to be able to reference it, we need to construct it first! So now (anticipating we’ll want to have a test set, too), we split up the original 1000 observations into three partitions, creating a <code>dataset</code> and a <code>dataloader</code> for each of them.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"/>train_ids <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(ds), <span class="at">size =</span> <span class="fl">0.6</span> <span class="sc">*</span> <span class="fu">length</span>(ds))</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"/>valid_ids <span class="ot">&lt;-</span> <span class="fu">sample</span>(</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"/>  <span class="fu">setdiff</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(ds), train_ids),</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"/>  <span class="at">size =</span> <span class="fl">0.2</span> <span class="sc">*</span> <span class="fu">length</span>(ds)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"/>)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"/>test_ids <span class="ot">&lt;-</span> <span class="fu">setdiff</span>(</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"/>  <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(ds),</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"/>  <span class="fu">union</span>(train_ids, valid_ids)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"/>)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"/></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"/>train_ds <span class="ot">&lt;-</span> <span class="fu">dataset_subset</span>(ds, <span class="at">indices =</span> train_ids)</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"/>valid_ds <span class="ot">&lt;-</span> <span class="fu">dataset_subset</span>(ds, <span class="at">indices =</span> valid_ids)</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"/>test_ds <span class="ot">&lt;-</span> <span class="fu">dataset_subset</span>(ds, <span class="at">indices =</span> test_ids)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"/></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"/>train_dl <span class="ot">&lt;-</span> <span class="fu">dataloader</span>(train_ds,</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"/>  <span class="at">batch_size =</span> <span class="dv">100</span>, <span class="at">shuffle =</span> <span class="cn">TRUE</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"/>)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"/>valid_dl <span class="ot">&lt;-</span> <span class="fu">dataloader</span>(valid_ds, <span class="at">batch_size =</span> <span class="dv">100</span>)</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"/>test_dl <span class="ot">&lt;-</span> <span class="fu">dataloader</span>(test_ds, <span class="at">batch_size =</span> <span class="dv">100</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>Now, we are ready to start the enhanced workflow:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"/>fitted <span class="ot">&lt;-</span> net <span class="sc">%&gt;%</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"/>  <span class="fu">setup</span>(</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"/>    <span class="at">loss =</span> <span class="fu">nn_mse_loss</span>(),</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"/>    <span class="at">optimizer =</span> optim_adam,</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"/>    <span class="at">metrics =</span> <span class="fu">list</span>(<span class="fu">luz_metric_mae</span>())</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"/>  ) <span class="sc">%&gt;%</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"/>  <span class="fu">set_hparams</span>(</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"/>    <span class="at">d_in =</span> d_in,</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"/>    <span class="at">d_hidden =</span> d_hidden, <span class="at">d_out =</span> d_out</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"/>  ) <span class="sc">%&gt;%</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"/>  <span class="fu">fit</span>(train_dl, <span class="at">epochs =</span> <span class="dv">200</span>, <span class="at">valid_data =</span> valid_dl)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<pre><code>Epoch 1/200
Train metrics: Loss: 2.5863 - MAE: 1.2832                                       
Valid metrics: Loss: 2.487 - MAE: 1.2365
Epoch 2/200
Train metrics: Loss: 2.4943 - MAE: 1.26                                          
Valid metrics: Loss: 2.4049 - MAE: 1.2161
Epoch 3/200
Train metrics: Loss: 2.4036 - MAE: 1.236                                         
Valid metrics: Loss: 2.3261 - MAE: 1.1962
...
...
Epoch 198/200
Train metrics: Loss: 0.8947 - MAE: 0.7504
Valid metrics: Loss: 1.0572 - MAE: 0.8287
Epoch 199/200
Train metrics: Loss: 0.8948 - MAE: 0.7503
Valid metrics: Loss: 1.0569 - MAE: 0.8286
Epoch 200/200
Train metrics: Loss: 0.8944 - MAE: 0.75
Valid metrics: Loss: 1.0579 - MAE: 0.8292</code></pre>
<p>Even though both training and validation sets come from the exact same distribution, we do see a bit of overfitting. This is a topic we’ll talk about more in the next chapter.</p>
<p>Once training has finished, the <code>fitted</code> object above holds a history of epoch-wise metrics, as well as references to a number of important objects involved in the training process. Among the latter is the fitted model itself – which enables an easy way to obtain predictions on the test set:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"/>fitted <span class="sc">%&gt;%</span> <span class="fu">predict</span>(test_dl)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<pre><code>torch_tensor
 0.7799
 1.7839
-1.1294
-1.3002
-1.8169
-1.6762
-0.7548
-1.2041
 2.9613
-0.9551
 0.7714
-0.8265
 1.1334
-2.8406
-1.1679
 0.8350
 2.0134
 2.1083
 1.4093
 0.6962
-0.3669
-0.5292
 2.0310
-0.5814
 2.7494
 0.7855
-0.5263
-1.1257
-3.3117
 0.6157
... [the output was truncated (use n=-1 to disable)]
[ CPUFloatType{200,1} ]</code></pre>
<p>We also want to evaluate performance on the test set:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"/>fitted <span class="sc">%&gt;%</span> <span class="fu">evaluate</span>(test_dl)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<pre><code>A `luz_module_evaluation`
── Results 
loss: 0.9271
mae: 0.7348</code></pre>
<p>This workflow of: training and validation in lock-step, then checking and extracting predictions on the test set is something we’ll encounter times and again in this book.</p>
</section>
<section id="using-callbacks-to-hook-into-the-training-process" class="level3" data-number="14.3.2">
<h3 data-number="14.3.2" class="anchored" data-anchor-id="using-callbacks-to-hook-into-the-training-process"><span class="header-section-number">14.3.2</span> Using callbacks to “hook” into the training process</h3>
<p>At this point, you may feel that what we’ve gained in code efficiency, we may have lost in flexibility. Coding the training loop yourself, you can arrange for all kinds of things to happen: save model weights, adjust the learning rate … whatever you need.</p>
<p>In reality, no flexibility is lost. Instead, <code>luz</code> offers a standardized way to achieve the same goals: callbacks. Callbacks are objects that can execute arbitrary R code, at any of the following points in time:</p>
<ul>
<li><p>when the overall training process starts or ends (<code>on_fit_begin()</code> / <code>on_fit_end()</code>);</p></li>
<li><p>when an epoch (comprising training and validation) starts or ends (<code>on_epoch_begin()</code> / <code>on_epoch_end()</code>);</p></li>
<li><p>when during an epoch, the training (validation, resp.) phase starts or ends (<code>on_train_begin()</code> / <code>on_train_end()</code>; <code>on_valid_begin()</code> / <code>on_valid_end()</code>);</p></li>
<li><p>when during training (validation, resp.), a new batch is either about to be or has been processed (<code>on_train_batch_begin()</code> / <code>on_train_batch_end()</code>; <code>on_valid_batch_begin()</code> / <code>on_valid_batch_end()</code>);</p></li>
<li><p>and even at specific landmarks inside the “innermost” training / validation logic, such as “after loss computation”, “after <code>backward()</code>” or “after <code>step()</code>”.</p></li>
</ul>
<p>While you can implement any logic you wish using callbacks (and we’ll see how to do this in a later chapter), <code>luz</code> already comes equipped with a very useful set. For example:</p>
<ul>
<li><p><code>luz_callback_model_checkpoint()</code> saves model weights after every epoch (or just in case of improvements, if so instructed).</p></li>
<li><p><code>luz_callback_lr_scheduler()</code> activates one of <code>torch</code>’s <em>learning rate schedulers</em>. Different scheduler objects exist, each following their own logic in dynamically updating the learning rate.</p></li>
<li><p><code>luz_callback_early_stopping()</code> terminates training once model performance stops to improve. What exactly “stops to improve” should mean is configurable by the user.</p></li>
</ul>
<p>Callbacks are passed to the <code>fit()</code> method in a list. For example, augmenting our most recent workflow:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"/>fitted <span class="ot">&lt;-</span> net <span class="sc">%&gt;%</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"/>  <span class="fu">setup</span>(</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"/>    <span class="at">loss =</span> <span class="fu">nn_mse_loss</span>(),</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"/>    <span class="at">optimizer =</span> optim_adam,</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"/>    <span class="at">metrics =</span> <span class="fu">list</span>(<span class="fu">luz_metric_mae</span>())</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"/>  ) <span class="sc">%&gt;%</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"/>  <span class="fu">set_hparams</span>(<span class="at">d_in =</span> d_in,</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"/>              <span class="at">d_hidden =</span> d_hidden,</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"/>              <span class="at">d_out =</span> d_out) <span class="sc">%&gt;%</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"/>  <span class="fu">fit</span>(</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"/>    train_dl,</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"/>    <span class="at">epochs =</span> <span class="dv">200</span>,</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"/>    <span class="at">valid_data =</span> valid_dl,</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"/>    <span class="at">callbacks =</span> <span class="fu">list</span>(</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"/>      <span class="fu">luz_callback_model_checkpoint</span>(<span class="at">path =</span> <span class="st">"./models/"</span>,</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"/>                                    <span class="at">save_best_only =</span> <span class="cn">TRUE</span>),</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"/>      <span class="fu">luz_callback_early_stopping</span>(<span class="at">patience =</span> <span class="dv">10</span>)</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"/>    )</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"/>  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>With this configuration, weights will be saved, but only if validation loss decreases. Training will halt if there is no improvement (again, in validation loss) for ten epochs. With both callbacks, you can pick any other metric to base the decision on, and the metric in question may also refer to the training set.</p>
<p>Here, we see early stopping happening after 111 epochs:</p>
<pre><code>Epoch 1/200
Train metrics: Loss: 2.5803 - MAE: 1.2547
Valid metrics: Loss: 3.3763 - MAE: 1.4232
Epoch 2/200
Train metrics: Loss: 2.4767 - MAE: 1.229
Valid metrics: Loss: 3.2334 - MAE: 1.3909
...
...
Epoch 110/200
Train metrics: Loss: 1.011 - MAE: 0.8034
Valid metrics: Loss: 1.1673 - MAE: 0.8578
Epoch 111/200
Train metrics: Loss: 1.0108 - MAE: 0.8032
Valid metrics: Loss: 1.167 - MAE: 0.8578
Early stopping at epoch 111 of 200</code></pre>
</section>
<section id="how-luz-helps-with-devices" class="level3" data-number="14.3.3">
<h3 data-number="14.3.3" class="anchored" data-anchor-id="how-luz-helps-with-devices"><span class="header-section-number">14.3.3</span> How <code>luz</code> helps with devices</h3>
<p>Finally, let’s quickly mention how <code>luz</code> helps with device placement. Devices, in a usual environment, are the CPU and perhaps, if available, a GPU. For training, data and model weights need to be located on the same device. This can introduce complexities, and – at the very least – necessitates additional code to keep all pieces in sync.</p>
<p>With <code>luz</code>, related actions happen transparently to the user. Let’s take the prediction step from above:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"/>fitted <span class="sc">%&gt;%</span> <span class="fu">predict</span>(test_dl)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>In case this code was executed on a machine that has a GPU, <code>luz</code> will have detected that, and the model’s weight tensors will already have been moved there. Now, for the above call to <code>predict()</code>, what happened “under the hood” was the following:</p>
<ul>
<li><code>luz</code> put the model in evaluation mode, making sure that weights are not updated.</li>
<li><code>luz</code> moved the test data to the GPU, batch by batch, and obtained model predictions.</li>
<li>These predictions were then moved back to the CPU, in anticipation of the caller wanting to process them further with R. (Conversion functions like <code>as.numeric()</code>, <code>as.matrix()</code> etc. can only act on CPU-resident tensors.)</li>
</ul>
<p>In the below appendix, you find a complete walk-through of how to implement the train-validate-test workflow by hand. You’ll likely find this a lot more complex than what we did above – and it does not even bring into play metrics, or any of the functionality afforded by <code>luz</code> callbacks.</p>
<p>In the next chapter, we discuss essential ingredients of modern deep learning we haven’t yet touched upon; and following that, we look at specific architectures destined to specifically handle different tasks and domains.</p>
</section>
</section>
<section id="appendix-a-train-validate-test-workflow-implemented-by-hand" class="level2" data-number="14.4">
<h2 data-number="14.4" class="anchored" data-anchor-id="appendix-a-train-validate-test-workflow-implemented-by-hand"><span class="header-section-number">14.4</span> Appendix: A train-validate-test workflow implemented by hand</h2>
<p>For clarity, we repeat here the two things that do <em>not</em> depend on whether you’re using <code>luz</code> or not: <code>dataloader()</code> preparation and model definition.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"/><span class="co"># input dimensionality (number of input features)</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"/>d_in <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"/><span class="co"># number of observations in training set</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"/>n <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"/></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"/>x <span class="ot">&lt;-</span> <span class="fu">torch_randn</span>(n, d_in)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"/>coefs <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.2</span>, <span class="sc">-</span><span class="fl">1.3</span>, <span class="sc">-</span><span class="fl">0.5</span>)</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"/>y <span class="ot">&lt;-</span> x<span class="sc">$</span><span class="fu">matmul</span>(coefs)<span class="sc">$</span><span class="fu">unsqueeze</span>(<span class="dv">2</span>) <span class="sc">+</span> <span class="fu">torch_randn</span>(n, <span class="dv">1</span>)</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"/></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"/>ds <span class="ot">&lt;-</span> <span class="fu">tensor_dataset</span>(x, y)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"/></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"/>dl <span class="ot">&lt;-</span> <span class="fu">dataloader</span>(ds, <span class="at">batch_size =</span> <span class="dv">100</span>, <span class="at">shuffle =</span> <span class="cn">TRUE</span>)</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"/></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"/>train_ids <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(ds), <span class="at">size =</span> <span class="fl">0.6</span> <span class="sc">*</span> <span class="fu">length</span>(ds))</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"/>valid_ids <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">setdiff</span>(</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"/>  <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(ds),</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"/>  train_ids</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"/>), <span class="at">size =</span> <span class="fl">0.2</span> <span class="sc">*</span> <span class="fu">length</span>(ds))</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"/>test_ids <span class="ot">&lt;-</span> <span class="fu">setdiff</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(ds), <span class="fu">union</span>(train_ids, valid_ids))</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"/></span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"/>train_ds <span class="ot">&lt;-</span> <span class="fu">dataset_subset</span>(ds, <span class="at">indices =</span> train_ids)</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"/>valid_ds <span class="ot">&lt;-</span> <span class="fu">dataset_subset</span>(ds, <span class="at">indices =</span> valid_ids)</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"/>test_ds <span class="ot">&lt;-</span> <span class="fu">dataset_subset</span>(ds, <span class="at">indices =</span> test_ids)</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"/></span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"/>train_dl <span class="ot">&lt;-</span> <span class="fu">dataloader</span>(train_ds,</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"/>  <span class="at">batch_size =</span> <span class="dv">100</span>,</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"/>  <span class="at">shuffle =</span> <span class="cn">TRUE</span></span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"/>)</span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"/>valid_dl <span class="ot">&lt;-</span> <span class="fu">dataloader</span>(valid_ds, <span class="at">batch_size =</span> <span class="dv">100</span>)</span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"/>test_dl <span class="ot">&lt;-</span> <span class="fu">dataloader</span>(test_ds, <span class="at">batch_size =</span> <span class="dv">100</span>)</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"/></span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"/><span class="co"># dimensionality of hidden layer</span></span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"/>d_hidden <span class="ot">&lt;-</span> <span class="dv">32</span></span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"/><span class="co"># output dimensionality (number of predicted features)</span></span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"/>d_out <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"/></span>
<span id="cb19-37"><a href="#cb19-37" aria-hidden="true" tabindex="-1"/>net <span class="ot">&lt;-</span> <span class="fu">nn_module</span>(</span>
<span id="cb19-38"><a href="#cb19-38" aria-hidden="true" tabindex="-1"/>  <span class="at">initialize =</span> <span class="cf">function</span>(d_in, d_hidden, d_out) {</span>
<span id="cb19-39"><a href="#cb19-39" aria-hidden="true" tabindex="-1"/>    self<span class="sc">$</span>net <span class="ot">&lt;-</span> <span class="fu">nn_sequential</span>(</span>
<span id="cb19-40"><a href="#cb19-40" aria-hidden="true" tabindex="-1"/>      <span class="fu">nn_linear</span>(d_in, d_hidden),</span>
<span id="cb19-41"><a href="#cb19-41" aria-hidden="true" tabindex="-1"/>      <span class="fu">nn_relu</span>(),</span>
<span id="cb19-42"><a href="#cb19-42" aria-hidden="true" tabindex="-1"/>      <span class="fu">nn_linear</span>(d_hidden, d_out)</span>
<span id="cb19-43"><a href="#cb19-43" aria-hidden="true" tabindex="-1"/>    )</span>
<span id="cb19-44"><a href="#cb19-44" aria-hidden="true" tabindex="-1"/>  },</span>
<span id="cb19-45"><a href="#cb19-45" aria-hidden="true" tabindex="-1"/>  <span class="at">forward =</span> <span class="cf">function</span>(x) {</span>
<span id="cb19-46"><a href="#cb19-46" aria-hidden="true" tabindex="-1"/>    self<span class="sc">$</span><span class="fu">net</span>(x)</span>
<span id="cb19-47"><a href="#cb19-47" aria-hidden="true" tabindex="-1"/>  }</span>
<span id="cb19-48"><a href="#cb19-48" aria-hidden="true" tabindex="-1"/>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>Recall that with <code>luz</code>, now all that separates you from watching how training and validation losses evolve is a snippet like this:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"/>fitted <span class="ot">&lt;-</span> net <span class="sc">%&gt;%</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"/>  <span class="fu">setup</span>(</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"/>    <span class="at">loss =</span> <span class="fu">nn_mse_loss</span>(),</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"/>    <span class="at">optimizer =</span> optim_adam</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"/>  ) <span class="sc">%&gt;%</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"/>  <span class="fu">set_hparams</span>(</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"/>    <span class="at">d_in =</span> d_in,</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"/>    <span class="at">d_hidden =</span> d_hidden, <span class="at">d_out =</span> d_out</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"/>  ) <span class="sc">%&gt;%</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"/>  <span class="fu">fit</span>(train_dl, <span class="at">epochs =</span> <span class="dv">200</span>, <span class="at">valid_data =</span> valid_dl)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>Without <code>luz</code>, however, things to be taken care of fall into three distinct categories.</p>
<p>First, instantiate the network, and, if CUDA is installed, move its weights to the GPU.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"/>device <span class="ot">&lt;-</span> <span class="fu">torch_device</span>(<span class="cf">if</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"/>(<span class="fu">cuda_is_available</span>()) {</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"/>  <span class="st">"cuda"</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"/>} <span class="cf">else</span> {</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"/>  <span class="st">"cpu"</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"/>})</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"/></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"/>model <span class="ot">&lt;-</span> <span class="fu">net</span>(<span class="at">d_in =</span> d_in, <span class="at">d_hidden =</span> d_hidden, <span class="at">d_out =</span> d_out)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"/>model <span class="ot">&lt;-</span> model<span class="sc">$</span><span class="fu">to</span>(<span class="at">device =</span> device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>Second, create an optimizer.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"/>optimizer <span class="ot">&lt;-</span> <span class="fu">optim_adam</span>(model<span class="sc">$</span>parameters)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>And third, the biggest chunk: In each epoch, iterate over training batches as well as validation batches, performing backpropagation when working on the former, while just passively reporting losses when processing the latter.</p>
<p>For clarity, we pack training logic and validation logic each into their own functions. <code>train_batch()</code> and <code>valid_batch()</code> will be called from inside loops over the respective batches. Those loops, in turn, will be executed for every epoch.</p>
<p>While <code>train_batch()</code> and <code>valid_batch()</code>, per se, trigger the usual actions in the usual order, note the device placement calls: For the model to be able to take in the data, they have to live on the same device. Then, for mean-squared-error computation to be possible, the target tensors need to live there as well.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"/>train_batch <span class="ot">&lt;-</span> <span class="cf">function</span>(b) {</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"/>  optimizer<span class="sc">$</span><span class="fu">zero_grad</span>()</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"/>  output <span class="ot">&lt;-</span> <span class="fu">model</span>(b[[<span class="dv">1</span>]]<span class="sc">$</span><span class="fu">to</span>(<span class="at">device =</span> device))</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"/>  target <span class="ot">&lt;-</span> b[[<span class="dv">2</span>]]<span class="sc">$</span><span class="fu">to</span>(<span class="at">device =</span> device)</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"/></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"/>  loss <span class="ot">&lt;-</span> <span class="fu">nn_mse_loss</span>(output, target)</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"/>  loss<span class="sc">$</span><span class="fu">backward</span>()</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"/>  optimizer<span class="sc">$</span><span class="fu">step</span>()</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"/></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"/>  loss<span class="sc">$</span><span class="fu">item</span>()</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"/>}</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"/></span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"/>valid_batch <span class="ot">&lt;-</span> <span class="cf">function</span>(b) {</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"/>  output <span class="ot">&lt;-</span> <span class="fu">model</span>(b[[<span class="dv">1</span>]]<span class="sc">$</span><span class="fu">to</span>(<span class="at">device =</span> device))</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"/>  target <span class="ot">&lt;-</span> b[[<span class="dv">2</span>]]<span class="sc">$</span><span class="fu">to</span>(<span class="at">device =</span> device)</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"/></span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"/>  loss <span class="ot">&lt;-</span> <span class="fu">nn_mse_loss</span>(output, target)</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"/>  loss<span class="sc">$</span><span class="fu">item</span>()</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"/>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>The loop over epochs contains two lines that deserve special attention: <code>model$train()</code> and <code>model$eval()</code>. The former instructs <code>torch</code> to put the model in training mode; the latter does the opposite. With the simple model we’re using here, it wouldn’t be a problem if you forgot those calls; however, when later we’ll be using regularization layers like <code>nn_dropout()</code> and <code>nn_batch_norm2d()</code>, calling these methods in the correct places is essential. This is because these layers behave differently during evaluation and training.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"/>num_epochs <span class="ot">&lt;-</span> <span class="dv">200</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"/></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"/><span class="cf">for</span> (epoch <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>num_epochs) {</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"/>  model<span class="sc">$</span><span class="fu">train</span>()</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"/>  train_loss <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"/></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"/>  <span class="co"># use coro::loop() for stability and performance</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"/>  coro<span class="sc">::</span><span class="fu">loop</span>(<span class="cf">for</span> (b <span class="cf">in</span> train_dl) {</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"/>    loss <span class="ot">&lt;-</span> <span class="fu">train_batch</span>(b)</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"/>    train_loss <span class="ot">&lt;-</span> <span class="fu">c</span>(train_loss, loss)</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"/>  })</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"/></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"/>  <span class="fu">cat</span>(<span class="fu">sprintf</span>(</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"/>    <span class="st">"</span><span class="sc">\n</span><span class="st">Epoch %d, training: loss: %3.5f </span><span class="sc">\n</span><span class="st">"</span>,</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"/>    epoch, <span class="fu">mean</span>(train_loss)</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"/>  ))</span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"/></span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"/>  model<span class="sc">$</span><span class="fu">eval</span>()</span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"/>  valid_loss <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"/></span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"/>  <span class="co"># disable gradient tracking to reduce memory usage</span></span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"/>  <span class="fu">with_no_grad</span>({ </span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"/>    coro<span class="sc">::</span><span class="fu">loop</span>(<span class="cf">for</span> (b <span class="cf">in</span> valid_dl) {</span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"/>      loss <span class="ot">&lt;-</span> <span class="fu">valid_batch</span>(b)</span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"/>      valid_loss <span class="ot">&lt;-</span> <span class="fu">c</span>(valid_loss, loss)</span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"/>    })  </span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"/>  })</span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"/>  </span>
<span id="cb24-29"><a href="#cb24-29" aria-hidden="true" tabindex="-1"/>  <span class="fu">cat</span>(<span class="fu">sprintf</span>(</span>
<span id="cb24-30"><a href="#cb24-30" aria-hidden="true" tabindex="-1"/>    <span class="st">"</span><span class="sc">\n</span><span class="st">Epoch %d, validation: loss: %3.5f </span><span class="sc">\n</span><span class="st">"</span>,</span>
<span id="cb24-31"><a href="#cb24-31" aria-hidden="true" tabindex="-1"/>    epoch, <span class="fu">mean</span>(valid_loss)</span>
<span id="cb24-32"><a href="#cb24-32" aria-hidden="true" tabindex="-1"/>  ))</span>
<span id="cb24-33"><a href="#cb24-33" aria-hidden="true" tabindex="-1"/>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>This completes our walk-through of manual training, and should have made more concrete my assertion that using <code>luz</code> significantly reduces the potential for casual (e.g., copy-paste) errors.</p>


</section>

    
</body>
</html>