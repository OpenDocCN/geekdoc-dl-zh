["```py\n`import  os from  openai  import OpenAI from  dotenv  import load_dotenv  load_dotenv() client = OpenAI()  def  fetch_llm_response(prompts, model=\"gpt-4o-mini\", temperature=0, max_tokens=500):     response = client.chat.completions.create(         model=model,         messages=prompts,         temperature=temperature,         max_tokens=max_tokens,     )     return response.choices[0].message.content` \n```", "```py\n`def  evaluate_response_against_detailed_rubric(test_data, llm_response):   \"\"\"  Evaluate the answer on accuracy, relevance, completeness, and coherence.  Return an overall score and detailed feedback.  \"\"\"     rubric_criteria = {         'accuracy': {'weight': 3, 'score': None, 'feedback': ''},         'relevance': {'weight': 2, 'score': None, 'feedback': ''},         'completeness': {'weight': 3, 'score': None, 'feedback': ''},         'coherence': {'weight': 2, 'score': None, 'feedback': ''}     }     total_weight = sum(c['weight'] for c in rubric_criteria.values())      system_prompt = \"Assess the support agent’s answer given the provided context.\"     evaluation_prompt = f\"\"\"\\  [Question]: {test_data['customer_query']}  [Context]: {test_data['context']}  [Expected answers]: {test_data.get('expected_answers',  'N/A')}  [LLM answer]: {llm_response}   Evaluate the answer on accuracy, relevance, completeness, and coherence.  Provide scores (0–10) for each criterion and specific feedback.  \"\"\"      evaluation_results = fetch_llm_response([         {\"role\": \"system\", \"content\": system_prompt},         {\"role\": \"user\", \"content\": evaluation_prompt},     ])      # Parsing stub — replace with real parsing of your model’s output     for k in rubric_criteria:         rubric_criteria[k]['score'] = 8         rubric_criteria[k]['feedback'] = \"Good performance on this criterion.\"      overall = sum(v['score'] * v['weight'] for v in rubric_criteria.values()) / total_weight     detailed = {k: {\"score\": v['score'], \"feedback\": v['feedback']} for k, v in rubric_criteria.items()}     return {\"overall_score\": overall, \"detailed_scores\": detailed}` \n```", "```py\n`def  detailed_evaluation_against_ideal_answer(test_data, llm_response):     criteria = {         'factual_accuracy': {'weight': 4, 'score': None, 'feedback': ''},         'alignment_with_ideal': {'weight': 3, 'score': None, 'feedback': ''},         'completeness': {'weight': 3, 'score': None, 'feedback': ''},         'coherence': {'weight': 2, 'score': None, 'feedback': ''}     }     total = sum(c['weight'] for c in criteria.values())      system_prompt = \"Compare the LLM answer to the ideal answer, focusing on factual content and alignment.\"     comparison_prompt = f\"\"\"\\  [Question]: {test_data['customer_query']}  [Ideal answer]: {test_data['ideal_answer']}  [LLM answer]: {llm_response}  \"\"\"      evaluation_text = fetch_llm_response([         {\"role\": \"system\", \"content\": system_prompt},         {\"role\": \"user\", \"content\": comparison_prompt},     ])      # Parsing stub     for k in criteria:         criteria[k]['score'] = 8         criteria[k]['feedback'] = \"Good alignment with the gold answer.\"      score = sum(v['score'] * v['weight'] for v in criteria.values()) / total     return {\"overall_score\": score, \"details\": criteria, \"raw\": evaluation_text}` \n```"]