["```r\nlibrary(torch)\n\nx1 <- torch_tensor(2, requires_grad = TRUE)\nx2 <- torch_tensor(2, requires_grad = TRUE)\n```", "```r\nx3 <- x1$square()\nx5 <- x3 * 0.2\n\nx4 <- x2$square()\nx6 <- x4 * 0.2\n\nx7 <- x5 + x6 - 5\nx7\n```", "```r\ntorch_tensor\n-3.4000\n[ CPUFloatType{1} ][ grad_fn = <SubBackward1> ]\n```", "```r\nx7$requires_grad\n```", "```r\n[1] TRUE\n```", "```r\nx7$backward()\n```", "```r\nx1$grad\nx2$grad\n```", "```r\n 0.8000\n[ CPUFloatType{1} ]\ntorch_tensor\n 0.8000\n[ CPUFloatType{1} ]\n```", "```r\nx3$grad\n```", "```r\n[W TensorBody.h:470] Warning: The .grad attribute of a Tensor\nthat is not a leaf Tensor is being accessed. Its .grad attribute\nwon't be populated during autograd.backward().\nIf you indeed want the .grad field to be populated for a \nnon-leaf Tensor, use .retain_grad() on the non-leaf Tensor.[...]\n\ntorch_tensor\n[ Tensor (undefined) ]\n```", "```r\nx3 <- x1$square()\nx3$retain_grad()\n\nx5 <- x3 * 0.2\nx5$retain_grad()\n\nx4 <- x2$square()\nx4$retain_grad()\n\nx6 <- x4 * 0.2\nx6$retain_grad()\n\nx7 <- x5 + x6 - 5\nx7$backward()\n```", "```r\nx3$grad\n```", "```r\ntorch_tensor\n 0.2000\n[ CPUFloatType{1} ]\n```", "```r\nx4$grad\nx5$grad\nx6$grad\n```", "```r\ntorch_tensor\n 0.2000\n[ CPUFloatType{1} ]\ntorch_tensor\n 1\n[ CPUFloatType{1} ]\ntorch_tensor\n 1\n[ CPUFloatType{1} ]\n```", "```r\nx3$grad_fn\n```", "```r\nPowBackward0\n```", "```r\nx4$grad_fn\nx5$grad_fn\nx6$grad_fn\n```", "```r\nPowBackward0\nMulBackward1\nMulBackward1\n```"]