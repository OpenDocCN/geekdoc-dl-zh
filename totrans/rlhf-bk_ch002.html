<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <title>ch002.xhtml</title>
  <link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css" />
</head>
<body epub:type="bodymatter">
<section id="key-related-works" class="level1">
<h1>Key Related Works</h1>
<p>RLHF and its related methods are very new. We highlight history to show how recently the procedures were formalized, and how much of this documentation is in the academic literature. With this, we want to emphasize that RLHF is very rapidly evolving, so the chapter sets the stage for a book that will express uncertainty over certain methods and an expectation that some details can change around a few, core practices. Otherwise, the papers and methods listed here showcase why many pieces of the RLHF pipeline are what they are, as some of the seminal papers were for applications totally distinct from modern language models.</p>
<p>In this chapter we detail the key papers and projects that got the RLHF field to where it is today. This is not intended to be a comprehensive review of RLHF and the related fields, but rather a starting point and retelling of how we got to today. It is intentionally focused on recent work that led to ChatGPT. There is substantial further work in the RL literature on learning from preferences <span class="citation" data-cites="wirth2017survey"><a href="ch021.xhtml#ref-wirth2017survey">[26]</a></span>. For a more exhaustive list, you should use a proper survey paper <span class="citation" data-cites="kaufmann2023survey"><a href="ch021.xhtml#ref-kaufmann2023survey">[27]</a></span>,<span class="citation" data-cites="casper2023open"><a href="ch021.xhtml#ref-casper2023open">[28]</a></span>.</p>
<section id="origins-to-2018-rl-on-preferences" class="level2">
<h2>Origins to 2018: RL on Preferences</h2>
<p>The field has recently been popularized with the growth of Deep Reinforcement Learning and has grown into a broader study of the applications of LLMs from many large technology companies. Still, many of the techniques used today are deeply related to core techniques from early literature on RL from preferences.</p>
<p>One of the first papers with an approach similar to modern RLHF was <em>TAMER</em>. <em>TAMER: Training an Agent Manually via Evaluative Reinforcement</em> proposed an approach in which humans iteratively scored an agent’s actions to learn a reward model, which was used to learn the action policy <span class="citation" data-cites="knox2008tamer"><a href="ch021.xhtml#ref-knox2008tamer">[29]</a></span>. Other concurrent or soon after work proposed an actor-critic algorithm, COACH, where human feedback (both positive and negative) is used to tune the advantage function <span class="citation" data-cites="macglashan2017interactive"><a href="ch021.xhtml#ref-macglashan2017interactive">[30]</a></span>.</p>
<p>The primary reference, Christiano et al. 2017, is an application of RLHF applied to preferences between trajectories of agents within Atari games <span class="citation" data-cites="christiano2017deep"><a href="ch021.xhtml#ref-christiano2017deep">[1]</a></span>. This work introducing RLHF followed soon after DeepMind’s seminal work in reinforcement learning on Deep Q-Networks (DQN), which showed that RL agents can solve popular video games learning from scratch. The work shows that humans choosing between trajectories can be more effective in some domains than directly interacting with the environment. This uses some clever conditions, but is impressive nonetheless. This method was expanded upon with more direct reward modeling <span class="citation" data-cites="ibarz2018reward"><a href="ch021.xhtml#ref-ibarz2018reward">[31]</a></span> and the adoption of deep learning within early RLHF work was capped by an extension to TAMER with neural network models just one year later <span class="citation" data-cites="warnell2018deep"><a href="ch021.xhtml#ref-warnell2018deep">[32]</a></span>.</p>
<p>This era began to transition, as reward models as a general notion were proposed as a method for studying alignment, rather than just a tool for solving RL problems <span class="citation" data-cites="leike2018scalable"><a href="ch021.xhtml#ref-leike2018scalable">[33]</a></span>.</p>
</section>
<section id="to-2022-rl-from-human-preferences-on-language-models" class="level2">
<h2>2019 to 2022: RL from Human Preferences on Language Models</h2>
<p>Reinforcement learning from human feedback, also referred to regularly as reinforcement learning from human preferences in its early days, was quickly adopted by AI labs increasingly turning to scaling large language models. A large portion of this work began between GPT-2, in 2018, and GPT-3, in 2020. The earliest work in 2019, <em>Fine-Tuning Language Models from Human Preferences</em> has many striking similarities to modern work on RLHF and the content that we will cover in this book <span class="citation" data-cites="ziegler2019fine"><a href="ch021.xhtml#ref-ziegler2019fine">[34]</a></span>. Many canonical terms, such as learning reward models, KL distances, feedback diagrams, etc. were formalized in this paper – just the evaluation tasks for the final models, and capabilities, were different to what people are doing today. From here, RLHF was applied to a variety of tasks. <!-- The popular applications were the ones that worked at the time. --> Important examples include general summarization <span class="citation" data-cites="stiennon2020learning"><a href="ch021.xhtml#ref-stiennon2020learning">[2]</a></span>, recursive summarization of books <span class="citation" data-cites="wu2021recursively"><a href="ch021.xhtml#ref-wu2021recursively">[35]</a></span>, instruction following (InstructGPT) <span class="citation" data-cites="ouyang2022training"><a href="ch021.xhtml#ref-ouyang2022training">[3]</a></span>, browser-assisted question-answering (WebGPT) <span class="citation" data-cites="nakano2021webgpt"><a href="ch021.xhtml#ref-nakano2021webgpt">[4]</a></span>, supporting answers with citations (GopherCite) <span class="citation" data-cites="menick2022teaching"><a href="ch021.xhtml#ref-menick2022teaching">[36]</a></span>, and general dialogue (Sparrow) <span class="citation" data-cites="glaese2022improving"><a href="ch021.xhtml#ref-glaese2022improving">[37]</a></span>.</p>
<p>Aside from applications, a number of seminal papers defined key areas for the future of RLHF, including those on:</p>
<ol type="1">
<li>Reward model over-optimization <span class="citation" data-cites="gao2023scaling"><a href="ch021.xhtml#ref-gao2023scaling">[38]</a></span>: The ability for RL optimizers to over-fit to models trained on preference data,</li>
<li>Language models as a general area of study for alignment <span class="citation" data-cites="askell2021general"><a href="ch021.xhtml#ref-askell2021general">[18]</a></span>, and</li>
<li>Red teaming <span class="citation" data-cites="ganguli2022red"><a href="ch021.xhtml#ref-ganguli2022red">[39]</a></span> – the process of assessing the safety of a language model.</li>
</ol>
<p>Work continued on refining RLHF for application to chat models. Anthropic continued to use it extensively for early versions of Claude <span class="citation" data-cites="bai2022training"><a href="ch021.xhtml#ref-bai2022training">[5]</a></span> and early RLHF open-source tools emerged <span class="citation" data-cites="ramamurthy2022reinforcement"><a href="ch021.xhtml#ref-ramamurthy2022reinforcement">[40]</a></span>,<span class="citation" data-cites="havrilla-etal-2023-trlx"><a href="ch021.xhtml#ref-havrilla-etal-2023-trlx">[41]</a></span>,<span class="citation" data-cites="vonwerra2022trl"><a href="ch021.xhtml#ref-vonwerra2022trl">[42]</a></span>.</p>
</section>
<section id="to-present-chatgpt-era" class="level2">
<h2>2023 to Present: ChatGPT Era</h2>
<p>The announcement of ChatGPT was very clear about the role of RLHF in its training <span class="citation" data-cites="openai2022chatgpt"><a href="ch021.xhtml#ref-openai2022chatgpt">[43]</a></span>:</p>
<blockquote>
<p>We trained this model using Reinforcement Learning from Human Feedback (RLHF), using the same methods as InstructGPT, but with slight differences in the data collection setup.</p>
</blockquote>
<p>Since then, RLHF has been used extensively in leading language models. It is well known to be used in Anthropic’s Constitutional AI for Claude <span class="citation" data-cites="bai2022constitutional"><a href="ch021.xhtml#ref-bai2022constitutional">[19]</a></span>, Meta’s Llama 2 <span class="citation" data-cites="touvron2023llama"><a href="ch021.xhtml#ref-touvron2023llama">[44]</a></span> and Llama 3 <span class="citation" data-cites="dubey2024llama"><a href="ch021.xhtml#ref-dubey2024llama">[24]</a></span>, Nvidia’s Nemotron <span class="citation" data-cites="adler2024nemotron"><a href="ch021.xhtml#ref-adler2024nemotron">[25]</a></span>, Ai2’s Tülu 3 <span class="citation" data-cites="lambert2024t"><a href="ch021.xhtml#ref-lambert2024t">[6]</a></span>, and more.</p>
<p>Today, RLHF is growing into a broader field of preference fine-tuning (PreFT), including new applications such as process reward for intermediate reasoning steps <span class="citation" data-cites="lightman2023let"><a href="ch021.xhtml#ref-lightman2023let">[45]</a></span>, covered in Chapter 7; direct alignment algorithms inspired by Direct Preference Optimization (DPO) <span class="citation" data-cites="rafailov2024direct"><a href="ch021.xhtml#ref-rafailov2024direct">[20]</a></span>, covered in Chapter 12; learning from execution feedback from code or math <span class="citation" data-cites="kumar2024training"><a href="ch021.xhtml#ref-kumar2024training">[46]</a></span>,<span class="citation" data-cites="singh2023beyond"><a href="ch021.xhtml#ref-singh2023beyond">[47]</a></span> and other online reasoning methods inspired by OpenAI’s o1 <span class="citation" data-cites="openai2024o1"><a href="ch021.xhtml#ref-openai2024o1">[48]</a></span>, covered in Chapter 14.</p>
</section>
</section>
</body>
</html>
