["```py\n`# Import environment and API helpers import  os from  dotenv  import load_dotenv, find_dotenv  # Ensure Panel is available for interactive apps import  panel  as  pn pn.extension()  # Load environment variables (including the OpenAI API key) _ = load_dotenv(find_dotenv())  # OPENAI_API_KEY is read by integrations automatically; no direct assignment required` \n```", "```py\n`# Choose a model version import  datetime current_date = datetime.datetime.now().date() language_model_version = \"gpt-3.5-turbo\" print(language_model_version)` \n```", "```py\n`# Embeddings and vector store from  langchain.vectorstores  import Chroma from  langchain_openai  import OpenAIEmbeddings  # Replace 'your_directory_path' with the directory where you will persist embeddings persist_directory = 'your_directory_path/' embedding_function = OpenAIEmbeddings() vector_database = Chroma(persist_directory=persist_directory, embedding_function=embedding_function)  # Query the vector store search_question = \"What are the key subjects covered in this course?\" top_documents = vector_database.similarity_search(search_question, k=3) print(f\"Relevant documents found: {len(top_documents)}\")  # Initialize a chat model and try a simple greeting from  langchain_openai  import ChatOpenAI language_model = ChatOpenAI(model='gpt-4o-mini', temperature=0) greeting_response = language_model.invoke(\"Greetings, universe!\") print(greeting_response)  # Prompt for concise, helpful answers from  langchain.prompts  import PromptTemplate prompt_template = \"\"\" Use the following pieces of context to answer the question at the end. If you're unsure about the answer, indicate so rather than speculating.  Try to keep your response within three sentences for clarity and conciseness.  End your answer with \"thanks for asking!\" to maintain a polite tone.  Context: {context} Question: {question} Helpful Answer: \"\"\" qa_prompt_template = PromptTemplate(input_variables=[\"context\", \"question\"], template=prompt_template)  # RetrievalQA chain default_question = \"Does this course require understanding of probability?\" from  langchain.chains  import RetrievalQA qa_chain = RetrievalQA.from_chain_type(     language_model,     retriever=vector_database.as_retriever(),     return_source_documents=True,     chain_type_kwargs={\"prompt\": qa_prompt_template} ) qa_result = qa_chain({\"query\": default_question}) print(\"Result:\", qa_result[\"result\"])` \n```", "```py\n`# Conversation memory from  langchain.memory  import ConversationBufferMemory  conversation_history_memory = ConversationBufferMemory(     memory_key=\"conversation_history\",     return_messages=True )` \n```", "```py\n`from  langchain.chains  import ConversationalRetrievalChain  document_retriever = vector_database.as_retriever()  question_answering_chain = ConversationalRetrievalChain.from_llm(     llm=language_model,     retriever=document_retriever,     memory=conversation_history_memory )` \n```", "```py\n`initial_question = \"Is probability a fundamental topic in this course?\" initial_result = question_answering_chain({\"question\": initial_question}) print(\"Answer:\", initial_result['answer'])  follow_up_question = \"Why are those topics considered prerequisites?\" follow_up_result = question_answering_chain({\"question\": follow_up_question}) print(\"Answer:\", follow_up_result['answer'])` \n```", "```py\n`from  langchain_openai  import OpenAIEmbeddings from  langchain.text_splitter  import RecursiveCharacterTextSplitter from  langchain.vectorstores  import DocArrayInMemorySearch from  langchain.document_loaders  import TextLoader, PyPDFLoader from  langchain.chains  import ConversationalRetrievalChain from  langchain_openai  import ChatOpenAI` \n```", "```py\n`def  load_documents_and_prepare_database(file_path, chain_type, top_k_results):   \"\"\"  Load documents from a file, split into manageable chunks, generate embeddings,  and prepare a vector database for retrieval.   Args:  - file_path: Path to the document file (PDF, text, etc.).  - chain_type: Conversational chain type to use.  - top_k_results: Number of top results to retrieve.   Returns:  - A conversational retrieval chain ready to answer questions.  \"\"\"     # Load documents using a loader appropriate for the file type     document_loader = PyPDFLoader(file_path)     documents = document_loader.load()      # Split into chunks     text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)     document_chunks = text_splitter.split_documents(documents)      # Embed chunks and build the vector store     embeddings_generator = OpenAIEmbeddings()     vector_database = DocArrayInMemorySearch.from_documents(document_chunks, embeddings_generator)      # Build the retriever     document_retriever = vector_database.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": top_k_results})      # Create the conversational retrieval chain     chatbot_chain = ConversationalRetrievalChain.from_llm(         llm=ChatOpenAI(model='gpt-4o-mini', temperature=0),          chain_type=chain_type,          retriever=document_retriever,          return_source_documents=True,         return_generated_question=True,     )      return chatbot_chain` \n```", "```py\n`def  load_db(document_path, retrieval_type, top_k_results):     return load_documents_and_prepare_database(document_path, retrieval_type, top_k_results)` \n```", "```py\n`import  panel  as  pn import  param` \n```", "```py\n`class  DocumentBasedChatbot(param.Parameterized):     conversation_history = param.List([])  # (question, answer) pairs     current_answer = param.String(\"\")      # Latest answer     database_query = param.String(\"\")      # Query sent to the document DB     database_response = param.List([])     # Retrieved source documents      def  __init__(self, **params):         super(DocumentBasedChatbot, self).__init__(**params)         self.interface_elements = []  # UI elements for the conversation         self.loaded_document = \"your_document.pdf\"  # Replace with your PDF path         self.chatbot_model = load_db(self.loaded_document, \"retrieval_type\", 4)  # Initialize the bot model` \n```", "```py\n `def  load_document(self, upload_count):         if upload_count == 0 or not file_input.value:             return pn.pane.Markdown(f\"Loaded document: {self.loaded_document}\")         else:             file_input.save(\"temp.pdf\")             self.loaded_document = file_input.filename             self.chatbot_model = load_db(\"temp.pdf\", \"retrieval_type\", 4)             self.clear_conversation_history()         return pn.pane.Markdown(f\"Loaded document: {self.loaded_document}\")` \n```", "```py\n `def  process_query(self, user_query):         if not user_query:             return pn.WidgetBox(pn.Row('User:', pn.pane.Markdown(\"\", width=600)), scroll=True)         result = self.chatbot_model({\"question\": user_query, \"chat_history\": self.conversation_history})         self.conversation_history.extend([(user_query, result[\"answer\"])])         self.database_query = result[\"generated_question\"]         self.database_response = result[\"source_documents\"]         self.current_answer = result['answer']         self.interface_elements.extend([             pn.Row('User:', pn.pane.Markdown(user_query, width=600)),             pn.Row('Assistant:', pn.pane.Markdown(self.current_answer, width=600, style={'background-color': '#F6F6F6'}))         ])         input_field.value = ''  # Clear input         return pn.WidgetBox(*self.interface_elements, scroll=True)` \n```", "```py\n `def  display_last_database_query(self):         if not self.database_query:             return pn.Column(                 pn.Row(pn.pane.Markdown(\"Last database query:\", style={'background-color': '#F6F6F6'})),                 pn.Row(pn.pane.Str(\"No database queries yet\"))             )         return pn.Column(             pn.Row(pn.pane.Markdown(\"Database query:\", style={'background-color': '#F6F6F6'})),             pn.pane.Str(self.database_query)         )      def  display_database_responses(self):         if not self.database_response:             return         response_list = [pn.Row(pn.pane.Markdown(\"Vector DB search result:\", style={'background-color': '#F6F6F6'}))]         for doc in self.database_response:             response_list.append(pn.Row(pn.pane.Str(doc)))         return pn.WidgetBox(*response_list, width=600, scroll=True)` \n```", "```py\n `def  display_chat_history(self):         if not self.conversation_history:             return pn.WidgetBox(pn.Row('Chat:', pn.pane.Str('No messages yet.')), scroll=True)         items = []         for q, a in self.conversation_history:             items.append(pn.Row('User:', pn.pane.Markdown(q, width=600)))             items.append(pn.Row('Assistant:', pn.pane.Markdown(a, width=600, style={'background-color': \"#FAFAFA\"})))         return pn.WidgetBox(*items, width=650, scroll=True)` \n```", "```py\n `def  clear_conversation_history(self, count=0):         self.conversation_history = []` \n```", "```py\n`import  panel  as  pn import  param  class  ChatWithYourDataBot(param.Parameterized):     conversation_history = param.List([])     latest_answer = param.String(\"\")     document_query = param.String(\"\")     document_response = param.List([])      def  __init__(self, **params):         super(ChatWithYourDataBot, self).__init__(**params)         self.interface_elements = []         self.default_document_path = \"your_document.pdf\"  # Replace with your PDF path         self.chatbot_model = load_db(self.default_document_path, \"retrieval_mode\", 4)` \n```", "```py\n `def  load_document(self, clicks):         if not getattr(document_upload, 'value', None):             return pn.pane.Markdown(f\"Loaded document: {self.default_document_path}\")         document_upload.save(\"temp.pdf\")         self.default_document_path = document_upload.filename or self.default_document_path         self.chatbot_model = load_db(\"temp.pdf\", \"retrieval_mode\", 4)         self.clear_history()         return pn.pane.Markdown(f\"Loaded document: {self.default_document_path}\")      def  process_query(self, user_query):         if not user_query:             return pn.WidgetBox(pn.Row('User:', pn.pane.Markdown(\"\", width=600)), scroll=True)         result = self.chatbot_model({\"question\": user_query, \"chat_history\": self.conversation_history})         self.conversation_history.extend([(user_query, result.get(\"answer\", \"\"))])         self.document_query = result.get(\"generated_question\", \"\")         self.document_response = result.get(\"source_documents\", [])         self.latest_answer = result.get('answer', \"\")         self.interface_elements.extend([             pn.Row('User:', pn.pane.Markdown(user_query, width=600)),             pn.Row('Assistant:', pn.pane.Markdown(self.latest_answer, width=600, style={'background-color': \"#F6F6F6\"}))         ])         user_query_input.value = \"\"         return pn.WidgetBox(*self.interface_elements, scroll=True)      def  display_last_database_query(self):         if not self.document_query:             return pn.Column(                 pn.Row(pn.pane.Markdown(\"Last database query:\", style={'background-color': \"#F6F6F6\"})),                 pn.Row(pn.pane.Str(\"No database queries yet\"))             )         return pn.Column(             pn.Row(pn.pane.Markdown(\"Database query:\", style={'background-color': \"#F6F6F6\"})),             pn.pane.Str(self.document_query)         )      def  display_database_responses(self):         if not self.document_response:             return         items = [pn.Row(pn.pane.Markdown(\"Vector DB search result:\", style={'background-color': \"#F6F6F6\"}))]         for doc in self.document_response:             items.append(pn.Row(pn.pane.Str(doc)))         return pn.WidgetBox(*items, width=600, scroll=True)      def  display_chat_history(self):         if not self.conversation_history:             return pn.WidgetBox(pn.Row('Chat:', pn.pane.Str('No messages yet.')), scroll=True)         items = []         for q, a in self.conversation_history:             items.append(pn.Row('User:', pn.pane.Markdown(q, width=600)))             items.append(pn.Row('Assistant:', pn.pane.Markdown(a, width=600, style={'background-color': \"#FAFAFA\"})))         return pn.WidgetBox(*items, width=650, scroll=True)      def  clear_history(self, *_):         self.conversation_history = []` \n```", "```py\n`document_upload = pn.widgets.FileInput(accept='.pdf') load_database_button = pn.widgets.Button(name=\"Load document\", button_type='primary') clear_history_button = pn.widgets.Button(name=\"Clear history\", button_type='warning') clear_history_button.on_click(ChatWithYourDataBot.clear_history) user_query_input = pn.widgets.TextInput(placeholder='Type your question hereâ€¦')  load_document_action = pn.bind(ChatWithYourDataBot.load_document, load_database_button.param.clicks) process_query = pn.bind(ChatWithYourDataBot.process_query, user_query_input)` \n```", "```py\n`# Optional: Add a conversation flow diagram image if available # conversation_visual = pn.pane.Image('path/to/your/conversation_flow.jpg')  conversation_tab = pn.Column(     pn.Row(user_query_input),     pn.layout.Divider(),     pn.panel(process_query, loading_indicator=True, height=300),     pn.layout.Divider(), )  database_query_tab = pn.Column(     pn.panel(ChatWithYourDataBot.display_last_database_query),     pn.layout.Divider(),     pn.panel(ChatWithYourDataBot.display_database_responses), )  chat_history_tab = pn.Column(     pn.panel(ChatWithYourDataBot.display_chat_history),     pn.layout.Divider(), )  configuration_tab = pn.Column(     pn.Row(document_upload, load_database_button, load_document_action),     pn.Row(clear_history_button, pn.pane.Markdown(\"Clears the conversation for a new topic.\")),     pn.layout.Divider(),     pn.Row(conversation_visual.clone(width=400)), )  chatbot_dashboard = pn.Column(     pn.Row(pn.pane.Markdown('# ChatWithYourDat-Bot')),     pn.Tabs(('Conversation', conversation_tab), ('DB queries', database_query_tab), ('Chat history', chat_history_tab), ('Setup', configuration_tab)) )` \n```"]