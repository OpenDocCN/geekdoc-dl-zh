- en: '7'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '7'
- en: Moving Beyond Foundation Models
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超越基础模型
- en: Introduction
  id: totrans-2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 引言
- en: In our previous chapters, we have focused on using or fine-tuning existing pre-trained
    models such as BERT to tackle a variety of natural language processing and computer
    vision tasks. While these models have demonstrated state-of-the-art performance
    on a wide range of benchmarks, they may not be sufficient for solving more complex
    or domain-specific tasks that require a deeper understanding of the problem.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的章节中，我们专注于使用或微调现有的预训练模型，如BERT，来解决各种自然语言处理和计算机视觉任务。虽然这些模型在广泛的基准测试中展示了最先进的性能，但它们可能不足以解决更复杂或特定领域的任务，这些任务需要更深入地理解问题。
- en: In this chapter, we explore the concept of constructing novel LLM architectures
    by combining existing models. By combining different models, we can leverage their
    strengths to create a hybrid architecture that performs either better than the
    individual models or simply to solve a task that wasn’t possible previously.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了通过结合现有模型来构建新颖的LLM架构的概念。通过结合不同的模型，我们可以利用它们的优势来创建一个混合架构，该架构的性能可能优于单个模型，或者简单地解决以前不可能的任务。
- en: We will be building a Visual Question and Answering system (VQA), combining
    the text processing capabilities of BERT, the image processing capabilities of
    a Vision Transformer (yes those exist), and the text generation capabilities of
    the open sourced GPT-2 to solve complex visual reasoning tasks. We will also explore
    the field of reinforcement learning and how it can be used to fine-tune pre-trained
    LLMs. Let’s dive in shall we?
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个视觉问答系统（VQA），结合BERT的文本处理能力、视觉变换器的图像处理能力（是的，这些确实存在），以及开源的GPT-2的文本生成能力，来解决复杂的视觉推理任务。我们还将探索强化学习领域以及它是如何被用来微调预训练的LLMs的。让我们深入探讨，怎么样？
- en: Case Study—Visual Q/A
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 案例研究——视觉问答
- en: '**Visual Question Answering** (VQA) is a challenging task that requires understanding
    and reasoning about both images and natural language (visualized in [Figure 7.1](ch07.html#ch07fig01)).
    Given an image and a related question in natural language, the objective is to
    generate a textual response that answers the question correctly. I showed a brief
    example of using pre-trained VQA systems in [Chapter 5](ch05.html#ch05) in a prompt
    chaining example but now we are going to make our own!'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**视觉问答** (VQA) 是一项具有挑战性的任务，需要理解和推理图像和自然语言（如图7.1所示）。给定一张图像和与之相关的问题，目标是生成一个文本响应，正确回答问题。我在第5章（ch05.html#ch05）中通过一个提示链示例展示了使用预训练的VQA系统的简要示例，但现在我们将要自己动手！'
- en: '![Images](graphics/07fig01.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![图像](graphics/07fig01.jpg)'
- en: '**Figure 7.1** *A Visual Question and Answering system (VQA) generally takes
    in two modes (types) of data - image and text - and will return a human readable
    answer to the question. This image outlines one of the most basic approaches to
    this problem where the image and text are encoded by separate encoders and a final
    layer predicts a single word as an answer.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**图7.1** 一个视觉问答系统（VQA）通常接受两种（类型）数据 - 图像和文本 - 并将返回一个人类可读的答案。这张图概述了处理此问题的一种最基本的方法，其中图像和文本分别由不同的编码器编码，最终层预测一个单词作为答案。'
- en: 'In this section, we focus on how to construct a VQA+LLM system using existing
    models and techniques. We start by introducing the foundational models used for
    this task: BERT, ViT, and GPT-2\. We then explore how to combine these models
    to create a hybrid architecture capable of processing both textual and visual
    inputs and generating coherent textual outputs.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们专注于如何使用现有模型和技术构建VQA+LLM系统。我们首先介绍用于此任务的基础模型：BERT、ViT和GPT-2。然后我们探索如何结合这些模型来创建一个能够处理文本和视觉输入并生成连贯文本输出的混合架构。
- en: We also demonstrate how to fine-tune the model using a dataset specifically
    designed for VQA tasks. We use the VQA v2.0 dataset, which contains a large number
    of images along with natural language questions about the images and corresponding
    answers. We explain how to prepare this dataset for training and evaluation and
    how to fine-tune the model using the dataset.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还展示了如何使用专门为VQA任务设计的特定数据集来微调模型。我们使用了VQA v2.0数据集，其中包含大量图像以及关于图像的自然语言问题和相应的答案。我们解释了如何为此数据集准备训练和评估，以及如何使用该数据集来微调模型。
- en: Introduction to our models - The Vision Transformer and GPT2, and DistilBERT
  id: totrans-12
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 我们模型的介绍 - 视觉变换器和GPT2，以及DistilBERT
- en: 'In this section, we introduce three foundational models that we will be using
    in our constructed multimodal system: the Vision Transformer, GPT-2, and DistilBERT.
    These models, while not currently state-of-the-art, are nonetheless powerful LLMs
    and have been widely used in various natural language processing and computer
    vision tasks. It’s also worth noting that when we are considering with LLMs to
    work with, we don’t always have to go right for the top shelf LLM as they tend
    to be larger and slower to use. With the right data, and the right motivation,
    we can make the smaller LLMs work just as well for our specific use-cases.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了我们将用于构建的多模态系统中的三个基础模型：视觉Transformer、GPT-2和DistilBERT。这些模型虽然目前不是最先进的，但它们无疑是强大的LLM，并且在各种自然语言处理和计算机视觉任务中得到了广泛应用。还值得注意的是，当我们考虑与LLM一起工作时，我们并不总是需要选择顶级的LLM，因为它们往往更大且使用起来较慢。有了合适的数据和动机，我们可以让较小的LLM在我们的特定用例中同样有效。
- en: Our Text Processor - DistilBERT
  id: totrans-14
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 我们的文本处理器 - DistilBERT
- en: DistilBERT is a distilled version of the popular BERT model that has been optimized
    for speed and memory efficiency. It is a pre-trained model that uses knowledge
    distillation to transfer knowledge from the larger BERT model to a smaller and
    more efficient one. This allows it to run faster and consume less memory while
    still retaining much of the performance of the larger model.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: DistilBERT是BERT流行模型的蒸馏版本，经过优化以提高速度和内存效率。它是一个预训练模型，使用知识蒸馏将知识从较大的BERT模型转移到较小且更高效的模型。这使得它运行更快，消耗更少的内存，同时仍然保留了较大模型的大部分性能。
- en: DistilBERT should have prior knowledge of language that will help during training,
    thanks to transfer learning. This allows it to understand natural language text
    with high accuracy.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 由于迁移学习，DistilBERT在训练期间应该具备语言先验知识，这有助于提高准确理解自然语言文本。
- en: Our Image Processor—Vision Transformer
  id: totrans-17
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 我们的形象处理器——视觉Transformer
- en: The Vision Transformer is a transformer-based architecture that is specifically
    designed for image understanding. It is a model that uses a self-attention mechanism
    to extract relevant features from images. It is a newer model that has gained
    popularity in recent years and has shown to be effective in various computer vision
    tasks.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉Transformer是一种基于Transformer的架构，专门设计用于图像理解。它是一个使用自注意力机制从图像中提取相关特征的模型。这是一个近年来获得流行的新模型，并在各种计算机视觉任务中显示出有效性。
- en: The Vision Transformer (ViT) has been pre-trained like BERT has on a dataset
    of images known as Imagenet and therefore should hold prior knowledge of image
    structures that should help during training as well. This allows it to understand
    and extract relevant features from images with high accuracy.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉Transformer（ViT）已经在名为ImageNet的图像数据集上进行了预训练，就像BERT一样，因此应该具备图像结构的先验知识，这有助于训练。这允许它以高精度理解和提取图像中的相关特征。
- en: We should note that when we use ViT, we should try to use the same image preprocessing
    steps that it used during pre-training so that the model has an easier time learning
    the new image sets. This is not strictly necessary and has its pros and cons.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该注意，当我们使用ViT时，我们应该尽量使用它在预训练期间使用的相同图像预处理步骤，以便模型更容易学习新的图像集。这并非绝对必要，并且有其优点和缺点。
- en: 'Pros of reusing the same preprocessing steps:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 重复使用相同预处理步骤的优点：
- en: '1\. **Consistency with pre-training**: Using data in the same format and distribution
    as during its pre-training can lead to better performance and faster convergence.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 1. **与预训练的一致性**：使用与预训练期间相同格式和分布的数据可以导致更好的性能和更快的收敛。
- en: '2\. **Leveraging prior knowledge**: Since the model has been pre-trained on
    a large dataset, it has already learned to extract meaningful features from images.
    Using the same preprocessing steps allows the model to apply this prior knowledge
    effectively to the new dataset.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 2. **利用先验知识**：由于模型已经在大型数据集上进行了预训练，它已经学会了从图像中提取有意义的特征。使用相同的预处理步骤允许模型有效地将这种先验知识应用于新数据集。
- en: '3\. **Improved generalization**: The model is more likely to generalize well
    to new data if the preprocessing steps are consistent with its pre-training, as
    it has already seen a wide variety of image structures and features.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 3. **改进的泛化能力**：如果预处理步骤与其预训练一致，则模型更有可能很好地泛化到新数据，因为它已经看到了广泛的各种图像结构和特征。
- en: 'Cons of reusing the same preprocessing steps:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 重复使用相同预处理步骤的缺点：
- en: '1\. **Limited flexibility**: Re-using the same preprocessing steps may limit
    the model''s ability to adapt to new data distributions or specific characteristics
    of the new dataset, which may require different preprocessing techniques for optimal
    performance.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 1. **灵活性有限**：重用相同的预处理步骤可能会限制模型适应新的数据分布或新数据集的特定特征的能力，这可能需要不同的预处理技术以实现最佳性能。
- en: '2\. **Incompatibility with new data**: In some cases, the new dataset may have
    unique properties or structures that are not well-suited to the original preprocessing
    steps, which could lead to suboptimal performance if the preprocessing steps are
    not adapted accordingly.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 2. **与新数据的不兼容性**：在某些情况下，新的数据集可能具有独特的属性或结构，这些属性或结构不适合原始的预处理步骤，如果不相应地调整预处理步骤，可能会导致性能不佳。
- en: '3\. **Overfitting to pre-training data**: Relying too heavily on the same preprocessing
    steps might cause the model to overfit to the specific characteristics of the
    pre-training data, reducing its ability to generalize to new and diverse datasets.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 3. **过度拟合预训练数据**：过度依赖相同的预处理步骤可能会使模型过度拟合预训练数据的特定特征，从而降低其泛化到新的和多样化的数据集的能力。
- en: We will re-use the ViT image preprocessor for now. [Figure 7.2](ch07.html#ch07fig02)
    shows a sample of an image before preprocessing and the same image after it has
    gone though ViT’s standard preprocessing steps.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将重用ViT图像预处理程序。[图7.2](ch07.html#ch07fig02)显示了预处理前后的图像样本。
- en: '![Images](graphics/07fig02.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图像](graphics/07fig02.jpg)'
- en: '**Figure 7.2** *Image systems like the Vision Transformer (ViT) generally have
    to standardize images to a set format with pre-defined normalization steps so
    that each image is processed as fairly and consistently as possible. For some
    images (like the downed tree in the top row) the image preprocessing really takes
    away context at the cost of standardization across all images.*'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**图7.2** *像Vision Transformer (ViT)这样的图像系统通常必须将图像标准化为具有预定义归一化步骤的特定格式，以便尽可能公平和一致地处理每个图像。对于某些图像（如顶行中的倒下的树）来说，图像预处理实际上是以标准化所有图像为代价来消除上下文的。*'
- en: Our Text decoder - GPT2
  id: totrans-32
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 我们的文本解码器 - GPT2
- en: GPT-2 is OpenAI’s precursor to GPT-3 (probably obvious) but more importantly
    is an open-source generative language model that is pre-trained on a large corpus
    of text data. GPT-2 was pre-trained on about 40 GB of data and so should also
    have prior knowledge of words that will help during training, again thanks to
    transfer learning.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2是OpenAI的GPT-3的前身（可能很明显），但更重要的是，它是一个开源的生成语言模型，在大量的文本数据语料库上进行了预训练。GPT-2在约40
    GB的数据上进行了预训练，因此也应该具备在训练期间帮助的词汇先验知识，这再次归功于迁移学习。
- en: The combination of these three models - DistilBERT for text processing, Vision
    Transformer for image processing, and GPT-2 for text decoding - will provide the
    basis for our multimodal system as shown in [Figure 7.3](ch07.html#ch07fig03).
    These models all have prior knowledge and we will rely on transfer learning capabilities
    to allow them to effectively process and generate highly accurate and relevant
    outputs for complex natural language and computer vision tasks.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个模型的组合——DistilBERT用于文本处理，Vision Transformer用于图像处理，GPT-2用于文本解码——将为我们多模态系统的基础，如图7.3所示。[图7.3](ch07.html#ch07fig03)展示了这些模型都具有先验知识，我们将依赖迁移学习的能力，使它们能够有效地处理和生成针对复杂自然语言和计算机视觉任务的非常准确和相关的输出。
- en: '![Images](graphics/07fig03.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图像](graphics/07fig03.jpg)'
- en: '**Figure 7.3** *A VQA system can have its final single-token-prediction layer
    replaced with an entirely separate language model like open-source GPT2\. The
    VQA system we are going to build has three transformer-based models working side
    by side to solve a single albeit very challenging task.*'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**图7.3** *一个VQA系统可以将其最终的单一标记预测层替换为一个完全独立的语言模型，例如开源的GPT2。我们将要构建的VQA系统有三个基于transformer的模型并行工作，以解决一个虽然非常具有挑战性的任务。*'
- en: Hidden States Projection and Fusion
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 隐藏状态投影和融合
- en: When we feed our text and image inputs into their respective models (DistilBERT
    and Vision Transformer), they produce output tensors that contain useful feature
    representations of the inputs. However, these features are not necessarily in
    the same format, and they may have different dimensionalities.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将文本和图像输入分别输入到它们各自的模式（DistilBERT和Vision Transformer）中时，它们会产生包含输入有用特征表示的输出张量。然而，这些特征不一定具有相同的格式，并且它们可能具有不同的维度。
- en: To address this, we use linear projection layers to project the output tensors
    of the text and image models onto a shared dimensional space. This allows us to
    fuse the features extracted from the text and image inputs effectively. The shared
    dimensional space makes it possible to combine the text and image features (by
    averaging them in our case) and feed them into the decoder (GPT-2) to generate
    a coherent and relevant textual response.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们使用线性投影层将文本和图像模型的输出张量投影到一个共享的维度空间。这使得我们能够有效地融合从文本和图像输入中提取的特征。共享的维度空间使得将文本和图像特征（在我们的情况下是通过平均它们）结合起来，并将它们输入到解码器（GPT-2）以生成一个连贯且相关的文本响应成为可能。
- en: But how will GPT-2 accept these inputs from the encoding models? The answer
    to that is a type of attention mechanism known as cross-attention.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 但GPT-2将如何接受来自编码模型的这些输入呢？答案是称为交叉注意力的一种注意力机制。
- en: 'Cross-Attention: What is it, and Why is it Critical?'
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 交叉注意力：它是什么，为什么它至关重要？
- en: Cross-attention is the mechanism that will allow our multimodal system to learn
    the interactions between our text and image inputs and the output text we want
    to generate. It is a critical component of the base transformer architecture that
    allows it to incorporate information from inputs to outputs (the hallmark of a
    sequence to sequence model) effectively. The cross-attention calculation is honestly
    the same as self-attention but between two different sequences rather than a single
    one. In cross attention, the input sequence (or combined sequences in our case
    because we will be inputting both text and images) will serve as the key and value
    input (which will be a combination of the query from the image and text encoder),
    whereas the output sequence acts as the query inputs (our text-generating GPT-2)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉注意力是允许我们的多模态系统学习我们的文本和图像输入以及我们想要生成的输出文本之间交互的机制。它是基本变换器架构的一个关键组件，允许它有效地将输入信息结合到输出中（这是序列到序列模型的特点）。交叉注意力计算实际上与自注意力相同，但是在两个不同的序列之间而不是单个序列之间。在交叉注意力中，输入序列（或在我们的情况下是结合的序列，因为我们将输入文本和图像）将作为键和值输入（这将是从图像和文本编码器中查询的组合），而输出序列则作为查询输入（我们的文本生成器GPT-2）
- en: Query, Key, and Value in Attention
  id: totrans-43
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意力机制中的查询、键和值
- en: The three internal components of attention – query, key, and value - haven’t
    really come up before in this book because frankly we haven’t really needed to
    understand why they exists, we simply relied on their ability to learn patterns
    in our data for us but it’s time to take a closer look at how these components
    interact so we can fully understand how cross-attention is working.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力的三个内部组件——查询、键和值——在这本书中之前并没有真正出现，因为坦白地说，我们并没有真正需要理解它们为什么存在，我们只是依赖于它们为我们学习数据中的模式的能力，但现在是我们更深入地了解这些组件如何相互作用的时候了，这样我们才能完全理解交叉注意力是如何工作的。
- en: In self-attention mechanisms used by transformers, Query, Key, and Value are
    the three components that are crucial for determining the importance of each input
    token relative to others in the sequence. The Query represents the token for which
    we want to compute the attention weights, while the Keys and Values represent
    the other tokens in the sequence. The attention scores are computed by taking
    the dot product between the Query and the Keys, scaling it by a normalization
    factor, and then multiplying it by the Values to create a weighted sum.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在由变换器使用的自注意力机制中，查询、键和值是三个对于确定序列中每个输入标记相对于其他标记的重要性至关重要的组件。查询代表我们想要计算注意力权重的标记，而键和值代表序列中的其他标记。注意力分数是通过计算查询和键的点积，通过归一化因子进行缩放，然后乘以值来创建一个加权总和。
- en: In simpler terms, the Query is employed to extract pertinent information from
    other tokens, as determined by the attention scores. The Keys help identify which
    tokens are relevant to the Query, while the Values supply the corresponding information.
    This can be visualized in [Figure 7.4](ch07.html#ch07fig04).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 用更简单的话说，查询被用来从其他标记中提取相关信息，这是由注意力分数确定的。键有助于识别哪些标记与查询相关，而值提供相应的信息。这可以在[图7.4](ch07.html#ch07fig04)中可视化。
- en: '![Images](graphics/07fig04.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图像](graphics/07fig04.jpg)'
- en: '**Figure 7.4** *These two images yield the scaled dot product attention value
    for the word “like” in the input “I like cats”. Every input token to a Transformer
    based LLM has an associated “query”, “key”, and “value” representation. The scaled-dot
    product attention calculation generates attention scores for each query token
    by taking the dot product with the key tokens (top) and then those scores are
    used to contextualize the value tokens with proper weighting (bottom) yielding
    a final vector for each token in the input that is now aware of the other tokens
    in the input and how much it should be paying attention to them. In this case,
    the token “like” should be paying 22% of its attention to the token “I”, 42% of
    attention to itself (yes, tokens need to pay attention to themselves – as we all
    should frankly – because they are part of the sequence and thus provide context),
    and 36% of its attention to the word “cats”.*'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**图7.4** *这两张图像显示了输入“我喜欢猫”中单词“喜欢”的缩放点积注意力值。每个输入到基于Transformer的LLM的标记都有一个相关的“查询”、“键”和“值”表示。缩放点积注意力计算通过将键标记（顶部）的点积与每个查询标记生成注意力分数，然后使用这些分数通过适当的加权（底部）来上下文化值标记，从而为输入中的每个标记生成一个最终向量，该向量现在了解输入中的其他标记以及它应该对它们投入多少注意力。在这种情况下，标记“喜欢”应该将22%的注意力投入到标记“I”上，42%的注意力投入到自身（是的，标记需要投入注意力——坦白地说，我们所有人都是如此——因为它们是序列的一部分，因此提供上下文），以及36%的注意力投入到单词“猫”上。*'
- en: In cross-attention, the Query, Key, and Value matrices serve slightly different
    purposes. In this case, the Query represents the output of one modality (e.g.,
    text), while the Keys and Values represent the outputs of another modality (e.g.,
    image). Cross-attention is used to calculate attention scores that determine the
    degree of importance given to the output of one modality when processing the other
    modality.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在交叉注意力中，查询、键和值矩阵服务于稍微不同的目的。在这种情况下，查询代表一个模态的输出（例如，文本），而键和值代表另一个模态的输出（例如，图像）。交叉注意力用于计算注意力分数，以确定在处理另一个模态时给予一个模态输出的重要程度。
- en: In a multimodal system, cross-attention calculates attention weights that express
    the relevance between text and image inputs (see [Figure 7.5](ch07.html#ch07fig05)).
    The Query is the output of the text model, while the Keys and Values are the output
    of the image model. The attention scores are computed by taking the dot product
    between the Query and the Keys and scaling it by a normalization factor. The resulting
    attention weights are then multiplied by the Values to create the weighted sum,
    which is utilized to generate a coherent and relevant textual response. [Listing
    7.1](ch07.html#list7_1) shows the hidden state sizes for our three models.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个多模态系统中，交叉注意力计算表示文本和图像输入之间相关性的注意力权重（请参阅[图7.5](ch07.html#ch07fig05)）。查询是文本模型的输出，而键和值是图像模型的输出。通过将查询和键之间的点积与归一化因子缩放来计算注意力分数。然后，将得到的注意力权重乘以值以创建加权总和，该总和被用于生成一个连贯且相关的文本响应。[列表7.1](ch07.html#list7_1)显示了我们的三个模型的隐藏状态大小。
- en: '![Images](graphics/07fig05.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图像](graphics/07fig05.jpg)'
- en: '**Figure 7.5** *Our VQA system needs to fuse the encoded knowledge from the
    image and text encoders and pass that fusion to the GPT-2 model via the cross-attention
    mechanism which will take the fused key and value vectors (See [Figure 7.4](ch07.html#ch07fig04)
    for more on that) from the image and text encoders and pass it onto our decoder
    GPT-2 to use to scale its own attention calculations.*'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**图7.5** *我们的VQA系统需要融合图像编码器和文本编码器编码的知识，并通过交叉注意力机制将融合传递给GPT-2模型，该机制将使用来自图像和文本编码器的融合键和值向量（有关更多信息，请参阅[图7.4](ch07.html#ch07fig04)）并将其传递给我们的解码器GPT-2，以用于缩放其自身的注意力计算。*'
- en: '**Listing 7.1** *Revealing LLMs’ Hidden States*'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**列表7.1** *揭示LLMs的隐藏状态*'
- en: '[PRE0]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In our case, all models have the same hidden state size so in theory we don’t
    need to project anything but it is still good practice to include projection layers
    so that the model has a trainable layer that translates our text/image representations
    into something more meaningful for the decoder.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，所有模型都具有相同的隐藏状态大小，因此从理论上讲，我们不需要投影任何东西，但仍然是一个好的实践来包含投影层，这样模型就有一个可训练的层，将我们的文本/图像表示转换为对解码器更有意义的表示。
- en: At first, our cross-attention attention parameters will have to be initially
    randomized and it will need to be learned during training. During the training
    process, the model learns to assign higher attention weights to relevant features
    while filtering out irrelevant ones. This way, the system can better understand
    the relationship between the text and image inputs, and generate more relevant
    and accurate textual responses. By assigning higher attention weights to relevant
    features while filtering out irrelevant ones, our system can better understand
    the relationship between the text and image inputs, generating more accurate and
    relevant textual responses.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们的交叉注意力注意力参数将需要初始随机化，并在训练过程中进行学习。在训练过程中，模型学会为相关特征分配更高的注意力权重，同时过滤掉不相关的特征。这样，系统可以更好地理解文本和图像输入之间的关系，并生成更相关、更准确的文本响应。通过为相关特征分配更高的注意力权重并过滤掉不相关的特征，我们的系统可以更好地理解文本和图像输入之间的关系，生成更准确、更相关的文本响应。
- en: With the ideas of cross-attention, fusion, and our models handy, let’s move
    on to defining a multimodal architecture.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在有了交叉注意力、融合以及我们手头的模型的想法之后，让我们继续定义一个多模态架构。
- en: Our Custom MultiModal Model
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 我们的定制多模态模型
- en: Before getting deeper into the code, I should note that not all of the code
    that powers this example is in these pages, but all of it lives in the notebooks
    on the github. I highly recommend following along using both!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入代码之前，我应该指出，支撑这个示例的并非所有代码都包含在这些页面中，但所有代码都生活在github上的笔记本中。我强烈推荐您使用两种方式来跟随！
- en: 'When creating a novel PyTorch Module (which we are doing) the main methods
    we need to define are the constructor (__init__) that will instantiate our three
    Transformer models and potentially freeze layers to speed up training (more on
    that in the next chapter) and the **forward** method which will take in inputs
    and potentially labels to generate an output and a loss value (remember loss is
    the same as error, the lower the better). The forward method will take the following
    as inputs:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 当创建一个新的PyTorch模块（我们正在做）时，我们需要定义的主要方法是构造函数(__init__)，它将实例化我们的三个Transformer模型，并可能冻结层以加快训练速度（更多内容将在下一章中介绍）以及**forward**方法，该方法将接受输入和可能的标签以生成输出和损失值（记住损失与错误相同，越低越好）。forward方法将以下作为输入：
- en: '![Images](graphics/square.jpg) **input_ids**: A tensor containing the input
    IDs for the text tokens. These IDs are generated by the tokenizer based on the
    input text. The shape of the tensor is [batch_size, sequence_length].'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](graphics/square.jpg) **input_ids**：一个包含文本标记输入ID的张量。这些ID由分词器根据输入文本生成。张量的形状为[batch_size,
    sequence_length]。'
- en: '![Images](graphics/square.jpg) **attention_mask**: A tensor of the same shape
    as input_ids that indicates which input tokens should be attended to (value 1)
    and which should be ignored (value 0). This is mainly used to handle padding tokens
    in the input sequence.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](graphics/square.jpg) **attention_mask**：一个与input_ids形状相同的张量，指示哪些输入标记应该被关注（值为1）以及哪些应该被忽略（值为0）。这主要用于处理输入序列中的填充标记。'
- en: '![Images](graphics/square.jpg) **decoder_input_ids**: A tensor containing the
    input IDs for the decoder tokens. These IDs are generated by the tokenizer based
    on the target text, which is used as a prompt for the decoder during training.
    The shape of the tensor during training is [batch_size, target_sequence_length]
    but at inference time will simply be a start token so the model will have to generate
    the rest.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](graphics/square.jpg) **decoder_input_ids**：一个包含解码器标记输入ID的张量。这些ID由分词器根据目标文本生成，在训练期间作为解码器的提示使用。训练期间张量的形状为[batch_size,
    target_sequence_length]，但在推理时间将只是一个起始标记，因此模型将不得不生成其余部分。'
- en: '![Images](graphics/square.jpg) **image_features**: A tensor containing the
    preprocessed image features for each sample in the batch. The shape of the tensor
    is [batch_size, num_features, feature_dimension].'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](graphics/square.jpg) **image_features**：一个包含每个样本预处理图像特征的张量。张量的形状为[batch_size,
    num_features, feature_dimension]。'
- en: '![Images](graphics/square.jpg) **labels**: A tensor containing the ground truth
    labels for the target text. The shape of the tensor is [batch_size, target_sequence_length].
    These labels are used to compute the loss during training but won’t exist at inference
    time because if we had the labels then we wouldn’t need this model!'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](graphics/square.jpg) **labels**：一个包含目标文本真实标签的张量。张量的形状为[batch_size, target_sequence_length]。这些标签用于训练期间计算损失，但在推理时间不存在，因为如果我们有标签，那么我们就不需要这个模型了！'
- en: '[Listing 7.2](ch07.html#list7_2) shows a snippet of the code it takes to create
    a custom model from our three separate Transformer-based models (BERT, ViT, and
    GPT2). The full class can of course be found in the repository for your copy and
    pasting needs.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 7.2](ch07.html#list7_2) 展示了从我们三个基于 Transformer 的模型（BERT、ViT 和 GPT2）创建自定义模型的代码片段。当然，完整的类可以在仓库中找到，供您复制粘贴使用。'
- en: '**Listing 7.2** *A snippet of our multi modal model*'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**列表 7.2** *我们多模态模型的一个片段*'
- en: '[PRE1]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: With a model defined and properly adjusted for cross-attention, let’s take a
    look at the data that will power our engine.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义并适当调整了交叉注意力模型后，让我们看看将驱动我们引擎的数据。
- en: Our Data—Visual QA
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 我们的数据——视觉问答
- en: Our dataset comes from [https://visualqa.org](https://visualqa.org) with samples
    shown in [Figure 7.6](ch07.html#ch07fig06). The dataset contains pairs of open-ended
    questions about images with human-annotated answers. The dataset is meant to produce
    questions that require an understanding of vision, language and just a bit of
    commonsense knowledge to answer.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集来自 [https://visualqa.org](https://visualqa.org)，如图 7.6 所示。该数据集包含关于图像的开放式问题对，并附有人工标注的答案。该数据集旨在产生需要理解视觉、语言以及一点点常识知识才能回答的问题。
- en: '![Images](graphics/07fig06.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图像](graphics/07fig06.jpg)'
- en: '**Figure 7.6** *The VisualQA.org website has a dataset with open-ended questions
    about images.*'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 7.6** *VisualQA.org 网站有一个包含关于图像开放式问题的数据集。*'
- en: Parsing the Dataset for Our Model
  id: totrans-74
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 为我们的模型解析数据集
- en: '[Listing 7.3](ch07.html#list7_3) shows a function I wrote to parse the image
    files and creates a dataset that we can use with HuggingFace’s Trainer object.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 7.3](ch07.html#list7_3) 展示了我编写的一个函数，用于解析图像文件并创建一个我们可以与 HuggingFace 的 Trainer
    对象一起使用的数据集。'
- en: '**Listing 7.3** *Parsing the Visual QA files*'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**列表 7.3** *解析视觉问答文件*'
- en: '[PRE2]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The VQA Training Loop
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VQA 训练循环
- en: Training is not going to be so different from what we have done before. Most
    of the hard work was done in our data parsing to be honest. We get to use the
    HuggingFace Trainer and TrainingArguments objects with our custom model and training
    will simply come down to expecting a drop in our validation loss. Full code can
    be found on our repository with a snippet found in [Listing 7.4](ch07.html#list7_4).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 训练不会与之前我们做过的有很大不同。说实话，大部分的艰苦工作是在我们的数据解析中完成的。我们可以使用 HuggingFace Trainer 和 TrainingArguments
    对象以及我们的自定义模型，训练将简单归结为期望我们的验证损失下降。完整的代码可以在我们的仓库中找到，其中片段可以在 [列表 7.4](ch07.html#list7_4)
    中找到。
- en: '**Listing 7.4** *Training Loop for VQA*'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**列表 7.4** *VQA 训练循环*'
- en: '[PRE3]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: There’s a lot of code that powers this example so once again, I would highly
    recommend following along with the notebook on the github for the full code and
    comments!
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子中有很多代码，所以我再次强烈建议您在 GitHub 上的笔记本中跟随，以获取完整的代码和注释！
- en: Summary of Results
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结果总结
- en: '[Figure 7.7](ch07.html#ch07fig07) shows a sample of images with a few questions
    asked of it. Note that some of the responses are more than a single token which
    is an immediate benefit of having the LLM as our decoder as opposed to outputting
    a single token like in standard VQA systems.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7.7](ch07.html#ch07fig07) 展示了几个问题被问到的图像样本。请注意，一些回答不止一个标记，这是与标准 VQA 系统输出单个标记相比，拥有
    LLM 作为解码器的直接好处。'
- en: '![Images](graphics/07fig07.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图像](graphics/07fig07.jpg)'
- en: '**Figure 7.7** *Our VQA system is not half bad at answering out of sample questions
    about images even though we used pretty small models (in terms of number of parameters
    and especially compared to what is considered state of the art today). Each percentage
    is the aggregated token prediction probabilities that GPT-2 generated while answering
    the given questions. Clearly it is getting some questions wrong and with more
    training/data we can reduce errors even further!*'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 7.7** *尽管我们使用了相当小的模型（就参数数量而言，尤其是与今天被认为是最佳实践相比），但我们的 VQA 系统在回答关于图像的样本问题方面表现并不差。每个百分比都是
    GPT-2 在回答给定问题时生成的聚合标记预测概率。显然，它在某些问题上犯了错误，并且随着更多的训练和数据，我们可以进一步减少错误！*'
- en: But this is only a sample of data and not a very holistic representation of
    performance. To showcase how our model training went, [Figure 7.8](ch07.html#ch07fig08)
    shows the drastic change in our language modeling loss value after only one epoch.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 但这只是一个数据样本，并不能全面地展示性能。为了展示我们的模型训练过程，图 7.8 显示了在仅经过一个训练周期后，我们的语言模型损失值发生了剧烈变化。
- en: '![Images](graphics/07fig08.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图像](graphics/07fig08.jpg)'
- en: '**Figure 7.8** *After only one epoch, our VQA system showed a massive drop
    in validation loss which is great!*'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**图7.8** *经过仅一个epoch后，我们的VQA系统在验证损失上出现了大幅下降，这真是太好了！*'
- en: Our model is far from perfect and will require some more advanced training strategies
    and a ton of more data before it can really be considered state of the art but
    you know what? Free data, free models, and (mostly) free compute power of my own
    laptop yielded and not half bad VQA system.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模式远非完美，在它真正被认为是行业领先之前，还需要一些更先进的训练策略和大量的数据。但你知道吗？免费的数据，免费的模式，以及（主要是）我自己的笔记本电脑的免费计算能力产生了一个相当不错的VQA系统。
- en: Let’s step away from the idea of pure language modeling and image processing
    for just a moment and step into the world of a novel way of fine-tuning language
    models using its powerful cousin - reinforcement learning.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们暂时放下纯语言建模和图像处理的想法，进入使用其强大的表亲——强化学习来微调语言模型的新颖方式的世界。
- en: Case Study—Reinforcement Learning from Feedback
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 案例研究——基于反馈的强化学习
- en: We have seen over and over the remarkable capabilities of language models in
    this book and usually we have dealt with relatively objective tasks like classification
    and when the task was more subjective like semantic retrieval and anime recommendations,
    we had to take some time to define an objective quantitative metric to guide the
    model’s fine-tuning and overall system performance. In general, defining what
    constitutes “good” output text can be challenging, as it is often subjective and
    task/context-dependent. Different applications may require different “good” attributes,
    such as creativity for storytelling, readability for summarization, or code-functionality
    for code snippets.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这本书中一次又一次地见证了语言模型令人瞩目的能力，通常我们处理的是相对客观的任务，如分类，而当任务更加主观，如语义检索和动漫推荐时，我们不得不花些时间去定义一个客观的定量指标来指导模型的微调和整体系统性能。一般来说，定义什么构成“好的”输出文本可能具有挑战性，因为它往往是主观的，并且与任务/上下文相关。不同的应用可能需要不同的“好的”属性，例如，故事讲述中的创造力，摘要中的可读性，或代码片段中的代码功能。
- en: When we fine-tune LLMs, we must design a loss function to guide training but
    designing a loss function that captures these more subjective attributes can seem
    intractable, and most language models continue to be trained using a simple next-token
    prediction loss (auto-regressive language modeling), such as cross-entropy. As
    for output evaluation, there are some metrics that were designed to better capture
    human preferences, such as BLEU or ROUGE; however, these metrics still have limitations,
    as they only compare generated text to reference texts using simple rules and
    heuristics. We could use an embedding similarity to compare outputs to ground
    truth sequences but this only considers semantic information which isn’t always
    the only thing we need to compare. We might want to consider the style of the
    text for example.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们微调LLM时，我们必须设计一个损失函数来指导训练，但设计一个能够捕捉这些更主观属性的损失函数似乎难以实现，而大多数语言模型仍然使用简单的下一个标记预测损失（自回归语言建模）进行训练，例如交叉熵。至于输出评估，有一些指标是为了更好地捕捉人类偏好而设计的，如BLEU或ROUGE；然而，这些指标仍然存在局限性，因为它们仅使用简单的规则和启发式方法将生成的文本与参考文本进行比较。我们可以使用嵌入相似度来比较输出与真实序列，但这只考虑了语义信息，而这并不是我们总是需要比较的唯一事物。我们可能还想考虑文本的风格，例如。
- en: If only we could use live feedback (human or automated) for evaluating generated
    text as a performance measure or even as a loss function to optimize the model.
    Well you can probably see where this is going because that's where **Reinforcement
    Learning from Feedback** (RLHF for human feedback and RLAIF for AI feedback) comes
    into play. By employing reinforcement learning methods, RLF can directly optimize
    a language model using real-time feedback, allowing models trained on a general
    corpus of text data to align more closely with nuanced human values.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能够使用实时反馈（人工或自动化）来评估生成的文本作为性能指标，甚至作为优化模型的损失函数，那会怎么样？你可能已经看到了这个趋势，因为这就是**基于反馈的强化学习**（RLHF用于人类反馈，RLAIF用于AI反馈）发挥作用的地方。通过采用强化学习方法，RLF可以直接使用实时反馈来优化语言模型，使得在通用文本数据集上训练的模型能够更紧密地与细微的人类价值观对齐。
- en: ChatGPT is one of the first notable applications of RLHF and while OpenAI provides
    an impressive explanation of RLHF, it doesn't cover everything, so I’ll fill in
    the gaps.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT是RLHF（基于人类反馈的强化学习）的第一个显著应用之一，虽然OpenAI提供了一个令人印象深刻的RLHF解释，但它并没有涵盖所有内容，所以我会填补这些空白。
- en: 'The training process basically breaks down into three core steps (shown in
    [Figure 7.9](ch07.html#ch07fig09)):'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程基本上可以分为三个核心步骤（如图7.9所示）：
- en: '![Images](graphics/07fig09.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/07fig09.jpg)'
- en: '**Figure 7.9** *The core steps of a reinforcement learning based LLM training
    consists of pre-training an LLM, defining and potentially training a reward model,
    and using that reward model to update the LLM from step 1.*'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**图7.9** *基于强化学习的LLM训练的核心步骤包括预训练LLM、定义和可能训练奖励模型，并使用该奖励模型从步骤1更新LLM*。'
- en: 1\. **Pretraining a language model** - Pretraining a language model involves
    training the model on a large corpus of text data, such as articles, books, and
    websites or could even be a curated dataset. During this phase, the model learns
    to generate text for general corpora or in service of a task. This process helps
    the model to learn grammar, syntax, and some level of semantics from the text
    data. The objective function used during pre-training is typically the cross-entropy
    loss, which measures the difference between the predicted token probabilities
    and the true token probabilities. Pretraining allows the model to acquire a foundational
    understanding of the language, which can later be fine-tuned for specific tasks.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 1. **预训练语言模型** - 预训练语言模型涉及在大量文本数据集上训练模型，如文章、书籍和网站，甚至可以是精心挑选的数据集。在这个阶段，模型学习为通用语料库或服务于特定任务生成文本。这个过程有助于模型从文本数据中学习语法、句法和一定程度的语义。预训练期间使用的目标函数通常是交叉熵损失，它衡量预测标记概率与真实标记概率之间的差异。预训练使模型能够获得语言的基础理解，这可以后来针对特定任务进行微调。
- en: 2\. **Defining (potentially training) a reward model** - After pretraining the
    language model, the next step is to define a reward model that can be used to
    evaluate the quality of the generated text. This involves gathering human feedback,
    such as rankings or scores for different text samples, which can be used to create
    a dataset of human preferences. The reward model aims to capture these preferences,
    and can be trained as a supervised learning problem, where the goal is to learn
    a function that maps generated text to a reward signal (a scalar value) that represents
    the quality of the text according to human feedback. The reward model serves as
    a proxy for human evaluation and is used during the reinforcement learning phase
    to guide the fine-tuning process.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 2. **定义（可能训练）奖励模型** - 在预训练语言模型之后，下一步是定义一个可以用来评估生成文本质量的奖励模型。这涉及到收集人类反馈，如对不同文本样本的排名或评分，这些可以用来创建一个人类偏好数据集。奖励模型的目的是捕捉这些偏好，并且可以作为一个监督学习问题进行训练，其目标是学习一个将生成文本映射到表示文本质量的奖励信号（一个标量值）的函数。奖励模型作为人类评估的代理，在强化学习阶段用于指导微调过程。
- en: 3\. **Fine-tuning the LM with reinforcement learning** - With a pretrained language
    model and a reward model in place, the final step is to fine-tune the language
    model using reinforcement learning techniques. In this phase, the model generates
    text, receives feedback from the reward model, and updates its parameters based
    on the reward signal. The objective is to optimize the language model such that
    the generated text aligns closely with human preferences. Popular reinforcement
    learning algorithms used in this context include Proximal Policy Optimization
    (PPO) and Trust Region Policy Optimization (TRPO). Fine-tuning with reinforcement
    learning allows the model to adapt to specific tasks and generate text that better
    reflects human values and preferences.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 3. **使用强化学习微调LM** - 在预训练语言模型和奖励模型就绪后，最后一步是使用强化学习技术微调语言模型。在这个阶段，模型生成文本，从奖励模型接收反馈，并根据奖励信号更新其参数。目标是优化语言模型，使生成的文本与人类偏好紧密一致。在此背景下常用的强化学习算法包括近端策略优化（PPO）和信任区域策略优化（TRPO）。使用强化学习进行微调使模型能够适应特定任务，并生成更好地反映人类价值观和偏好的文本。
- en: We are going to perform this process in its entirety in the next chapter but
    to set up this pretty complicated process I am going to outline a simpler version
    first. In our version we will take a pre-trained LLM off the shelf (FLAN-T5),
    use an already defined and trained reward model, and really focus on step 3 -
    the reinforcement learning loop.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一章中完整地执行这个过程，但为了设置这个相当复杂的过程，我首先将概述一个更简单的版本。在我们的版本中，我们将从货架上取一个预训练的LLM（FLAN-T5），使用一个已经定义和训练的奖励模型，并真正专注于第3步——强化学习循环。
- en: Our model - FLAN-T5
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 我们的模式 - FLAN-T5
- en: We have seen and used FLAN-T5 (visualized in an image taken from the original
    FLAN-T5 paper in [Figure 7.10](ch07.html#ch07fig10)) before so this should hopefully
    be a refresher. FLAN-T5 is an encoder-decoder model (effectively a pure Transformer
    model) which means it already has trained cross-attention layers built in and
    it has the benefit of being instruction fine-tuned (like GPT3.5, ChatGPT, and
    GPT-4 were). We are going to use the open-sourced “small” version of the model.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前已经见过并使用过 FLAN-T5（如图 7.10 所示，图片来自 FLAN-T5 原始论文）了，所以这应该是一个复习的机会。FLAN-T5 是一个编码器-解码器模型（实际上是一个纯
    Transformer 模型），这意味着它已经内置了训练好的交叉注意力层，并且它有像 GPT3.5、ChatGPT 和 GPT-4 那样进行指令微调的优势。我们将使用该模型的“小型”开源版本。
- en: '![Images](graphics/07fig10.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/07fig10.jpg)'
- en: '**Figure 7.10** *FLAN-T5 is an encoder-decoder architecture that has been instruction-fine-tuned
    and is open-sourced.*'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 7.10** *FLAN-T5 是一个经过指令微调并开源的编码器-解码器架构。*'
- en: In the next chapter, we will perform our own version of instruction fine-tuning
    but for now, we will borrow this already instruction-fine-tuned LLM from the good
    people at Google AI and move on to define a reward model.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将执行我们自己的指令微调版本，但到目前为止，我们将借用 Google AI 中的好人已经指令微调好的 LLM，并继续定义奖励模型。
- en: Our Reward Model—Sentiment and Grammar Correctness
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 我们的奖励模型——情感和语法正确性
- en: A reward model has to take in the output of an LLM (in our case a sequence of
    text) and returns a scalar (single number) reward which should numerically represent
    feedback on the output. This feedback can come from an actual human which would
    be very slow to run or could come from another language model or even a more complicated
    system that ranks potential model outputs, and those rankings are converted to
    rewards. As long as we are assigning a scalar reward for each output, it is a
    viable reward system.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 一个奖励模型必须接受一个 LLM（在我们的案例中是一个文本序列）的输出，并返回一个标量（单个数字）奖励，该奖励应从数值上表示对输出的反馈。这种反馈可以来自实际的人类，这将非常慢，或者可以来自另一个语言模型，甚至是一个更复杂的系统，该系统对潜在模型输出进行排名，并将这些排名转换为奖励。只要我们为每个输出分配一个标量奖励，它就是一个可行的奖励系统。
- en: 'In the next chapter, we will be doing some really interesting work to define
    our own reward model but for now we will again rely on the hard work of others
    and use the following pre-built LLMS:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将做一些非常有趣的工作来定义我们自己的奖励模型，但现在我们再次依赖他人的辛勤工作，并使用以下预构建的 LLM：
- en: '![Images](graphics/square.jpg) Sentiment from the `cardiffnlp/twitter-roberta-base-sentiment`
    LLM - the idea is to promote summaries that are neutral in nature so the reward
    from this model will be defined as the logit value (logit values can be negative
    which is preferred) of the **“neutral”** class'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](graphics/square.jpg) 来自 `cardiffnlp/twitter-roberta-base-sentiment` LLM
    的情感——目的是促进性质中立的摘要，因此这个模型的奖励将被定义为“中性”类的 logit 值'
- en: '![Images](graphics/square.jpg) A “grammar score” from the `textattack/roberta-base-CoLA`
    LLM - we want our summaries to be grammatically correct so using a score from
    this model should promote summaries that are easier to read. The reward will be
    defined as the logit value of the **“grammatically correct”** class'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](graphics/square.jpg) 来自 `textattack/roberta-base-CoLA` LLM 的“语法分数”——我们希望我们的摘要语法正确，因此使用这个模型的分数应该会促进易于阅读的摘要。奖励将被定义为“语法正确”类的
    logit 值'
- en: I should note that by choosing these classifiers to form the basis of our reward
    system, I am implicitly trusting in their performance. I checked out their descriptions
    on the HuggingFace model repository to see how they were trained and what performance
    metrics I could find but in general be aware that the reward systems play a big
    role in this process so if they are not aligned with how you truly would reward
    text sequences, you are in for some trouble.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我应该指出，通过选择这些分类器作为我们奖励系统的基础，我隐含地信任它们的性能。我查看了 HuggingFace 模型存储库中它们的描述，以了解它们的训练方式和可以找到的性能指标，但一般来说，要注意奖励系统在这个过程中起着重要作用，所以如果它们与您真正奖励文本序列的方式不一致，您可能会遇到一些麻烦。
- en: A snippet of the code that translates generated text into scores (rewards) using
    a weighted sum of logits from our two models can be found in [Listing 7.5](ch07.html#list7_5).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在 [列表 7.5](ch07.html#list7_5) 中找到一段代码片段，该代码片段使用我们两个模型的 logit 权重和将生成的文本转换为分数（奖励）。
- en: '**Listing 7.5** *Defining our reward system*'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**列表 7.5** *定义我们的奖励系统*'
- en: '[PRE4]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'With a model and a reward system ready to go, we just need to introduce one
    final net new component, our Reinforcement Learning library: TRL.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在准备好模型和奖励系统后，我们只需要引入一个最终的全新组件，我们的强化学习库：TRL。
- en: Transformer Reinforcement Learning
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Transformer强化学习
- en: 'Transformer reinforcement learning (TRL) is an open-source library we can use
    to train transformer models with reinforcement learning. The library is integrated
    with our favorite package: HuggingFace `transformers`.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer强化学习（TRL）是一个开源库，我们可以用它来使用强化学习训练transformer模型。该库与我们的最爱包：HuggingFace
    `transformers`集成。
- en: The TRL library supports both pure decoder models like GPT-2 and GPT-Neo (more
    on that in the next chapter) as well as Sequence to Sequence models like FLAN-T5\.
    All models can be optimized using what is known as **Proximal Policy Optimization**
    (PPO). Honestly I won’t into how it works in this book but it’s definitely something
    for you to look up if you’re curious. TRL also has many examples on their github
    page if you want to see even more applications.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: TRL库支持纯解码器模型，如GPT-2和GPT-Neo（关于这一点将在下一章中详细介绍）以及序列到序列模型，如FLAN-T5。所有模型都可以使用所谓的**近端策略优化**（PPO）进行优化。说实话，我不会在这本书中深入探讨它是如何工作的，但如果你好奇，这绝对是你应该查找的东西。TRL在其GitHub页面上也有许多示例，如果你想看到更多应用。
- en: '[Figure 7.11](ch07.html#ch07fig11) shows the high level process of our (for
    now) simplified RLF loop.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.11](ch07.html#ch07fig11)展示了我们（目前）简化的RLF循环的高级过程。'
- en: '![Images](graphics/07fig11.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/07fig11.jpg)'
- en: '**Figure 7.11** *Our first Reinforcement Learning from Feedback loop has our
    pre-trained LLM (FLAN-T5) learning from a pre-curated dataset and a pre-built
    reward system. In the next chapter, we will see this loop performed with much
    more customization and rigor.*'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**图7.11** *我们的第一个基于反馈的强化学习循环使用预训练的LLM（FLAN-T5）从预筛选的数据集和预构建的奖励系统中学习。在下一章中，我们将看到这个循环以更多的定制和严谨性执行。*'
- en: Let’s jump into defining our training loop with some code to really see some
    results here.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一些代码来定义我们的训练循环，真正看到一些结果。
- en: The RL Training Loop
  id: totrans-126
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 强化学习训练循环
- en: 'Our RL fine-tuning loop has a few steps:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的RL微调循环有几个步骤：
- en: '1\. Instantiate **two** version of our model:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 实例化**两个**版本的我们的模型：
- en: a. Our “reference” model which is the original FLAN-T5 model which will never
    be updated **ever**
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: a. 我们的“参考”模型，这是原始的FLAN-T5模型，它将永远不会更新**任何**时刻
- en: b. Our “current” model which will get updated after every batch of data
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: b. 我们的“当前”模型，在每批数据更新后都会进行更新
- en: 2\. Grab a batch of data from a source (in our case we will use a corpus of
    news articles I found from HuggingFace)
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 从一个来源（在我们的案例中，我们将使用从HuggingFace找到的新闻文章语料库）获取一批数据
- en: 3\. Calculate rewards from our two reward models and aggregate into a single
    scalar (number) as a weighted sum of the two rewards
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 从我们的两个奖励模型中计算奖励，并将它们汇总成一个单一的标量（数字），作为两个奖励的加权总和
- en: '4\. Pass the rewards to the TRL package which calculates two things:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 将奖励传递给TRL包，它计算两件事：
- en: a. How to update the model slightly based on the reward system
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: a. 如何根据奖励系统稍微更新模型
- en: b. How divergent the text is from text generated from the reference model (this
    is measured the **KL-Divergence** between our two outputs. We won’t have the chance
    to go deep into this calculation in this text but simply put, it measures the
    difference between two sequences (two pieces of text in our case) with the goal
    of not letting the outputs diverge too far from the original model’s generation
    capacity.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: b. 文本与参考模型生成的文本之间的差异程度（这通过我们两个输出的**KL-Divergence**来衡量。在这篇文章中，我们不会深入探讨这个计算，但简单来说，它衡量的是两个序列（在我们的案例中是两段文本）之间的差异，目的是不让输出偏离原始模型的生成能力太远。
- en: 5\. TRL updates the “current” model from the batch of data, logs anything to
    a reporting system (I like the free Weights & Biases platform) and start over
    from the beginning of the steps!
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 5. TRL从数据批次中更新“当前”模型，将任何内容记录到报告系统中（我喜欢免费的Weights & Biases平台），然后从头开始步骤！
- en: This training loop can be visualized in [Figure 7.12](ch07.html#ch07fig12).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这个训练循环可以在[图7.12](ch07.html#ch07fig12)中可视化。
- en: '![Images](graphics/07fig12.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/07fig12.jpg)'
- en: '**Figure 7.12** *Our RL training loop has 4 main steps: our LLM will generate
    an output, our reward system will assign a scalar reward (positive for good, negative
    for bad), the TRL library will factor in rewards and divergence before doing any
    updating, and then finally the PPO policy will update the LLM.*'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**图7.12** *我们的强化学习训练循环有4个主要步骤：我们的LLM将生成输出，我们的奖励系统将分配一个标量奖励（正面表示好，负面表示坏），TRL库将在进行任何更新之前考虑奖励和发散，然后最终PPO策略将更新LLM。*'
- en: A snippet of this training loop is in [Listing 7.6](ch07.html#list7_6) with
    the entire loop defined in our code repository.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这个训练循环的一个片段在[列表7.6](ch07.html#list7_6)中，整个循环定义在我们的代码仓库中。
- en: '**Listing 7.6** *Defining our RL Training Loop with TRL*'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '**列表7.6** *使用TRL定义我们的RL训练循环*'
- en: '[PRE5]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Let’s see how it does after 2 epochs!
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看经过2个训练轮次后它的表现如何！
- en: Summary of Results
  id: totrans-144
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结果总结
- en: '[Figure 7.13](ch07.html#ch07fig13) shows how rewards were given over the training
    loop of 2 epochs. We can see that as the system progressed, we were giving out
    more rewards which is generally a good sign. I should note that the rewards started
    out pretty high so FLAN-T5 was already giving relatively neutral and readable
    responses so I would not expect drastic changes in the summaries.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.13](ch07.html#ch07fig13) 展示了在2个训练轮次中如何分配奖励。我们可以看到，随着系统的进展，我们给予的奖励越来越多，这通常是一个好兆头。我应该指出，奖励一开始就相当高，所以FLAN-T5已经能够给出相对中性和易读的响应，因此我不期望总结中会有剧烈的变化。'
- en: '![Images](graphics/07fig13.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/07fig13.jpg)'
- en: '**Figure 7.13** *Our system is giving out more rewards as training progresses
    (the graph is smoothed to see the overall movement).*'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '**图7.13** *随着训练的进行，我们的系统给予的奖励越来越多（图表已平滑处理以显示整体趋势）。*'
- en: But what do these adjusted generations look like? [Figure 7.14](ch07.html#ch07fig14)
    shows a sample of generated summaries before and after our RL fine-tuning
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 但这些调整后的生成内容看起来是什么样子？[图7.14](ch07.html#ch07fig14) 展示了在强化学习微调前后生成的摘要样本。
- en: '![Images](graphics/07fig14.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/07fig14.jpg)'
- en: '**Figure 7.14** *Our fine-tuned model barely differs in most summaries but
    does tend to use more neutral sounding words that are grammatically correct and
    easy to read.*'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '**图7.14** *我们的微调模型在大多数摘要中几乎没有差异，但倾向于使用更多听起来中性和语法正确且易于阅读的词语。*'
- en: This is our first example of a non supervised data fine-tuning of an LLM. We
    never gave FLAN-T5 example pairs of (article, summary) to learn **how** to summarize
    articles and that’s important. FLAN-T5 has already seen supervised datasets on
    summarization so it should already know how to do that. All we wanted to do was
    to nudge the responses to be more aligned with a reward metric that we defined.
    Our next chapter will see a much more in depth example of this where we train
    an LLM with supervised data, train our own reward system, and do this same TRL
    loop with (in my opinion) much more interesting results.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的第一个非监督数据微调LLM的例子。我们从未给FLAN-T5提供（文章，摘要）的示例对来学习**如何**总结文章，这很重要。FLAN-T5已经看到了关于摘要的监督数据集，因此它应该已经知道如何做这件事。我们想要的只是让响应更符合我们定义的奖励指标。我们下一章将看到一个更深入的例子，其中我们将使用监督数据训练LLM，训练我们自己的奖励系统，并使用（在我看来）更有趣的结果进行相同的TRL循环。
- en: Conclusion
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: Foundation models like FLAN-T5, ChatGPT, GPT-4, Cohere’s Command Series, GPT-2,
    BERT are a wonderful starting point to solve a wide variety of tasks. Fine-tuning
    them with supervised labeled data to fine-tune classifications, embeddings can
    get us even further but some tasks require us to get creative with our fine-tuning
    processes, with our data, and with our model architectures. This chapter is scratching
    the surface of what is possible. The next two chapters will dive even deeper on
    how to modify models, use data more creatively, and even start to answer the question
    of how do we share our amazing work with the world with efficient deployments
    of LLMs so I’ll see you there!
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 像FLAN-T5、ChatGPT、GPT-4、Cohere的Command系列、GPT-2、BERT这样的基础模型是解决各种任务的绝佳起点。使用监督标记数据对它们进行微调以优化分类、嵌入可以让我们更进一步，但有些任务需要我们在微调过程、数据以及模型架构上发挥创意。本章只是触及了可能性的表面。接下来的两章将更深入地探讨如何修改模型、更创造性地使用数据，甚至开始回答如何通过高效部署LLM将我们的出色工作与世界分享的问题，所以我会在那里等你！
