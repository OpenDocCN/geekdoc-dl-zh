["```py\n``# Import required libraries import  os from  openai  import OpenAI import  sys  # Add the project root to sys.path for relative imports sys.path.append('../..')  # Load environment variables from .env for safe API key management from  dotenv  import load_dotenv, find_dotenv _ = load_dotenv(find_dotenv())  # Initialize OpenAI using environment variables client = OpenAI()  # Ensure required packages are installed, including `lark` for parsing if needed # !pip install lark`` \n```", "```py\n`# Import the Chroma vector store and OpenAI embeddings from LangChain from  langchain.vectorstores  import Chroma from  langchain_openai  import OpenAIEmbeddings  # Directory for the vector database to persist its data persist_directory = 'vector_db/chroma/'  # Initialize the embedding function using an OpenAI model embedding_function = OpenAIEmbeddings()  # Create a Chroma vector database with persistence and the embedding function vector_database = Chroma(     persist_directory=persist_directory,     embedding_function=embedding_function )  # Print the current record count to verify readiness # Note: Using internal API for demo purposes. For production, use len() or other supported methods. print(len(vector_database.get()['ids']))  # Alternative to deprecated _collection.count()` \n```", "```py\n`# A small set of texts to populate the database texts = [     \"The Death Cap mushroom has a notable large fruiting body, often found above ground.\",     \"Among mushrooms, the Death Cap stands out for its large fruiting body, sometimes appearing in all-white.\",     \"The Death Cap, known for its toxicity, is one of the most dangerous mushrooms.\", ]  # Create a tiny demonstration vector database from the texts demo_vector_database = Chroma.from_texts(texts, embedding_function=embedding_function)  # A sample query for the demo vector database query_text = \"Discuss mushrooms characterized by their significant white fruiting bodies\"  # Similarity search: top‑2 most relevant similar_texts = demo_vector_database.similarity_search(query_text, k=2) print(\"Similarity search results:\", similar_texts)  # MMR search: diverse yet relevant (fetch extra candidates) diverse_texts = demo_vector_database.max_marginal_relevance_search(query_text, k=2, fetch_k=3) print(\"Diverse search (MMR) results:\", diverse_texts)` \n```", "```py\n`# An information‑seeking query query_for_information = \"what insights are available on data analysis tools?\"  # Standard similarity search: top‑3 relevant documents top_similar_documents = vector_database.similarity_search(query_for_information, k=3)  # Show the beginning of the first two documents for comparison print(top_similar_documents[0].page_content[:100]) print(top_similar_documents[1].page_content[:100])  # Note potential overlap. Introduce diversity with MMR. diverse_documents = vector_database.max_marginal_relevance_search(query_for_information, k=3)  # Show the beginning of the first two diverse documents to observe differences print(diverse_documents[0].page_content[:100]) print(diverse_documents[1].page_content[:100])` \n```", "```py\n`# A query scoped to a specific context specific_query = \"what discussions were there about regression analysis in the third lecture?\"  # Similarity search with a metadata filter to target a specific lecture targeted_documents = vector_database.similarity_search(     specific_query,     k=3,     filter={\"source\": \"documents/cs229_lectures/MachineLearning-Lecture03.pdf\"} )  # Inspect metadata to highlight the specificity of the search for document in targeted_documents:     print(document.metadata)` \n```", "```py\n`# Import required modules from LangChain from  langchain_openai  import OpenAI from  langchain.retrievers.self_query.base  import SelfQueryRetriever from  langchain.chains.query_constructor.base  import AttributeInfo  # Define the metadata attributes with detailed descriptions metadata_attributes = [     AttributeInfo(         name=\"source\",         description=\"Specifies the source document path.\",         type=\"string\",     ),     AttributeInfo(         name=\"page\",         description=\"Page number within the lecture document.\",         type=\"integer\",     ), ]  # Note: switching to the OpenAI model gpt‑4o‑mini, as the previous default is deprecated document_content_description = \"Detailed lecture notes\" language_model = OpenAI(model='gpt-4o-mini', temperature=0)` \n```", "```py\n`# Initialize the Self‑Query Retriever with the LLM, vector DB, and metadata attributes self_query_retriever = SelfQueryRetriever.from_llm(     language_model,     vector_database,     document_content_description,     metadata_attributes,     verbose=True )` \n```", "```py\n``# A query that encodes context directly in the question specific_query = \"what insights are provided on regression analysis in the third lecture?\"  # Note: the first run may emit a deprecation warning for `predict_and_parse`; you can ignore it. # Retrieve documents relevant to the specific query using inferred metadata relevant_documents = self_query_retriever.get_relevant_documents(specific_query)  # Display metadata to demonstrate specificity for document in relevant_documents:     print(document.metadata)`` \n```", "```py\n`# Import classes for contextual compression and document retrieval from  langchain.retrievers  import ContextualCompressionRetriever from  langchain.retrievers.document_compressors  import LLMChainExtractor from  langchain_openai  import OpenAI` \n```", "```py\n`# Initialize the language model with deterministic settings language_model = OpenAI(temperature=0, model=\"gpt-4o-mini\")  # Create a compressor that uses the LLM to extract relevant segments document_compressor = LLMChainExtractor.from_llm(language_model)` \n```", "```py\n`# Combine the document compressor with the vector DB retriever compression_retriever = ContextualCompressionRetriever(     base_compressor=document_compressor,     base_retriever=vector_database.as_retriever() )` \n```", "```py\n`# Define a query to look for relevant document segments query_text = \"what insights are offered on data analysis tools?\"  # Retrieve documents relevant to the query, automatically compressed for relevance compressed_documents = compression_retriever.get_relevant_documents(query_text)  # Helper to pretty‑print compressed document contents def  pretty_print_documents(documents):     print(f\"\\n{'-'  *  100}\\n\".join([f\"Document {index  +  1}:\\n\\n\" + doc.page_content for index, doc in enumerate(documents)]))  # Display the compressed documents pretty_print_documents(compressed_documents)` \n```", "```py\n`# Initialize a retriever that uses both contextual compression and MMR compression_based_retriever = ContextualCompressionRetriever(     base_compressor=document_compressor,     base_retriever=vector_database.as_retriever(search_type=\"mmr\") )  # A query to test the combined approach query_for_insights = \"what insights are available on statistical analysis methods?\"  # Retrieve compressed documents using the compression‑aware retriever compressed_documents = compression_based_retriever.get_relevant_documents(query_for_insights)  # Pretty‑print the compressed documents pretty_print_documents(compressed_documents)` \n```"]