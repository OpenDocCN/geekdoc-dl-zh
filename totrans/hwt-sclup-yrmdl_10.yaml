- en: How to Profile TPU Programs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://jax-ml.github.io/scaling-book/profiling](https://jax-ml.github.io/scaling-book/profiling)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '<d-title>Part 9 of [How To Scale Your Model](/scaling-book) ([Part 8: Serving
    LLaMA](../applied-inference) | [Part 10: JAX](../jax-stuff))'
  prefs: []
  type: TYPE_NORMAL
- en: 'So far this series has been entirely theoretical: back-of-the-envelope calculations
    based on hardware rooflines. That understanding gets you far but a lot of optimization
    comes down to practical details: how the XLA compiler works and how to use profiling
    tools like the JAX/Tensorboard Profiler to figure out what to do when it fails.
    We discuss this here.</d-title>  <d-byline><d-article><d-contents>### Contents'
  prefs: []
  type: TYPE_NORMAL
- en: '[A Thousand-Foot View of the TPU Software Stack](#a-thousand-foot-view-of-the-tpu-software-stack)[The
    TensorBoard Profiler: A Multi-Purpose TPU Profiler](#the-tensorboard-profiler-a-multi-purpose-tpu-profiler)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Trace Viewer](#trace-viewer)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to read an XLA op](#how-to-read-an-xla-op)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Graph Viewer](#graph-viewer)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Looking at a real(ish) example profile](#looking-at-a-real-ish-example-profile)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Memory Profile](#memory-profile)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Worked Problems](#worked-problems)</d-contents>'
  prefs: []
  type: TYPE_NORMAL
- en: A Thousand-Foot View of the TPU Software Stack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Google exposes a bunch of APIs for programming TPUs, from high level JAX code
    to low level Pallas or HLO. Most programmers write JAX code exclusively, which
    lets you write abstract NumPy-style linear algebra programs that are compiled
    automatically to run efficiently on TPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a simple example, a JAX program that multiplies two matrices together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'By calling `jax.jit`, we tell JAX to trace this function and emit a lower-level
    IR called [StableHLO](https://openxla.org/stablehlo), a platform-agnostic IR for
    ML computation, which is in turn lowered to HLO by the XLA compiler. The compiler
    runs many passes to determine fusions, layouts, and other factors that result
    in the HLO that is observable in a JAX profile. This HLO represents all the core
    linear algebra operations in the JAX code (matmuls, pointwise ops, convolutions,
    etc) in an LLVM-style graph view. For instance, here is an abridged version of
    the above program as HLO<d-footnote>To get this HLO, you can run `jax.jit(f).lower(*args,
    **kwargs).compile().as_text()`.</d-footnote>:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We’ll explain the syntax of HLO in just a second, but for now just note that
    it actually matches the JAX code above fairly well. For instance,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: is the actual matmul above that multiplies two f32 matrices along the 0 and
    1 dimension, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '**To transform this HLO to code that can be executed on the TPU, the XLA compiler
    first lowers it to LLO** (low-level optimizer) IR. LLO programs the TPU directly,
    scheduling copies between memories, pushing arrays onto the systolic array, etc.
    LLO code contains primitives that push buffers into the systolic array, pull results
    off, and schedule DMAs that communicate between different pieces of TPU memory.
    Once this has been lowered to LLO, it is then compiled to machine code that is
    loaded into the TPU IMEM and executed.'
  prefs: []
  type: TYPE_NORMAL
- en: When a program is running slower than we’d like, we primarily work with the
    JAX level to improve performance. Doing so, however, often requires us to understand
    some of the semantics of HLO and how the code is actually running on the TPU.
    When something goes wrong at a lower level, we pull yet another escape hatch and
    write custom kernels in [Pallas](https://jax.readthedocs.io/en/latest/pallas/tpu/details.html).
    To view the HLO of a program and its runtime statistics, we use the JAX profiler.
  prefs: []
  type: TYPE_NORMAL
- en: 'The JAX Profiler: A Multi-Purpose TPU Profiler'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: JAX provides a multi-purpose TPU profiler with a bunch of useful tools for understanding
    what’s happening on the TPU when a program is run. You can use the `jax.profiler`
    module to trace a program as it’s running and record everything from the duration
    of each subcomponent, the HLO of each program, memory usage, and more. For example,
    this code will dump a trace to a file in `/tmp/tensorboard` that can be viewed
    in TensorBoard ([here](https://docs.jax.dev/en/latest/profiling.html#tensorboard-profiling)
    is a step-by-step guide).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s an overview of what you can do in the profiler:'
  prefs: []
  type: TYPE_NORMAL
- en: <picture>![](../Images/7ca0d03081b27260888374f26a715b60.png)</picture>
  prefs: []
  type: TYPE_NORMAL
- en: 'Once in TensorBoard, the profiler has a few key tabs that help you understand
    your program:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Trace Viewer** shows a detailed timeline of what’s actually happening on
    the TPU as a timeline.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Graph Viewer** shows the HLO graph, letting you see what parts of the program
    feed into each other and how things are sharded.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Memory Profile and Memory Viewer:** these show how much memory your program
    is using.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While it’s slightly difficult to share profiles, [here](https://ui.perfetto.dev/#!/?s=fa9f13b487bde622707c1a503f9227c34594760a)
    is a Perfetto link that contains at least the Trace Viewer component for a simple
    Transformer. [This Colab](https://colab.research.google.com/drive/1_6krERgtolH7hbUIo7ewAMLlbA4fqEF8?usp=sharing)
    lets you generate the full JAX/TensorBoard trace and play around with it.
  prefs: []
  type: TYPE_NORMAL
- en: Trace Viewer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**The Trace Viewer is probably the most useful part of the profiler.** The
    example below shows a simple Transformer with pieces annotated. Names come from
    labels provided in the code.'
  prefs: []
  type: TYPE_NORMAL
- en: <picture>![](../Images/4d7370d4f4d1b84ae2ee73be05bb8b03.png)</picture>
  prefs: []
  type: TYPE_NORMAL
- en: 'The Trace Viewer shows a chronological timeline of all the actions on each
    TPU core. We’re only looking at TPU:0 here, since typically all TPUs execute the
    same instructions. A few key notes:'
  prefs: []
  type: TYPE_NORMAL
- en: The top row (XLA Ops) shows the actual TPU operations (the names are HLO names).
    Everything else is an approximate trace based on `jax.named_scope`, `jax.named_call`,
    and the Python stack trace.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Noting the repeated blocks, we can isolate a single layer here. We can also
    see (from looking at the code/understanding how a Transformer works) what parts
    are attention and what parts are MLPs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By clicking on an XLA op, we can view where in the code it comes from (useful
    for understanding the trace) and see links to the Graph viewer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Tip:** you can navigate the Trace Viewer using “video game” style controls,
    with A/D panning left and right, and W/S zooming in and out. These controls make
    navigating a lot easier.'
  prefs: []
  type: TYPE_NORMAL
- en: How to read an XLA op
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: HLO isn’t actually very hard to read, and it’s very helpful for understanding
    what a given part of the trace above corresponds to. Here’s an example op called
    fusion.3.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Let’s break this down into its pieces.
  prefs: []
  type: TYPE_NORMAL
- en: '**Op Name**: fusion.3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A dot or fusion op is a set of operations containing at most 1 matrix multiplication
    and possibly a bunch of related pointwise VPU-ops.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Shape/layout**: `bf16[32,32,4096]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the output shape of the op. We can see the dtype is bf16 (2 bytes per
    parameter) and `[32,32,4096]` is the shape.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Layout:** `{2,1,0:T(8,128)(2,1)}`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`{2,1,0:T(8,128)(2,1)}` tells us the order of the axes in memory (column major,
    row major, etc.) and the array padding. More below.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory location:** S(1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: S(1) tells us this array lives in VMEM. S(0) (sometimes omitted) is HBM. S(2)
    and S(3) are other memory spaces.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Arguments**: `bf16[32,32,8192]{2,1,0:T(8,128)(2,1)S(1)} %fusion.32`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This op has one input, a bf16 array called fusion.32 with a particular shape.
    This tells us what function feeds into this one.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s try to understand this notation a little more. Let’s take this as a simple
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '`f32[3,5]{1,0:T(2,2)}`'
  prefs: []
  type: TYPE_NORMAL
- en: 'which again tells us that this Op returns a float32 array of shape `[3, 5]`
    with a particular tiling `{1,0:T(2,2)}`. While tilings don’t matter *too* much,
    briefly, tilings tell us how an N-dimensional array is laid out sequentially in
    memory. Here’s a diagram showing how this array is laid out:'
  prefs: []
  type: TYPE_NORMAL
- en: <picture>![](../Images/39e893566682f211745ab800a3352350.png)</picture>
  prefs: []
  type: TYPE_NORMAL
- en: 'Within `{1,0:T(2,2)}`, the `1,0` part tells us the ordering of array dimensions
    in physical memory, from most minor to most major. You can read this part from
    right to left and pick out the corresponding dimensions in `f32[3,5]` to figure
    out the physical layout of the array. In this example, the physical layout is
    `[3,5]`, identical to the logical shape. After that, `T(2,2)` tells us that the
    array is tiled in chunks of `(2, 2)` where within each chunk, the array has rows
    first (**row-major**), then columns, i.e. `(0, 0)` is followed by `(0, 1)`, then
    `(1, 0)` and `(1,1)`. Because of the `T(2, 2)` tiling, the array is padded to
    `[4, 6]`, expanding its memory usage by about 1.6x. For the big bf16 array given
    above, `bf16[32,32,8192]{2,1,0:T(8,128)(2,1)S(1)}`, we do `T(8,128)(2,1)` which
    tells us the array has two levels of tiling, an outer `(8, 128)` tiling and an
    inner `(2, 1)` tiling within that unit (used for bf16 so our loads are always
    multiples of 4 bytes). For example, here’s `bf16[4,8]{1,0,T(2,4)(2,1)}` (colors
    are (2,4) tiles, red boxes are (2,1) tiles):'
  prefs: []
  type: TYPE_NORMAL
- en: <picture>![](../Images/d0f53ac6802fd5ea002fff7bff2b63ea.png)</picture>
  prefs: []
  type: TYPE_NORMAL
- en: Tiling can affect how efficiently chunks of tensors can be loaded into VMEM
    and XLA will sometimes introduce copies that “retile” or “re-layout” a tensor
    inside a program, sometimes at non-trivial overhead.<d-footnote>JAX provides an
    experimental feature to work around this issue, by allowing XLA to compute its
    "preferred" layout for inputs to a program. When you "just-in-time" compile a
    program with `jax.jit`, you typically pass in "mock" inputs that tell JAX what
    shape and dtype to expect. These typically also carry tiling information that
    may not be optimal. Instead, you can specify the input layouts as AUTO, and `jax.jit`
    will return a layout that the jitted program prefers. You can then explicitly
    load the tensor in that layout to avoid inducing copies within the program.</d-footnote>
  prefs: []
  type: TYPE_NORMAL
- en: Graph Viewer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'While some of the fusions above can seem complicated, the XLA Graph Viewer
    makes them easier to parse. For example here’s the view of a fairly complicated
    fusion:'
  prefs: []
  type: TYPE_NORMAL
- en: <picture>![](../Images/cb6f8c921a1f57c9bd1c39a69ad868cb.png)</picture>
  prefs: []
  type: TYPE_NORMAL
- en: It’s really helpful to stare at a bunch of HLO graphs and try to map HLO ops
    onto the code you’re profiling. By hovering over a box you’ll often see the line
    of code where the function was defined.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at a real(ish) example profile
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[This Colab](https://colab.research.google.com/drive/1_6krERgtolH7hbUIo7ewAMLlbA4fqEF8?usp=sharing)
    has an example profile for a fake Transformer. [Here’s](https://ui.perfetto.dev/#!/?s=fa9f13b487bde622707c1a503f9227c34594760a)
    a Perfetto link to at least see the Trace Viewer if you’re in a hurry. I’ve gone
    to more effort than usual to annotate the trace with `jax.named_scope` calls so
    you can identify what’s going on.'
  prefs: []
  type: TYPE_NORMAL
- en: <picture>![](../Images/a54c6d9950d138820493c5c19de0cff6.png)</picture>
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the profile and try to really understand what each part is doing.
    Let’s break it down a bit, starting with the FFW block:'
  prefs: []
  type: TYPE_NORMAL
- en: <picture>![](../Images/7ebf5515776433b1baaac68e91082100.png)</picture>
  prefs: []
  type: TYPE_NORMAL
- en: Here we’ve zoomed into the FFW block. You’ll see the up-projection Op is a fusion
    (matmul) with inputs `bf16[8, 1024, 8192]` and `bf16[8192, 16384]` and output
    `bf16[32, 1024, 16384]`. I know (because I wrote this code) that this is a local
    view of a 4-way DP, 2-way MP sharded matmul, so we’re actually doing
  prefs: []
  type: TYPE_NORMAL
- en: '**X:** `bf16[32, 1024, 8192]` * **W[in]**: `bf16[8192, 32768]` -> **Tmp**:
    `bf16[32, 1024, 32768]`'
  prefs: []
  type: TYPE_NORMAL
- en: '**How long do we expect this to take?** First of all, our batch size per data
    parallel shard is `8 * 1024 = 8192`, so we should be solidly compute-bound. This
    is on 8 TPUv2 cores (freely available on Google Colab), so we expect it to take
    about `2 * 32 * 1024 * 8192 * 32768 / (23e12 * 8) = 95.6ms` which is pretty much
    exactly how long it takes (96ms). That’s great! That means we’re getting fantastic
    FLOPs utilization!'
  prefs: []
  type: TYPE_NORMAL
- en: '**What about communication?** You’ll notice the little fusion hidden at the
    end of the second matmul. If we click on it, you’ll see'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: which is basically a little ReduceScatter (here’s the GraphViewer);
  prefs: []
  type: TYPE_NORMAL
- en: <picture>![](../Images/1ad0d538100c38142a79be15e5ae5f0c.png)</picture>
  prefs: []
  type: TYPE_NORMAL
- en: How long do we expect this to take? Well, we’re doing a ReduceScatter on a TPUv2
    4x2, which should require only one hop on 1.2e11 bidirectional bandwidth. The
    array has size `2*32*1024*8192` with the batch axis sharded 4 ways, so each shard
    is `2*8*1024*8192=134MB`. So this should take roughly 1.1ms. **How long does it
    actually take?** 1.13ms reported in the profile. So we’re really close to the
    roofline!
  prefs: []
  type: TYPE_NORMAL
- en: '**Let’s look at attention too!** Here’s a profile of the attention component:'
  prefs: []
  type: TYPE_NORMAL
- en: <picture>![](../Images/96cab32e8daf46e2d4cf3c73054084b2.png)</picture>
  prefs: []
  type: TYPE_NORMAL
- en: I’ve clicked on the Q projection op, which uses a matrix \(W_Q\) of shape [d[model]
    = 8192, n[heads] = 32, d[qkv] = 256]. We’re Megatron sharding along the head dimension.
    Try to do the same exercise of calculating how long these should take.
  prefs: []
  type: TYPE_NORMAL
- en: Memory Profile
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Memory Profile makes it easy to see the program memory as a function of
    time. This is helpful for debugging OOMs. You can see here about 7.5GB allocated
    to model parameters and about 10GB free. So we can fit a lot more into memory.
  prefs: []
  type: TYPE_NORMAL
- en: <picture>![](../Images/cdf86183a49c1b87ea3b05ed622deb5f.png)</picture>
  prefs: []
  type: TYPE_NORMAL
- en: Worked Problems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Question 1**: take a look at [this](https://colab.research.google.com/drive/1LfLO3OTr-_MWFPxUN36KJ3cqH0BcAoli?usp=sharing)
    Colab/profile and figure out what looks suspicious and what’s going on here. Can
    you tell me exactly what computations are happening and what each operation is
    doing? What are the true shapes of each matrix involved and how are they sharded?
    *Try looking at the profile first without reading the code.*'
  prefs: []
  type: TYPE_NORMAL
- en: <picture>![](../Images/18f62042543044cd5cefa97614c1cbc2.png)</picture> <details><summary>Click
    here for the answer.</summary>
  prefs: []
  type: TYPE_NORMAL
- en: 'This is two matrix multiplications, i.e. specifically this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see a reduce, two big fusions, and an all-reduce. The first big fusion
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '`%fusion.1 = bf16[4096]{0:T(1024)(128)(2,1)} fusion(bf16[4096,8192]{1,0:T(8,128)(2,1)}
    %param.1, bf16[8192]{0:T(1024)(128)(2,1)} %reduce.6), kind=kLoop, calls=%fused_computation.1`'
  prefs: []
  type: TYPE_NORMAL
- en: which tells us the per-shard shape is `bf16[8192] * bf16[4096, 8192] -> bf16[4096]`
    (over the 8192 dimension). By observing the final AllReduce with `replica_groups=\{\{0,16,32,48,64,80,96,112\},
    ...\}`, we can tell we’re doing 8-way model parallelism, so the true shapes are
    `[8, 8192] * bf16[32,768, 8192] -> bf16[8, 32,768]`.</details>
  prefs: []
  type: TYPE_NORMAL
- en: '**Question 2:** [The Transformer Colab from earlier](https://colab.research.google.com/drive/1_6krERgtolH7hbUIo7ewAMLlbA4fqEF8?usp=sharing)
    implements a simple mock Transformer. Follow the instructions in the Colab and
    get a benchmark of the naive Transformer with GSPMD partitioning. How long does
    each part take? How long should it take? What sharding is being used. Try fixing
    the sharding! *Hint: use `jax.lax.with_sharding_constraints` to constrain the
    behavior. With this fix, what’s the best MXU you can get?*'
  prefs: []
  type: TYPE_NORMAL
- en: 'For reference, the initial version gets roughly 184ms / layer and the optimized
    profile gets 67 ms / layer. Once you’ve done this, try staring at the profile
    and see if you can answer these questions purely from the profile:'
  prefs: []
  type: TYPE_NORMAL
- en: What sharding strategy is this?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the batch size, \(d_\text{model}\), \(d_\text{ff}\)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What fraction of time is spent on attention vs. the MLP block?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What fraction of time should be spent on each op at the roofline?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Note:** since this problem was written, the XLA compiler has gotten better.
    The initial version is now at roughly 90ms / layer and the optimized profile is
    only about 10ms / layer better (80 ms / layer). Still, it’s worth playing with
    and seeing if you can do better.'
  prefs: []
  type: TYPE_NORMAL
- en: That’s all for Part 9\. For Part 10, with a deep dive into JAX parallelism,
    click [here](../jax-stuff).</d-article>  <d-appendix><d-footnote-list><d-citation-list>###
    Miscellaneous
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ^*Work done at Google DeepMind, now at MatX.
  prefs: []
  type: TYPE_NORMAL
- en: Citation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For attribution in academic contexts, please cite this work as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'or as a BibTeX entry:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]</d-citation-list></d-footnote-list></d-appendix> <d-bibliography src="/scaling-book/assets/bibliography/"></d-bibliography></d-byline>'
  prefs: []
  type: TYPE_NORMAL
