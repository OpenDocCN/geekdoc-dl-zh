<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <title>ch013.xhtml</title>
  <link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css" />
</head>
<body epub:type="bodymatter">
<section id="constitutional-ai-ai-feedback" class="level1">
<h1>Constitutional AI &amp; AI Feedback</h1>
<p>RL from AI Feedback (RLAIF) is a larger set of techniques for using AI to augment or generate feedback data, including pairwise preferences <span class="citation" data-cites="lee2023rlaif"><a href="ch021.xhtml#ref-lee2023rlaif">[227]</a></span> <span class="citation" data-cites="sharma2024critical"><a href="ch021.xhtml#ref-sharma2024critical">[228]</a></span> <span class="citation" data-cites="castricato2024suppressing"><a href="ch021.xhtml#ref-castricato2024suppressing">[229]</a></span>. There are many motivations to using RLAIF to either entirely replace human feedback or augment it. AI models are far cheaper than humans, with a single piece of human preference data costing on the order of $1 or higher (or even above $10 per prompt), AI feedback with a frontier AI model, such as GPT-4o costs less than $0.01. This cost difference opens the market of experimentation with RLHF methods to an entire population of people previously priced out. Other than price, AI feedback introduces different <em>tradeoffs</em> on performance than human feedback, which are still being investigated. The peak performance for AI feedback is at least in the same ballpark of human data on skill-based evaluations, but it is not studied if human data allows finer control of the models in real-world product settings or for newer training methods such as character training.</p>
<p>The term RLAIF was introduced in Anthropic‚Äôs work <em>Constitutional AI: Harmlessness from AI Feedback</em> <span class="citation" data-cites="bai2022constitutional"><a href="ch021.xhtml#ref-bai2022constitutional">[19]</a></span>, which resulted in initial confusion in the AI community over the relationship between the methods. Since the release of the Constitutional AI (CAI) paper and the formalization of RLAIF, RLAIF has become a default method within the post-training and RLHF literatures ‚Äì there are far more examples than one can easily enumerate. The relationship should be understood as CAI was the example that kickstarted the broader field of RLAIF.</p>
<p>A rule of thumb for the difference between human data and AI feedback data is as follows:</p>
<ol type="1">
<li>Human data is high-noise and low-bias,</li>
<li>Synthetic preference data is low-noise and high-bias,</li>
</ol>
<p>Results in many academic results showing how one can substitute AI preference data in RLHF workflows and achieve strong evaluation scores <span class="citation" data-cites="miranda2024hybrid"><a href="ch021.xhtml#ref-miranda2024hybrid">[230]</a></span>, but shows how the literature of RLHF is separated from industrial best practices.</p>
<section id="constitutional-ai" class="level2">
<h2>Constitutional AI</h2>
<p>The method of Constitutional AI (CAI), which Anthropic uses extensively in their Claude models, is the earliest, large-scale use of synthetic data for RLHF training. Constitutional AI has two uses of synthetic data:</p>
<ol type="1">
<li>Critiques of instruction-tuned data to follow a set of principles like ‚ÄúIs the answer encouraging violence‚Äù or ‚ÄúIs the answer truthful.‚Äù When the model generates answers to questions, it checks the answer against the list of principles in the constitution, refining the answer over time. Then, they fine-tune the model on this resulting dataset.</li>
<li>Generates pairwise preference data by using a language model to answer which completion was better, given the context of a random principle from the constitution (similar to this paper for principle-guided reward models). Then, RLHF proceeds as normal with synthetic data, hence the RLAIF name.</li>
</ol>
<p>Largely, CAI is known for the second half above, the preference data, but the methods introduced for instruction data are used in general data filtering and synthetic data generation methods across post-training.</p>
<p>CAI can be formalized as follows.</p>
<p>By employing a human-written set of principles, which they term a <em>constitution</em>, Bai et al.¬†2022 use a separate LLM to generate artificial preference and instruction data used for fine-tuning <span class="citation" data-cites="bai2022constitutional"><a href="ch021.xhtml#ref-bai2022constitutional">[19]</a></span>. A constitution <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ùíû</mi><annotation encoding="application/x-tex">\mathcal{C}</annotation></semantics></math> is a set of written principles indicating specific aspects to focus on during a critique phase. The instruction data is curated by repeatedly sampling a principle <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>‚àà</mo><mi>ùíû</mi></mrow><annotation encoding="application/x-tex">c_i \in \mathcal{C}</annotation></semantics></math> and asking the model to revise its latest output <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>y</mi><mi>i</mi></msup><annotation encoding="application/x-tex">y^i</annotation></semantics></math> to the prompt <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> to align with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>c</mi><mi>i</mi></msub><annotation encoding="application/x-tex">c_i</annotation></semantics></math>. This yields a series of instruction variants <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">{</mo><msup><mi>y</mi><mn>0</mn></msup><mo>,</mo><msup><mi>y</mi><mn>1</mn></msup><mo>,</mo><mi>‚ãØ</mi><mo>,</mo><msup><mi>y</mi><mi>n</mi></msup><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\{y^0, y^1, \cdots, y^n\}</annotation></semantics></math> from the principles <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">{</mo><msub><mi>c</mi><mn>0</mn></msub><mo>,</mo><msub><mi>c</mi><mn>1</mn></msub><mo>,</mo><mi>‚ãØ</mi><mo>,</mo><msub><mi>c</mi><mrow><mi>n</mi><mo>‚àí</mo><mn>1</mn></mrow></msub><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\{c_{0}, c_{1}, \cdots, c_{n-1}\}</annotation></semantics></math> used for critique. The final data point is the prompt <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> together with the final completion <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>y</mi><mi>n</mi></msup><annotation encoding="application/x-tex">y^n</annotation></semantics></math>, for some <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>.</p>
<p>The preference data is constructed in a similar, yet simpler way by using a subset of principles from <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ùíû</mi><annotation encoding="application/x-tex">\mathcal{C}</annotation></semantics></math> as context for a feedback model. The feedback model is presented with a prompt <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>, a set of principles <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">{</mo><msub><mi>c</mi><mn>0</mn></msub><mo>,</mo><mi>‚ãØ</mi><mo>,</mo><msub><mi>c</mi><mi>n</mi></msub><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\{c_0, \cdots, c_n\}</annotation></semantics></math>, and two completions <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mn>0</mn></msub><annotation encoding="application/x-tex">y_0</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mn>1</mn></msub><annotation encoding="application/x-tex">y_1</annotation></semantics></math> labeled as answers (A) and (B) from a previous RLHF dataset. The feedback models‚Äô probability of outputting either (A) or (B) is recorded as a training sample for the reward model</p>
</section>
<section id="specific-llms-for-judgement" class="level2">
<h2>Specific LLMs for Judgement</h2>
<p>As RLAIF methods have become more prevalent, many have wondered if we should be using the same models for generating responses as those for generating critiques or ratings. Specifically, the calibration of the LLM-as-a-judge used has come into question. Several works have shown that LLMs are inconsistent evaluators <span class="citation" data-cites="wang2023large"><a href="ch021.xhtml#ref-wang2023large">[231]</a></span> and prefer their own responses over responses from other models (coined self-preference bias) <span class="citation" data-cites="panickssery2024llm"><a href="ch021.xhtml#ref-panickssery2024llm">[232]</a></span>.</p>
<p>As a result, many have wondered if we should be using the same models for generating responses as those for generating critiques or ratings. Would a solution be to train a separate model just for this? Multiple models have been released with the goal of substituting for frontier models as a data labeling tool, such as critic models Shepherd <span class="citation" data-cites="wang2023shepherd"><a href="ch021.xhtml#ref-wang2023shepherd">[233]</a></span> and CriticLLM <span class="citation" data-cites="ke2023critiquellm"><a href="ch021.xhtml#ref-ke2023critiquellm">[234]</a></span> or models for evaluating response performance akin to Auto-J <span class="citation" data-cites="li2023generative"><a href="ch021.xhtml#ref-li2023generative">[235]</a></span>, Prometheus <span class="citation" data-cites="kim2023prometheus"><a href="ch021.xhtml#ref-kim2023prometheus">[138]</a></span>, Prometheus 2 <span class="citation" data-cites="kim2024prometheus"><a href="ch021.xhtml#ref-kim2024prometheus">[236]</a></span>, or Prometheus-Vision <span class="citation" data-cites="lee2024prometheus"><a href="ch021.xhtml#ref-lee2024prometheus">[237]</a></span> but they are not widely adopted in documented training recipes. Some find scaling inference via repeated sampling <span class="citation" data-cites="brown2024large"><a href="ch021.xhtml#ref-brown2024large">[238]</a></span> <span class="citation" data-cites="zhao2025sample"><a href="ch021.xhtml#ref-zhao2025sample">[239]</a></span> <span class="citation" data-cites="kalra2025verdict"><a href="ch021.xhtml#ref-kalra2025verdict">[240]</a></span>, self-refinement <span class="citation" data-cites="madaan2023self"><a href="ch021.xhtml#ref-madaan2023self">[241]</a></span>, or tournament ranking <span class="citation" data-cites="pace2024west"><a href="ch021.xhtml#ref-pace2024west">[242]</a></span> provides a better estimate of the true judgement or higher-quality preference pairs. Other calibration techniques co-evolve the generation and judgement capabilities of the model <span class="citation" data-cites="wu2024meta"><a href="ch021.xhtml#ref-wu2024meta">[243]</a></span>.</p>
</section>
<section id="further-reading-2" class="level2">
<h2>Further Reading</h2>
<p>There are many related research directions and extensions of Constitutional AI, but few of them have been documented as clear improvements in RLHF and post-training recipes. For now, they are included as further reading.</p>
<ul>
<li>OpenAI has released a Model Spec <span class="citation" data-cites="openai2024modelspec"><a href="ch021.xhtml#ref-openai2024modelspec">[124]</a></span>, which is a document stating the intended behavior for their models, and stated that they are exploring methods for alignment where the model references the document directly (which could be seen as a close peer to CAI). OpenAI has continued and trained their reasoning models such as o1 with a method called Deliberative Alignment <span class="citation" data-cites="guan2024deliberative"><a href="ch021.xhtml#ref-guan2024deliberative">[244]</a></span> to align the model while referencing these safety or behavior policies.</li>
<li>Anthropic has continued to use CAI in their model training, updating the constitution Claude uses <span class="citation" data-cites="Anthropic2023ClaudesConstitution"><a href="ch021.xhtml#ref-Anthropic2023ClaudesConstitution">[245]</a></span> and experimenting with how population collectives converge on principles for models and how that changes model behavior <span class="citation" data-cites="ganguli2023"><a href="ch021.xhtml#ref-ganguli2023">[246]</a></span>.</li>
<li>The open-source community has explored replications of CAI applied to open datasets <span class="citation" data-cites="Huang2024cai"><a href="ch021.xhtml#ref-Huang2024cai">[247]</a></span> and for explorations into creating dialogue data between LMs <span class="citation" data-cites="lambert2024self"><a href="ch021.xhtml#ref-lambert2024self">[248]</a></span>.</li>
<li>Other work has used principle-driven preferences or feedback with different optimization methods. <span class="citation" data-cites="sun2023principledriven"><a href="ch021.xhtml#ref-sun2023principledriven">[249]</a></span> uses principles as context for the reward models, which was used to train the Dromedary models <span class="citation" data-cites="sun2024salmon"><a href="ch021.xhtml#ref-sun2024salmon">[250]</a></span>. <span class="citation" data-cites="glaese2022improving"><a href="ch021.xhtml#ref-glaese2022improving">[37]</a></span> uses principles to improve the accuracy of human judgments in the RLHF process. <span class="citation" data-cites="liu2025inference"><a href="ch021.xhtml#ref-liu2025inference">[251]</a></span> train a reward model to generate its own principles at inference time, and use these to deliver a final score. <span class="citation" data-cites="franken2024self"><a href="ch021.xhtml#ref-franken2024self">[252]</a></span> formulate principle-following as a mutual information maximization problem that the pretrained model can learn with no labels.</li>
</ul>
</section>
</section>
</body>
</html>
