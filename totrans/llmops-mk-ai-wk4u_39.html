<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Answers 3.3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Answers 3.3</h1>
<blockquote>原文：<a href="https://boramorka.github.io/LLM-Book/en/CHAPTER-3/Answers%203.3/">https://boramorka.github.io/LLM-Book/en/CHAPTER-3/Answers%203.3/</a></blockquote>
                
                  


  
  



<h2 id="theory">Theory</h2>
<ol>
<li>Setting up the environment for a quiz generator includes importing required libraries, suppressing non‑essential warnings, and loading API keys (CircleCI, GitHub, OpenAI).</li>
<li>The dataset structure should include a question template and a “quiz bank” organized by subjects, categories, and facts. For example: “History”, “Technology”, “Geography” with corresponding facts.</li>
<li>Prompt engineering guides the AI to generate content relevant to the selected category. The prompt template can prescribe selecting subjects from the bank and forming quiz questions.</li>
<li>LangChain’s role is to structure the prompt, choose the language model (LLM), and configure a parser for processing the output.</li>
<li>The quiz generation pipeline is a composition of a structured prompt, model, and parser, implemented using the LangChain Expression Language.</li>
<li>Functions such as <code>evaluate_quiz_content</code> are used to assess the relevance and correctness of generated quiz content by checking for expected keywords.</li>
<li>Proper refusal handling is tested via <code>evaluate_request_refusal</code>, which ensures the system returns the expected refusal for out‑of‑scope requests.</li>
<li>The “science” test checks that generated questions contain indicators of scientific topics (e.g., “physics”, “chemistry”, “biology”, “astronomy”).</li>
<li>The basic components of a CircleCI config for a Python project include: version, orbs, jobs (build/test), Docker image, steps (checkout/tests), and workflows.</li>
<li>Customizing the CircleCI configuration for a project involves setting the Python version, test commands, and adding extra steps to accurately reflect real build, test, and deployment processes.</li>
</ol>
<h2 id="practice">Practice</h2>
<p>Solutions to the tasks:</p>
<h3 id="task-1-creating-a-quiz-dataset">Task 1: Creating a quiz dataset</h3>
<p>We define a Python dictionary representing a collection of quiz items, organized by subjects, each with its categories and facts.</p>
<div class="highlight"><pre><span/><code><span class="n">quiz_bank</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"Historical Conflict"</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">"categories"</span><span class="p">:</span> <span class="p">[</span><span class="s2">"History"</span><span class="p">,</span> <span class="s2">"Politics"</span><span class="p">],</span>
        <span class="s2">"facts"</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">"Began in 1914 and ended in 1918"</span><span class="p">,</span>
            <span class="s2">"Involved two major alliances: the Allies and the Central Powers"</span><span class="p">,</span>
            <span class="s2">"Known for the extensive use of trench warfare on the Western Front"</span>
        <span class="p">]</span>
    <span class="p">},</span>
    <span class="s2">"Revolutionary Communication Technology"</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">"categories"</span><span class="p">:</span> <span class="p">[</span><span class="s2">"Technology"</span><span class="p">,</span> <span class="s2">"History"</span><span class="p">],</span>
        <span class="s2">"facts"</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">"Invented by Alexander Graham Bell in 1876"</span><span class="p">,</span>
            <span class="s2">"Revolutionized long-distance communication"</span><span class="p">,</span>
            <span class="s2">"First words transmitted were 'Mr. Watson, come here, I want to see you'"</span>
        <span class="p">]</span>
    <span class="p">},</span>
    <span class="s2">"Iconic American Landmark"</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">"categories"</span><span class="p">:</span> <span class="p">[</span><span class="s2">"Geography"</span><span class="p">,</span> <span class="s2">"History"</span><span class="p">],</span>
        <span class="s2">"facts"</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">"Gifted to the United States by France in 1886"</span><span class="p">,</span>
            <span class="s2">"Symbolizes freedom and democracy"</span><span class="p">,</span>
            <span class="s2">"Located on Liberty Island in New York Harbor"</span>
        <span class="p">]</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
<h3 id="task-2-generating-quiz-questions-using-prompts">Task 2: Generating quiz questions using prompts</h3>
<p>This function generates quiz questions based on a given category by referencing relevant subjects and facts from <code>quiz_bank</code>. It demonstrates string manipulation and formatting in Python to construct meaningful quiz questions.</p>
<div class="highlight"><pre><span/><code><span class="k">def</span><span class="w"> </span><span class="nf">generate_quiz_questions</span><span class="p">(</span><span class="n">category</span><span class="p">):</span>
    <span class="c1"># A list to store generated questions</span>
    <span class="n">generated_questions</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># Iterate over each subject in the quiz bank</span>
    <span class="k">for</span> <span class="n">subject</span><span class="p">,</span> <span class="n">details</span> <span class="ow">in</span> <span class="n">quiz_bank</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="c1"># Check whether the category appears in the subject’s categories</span>
        <span class="k">if</span> <span class="n">category</span> <span class="ow">in</span> <span class="n">details</span><span class="p">[</span><span class="s2">"categories"</span><span class="p">]:</span>
            <span class="c1"># For each fact, create a question and add it to the list</span>
            <span class="k">for</span> <span class="n">fact</span> <span class="ow">in</span> <span class="n">details</span><span class="p">[</span><span class="s2">"facts"</span><span class="p">]:</span>
                <span class="n">question</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"What is described by the following fact: </span><span class="si">{</span><span class="n">fact</span><span class="si">}</span><span class="s2">? Answer: </span><span class="si">{</span><span class="n">subject</span><span class="si">}</span><span class="s2">."</span>
                <span class="n">generated_questions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">question</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">generated_questions</span>

<span class="c1"># Example usage</span>
<span class="n">history_questions</span> <span class="o">=</span> <span class="n">generate_quiz_questions</span><span class="p">(</span><span class="s2">"History"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">question</span> <span class="ow">in</span> <span class="n">history_questions</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">question</span><span class="p">)</span>
</code></pre></div>
<h3 id="task-3-implementing-langchainstyle-prompt-structuring">Task 3: Implementing LangChain‑style prompt structuring</h3>
<p>To simulate structuring a quiz prompt as it might be done with LangChain, we can define a Python function that formats a list of quiz questions into a structured prompt. This structured prompt imitates detailed instructions and formatting that would guide an LLM in generating or processing quiz content.</p>
<div class="highlight"><pre><span/><code><span class="k">def</span><span class="w"> </span><span class="nf">structure_quiz_prompt</span><span class="p">(</span><span class="n">quiz_questions</span><span class="p">):</span>
    <span class="c1"># Define a delimiter for separating questions</span>
    <span class="n">section_delimiter</span> <span class="o">=</span> <span class="s2">"####"</span>

    <span class="c1"># Start with an introductory instruction</span>
    <span class="n">structured_prompt</span> <span class="o">=</span> <span class="s2">"Instructions for generating a personalized quiz:</span><span class="se">\n</span><span class="s2">Each question is separated by four hash symbols (####)</span><span class="se">\n\n</span><span class="s2">"</span>

    <span class="c1"># Add each question, separated by the delimiter</span>
    <span class="k">for</span> <span class="n">question</span> <span class="ow">in</span> <span class="n">quiz_questions</span><span class="p">:</span>
        <span class="n">structured_prompt</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">section_delimiter</span><span class="si">}</span><span class="se">\n</span><span class="si">{</span><span class="n">question</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span>

    <span class="k">return</span> <span class="n">structured_prompt</span>

<span class="c1"># Example usage</span>
<span class="n">quiz_questions</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">"In which year was the Declaration of Independence signed?"</span><span class="p">,</span>
    <span class="s2">"Who invented the telephone?"</span>
<span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">structure_quiz_prompt</span><span class="p">(</span><span class="n">quiz_questions</span><span class="p">))</span>
</code></pre></div>
<p>This function accepts a list of quiz questions and returns a single string that structures them to simulate input for a quiz‑generation LLM, using the specified delimiter to separate questions.</p>
<h3 id="task-4-quiz-generation-pipeline">Task 4: Quiz generation pipeline</h3>
<div class="highlight"><pre><span/><code><span class="k">def</span><span class="w"> </span><span class="nf">generate_quiz_questions</span><span class="p">(</span><span class="n">category</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Simulates generating quiz questions based on a category.</span>
<span class="sd">    """</span>
    <span class="c1"># Placeholder for simple generation logic by category</span>
    <span class="n">questions</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">"Science"</span><span class="p">:</span> <span class="p">[</span><span class="s2">"What is the chemical symbol for water?"</span><span class="p">,</span> <span class="s2">"Which planet is known as the Red Planet?"</span><span class="p">],</span>
        <span class="s2">"History"</span><span class="p">:</span> <span class="p">[</span><span class="s2">"Who was the first President of the United States?"</span><span class="p">,</span> <span class="s2">"In what year did the Titanic sink?"</span><span class="p">]</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">questions</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">category</span><span class="p">,</span> <span class="p">[])</span>

<span class="k">def</span><span class="w"> </span><span class="nf">structure_quiz_prompt</span><span class="p">(</span><span class="n">quiz_questions</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Structures a chat prompt with the provided quiz questions.</span>
<span class="sd">    """</span>
    <span class="n">section_delimiter</span> <span class="o">=</span> <span class="s2">"####"</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="s2">"Generated quiz questions:</span><span class="se">\n\n</span><span class="s2">"</span>
    <span class="k">for</span> <span class="n">question</span> <span class="ow">in</span> <span class="n">quiz_questions</span><span class="p">:</span>
        <span class="n">prompt</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">section_delimiter</span><span class="si">}</span><span class="s2"> Question: </span><span class="si">{</span><span class="n">question</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span>
    <span class="k">return</span> <span class="n">prompt</span>

<span class="k">def</span><span class="w"> </span><span class="nf">select_language_model</span><span class="p">():</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Simulates selecting a language model.</span>
<span class="sd">    """</span>
    <span class="c1"># For this example, assume the model is a constant string</span>
    <span class="k">return</span> <span class="s2">"gpt-3.5-turbo"</span>

<span class="k">def</span><span class="w"> </span><span class="nf">execute_language_model</span><span class="p">(</span><span class="n">prompt</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Simulates executing the selected language model with the given prompt.</span>
<span class="sd">    """</span>
    <span class="c1"># Normally this would send the prompt to the model and receive output.</span>
    <span class="c1"># Here we simulate it by echoing the prompt with a confirmation.</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s2">"The model received the following prompt: </span><span class="si">{</span><span class="n">prompt</span><span class="si">}</span><span class="se">\n</span><span class="s2">Model: 'Questions created successfully.'"</span>

<span class="k">def</span><span class="w"> </span><span class="nf">generate_quiz_pipeline</span><span class="p">(</span><span class="n">category</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Simulates creating and executing a quiz generation pipeline using placeholders.</span>
<span class="sd">    """</span>
    <span class="c1"># Step 1: Generate questions based on the chosen category</span>
    <span class="n">quiz_questions</span> <span class="o">=</span> <span class="n">generate_quiz_questions</span><span class="p">(</span><span class="n">category</span><span class="p">)</span>

    <span class="c1"># Step 2: Structure the prompt with the generated questions</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">structure_quiz_prompt</span><span class="p">(</span><span class="n">quiz_questions</span><span class="p">)</span>

    <span class="c1"># Step 3: Select the language model to simulate</span>
    <span class="n">model_name</span> <span class="o">=</span> <span class="n">select_language_model</span><span class="p">()</span>

    <span class="c1"># Step 4: Execute the language model with the structured prompt</span>
    <span class="n">model_output</span> <span class="o">=</span> <span class="n">execute_language_model</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>

    <span class="c1"># Final: Return a message simulating pipeline execution</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s2">"Pipeline executed using model: </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">. Output: </span><span class="si">{</span><span class="n">model_output</span><span class="si">}</span><span class="s2">"</span>

<span class="c1"># Example usage</span>
<span class="nb">print</span><span class="p">(</span><span class="n">generate_quiz_pipeline</span><span class="p">(</span><span class="s2">"Science"</span><span class="p">))</span>
</code></pre></div>
<p>This set of functions simulates a quiz generation pipeline: generating questions based on a category, structuring them into a prompt, selecting a model, and executing it to produce mock output.</p>
<h3 id="task-5-reusable-quiz-generation-function">Task 5: Reusable quiz generation function</h3>
<div class="highlight"><pre><span/><code><span class="k">def</span><span class="w"> </span><span class="nf">create_structured_prompt</span><span class="p">(</span><span class="n">system_prompt_message</span><span class="p">,</span> <span class="n">user_question_template</span><span class="o">=</span><span class="s2">"</span><span class="si">{question}</span><span class="s2">"</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Creates a structured prompt using a system message and a user question template.</span>
<span class="sd">    """</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">"System instructions: </span><span class="si">{</span><span class="n">system_prompt_message</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span>
        <span class="sa">f</span><span class="s2">"User template: </span><span class="si">{</span><span class="n">user_question_template</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">prompt</span>

<span class="k">def</span><span class="w"> </span><span class="nf">select_language_model</span><span class="p">():</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Simulates selecting a language model and temperature.</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="s2">"gpt-3.5-turbo"</span><span class="p">,</span> <span class="mi">0</span>

<span class="k">def</span><span class="w"> </span><span class="nf">simulate_model_response</span><span class="p">(</span><span class="n">structured_prompt</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Simulates generating a response from the selected language model based on the structured prompt.</span>
<span class="sd">    """</span>
    <span class="c1"># Here the actual API call to the language model would occur</span>
    <span class="c1"># For simulation purposes we return a mock response</span>
    <span class="k">return</span> <span class="s2">"A mock quiz has been generated based on the structured prompt."</span>

<span class="k">def</span><span class="w"> </span><span class="nf">setup_output_parser</span><span class="p">(</span><span class="n">model_output</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Simulates configuring an output parser for formatting the model’s response.</span>
<span class="sd">    """</span>
    <span class="c1"># Simple formatting for demonstration</span>
    <span class="n">formatted_output</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"Formatted quiz: </span><span class="si">{</span><span class="n">model_output</span><span class="si">}</span><span class="s2">"</span>
    <span class="k">return</span> <span class="n">formatted_output</span>

<span class="k">def</span><span class="w"> </span><span class="nf">generate_quiz_assistant_pipeline</span><span class="p">(</span><span class="n">system_prompt_message</span><span class="p">,</span> <span class="n">user_question_template</span><span class="o">=</span><span class="s2">"</span><span class="si">{question}</span><span class="s2">"</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Creating a structured prompt with the system message and user question template..."</span><span class="p">)</span>
    <span class="n">structured_prompt</span> <span class="o">=</span> <span class="n">create_structured_prompt</span><span class="p">(</span><span class="n">system_prompt_message</span><span class="p">,</span> <span class="n">user_question_template</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"Selecting language model: GPT-3.5-turbo with temperature 0"</span><span class="p">)</span>
    <span class="n">model_name</span><span class="p">,</span> <span class="n">temperature</span> <span class="o">=</span> <span class="n">select_language_model</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"Simulating language model response..."</span><span class="p">)</span>
    <span class="n">model_output</span> <span class="o">=</span> <span class="n">simulate_model_response</span><span class="p">(</span><span class="n">structured_prompt</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"Configuring output parser to format responses"</span><span class="p">)</span>
    <span class="n">formatted_output</span> <span class="o">=</span> <span class="n">setup_output_parser</span><span class="p">(</span><span class="n">model_output</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"Assembling components into a quiz generation pipeline..."</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">formatted_output</span>

<span class="c1"># Example usage with a detailed system prompt</span>
<span class="n">system_prompt_message</span> <span class="o">=</span> <span class="s2">"Please generate a quiz based on the following categories: Science, History."</span>
<span class="nb">print</span><span class="p">(</span><span class="n">generate_quiz_assistant_pipeline</span><span class="p">(</span><span class="n">system_prompt_message</span><span class="p">))</span>
</code></pre></div>
<p>These functions provide a basic simulation of the processes involved in structuring prompts for AI‑based quiz generation, assembling the pipeline to perform that generation, and creating a reusable function for generating quizzes with customizable parameters.</p>
<h3 id="task-6-evaluating-generated-quiz-content">Task 6: Evaluating generated quiz content</h3>
<p>This function accepts generated quiz content and a list of expected keywords to ensure the output aligns with expected topics or subjects. It raises an assertion error if none of the expected keywords are present, indicating a mismatch between expected and generated content.</p>
<div class="highlight"><pre><span/><code><span class="k">def</span><span class="w"> </span><span class="nf">evaluate_quiz_content</span><span class="p">(</span><span class="n">generated_content</span><span class="p">,</span> <span class="n">expected_keywords</span><span class="p">):</span>
    <span class="c1"># Check whether any expected keyword appears in the generated content</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span><span class="n">keyword</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="n">generated_content</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">keyword</span> <span class="ow">in</span> <span class="n">expected_keywords</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">"The generated content does not contain any of the expected keywords."</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"The generated content successfully contains the expected keywords."</span><span class="p">)</span>

<span class="c1"># Example usage</span>
<span class="n">generated_content</span> <span class="o">=</span> <span class="s2">"The law of universal gravitation was formulated by Isaac Newton in the 17th century."</span>
<span class="n">expected_keywords</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"gravity"</span><span class="p">,</span> <span class="s2">"Newton"</span><span class="p">,</span> <span class="s2">"physics"</span><span class="p">]</span>
<span class="n">evaluate_quiz_content</span><span class="p">(</span><span class="n">generated_content</span><span class="p">,</span> <span class="n">expected_keywords</span><span class="p">)</span>
</code></pre></div>
<h3 id="task-7-handling-invalid-quiz-requests">Task 7: Handling invalid quiz requests</h3>
<p>This function simulates evaluating the system’s response to an invalid quiz request. It verifies whether the generated refusal matches the expected refusal response, confirming correct handling of requests the system cannot fulfill.</p>
<div class="highlight"><pre><span/><code><span class="k">def</span><span class="w"> </span><span class="nf">evaluate_request_refusal</span><span class="p">(</span><span class="n">invalid_request</span><span class="p">,</span> <span class="n">expected_response</span><span class="p">):</span>
    <span class="c1"># Simulate generating a response to an invalid request</span>
    <span class="n">generated_response</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"Unable to generate a quiz for: </span><span class="si">{</span><span class="n">invalid_request</span><span class="si">}</span><span class="s2">"</span>  <span class="c1"># Placeholder for an actual refusal response</span>

    <span class="c1"># Check whether the generated response matches the expected refusal response</span>
    <span class="k">assert</span> <span class="n">generated_response</span> <span class="o">==</span> <span class="n">expected_response</span><span class="p">,</span> <span class="s2">"The refusal response does not match the expected response."</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"The refusal response correctly matches the expected response."</span><span class="p">)</span>

<span class="c1"># Example usage</span>
<span class="n">invalid_request</span> <span class="o">=</span> <span class="s2">"Generate a quiz about unicorns."</span>
<span class="n">expected_response</span> <span class="o">=</span> <span class="s2">"Unable to generate a quiz for: Generate a quiz about unicorns."</span>
<span class="n">evaluate_request_refusal</span><span class="p">(</span><span class="n">invalid_request</span><span class="p">,</span> <span class="n">expected_response</span><span class="p">)</span>
</code></pre></div>
<h3 id="task-8-science-quiz-evaluation-test">Task 8: Science quiz evaluation test</h3>
<p>This function demonstrates using <code>evaluate_quiz_content</code> in a specific test scenario—checking that a generated science quiz includes questions related to expected science topics. It simulates generating quiz content and then evaluates it for science‑oriented keywords.</p>
<div class="highlight"><pre><span/><code><span class="k">def</span><span class="w"> </span><span class="nf">test_science_quiz</span><span class="p">():</span>
    <span class="c1"># Simulate generating quiz content</span>
    <span class="n">generated_content</span> <span class="o">=</span> <span class="s2">"The study of the natural world through observation and experiment is known as science. Key subjects include biology, chemistry, physics, and Earth sciences."</span>

    <span class="c1"># Define expected keywords or subjects for a science quiz</span>
    <span class="n">expected_science_subjects</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"biology"</span><span class="p">,</span> <span class="s2">"chemistry"</span><span class="p">,</span> <span class="s2">"physics"</span><span class="p">,</span> <span class="s2">"Earth sciences"</span><span class="p">]</span>

    <span class="c1"># Use evaluate_quiz_content to check for expected keywords</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">evaluate_quiz_content</span><span class="p">(</span><span class="n">generated_content</span><span class="p">,</span> <span class="n">expected_science_subjects</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Science quiz content evaluation passed successfully."</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">AssertionError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Science quiz content evaluation failed: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="c1"># Example usage</span>
<span class="n">test_science_quiz</span><span class="p">()</span>
</code></pre></div>
<p>Taken together, these functions provide mechanisms for evaluating the relevance and accuracy of generated quiz content, handling invalid requests appropriately, and running targeted tests to ensure quiz content meets specific educational or thematic criteria.</p>












                
                  
</body>
</html>