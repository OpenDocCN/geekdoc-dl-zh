- en: 3.3 AI Quiz Generation Mechanism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://boramorka.github.io/LLM-Book/en/CHAPTER-3/3.3%20AI%20Quiz%20Generation%20Mechanism/](https://boramorka.github.io/LLM-Book/en/CHAPTER-3/3.3%20AI%20Quiz%20Generation%20Mechanism/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'This chapter assembles a working AI‑powered quiz generator end to end: we set
    up the environment and access to external services, prepare a compact dataset
    of subjects/categories/facts, design a prompt so questions strictly match the
    chosen category, and wire it all into a LangChain pipeline. We start with environment
    setup and keys; to keep output clean you can suppress non‑essential warnings.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we form the app’s backbone — a compact dataset from which questions will
    be composed: we fix subjects, categories, and facts that quizzes will be built
    from.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'To ensure questions are relevant to the user’s selected category, we design
    a detailed prompt template: from category selection via the quiz bank to formulating
    questions in the prescribed format.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'With this template, we move to LangChain: form a ChatPrompt, select a model,
    and a parser to normalize the response into a readable form.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now we connect everything using the LangChain Expression Language into a single
    pipeline for reproducible generation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Next, encapsulate the setup and execution of quiz generation into a single reusable
    function. This increases modularity and simplifies maintenance. `generate_quiz_assistant_pipeline`
    bundles prompt creation, model selection, and parsing into one workflow.
  prefs: []
  type: TYPE_NORMAL
- en: 'Quick overview: `generate_quiz_assistant_pipeline` is flexible and allows plugging
    in different templates and configurations (models/parsers). Function definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Practical usage. The function hides the complexity of composing components:
    simply call `generate_quiz_assistant_pipeline` with the required arguments to
    generate topic/category quizzes and easily integrate into larger systems. A few
    practical tips:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Configuration**: use parameters to flexibly tune the process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model choice**: experiment with models for quality/creativity trade‑offs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prompt design**: plan `user_question_template` and `system_prompt_message`
    thoughtfully.'
  prefs:
  - PREF_UL
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Error handling**: account for API limits and unexpected responses.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Including this function in your project simplifies creating AI‑driven quizzes,
    enabling innovative educational tools and interactive content.
  prefs: []
  type: TYPE_NORMAL
- en: 'To add quality checks, introduce `evaluate_quiz_content`: it verifies that
    the generated quiz contains the expected topic keywords — essential for relevance
    and correctness in learning scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now about content evaluation. The function integrates with the generation pipeline:
    it accepts the system message (instructions/context), a specific request (e.g.,
    a topic for the quiz), and a list of expected words/phrases that should appear
    in the result. Function definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Consider an example: generate and evaluate a science quiz.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This example shows how `evaluate_quiz_content` can confirm that a science quiz
    includes relevant themes (figures, instruments, concepts). Good practices:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Keyword selection** — make them specific enough but leave room for variation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Broad checks** — use multiple keyword sets for different topics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Iterative approach** — refine template/parameters/dataset based on evaluation
    results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Structured testing helps maintain quality and uncover opportunities to improve
    relevance and engagement.
  prefs: []
  type: TYPE_NORMAL
- en: 'To handle out‑of‑scope requests, introduce `evaluate_request_refusal`, which
    tests proper refusal in inappropriate scenarios. This matters for trust and user
    experience (UX): the function simulates cases where the system should refuse (based
    on relevance/constraints) and verifies that the expected refusal message is returned.
    Function definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: To illustrate `evaluate_request_refusal`, consider a scenario where the quiz
    generator should refuse to create a quiz because the request is outside its scope
    or unsupported by the current configuration.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This example demonstrates how to test the quiz generator’s response to a request
    that should be declined: by checking for the expected refusal message, we ensure
    the system behaves correctly when facing requests it cannot fulfill. Tips and
    suggestions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Clear refusal messages**: make them informative so users understand why the
    request cannot be completed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Comprehensive testing**: use diverse scenarios, including unsupported topics
    or formats, to thoroughly evaluate refusal logic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Refinement and feedback**: iterate on refusal logic and messaging to improve
    user understanding and satisfaction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consider UX**: where possible, offer alternatives or suggestions to maintain
    a positive interaction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing and testing refusal scenarios ensures the quiz generator can reliably
    handle a wide range of requests, maintaining robustness and user trust even when
    it cannot provide the requested content.
  prefs: []
  type: TYPE_NORMAL
- en: To adapt the provided template to a practical test scenario focused on a science‑themed
    quiz, we add a `test_science_quiz` function. It evaluates whether AI‑generated
    quiz questions truly center on expected scientific topics or subjects. By integrating
    `evaluate_quiz_content`, we can ensure the quiz includes specific keywords or
    themes characteristic of the science category.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we tailor `evaluate_quiz_content` for a science test case: the function
    checks whether the generated content aligns with expected scientific themes. Function
    definition for testing a science quiz:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This function encapsulates the validation logic: for a science request, content
    must contain expected science themes/keywords. Calling `test_science_quiz` simulates
    the request and checks for scientific themes — a key indicator of correct generation.
    Refine the keyword list for your domain and coverage, expand tests for other categories
    (history/geography/art), and analyze failures: compare expectations with results
    to improve prompt logic/dataset. Structured testing helps maintain quality and
    discover opportunities to improve relevance and engagement.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly — a quick look at CI/CD: the `.circleci/config.yml` file in the repository
    root describes a YAML‑based pipeline (build/test/deploy). Below is a sketch for
    a Python project with automated tests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Key elements: `version` — the config version (commonly 2.1); `orbs` — reusable
    blocks, here the `python` orb helps with environment setup; `jobs` — a set of
    tasks, here a single `build-and-test`; `docker` — the image to run (e.g., `cimg/python:3.8`);
    `steps` — the sequence (checkout, cache, dependencies, tests); `workflows` — ties
    jobs into a process and triggers them by rule.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To customize: pick your Python version under `docker`, replace `pytest` with
    your test command, and add extra steps (DB, env vars, etc.) as additional `- run:`
    blocks. After committing `.circleci/config.yml`, CircleCI detects the configuration
    and will run the pipeline on each commit per your rules.'
  prefs: []
  type: TYPE_NORMAL
- en: Theoretical Questions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What components are necessary to set up the environment for an AI‑based quiz
    generator?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you structure a dataset for generating quiz questions? Include examples
    of categories and facts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does prompt engineering influence customized quiz generation? Provide a
    sample prompt template.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain LangChain’s role in structuring prompts for LLM processing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What constitutes the quiz generation pipeline when using the LangChain Expression
    Language?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can functions for evaluation ensure the relevance and accuracy of generated
    quiz content?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Describe a method for testing the system’s ability to refuse quiz generation
    under certain conditions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can you test LLM‑generated quiz questions for alignment with expected science
    topics or subjects?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Describe the key components of a CircleCI configuration file for a Python project,
    including automated test execution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Discuss the importance of customizing the CircleCI config to match a project’s
    specific needs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Practical Assignments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Create a quiz dataset: Define a Python dictionary named `quiz_bank` representing
    a collection of quiz entries, each containing subjects, categories, and facts
    similar to the example. Ensure your dictionary supports easy access to subjects,
    categories, and facts.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Generate quiz questions using prompts: Implement a function `generate_quiz_questions(category)`
    that accepts a category (e.g., "History", "Technology") as input and returns a
    list of generated quiz questions based on subjects and facts from `quiz_bank`.
    Use string operations or templates to construct the questions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Implement LangChain‑style prompt structuring: Simulate using LangChain’s capabilities
    by writing a function `structure_quiz_prompt(quiz_questions)` that accepts a list
    of quiz questions and returns a structured chat prompt in a format similar to
    the one described, without actually integrating LangChain.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Quiz generation pipeline: Create a Python function `generate_quiz_pipeline()`
    that simulates creating and running a quiz generation pipeline using placeholders
    for LangChain components. The function should print a message emulating pipeline
    execution.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Reusable quiz generation function: Implement a Python function `generate_quiz_assistant_pipeline(system_prompt_message,
    user_question_template="{question}")` that simulates assembling the components
    needed for quiz generation. Use string formatting to construct the detailed prompt
    from inputs.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Evaluate generated quiz content: Write a function `evaluate_quiz_content(generated_content,
    expected_keywords)` that accepts generated quiz content and a list of expected
    keywords, and checks whether the content contains any of the keywords. Raise an
    assertion error with a custom message if none are found.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Handle invalid quiz requests: Develop a function `evaluate_request_refusal(invalid_request,
    expected_response)` that simulates evaluating the system’s response to an invalid
    quiz request. The function should verify whether the refusal text matches the
    expected refusal response.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Science Quiz Evaluation Test: Develop a Python function `test_science_quiz()`
    that uses the `evaluate_quiz_content` function to test if a generated science
    quiz includes questions related to expected scientific topics, such as "physics"
    or "chemistry".'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
