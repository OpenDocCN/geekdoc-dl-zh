["```py\n[](#cb1-1)import torch.nn as nn\n[](#cb1-2)# inputs_chosen / inputs_rejected include the prompt tokens x and the respective\n[](#cb1-3)# completion tokens (y_c or y_r) that the reward model scores jointly.\n[](#cb1-4)rewards_chosen = model(**inputs_chosen)\n[](#cb1-5)rewards_rejected = model(**inputs_rejected)\n[](#cb1-6)\n[](#cb1-7)loss = -nn.functional.logsigmoid(rewards_chosen - rewards_rejected).mean()\n```", "```py\n[](#cb2-1)import torch\n[](#cb2-2)import torch.nn as nn\n[](#cb2-3)import torch.nn.functional as F\n[](#cb2-4)\n[](#cb2-5)class BradleyTerryRewardModel(nn.Module):\n[](#cb2-6)    \"\"\"\n[](#cb2-7) Standard scalar reward model for Bradley-Terry preference learning.\n[](#cb2-8)\n[](#cb2-9) Usage (pairwise BT loss):\n[](#cb2-10) rewards_chosen = model(**inputs_chosen)    # (batch,)\n[](#cb2-11) rewards_rejected = model(**inputs_rejected)  # (batch,)\n[](#cb2-12) loss = -F.logsigmoid(rewards_chosen - rewards_rejected).mean()\n[](#cb2-13) \"\"\"\n[](#cb2-14)    def __init__(self, base_lm):\n[](#cb2-15)        super().__init__()\n[](#cb2-16)        self.lm = base_lm  # e.g., AutoModelForCausalLM\n[](#cb2-17)        self.head = nn.Linear(self.lm.config.hidden_size, 1)\n[](#cb2-18)\n[](#cb2-19)    def _sequence_rep(self, hidden, attention_mask):\n[](#cb2-20)        \"\"\"\n[](#cb2-21) Get a single vector per sequence to score.\n[](#cb2-22) Default: last non-padding token (EOS token); if no mask, last token.\n[](#cb2-23) hidden: (batch, seq_len, hidden_size)\n[](#cb2-24) attention_mask: (batch, seq_len)\n[](#cb2-25) \"\"\"\n[](#cb2-26)\n[](#cb2-27)        # Index of last non-pad token in each sequence\n[](#cb2-28)        # attention_mask is 1 for real tokens, 0 for padding\n[](#cb2-29)        lengths = attention_mask.sum(dim=1) - 1  # (batch,)\n[](#cb2-30)        batch_idx = torch.arange(hidden.size(0), device=hidden.device)\n[](#cb2-31)        return hidden[batch_idx, lengths]  # (batch, hidden_size)\n[](#cb2-32)\n[](#cb2-33)    def forward(self, input_ids, attention_mask):\n[](#cb2-34)        \"\"\"\n[](#cb2-35) A forward pass designed to show inference structure of a standard reward model.\n[](#cb2-36) To train one, this function will need to be modified to compute rewards from both\n[](#cb2-37) chosen and rejected inputs, applying the loss above.\n[](#cb2-38) \"\"\"\n[](#cb2-39)        outputs = self.lm(\n[](#cb2-40)            input_ids=input_ids,\n[](#cb2-41)            attention_mask=attention_mask,\n[](#cb2-42)            output_hidden_states=True,\n[](#cb2-43)            return_dict=True,\n[](#cb2-44)        )\n[](#cb2-45)        # Final hidden states: (batch, seq_len, hidden_size)\n[](#cb2-46)        hidden = outputs.hidden_states[-1]\n[](#cb2-47)\n[](#cb2-48)        # One scalar reward per sequence: (batch,)\n[](#cb2-49)        seq_repr = self._sequence_rep(hidden, attention_mask)\n[](#cb2-50)        rewards = self.head(seq_repr).squeeze(-1)\n[](#cb2-51)\n[](#cb2-52)        return rewards\n```", "```py\n[](#cb3-1)import torch.nn as nn\n[](#cb3-2)import torch.nn.functional as F\n[](#cb3-3)\n[](#cb3-4)class OutcomeRewardModel(nn.Module):\n[](#cb3-5)    def __init__(self, base_lm):\n[](#cb3-6)        super().__init__()\n[](#cb3-7)        self.lm = base_lm  # e.g., AutoModelForCausalLM\n[](#cb3-8)        self.head = nn.Linear(self.lm.config.hidden_size, 1)\n[](#cb3-9)\n[](#cb3-10)    def forward(self, input_ids, attention_mask=None, labels=None):\n[](#cb3-11)        \"\"\"\n[](#cb3-12) The input data here will be tokenized prompts and completions along with labels\n[](#cb3-13) per prompt for correctness.\n[](#cb3-14) \"\"\"\n[](#cb3-15)        outputs = self.lm(\n[](#cb3-16)            input_ids=input_ids,\n[](#cb3-17)            attention_mask=attention_mask,\n[](#cb3-18)            output_hidden_states=True,\n[](#cb3-19)            return_dict=True,\n[](#cb3-20)        )\n[](#cb3-21)        # Final hidden states: (batch, seq_len, hidden_size)\n[](#cb3-22)        hidden = outputs.hidden_states[-1]\n[](#cb3-23)        # One scalar logit per token: (batch, seq_len)\n[](#cb3-24)        logits = self.head(hidden).squeeze(-1)\n[](#cb3-25)\n[](#cb3-26)        # Only compute loss on completion tokens (labels 0 or 1)\n[](#cb3-27)        # Prompt tokens have labels = -100\n[](#cb3-28)        mask = labels != -100\n[](#cb3-29)        if mask.any():\n[](#cb3-30)            loss = F.binary_cross_entropy_with_logits(\n[](#cb3-31)                logits[mask], labels[mask].float()\n[](#cb3-32)            )\n[](#cb3-33)        return loss, logits\n```", "```py\n[](#cb4-1)# Assume model already has: model.lm (backbone) + model.head\n[](#cb4-2)hidden = model.lm(**inputs, output_hidden_states=True).hidden_states[-1]\n[](#cb4-3)logits_per_token = model.head(hidden).squeeze(-1)  # (batch, seq_len)\n[](#cb4-4)# This will sometimes be compressed as model.forward() in other implementations\n[](#cb4-5)\n[](#cb4-6)# Binary labels: 1=correct, 0=incorrect (prompt tokens masked as -100)\n[](#cb4-7)mask = labels != -100\n[](#cb4-8)loss = F.binary_cross_entropy_with_logits(\n[](#cb4-9)    logits_per_token[mask], labels[mask].float()\n[](#cb4-10))\n```", "```py\n# Get the ID of the separator token and add it to the completions\nseparator_ids = tokenizer.encode(step_separator, add_special_tokens=False)\ncompletions_ids = [completion + separator_ids for completion in completions_ids]\n\n# Create the label \nlabels = [[-100] * (len(completion) - 1) + [label] for completion, label in zip(completions_ids, labels)]\n```", "```py\n[](#cb6-1)import torch.nn as nn\n[](#cb6-2)import torch.nn.functional as F\n[](#cb6-3)\n[](#cb6-4)class ProcessRewardModel(nn.Module):\n[](#cb6-5)    def __init__(self, base_lm, num_classes=3):\n[](#cb6-6)        super().__init__()\n[](#cb6-7)        self.lm = base_lm  # e.g., AutoModelForCausalLM\n[](#cb6-8)        self.head = nn.Linear(self.lm.config.hidden_size, num_classes)\n[](#cb6-9)\n[](#cb6-10)    def forward(self, input_ids, attention_mask=None, labels=None):\n[](#cb6-11)        \"\"\"\n[](#cb6-12) The inputs are tokenizer prompts and completions, where the the end of a \n[](#cb6-13) \"reasoning step\" is denoted by another non-padding token. \n[](#cb6-14) labels will be a list of labels, True, False, and Neutral (3 labels) which\n[](#cb6-15) will be predicted by the model.\n[](#cb6-16) \"\"\"\n[](#cb6-17)        outputs = self.lm(\n[](#cb6-18)            input_ids=input_ids,\n[](#cb6-19)            attention_mask=attention_mask,\n[](#cb6-20)            output_hidden_states=True,\n[](#cb6-21)            return_dict=True,\n[](#cb6-22)        )\n[](#cb6-23)        # Final hidden states: (batch, seq_len, hidden_size)\n[](#cb6-24)        hidden = outputs.hidden_states[-1]\n[](#cb6-25)        # One logit vector per token: (batch, seq_len, num_classes)\n[](#cb6-26)        logits = self.head(hidden)\n[](#cb6-27)\n[](#cb6-28)        # Only compute loss at step boundaries (where labels != -100)\n[](#cb6-29)        # Labels map: -1 -> 0, 0 -> 1, 1 -> 2 (class indices)\n[](#cb6-30)        mask = labels != -100\n[](#cb6-31)        if mask.any():\n[](#cb6-32)            loss = F.cross_entropy(\n[](#cb6-33)                logits[mask], labels[mask]\n[](#cb6-34)            )\n[](#cb6-35)        return loss, logits\n```", "```py\n[](#cb7-1)# Assume model outputs 3-class logits per token\n[](#cb7-2)hidden = model.lm(**inputs, output_hidden_states=True).hidden_states[-1]\n[](#cb7-3)logits = model.head(hidden)  # (batch, seq_len, 3)\n[](#cb7-4)\n[](#cb7-5)# 3-class labels at step boundaries only: 0=-1, 1=0, 2=1 (others masked as -100)\n[](#cb7-6)mask = labels != -100\n[](#cb7-7)loss = F.cross_entropy(logits[mask], labels[mask])\n```", "```py\n[System]\nPlease act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below.\nYou should choose the assistant that follows the user's instructions and answers the user's question better.\nYour evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses.\nBegin your evaluation by comparing the two responses and provide a short explanation.\nAvoid any position biases and ensure that the order in which the responses were presented does not influence your decision.\nDo not allow the length of the responses to influence your evaluation.\nDo not favor certain names of the assistants.\nBe as objective as possible.\nAfter providing your explanation, output your final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\" if assistant B is better, and \"[[C]]\" for a tie.\n[User Question]\n{question}\n[The Start of Assistant A's Answer]\n{answer_a}\n[The End of Assistant A's Answer]\n[The Start of Assistant B's Answer]\n{answer_b}\n[The End of Assistant B's Answer]\n```"]