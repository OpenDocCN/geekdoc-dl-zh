["```c\n[](#cb1-1)Small file, giant dream,\n[](#cb1-2)llm.c whispers tokens,\n[](#cb1-3)worlds unfold in text.\n```", "```c\n[](#cb2-1)make train_gpt2\n```", "```c\n[](#cb3-1)./train_gpt2\n[](#cb3-2)./train_gpt2cu\n[](#cb3-3)./test_gpt2\n```", "```c\n[](#cb4-1)make train_gpt2cu USE_CUDNN=1\n```", "```c\n[](#cb5-1)make train_gpt2 DEBUG=1\n```", "```c\n[](#cb6-1)make train_gpt2\n```", "```c\n[](#cb7-1)./dev/download_starter_pack.sh\n```", "```c\n[](#cb8-1)./train_gpt2\n```", "```c\n[GPT-2]\nmax_seq_len: 1024\nvocab_size: 50257\npadded_vocab_size: 50304\nnum_layers: 12\nnum_heads: 12\nchannels: 768\nnum_parameters: 124475904\ntrain dataset num_batches: 1192\nval dataset num_batches: 128\nnum_activations: 73347840\nval loss 5.325529\nstep 0: train loss 4.677779 (took 1987.546000 ms)\nstep 1: train loss 5.191576 (took 1927.230000 ms)\n...\n```", "```c\n    [](#cb10-1)OMP_NUM_THREADS=8 ./train_gpt2\n    ```", "```c\n[](#cb11-1)make train_gpt2_fp32\n```", "```c\n[](#cb12-1)./dev/download_starter_pack.sh\n```", "```c\n[](#cb13-1)./train_gpt2_fp32\n```", "```c\n[](#cb14-1)make train_gpt2cu\n```", "```c\n    [](#cb15-1)make train_gpt2cu USE_CUDNN=1\n    ```", "```c\n    [](#cb16-1)make train_gpt2cu DEBUG=1\n    ```", "```c\n[](#cb17-1)./dev/download_starter_pack.sh\n```", "```c\n[](#cb18-1)./train_gpt2cu\n```", "```c\n    [](#cb19-1)OMP_NUM_THREADS=8 ./train_gpt2cu\n    ```", "```c\n    [](#cb20-1)./train_gpt2cu --batch_size 4 --micro_batch_size 1 --seq_len 512\n    ```", "```c\n    [](#cb21-1)./train_gpt2cu --warmup_steps 1000 --lr 6e-4 --scheduler cosine\n    ```", "```c\n    [](#cb22-1)./train_gpt2cu --eval_interval 200 --eval_batches 50\n    ```", "```c\n    [](#cb23-1)make test_gpt2cu\n    [](#cb23-2)./test_gpt2cu\n    ```", "```c\n[](#cb24-1)make train_gpt2cu USE_CUDNN=1\n[](#cb24-2)./train_gpt2cu\n```", "```c\n[](#cb25-1)./dev/download_starter_pack.sh\n```", "```c\ngpt2_tokenizer.bin\ntrain.bin\nval.bin\n```", "```c\n[](#cb27-1)make train_gpt2 DEBUG=1\n```", "```c\n[](#cb28-1)gdb ./train_gpt2\n```", "```c\n[](#cb29-1)printf(\"Step %d: loss=%f\\n\", step, loss);\n```", "```c\n[](#cb30-1)./dev/download_starter_pack.sh\n```", "```c\n[](#cb31-1)int tokenizer_init(Tokenizer *t, const char *filename);\n[](#cb31-2)int tokenizer_decode(Tokenizer *t, const int *ids, int n, char *out);\n[](#cb31-3)void tokenizer_free(Tokenizer *t);\n```", "```c\n    [](#cb32-1)Tokenizer t;\n    [](#cb32-2)tokenizer_init(&t, \"gpt2_tokenizer.bin\");\n    ```", "```c\n    [](#cb33-1)char buf[512];\n    [](#cb33-2)tokenizer_decode(&t, tokens, ntokens, buf);\n    [](#cb33-3)printf(\"%s\\n\", buf);\n    ```", "```c\n    [](#cb34-1)tokenizer_free(&t);\n    ```", "```c\n[](#cb35-1)int dataloader_init(Dataloader *loader, const char *filename, int B, int T);\n[](#cb35-2)int dataloader_next_batch(Dataloader *loader, int *inputs, int *targets);\n[](#cb35-3)void dataloader_reset(Dataloader *loader);\n[](#cb35-4)void dataloader_free(Dataloader *loader);\n```", "```c\n    [](#cb36-1)python dev/data/prepare_shakespeare.py\n    ```", "```c\n[](#cb37-1)int dataloader_init(Dataloader *loader, const char *filename, int B, int T);\n[](#cb37-2)int dataloader_next_batch(Dataloader *loader, int *inputs, int *targets);\n[](#cb37-3)void dataloader_reset(Dataloader *loader);\n[](#cb37-4)void dataloader_free(Dataloader *loader);\n```", "```c\n[](#cb38-1)for (step = 0; step < max_steps; step++) {\n[](#cb38-2)    dataloader_next_batch(&train_loader, inputs, targets);\n[](#cb38-3)    forward_backward_update(model, inputs, targets);\n[](#cb38-4) \n[](#cb38-5)    if (step % eval_interval == 0) {\n[](#cb38-6)        float val_loss = 0.0f;\n[](#cb38-7)        for (int i = 0; i < eval_batches; i++) {\n[](#cb38-8)            evalloader_next_batch(&val_loader, inputs, targets);\n[](#cb38-9)            val_loss += forward_only(model, inputs, targets);\n[](#cb38-10)        }\n[](#cb38-11)        val_loss /= eval_batches;\n[](#cb38-12)        printf(\"step %d: val loss %.4f\\n\", step, val_loss);\n[](#cb38-13)    }\n[](#cb38-14)}\n```", "```c\nmax_seq_len: 1024\n```", "```c\n[](#cb40-1)void manual_seed(uint64_t seed);\n[](#cb40-2)float normal_(float mean, float std);\n```", "```c\n[](#cb41-1)manual_seed(1337);\n```", "```c\n[](#cb42-1)assert(id >= 0 && id < vocab_size);\n```", "```c\nstep 0: train loss 5.19, val loss 5.32\nstep 100: train loss 4.87, val loss 5.01\nstep 200: train loss 4.62, val loss 4.88\n```", "```c\n[](#cb44-1)GPT2Config config = {\n[](#cb44-2)    .vocab_size = 50257,\n[](#cb44-3)    .max_seq_len = 1024,\n[](#cb44-4)    .num_layers = 12,\n[](#cb44-5)    .num_heads = 12,\n[](#cb44-6)    .channels = 768,\n[](#cb44-7)};\n```", "```c\n[](#cb45-1)float* params = (float*)mallocCheck(config.num_parameters * sizeof(float));\n```", "```c\n[](#cb46-1)float* token_embedding_table = params;\n```", "```c\n[](#cb47-1)float* token_embedding_table = params;  // first slice of parameters\n```", "```c\nembedding_out[token, pos] = token_embedding[token] + positional_embedding[pos]\n```", "```c\nscores = Q × K^T / sqrt(d_k)\nweights = softmax(scores + mask)\noutput  = weights × V\n```", "```c\nhidden = Linear1(x)\nhidden = GELU(hidden)\nout    = Linear2(hidden)\n```", "```c\n[](#cb51-1)float gelu(float x) {\n[](#cb51-2)    return 0.5f * x * (1.0f + tanhf(0.79788456f * (x + 0.044715f * x * x * x)));\n[](#cb51-3)}\n```", "```c\nmean = (1/d) * Σ x_i\nvar  = (1/d) * Σ (x_i - mean)^2\nx_norm = (x - mean) / sqrt(var + eps)\ny = γ * x_norm + β\n```", "```c\n[](#cb53-1)void layernorm_forward(float* out, float* inp, float* weight, float* bias, int N) {\n[](#cb53-2)    float mean = 0.0f, var = 0.0f;\n[](#cb53-3)    for (int i = 0; i < N; i++) mean += inp[i];\n[](#cb53-4)    mean /= N;\n[](#cb53-5)    for (int i = 0; i < N; i++) var += (inp[i] - mean) * (inp[i] - mean);\n[](#cb53-6)    var /= N;\n[](#cb53-7)    float inv_std = 1.0f / sqrtf(var + 1e-5f);\n[](#cb53-8)    for (int i = 0; i < N; i++) {\n[](#cb53-9)        out[i] = (inp[i] - mean) * inv_std * weight[i] + bias[i];\n[](#cb53-10)    }\n[](#cb53-11)}\n```", "```c\ny = F(x)\n```", "```c\ny = F(x) + x\n```", "```c\n[](#cb56-1)void residual_forward(float* out, float* inp1, float* inp2, int N) {\n[](#cb56-2)    for (int i = 0; i < N; i++) {\n[](#cb56-3)        out[i] = inp1[i] + inp2[i];\n[](#cb56-4)    }\n[](#cb56-5)}\n```", "```c\n[](#cb57-1)for (int t = 0; t < T; t++) {\n[](#cb57-2)    for (int u = t + 1; u < T; u++) {\n[](#cb57-3)        scores[t][u] = -1e9; // block future positions\n[](#cb57-4)    }\n[](#cb57-5)}\n```", "```c\nhidden states → Linear projection → Logits → Softmax → Probabilities\n```", "```c\n[](#cb59-1)// token embedding table\n[](#cb59-2)float* token_embedding_table = params;\n[](#cb59-3)// output head reuses the same memory\n[](#cb59-4)float* output_head = token_embedding_table;\n```", "```c\nprobs = softmax(logits)\n```", "```c\nloss = -log(p[true_token])\n```", "```c\n[](#cb62-1)float loss = 0.0f;\n[](#cb62-2)for (int b = 0; b < B; b++) {\n[](#cb62-3)    for (int t = 0; t < T; t++) {\n[](#cb62-4)        int target = targets[b*T + t];\n[](#cb62-5)        float logit_max = -1e9;\n[](#cb62-6)        for (int v = 0; v < vocab_size; v++) {\n[](#cb62-7)            if (logits[b*T*vocab_size + t*vocab_size + v] > logit_max) {\n[](#cb62-8)                logit_max = logits[b*T*vocab_size + t*vocab_size + v];\n[](#cb62-9)            }\n[](#cb62-10)        }\n[](#cb62-11)        // compute softmax denominator\n[](#cb62-12)        float sum = 0.0f;\n[](#cb62-13)        for (int v = 0; v < vocab_size; v++) {\n[](#cb62-14)            sum += expf(logits[b*T*vocab_size + t*vocab_size + v] - logit_max);\n[](#cb62-15)        }\n[](#cb62-16)        float logprob = logits[b*T*vocab_size + t*vocab_size + target] - logit_max - logf(sum);\n[](#cb62-17)        loss += -logprob;\n[](#cb62-18)    }\n[](#cb62-19)}\n[](#cb62-20)loss /= (B * T);\n```", "```c\n\"The cat sat\" → [464, 3290, 616]\n```", "```c\n\"The cat sat on the mat.\"\n```", "```c\n[464, 3290, 616, 319, 262, 1142, 13]\n```", "```c\n[](#cb66-1)for (int b = 0; b < B; b++) {\n[](#cb66-2)    for (int t = 0; t < T; t++) {\n[](#cb66-3)        float* out_bt = out + b * T * C + t * C;\n[](#cb66-4)        int ix = inp[b * T + t];\n[](#cb66-5)        float* wte_ix = wte + ix * C;\n[](#cb66-6)        float* wpe_t = wpe + t * C;\n[](#cb66-7)        for (int i = 0; i < C; i++) {\n[](#cb66-8)            out_bt[i] = wte_ix[i] + wpe_t[i];\n[](#cb66-9)        }\n[](#cb66-10)    }\n[](#cb66-11)}\n```", "```c\n[](#cb67-1)matmul_forward(acts.q, acts.ln1, params.wq, params.bq, B, T, C, C);\n[](#cb67-2)matmul_forward(acts.k, acts.ln1, params.wk, params.bk, B, T, C, C);\n[](#cb67-3)matmul_forward(acts.v, acts.ln1, params.wv, params.bv, B, T, C, C);\n```", "```c\nscore[t][u] = (Q[t] ⋅ K[u]) / sqrt(dk)\n```", "```c\nattention_weights[t][u] = exp(score[t][u]) / Σ exp(score[t][v])\n```", "```c\noutput[t] = Σ attention_weights[t][u] * V[u]\n```", "```c\n[](#cb71-1)residual_forward(acts.residual2, acts.ln1, acts.att, B*T, C);\n```", "```c\n[](#cb72-1)matmul_forward(acts.mlp_in, acts.ln2, params.wfc, params.bfc, B, T, C, 4*C);\n```", "```c\nGELU(x) = 0.5 * x * (1 + tanh(√(2/π) * (x + 0.044715 * x^3)))\n```", "```c\n[](#cb74-1)matmul_forward(acts.mlp_out, acts.mlp_in_gelu, params.wproj, params.bproj, B, T, 4*C, C);\n```", "```c\n[](#cb75-1)residual_forward(acts.residual3, acts.residual2, acts.mlp_out, B*T, C);\n```", "```c\n    μ = (1/C) * Σ x[i]\n    ```", "```c\n    σ² = (1/C) * Σ (x[i] - μ)²\n    ```", "```c\n    x_norm[i] = (x[i] - μ) / sqrt(σ² + ε)\n    ```", "```c\n    y[i] = g[i] * x_norm[i] + b[i]\n    ```", "```c\n[](#cb80-1)void layernorm_forward(float* out, float* inp, float* weight, float* bias, int B, int T, int C) {\n[](#cb80-2)    for (int b = 0; b < B; b++) {\n[](#cb80-3)        for (int t = 0; t < T; t++) {\n[](#cb80-4)            float* x = inp + b*T*C + t*C;\n[](#cb80-5)            float* o = out + b*T*C + t*C;\n[](#cb80-6)\n[](#cb80-7)            // mean\n[](#cb80-8)            float mean = 0.0f;\n[](#cb80-9)            for (int i = 0; i < C; i++) mean += x[i];\n[](#cb80-10)            mean /= C;\n[](#cb80-11)\n[](#cb80-12)            // variance\n[](#cb80-13)            float var = 0.0f;\n[](#cb80-14)            for (int i = 0; i < C; i++) {\n[](#cb80-15)                float diff = x[i] - mean;\n[](#cb80-16)                var += diff * diff;\n[](#cb80-17)            }\n[](#cb80-18)            var /= C;\n[](#cb80-19)\n[](#cb80-20)            // normalize, scale, shift\n[](#cb80-21)            for (int i = 0; i < C; i++) {\n[](#cb80-22)                float norm = (x[i] - mean) / sqrtf(var + 1e-5f);\n[](#cb80-23)                o[i] = norm * weight[i] + bias[i];\n[](#cb80-24)            }\n[](#cb80-25)        }\n[](#cb80-26)    }\n[](#cb80-27)}\n```", "```c\n    [ (2-1)/1.58, (-1-1)/1.58, (3-1)/1.58, (0-1)/1.58 ]\n    = [0.63, -1.26, 1.26, -0.63]\n    ```", "```c\n    [0.63, -1.26, 1.26, -0.63]\n    ```", "```c\noutput = input + transformation(input)\n```", "```c\n[](#cb84-1)void residual_forward(float* out, float* inp1, float* inp2, int N) {\n[](#cb84-2)    for (int i = 0; i < N; i++) {\n[](#cb84-3)        out[i] = inp1[i] + inp2[i];\n[](#cb84-4)    }\n[](#cb84-5)}\n```", "```c\n[0.5 + 0.2, -0.3 + 0.1, 0.7 - 0.4] = [0.7, -0.2, 0.3]\n```", "```c\n    x = x + Attention(x)\n    ```", "```c\n    x = x + MLP(x)\n    ```", "```c\nx → LayerNorm → Attention → Residual Add → LayerNorm → MLP → Residual Add\n```", "```c\nlogits = [5.2, 1.1, -2.7]\n```", "```c\np[i] = exp(logits[i]) / Σ exp(logits[j])\n```", "```c\nloss = -log(probability_of_correct_word)\n```", "```c\n[](#cb92-1)float loss = 0.0f;\n[](#cb92-2)for (int i = 0; i < B*T; i++) {\n[](#cb92-3)    int target = targets[i];\n[](#cb92-4)    float logit_max = -1e9;\n[](#cb92-5)    for (int j = 0; j < Vp; j++) {\n[](#cb92-6)        if (logits[i*Vp + j] > logit_max) logit_max = logits[i*Vp + j];\n[](#cb92-7)    }\n[](#cb92-8)    float sum = 0.0f;\n[](#cb92-9)    for (int j = 0; j < Vp; j++) {\n[](#cb92-10)        sum += expf(logits[i*Vp + j] - logit_max);\n[](#cb92-11)    }\n[](#cb92-12)    float log_sum = logf(sum);\n[](#cb92-13)    float correct_logit = logits[i*Vp + target];\n[](#cb92-14)    loss += (log_sum + logit_max - correct_logit);\n[](#cb92-15)}\n[](#cb92-16)loss /= (B*T);\n```", "```c\n[](#cb93-1)void gpt2_forward(GPT2 *model, int *tokens, int *labels, int B, int T) {\n[](#cb93-2)    // Step 1: Embedding lookup\n[](#cb93-3)    embedding_forward(model->token_embedding, tokens, B, T);\n[](#cb93-4)    embedding_forward(model->position_embedding, positions, B, T);\n[](#cb93-5)\n[](#cb93-6)    // Step 2: Transformer blocks\n[](#cb93-7)    for (int l = 0; l < model->config->n_layer; l++) {\n[](#cb93-8)        attention_forward(&model->blocks[l].attn, ...);\n[](#cb93-9)        mlp_forward(&model->blocks[l].mlp, ...);\n[](#cb93-10)        layernorm_forward(&model->blocks[l].ln, ...);\n[](#cb93-11)        residual_forward(...);\n[](#cb93-12)    }\n[](#cb93-13)\n[](#cb93-14)    // Step 3: Final normalization + logits\n[](#cb93-15)    layernorm_forward(model->final_ln, ...);\n[](#cb93-16)    matmul_forward(model->lm_head, ...); // project to vocab\n[](#cb93-17)\n[](#cb93-18)    // Step 4: Loss (optional)\n[](#cb93-19)    if (labels != NULL) {\n[](#cb93-20)        crossentropy_forward(...);\n[](#cb93-21)    }\n[](#cb93-22)}\n```", "```c\n[](#cb94-1)#pragma omp parallel for collapse(2)\n[](#cb94-2)for (int b = 0; b < B; b++) {\n[](#cb94-3)    for (int t = 0; t < T; t++) {\n[](#cb94-4)        // compute outputs for (b, t) independently\n[](#cb94-5)    }\n[](#cb94-6)}\n```", "```c\n    [](#cb95-1)#ifdef OMP\n    [](#cb95-2)#include <omp.h>\n    [](#cb95-3)#endif\n    ```", "```c\n    -D OMP -fopenmp\n    ```", "```c\n    export OMP_NUM_THREADS=8\n    ```", "```c\n[](#cb98-1)#pragma omp parallel for collapse(2)\n[](#cb98-2)for (int b = 0; b < B; b++) {\n[](#cb98-3)    for (int t = 0; t < T; t++) {\n[](#cb98-4)        // 1) find max over V\n[](#cb98-5)        // 2) exp/log-sum\n[](#cb98-6)        // 3) normalize first V entries; zero out padded [V..Vp)\n[](#cb98-7)    }\n[](#cb98-8)}\n```", "```c\nstep 1: train loss 5.191576 (took 1927.230000 ms)\n```", "```c\n[](#cb100-1)void gpt2_backward(GPT2 *model, int *tokens, int *labels, int B, int T) {\n[](#cb100-2)    // Step 1: loss gradient\n[](#cb100-3)    crossentropy_backward(...);\n[](#cb100-4)\n[](#cb100-5)    // Step 2: final projection (lm_head)\n[](#cb100-6)    matmul_backward(model->lm_head, ...);\n[](#cb100-7)\n[](#cb100-8)    // Step 3: transformer blocks in reverse\n[](#cb100-9)    for (int l = model->config->n_layer - 1; l >= 0; l--) {\n[](#cb100-10)        residual_backward(...);\n[](#cb100-11)        layernorm_backward(&model->blocks[l].ln, ...);\n[](#cb100-12)        mlp_backward(&model->blocks[l].mlp, ...);\n[](#cb100-13)        attention_backward(&model->blocks[l].attn, ...);\n[](#cb100-14)    }\n[](#cb100-15)\n[](#cb100-16)    // Step 4: embeddings\n[](#cb100-17)    embedding_backward(model->token_embedding, tokens, ...);\n[](#cb100-18)    embedding_backward(model->position_embedding, positions, ...);\n[](#cb100-19)}\n```", "```c\n[](#cb101-1)for (int step = 0; step < max_steps; step++) {\n[](#cb101-2)    // 1\\. Load batch of tokens and labels\n[](#cb101-3)    dataloader_next_batch(&train_loader, tokens, labels, B, T);\n[](#cb101-4)\n[](#cb101-5)    // 2\\. Forward pass\n[](#cb101-6)    gpt2_forward(&model, tokens, labels, B, T);\n[](#cb101-7)\n[](#cb101-8)    // 3\\. Zero gradients\n[](#cb101-9)    gpt2_zero_grad(&model);\n[](#cb101-10)\n[](#cb101-11)    // 4\\. Backward pass\n[](#cb101-12)    gpt2_backward(&model, tokens, labels, B, T);\n[](#cb101-13)\n[](#cb101-14)    // 5\\. Optimizer step (AdamW)\n[](#cb101-15)    adamw_update(&opt, &model, learning_rate);\n[](#cb101-16)\n[](#cb101-17)    // 6\\. Logging and validation\n[](#cb101-18)    if (step % log_interval == 0) { print_loss(step, model.loss); }\n[](#cb101-19)    if (step % val_interval == 0) { run_validation(...); }\n[](#cb101-20)}\n```", "```c\n[](#cb102-1)gpt2_zero_grad(&model);\n```", "```c\nnew_weight = old_weight - learning_rate * gradient (with AdamW tweaks)\n```", "```c\nnew_param = old_param - learning_rate * gradient\n```", "```c\nm_t = β1 * m_(t-1) + (1 - β1) * g_t\nv_t = β2 * v_(t-1) + (1 - β2) * g_t²\nm̂_t = m_t / (1 - β1^t)\nv̂_t = v_t / (1 - β2^t)\n\nnew_param = old_param - lr * ( m̂_t / (sqrt(v̂_t) + ε) + λ * old_param )\n```", "```c\n[](#cb106-1)void gpt2_update(GPT2 *model, float learning_rate, float beta1, float beta2,\n[](#cb106-2)                 float eps, float weight_decay, int t) {\n[](#cb106-3)    if (model->m_memory == NULL) {\n[](#cb106-4)        model->m_memory = (float*)calloc(model->num_parameters, sizeof(float));\n[](#cb106-5)        model->v_memory = (float*)calloc(model->num_parameters, sizeof(float));\n[](#cb106-6)    }\n[](#cb106-7)\n[](#cb106-8)    for (size_t i = 0; i < model->num_parameters; i++) {\n[](#cb106-9)        float param = model->params_memory[i];\n[](#cb106-10)        float grad = model->grads_memory[i];\n[](#cb106-11)\n[](#cb106-12)        float m = beta1 * model->m_memory[i] + (1.0f - beta1) * grad;\n[](#cb106-13)        float v = beta2 * model->v_memory[i] + (1.0f - beta2) * grad * grad;\n[](#cb106-14)\n[](#cb106-15)        float m_hat = m / (1.0f - powf(beta1, t));\n[](#cb106-16)        float v_hat = v / (1.0f - powf(beta2, t));\n[](#cb106-17)\n[](#cb106-18)        model->m_memory[i] = m;\n[](#cb106-19)        model->v_memory[i] = v;\n[](#cb106-20)        model->params_memory[i] -= learning_rate *\n[](#cb106-21)            (m_hat / (sqrtf(v_hat) + eps) + weight_decay * param);\n[](#cb106-22)    }\n[](#cb106-23)}\n```", "```c\n    m = 0.9 * 0 + 0.1 * 0.2 = 0.02\n    ```", "```c\n    v = 0.999 * 0 + 0.001 * 0.04 = 0.00004\n    ```", "```c\n    m̂ = 0.02 / (1 - 0.9) = 0.2\n    v̂ = 0.00004 / (1 - 0.999) = 0.04\n    ```", "```c\n    update = 0.001 * (0.2 / sqrt(0.04) + 0.01 * 0.5)\n           = 0.001 * (1.0 + 0.005)\n           = 0.001005\n    ```", "```c\n[](#cb111-1)int accumulation_steps = 8;\n[](#cb111-2)for (int step = 0; step < total_steps; step++) {\n[](#cb111-3)    zero_grad(&model);\n[](#cb111-4)    for (int i = 0; i < accumulation_steps; i++) {\n[](#cb111-5)        dataloader_next_batch(&train_loader, tokens, labels, B, T);\n[](#cb111-6)        forward(&model, tokens, labels, B, T);\n[](#cb111-7)        backward(&model, tokens, labels, B, T);\n[](#cb111-8)        // do NOT call optimizer update yet\n[](#cb111-9)    }\n[](#cb111-10)    adamw_update(&model, lr, beta1, beta2, eps, weight_decay, step+1);\n[](#cb111-11)}\n```", "```c\n[](#cb112-1)// do a training step\n[](#cb112-2)clock_gettime(CLOCK_MONOTONIC, &start);\n[](#cb112-3)dataloader_next_batch(&train_loader);\n[](#cb112-4)gpt2_forward(&model, train_loader.inputs, train_loader.targets, B, T);\n[](#cb112-5)gpt2_zero_grad(&model);\n[](#cb112-6)gpt2_backward(&model);\n[](#cb112-7)gpt2_update(&model, 1e-4f, 0.9f, 0.999f, 1e-8f, 0.0f, step+1);\n[](#cb112-8)clock_gettime(CLOCK_MONOTONIC, &end);\n[](#cb112-9)\n[](#cb112-10)double time_elapsed_s = (end.tv_sec - start.tv_sec) +\n[](#cb112-11)                        (end.tv_nsec - start.tv_nsec) / 1e9;\n[](#cb112-12)printf(\"step %d: train loss %f (took %f ms)\\n\",\n[](#cb112-13)       step, model.mean_loss, time_elapsed_s * 1000);\n```", "```c\nstep 0: train loss 4.677779 (took 1987.546 ms)\nstep 1: train loss 5.191576 (took 1927.230 ms)\nstep 2: train loss 4.438685 (took 1902.987 ms)\n```", "```c\n    [](#cb114-1)./train_gpt2 > log.txt\n    ```", "```c\n[](#cb115-1)if (step % 10 == 0) {\n[](#cb115-2)    float val_loss = 0.0f;\n[](#cb115-3)    dataloader_reset(&val_loader);\n[](#cb115-4)    for (int i = 0; i < val_num_batches; i++) {\n[](#cb115-5)        dataloader_next_batch(&val_loader);\n[](#cb115-6)        gpt2_forward(&model, val_loader.inputs, val_loader.targets, B, T);\n[](#cb115-7)        val_loss += model.mean_loss;\n[](#cb115-8)    }\n[](#cb115-9)    val_loss /= val_num_batches;\n[](#cb115-10)    printf(\"val loss %f\\n\", val_loss);\n[](#cb115-11)}\n```", "```c\n[](#cb116-1)if (step % 10 == 0) {\n```", "```c\n[](#cb117-1)float val_loss = 0.0f;\n```", "```c\n[](#cb118-1)dataloader_reset(&val_loader);\n```", "```c\n[](#cb119-1)for (int i = 0; i < val_num_batches; i++) {\n[](#cb119-2)    dataloader_next_batch(&val_loader);\n[](#cb119-3)    gpt2_forward(&model, val_loader.inputs, val_loader.targets, B, T);\n[](#cb119-4)    val_loss += model.mean_loss;\n[](#cb119-5)}\n```", "```c\n[](#cb120-1)val_loss /= val_num_batches;\n```", "```c\n[](#cb121-1)printf(\"val loss %f\\n\", val_loss);\n```", "```c\nstep 0: train loss 4.677779 (took 1987.546 ms)\nval loss 4.901234\nstep 1: train loss 5.191576 (took 1927.230 ms)\nstep 2: train loss 4.438685 (took 1902.987 ms)\n...\nstep 10: train loss 3.912342 (took 1890.321 ms)\nval loss 4.100321\n```", "```c\n[](#cb123-1)FILE *f = fopen(\"checkpoint.bin\", \"wb\");\n[](#cb123-2)fwrite(model.params_memory, sizeof(float), model.num_parameters, f);\n[](#cb123-3)fwrite(model.m_memory, sizeof(float), model.num_parameters, f);\n[](#cb123-4)fwrite(model.v_memory, sizeof(float), model.num_parameters, f);\n[](#cb123-5)fwrite(&step, sizeof(int), 1, f);\n[](#cb123-6)fclose(f);\n```", "```c\n[](#cb124-1)FILE *f = fopen(\"checkpoint.bin\", \"rb\");\n[](#cb124-2)fread(model.params_memory, sizeof(float), model.num_parameters, f);\n[](#cb124-3)fread(model.m_memory, sizeof(float), model.num_parameters, f);\n[](#cb124-4)fread(model.v_memory, sizeof(float), model.num_parameters, f);\n[](#cb124-5)fread(&step, sizeof(int), 1, f);\n[](#cb124-6)fclose(f);\n```", "```c\n[](#cb125-1)manual_seed(1337); \n```", "```c\nRun A: step 1000 → val loss 3.42  \nRun B: step 1000 → val loss 3.43 \n```", "```c\nRun A: step 1000 → val loss 3.42  \nRun B: step 1000 → val loss 3.89 \n```", "```c\n    [](#cb128-1)int B = 4; // number of sequences per batch\n    ```", "```c\n    [](#cb129-1)int T = 64; // tokens per sequence\n    ```", "```c\n    [](#cb130-1)int val_num_batches = 5;\n    ```", "```c\n    [](#cb131-1)for (int step = 0; step <= 40; step++) {\n    ```", "```c\n    [](#cb132-1)gpt2_update(&model, 1e-4f, 0.9f, 0.999f, 1e-8f, 0.0f, step+1);\n    ```", "```c\n[](#cb133-1)int main(int argc, char argv) {\n[](#cb133-2)    int B = 4;\n[](#cb133-3)    int T = 64;\n[](#cb133-4)    int max_steps = 40;\n[](#cb133-5)    if (argc > 1) B = atoi(argv[1]);\n[](#cb133-6)    if (argc > 2) T = atoi(argv[2]);\n[](#cb133-7)    if (argc > 3) max_steps = atoi(argv[3]);\n[](#cb133-8)    ...\n[](#cb133-9)}\n```", "```c\n[](#cb134-1)./train_gpt2 8 128 100\n```", "```c\ntrain dataset num_batches: 1192\nval dataset num_batches: 128\n[GPT-2]\nmax_seq_len: 1024\nvocab_size: 50257\npadded_vocab_size: 50304\nnum_layers: 12\nnum_heads: 12\nchannels: 768\nnum_parameters: 124475904\nnum_activations: 73347840\nval loss 5.325529\nstep 0: train loss 4.677779 (took 1987.546000 ms)\nstep 1: train loss 5.191576 (took 1927.230000 ms)\nstep 2: train loss 4.438685 (took 1902.987000 ms)\n...\n```", "```c\ngenerating:\n\n The King had not\n that the Duke of Northumberland and the Duke of\n... \n```", "```c\nMismatch at position [0, 12, 42]: C=0.1234, Torch=0.1235\n```", "```c\nMismatch at position [0, 12, 42]: C=0.3456, Torch=0.1235\n```", "```c\n    |0.123456 - 0.123455| = 0.000001 < 1e-5\n    ```", "```c\n    |1000.0 - 1000.1| / 1000.0 = 0.0001\n    ```", "```c\n    step 0: train loss 4.87\n    step 10: train loss 4.12\n    step 20: train loss 3.75\n    ...\n    ```", "```c\nstep 0: train loss 4.95\nstep 1: train loss 4.72\nstep 2: train loss nan\n```", "```c\n[](#cb143-1)cublasHandle_t handle;\n[](#cb143-2)cublasCreate(&handle);\n[](#cb143-3)\n[](#cb143-4)float alpha = 1.0f, beta = 0.0f;\n[](#cb143-5)cublasSgemm(handle,\n[](#cb143-6)    CUBLAS_OP_N, CUBLAS_OP_N,\n[](#cb143-7)    m, n, k,\n[](#cb143-8)    &alpha,\n[](#cb143-9)    A, m,\n[](#cb143-10)    B, k,\n[](#cb143-11)    &beta,\n[](#cb143-12)    C, m);\n```", "```c\n[](#cb144-1)float scale = 1024.0f; // example scale factor\n[](#cb144-2)float scaled_loss = model.mean_loss * scale;\n[](#cb144-3)gpt2_backward_scaled(&model, scaled_loss); // backprop with scaled loss\n[](#cb144-4)unscale_gradients(&model, scale); // divide gradients by scale\n[](#cb144-5)gpt2_update(&model, lr, beta1, beta2, eps, weight_decay, step);\n```", "```c\n[](#cb145-1)my_kernel<<<num_blocks, threads_per_block>>>(...);\n```", "```c\n[](#cb146-1)int threads = 256;\n[](#cb146-2)int blocks = (N + threads - 1) / threads;\n[](#cb146-3)my_kernel<<<blocks, threads>>>(...);\n```", "```c\n[](#cb147-1)cudaError_t err = cudaMalloc(&ptr, size);\n[](#cb147-2)if (err != cudaSuccess) {\n[](#cb147-3)    printf(\"Error: %s\\n\", cudaGetErrorString(err));\n[](#cb147-4)}\n```", "```c\n[](#cb148-1)my_kernel<<<blocks, threads>>>(...);\n[](#cb148-2)cudaError_t err = cudaGetLastError();\n[](#cb148-3)if (err != cudaSuccess) {\n[](#cb148-4)    printf(\"Kernel launch failed: %s\\n\", cudaGetErrorString(err));\n[](#cb148-5)}\n[](#cb148-6)cudaDeviceSynchronize(); // forces the GPU to finish and report errors\n```", "```c\nInvalid global read of size 4\n    at softmax.cu:42\n    by thread (1025,0,0) in block (1,0,0)\n```", "```c\n[](#cb150-1)__global__ void vector_add(const float* A, const float* B, float* C, int N) {\n[](#cb150-2)    int i = blockIdx.x * blockDim.x + threadIdx.x;\n[](#cb150-3)    if (i < N) {\n[](#cb150-4)        C[i] = A[i] + B[i];\n[](#cb150-5)    }\n[](#cb150-6)}\n```", "```c\n[](#cb151-1)# Run with 4 GPUs\n[](#cb151-2)mpirun -np 4 \\\n[](#cb151-3)  -x CUDA_VISIBLE_DEVICES=0,1,2,3 \\\n[](#cb151-4)  ./train_gpt2_mpi\n```", "```c\n[](#cb152-1)int rank;\n[](#cb152-2)MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n[](#cb152-3)int device;\n[](#cb152-4)cudaGetDevice(&device);\n[](#cb152-5)printf(\"Process %d is using GPU %d\\n\", rank, device);\n```", "```c\ng_all = (g0 + g1 + g2 + g3) / 4\n```", "```c\n[](#cb154-1)ncclAllReduce(model.grads_memory,\n[](#cb154-2)              model.grads_memory,\n[](#cb154-3)              model.num_parameters,\n[](#cb154-4)              ncclFloat32,  // or half precision\n[](#cb154-5)              ncclSum,\n[](#cb154-6)              comm, stream);\n```", "```c\n[](#cb155-1)mpirun -np 4 ./train_gpt2_cu\n```", "```c\n[](#cb156-1)cudaSetDevice(rank);\n```", "```c\n[](#cb157-1)ncclCommInitRank(&comm, world_size, nccl_id, rank);\n```", "```c\n[](#cb158-1)mpirun -np 2 ./train_gpt2_cu -batch_size 8 -seq_len 128\n```", "```c\n[](#cb159-1)mpirun -np 8 -hostfile myhosts ./train_gpt2_cu\n```", "```c\n    [](#cb160-1)export NCCL_SOCKET_IFNAME=eth0\n    ```", "```c\nnode1 slots=4\nnode2 slots=4\n```", "```c\n[](#cb162-1)salloc -N 2 -G 8 --time=01:00:00\n```", "```c\n    [](#cb163-1)srun -n 8 ./train_gpt2_cu\n    ```", "```c\n    [](#cb164-1)export NCCL_DEBUG=INFO\n    [](#cb164-2)export NCCL_DEBUG_SUBSYS=ALL\n    ```", "```c\n    [](#cb165-1)cuda-memcheck ./train_gpt2_cu\n    ```", "```c\n    [](#cb166-1)export NCCL_SOCKET_IFNAME=ib0\n    ```", "```c\n__global__ void add_kernel(const float* a, const float* b, float* out, int N) {\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        out[i] = a[i] + b[i];\n    }\n}\n```", "```c\n    [](#cb168-1)python dev/data/tokenizer.py --input my_corpus.txt --output my_corpus.bin\n    ```", "```c\n    [](#cb169-1)dataloader_init(&train_loader, \"dev/data/my_corpus_train.bin\", B, T, 0, 1, 1);\n    [](#cb169-2)dataloader_init(&val_loader, \"dev/data/my_corpus_val.bin\", B, T, 0, 1, 0);\n    ```", "```c\n[](#cb170-1)// SGD with momentum update\n[](#cb170-2)void gpt2_update_sgd(GPT2 *model, float learning_rate, float momentum) {\n[](#cb170-3)    if (model->m_memory == NULL) {\n[](#cb170-4)        model->m_memory = (float*)calloc(model->num_parameters, sizeof(float));\n[](#cb170-5)    }\n[](#cb170-6)\n[](#cb170-7)    for (size_t i = 0; i < model->num_parameters; i++) {\n[](#cb170-8)        float grad = model->grads_memory[i];\n[](#cb170-9)        float m = momentum * model->m_memory[i] + grad;\n[](#cb170-10)        model->m_memory[i] = m;\n[](#cb170-11)        model->params_memory[i] -= learning_rate * m;\n[](#cb170-12)    }\n[](#cb170-13)}\n```", "```c\n[](#cb171-1)gpt2_update(&model, lr, beta1, beta2, eps, weight_decay, step+1);\n```", "```c\n    [](#cb172-1)float step_decay(int step, float base_lr, int decay_every, float decay_factor) {\n    [](#cb172-2)    int k = step / decay_every;\n    [](#cb172-3)    return base_lr * powf(decay_factor, k);\n    [](#cb172-4)}\n    ```", "```c\n    [](#cb173-1)float cosine_decay(int step, int max_steps, float base_lr) {\n    [](#cb173-2)    float progress = (float)step / max_steps;\n    [](#cb173-3)    return base_lr * 0.5f * (1.0f + cosf(M_PI * progress));\n    [](#cb173-4)}\n    ```", "```c\n[](#cb174-1)float lr_scheduler(int step, int warmup_steps, int max_steps, float base_lr) {\n[](#cb174-2)    if (step < warmup_steps) {\n[](#cb174-3)        return base_lr * (float)(step + 1) / warmup_steps;\n[](#cb174-4)    } else {\n[](#cb174-5)        float progress = (float)(step - warmup_steps) / (max_steps - warmup_steps);\n[](#cb174-6)        return base_lr * 0.5f * (1.0f + cosf(M_PI * progress));\n[](#cb174-7)    }\n[](#cb174-8)}\n```", "```c\n[](#cb175-1)import torch\n[](#cb175-2)from transformers import GPT2Model, GPT2Tokenizer\n[](#cb175-3)\n[](#cb175-4)tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n[](#cb175-5)model = GPT2Model.from_pretrained(\"gpt2\")\n[](#cb175-6)tokens = torch.tensor([[50256, 200]])  # EOT and an example token\n[](#cb175-7)out = model.wte(tokens)  # word token embeddings\n[](#cb175-8)print(out.detach().numpy())\n```", "```c\n[](#cb176-1)fn layernorm_forward(out: &mut [f32], inp: &[f32], weight: &[f32], bias: &[f32], c: usize) {\n[](#cb176-2)    let mean: f32 = inp.iter().sum::<f32>() / c as f32;\n[](#cb176-3)    let var: f32 = inp.iter().map(|x| (x - mean).powi(2)).sum::<f32>() / c as f32;\n[](#cb176-4)    let rstd = 1.0 / (var + 1e-5).sqrt();\n[](#cb176-5)\n[](#cb176-6)    for i in 0..c {\n[](#cb176-7)        let norm = (inp[i] - mean) * rstd;\n[](#cb176-8)        out[i] = norm * weight[i] + bias[i];\n[](#cb176-9)    }\n[](#cb176-10)}\n```", "```c\nkernel void attention_forward(\n    device float* out [[buffer(0)]],\n    device float* qkv [[buffer(1)]],\n    uint id [[thread_position_in_grid]]\n) {\n    // compute a dot product between query and key vectors\n    // accumulate into out, using threadgroup memory for efficiency\n}\n```", "```c\n    [](#cb178-1)GPT2Config config = {\n    [](#cb178-2)    .max_seq_len = 1024,\n    [](#cb178-3)    .vocab_size = 50257,\n    [](#cb178-4)    .num_layers = 12,\n    [](#cb178-5)    .num_heads = 12,\n    [](#cb178-6)    .channels = 768\n    [](#cb178-7)};\n    ```", "```c\nstep 0: train loss 6.9321 (took 421.5 ms)\nstep 100: train loss 4.2137 (took 95.2 ms)\nstep 200: train loss 3.8914 (took 94.8 ms)\nval loss 3.7725\n```", "```c\nstep 0: train loss 7.1032 (took 321.8 ms)\nstep 50: train loss 5.4231 (took 310.4 ms)\nstep 100: train loss 4.8217 (took 311.0 ms)\nval loss 4.7322\n```", "```c\nstep 0: train loss 8.0123 (took 512.4 ms)\nstep 50: train loss 5.7892 (took 490.7 ms)\nstep 100: train loss 5.1428 (took 495.1 ms)\nval loss 5.0039\n```", "```c\n[rank 0] step 0: train loss 8.5029 (took 312.6 ms)\n[rank 0] step 50: train loss 6.3121 (took 308.2 ms)\n[rank 0] step 100: train loss 5.7210 (took 309.4 ms)\n[rank 0] val loss 5.5347\n```", "```c\n[](#cb183-1)int B = 4;    // batch size\n[](#cb183-2)int T = 64;   // sequence length\n[](#cb183-3)int steps = 1000;  // training iterations\n```", "```c\ndev/data/tinyshakespeare/tiny_shakespeare_train.bin\ndev/data/tinyshakespeare/tiny_shakespeare_val.bin\n```", "```c\nstep 0: train loss 6.9312 (took 1220.4 ms)\nstep 50: train loss 4.3217 (took 1175.1 ms)\nstep 100: train loss 3.7120 (took 1169.4 ms)\nval loss 3.5894\n```", "```c\ngenerating:\n\nROMEO: But hark, what light through yonder window breaks?\nJULIET: Ay me! the time is near, and I must away.\nROMEO: Fear not, sweet love, for night shall bring us peace. \n```", "```c\n[](#cb187-1)#!/bin/bash\n[](#cb187-2)for lr in 1e-3 5e-4 1e-4\n[](#cb187-3)do\n[](#cb187-4)    echo \"Running with learning rate $lr\"\n[](#cb187-5)    ./train_gpt2 -lr $lr -epochs 5 > logs/lr_$lr.txt\n[](#cb187-6)done\n```", "```c\n[](#cb188-1)for lr in 3e-4 2e-4 1e-4\n[](#cb188-2)do\n[](#cb188-3)    for B in 4 8\n[](#cb188-4)    do\n[](#cb188-5)        echo \"Running with lr=$lr and batch_size=$B\"\n[](#cb188-6)        ./train_gpt2 -lr $lr -B $B -steps 500 > logs/lr_${lr}_B${B}.txt\n[](#cb188-7)    done\n[](#cb188-8)done\n```", "```c\n[](#cb189-1)./train_gpt2 > logs/run1.txt\n```", "```c\n[](#cb190-1)import re, matplotlib.pyplot as plt\n[](#cb190-2)\n[](#cb190-3)steps, train_loss = [], []\n[](#cb190-4)for line in open(\"logs/run1.txt\"):\n[](#cb190-5)    match = re.match(r\"step (\\d+): train loss ([0-9.]+)\", line)\n[](#cb190-6)    if match:\n[](#cb190-7)        steps.append(int(match.group(1)))\n[](#cb190-8)        train_loss.append(float(match.group(2)))\n[](#cb190-9)\n[](#cb190-10)plt.plot(steps, train_loss, label=\"train loss\")\n[](#cb190-11)plt.legend()\n[](#cb190-12)plt.show()\n```", "```c\nstep 0: train loss 6.92, val loss 6.85\nstep 100: train loss 4.31, val loss 4.52\nstep 200: train loss 3.78, val loss 4.01\nstep 500: train loss 2.91, val loss 3.95\n```", "```c\n__global__ void vec_add(float* a, float* b, float* c, int N) {\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        c[i] = a[i] + b[i];\n    }\n}\n```"]