- en: 4  From Simple to Advanced RAG
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4  从简单到高级的RAG
- en: 原文：[https://mallahyari.github.io/rag-ebook/04_advanced_rag.html](https://mallahyari.github.io/rag-ebook/04_advanced_rag.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://mallahyari.github.io/rag-ebook/04_advanced_rag.html](https://mallahyari.github.io/rag-ebook/04_advanced_rag.html)
- en: 4.1 Introduction
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 简介
- en: As we traverse the path from development to production, the world of Retrieval
    Augmented Generation (RAG) applications unveils its potential to transform the
    way we interact with vast collections of information. In the preceding chapters,
    we’ve laid the groundwork for building RAG systems that can answer questions,
    provide insights, and deliver valuable content. However, the journey is far from
    over.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们从开发到生产的转变，检索增强生成（RAG）应用的世界揭示了其改变我们与大量信息互动方式的潜力。在前面的章节中，我们已经为构建能够回答问题、提供见解和提供有价值内容的RAG系统奠定了基础。然而，这段旅程还远未结束。
- en: '*Please note that [LlamaIndex](https://gpt-index.readthedocs.io/en/stable/index.html)
    framework has been used for several of the code implementations in this chapter.
    This framework also contains a lot of advanced tutorials about RAG that inspired
    the content of this chapter.*'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**请注意，[LlamaIndex](https://gpt-index.readthedocs.io/en/stable/index.html)框架已被用于本章中的一些代码实现。此框架还包含大量关于RAG的高级教程，这些教程启发了本章的内容。**'
- en: The transition from a well-crafted RAG system in the development environment
    to a real-world production application is a monumental step, one that demands
    careful consideration of a myriad of factors and deals with limitations of existing
    approaches. These considerations ensure your RAG application operates seamlessly
    in a real-world production environment. There are so many challenges and consideration
    for building *production-ready* RAG. Nevertheless, we will discuss some of the
    primary ones.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 从开发环境中的精心设计的RAG系统过渡到现实世界的生产应用是一个巨大的步骤，这需要仔细考虑众多因素，并处理现有方法的局限性。这些考虑确保您的RAG应用在现实世界的生产环境中无缝运行。构建*生产就绪*的RAG有许多挑战和考虑因素。尽管如此，我们仍将讨论其中的一些主要因素。
- en: 'RAG pipeline consists of two components: i) Retrieval and, ii) response generation
    (synthesis). Each component has its own challenges.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: RAG管道由两个组件组成：i) 检索和，ii) 响应生成（综合）。每个组件都有其自身的挑战。
- en: Retrieval Challenges
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检索挑战
- en: '**Low precision:** When retrieving the top-k chunks, there’s a risk of including
    irrelevant content, potentially leading to issues like hallucination and generating
    inaccurate responses.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**低精度：** 在检索top-k片段时，存在包含不相关内容的风险，可能导致幻觉和生成不准确响应等问题。'
- en: '**Low recall:** In certain cases, even when all the relevant chunks are retrieved,
    the text chunks might lack the necessary global context beyond the retrieved chunks
    to generate a coherent response.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**低召回率：** 在某些情况下，即使检索到了所有相关的片段，文本片段可能缺乏必要的全局上下文，这些上下文超出了检索到的片段，以生成连贯的响应。'
- en: '**Obsolete information:** Ensuring the data remains up-to-date is critical
    to avoid reliance on obsolete information. Regular updates are essential to maintain
    data relevance and accuracy.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过时信息：** 确保数据保持最新至关重要，以避免依赖过时信息。定期更新对于保持数据的相关性和准确性至关重要。'
- en: Generation Challenges
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成挑战
- en: '**Hallucination:** The model generates responses that include information not
    present in the context, potentially leading to inaccuracies or fictional details.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**幻觉：** 模型生成的响应中包含上下文中不存在的信息，可能导致不准确或虚构的细节。'
- en: '**Irrelevance:** The model produces answers that do not directly address the
    user’s question, resulting in responses that lack relevance or coherence.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不相关性：** 模型生成的答案没有直接回答用户的问题，导致缺乏相关性和连贯性的响应。'
- en: '**Bias:** The model generates answers that contai n harmful or offensive content,
    potentially reflecting biases and undermining user trust and safety.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏差：** 模型生成的答案包含有害或冒犯性内容，可能反映了偏见，并损害用户信任和安全。'
- en: What Can be done
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可以做什么
- en: 'In order to effectively tackle the challenges in Retrieval Augmented Generation
    (RAG) systems, several key strategies can be employed:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效地解决检索增强生成（RAG）系统中的挑战，可以采用几种关键策略：
- en: '**Data Augmentation:** Can we enrich our dataset by storing supplementary information
    beyond raw text chunks, such as metadata or structured data, to provide richer
    context for retrieval and generation processes?'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据增强：** 我们能否通过存储除原始文本片段之外的信息（如元数据或结构化数据）来丰富我们的数据集，为检索和生成过程提供更丰富的上下文？'
- en: '**Optimized Embeddings:** Can we refine and enhance our embedding representations
    to capture context more effectively, improve the relevance of retrieved information,
    and enable more coherent response generation?'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**优化嵌入**：我们能否改进和增强我们的嵌入表示，以更有效地捕捉上下文，提高检索信息的相关性，并实现更连贯的响应生成？'
- en: '**Advanced Retrieval Techniques:** Can we go beyond basic top-k embedding lookup
    and implement advanced retrieval methods, such as semantic search, or hybrid search
    (keyword search + semantic search) to enhance the precision and recall of information
    retrieval?'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**高级检索技术**：我们能否超越基本的 top-k 嵌入查找并实现高级检索方法，例如语义搜索或混合搜索（关键词搜索+语义搜索），以增强信息检索的精确度和召回率？'
- en: '**Multi-Purpose Use of LLMs:** Can we leverage Large Language Models (LLMs)
    for tasks beyond text generation, such as question answering, summarization, or
    knowledge graph construction, to augment the capabilities of RAG systems and provide
    more comprehensive responses to user queries?'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**LLMs 的多用途**：我们能否利用大型语言模型（LLMs）执行文本生成之外的任务，如问答、摘要或知识图谱构建，以增强 RAG 系统的能力，并为用户查询提供更全面的响应？'
- en: Let’s dive in a bit more deeply into aforementioned challenges and propose how
    to alleiviate each one.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地探讨上述挑战，并提出如何缓解每一个挑战。
- en: 4.2 Optimul Chunk Size for Efficient Retrieval
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 高效检索的最佳块大小
- en: 'The chunk size in a RAG system is the size of the text passages that are extracted
    from the source text and used to generate the retrieval index. The chunk size
    has a significant impact on the system’s efficiency and performance in several
    ways:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在 RAG 系统中，块大小是从源文本中提取并用于生成检索索引的文本段落的尺寸。块大小以多种方式对系统的效率和性能产生重大影响：
- en: 4.2.1 Balance Between Context and Efficiency
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.1 上下文与效率之间的平衡
- en: The chunk size should strike a balance between providing sufficient context
    for generating coherent responses (i.e. Relevance and Granularity) and ensuring
    efficient retrieval and processing (i.e Performance).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 块大小应该在提供足够上下文以生成连贯响应（即相关性及粒度）和确保高效检索和处理（即性能）之间取得平衡。
- en: A smaller chunk size results in more granular chunks, which can improve the
    relevance of the retrieved chunks. However, it is important to note that the most
    relevant information may not be contained in the top retrieved chunks, especially
    if the *similarity_top_k* setting is low (e.g. k=2). A smaller chunk size also
    results in more chunks, which can increase the system’s memory and processing
    requirements.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 较小的块大小会产生更细粒度的块，这可以提高检索到的块的相关性。然而，需要注意的是，最相关的信息可能不在检索到的最顶层块中，特别是如果 *similarity_top_k*
    设置较低（例如，k=2）。较小的块大小还会导致更多的块，这可能会增加系统的内存和处理需求。
- en: A larger chunk size can improve the system’s performance by reducing the number
    of chunks that need to be processed. However, it is important to note that a larger
    chunk size can also reduce the relevance of the retrieved chunks.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 较大的块大小可以通过减少需要处理的块的数量来提高系统的性能。然而，需要注意的是，较大的块大小也可能降低检索到的块的相关性。
- en: The optimal chunk size for a RAG system depends on a number of factors, including
    the size and complexity of the source text, the desired retrieval performance,
    and the available system resources. However, it is important to experiment with
    different chunk sizes to find the one that works best for your specific system.
    But, how do we know what works and what doesn’t?
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 RAG 系统的最佳块大小取决于多个因素，包括源文本的大小和复杂性、期望的检索性能以及可用的系统资源。然而，重要的是要尝试不同的块大小，以找到最适合您特定系统的那个。但是，我们如何知道什么有效，什么无效？
- en: '**Question:** *How can we measure the performance of RAG?*'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：*我们如何衡量 RAG 的性能？*'
- en: '**Answer:** We have to define evaluation metrics and then use evaluation tools
    to measure how the RAG performs considering the metrics. There are several tools
    to evaluate the RAG including LlamaIndex **Response Evaluation** [module](https://docs.llamaindex.ai/en/stable/optimizing/evaluation/evaluation.html)
    to test, evaluate and choose the right *chunk size*. It contains a few components,
    particularly:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**回答**：我们必须定义评估指标，然后使用评估工具来衡量 RAG 的表现，考虑到这些指标。有几个工具可以评估 RAG，包括 LlamaIndex **响应评估**
    [模块](https://docs.llamaindex.ai/en/stable/optimizing/evaluation/evaluation.html)来测试、评估和选择合适的
    *块大小*。它包含一些组件，尤其是：'
- en: '**Faithfulness Evaluator:** This tool assesses whether the response includes
    fabricated information and determines if the response aligns with any source nodes
    from the query engine.'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**忠实度评估器**：此工具评估响应是否包含虚构信息，并确定响应是否与查询引擎的任何源节点对齐。'
- en: '**Relevancy Evaluator:** This tool gauges whether the response effectively
    addresses the query and evaluates if the combined information from the response
    and source nodes corresponds to the query.'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**相关性评估器**：此工具衡量响应是否有效地解决了查询，并评估响应和源节点结合的信息是否与查询相符。'
- en: The code below shows how to use *Evaluation module* and determine the optimal
    *chunk size* for retrieval. To read the full article, see [this link](https://blog.llamaindex.ai/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码展示了如何使用*评估模块*并确定检索的最佳*块大小*。要阅读完整文章，请参阅[此链接](https://blog.llamaindex.ai/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5)。
- en: '[Figure 4.1](#fig-llamaindex-eval-part1) shows the data preparation for evaluation
    module.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4.1](#fig-llamaindex-eval-part1)展示了评估模块的数据准备。'
- en: '![data preparation for reponse evaluation](../Images/8d6a3e86600d31fac6e07bb4001b5ce1.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![响应评估的数据准备](../Images/8d6a3e86600d31fac6e07bb4001b5ce1.png)'
- en: 'Figure 4.1: data preparation for reponse evaluation'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1：响应评估的数据准备
- en: '[Figure 4.2](#fig-llamaindex-eval-part2) displays the criteria to set for evaluation.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4.2](#fig-llamaindex-eval-part2)显示了为评估设置的准则。'
- en: '![Define criteria for evaluation](../Images/8311945b9e093422ef27ebcce3a75bf9.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![定义评估标准](../Images/8311945b9e093422ef27ebcce3a75bf9.png)'
- en: 'Figure 4.2: Define criteria for evaluation'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2：定义评估标准
- en: '[Figure 4.3](#fig-llamaindex-eval-part3) shows the evaluation function definition.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4.3](#fig-llamaindex-eval-part3)展示了评估函数的定义。'
- en: '![Define a function to perform evaluation](../Images/b8b01afa38bba2f8fd8f8381e8bfddf9.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![定义一个用于评估的函数](../Images/b8b01afa38bba2f8fd8f8381e8bfddf9.png)'
- en: 'Figure 4.3: Define a function to perform evaluation'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3：定义一个用于评估的函数
- en: And [Figure 4.4](#fig-llamaindex-eval-part4) demonstrates testing the evaluation
    function with different chunk sizes.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 并且[图4.4](#fig-llamaindex-eval-part4)演示了使用不同块大小测试评估函数。
- en: '![Run the evaluation function with different parameters](../Images/23b8bd25fcae2befaa0dd2f84067081b.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![使用不同参数运行评估函数](../Images/23b8bd25fcae2befaa0dd2f84067081b.png)'
- en: 'Figure 4.4: Run the evaluation function with different parameters'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4：使用不同参数运行评估函数
- en: They test across different chunk sizes and conclude (in this experiment) that
    `chunk_size = 1024` results in peaking of *Average Faithfulness* and *Average
    Relevancy*.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 他们测试了不同的块大小，并得出结论（在这个实验中），`chunk_size = 1024`导致*平均忠实度*和*平均相关性*达到峰值。
- en: 'Here are summary of the tips for choosing the optimal chunk size for a RAG
    system:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是选择RAG系统最佳块大小的技巧总结：
- en: Consider the size and complexity of the source text. Larger and more complex
    texts will require larger chunk sizes to ensure that all of the relevant information
    is captured.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑源文本的大小和复杂性。更大的文本和更复杂的文本需要更大的块大小以确保捕获所有相关信息。
- en: Consider the desired retrieval performance. If you need to retrieve the most
    relevant chunks possible, you may want to use a smaller chunk size. However, if
    you need to retrieve chunks quickly, you may want to use a larger chunk size.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑所需的检索性能。如果您需要检索尽可能相关的块，您可能希望使用较小的块大小。然而，如果您需要快速检索块，您可能希望使用较大的块大小。
- en: 'Consider the available system resources. If you are limited on system resources,
    you may want to use a smaller chunk size. However, if you have ample system resources,
    you may want to use a larger chunk size. You can evaluate the optimal chunk size
    for your RAG system by using a variety of metrics, such as:'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑可用的系统资源。如果您系统资源有限，您可能希望使用较小的块大小。然而，如果您有充足的系统资源，您可能希望使用较大的块大小。您可以通过使用各种指标来评估您RAG系统的最佳块大小，例如：
- en: '**Relevance:** The percentage of retrieved chunks that are relevant to the
    query.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相关性**：检索到的块与查询的相关性百分比。'
- en: '**Faithfulness:** The percentage of retrieved chunks that are faithful to the
    source text.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**忠实度**：检索到的块忠实于源文本的百分比。'
- en: '**Response time:** The time it takes to retrieve chunks for a query. Once you
    have evaluated the performance of your RAG system for different chunk sizes, you
    can choose the chunk size that strikes the best balance between relevance, faithfulness,
    and response time.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**响应时间**：检索查询块所需的时间。一旦你评估了不同块大小的RAG系统性能，你可以选择在相关性、忠实度和响应时间之间取得最佳平衡的块大小。'
- en: 4.2.2 Additional Resources for RAG Evaluation
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.2 RAG评估的额外资源
- en: 'Evaluation of RAG applications remains an unsolved problem today and is an
    active research area. Here are just a few references for practical evaluation
    of RAG:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: RAG应用的评估至今仍是一个未解决的问题，并且是一个活跃的研究领域。以下是一些关于RAG实际评估的参考文献：
- en: 1\. This [blog](https://www.databricks.com/blog/LLM-auto-eval-best-practices-RAG)
    from Databricks has some best practices to evaluate RAG applications. [Figure 4.5](#fig-databricks-rag)
    illustrates what their experiment setup looks like.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 1. Databricks的这篇[博客](https://www.databricks.com/blog/LLM-auto-eval-best-practices-RAG)提供了一些评估RAG应用的最佳实践。[图4.5](#fig-databricks-rag)展示了他们的实验设置。
- en: '![experiment setup](../Images/c969d87960d565633040739b20355bb2.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![实验设置](../Images/c969d87960d565633040739b20355bb2.png)'
- en: 'Figure 4.5: Databricks evaluation experiment setup. [Image source](https://www.databricks.com/blog/LLM-auto-eval-best-practices-RAG)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5：Databricks评估实验设置。[图片来源](https://www.databricks.com/blog/LLM-auto-eval-best-practices-RAG)
- en: Zheng et al. ([2023](references.html#ref-zheng2023judging)) propose a strong
    LLMs as judges to evaluate these models on more open-ended questions.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Zheng等人([2023](references.html#ref-zheng2023judging))提出使用强大的LLM作为评委来评估这些模型在更开放性问题上的表现。
- en: '[RAG Evaluation](https://cobusgreyling.medium.com/rag-evaluation-9813a931b3d4)
    is another interesting blog that discuss how to use Langchain for evaluating RAG
    applications.'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[RAG评估](https://cobusgreyling.medium.com/rag-evaluation-9813a931b3d4)是另一篇有趣的博客，讨论了如何使用Langchain评估RAG应用。'
- en: 4.3 Retrieval Chunks vs. Synthesis Chunks
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 检索块与合成块
- en: Another fundamental technique for enhancing retrieval in Retrieval Augmented
    Generation (RAG) systems is the *decoupling of chunks* used for retrieval from
    those used for synthesis (i.e. response generation). The main idea is optimal
    chunk representation for retrieval may not necessarily align with the requirements
    for effective synthesis. While a raw text chunk could contain essential details
    for the LLM to generate a comprehensive response, it might also contain filler
    words or information that could introduce biases into the embedding representation.
    Furthermore, it might lack the necessary global context, making it challenging
    to retrieve the chunk when a relevant query is received. To give an example, think
    about having a question answering system on emails. Emails often contain so much
    *fluff* (a big portion of the email is “looking forward”, “great to hear from
    you”, etc.) and so little information. Thus, retaining semantics in this context
    for better and more accurate question answering is very important.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在检索增强生成（RAG）系统中增强检索的另一个基本技术是将用于检索的块与用于合成的块（即响应生成）*解耦*。主要思想是检索的最佳块表示可能不一定与有效合成的需求相一致。虽然原始文本块可能包含LLM生成全面响应所必需的详细信息，但它也可能包含填充词或可能引入嵌入表示偏差的信息。此外，它可能缺乏必要的全局上下文，使得在接收到相关查询时检索块变得具有挑战性。以一个电子邮件问答系统为例，电子邮件通常包含很多*冗余内容*（电子邮件的大部分是“期待”，“很高兴收到你的来信”等），而信息很少。因此，在这个上下文中保留语义以实现更好的和更准确的问答非常重要。
- en: 'There are a few ways to implement this technique including:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 实施此技术有几种方法，包括：
- en: Embed references to text chunks
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 嵌入文本块的引用
- en: Expand sentence-level context window
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 扩展句子级上下文窗口
- en: 4.3.1 Embed References to Text Chunks
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.1 嵌入文本块的引用
- en: 'The main idea here is that, we create an index in the vector database for storing
    document summaries. When a query comes in, we first fetch relevant document(s)
    at the high-level (i.e. summaries) before retrieving smaller text chunks directly,
    because it might retrieve irrelavant chunks. Then we can retrieve smaller chunks
    from the fetched document(s). In other words, we store the data in a hierarchical
    fashion: summaries of documents and chunks for each document. We can consider
    this approach as *Dense Hierarchical Retrieval*, in which a document-level retriever
    (i.e. summary index) first identifies the relevant documents, and then a passage-level
    retriever finds the relevant passages/chunks. Y. Liu et al. ([2021](references.html#ref-liu2021dense))
    and Zhao et al. ([2022](references.html#ref-zhao2022dense)) gives you a deep understanding
    of this approach. [Figure 4.6](#fig-document-summary-index) shows how this technique
    works.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的主要思想是，我们在向量数据库中创建一个索引来存储文档摘要。当查询到来时，我们首先在高级别（即摘要）检索相关的文档，然后再直接检索较小的文本块，因为这样可能会检索到不相关的块。然后我们可以从检索到的文档中检索较小的块。换句话说，我们以分层的方式存储数据：文档的摘要和每个文档的块。我们可以将这种方法视为*密集分层检索*，其中文档级检索器（即摘要索引）首先识别相关的文档，然后段落级检索器找到相关的段落/块。Y.
    Liu等人([2021](references.html#ref-liu2021dense))和Zhao等人([2022](references.html#ref-zhao2022dense))为您提供了对这个方法的深入了解。[图4.6](#fig-document-summary-index)展示了这项技术是如何工作的。
- en: '![document summary index](../Images/afe45d004a3251c1bae8b1c69643edba.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![文档摘要索引](../Images/afe45d004a3251c1bae8b1c69643edba.png)'
- en: 'Figure 4.6: Document summary index'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6：文档摘要索引
- en: We can choose different strategies based on the type of documents we are dealing
    with. For example, if we have a list of web pages, we can consider each page as
    a *document* that we summarize, and also we split each document into a set of
    smaller chunks as the second level of our data store strategy. When user asks
    a question, we first find the relevant page using the summary embeddings, and
    then we can retrieve the relevant chunks from that particular page.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以根据我们处理文档的类型选择不同的策略。例如，如果我们有一份网页列表，我们可以考虑每个页面作为一个*文档*进行总结，并且我们还可以将每个文档拆分成一组更小的块作为我们数据存储策略的第二级。当用户提问时，我们首先使用摘要嵌入找到相关的页面，然后我们可以从该特定页面检索相关的块。
- en: If we have a pdf document, we can consider each page of the pdf as a separate
    document, and then split each page into smaller chunks. If we have a list of pdf
    files, we can choose the entire content of each pdf to be a document and split
    the it into smaller chunks.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一个PDF文档，我们可以考虑PDF的每一页作为一个单独的文档，然后拆分每一页成更小的块。如果我们有一份PDF文件列表，我们可以选择每个PDF的整个内容作为一个文档，并将其拆分成较小的块。
- en: Let’s code it up for our pdf file.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为我们的PDF文件编写代码。
- en: 'Step 1: Read the PDF file'
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第1步：读取PDF文件
- en: We read the pdf file, [Figure 4.7](#fig-reading_pdf), and create a list of pages
    as later we will view each page as a separate document.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们读取PDF文件，[图4.7](#fig-reading_pdf)，并创建一个页面列表，因为我们稍后会将每一页视为一个单独的文档。
- en: '![reading documents](../Images/30424e75a44a11a8d43e28dc0dfaee2f.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![阅读文档](../Images/30424e75a44a11a8d43e28dc0dfaee2f.png)'
- en: 'Figure 4.7: Read a list of documents from each page of the pdf'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7：从PDF的每一页读取文档列表
- en: 'Step 2: Create Document Summary Index'
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第2步：创建文档摘要索引
- en: In order to create an *index*, first we have to convert a list of texts into
    a list of *Document* that is compatible with LlamaIndex.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建一个*索引*，我们首先必须将一系列文本转换为与LlamaIndex兼容的*文档*列表。
- en: '**Definition:** A [Document](https://gpt-index.readthedocs.io/en/latest/core_modules/data_modules/documents_and_nodes/root.html)
    is a generic container around any data source, for instance, a PDF, an API output,
    or retrieved data from a database. It stores text along with some other properties
    such as *metadata* and *relationships (Links to other Documents/Nodes)*'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**：一个[文档](https://gpt-index.readthedocs.io/en/latest/core_modules/data_modules/documents_and_nodes/root.html)是围绕任何数据源的一个通用容器，例如，PDF、API输出或从数据库检索的数据。它存储文本以及一些其他属性，如*元数据*和*关系（链接到其他文档/节点）*。'
- en: '[Figure 4.8](#fig-create-summary-index) shows the code.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4.8](#fig-create-summary-index)展示了代码。'
- en: '![build document summary index](../Images/45d5989d0fbbbbaff7e042f8977d49a7.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![构建文档摘要索引](../Images/45d5989d0fbbbbaff7e042f8977d49a7.png)'
- en: 'Figure 4.8: Build a document summary index'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8：构建文档摘要索引
- en: We can use the summary index to get the summary of each page/document using
    the document `id`, for instanc, [Figure 4.9](#fig-document-summary-example) shows
    the output of a summary of a document.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用摘要索引通过文档`id`获取每页/文档的摘要，例如，[图4.9](#fig-document-summary-example)展示了文档摘要的输出。
- en: '![example of document summary](../Images/bd868ca4fc40a4527e03b8940d20dcf8.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![文档摘要示例](../Images/bd868ca4fc40a4527e03b8940d20dcf8.png)'
- en: 'Figure 4.9: Example of a document summary'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.9：文档摘要示例
- en: 'Step 3: Retrieve and Generate Response using Document Summary Index'
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第3步：使用文档摘要索引检索和生成响应
- en: In this step, when a query comes, we run a retrieval from the document summary
    index to find the relevant pages. Retrieved document has links to its corresponding
    chunks, that are used to generate the final response to the query.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个步骤中，当查询到来时，我们从文档摘要索引中进行检索以找到相关的页面。检索到的文档包含指向其对应块（chunks）的链接，这些块用于生成对查询的最终响应。
- en: 'There are multiple ways to that in LlamaIndex:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在LlamaIndex中有多种实现方式：
- en: High-level query execution
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级查询执行
- en: LLM based retrieval
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于LLM的检索
- en: Embedding based retrieval
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于嵌入的检索
- en: The high-level approach is depicted in [Figure 4.10](#fig-high-level-q-exec)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 高级方法在[图4.10](#fig-high-level-q-exec)中展示。
- en: '![high level quey execution](../Images/31a328aa8e951991a441d85f83f03b14.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![高级查询执行](../Images/31a328aa8e951991a441d85f83f03b14.png)'
- en: 'Figure 4.10: High-level query execution approach (default approach)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.10：高级查询执行方法（默认方法）
- en: '**LLM based retrieval:** This approach is low-level so we can view and change
    the parameters. [Figure 4.11](#fig-llm-based-retrieval) below displays the code
    snippet:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于LLM的检索**：这种方法是低级的，因此我们可以查看和更改参数。[图4.11](#fig-llm-based-retrieval)下面显示了代码片段：'
- en: '![llm based retrieval](../Images/8dde311ae3d44095656231e19596d014.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![基于LLM的检索](../Images/8dde311ae3d44095656231e19596d014.png)'
- en: 'Figure 4.11: LLM based retrieval approach'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.11：基于LLM的检索方法
- en: '**Embedding based retrieval:** In this technique, we first define `DocumentSummaryIndexEmbeddingRetriever`
    retriever, and configure the response generator to use this retriever. We then,
    integrate these two components into a `RetrieverQueryEngine` and run that for
    the query. [Figure 4.12](#fig-embedding-based-retrieval) shows the code snippet
    for this approach.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于嵌入的检索**：在这个技术中，我们首先定义`DocumentSummaryIndexEmbeddingRetriever`检索器，并配置响应生成器使用这个检索器。然后，我们将这两个组件集成到`RetrieverQueryEngine`中并运行它以进行查询。[图4.12](#fig-embedding-based-retrieval)展示了这个方法的代码片段。'
- en: '![embedding based retrieval](../Images/8e581dbed343a4cb74ce737dcd917b9a.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![基于嵌入的检索](../Images/8e581dbed343a4cb74ce737dcd917b9a.png)'
- en: 'Figure 4.12: Embedding based retrieval'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.12：基于嵌入的检索
- en: 4.3.2 Expand sentence-level context window
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.2 扩展句子级上下文窗口
- en: In this approach, we have split the text into sentence level chunks to be able
    to perform fine-grained retrieval. However, before passing the fetched sentences
    to LLM response generator, we include the sentences surrounding the retrieved
    sentence, to enlarge the context window for better accuaracy. Please be mindful
    of *lost in the middle* problem when splitting large textual content at a very
    fine-grained level, such as sentence-level.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方法中，我们将文本分割成句子级别的块以执行细粒度检索。然而，在将检索到的句子传递给LLM响应生成器之前，我们包括检索句子周围的句子，以扩大上下文窗口以提高准确性。请注意，在非常细粒度地分割大型文本内容时，例如句子级别，可能会出现*中间丢失*的问题。
- en: '[Figure 4.13](#fig-small-to-big) illustrates this technique.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4.13](#fig-small-to-big)说明了这个技术。'
- en: '![Expanding the sentence level context](../Images/48db55a99ab6f6914081c81cfc40a511.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![扩展句子级上下文](../Images/48db55a99ab6f6914081c81cfc40a511.png)'
- en: 'Figure 4.13: Expanding the sentence level context, so LLM has a bigger context
    to use to generate the response'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.13：扩展句子级上下文，以便LLM有更大的上下文来生成响应
- en: Implementation
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现
- en: Again, we rely on LlamaIndex to implement this technique. We use `SentenceWindowNodeParser`
    to split document into sentences and save each sentence in a node. Node contains
    a *window* property where we can adjust. During the retrieval step, each fetched
    sentence will be replaced with surrounding sentences depending on the window size
    via `MetadataReplacementNodePostProcessor` function.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们依赖LlamaIndex来实现这个技术。我们使用`SentenceWindowNodeParser`将文档分割成句子并将每个句子保存到一个节点中。节点包含一个*窗口*属性，我们可以调整。在检索步骤中，每个检索到的句子将通过`MetadataReplacementNodePostProcessor`函数根据窗口大小替换为周围的句子。
- en: '[Figure 4.14](#fig-sentence-window-code1) shows the basic setup such as importing
    necessary modules, reading the pdf file and initializing the LLM and embedding
    models.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4.14](#fig-sentence-window-code1)展示了基本设置，例如导入必要的模块、读取PDF文件以及初始化LLM和嵌入模型。'
- en: '![sentence window implementation](../Images/74389042ece4d713efe22787e504c283.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![句子窗口实现](../Images/74389042ece4d713efe22787e504c283.png)'
- en: 'Figure 4.14: Basic setup for sentence window implementation'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.14：句子窗口实现的初步设置
- en: Next, we have to define nodes that are going to be stored in the `VectorIndex`
    as well as sentence index. Then, we create a query engine and run the query. [Figure 4.15](#fig-sentence-window-code2)
    shows the steps.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们必须定义将要存储在`VectorIndex`以及句子索引中的节点。然后，我们创建一个查询引擎并运行查询。[图4.15](#fig-sentence-window-code2)展示了这些步骤。
- en: '![Build the sentence index, and run the query](../Images/70e1721a154ef05cadd5306eb9dceee7.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![构建句子索引并运行查询](../Images/70e1721a154ef05cadd5306eb9dceee7.png)'
- en: 'Figure 4.15: Build the sentence index, and run the query'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.15：构建句子索引并运行查询
- en: '[Figure 4.16](#fig-sentence-window-response) displays the response output.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4.16](#fig-sentence-window-response)显示了响应输出。'
- en: '![output of the window response](../Images/a8ee550f3d5d589feac2e4956bf36825.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![窗口响应的输出](../Images/a8ee550f3d5d589feac2e4956bf36825.png)'
- en: 'Figure 4.16: Output of the window response'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.16：窗口响应的输出
- en: We can see the original sentence that is retrieved for each node (we show the
    first node below) and also the actual window of sentences in the [Figure 4.17](#fig-sentence-window-original-sentence).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到为每个节点检索的原始句子（以下展示第一个节点）以及实际的句子窗口在[图4.17](#fig-sentence-window-original-sentence)中。
- en: '![original sentence that was retrieved for each node, as well as the actual
    window of sentences](../Images/fc4203b72dc7df15fc96293bdebca673.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![为每个节点检索的原始句子以及实际的句子窗口](../Images/fc4203b72dc7df15fc96293bdebca673.png)'
- en: 'Figure 4.17: Original sentence that was retrieved for each node, as well as
    the actual window of sentences'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.17：为每个节点检索的原始句子以及实际的句子窗口
- en: 4.3.3 Lost in the Middle Problem
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.3 中间丢失问题
- en: Retrieval process in a RAG based application is all about retrieving the right
    and most relevant documents for a given user’s query. The way we find these documents
    is that retrieval method assigns a relevance score to each document based on their
    similarity to the query. Then, sorts them descendingly and returns them. Nevertheless,
    this approach might not work well when we are returning many documents such as
    `top-k >= 10`. The reason is when we pass a very long context to LLM, it tend
    to ignore or overlook the documents in the middle. Consequently, putting the least
    relevant document to the bottom of the fetch documents is not the best strategy.
    A better way is to place the least relevant documents in the middle.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于RAG的应用程序中，检索过程完全是关于为给定用户的查询检索正确且最相关的文档。我们找到这些文档的方式是检索方法根据文档与查询的相似度为每个文档分配一个相关性分数。然后，按降序排序并返回它们。然而，当我们返回许多文档，例如`top-k
    >= 10`时，这种方法可能效果不佳。原因是当我们向LLM传递一个非常长的上下文时，它往往会忽略或忽视中间的文档。因此，将最不相关的文档放在检索文档的底部并不是最佳策略。更好的方法是将这些最不相关的文档放在中间。
- en: 'N. F. Liu et al. ([2023](references.html#ref-liu2023lost)) in [Lost in the
    Middle: How Language Models Use Long Contexts](https://www-cs.stanford.edu/~nfliu/papers/lost-in-the-middle.arxiv2023.pdf)
    demonstrated interesting findings about LLMs behavior. They realized that performance
    of LLMs is typically at its peak when relevant information is located at the beginning
    or end of the input context. However, it notably deteriorates when models are
    required to access relevant information buried within the middle of lengthy contexts.
    [Figure 4.18](#fig-lost-inthe-middle-paper) demonstrates the results.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: N. F. Liu等人([2023](references.html#ref-liu2023lost))在[《中间丢失：语言模型如何使用长上下文》](https://www-cs.stanford.edu/~nfliu/papers/lost-in-the-middle.arxiv2023.pdf)中展示了关于LLM行为的有趣发现。他们意识到，当相关信息位于输入上下文的开始或结束时，LLM的性能通常达到顶峰。然而，当模型需要访问埋藏在长上下文中间的相关信息时，性能会明显下降。[图4.18](#fig-lost-inthe-middle-paper)展示了这些结果。
- en: '![lost in the middle paper results](../Images/41e66da443d4d962f000dc83927301d3.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![中间丢失的论文结果](../Images/41e66da443d4d962f000dc83927301d3.png)'
- en: 'Figure 4.18: Accuracy of the RAG based on the postions of the retrieved documents.
    [Image source](https://www-cs.stanford.edu/~nfliu/papers/lost-in-the-middle.arxiv2023.pdf)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.18：基于RAG的准确性与检索文档的位置。[图片来源](https://www-cs.stanford.edu/~nfliu/papers/lost-in-the-middle.arxiv2023.pdf)
- en: They also show that LLMs with longer context windows still face this problem
    and increasing the context window doesn’t solve this issue. The following demonstrates
    this experiment.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 他们还表明，具有更长上下文窗口的 LLM 仍然面临这个问题，增加上下文窗口并不能解决这个问题。以下演示了这项实验。
- en: '![Comparing LLM models with various context size](../Images/2c8b887e1f84228a4d2b26eccfb33866.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![比较不同上下文大小的 LLM 模型](../Images/2c8b887e1f84228a4d2b26eccfb33866.png)'
- en: 'Figure 4.19: Comparing LLM models with various context size and the impact
    of changing the position of relevant documents'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.19：比较不同上下文大小的 LLM 模型和改变相关文档位置的影响
- en: '**How can we alleviate this problem?** The answer is to reorder the retrieved
    documents in such a way that most similar documents to the query are placed at
    the top, the less similar documents at the bottom, and the least similar documents
    in the middle.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们如何减轻这个问题呢？** 答案是将检索到的文档重新排序，以便将查询的最相似文档放在顶部，不太相似的文档放在底部，最不相似的文档放在中间。'
- en: For implementation, we need a function to get the retrieved documents from the
    `retriever` and reorder them, i.e. place most relevant documents at the beginning
    and end. [Figure 4.20](#fig-lost-inthe-middle-pseudocode) shows our code.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 对于实现，我们需要一个函数来从 `retriever` 获取检索到的文档并重新排序它们，即放置最相关的文档在开始和末尾。[图 4.20](#fig-lost-inthe-middle-pseudocode)
    展示了我们的代码。
- en: '![pseudocode of a function to solve lost in the middle problem](../Images/73b89646955c3a2e97c591517193a4be.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![解决中间丢失问题的函数伪代码](../Images/73b89646955c3a2e97c591517193a4be.png)'
- en: 'Figure 4.20: Pseudocode of a function to solve lost in the middle problem'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.20：解决中间丢失问题的函数伪代码
- en: 'We can instead utilize Langchain solution: `LongContextReorder`. It essentially
    implements a similar approach to [Figure 4.20](#fig-lost-inthe-middle-pseudocode)
    function. You can read the [documentation](https://api.python.langchain.com/en/latest/document_transformers/langchain.document_transformers.long_context_reorder.LongContextReorder.html)
    for more details.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以改用 Langchain 解决方案：`LongContextReorder`。它本质上实现了与[图 4.20](#fig-lost-inthe-middle-pseudocode)
    函数类似的方法。您可以阅读[文档](https://api.python.langchain.com/en/latest/document_transformers/langchain.document_transformers.long_context_reorder.LongContextReorder.html)获取更多详细信息。
- en: '[Figure 4.21](#fig-lost-inthe-middle-langchain) shows how to use Langchain
    solution to deal with this problem.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4.21](#fig-lost-inthe-middle-langchain) 展示了如何使用 Langchain 解决方案来处理这个问题。'
- en: '![langchain approach for solving lost in the middle problem](../Images/84fb036c2a2eb5215190d6fc12f8e335.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![langchain 解决方案用于解决中间丢失问题](../Images/84fb036c2a2eb5215190d6fc12f8e335.png)'
- en: 'Figure 4.21: Langchain approach for solving lost in the middle problem'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.21：Langchain 解决方案用于解决中间丢失问题
- en: We can also use [Haystack](https://haystack.deepset.ai/) to deal with this problem.
    Haystack is the open source framework for building custom NLP apps with large
    language models (LLMs) in an end-to-end fashion. It offers a few components that
    are building blocks for performing various tasks like document retrieval, and
    summarization. We can connect these components and create an end-to-end *pipeline*.
    The two very useful components that we can utilize are *DiversityRanker* and *LostInTheMiddleRanker*.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用 [Haystack](https://haystack.deepset.ai/) 来处理这个问题。Haystack 是一个开源框架，用于以端到端的方式构建使用大型语言模型
    (LLM) 的自定义 NLP 应用程序。它提供了一些组件，这些组件是执行各种任务（如文档检索和摘要）的构建块。我们可以连接这些组件并创建一个端到端 *管道*。我们可以利用的两个非常有用的组件是
    *DiversityRanker* 和 *LostInTheMiddleRanker*。
- en: '**DiversityRanker** is designed to maximize the variety of given documents.
    It does so by selecting the most semantically similar document to the query, then
    selecting the least similar one, and continuing this process with the remaining
    documents until a diverse set is formed. It operates on the principle that a diverse
    set of documents can increase the LLM’s ability to generate answers with more
    breadth and depth.'
  id: totrans-137
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**DiversityRanker** 设计用于最大化给定文档的多样性。它通过选择与查询最语义相似的文档，然后选择最不相似的文档，并继续使用剩余文档进行此过程，直到形成一个多样化的集合。它基于这样一个原则，即多样化的文档集合可以增加
    LLM 生成更广泛和深入答案的能力。'
- en: '**LostInTheMiddleRanker** sorts the documents based on the “Lost in the Middle”
    order. The ranker positions the most relevant documents at the beginning and at
    the end of the resulting list while placing the least relevant documents in the
    middle.'
  id: totrans-138
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**LostInTheMiddleRanker** 根据中间丢失的顺序对文档进行排序。排序器将最相关的文档放在结果列表的开始和末尾，而将最不相关的文档放在中间。'
- en: Please check their [documentation](https://docs.haystack.deepset.ai/docs/ranker)
    for more details.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 请查阅它们的[文档](https://docs.haystack.deepset.ai/docs/ranker)以获取更多详细信息。
- en: 4.3.4 Embedding Optimization
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.4 嵌入优化
- en: 'Optimizing embeddings can have a significant impact on the results of your
    use cases. There are various APIs and providers of embedding models, each catering
    to different objectives:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 优化嵌入可以对您用例的结果产生重大影响。有各种嵌入模型的API和提供商，每个都针对不同的目标：
- en: Some are best suited for coding tasks.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有些模型最适合编码任务。
- en: Others are designed specifically for the English language.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有些是为英语语言专门设计的。
- en: And there are also embedding models that excel in handling multilingual datasets
    (e.g., Multilingual BERT/[mBERT](https://huggingface.co/bert-base-multilingual-cased)).
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，还有一些嵌入模型在处理多语言数据集方面表现出色（例如，多语言BERT/[mBERT](https://huggingface.co/bert-base-multilingual-cased)）。
- en: However, determining which embedding model is the best fit for your dataset
    requires an effective evaluation method.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，确定哪种嵌入模型最适合您的数据集需要有效的评估方法。
- en: '**So which embedding models should we use?**'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**那么我们应该使用哪些嵌入模型呢？**'
- en: One approach is to rely on existing academic benchmarks. However, it’s important
    to note that these benchmarks may not fully capture the real-world usage of retrieval
    systems in AI use cases. They are often synthetic benchmarks specifically designed
    for information retrieval problems.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 一种方法是通过依赖现有的学术基准。然而，重要的是要注意，这些基准可能无法完全捕捉到AI用例中检索系统在现实世界中的使用。它们通常是专门为信息检索问题设计的合成基准。
- en: 'For example, there is a benchmark called MTEB (Massive Text Embedding Benchmark).
    MTEB’s [leaderboard](https://huggingface.co/spaces/mteb/leaderboard) showcases
    embedding models across 8 tasks, including multilingual tasks, and currently features
    132 models. You can compare the performance, speed, or both for these models (refer
    to the graph below: [Figure 4.22](#fig-MTEB-embeddings)).'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，有一个名为MTEB（大规模文本嵌入基准）的基准。MTEB的[排行榜](https://huggingface.co/spaces/mteb/leaderboard)展示了包括多语言任务在内的8个任务中的嵌入模型，目前有132个模型。您可以比较这些模型的性能、速度或两者（参考下面的图表：[图4.22](#fig-MTEB-embeddings)）。
- en: '![MTEB-embeddings](../Images/4e778eee667e3581fd740d91633c83fd.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![MTEB-embeddings](../Images/4e778eee667e3581fd740d91633c83fd.png)'
- en: 'Figure 4.22: Models by average English MTEB score (y) vs speed (x) vs embedding
    size (circle size). [Image source](https://huggingface.co/blog/mteb)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.22：平均英语MTEB分数（y）与速度（x）与嵌入大小（圆圈大小）的模型。[图片来源](https://huggingface.co/blog/mteb)
- en: For better results, you can still utilize open-source tools by applying them
    to your specific data and use case. Additionally, you can enhance relevance by
    incorporating human feedback through a simple relevance feedback endpoint.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得更好的结果，您仍然可以通过将它们应用于您的特定数据和用例来利用开源工具。此外，您可以通过简单地将人类反馈纳入相关性反馈端点来提高相关性。
- en: Constructing your own datasets is also important as you have a deep understanding
    of your production data, relevant metrics, and what truly matters to you. This
    allows you to tailor the training and evaluation process to your specific needs.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 构建自己的数据集也很重要，因为您对生产数据、相关指标以及真正对您重要的事情有深入的了解。这使您能够根据特定需求定制训练和评估过程。
- en: In terms of evaluating the performance of embedding models, there are excellent
    evaluation tools available in the market. These tools can help you assess the
    effectiveness of different models and make informed decisions.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估嵌入模型性能方面，市场上提供了出色的评估工具。这些工具可以帮助您评估不同模型的有效性，并做出明智的决定。
- en: It is worth noting that recent research and experiments have shown that embedding
    models with the same training objective and similar data tend to learn very similar
    representations, up to an affine linear transform. This means that it is possible
    to project one model’s embedding space into another model’s embedding space using
    a simple linear transform.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，最近的研究和实验表明，具有相同训练目标和相似数据的嵌入模型往往会学习到非常相似的表现，甚至可以达到仿射线性变换的程度。这意味着可以使用简单的线性变换将一个模型的嵌入空间投影到另一个模型的嵌入空间。
- en: By understanding and leveraging linear identifiability, you can explore ways
    to transfer knowledge between embedding models and potentially improve their performance
    in specific tasks.”
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 通过理解和利用线性可识别性，您可以探索在嵌入模型之间转移知识的方法，并有可能提高它们在特定任务中的性能。”
- en: This is called linear identifiability, and it was discussed in the 2020 paper
    by Roeder and Kingma ([2021](references.html#ref-Roeder2021linear)) [On Linear
    Identifiability of Learned Representations](https://arxiv.org/pdf/2007.00810.pdf)
    from Google Brain. The paper states,
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为线性可识别性，它是在2020年由Roeder和Kingma在Google Brain发表的论文《On Linear Identifiability
    of Learned Representations》中讨论的。[2021](references.html#ref-Roeder2021linear) [论文链接](https://arxiv.org/pdf/2007.00810.pdf)。论文中提到，
- en: '*“We demonstrate that as one increases the representational capacity of the
    model and dataset size, learned representations indeed tend towards solutions
    that are equal up to only a linear transformation.”*'
  id: totrans-157
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“我们证明，随着模型表示能力和数据集大小的增加，学习到的表示确实趋向于仅通过线性变换即可相等的解决方案。”*'
- en: 'See an example below:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个示例：
- en: '![linear-identifiability](../Images/e5c1e159623501ed6b50b35507197a2c.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![线性可识别性](../Images/e5c1e159623501ed6b50b35507197a2c.png)'
- en: 'Figure 4.23: Linear Identifiability. [Image source](https://arxiv.org/pdf/2007.00810.pdf)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.23：线性可识别性。[图片来源](https://arxiv.org/pdf/2007.00810.pdf)
- en: Therefore, the selection of a particular embedding model may not be that important
    if you are able to discover and implement a suitable transformation from your
    own dataset.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果你能够从自己的数据集中发现并实现一个合适的转换，那么选择特定的嵌入模型可能并不那么重要。
- en: 4.4 Rethinking Retrieval Methods for Heterogeneous Document Corpora
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.4 重新思考异构文档集合的检索方法
- en: Retrieval-augmented generation (RAG) applications, especially when dealing with
    a substantial volume of documents (e.g. having many pdf files), often face challenges
    related to performance, relevance, and latency.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）应用，尤其是在处理大量文档（例如拥有许多PDF文件）时，通常会面临与性能、相关性和延迟相关的问题。
- en: '**Example:** Assume a user asks a question and the answer to user’s question
    only involves two pdf files, we would rather first get those two relevant pdf
    documents and then find the actual answer from their chunks, instead of searching
    through thusands of text chunks. But how to do that?'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** 假设一个用户提出一个问题，而用户问题的答案仅涉及两个PDF文件，我们更愿意首先获取这两个相关的PDF文档，然后从它们的片段中找到实际的答案，而不是在成千上万的文本片段中进行搜索。但如何做到这一点呢？'
- en: 'There are multiple ways to achieve that goal:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法可以实现这一目标：
- en: Have multi-level embeddings, i.e. embed document summaries, where each document
    summary is related to its text chunks. This approach is implemented in [Section 4.3.1](#sec-document-summary-sec).
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有多级嵌入，即嵌入文档摘要，其中每个文档摘要都与它的文本片段相关。这种方法在[第4.3.1节](#sec-document-summary-sec)中实现。
- en: Add metadata about each document and store that along with the document in the
    vector database.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为每个文档添加元数据并将其与文档一起存储在向量数据库中。
- en: 4.4.1 How metadata can help
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.1 元数据如何帮助
- en: Including metadata in the indexing process can be a powerful strategy to address
    these challenges and significantly enhance the overall system’s efficiency. By
    narrowing down the search scope using metadata filters, the system can reduce
    the number of documents to be considered, resulting in faster retrieval times.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在索引过程中包含元数据可以是一种强大的策略，以解决这些挑战并显著提高整体系统的效率。通过使用元数据过滤器缩小搜索范围，系统可以减少需要考虑的文档数量，从而实现更快的检索时间。
- en: '[Figure 4.24](#fig-metadata-filtering) shows how metadata filtering can help
    the retrieval process. *When user asks a question, they can explicitly give metadata
    for instance by specifying filters such as dropdown list, etc., or we can use
    LLM to determine the metadata filters from the query and search the vector database
    using the filters. Vector database utilizes the filters to narrow down search
    to the documents that match with the filters, and then finds the most similar
    chunks from documents and returns the top-k chunks.*'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4.24](#fig-metadata-filtering)展示了元数据过滤如何帮助检索过程。*当用户提出问题时，他们可以明确地提供元数据，例如通过指定下拉列表等过滤器，或者我们可以使用LLM从查询中确定元数据过滤器，并使用这些过滤器在向量数据库中进行搜索。向量数据库利用这些过滤器缩小搜索范围，以匹配过滤器的文档，然后从文档中找到最相似的片段并返回前k个片段。*'
- en: '![how metadata filtering can help retrieval process](../Images/c2d0ab6ef1e9ca4452ae8062642d3bbc.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![元数据过滤如何帮助检索过程](../Images/c2d0ab6ef1e9ca4452ae8062642d3bbc.png)'
- en: 'Figure 4.24: How metadata filtering can improve retrieval process'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.24：元数据过滤如何改善检索过程
- en: Please note that although we can add metadata to the text chunks after they
    are stored in the vector database, we should do that in the preprocessing step
    while we are splitting and embedding the documents, because if the vector database
    index becomes very large (i.e. we already have a great deal of embeddings in vector
    database), updating it will be significantly time consuming.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，尽管我们可以在文本块存储在向量数据库之后添加元数据，但我们应该在预处理步骤中这样做，当我们分割和嵌入文档时，因为如果向量数据库索引变得非常大（即，我们已经在向量数据库中有很多嵌入），更新它将非常耗时。
- en: Use Langchain for Metadata Filtering
  id: totrans-174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用Langchain进行元数据过滤
- en: '[Figure 4.25](#fig-metadata-filtering-code1) shows how to define new metadata
    for text chunks, and how to use filters to perform retrieval.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4.25](#fig-metadata-filtering-code1)展示了如何为文本块定义新的元数据，以及如何使用过滤器进行检索。'
- en: In this example, we load two pdf files, one file is about machine learning interview
    questions, and the other file is a research paper. We would like to add a topic
    or category of each file as metadata, so later we can restrict our search to the
    category. Therefore, we update the initial metadata field by adding a `category`
    property to each text chunk and then store them in the vector database.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们加载了两个PDF文件，一个文件是关于机器学习面试问题的，另一个文件是一篇研究论文。我们希望为每个文件添加一个主题或分类作为元数据，以便以后我们可以将搜索限制在分类上。因此，我们通过为每个文本块添加一个`分类`属性来更新初始元数据字段，然后将它们存储在向量数据库中。
- en: '![Read the files and update the metadata property](../Images/82790288f463322ad704809d4b86a4f3.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![读取文件并更新元数据属性](../Images/82790288f463322ad704809d4b86a4f3.png)'
- en: 'Figure 4.25: Read the files and update the metadata property'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.25：读取文件并更新元数据属性
- en: If we print out the `metadata` of documents, we can see the `category` property
    has been added, shown in [Figure 4.26](#fig-metadata-filtering-code2-output).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们打印出文档的`元数据`，我们可以看到`分类`属性已经被添加，如图[图4.26](#fig-metadata-filtering-code2-output)所示。
- en: '![Output example of metadata for text chunks](../Images/f6abf5c5ec7c24cb9f488d912d0f3909.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![文本块的元数据输出示例](../Images/f6abf5c5ec7c24cb9f488d912d0f3909.png)'
- en: 'Figure 4.26: Output example of metadata for text chunks'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.26：文本块的元数据输出示例
- en: Now, we define the index and use it to perform search and retrieval, which is
    shown in [Figure 4.27](#fig-metadata-filtering-code3-retrieval).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们定义索引并使用它进行搜索和检索，如图[图4.27](#fig-metadata-filtering-code3-retrieval)所示。
- en: '![Insert text chunks into the vector database and perform retrieval](../Images/2ca071414d96af6f399662a0384df38f.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![将文本块插入向量数据库并执行检索](../Images/2ca071414d96af6f399662a0384df38f.png)'
- en: 'Figure 4.27: Insert text chunks into the vector database and perform retrieval'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.27：将文本块插入向量数据库并执行检索
- en: Use LlamaIndex for Metadata Filtering
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用LlamaIndex进行元数据过滤
- en: '[Figure 4.28](#fig-metadata-filtering-code3-retrieval-full-llamaindex) shows
    how to use LlamaIndex for metadata filtering.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4.28](#fig-metadata-filtering-code3-retrieval-full-llamaindex)展示了如何使用LlamaIndex进行元数据过滤。'
- en: '![Metadata filtering in LlamaIndex for document retrieval](../Images/97b0c02a34faa9dd2601138b805e6ca4.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![在LlamaIndex中进行文档检索的元数据过滤](../Images/97b0c02a34faa9dd2601138b805e6ca4.png)'
- en: 'Figure 4.28: Metadata filtering in LlamaIndex for document retrieval'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.28：在LlamaIndex中进行文档检索的元数据过滤
- en: How to let LLM infer the metadata from user question
  id: totrans-189
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何让LLM从用户问题中推断元数据
- en: 'In both of the previous techniques, we have to explicitly define the metadata/filters
    while doing the retrieval. However, the questions is: *Can we ask LLM to infer
    the metadata from user query?* The short answer is: *Yes*.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种之前的技术中，我们在检索时必须明确定义元数据/过滤器。然而，问题是：*我们能否让LLM从用户查询中推断元数据？*简短的答案是：*是的*。
- en: Therefore, the general approach is we need to define a particular prompt for
    LLM, so it can use that to extract entities or metadata from the user query, map
    them to the existing metadata stored with text chunks in the vector database,
    and then perform the retrieval.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一般的方法是我们需要为LLM定义一个特定的提示，这样它就可以使用它来从用户查询中提取实体或元数据，将它们映射到与文本块一起存储在向量数据库中的现有元数据，然后执行检索。
- en: That said, LlamaIndex provides us an implemented version of this approach. The
    following code is from the LlamaIndex documentation [here](https://gpt-index.readthedocs.io/en/latest/examples/vector_stores/chroma_auto_retriever.html).
    For this technique, we define the metadata (in this example `category` and `country`)
    along with each text chunk. [Figure 4.29](#fig-llamaindex-autoretrieval-code1)
    shows the code snippet for this step.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，LlamaIndex为我们提供了一个实现版本的这种方法。以下代码来自LlamaIndex文档[这里](https://gpt-index.readthedocs.io/en/latest/examples/vector_stores/chroma_auto_retriever.html)。对于这种技术，我们定义了元数据（在这个例子中是
    `category` 和 `country`）以及每个文本块。[图4.29](#fig-llamaindex-autoretrieval-code1) 展示了这一步骤的代码片段。
- en: '![Define text node and metadata for auto retrieval](../Images/314229deded46eb6f6faeca80c11565c.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![定义文本节点和元数据以实现自动检索](../Images/314229deded46eb6f6faeca80c11565c.png)'
- en: 'Figure 4.29: Define text node and metadata for auto retrieval'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.29：定义文本节点和元数据以实现自动检索
- en: '[Figure 4.30](#fig-llamaindex-autoretrieval_code2) shows the retrieval process
    including how to define vector index, vector store, and `VectorIndexAutoRetriever`
    object.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4.30](#fig-llamaindex-autoretrieval_code2) 展示了检索过程，包括如何定义向量索引、向量存储和 `VectorIndexAutoRetriever`
    对象。'
- en: '![Define VectorIndexAutoRetriever retriever and VectorStoreInfo](../Images/80c8601de75dcfcc2b4798de903c2fe9.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![定义 `VectorIndexAutoRetriever` 检索器和 `VectorStoreInfo`](../Images/80c8601de75dcfcc2b4798de903c2fe9.png)'
- en: 'Figure 4.30: Define VectorIndexAutoRetriever retriever and VectorStoreInfo,
    which contains a structured description of the vector store collection and the
    metadata filters it supports.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.30：定义 `VectorIndexAutoRetriever` 检索器和 `VectorStoreInfo`，它包含对向量存储集合的结构化描述以及它支持的元数据过滤器。
- en: 4.5 Hybrid Document Retrieval
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.5 混合文档检索
- en: Hybrid document retrieval is an approach that combines traditional keyword-based
    search like BM25 with semantic (dense) search using embeddings, such as BERT or
    word2vec. Integrating this technique to Retrieval-Augmented Generation (RAG) applications
    can significantly enhance the effectiveness of document retrieval. It addresses
    scenarios where a basic keyword-based approach can outperform semantic search
    and demonstrates how combining these methods improves retrieval in RAG applications.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 混合文档检索是一种结合传统基于关键词的搜索（如BM25）和语义（密集）搜索（使用嵌入，如BERT或word2vec）的方法。将这种技术集成到检索增强生成（RAG）应用中可以显著提高文档检索的有效性。它解决了基本基于关键词的方法可能优于语义搜索的场景，并展示了如何结合这些方法来改进RAG应用中的检索。
- en: In addition, often times a complete migration to a semantic-based search using
    RAG is challenging for most companies. They might already have a keyword-based
    search system and have been using it for quite a long time. Performing an overhaul
    to the company’s information architecture, and migrating to a vector database
    is just infeasible.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于大多数公司来说，完全迁移到基于RAG的语义搜索是具有挑战性的。他们可能已经拥有一个基于关键词的搜索系统，并且已经使用它很长时间了。对公司信息架构进行彻底的改造，并迁移到向量数据库是不切实际的。
- en: '**Scenarios Favoring Keyword-Based Search:**'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '**有利于基于关键词搜索的场景**：'
- en: '**Highly Specific Queries:** In cases where a user’s query is highly specific
    and focuses on precise terms or phrases, a keyword-based approach can outperform
    semantic search. Keyword matching excels at finding exact matches within documents.'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**高度特定的查询**：在用户的查询非常具体且专注于精确术语或短语的情况下，基于关键词的方法可能优于语义搜索。关键词匹配在找到文档中的精确匹配方面表现出色。'
- en: '**Niche Domains:** In specialized domains with industry-specific jargon or
    acronyms, keyword search can be more effective as it directly matches terminology
    without requiring extensive semantic understanding.'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**利基领域**：在具有行业特定术语或缩写的专业领域，关键词搜索可能更有效，因为它直接匹配术语，而不需要广泛的语义理解。'
- en: '**Short Documents:** When dealing with very short documents, such as tweets
    or headlines, keyword-based search can be more efficient. Semantic models often
    require longer text to derive meaningful embeddings.'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**短文档**：当处理非常短的文档，如推文或标题时，基于关键词的搜索可能更有效。语义模型通常需要更长的文本来推导出有意义的嵌入。'
- en: '**Low Resource Usage:** Keyword search typically requires fewer computational
    resources compared to semantic search. This can be advantageous when resource
    efficiency is a critical concern.'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**资源使用低**：与语义搜索相比，关键词搜索通常需要的计算资源更少。当资源效率是一个关键问题时，这可能是有利的。'
- en: '**Combining Keyword and Semantic Approaches for Improved Retrieval:**'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '**结合关键词和语义方法以改进检索**：'
- en: To harness the strengths of both keyword-based and semantic (dense) retrievers
    effectively, a pragmatic approach is to integrate two retrievers into the pipeline
    and merge their outputs. This two-pronged strategy capitalizes on the unique advantages
    of each retriever, resulting in more comprehensive and accurate retrieval in Retrieval-Augmented
    Generation (RAG) applications.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效地利用基于关键词和语义（密集型）检索器的优势，一种实用方法是集成两个检索器到管道中并合并它们的输出。这种双管齐下的策略利用了每个检索器的独特优势，从而在检索增强生成（RAG）应用中实现更全面和准确的检索。
- en: The process begins by employing both **keyword-based** as well as **dense**
    retrievers within the RAG pipeline. However, the challenge lies in merging the
    results obtained from these two retrievers, each of which returns ranked lists
    of results with relevance scores assigned to each document. [Figure 4.31](#fig-hybrid-retrieval-pipeline)
    illustrates the hybrid retrieval pipeline.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程首先在RAG管道中同时使用基于**关键词**和**密集型**检索器。然而，挑战在于合并这两个检索器获得的结果，每个检索器都返回带有每个文档相关性的排名列表。[图4.31](#fig-hybrid-retrieval-pipeline)展示了混合检索管道。
- en: '![hybrid retrieval pipeline](../Images/2d34018f24c0ea911e363fef249e843a.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![混合检索管道](../Images/2d34018f24c0ea911e363fef249e843a.png)'
- en: 'Figure 4.31: Hybrid retrieval pipeline'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.31：混合检索管道
- en: 'Merging the results from keyword-based and semantic retrievers can be approached
    in several ways, depending on the nature of the RAG application:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 根据RAG应用的本质，合并基于关键词和语义检索的结果可以采用多种方法：
- en: '**Concatenation:** This method involves simply appending all documents from
    both retrievers (excluding duplicates) to create the final list of results. Concatenation
    is suitable when you intend to use all retrieved documents and the order of the
    results is not crucial. This approach can be valuable in extractive question-answering
    pipelines, where you aim to extract information from multiple sources and are
    less concerned about ranking.'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**连接法**：这种方法涉及简单地将两个检索器（排除重复项）的所有文档附加在一起，以创建最终的列表。当您打算使用所有检索到的文档且结果顺序不是关键时，连接法是合适的。在提取式问答管道中，这种方法非常有价值，您旨在从多个来源提取信息，而不太关注排名。'
- en: '**Reciprocal Rank Fusion (RRF):** RRF operates with a formula that re-ranks
    documents from both retrievers, giving priority to those that appear in both results
    lists. Its purpose is to elevate the most relevant documents to the top of the
    list, thereby enhancing the overall relevance of the results. RRF is particularly
    useful when the order of the results is important or when you intend to pass on
    only a subset of results to the subsequent processing stages.'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**相互排名融合（RRF）**：RRF使用一个公式重新排名来自两个检索器的文档，优先考虑那些出现在两个结果列表中的文档。其目的是将最相关的文档提升到列表的顶部，从而增强结果的总体相关性。当结果顺序很重要或您打算仅将结果子集传递到后续处理阶段时，RRF特别有用。'
- en: '**Merging by Scoring:** In this approach, documents are ranked based on the
    scores assigned by the individual retrievers. This method is suitable when you
    aim to prioritize results from one retriever over the other. If the relevance
    scores assigned by the two retrievers are comparable and you wish to emphasize
    results from a particular retriever, this method can be employed. For instance,
    if you’re using dense retrievers from different sources that return documents
    from various document stores, this method allows you to choose one retriever’s
    output over another.'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**按评分合并**：在此方法中，文档根据各个检索器分配的评分进行排名。当您希望优先考虑来自一个检索器的结果而不是另一个检索器时，这种方法是合适的。如果两个检索器分配的相关性评分可以比较，并且您希望强调来自特定检索器的结果，则可以采用此方法。例如，如果您使用来自不同来源的密集型检索器，这些检索器从不同的文档存储中返回文档，此方法允许您选择一个检索器的输出而不是另一个。'
- en: '**Advantages of Hybrid Retrieval:**'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '**混合检索的优势**：'
- en: '**Enhanced Relevance:** Hybrid retrieval leverages the strengths of both keyword
    and semantic approaches, increasing the chances of returning highly relevant documents.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增强相关性**：混合检索利用了关键词和语义方法的优势，增加了返回高度相关文档的可能性。'
- en: '**Coverage:** It addresses scenarios where purely keyword or purely semantic
    approaches might fail, providing a broader scope of relevant documents.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**覆盖范围**：它解决了纯关键词或纯语义方法可能失败的场景，提供了更广泛的相关文档范围。'
- en: '**Resource Efficiency:** By initially narrowing the search with keyword-based
    filtering, the system conserves computational resources, making the retrieval
    process more efficient.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源效率**：通过初始的基于关键字的过滤来缩小搜索范围，系统可以节省计算资源，使检索过程更加高效。'
- en: '**Adaptability:** Hybrid retrieval allows for adaptability to different user
    queries and document types, striking a balance between precision and recall.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**适应性**：混合检索允许适应不同的用户查询和文档类型，在精确度和召回率之间取得平衡。'
- en: There are frameworks which support hybrid retrieval out of the box such as [ElasticSearch](https://www.elastic.co/blog/improving-information-retrieval-elastic-stack-hybrid),
    [Haystack](https://haystack.deepset.ai/tutorials/26_hybrid_retrieval), [Weaviate](https://weaviate.io/developers/weaviate/search/hybrid),
    and [Cohere Rerank](https://txt.cohere.com/rerank/). Let’s find out how to implement
    this approach using Haystack. The following code is from Haystack documentation,
    you can see all the implementation details [here](https://haystack.deepset.ai/tutorials/26_hybrid_retrieval).
    The documents that are used for this example are abstracts of papers from PubMed.
    You can find and download the dataset [here](https://huggingface.co/datasets/ywchoi/pubmed_abstract_3/viewer/default/test).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些框架支持开箱即用的混合检索，例如 [ElasticSearch](https://www.elastic.co/blog/improving-information-retrieval-elastic-stack-hybrid)，[Haystack](https://haystack.deepset.ai/tutorials/26_hybrid_retrieval)，[Weaviate](https://weaviate.io/developers/weaviate/search/hybrid)，和
    [Cohere Rerank](https://txt.cohere.com/rerank/)。让我们来看看如何使用Haystack实现这种方法。以下代码来自Haystack文档，你可以在这里看到所有实现细节[这里](https://haystack.deepset.ai/tutorials/26_hybrid_retrieval)。本例中使用的文档来自PubMed的论文摘要。你可以在这里找到并下载数据集[这里](https://huggingface.co/datasets/ywchoi/pubmed_abstract_3/viewer/default/test)。
- en: '[PRE0]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '***Step 1:** We load the dataset and initialize the document store (i.e. vector
    database). [Figure 4.32](#fig-haystack-initialization) shows this step.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '***步骤1**：我们加载数据集并初始化文档存储（即向量数据库）。[图4.32](#fig-haystack-initialization)展示了这一步骤。'
- en: '![Load documents and initialize document store](../Images/09596d7bb00c48a7ecc527489e579f4b.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![加载文档并初始化文档存储](../Images/09596d7bb00c48a7ecc527489e579f4b.png)'
- en: 'Figure 4.32: Load documents and initialize document store'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.32：加载文档并初始化文档存储
- en: '**Step 2:** Define the retrievers, insert the documents and embeddings into
    the document store and choose the *join document* strategy. You can see this step
    in [Figure 4.33](#fig-haystack-define-retrievers).'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2**：定义检索器，将文档和嵌入插入到文档存储中，并选择“联合文档”策略。你可以在[图4.33](#fig-haystack-define-retrievers)中看到这一步骤。'
- en: '![Define keyword and embedding based retrievers](../Images/dd1c42117f53b7c3e522b9f1d5f8326c.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![定义基于关键字和嵌入的检索器](../Images/dd1c42117f53b7c3e522b9f1d5f8326c.png)'
- en: 'Figure 4.33: Define keyword and embedding based retrievers'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.33：定义基于关键字和嵌入的检索器
- en: '**Step 3:** Create an end-to-end pipeline in Haystack and perform the hybrid
    retrieval for a query. This step is depicted in [Figure 4.34](#fig-haystack-run-retrievers).'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3**：在Haystack中创建端到端管道并执行查询的混合检索。这一步骤在[图4.34](#fig-haystack-run-retrievers)中展示。'
- en: '![Create end-to-end pipeline and run the retrievers](../Images/61770bf83b9c0cc011e2c517ecafd331.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![创建端到端管道并运行检索器](../Images/61770bf83b9c0cc011e2c517ecafd331.png)'
- en: 'Figure 4.34: Create end-to-end pipeline and run the retrievers*  *## 4.6 Query
    Rewriting for Retrieval-Augmented Large Language Models'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.34：创建端到端管道并运行检索器
- en: Query rewriting is a sophisticated technique that plays a pivotal role in enhancing
    the performance of Retrieval-Augmented Large Language Models (RAG). The fundamental
    idea behind query rewriting is to optimize and fine-tune the queries presented
    to the retrieval component of RAG systems, ultimately leading to more accurate
    and contextually relevant results.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 查询重写是一种复杂的技巧，在增强检索增强大型语言模型（RAG）性能方面发挥着关键作用。查询重写背后的基本思想是优化和微调呈现给RAG系统检索组件的查询，最终导致更准确和上下文相关的结果。
- en: 'The core concept is to transform the initial user query into an improved form
    that effectively captures the user’s intent and aligns with the document retrieval
    phase. This often involves various steps and considerations, including:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 核心概念是将初始用户查询转换为一种改进形式，这种形式能够有效地捕捉用户的意图并与文档检索阶段保持一致。这通常涉及各种步骤和考虑因素，包括：
- en: '**Expanding User Queries**: Query rewriting can involve expanding the initial
    user query by adding synonyms, related terms, or concepts that might improve the
    retrieval of relevant documents. This expansion can be based on linguistic analysis
    or external knowledge sources.'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**扩展用户查询**：查询重写可能涉及通过添加同义词、相关术语或可能改进相关文档检索的概念来扩展初始用户查询。这种扩展可以基于语言分析或外部知识源。'
- en: '**Rephrasing for Clarity**: Queries are often rewritten to improve their clarity
    and conciseness. Ambiguous or complex phrasings can be simplified to make the
    user’s intent more explicit.'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**清晰重述**：查询通常被重写以提高其清晰度和简洁性。含糊或复杂的表述可以被简化，以便更明确地表达用户的意图。'
- en: '**Contextual Adaptation**: The rewriting process may take into account the
    specific context of the documents available for retrieval. It can adapt the query
    to the characteristics of the document corpus, which is particularly valuable
    in domain-specific applications.'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**情境适应**：重写过程可能考虑可用于检索的文档的具体情境。它可以调整查询以适应文档语料库的特征，这在特定领域应用中尤其有价值。'
- en: Query rewriting is closely tied to the document retrieval phase in RAG systems.
    It contributes to better retrieval rankings, which, in turn, leads to more informative
    and contextually relevant answers generated by the language model. The goal is
    to ensure that the retrieved documents align closely with the user’s intent and
    cover a wide spectrum of relevant information.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 查询重写与RAG系统中的文档检索阶段密切相关。它有助于提高检索排名，进而导致语言模型生成的更丰富、更具有情境相关性的答案。目标是确保检索到的文档与用户的意图紧密一致，并涵盖广泛的相关信息。
- en: 4.6.1 Leveraging Large Language Models (LLMs) for Query Rewriting in RAGs
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6.1 利用大型语言模型（LLMs）在RAG中进行查询重写
- en: '**Question:** *With the advent of Large Language Models (LLMs) that have revolutionized
    natural language understanding and generation tasks, can we use them for query
    rewriting?* The answer is: *Yes*. They can help in two primary ways: query expansion
    and generating better prompts. [Figure 4.36](#fig-query-rewriting) shows how query
    rewriting works. We can use LLM to either expand (enhance) a query or generate
    multiple (sub)queries for better retreival process.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：*随着大型语言模型（LLMs）的出现，这些模型已经彻底改变了自然语言理解和生成任务，我们能否将它们用于查询重写？* 答案是：*是的*。它们可以通过两种主要方式帮助：查询扩展和生成更好的提示。[图4.36](#fig-query-rewriting)展示了查询重写的工作原理。我们可以使用LLM来扩展（增强）查询或为更好的检索过程生成多个（子）查询。'
- en: '![Query re-writing using LLMs. LLM can expand the query or create multiple
    sub-queries.](../Images/e49f5002a8117f0dd7c6208ff33375f5.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![使用LLMs进行查询重写。LLM可以扩展查询或创建多个子查询。](../Images/e49f5002a8117f0dd7c6208ff33375f5.png)'
- en: 'Figure 4.35: Query re-writing using LLMs. LLM can expand the query or create
    multiple sub-queries.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.35：使用LLMs进行查询重写。LLM可以扩展查询或创建多个子查询。
- en: 4.6.1.1 Query Expansion with LLMs
  id: totrans-241
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.6.1.1 使用LLMs进行查询扩展
- en: 'LLMs possess an extensive understanding of language and vast knowledge repositories,
    which makes them ideal for query expansion. Here’s how LLMs can aid in query expansion:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs拥有对语言的广泛理解和庞大的知识库，这使得它们非常适合查询扩展。以下是LLMs如何帮助查询扩展的说明：
- en: '**Synonym Generation:** LLMs can identify synonyms and related terms for words
    in the user’s query. By expanding the query with synonyms, it increases the chances
    of retrieving documents that may use different terminology but are contextually
    relevant.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '**同义词生成**：LLMs可以识别用户查询中单词的同义词和相关术语。通过使用同义词扩展查询，它增加了检索到可能使用不同术语但情境相关的文档的机会。'
- en: '**Example:**'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例**：'
- en: '*User Query:* “Renewable energy sources”.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '*用户查询*：“可再生能源”。'
- en: '*LLM Query Expansion:* “Renewable energy sources” -> “Green energy sources,”
    “Sustainable energy sources,” “Eco-friendly energy sources”.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '*LLM查询扩展*：“可再生能源”->“绿色能源”，“可持续能源”，“环保能源”。'
- en: By suggesting synonyms for “renewable,” the LLM broadens the query’s scope to
    retrieve documents that may use alternative terminology.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 通过建议“可再生”的同义词，LLM扩大了查询的范围，以检索可能使用替代术语的文档。
- en: '**Conceptual Expansion:** LLMs can identify concepts and entities related to
    the query. They can suggest adding relevant concepts, entities, or phrases that
    can lead to more comprehensive results. For instance, if a user queries about
    “climate change,” the LLM might suggest adding “global warming” to ensure a broader
    document retrieval.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '**概念扩展**：LLMs可以识别与查询相关的概念和实体。它们可以建议添加相关的概念、实体或短语，以获得更全面的结果。例如，如果用户查询“气候变化”，LLM可能会建议添加“全球变暖”，以确保更广泛的文档检索。'
- en: '**Example:**'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例**：'
- en: '*User Query:* “Mars exploration”.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '*用户查询*：“火星探索”。'
- en: '*LLM Query Expansion:* “Mars exploration” -> “Mars mission,” “Red planet research,”
    “Space exploration of Mars”.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '*LLM查询扩展*：“火星探索” -> “火星任务”，“红色星球研究”，“火星太空探索”。'
- en: '**Multilingual Query Expansion:** For multilingual queries, LLMs can assist
    in translating and expanding the query into multiple languages, broadening the
    search scope and retrieving documents in various languages.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '**多语言查询扩展**：对于多语言查询，LLMs可以帮助将查询翻译和扩展成多种语言，扩大搜索范围并检索各种语言的文档。'
- en: 4.6.1.2 Generating Better Prompts with LLMs
  id: totrans-253
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.6.1.2 使用LLM生成更好的提示
- en: LLMs can assist in generating more effective prompts for retrieval, especially
    when a prompt-based retrieval mechanism is employed. Here’s how LLMs contribute
    to prompt generation.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: LLM可以帮助生成更有效的检索提示，尤其是在采用基于提示的检索机制时。以下是LLM对提示生成的贡献。
- en: '**Query Refinement:** LLMs can refine a user query by making it more concise,
    unambiguous, and contextually relevant. The refined query can then serve as a
    prompt for the retrieval component, ensuring a more focused search.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '**查询优化**：LLMs可以通过使其更简洁、明确和上下文相关来优化用户查询。优化后的查询可以作为检索组件的提示，确保更集中的搜索。'
- en: '**Example:**'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例**：'
- en: '*User Query:* “How does photosynthesis work?”'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '*用户查询*：“光合作用是如何工作的？”'
- en: '*LLM-Generated Prompt:* “Explain the process of photosynthesis.”'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '*LLM生成的提示*：“解释光合作用的过程。”'
- en: '**Multi-step Prompts:** In complex queries, LLMs can generate multi-step prompts
    that guide the retrieval component through a series of sub-queries. This can help
    break down intricate requests into more manageable retrieval tasks.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '**多步提示**：在复杂查询中，LLM可以生成多步提示，引导检索组件通过一系列子查询。这有助于将复杂的请求分解成更易于管理的检索任务。'
- en: '**Example:**'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例**：'
- en: '*User Query:* “Market trends for electric cars in 2021”.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '*用户查询*：“2021年电动汽车的市场趋势”。'
- en: '*LLM-Generated Multi-Step Prompts:*'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '*LLM生成的多步提示*：'
- en: “Retrieve market trends for electric cars.”
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “检索2021年电动汽车的市场趋势。”
- en: “Filter results to focus on trends in 2021.”
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “将结果过滤以关注2021年的趋势。”
- en: The LLM generates sequential prompts to guide the retrieval component in finding
    documents related to market trends for electric cars in 2021.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: LLM生成一系列提示，引导检索组件找到与2021年电动汽车市场趋势相关的文档。
- en: '**Context-Aware Prompts:** LLMs can consider the context of the available document
    corpus and generate prompts that align with the characteristics of the documents.
    They can adapt prompts for specific domains or industries, ensuring that the retrieval
    phase retrieves contextually relevant documents.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '**上下文感知提示**：LLMs可以考虑到可用文档语料库的上下文，并生成与文档特征一致的提示。它们可以为特定领域或行业调整提示，确保检索阶段检索到上下文相关的文档。'
- en: '**Example:**'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例**：'
- en: '*User Query:* “Legal documents for the healthcare industry”.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '*用户查询*：“医疗行业的法律文件”。'
- en: '*LLM-Generated Domain-Specific Prompt:* “Retrieve legal documents relevant
    to the healthcare industry.”'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '*LLM生成的特定领域提示*：“检索与医疗行业相关的法律文件。”'
- en: Understanding the context, the LLM generates a prompt tailored to the healthcare
    industry, ensuring documents retrieved are pertinent to that domain.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 理解上下文，LLM生成针对医疗行业的定制提示，确保检索到的文档与该领域相关。
- en: Query rerwriting for RAGs is an active research area, and new approaches are
    suggested regularly. One recent research is Ma et al. ([2023](references.html#ref-ma2023query)),
    where they propose a new framework for query generation. See [here](https://arxiv.org/pdf/2305.14283.pdf)
    to learn more about their method.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: RAGs的查询重写是一个活跃的研究领域，并且经常有新的方法被提出。最近的一项研究是Ma等人（[2023](references.html#ref-ma2023query)），他们提出了一种新的查询生成框架。请参阅[这里](https://arxiv.org/pdf/2305.14283.pdf)了解更多关于他们方法的信息。
- en: 4.7 Query Routing in RAG
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.7 RAG中的查询路由
- en: Query routing in RAG, often facilitated by a router, is the process of automatically
    selecting and invoking the most suitable retrieval technique or tool for a given
    user query. It enables a dynamic, adaptive approach to choose how to retrieve
    information based on the specific requirements of each query.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: RAG中的查询路由，通常由路由器辅助，是自动选择和调用针对特定用户查询的最合适检索技术或工具的过程。它使系统能够根据每个查询的具体要求动态、自适应地选择如何检索信息。
- en: Rather than relying on a fixed retrieval method, query routing empowers the
    system to intelligently assess the user’s query and select the appropriate retrieval
    mechanism. This approach is particularly powerful in scenarios where a diverse
    range of retrieval techniques or tools can be employed to answer different types
    of queries. The following shows the generate architecture of this approach.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 与依赖于固定的检索方法不同，查询路由赋予系统智能评估用户查询并选择适当检索机制的能力。这种方法在可以采用多种检索技术或工具来回答不同类型查询的场景中特别强大。以下展示了这种方法的生成架构。
- en: '![Query re-writing using LLMs. LLM can expand the query or create multiple
    sub-queries.](../Images/795dc05fa1533d987e499ccf39ff878d.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![使用LLMs进行查询重写。LLM可以扩展查询或创建多个子查询。](../Images/795dc05fa1533d987e499ccf39ff878d.png)'
- en: 'Figure 4.36: Query router architecture'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.36：查询路由器架构
- en: '**How Query Routing Works:**'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '**查询路由工作原理**:'
- en: '**User Query Input**:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**用户查询输入**:'
- en: A user enters a query into the RAG system. This query could encompass various
    types of information needs, such as fact-based lookup, summarization, translation,
    question answering, etc.
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 用户将查询输入到RAG系统中。这个查询可能包含各种类型的信息需求，如基于事实的查找、摘要、翻译、问答等。
- en: '**Query Analysis**:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**查询分析**:'
- en: The router or the query routing component first performs an analysis of the
    user’s query. This analysis involves understanding the nature of the query, its
    intent, and the type of information required.
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 路由器或查询路由组件首先对用户的查询进行分析。这种分析涉及理解查询的性质、其意图和所需信息类型。
- en: '**Detection of Retrieval Technique Requirement**:'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**检索技术需求检测**:'
- en: Based on the analysis, the router detects the retrieval technique or tool that
    best matches the query’s requirements. This detection is often driven by heuristics,
    pre-defined rules, machine learning models, or a combination of these methods.
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于分析，路由器检测与查询需求最匹配的检索技术或工具。这种检测通常由启发式方法、预定义规则、机器学习模型或这些方法的组合驱动。
- en: '**Selection of Retrieval Technique**:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**检索技术选择**:'
- en: The router selects the most appropriate retrieval technique from a predefined
    set, which can include methods like fact-based lookup in a vector store, summarization,
    document retrieval, question answering, or translation, among others.
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 路由器从预定义的集合中选择最合适的检索技术，这可能包括在向量存储中进行基于事实的查找、摘要、文档检索、问答或翻译等方法。
- en: '**Invoke the Retrieval Component**:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**调用检索组件**:'
- en: The router then calls the relevant retrieval component or “tool” that specializes
    in the chosen retrieval technique. This component may be a vector store interface,
    a summarization tool, a translation service, or any other retrieval method.
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，路由器调用相关检索组件或“工具”，该组件专门从事所选的检索技术。这个组件可能是一个向量存储接口、一个摘要工具、一个翻译服务或任何其他检索方法。
- en: '**Information Retrieval**:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**信息检索**:'
- en: The selected retrieval component performs the necessary information retrieval
    or processing based on the chosen technique. For example, if fact-based lookup
    is required, it retrieves facts from a vector store. If summarization is needed,
    it generates a concise summary. If translation is the goal, it translates the
    content.
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选定的检索组件根据所选技术执行必要的信息检索或处理。例如，如果需要基于事实的查找，它将从向量存储中检索事实。如果需要摘要，它将生成简洁的摘要。如果目标是翻译，它将翻译内容。
- en: '**Generation of Output**:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**输出生成**:'
- en: The retrieved or processed information is then used by the generation component
    in the RAG system to compose a response, which is presented to the user.
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 检索或处理后的信息随后被RAG系统中的生成组件用于组成响应，并将其呈现给用户。
- en: 'There are multiple ways to implement query routing in Retrieval-Augmented Generation
    (RAG) systems:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在检索增强生成（RAG）系统中实现查询路由有多种方式：
- en: '**Intent Classification Model:**'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**意图分类模型**:'
- en: '**Use a Classifier**: Employ a pre-trained classifier model that can categorize
    user queries based on their intent. This classifier can have predefined categories
    such as fact-based lookup, summarization, translation, question answering, etc.
    The selected category then dictates the retrieval method to be used.'
  id: totrans-294
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用分类器**：使用一个预训练的分类器模型，可以根据用户的意图对查询进行分类。这个分类器可以包含预定义的分类，如基于事实的查找、总结、翻译、问答等。选定的类别将决定要使用的检索方法。'
- en: '**Machine Learning**: Train a custom intent classification model on labeled
    query data to predict the query’s intent. This model can be fine-tuned on specific
    intent detection tasks relevant to the RAG system.'
  id: totrans-295
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器学习**：在标记的查询数据上训练一个定制的意图分类模型，以预测查询的意图。这个模型可以在与RAG系统相关的特定意图检测任务上进行微调。'
- en: '[Figure 4.37](#fig-query-routing-zeroshot) shows how to use a zero-shot classifer
    to categorize user queries.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4.37](#fig-query-routing-zeroshot) 展示了如何使用零样本分类器对用户查询进行分类。'
- en: '![Using a zero-shot classifier to categorize and route user queries](../Images/0f026c544c013a3ddf3c07720fdd2766.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![使用零样本分类器对用户查询进行分类和路由](../Images/0f026c544c013a3ddf3c07720fdd2766.png)'
- en: 'Figure 4.37: Using a zero-shot classifier to categorize and route user queries'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.37：使用零样本分类器对用户查询进行分类和路由
- en: '**Prompt-Based Routing**:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**基于提示的路由**：'
- en: '**Leverage LLMs**: Utilize Large Language Models (LLMs), such as GPT-4 or similar
    models, to perform query classification. A prompt can be designed to guide the
    LLM in classifying the query based on its intent.'
  id: totrans-300
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**利用LLM**：利用大型语言模型（LLM），如GPT-4或类似模型，进行查询分类。可以设计一个提示来引导LLM根据查询的意图进行分类。'
- en: '**Template Prompts**: Create a set of template prompts that are specifically
    designed to categorize queries. These templates can include leading questions
    or cues to elicit the intent of the query.'
  id: totrans-301
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模板提示**：创建一组专门设计用于分类查询的模板提示。这些模板可以包括引导性问题或提示来激发查询的意图。'
- en: The code in [Figure 4.38](#fig-query-routing-prompt) shows how to use a LLM
    `text-davinci-002` for query routing.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4.38](#fig-query-routing-prompt) 中的代码展示了如何使用大型语言模型 `text-davinci-002` 进行查询路由。'
- en: '![Using a LLM classifier to categorize and route user queries](../Images/233c20c060568eba7f1f15e9488685c9.png)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![使用LLM分类器对用户查询进行分类和路由](../Images/233c20c060568eba7f1f15e9488685c9.png)'
- en: 'Figure 4.38: Using a LLM to categorize and route user queries'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.38：使用LLM对用户查询进行分类和路由
- en: '**Rule-Based Routing**:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**基于规则的路由**：'
- en: '**Rule-Based System**: Develop a rule-based system that consists of predefined
    rules or conditions to categorize queries. For example, if a query starts with
    “Translate,” it is routed to a translation retrieval method.'
  id: totrans-306
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于规则的系统**：开发一个基于规则的系统，该系统包含预定义的规则或条件来分类查询。例如，如果一个查询以“Translate”开头，它将被路由到翻译检索方法。'
- en: '**Regular Expressions**: Use regular expressions to match query patterns and
    automatically route queries based on predefined patterns, keywords, or structures.'
  id: totrans-307
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正则表达式**：使用正则表达式匹配查询模式，并根据预定义的模式、关键词或结构自动路由查询。'
- en: The choice of implementation depends on the complexity of the RAG system, the
    available resources, and the specific requirements of the application. Implementing
    a combination of these methods can provide a robust and adaptive query routing
    mechanism that enhances the overall performance of RAG systems.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 实现的选择取决于RAG系统的复杂性、可用资源以及应用程序的具体要求。实现这些方法的组合可以提供一个强大且自适应的查询路由机制，从而提高RAG系统的整体性能。
- en: Let’s take a look at an example where we use LlamaIndex framework for routing
    queries between a *summarization route* and *fact-based route*. LlamaIndex has
    a concept called `RouterQueryEngine` which accepts a set of query engines `QueryEngineTool`
    objects as input. A `QueryEngineTool` is essentially an index used for retrieving
    documents from vector database. First step is to load the documents and define
    the summary index and fact-based index, which is displayed in [Figure 4.39](#fig-query-routing-llamaindex-code1).
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个示例，其中我们使用LlamaIndex框架在*总结路由*和*基于事实的路由*之间路由查询。LlamaIndex有一个名为`RouterQueryEngine`的概念，它接受一组查询引擎`QueryEngineTool`对象作为输入。`QueryEngineTool`本质上是一个用于从向量数据库检索文档的索引。第一步是加载文档并定义总结索引和基于事实的索引，这在[图4.39](#fig-query-routing-llamaindex-code1)中显示。
- en: '![Query routing example in LlamaIndex.](../Images/5a770559802922748cdd3fe010afd86b.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![LlamaIndex中的查询路由示例。](../Images/5a770559802922748cdd3fe010afd86b.png)'
- en: 'Figure 4.39: Query routing example in LlamaIndex. First we load documents and
    create different indecies.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.39：LlamaIndex中的查询路由示例。首先我们加载文档并创建不同的索引。
- en: Then, we create `QueryEngine` objects for each index and add them to the `RouterQueryEngine`
    object. [Figure 4.40](#fig-query-routing-llamaindex-code2) shows this step.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们为每个索引创建`QueryEngine`对象，并将它们添加到`RouterQueryEngine`对象中。[图4.40](#fig-query-routing-llamaindex-code2)展示了这一步骤。
- en: '![Define `QueryEngine` and `RouterQueryEngine` objects, and run the engine
    for user queries.](../Images/de9885200cc4f36072c4f40215721c79.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![定义`QueryEngine`和`RouterQueryEngine`对象，并运行用户查询的引擎。](../Images/de9885200cc4f36072c4f40215721c79.png)'
- en: 'Figure 4.40: Define `QueryEngine` and `RouterQueryEngine` objects, and run
    the engine for user queries.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.40：定义`QueryEngine`和`RouterQueryEngine`对象，并运行用户查询的引擎。
- en: By dynamically routing queries to the most suitable retrieval techniques, query
    routing enhances RAG by ensuring that the system adapts to the specific requirements
    of each user query, providing more accurate and context-aware responses. It overcomes
    the challenge of needing to know in advance which retrieval technique to apply
    and optimizes the retrieval process for different types of information needs.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 通过动态路由查询到最合适的检索技术，查询路由通过确保系统适应每个用户查询的具体要求，提供更准确和上下文感知的响应来增强RAG。它克服了需要事先知道应用哪种检索技术的挑战，并优化了不同类型信息需求的检索过程。
- en: 4.8 Leveraging User History to Enhance RAG Performance
  id: totrans-316
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.8 利用用户历史记录提升RAG性能
- en: Retrieval-Augmented Generation (RAG) systems have gained prominence in natural
    language processing applications due to their ability to combine the strengths
    of information retrieval and language generation. However, RAG applications often
    involve significant computational and financial costs due to the necessity of
    embeddings, retrievals and even response generation. When dealing with recurrent
    user queries or similar questions, these costs can be optimized by leveraging
    the memory of previously asked questions.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 由于能够结合信息检索和语言生成的优势，检索增强生成（RAG）系统在自然语言处理应用中获得了显著的关注。然而，RAG应用往往涉及大量的计算和财务成本，这是由于需要嵌入、检索甚至响应生成的必要性。当处理重复的用户查询或类似问题时，可以通过利用先前询问的问题的内存来优化这些成本。
- en: 4.8.1 Challenge
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8.1 挑战
- en: In many real-world applications, users tend to ask similar or nearly identical
    questions repeatedly. For instance, in a customer support chatbot, users may have
    common queries related to product information or troubleshooting procedures. While
    the RAG framework excels at generating relevant responses based on retrieval,
    it may not be efficient to re-embed and retrieve information for the same questions
    each time they are asked. This is where the concept of leveraging user history
    comes into play.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多现实世界的应用中，用户往往反复提出相似或几乎相同的问题。例如，在一个客户支持聊天机器人中，用户可能有与产品信息或故障排除程序相关的常见查询。虽然RAG框架在基于检索生成相关响应方面表现出色，但每次询问相同问题时重新嵌入和检索信息可能并不高效。这就是利用用户历史记录概念发挥作用的地方。
- en: 4.8.2 How User History Enhances RAG Performance
  id: totrans-320
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8.2 用户历史记录如何提升RAG性能
- en: 'Leveraging user history to enhance RAG performance involves building a memory
    or cache of previously asked questions and their corresponding responses. When
    a user query is received, instead of immediately triggering an embedding and retrieval
    process, the system can check the user history to see if it has encountered a
    similar or identical query before. This approach offers several advantages:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 利用用户历史记录来提升RAG性能涉及构建一个先前询问的问题及其相应响应的内存或缓存。当收到用户查询时，系统不是立即触发嵌入和检索过程，而是可以检查用户历史记录以查看是否之前遇到过相似或相同的查询。这种方法提供了一些优势：
- en: '**Faster Response Times (i.e. reduced latency):** By comparing the new query
    to the user history, the system can identify duplicate or closely related questions.
    In such cases, it can bypass the embedding and retrieval steps, significantly
    reducing response times.'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**更快的响应时间（即减少延迟）**：通过将新查询与用户历史记录进行比较，系统可以识别重复或密切相关的问题。在这种情况下，它可以跳过嵌入和检索步骤，显著减少响应时间。'
- en: '**Cost Reduction:** Skipping the resource-intensive retrieval process for recurrent
    queries leads to a notable reduction in computational and financial costs, as
    the need for additional API calls or data processing diminishes.'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**成本降低**：跳过重复查询的资源密集型检索过程，导致计算和财务成本显著降低，因为额外的API调用或数据处理需求减少。'
- en: '**Consistency and Accuracy:** Responses to repeated questions can remain consistent
    and accurate over time. The use of cached responses from the user history ensures
    that users receive reliable information without relying on new retrievals. Additionally,
    by keeping track of user history, RAG can learn to better understand the user’s
    intent and context. This can lead to more accurate answers to questions, even
    if the questions are not perfectly phrased.'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**一致性和准确性：** 对重复问题的回答可以随着时间的推移保持一致和准确。使用用户历史记录中的缓存响应确保用户接收可靠的信息，而无需依赖新的检索。此外，通过跟踪用户历史记录，RAG可以更好地理解用户的意图和上下文。这可能导致对问题的更准确回答，即使问题表述并不完美。'
- en: 4.8.3 How Memory/User History Works
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8.3 内存/用户历史如何工作
- en: 'A simple RAG memory can be implemented as follows: We need to maintain a log
    of user queries and their corresponding answers. Therefore, we can define the
    memory as a key-value store, where the keys are questions and the values are answers.
    When a user asks a question, RAG first checks its memory to see if it has already
    answered a similar question. If it has, then it simply retrieves the answer from
    its memory and returns it to the user. If it has not already answered a similar
    question, then it performs the embedding and retrieval process as usual.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的RAG内存可以这样实现：我们需要维护用户查询及其对应答案的日志。因此，我们可以将内存定义为键值存储，其中键是问题，值是答案。当用户提问时，RAG首先检查其内存以查看是否已经回答了类似的问题。如果是，它就简单地从其内存中检索答案并返回给用户。如果没有回答过类似的问题，那么它就按常规执行嵌入和检索过程。
- en: We need to have a mechanism that constantly updates the RAG’s memory with new
    questions and answers. As a user asks more questions, RAG’s memory grows and it
    becomes better able to answer future questions accurately and efficiently.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个机制来不断更新RAG的内存以包含新的问题和答案。随着用户提出更多问题，RAG的内存增长，它变得能够更准确、更有效地回答未来的问题。
- en: '**Implementation:** The snippet code in [Figure 4.41](#fig-rag-memory) shows
    a very basic implementation of memory for RAG.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '**实现：** [图4.41](#fig-rag-memory)中的代码片段展示了RAG内存的一个非常基本的实现。'
- en: '![A basic key-value implementation of memory for RAG](../Images/0601083f1df18e40c57d68025a165f09.png)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![RAG内存的基本键值实现](../Images/0601083f1df18e40c57d68025a165f09.png)'
- en: 'Figure 4.41: A basic key-value implementation of memory for RAG.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.41：RAG内存的基本键值实现。
- en: While a simple in-memory dictionary, as shown in the previous example, can be
    effective for maintaining recent user history, it might not be sufficient for
    handling a large-scale conversation history or long-term memory. To address these
    challenges, RAG systems can benefit from using a vector database for memory.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然如前例所示的一个简单的内存字典对于维护最近用户历史可能有效，但它可能不足以处理大规模对话历史或长期记忆。为了解决这些挑战，RAG系统可以从使用向量数据库作为内存中受益。
- en: 'The implementation of a vector database for memory in a RAG system involves
    several key steps:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 在RAG系统中实现用于内存的向量数据库涉及几个关键步骤：
- en: '**Data Ingestion:** Store historical question-answer pairs along with their
    corresponding embeddings in the vector database. This process typically involves
    a batch or incremental data ingestion pipeline.'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据摄取：** 将历史问答对及其对应的嵌入存储在向量数据库中。此过程通常涉及批量或增量数据摄取管道。'
- en: '**Retrieval:** When a user poses a question, retrieve the most relevant historical
    answers by calculating the similarity between the question’s embedding and the
    embeddings in the vector database.'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**检索：** 当用户提出问题时，通过计算问题的嵌入与向量数据库中的嵌入之间的相似度来检索最相关的历史答案。'
- en: '**Updating Memory:** Periodically update the vector database with new question-answer
    pairs and their embeddings to keep the memory up to date.'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**更新内存：** 定期用新的问答对及其嵌入更新向量数据库，以保持内存的更新。'
- en: '**Caching:** Implement a caching mechanism to enhance retrieval speed by temporarily
    storing frequently accessed data in memory.'
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**缓存：** 实现一个缓存机制，通过在内存中临时存储频繁访问的数据来提高检索速度。'
- en: '**Query Optimization:** Use efficient indexing and search algorithms to optimize
    the retrieval process, reducing query response times.'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**查询优化：** 使用高效的索引和搜索算法来优化检索过程，减少查询响应时间。'
- en: Incorporating a vector database for memory in a RAG system enhances its ability
    to provide contextually relevant and coherent responses by efficiently managing
    and retrieving historical information. This approach is particularly valuable
    in scenarios where extensive conversation histories or long-term memory are essential
    for the application’s success.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在 RAG 系统中引入向量数据库作为记忆，通过有效地管理和检索历史信息，增强了其提供上下文相关和连贯响应的能力。这种方法在需要大量对话历史或长期记忆以支持应用成功的情况下尤其有价值。
- en: As we wrap up our exploration of advanced RAG systems in this Chapter, we are
    on the cusp of a new frontier. In Chapter 5 - “Observability Tools for RAG,” we
    will discuss various observability tools tailored for RAG systems. We will explore
    their integration with LlamaIndex, including Weights & Biases, Phoenix, and HoneyHive.
    These tools will not only help us monitor and evaluate the performance of our
    RAG systems but also provide valuable insights for continuous improvement.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束我们对高级 RAG 系统的探索之际，我们正站在一个新的前沿。在第 5 章——“RAG 的可观察性工具”中，我们将讨论为 RAG 系统量身定制的各种可观察性工具。我们将探讨它们与
    LlamaIndex 的集成，包括 Weights & Biases、Phoenix 和 HoneyHive。这些工具不仅可以帮助我们监控和评估我们的 RAG
    系统的性能，还能为持续改进提供宝贵的见解。
- en: 'Liu, Nelson F, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua,
    Fabio Petroni, and Percy Liang. 2023\. “Lost in the Middle: How Language Models
    Use Long Contexts.” *arXiv Preprint arXiv:2307.03172*.Liu, Ye, Kazuma Hashimoto,
    Yingbo Zhou, Semih Yavuz, Caiming Xiong, and Philip S Yu. 2021\. “Dense Hierarchical
    Retrieval for Open-Domain Question Answering.” *arXiv Preprint arXiv:2110.15439*.Ma,
    Xinbei, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023\. “Query Rewriting
    for Retrieval-Augmented Large Language Models.” *arXiv Preprint arXiv:2305.14283*.Roeder,
    Luke Metz, Geoffrey, and Durk Kingma. 2021\. “On Linear Identifiability of Learned
    Representations.” *arXiv Preprint arXiv:2007.00810*.Zhao, Wayne Xin, Jing Liu,
    Ruiyang Ren, and Ji-Rong Wen. 2022\. “Dense Text Retrieval Based on Pretrained
    Language Models: A Survey.” *arXiv Preprint arXiv:2211.14876*.Zheng, Lianmin,
    Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
    et al. 2023\. “Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena.” *arXiv
    Preprint arXiv:2306.05685*.*'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 刘，Nelson F，林，Kevin，休伊特，John，帕兰贾佩，Ashwin，贝维拉卡，Michele，佩特罗尼，Fabio，梁，Percy。2023。“迷失在中间：语言模型如何使用长上下文。”*arXiv
    预印本 arXiv:2307.03172*。刘，叶，ハシモト，カズマ，周，英波，亚武兹，Semih，熊，蔡明，于，Philip S。2021。“开放域问答的密集分层检索。”*arXiv
    预印本 arXiv:2110.15439*。马，新北，公，叶云，何，彭程，赵，海，端，南。2023。“检索增强大型语言模型的查询重写。”*arXiv 预印本
    arXiv:2305.14283*。罗德，Luke Metz，杰弗里，金卡玛，Durk。2021。“学习表示的线性可识别性。”*arXiv 预印本 arXiv:2007.00810*。赵，韦恩，辛，刘，刘，瑞阳，任，文，吉荣。2022。“基于预训练语言模型的密集文本检索：综述。”*arXiv
    预印本 arXiv:2211.14876*。郑，连民，江，韦林，生，应，庄，三元，吴，张浩，庄，永豪，林，子。2023。“使用 MT-Bench 和聊天机器人竞技场评估
    LLM-as-a-Judge。”*arXiv 预印本 arXiv:2306.05685*。
