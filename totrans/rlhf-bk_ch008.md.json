["```py\n[](#cb1-1)# Step 1: sample (or otherwise generate) a sequence from your policy\n[](#cb1-2)generated_tokens = model.generate(inputs)\n[](#cb1-3)\n[](#cb1-4)# Step 2: score that generated sequence under both models\n[](#cb1-5)#    for autoregressive LMs, you usually do:\n[](#cb1-6)#      inputs_for_scoring = generated_tokens[:, :-1]\n[](#cb1-7)#      labels           = generated_tokens[:, 1:]\n[](#cb1-8)logits       = model.forward(generated_tokens[:, :-1]).logits\n[](#cb1-9)ref_logits   = ref_model.forward(generated_tokens[:, :-1]).logits\n[](#cb1-10)\n[](#cb1-11)# convert to log-probs, then align labels to index into the logits\n[](#cb1-12)logprobs     = F.log_softmax(logits, dim=-1)\n[](#cb1-13)ref_logprobs = F.log_softmax(ref_logits, dim=-1)\n[](#cb1-14)\n[](#cb1-15)# gather the log-probs of the actual next tokens\n[](#cb1-16)token_logprobs     = logprobs.gather(-1, generated_tokens[:, 1:].unsqueeze(-1)).squeeze(-1)\n[](#cb1-17)ref_token_logprobs = ref_logprobs.gather(-1, generated_tokens[:, 1:].unsqueeze(-1)).squeeze(-1)\n[](#cb1-18)\n[](#cb1-19)# now you can sum (or average) those to get the sequence log-prob,\n[](#cb1-20)# and compute KL:\n[](#cb1-21)seq_logprob     = token_logprobs.sum(dim=-1)\n[](#cb1-22)ref_seq_logprob = ref_token_logprobs.sum(dim=-1)\n[](#cb1-23)\n[](#cb1-24)kl_approx = seq_logprob - ref_seq_logprob\n[](#cb1-25)kl_full   = F.kl_div(ref_logprobs, logprobs, reduction='batchmean')\n```"]