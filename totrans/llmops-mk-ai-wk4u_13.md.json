["```py\n`# Install required packages (they may already be present in your environment) # !pip install langchain dotenv  import  os from  dotenv  import load_dotenv, find_dotenv  # Load environment variables from .env load_dotenv(find_dotenv())  # Fetch the OpenAI API key from the environment openai_api_key = os.environ['OPENAI_API_KEY']` \n```", "```py\n`from  langchain.document_loaders  import PyPDFLoader import  re from  collections  import Counter  # Initialize PDF loader with the path to your PDF document # Replace with your own file path or use a URL-based loader for remote PDFs pdf_loader = PyPDFLoader(\"path/to/your/document.pdf\")  # Example: \"lecture.pdf\"  # Load the document pages document_pages = pdf_loader.load()  # Clean and tokenize def  clean_and_tokenize(text):     # Remove nonâ€‘alphabetic characters and split into words     words = re.findall(r'\\b[a-z]+\\b', text.lower())     return words  word_frequencies = Counter()  for page in document_pages:     if page.page_content.strip():         words = clean_and_tokenize(page.page_content)         word_frequencies.update(words)     else:         print(f\"Empty page found at index {document_pages.index(page)}\")  print(\"Most frequent words:\") for word, freq in word_frequencies.most_common(10):     print(f\"{word}: {freq}\")  # Inspect metadata of the first page first_page_metadata = document_pages[0].metadata print(\"\\nFirst page metadata:\") print(first_page_metadata)  # Optionally save cleaned text to a file with open(\"cleaned_lecture_series_lecture01.txt\", \"w\") as text_file:     for page in document_pages:         if page.page_content.strip():             cleaned_text = ' '.join(clean_and_tokenize(page.page_content))             text_file.write(cleaned_text + \"\\n\")` \n```", "```py\n`from  langchain.document_loaders.generic  import GenericLoader from  langchain.document_loaders.parsers  import OpenAIWhisperParser from  langchain.document_loaders.blob_loaders.youtube_audio  import YoutubeAudioLoader from  nltk.tokenize  import sent_tokenize from  textblob  import TextBlob import  os  import  nltk nltk.download('punkt')  video_url = \"https://www.youtube.com/watch?v=example_video_id\" audio_save_directory = \"docs/youtube/\" os.makedirs(audio_save_directory, exist_ok=True)  youtube_loader = GenericLoader(     YoutubeAudioLoader([video_url], audio_save_directory),     OpenAIWhisperParser() )  youtube_documents = youtube_loader.load()  transcribed_text = youtube_documents[0].page_content[:500] print(transcribed_text)  sentences = sent_tokenize(transcribed_text)  print(\"\\nFirst 5 sentences:\") for sentence in sentences[:5]:     print(sentence)  sentiment = TextBlob(transcribed_text).sentiment print(\"\\nSentiment:\") print(f\"Polarity: {sentiment.polarity}, Subjectivity: {sentiment.subjectivity}\")  # Polarity: float in [-1.0, 1.0] (negative to positive) # Subjectivity: float in [0.0, 1.0] (objective to subjective)` \n```", "```py\n`from  langchain.document_loaders  import WebBaseLoader from  bs4  import BeautifulSoup from  nltk.tokenize  import sent_tokenize from  nltk.corpus  import stopwords from  nltk.probability  import FreqDist from  nltk  import download download('punkt') download('stopwords')  web_loader = WebBaseLoader(\"https://example.com/path/to/document\") web_documents = web_loader.load()  soup = BeautifulSoup(web_documents[0].page_content, 'html.parser')  for script_or_style in soup([\"script\", \"style\"]):     script_or_style.decompose()  clean_text = ' '.join(soup.stripped_strings) print(clean_text[:500])  links = [(a.text, a['href']) for a in soup.find_all('a', href=True)] print(\"\\nExtracted links:\") for text, href in links[:5]:     print(f\"{text}: {href}\")  headings = [h1.text for h1 in soup.find_all('h1')] print(\"\\nHeadings found:\") for heading in headings:     print(heading)  sentences = sent_tokenize(clean_text) stop_words = set(stopwords.words(\"english\")) filtered_sentences = [' '.join([w for w in s.split() if w.lower() not in stop_words]) for s in sentences]  word_freq = FreqDist(w.lower() for s in filtered_sentences for w in s.split())  print(\"\\nMost frequent words:\") for word, frequency in word_freq.most_common(5):     print(f\"{word}: {frequency}\")  print(\"\\nContent summary:\") for sentence in sentences[:5]:     print(sentence)` \n```", "```py\n`from  langchain.document_loaders  import NotionDirectoryLoader import  markdown from  bs4  import BeautifulSoup import  pandas  as  pd  notion_directory = \"docs/Notion_DB\" notion_loader = NotionDirectoryLoader(notion_directory) notion_documents = notion_loader.load()  print(notion_documents[0].page_content[:200]) print(notion_documents[0].metadata)  html_content = [markdown.markdown(doc.page_content) for doc in notion_documents]  parsed_data = [] for content in html_content:     soup = BeautifulSoup(content, 'html.parser')     headings = [heading.text for heading in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])]     links = [(a.text, a['href']) for a in soup.find_all('a', href=True)]     parsed_data.append({'headings': headings, 'links': links})  df = pd.DataFrame({     'metadata': [doc.metadata for doc in notion_documents],     'parsed_content': parsed_data })  keyword = 'Project' filtered_docs = df[df['metadata'].apply(lambda x: keyword.lower() in x.get('title', '').lower())]  print(\"\\nDocuments with the keyword in the title:\") print(filtered_docs)  # Example category summary (if categories exist in metadata) if 'category' in df['metadata'].iloc[0]:     category_counts = df['metadata'].apply(lambda x: x['category']).value_counts()     print(\"\\nDocuments by category:\")     print(category_counts)` \n```"]