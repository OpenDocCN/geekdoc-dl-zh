- en: Tool Use & Function Calling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Language models using tools is a natural way to expand their capabilities, especially
    for high-precision tasks where external tools contain the information or for agents
    that need to interact with complex web systems. These can be thought of in a few
    strategies where tool use is the general category. An AI model uses any external
    tools by outputting special tokens to trigger a certain endpoint. These can be
    anything from highly specific tools, such as functions that return the weather
    at a specific place, to code interpreters or search engines that act as fundamental
    building blocks of complex behaviors. This chapter provides an overview of the
    origins of tool-use in modern language models, its fundamentals and formatting,
    and current trade-offs in utilizing tools well in leading models.
  prefs: []
  type: TYPE_NORMAL
- en: The exact origin of the term “tool use” is not clear, but the origins of the
    idea far predates the post ChatGPT world where RLHF proliferated. Early examples
    circa 2015 attempted to build systems predating modern language models, such as
    Neural Programmer-Interpreters (NPI) [[290]](ch021.xhtml#ref-reed2015neural),
    “a recurrent and compositional neural network that learns to represent and execute
    programs.” As language models became more popular, many subfields were using integrations
    with external capabilities to boost performance. To obtain information outside
    of just the weights many used retrieval augmented generation [[291]](ch021.xhtml#ref-lewis2020retrieval)
    or web browsing [[4]](ch021.xhtml#ref-nakano2021webgpt). Soon after, others were
    exploring language models integrated with programs [[292]](ch021.xhtml#ref-gao2023pal)
    or tools [[293]](ch021.xhtml#ref-parisi2022talm).
  prefs: []
  type: TYPE_NORMAL
- en: As the field matured, these models gained more complex abilities in addition
    to the vast improvements to the underlying language modeling. For example, ToolFormer
    could use “a calculator, a Q&A system, two different search engines, a translation
    system, and a calendar” [[294]](ch021.xhtml#ref-schick2023toolformerlanguagemodelsteach).
    Soon after, Gorilla was trained to use 1645 APIs (from PyTorch Hub, TensorFlow
    Hub v2, and HuggingFace) and its evaluation APIBench became a foundation of the
    popular Berkeley Function Calling Leaderboard [[295]](ch021.xhtml#ref-patil2023gorilla).
    Since these early models, the diversity of actions called has grown substantially.
  prefs: []
  type: TYPE_NORMAL
- en: Tool-use models are now deeply intertwined with regular language model interactions.
    Model Context Protocol (MCP) emerged as a common formatting used to connect language
    models to external data sources (or tools) [[296]](ch021.xhtml#ref-anthropic_mcp_2024).
    With stronger models and better formats, tool-use language models are used in
    many situations, including productivity copilots within popular applications such
    as Microsoft Office or Google Workspace, scientific domains [[297]](ch021.xhtml#ref-bran2023chemcrow),
    medical domains [[298]](ch021.xhtml#ref-li2024mmedagent), coding agents [[299]](ch021.xhtml#ref-zhang2024codeagent)
    such as Claude Code or Cursor, integrations with databases, and many other autonomous
    workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Interweaving Tool Calls in Generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Function calling agents are presented data very similarly to other post-training
    stages. The addition is the content in the system prompt that instructs the model
    what tools it has available. An example formatted data point with the system prompt
    and tools available in JSON format is shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: While the language model is generating, if following the above example, it would
    generate the tokens `search_torrents("Star Wars")` to search for Star Wars. This
    is often encoded inside special formatting tokens, and then the next tokens inserted
    into the sequence will contain the tool outputs. With this, models can learn to
    accomplish more challenging tasks than many simple standalone models.
  prefs: []
  type: TYPE_NORMAL
- en: A popular form of tool use is code-execution, allowing the model to get precise
    answers to complex logic or mathematics problems. For example, code-execution
    within a language model execution can occur during the thinking tokens of a reasoning
    model. As with function calling, there are tags first for the code to execute
    (generated by the model) and then a separate tag for output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Multi-step Tool Reasoning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'OpenAI’s o3 model represented a substantial step-change in how multi-step tool-use
    can be integrated with language models. This behavior is related to research trends
    much earlier in the community. For example, ReAct [[300]](ch021.xhtml#ref-yao2023react),
    showcased how actions and reasoning can be interleaved into one model generation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper, we explore the use of LLMs to generate both reasoning traces
    and task-specific actions in an interleaved manner, allowing for greater synergy
    between the two: reasoning traces help the model induce, track, and update action
    plans as well as handle exceptions, while actions allow it to interface with and
    gather additional information from external sources such as knowledge bases or
    environments.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: With the solidification of tool-use capabilities and the take-off of reasoning
    models, multi-turn tool-use has grown into an exciting area of research [[286]](ch021.xhtml#ref-wang2025ragenunderstandingselfevolutionllm).
  prefs: []
  type: TYPE_NORMAL
- en: Model Context Protocol (MCP)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Model Context Protocol (MCP) is a standard for connecting language models to
    external data sources and information systems [[296]](ch021.xhtml#ref-anthropic_mcp_2024).
    Rather than focusing on specific tool call formatting per external system, MCP
    enables models to access rich contextual information through a standardized protocol.
  prefs: []
  type: TYPE_NORMAL
- en: 'MCP is a simple addition on top of the tool-use content in this chapter – it
    is how applications pass context (data + actions) to language models in a predictable
    JSON schema. MCP servers that the models interact with have core primitives: resources
    (read-only data blobs), prompts (templated messages/workflows), and tools (functions
    the model can call). With this, the MCP architecture can be summarized as:'
  prefs: []
  type: TYPE_NORMAL
- en: MCP servers wrap a specific data source or capability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MCP clients (e.g., Claude Desktop, IDE plug-ins) aggregate one or more servers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hosts, e.g. Claude or ChatGPT applications, provide the user/LLM interface;
    switching model vendors or back-end tools only means swapping the client in the
    middle.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MCP enables developers of tool-use models to use the same infrastructure to
    attach their servers or clients to different models, and at the same time models
    have a predictable format they can use to integrate external components. These
    together make for a far more predictable development environment for tool-use
    models in real-world domains.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are multiple formatting and masking decisions when implementing a tool-use
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Python vs. JSON formatting**: In this chapter, we included examples that
    format tool use as both JSON data-structures and Python code. Models tend to select
    one structure, different providers across the industry use different formats.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Masking tool outputs**: An important detail when training tool-use models
    is that the tokens in the tool output are masked from the model’s training loss.
    This ensures the model is not learning to predict the output of the system that
    it does not directly generate in use (similar to prompt masking for other post-training
    stages).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-turn formatting for tool invocations**: It is common practice when
    implementing tool-calling models to add more structure to the dataloading format.
    Standard practice for post-training datasets is a list of messages alternating
    between user and assistant (and often a system message). The overall structure
    is the same for tool-use, but the turns of the model are split into subsections
    of content delimited by each tool call. An example is below.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Tokenization and message format details**: Tool calls in OpenAI messages
    format often undergo tokenization through chat templates (the code for controlling
    format of messages sent to the model), converting structured JSON representations
    into raw token streams. This process varies across model architectures—some use
    special tokens to demarcate tool calls, while others maintain structured formatting
    within the token stream itself. [Chat template playgrounds](https://huggingface.co/spaces/huggingfacejs/chat-template-playground?modelId=Qwen/Qwen3-8B)
    provides an interactive environment to explore how different models convert message
    formats to token streams.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reasoning token continuity**: As reasoning models have emerged, with their
    separate token stream of “reasoning” before an answer, different implementations
    exist for how they’re handled with tool-use in the loop. Some models preserve
    reasoning tokens between tool-calling steps within a single turn, maintaining
    context across multiple tool invocations. However, these tokens are typically
    erased between turns to minimize serving cost.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**API formatting across providers** (As of July 2025): Different providers
    use conceptually similar but technically distinct formats. OpenAI uses `tool_calls`
    arrays with unique IDs, Anthropic employs detailed `input_schema` specifications
    with `<thinking>` tags, and Gemini offers function calling modes (AUTO/ANY/NONE).
    When using these models via an API, the tools available are defined in a JSON
    format and then the tool outputs in the model response are stored in a separate
    field from the standard “tokens generated.” For another example, the open-source
    vLLM inference codebase implements extensive parsing logic supporting multiple
    tool calling modes and model-specific parsers, providing insights into lower-level
    implementation considerations [[301]](ch021.xhtml#ref-kwon2023efficient).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
