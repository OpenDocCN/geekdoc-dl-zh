<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><h2 class="h2" id="ch04"><span class="ash">4</span></h2>
<h2 class="h2a">Optimizing LLMs with Customized Fine-Tuning</h2>
<h3 class="h3" id="ch04lev1sec1">Introduction</h3>
<p>So far, we’ve exclusively used LLMs, both open and closed sourced, as they are off the shelf. We were relying on the power of the Transformer’s attention mechanisms and their speed of computation to perform pretty complex problems with relative ease. As you can probably guess, that isn’t always enough.</p>
<p>In this chapter, we will delve into the world of fine-tuning Large Language Models (LLMs) to unlock their full potential. Fine-tuning updates off the shelf models and empowers them to achieve higher quality results, leads to token savings, and often lower latency requests. While GPT-like LLMs’ pre-training on extensive text data enables impressive few-shot learning capabilities, fine-tuning takes it a step further by refining the model on a multitude of examples, resulting in superior performance across various tasks.</p>
<p>Running inference with fine-tuned models can be extremely cost-effective in the long run particularly when working with smaller models. For instance, a fine-tuned Ada model from OpenAI (only 350M parameters) costs only $0.0016 per 1k tokens, while ChatGPT (1.5B parameters) costs $0.002, and Davinci (175B parameters) costs $0.002. Over time, the cost of using a fine tuned model is much more attractive as shown in <a href="ch04.html#ch04fig01">Figure 4.1</a>.</p>
<div class="group">
<div class="image" id="ch04fig01"><img src="graphics/04fig01.jpg" alt="Images" width="745" height="466"/></div>
<p class="fig-caption"><strong>Figure 4.1</strong> <em>Assuming only 1,000 classifications a day and a relatively liberal prompt ratio (150 tokens (for few-shot examples, instructions,other) for DaVinci or ChatGPT for every 40 tokens), the cost of a fine-tuned model, even with an up-front cost, almost always wins the day overall cost-wise. Note that this does not take into the cost of fine-tuning a model which we will begin to explore later in this chapter.</em></p>
</div>
<p>My goal in this chapter is to guide you through the fine-tuning process, beginning with the preparation of training data, strategies for training a new or existing fine-tuned model, and a discussion of how to incorporate your fine-tuned model into real-world applications. This is a big topic and therefore we will have to assume some big pieces are being done behind the scenes like data labeling. Labeling data can be a huge expense in many cases of complex and specific tasks but for now, we will assume we can rely on the labels in our data for the most part. For more information on how to handle cases like these, feel free to check out some of my other content on feature engineering and label cleaning!</p>
<p>By understanding the nuances of fine-tuning and mastering its techniques, you will be well-equipped to harness the power of LLMs and create tailored solutions for your specific needs.</p>
<h3 class="h3" id="ch04lev1sec2">Transfer Learning and Fine-Tuning: A Primer</h3>
<p>Fine-tuning hinges on the idea of transfer learning. <strong>Transfer learning</strong> is a technique that leverages pre-trained models to build upon existing knowledge for new tasks or domains. In the case of LLMs, this involves utilizing the pre-training to transfer general language understanding, including grammar and general knowledge, to specific domain-specific tasks. However, the pre-training may not be sufficient to understand the nuances of certain closed or specialized topics, such as a company's legal structure or guidelines.</p>
<p><strong>Fine-tuning</strong> is a specific form of transfer learning that adjusts the parameters of a pre-trained model to better suit a “downstream” target task. Through fine-tuning, LLMs can learn from custom examples and become more effective at generating relevant and accurate responses.</p>
<h4 class="h4" id="ch04lev2sec1">The Fine-Tuning Process Explained</h4>
<p>Fine-tuning a deep learning model involves updating the model's parameters to improve its performance on a specific task or dataset.</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> <strong>Training set:</strong> A collection of labeled examples used to train the model. The model learns to recognize patterns and relationships in the data by adjusting its parameters based on the training examples.</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> <strong>Validation set:</strong> A separate collection of labeled examples used to evaluate the model's performance during training.</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> <strong>Test set:</strong> A third collection of labeled examples that is separate from both the training and validation sets. It is used to evaluate the final performance of the model after the training and fine-tuning processes are complete. The test set provides a final, unbiased estimate of the model's ability to generalize to new, unseen data.</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> <strong>Loss function:</strong> The loss function quantifies the difference between the model's predictions and the actual target values. It serves as a metric of error to evaluate the model's performance and guide the optimization process. During training, the goal is to minimize the loss function to achieve better predictions.</p>
<p>The process of fine-tuning can be broken down into a few steps:</p>
<p class="numbera">1. <strong>Collecting Labeled data:</strong> The first step in fine-tuning is to gather our training, validation, and testing datasets of labeled examples relevant to the target task or domain. Labeled data serves as a guide for the model to learn the task-specific patterns and relationships. For example, if the goal is to fine-tune a model for sentiment classification (our first example), the dataset should contain text examples along with their respective sentiment labels, such as positive, negative, or neutral.</p>
<p class="numbera">2. <strong>Hyperparameter selection:</strong> Fine-tuning involves adjusting hyperparameters that influence the learning process. These include hyperparameters like learning rate, batch size, and the number of epochs. The learning rate determines the step size of the model's weight updates, while the batch size refers to the number of training examples used in a single update. The number of epochs denotes how many times the model will iterate over the entire training dataset. Properly setting these hyperparameters can significantly impact the model's performance and help prevent issues such as overfitting - when a model learns the noise in the training data more than the signals - or underfitting - when a model fails to capture the underlying structure of the data.</p>
<p class="numbera">3. <strong>Model adaptation:</strong> Once the labeled data and hyperparameters are set, the model may have to be adapted to the target task.This involves modifying the model's architecture, such as adding custom layers or changing the output structure, to better suit the target task. For example, BERT’s architecture cannot perform sequence classification as is but they can be modified very slightly to achieve the task. In our case study, we will not need to deal with that because OpenAI will deal with it for us. We will, however, have to deal with this issue in a later chapter.</p>
<p class="numbera">4. <strong>Evaluation and iteration:</strong> After the fine-tuning process is over, we have to evaluate the model's performance on a separate holdout validation set to ensure that it generalizes well to unseen data. Performance metrics such as accuracy, F1-score, or Mean Absolute Error (MAE) can be used, depending on the task. If the performance is not satisfactory, adjustments to the hyperparameters or dataset may be necessary, followed by retraining the model.</p>
<p class="numbera">5. <strong>Model implementation and further training:</strong> Once the model is fine-tuned and we are happy with performance, we need to integrate it with existing infrastructures in a way that can handle any errors and collect feedback from users so we can add to our total dataset to run the process over again in the future</p>
<p>This process is outlined in <a href="ch04.html#ch04fig02">Figure 4.2</a>.</p>
<div class="group">
<div class="image" id="ch04fig02"><img src="graphics/04fig02.jpg" alt="Images" width="748" height="392"/></div>
<p class="fig-caption"><strong>Figure 4.2</strong> <em>The fine-tuning process visualized. A dataset is broken up into training, validation, and testing tests. The training set is used to update the model’s weights and evaluate while the validation set is used to evaluate during training. The final model is then tested against the testing set and evaluated against a set of criteria. If the model passes our test, it is used in production and monitored for further iterations.</em></p>
</div>
<p>This process may require several iterations and careful consideration of hyperparameters, data quality, and model architecture to achieve the desired results.</p>
<h4 class="h4" id="ch04lev2sec2">Closed-Source Pre-trained Models as a Foundation</h4>
<p>Pre-trained LLMs play a vital role in transfer learning and fine-tuning, providing a foundation of general language understanding and knowledge. This foundation allows for efficient adaptation to specific tasks and domains, reducing the need for extensive training resources and data.</p>
<p>This chapter focuses on fine-tuning LLMs using OpenAI's infrastructure, which has been specifically designed to facilitate the process. OpenAI has developed tools and resources to make it easier for researchers and developers to fine-tune smaller models, such as Ada and Babbage, for their specific needs. The infrastructure offers a streamlined approach to fine-tuning, allowing users to efficiently adapt pre-trained models to a wide variety of tasks and domains.</p>
<h5 class="h5" id="ch04lev3sec1">Benefits of Using OpenAI's Fine-Tuning Infrastructure</h5>
<p>Leveraging OpenAI's infrastructure for fine-tuning offers several advantages:</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> Access to powerful pre-trained models, like GPT-3, which have been trained on extensive and diverse datasets.</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> A relatively user-friendly interface that simplifies the fine-tuning process for people with varying levels of expertise.</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> A range of tools and resources that help users optimize their fine-tuning process, such as guidelines for selecting hyperparameters, tips on preparing custom examples, and advice on model evaluation.</p>
<p>This streamlined process saves time and resources while ensuring the development of high-quality models capable of generating accurate and relevant responses in a wide array of applications. We will dive deep into open-source fine-tuning and the benefits/drawbacks it offers in a later chapter.</p>
<h3 class="h3" id="ch04lev1sec3">A Look at the OpenAI Fine-Tuning API</h3>
<p>The GPT-3 API offers developers access to one of the most advanced LLMs available. The API provides a range of fine-tuning capabilities, allowing users to adapt the model to specific tasks, languages, and domains. This section will discuss the key features of the GPT-3 fine-tuning API, the supported methods, and best practices for successfully fine-tuning models.</p>
<h4 class="h4" id="ch04lev2sec3">The GPT-3 Fine-Tuning API</h4>
<p>The GPT-3 fine-tuning API is like a treasure chest, brimming with powerful features that make customizing the model a breeze. From supporting various fine-tuning capabilities to offering a range of methods, it's a one-stop-shop for tailoring the model to your specific tasks, languages, or domains. This section will unravel the secrets of the GPT-3 fine-tuning API, uncovering the tools and techniques that make it such an invaluable resource.</p>
<h4 class="h4" id="ch04lev2sec4">Case Study: Amazon Review Sentiment Classification</h4>
<p>Let’s introduce our first case study. We will be working with the <strong>amazon_reviews_multi</strong> dataset (previewed in <a href="ch04.html#ch04fig03">Figure 4.3</a>). This dataset is a collection of product reviews from Amazon, spanning multiple product categories and languages (English, Japanese, German, French, Chinese and Spanish). Each review in the dataset is accompanied by a rating on a scale of 1 to 5 stars, with 1 being the lowest and 5 being the highest. The goal of this case study is to fine-tune a pre-trained model from OpenAI to perform sentiment classification on these reviews, enabling it to predict the number of stars given to a review. Taking a page out of my own book (albeit one just a few pages ago), let’s start with taking a look at the data.</p>
<div class="group">
<div class="image" id="ch04fig03"><img src="graphics/04fig03.jpg" alt="Images" width="745" height="297"/></div>
<p class="fig-caption"><strong>Figure 4.3</strong> <em>A snippet of the amazon_reviews_multi dataset shows our input context (review titles and bodies) and our response - the thing we are trying to predict - the number of stars the review was for (1-5).</em></p>
</div>
<p>The columns we will care about for this round of fine-tuning are:</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> <code>review_title</code>: The text title of the review.</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> <code>review_body</code>: The text body of the review.</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> <code>stars</code>: An int between 1-5 indicating the number of stars.</p>
<p>Our goal will be to use the context of the title and body of the review and predict the rating that was given.</p>
<h4 class="h4" id="ch04lev2sec5">Guidelines and Best Practices for Data</h4>
<p>In general, there are a few items to consider when selecting data for fine-tuning:</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> <strong>Data quality:</strong> Ensure that the data used for fine-tuning is of high quality, free from noise, and accurately represents the target domain or task. This will enable the model to learn effectively from the given examples.</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> <strong>Data diversity:</strong> Make sure your dataset is diverse, covering a broad range of scenarios to help the model generalize well across different situations.</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> <strong>Data balancing:</strong> Maintaining a balanced distribution of examples across different tasks and domains helps prevent overfitting and biases in the model's performance. This can be achieved from unbalanced datasets by undersampling majority classes, oversampling minority classes, or adding synthetic data. Our sentiment is perfectly balanced due to the fact that this dataset was curated but check out an even harder example in our code base where we attempt to classify the very unbalanced category classification task.</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> <strong>Data Quantity</strong>: The total amount of data used to fine-tune the model. Generally, larger language models like LLMs require extensive data to capture and learn various patterns effectively but fewer if the LLM was pre-trained on similar enough data. The exact quantity needed can vary based on the complexity of the task at hand. Any dataset should not only be extensive but also diverse and representative of the problem space to avoid potential biases and ensure robust performance across a wide range of inputs. While a large quantity of data can help to improve model performance, it also increases the computational resources required for model training and fine-tuning. This trade-off needs to be considered in the context of the specific project requirements and resources.</p>
<h3 class="h3" id="ch04lev1sec4">Preparing Custom Examples with the OpenAI CLI</h3>
<p>Before diving into fine-tuning, we need to prepare the data by cleaning and formatting it according to the API's requirements. This includes the following:</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> <strong>Removing duplicates:</strong> To ensure the highest data quality, start by removing any duplicate reviews from the dataset. This will prevent the model from overfitting to certain examples and improve its ability to generalize to new data.</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> <strong>Splitting the data:</strong> Divide the dataset into training, validation, and test sets, maintaining a random distribution of examples across each set. If necessary, consider using stratified sampling to ensure that each set contains a representative proportion of the different sentiment labels, thus preserving the overall distribution of the dataset.</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> <strong>Shuffle the training data:</strong> shuffling training data before fine-tuning helps to avoid biases in the learning process by ensuring that the model encounters examples in a random order, reducing the risk of learning unintended patterns based on the order of the examples. It also improves model generalization by exposing the model to a more diverse range of instances at each stage of training which also helps to prevent overfitting, as the model is less likely to memorize the training examples and instead focuses on learning the underlying patterns. <a href="ch04.html#ch04fig05">Figure 4.5</a> shows the benefits of shuffling training data. Note that data are ideally shuffled before every single epoch to reduce the chance of the model over-fitting on the data as much as possible.</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> <strong>Creating the OpenAI JSONL format:</strong> OpenAI's API expects the training data to be in JSONL (newline-delimited JSON) format. For each example in the training and validation sets, create a JSON object with two fields: “prompt” (the input) and “completion” (the target class). The “prompt” field should contain the review text, and the “completion” field should store the corresponding sentiment label (stars). Save these JSON objects as newline-delimited records in separate files for the training and validation sets.</p>
<p>For completion tokens in our dataset, we should make sure there is a leading space before the class label, as this enables the model to understand that it should generate a new token. Additionally, when preparing the prompts for the fine-tuning process, there's no need to include few-shot examples, as the model has already been fine-tuned on the task-specific data. Instead, provide a prompt that includes the review text and any necessary context, followed by a suffix (e.g., “Sentiment:” with no trailing space or “\n\n###\n\n” like in <a href="ch04.html#ch04fig04">Figure 4.4</a>) that indicates the desired output format. <a href="ch04.html#ch04fig04">Figure 4.4</a> shows an example of a single line of our JSONL file.</p>
<div class="group">
<div class="image" id="ch04fig04"><img src="graphics/04fig04.jpg" alt="Images" width="610" height="683"/></div>
<p class="fig-caption"><strong>Figure 4.4</strong> <em>A single JSONL example for our training data that we will feed to OpenAI. Every JSON has a prompt key - denoting the input to the model sans any few-shot, instructions, etc and a completion key - denoting what we want the model to output, which in our case is a single classification token. In this example, the user is rating the product with 1 star.</em></p>
</div>
<p>I should note that for our input data, I have concatenated the title and the body of the review as the singular input. This was a personal choice and I made it because I believe that the title can have more direct language to indicate general sentiment while the body likely has more nuanced to pinpoint the exact number of stars they are going to give. Feel free to explore different ways of combining text fields together! We are going to explore this further in later case studies along with other ways of formatting fields for a single text input.</p>
<div class="group">
<div class="image" id="ch04fig05"><img src="graphics/04fig05.jpg" alt="Images" width="763" height="468"/></div>
<p class="fig-caption"><strong>Figure 4.5</strong> <em>Unshuffled data makes for bad training data! It gives the model room to overfit on specific batches of data and overall lowers the quality of the responses. The top three graphs represent a model trained on unshuffled training data and the accuracy is horrible compared to a model trained on shuffled data, seen in the bottom three graphs.</em></p>
</div>
<p>The following listing (<a href="ch04.html#list4_1">Listing 4.1</a>) loads the Amazon Reviews dataset and converts the 'train' subset into a pandas DataFrame. Then, it preprocesses the DataFrame using the custom <code>prepare_df_for_openai</code> function, which combines the review title and review body into a prompt, creates a new completion column, and filters the DataFrame to only include English reviews. Finally, it removes duplicate rows based on the 'prompt' column and returns a DataFrame with only the 'prompt' and 'completion' columns.</p>
<p class="ex-caption" id="list4_1"><strong>Listing 4.1</strong> Generating a JSONL file for our sentiment training data</p>
<div class="pre-box">
<pre>from datasets import load_dataset
import pandas as pd

# Load the Amazon Reviews Multi-Languages dataset
dataset = load_dataset("amazon_reviews_multi", "all_languages")
# Convert the 'train' subset of the dataset to a pandas DataFrame
training_df = pd.DataFrame(dataset['train'])
def prepare_df_for_openai(df):
    # Combine 'review_title' and 'review_body' columns, and add a custom suffix '\n\n###\n\n' at the end to create the 'prompt' column
    df['prompt'] = df['review_title'] + '\n\n' + df['review_body'] + '\n\n###\n\n'
    # Create a new 'completion' column by adding a space before the 'stars' values
    df['completion'] = ' ' + df[stars]
    # Filter the DataFrame to only include rows with 'language' equal to 'en' (English)
    english_df = df[df['language'] == 'en']
    # Remove duplicate rows based on the 'prompt' column
    english_df.drop_duplicates(subset=['prompt'], inplace=True)
    # Return the shuffled and filtered DataFrame with only the 'prompt' and 'completion' columns
    return english_df[['prompt', 'completion']].sample(len(english_df))

english_training_df = prepare_df_for_openai(training_df)
# export the prompts and completions to a JSONL file
english_training_df.to_json("amazon-english-full-train-sentiment.jsonl", orient='records', lines=True)</pre>
</div>
<p>We would do a similar process with the <code>validation</code> subset of the dataset and the holdout <code>test</code> subset for a final test of the fine-tuned model. A quick note that we are filtering for English only in this case but you are free to train on more languages mixed in. I simply wanted to get some quicker results at an efficient price.</p>
<h5 class="h5" id="ch04lev3sec2">Setting Up the OpenAI CLI</h5>
<p>The OpenAI Command Line Interface (CLI) simplifies the process of fine-tuning and interacting with the API. The CLI allows you to submit fine-tuning requests, monitor training progress, and manage your models, all from your command line. Ensure that you have the OpenAI CLI installed and configured with your API key before proceeding with the fine-tuning process.</p>
<p>To install the OpenAI CLI, you can use pip, the Python package manager. First, make sure you have Python 3.6 or later installed on your system. Then, follow these steps:</p>
<p class="numbera">1. Open a terminal (on macOS or Linux) or a command prompt (on Windows).</p>
<p class="numbera">2. Run the following command to install the openai package: <code><strong>pip install openai</strong></code></p>
<p class="number-a">a. This command installs the OpenAI Python package, which includes the CLI.</p>
<p class="numbera">3. To verify that the installation was successful, run the following command: <code><strong>openai --version</strong></code></p>
<p class="number-a">a. This command should display the version number of the installed OpenAI CLI.</p>
<p>Before you can use the OpenAI CLI, you need to configure it with your API key. To do this, set the OPENAI_API_KEY environment variable to your API key value. You can find your API key in your OpenAI account dashboard.</p>
<h4 class="h4" id="ch04lev2sec6">Hyperparameter Selection and Optimization</h4>
<p>With our JSONL document created and OpenAI CLI installed, we are ready to select our hyperparameters! Here's a list of key hyperparameters and their definitions:</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> <strong>Learning rate</strong>: The learning rate determines the size of the steps the model takes during optimization. A smaller learning rate leads to slower convergence but potentially better accuracy, while a larger learning rate speeds up training but may cause the model to overshoot the optimal solution.</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> <strong>Batch size</strong>: Batch size refers to the number of training examples used in a single iteration of model updates. A larger batch size can lead to more stable gradients and faster training, while a smaller batch size may result in a more accurate model but slower convergence.</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> <strong>Training epochs</strong>: An epoch is a complete pass through the entire training dataset. The number of training epochs determines how many times the model will iterate over the data, allowing it to learn and refine its parameters.</p>
<p>OpenAI has done a lot of work to find optimal settings for most cases, so we will lean on their recommendations for our first attempt. The only thing we will change is to train for 1 epoch instead of the default 4. We're doing this because we want to see how the performance looks before investing too much time and money. Experimenting with different values and using techniques like grid search will help you find the optimal hyperparameter settings for your task and dataset, but be mindful that this process can be time-consuming and costly.</p>
<h3 class="h3" id="ch04lev1sec5">Our First Fine-Tuned LLM!</h3>
<p>Let’s kick off our first fine-tuning! <a href="ch04.html#list4_2">Listing 4.2</a> makes a call to OpenAI to train an ada model (fastest, cheapest, weakest) for 1 epoch on our training and validation data.</p>
<p class="ex-caption" id="list4_2"><strong>Listing 4.2</strong> making our first fine-tuning call</p>
<div class="pre-box">
<pre># Execute the 'fine_tunes.create' command using the OpenAI API
!openai api fine_tunes.create \
  # Specify the training dataset file in JSONL format
  -t "amazon-english-full-train-sentiment.jsonl" \
  # Specify the validation dataset file in JSONL format
  -v "amazon-english-full-val-sentiment.jsonl" \
  # Enable computation of classification metrics after fine-tuning
  --compute_classification_metrics \
  # Set the number of classes for classification (5 in this case)
  --classification_n_classes 5 \
  # Specify the base model to be fine-tuned (using the smallest model, ada)
  -m ada \
  # Set the number of epochs for training (1 in this case)
  --n_epochs 1</pre>
</div>
<h4 class="h4" id="ch04lev2sec7">Evaluating Fine-Tuned Models with Quantitative Metrics</h4>
<p>Measuring the performance of fine-tuned models is essential for understanding their effectiveness and identifying areas for improvement. Utilizing metrics and benchmarks, such as accuracy, F1 score, or perplexity, will provide quantitative measures of the model's performance. In addition to quantitative metrics, qualitative evaluation techniques, such as human evaluation or analyzing example outputs, can offer valuable insights into the model's strengths and weaknesses, helping identify areas for further fine-tuning.</p>
<p>After one epoch (further metrics shown in <a href="ch04.html#ch04fig06">Figure 4.6</a>), our classifier is getting above 63% accuracy on the holdout testing dataset! Remember the testing subset was not given to OpenAI but rather we held it out for final model comparisons.</p>
<div class="group">
<div class="image" id="ch04fig06"><img src="graphics/04fig06.jpg" alt="Images" width="773" height="205"/></div>
<p class="fig-caption"><strong>Figure 4.6</strong> <em>Our model is performing pretty well after only one epoch on de-duplicated shuffled training data</em></p>
</div>
<p>63% accuracy might sound low to you but hear me out: predicting the <strong>exact</strong> number of stars is tricky because people aren’t always consistent in what they write and how they finally review the product so I’ll offer two more metrics:</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> Relaxing our accuracy calculation to be binary (did the model predict &lt;= 3 stars and was the review &lt;=3 stars) is <strong>92%</strong> so the model can tell between “good” and “bad”</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> Relaxing the calculation to be “one-off” so for the example the model predicting 2 would count as correct if the actual rating was 1, 2, or 3 is <strong>93%</strong></p>
<p>So you know what? Not bad. Our classifier is definitely learning the difference between good and bad so the next logical thought might be, “let’s keep the training going”! We only trained for a single epoch so more epochs must be better, right?</p>
<p>This process of taking smaller steps in training and updating already fine-tuned models for more training steps/epochs potentially with new labeled datapoints is called <strong>incremental learning</strong> aka continuous learning or online learning. Incremental learning often results in more controlled learning, which can be ideal when working with smaller datasets or when you want to preserve some of the model's general knowledge. Let’s try some incremental learning! Let’s take our already fine-tuned ada model and let it run for 3 more epochs on the same data and see the results in <a href="ch04.html#ch04fig07">Figure 4.7</a>.</p>
<div class="group">
<div class="image" id="ch04fig07"><img src="graphics/04fig07.jpg" alt="Images" width="763" height="486"/></div>
<p class="fig-caption"><strong>Figure 4.7</strong>: The model’s performance seems to barely move during a further 3 epochs of incremental learning after a successful single epoch. 4x the cost for 1.02x the performance? No thank you.</p>
</div>
<p>Uh oh, more epochs didn’t seem to really do anything, but nothing is set in stone until we test on our holdout test data subset and compare it to our first model. <a href="ch04.html#ch04tab01">Table 4.1</a> shows our results:</p>
<div class="group">
<p class="tab-caption"><strong>Table 4.1</strong> <em>Results</em></p>
<div class="imaget" id="ch04tab01"><img src="graphics/04tab01.jpg" alt="Images" width="779" height="376"/></div>
</div>
<p>So for 4x the price, we get a single % point increase in accuracy? Not worth it in my book but maybe it is for you! Some industries demand near perfection in their models and single percentage points matter. I’ll leave that up to you but in general more epochs will not always lead to better results. Incremental/online learning can help you find the right stopping point at the cost of more up-front effort but will be well worth it in the long run.</p>
<h4 class="h4" id="ch04lev2sec8">Qualitative Evaluation Techniques</h4>
<p>Alongside quantitative metrics, qualitative evaluation techniques offer valuable insights into the strengths and weaknesses of our fine-tuned model. Examining generated outputs or employing human evaluators can help identify areas where the model excels or falls short, guiding our future fine-tuning efforts.</p>
<p>To help, we can get the probability for our classification by looking at the probabilities of predicting the first token in either the Playground (as seen in <a href="ch04.html#ch04fig08">Figure 4.8</a>) or via the API’s <code>logprobs</code> value (as seen in <a href="ch04.html#list4_3">Listing 4.3</a>)</p>
<div class="group">
<div class="image" id="ch04fig08"><img src="graphics/04fig08.jpg" alt="Images" width="710" height="396"/></div>
<p class="fig-caption"><strong>Figure 4.8</strong> <em>The playground and the API for GPT-3 like models (including our fine-tuned ada model as seen in this figure) offer token probabilities that we can use to check the model’s confidence on a particular classification. Note that the main option is “ 1“ with a leading space just like we have in our training data but one of the tokens on the top of the list is “1” with no leading space. These are two separate tokens according to many LLMs which is why I am calling it out so often. It can be easy to forget and mix them up.</em></p>
</div>
<p class="ex-caption" id="list4_3"><strong>Listing 4.3</strong> getting token probabilities from the OpenAI API</p>
<div class="pre-box">
<pre>import math
# Select a random prompt from the test dataset

prompt = english_test_df['prompt'].sample(1).iloc[0]

# Generate a completion using the fine-tuned model
res = openai.Completion.create(
    model='ada:ft-personal-2023-03-31-05-30-46', 
    prompt=prompt,
    max_tokens=1, 
    temperature=0, 
    logprobs=5,
)

# Initialize an empty list to store probabilities
probs = []
# Extract logprobs from the API response
logprobs = res['choices'][0]['logprobs']['top_logprobs']
# Convert logprobs to probabilities and store them in the 'probs' list
for logprob in logprobs:
    _probs = {}
    for key, value in logprob.items():
        _probs[key] = math.exp(value)
    probs.append(_probs)
# Extract the predicted category (star) from the API response
pred = res['choices'][0].text.strip()
# Nicely print the prompt, predicted category, and probabilities
print("Prompt: \n", prompt[:200], "...\n")
print("Predicted Star:", pred)
print("Probabilities:")
for prob in probs:
    for key, value in sorted(prob.items(), key=lambda x: x[1], reverse=True):
        print(f"{key}: {value:.4f}")
    print()</pre>
</div>
<p><strong>Output</strong>:</p>
<pre>Prompt:
 Great pieces of jewelry for the price

Great pieces of jewelry for the price. The 6mm is perfect for my tragus piercing. I gave four stars because I already lost one because it fell out! Other than that I am very happy with the purchase!

###


Predicted Star: 4
Probabilities:
 4: 0.9831
 5: 0.0165
 3: 0.0002
 2: 0.0001
 1: 0.0001</pre>
<p>Between quantitative and qualitative measures, let’s assume we believe our model is ready to go into production – if not at least a dev or staging environment for further testing, let’s take a minute to talk about how we can incorporate our new model into our applications.</p>
<h4 class="h4" id="ch04lev2sec9">Integrating Fine-Tuned GPT-3 Models into Applications</h4>
<p>Integrating a fine-tuned GPT-3 model into your application is identical to using a base model provided by OpenAI. The primary difference is that you'll need to reference your fine-tuned model's unique identifier when making API calls. Here are the key steps to follow:</p>
<p class="numbera">1. <strong>Identify your fine-tuned model</strong>: After completing the fine-tuning process, you will receive a unique identifier for your fine-tuned model like <code>‘ada:ft-personal-2023-03-31-05-30-46’</code>. Make sure to note this identifier, as it will be required for API calls.</p>
<p class="numbera">2. <strong>Use the OpenAI API Normally</strong>: Use yourOpenAI API to make requests to your fine-tuned model. When making requests, replace the base model's name with your fine-tuned model's unique identifier. <a href="ch04.html#list4_3">Listing 4.3</a> offers an example of doing this.</p>
<p class="numbera">3. <strong>Adapt any application logic</strong>: Since fine-tuned models may require different prompt structures or generate different output formats, you may need to update your application's logic to handle these variations. For example in our prompts, we concatenated the review title with the body and added a custom suffix “\n\n###\n\n”.</p>
<p class="numbera">4. <strong>Monitor and evaluate performance</strong>: Continuously monitor your fine-tuned model's performance and collect user feedback. You may need to iteratively fine-tune your model with even more data to improve its accuracy and effectiveness.</p>
<h3 class="h3" id="ch04lev1sec6">Case Study 2: Amazon Review Category Classification</h3>
<p>With a successfully fine-tuned ada model for a relatively simple example like sentiment classification, let's up the stakes and tackle a more challenging task. In a second case study, we will explore how fine-tuning a GPT-3 model can improve its performance on the task of Amazon review category classification from the same dataset. This task involves classifying Amazon product reviews into their respective product categories based on the review title and body - just like we did for sentiment. We no longer have 5 classes for example, we now have 31 unbalanced classes (see <a href="ch04.html#ch04fig09">Figure 4.9</a>)!</p>
<div class="group">
<div class="image" id="ch04fig09"><img src="graphics/04fig09.jpg" alt="Images" width="776" height="603"/></div>
<p class="fig-caption"><strong>Figure 4.9</strong> <em>The category classification task has 31 unique categories to choose from and a very unbalanced class distribution which is a perfect storm for a difficult classification task</em></p>
</div>
<p>The much harder category classification task reveals a lot of hidden difficulties of ML, such as dealing with unbalanced data and <strong>ill-defined data</strong>—where the distinction between categories is subtle or ambiguous. In these cases, the model may struggle to discern the correct category. To improve performance, consider refining the problem definition, deleting redundant or confusing training examples, merging similar categories, or providing additional context to the model through prompts.</p>
<p>Check out all of that work in our code repository!</p>
<h3 class="h3" id="ch04lev1sec7">Summary</h3>
<p>Fine-tuning LLMs like GPT-3 is an effective way to enhance their performance on specific tasks or domains. By integrating a fine-tuned model into your application and following best practices for deployment, you can create a more efficient, accurate, and cost-effective language processing solution. Continuously monitor and evaluate your model's performance, and iterate on its fine-tuning to ensure it meets the evolving needs of your application and users.</p>
<p>We will revisit the idea of fine-tuning in later chapters with some more complicated examples while also exploring the fine-tuning strategies for open-source models for even further cost reductions.</p>
</div>
</div>
</body></html>