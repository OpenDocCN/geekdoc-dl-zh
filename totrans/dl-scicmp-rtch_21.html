<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>16  Making models generalize</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>16  Making models generalize</h1>
<blockquote>原文：<a href="https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/overfitting.html">https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/overfitting.html</a></blockquote>

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">

</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>In deep learning, as in machine learning in general, we face a problem of generalization. The training set is limited, after all. There is no guarantee that what a model has learned generalizes to the “outside world”.</p>
<p>How big of a problem is this? It depends. When we construct a toy problem ourselves, we can actively make sure that training and test set come from the very same distribution. On the other end of the spectrum are configurations where significant structural disparities are known to exist from the outset. In the latter case, the measures described in this chapter will be useful, but not sufficient to resolve the problem. Instead, expert domain knowledge and appropriate workflow (re-)design will be required.</p>
<p>Between both extremes, there is a wide range of possibilities. For example:</p>
<ul>
<li><p>An image dataset has had all pictures taken using excellent photographic equipment, and as viewed from a 180 degrees angle; but prediction will take place “in the wild”, using cell phones.</p></li>
<li><p>A application designed to diagnose skin cancer has been trained on white males mostly, but is supposed to be used by people of any gender and/or skin color.</p></li>
</ul>
<p>The second example should make clear that lack of generalization (a.k.a. <em>overfitting</em><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>) – is anything but an exclusively technical problem. Insufficient representation, or even <em>lack</em> of representation, is one of the major manifestations of <em>dataset bias</em>. Among the measures intended to prevent overfitting we discuss below, just one – the most important of all – applies to the second scenario. (This book is dedicated to technical topics; but as emphasized in the preface, we all live in a “real world” that is not just extremely complex, but also, fraught with inequality, inequity, and injustice. However strong our technical focus, this is something we should never lose awareness of.)</p>
<p>The counter-measures I’ll be presenting can be ordered by stage in the machine learning workflow they apply to: data collection, data pre-processing, model definition, and model training. As you’ll be expecting by now, the most important measure, as hinted at above, is to collect more representative data.</p>
<section id="the-royal-road-more-and-more-representative-data" class="level2" data-number="16.1">
<h2 data-number="16.1" class="anchored" data-anchor-id="the-royal-road-more-and-more-representative-data"><span class="header-section-number">16.1</span> The royal road: more – and more representative! – data</h2>
<p>Depending on your situation, you may be forced to work with pre-existing datasets. In that case, you may still have the option of supplementing external sources with data you’ve collected yourself, albeit of lesser quantity.</p>
<p>In the next chapter, we’ll encounter the technique of <em>transfer learning</em>. Transfer learning means making use of <em>pre-trained</em> models, employing them as feature extractors for your “downstream” task. Often, these models have been trained on huge datasets. In consequence, they are not just “very good at what they do”, but they also generalize to unseen data – <em>provided</em> the new data are similar to those they have been trained on. What if they’re not? For many tasks, it will still be possible to make use of a pre-trained model: Take it the way it comes, and go on training it – but now, adding in the kinds of data you want it to generalize to.</p>
<p>Of course, being able to add in <em>any</em> data of your own may still be a dream. In that case, all you can do is think hard about how your results will be biased, and be honest about it.</p>
<p>Now, let’s say you’ve thought it through, and are confident that there are no systematic deficiencies in your training data preventing generalization to use cases in the wild. Or maybe you’ve restricted the application domain of your model as required. Then, if you have a small training set and want it to generalize as much as possible, what can you do?</p>
</section>
<section id="pre-processing-stage-data-augmentation" class="level2" data-number="16.2">
<h2 data-number="16.2" class="anchored" data-anchor-id="pre-processing-stage-data-augmentation"><span class="header-section-number">16.2</span> Pre-processing stage: Data augmentation</h2>
<p>Data augmentation means taking the data you have and modifying them, so as to force the algorithm to abstract over some things. What things? It depends on the domain operated upon. This should become clear by means of a concrete example.</p>
<p>In this chapter, I’ll introduce two popular variants of data augmentation: one I’ll refer to as “classic”, the other one going by the name of “mixup”.</p>
<section id="classic-data-augmentation" class="level3" data-number="16.2.1">
<h3 data-number="16.2.1" class="anchored" data-anchor-id="classic-data-augmentation"><span class="header-section-number">16.2.1</span> Classic data augmentation</h3>
<p>Classically, when people talk about (image) data augmentation, it is the following they’re having in mind. You take an image, and apply some random transformation to it. That transformation could be geometric, such as when rotating, translating, or scaling the image. Alternatively, instead of moving things around, the operation could affect the colors, as when changing brightness or saturation. Other options include blurring the image, or, quite the contrary, sharpening it. Technically, you are free to implement whatever algorithm you desire – you don’t have to use any of the (numerous!) transformations provided by <code>torchvision</code>. In practice, though, you’ll likely find much of what you need among the transformations already available.</p>
<p>In our running example, we’ll work with MNIST, the <em>Hello World</em> of image-classification datasets we’ve already made quick use of before. It contains 70,000 images of the digits <code>0</code> to <code>9</code>, split into training and test sets in a ratio of six to one. Like before, we get the data from <code>torchvision</code>.</p>
<p>To see how the digits appear without any data augmentation, take a look at the first thirty-two images in the test set (<a href="#fig-overfitting-mnist-test">fig. <span>16.1</span></a>):</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"/><span class="fu">library</span>(torch)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"/><span class="fu">library</span>(torchvision)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"/><span class="fu">library</span>(luz)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"/></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"/>dir <span class="ot">&lt;-</span> <span class="st">"~/.torch-datasets"</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"/></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"/>valid_ds <span class="ot">&lt;-</span> <span class="fu">mnist_dataset</span>(</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"/>  dir,</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"/>  <span class="at">download =</span> <span class="cn">TRUE</span>,</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"/>  <span class="at">train =</span> <span class="cn">FALSE</span>,</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"/>  <span class="at">transform =</span> transform_to_tensor</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"/>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"/></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"/>valid_dl <span class="ot">&lt;-</span> <span class="fu">dataloader</span>(valid_ds, <span class="at">batch_size =</span> <span class="dv">128</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"/></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"/><span class="co"># a convenient way to obtain individual images without </span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"/><span class="co"># manual iteration</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"/>test_images <span class="ot">&lt;-</span> coro<span class="sc">::</span><span class="fu">collect</span>(</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"/>  valid_dl, <span class="dv">1</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"/>)[[<span class="dv">1</span>]]<span class="sc">$</span>x[<span class="dv">1</span><span class="sc">:</span><span class="dv">32</span>, <span class="dv">1</span>, , ] <span class="sc">%&gt;%</span> <span class="fu">as.array</span>()</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"/></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"/><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">4</span>, <span class="dv">8</span>), <span class="at">mar =</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">4</span>), <span class="at">mai =</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">4</span>))</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"/>test_images <span class="sc">%&gt;%</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"/>  purrr<span class="sc">::</span><span class="fu">array_tree</span>(<span class="dv">1</span>) <span class="sc">%&gt;%</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"/>  purrr<span class="sc">::</span><span class="fu">map</span>(as.raster) <span class="sc">%&gt;%</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"/>  purrr<span class="sc">::</span><span class="fu">iwalk</span>(<span class="sc">~</span> {</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"/>    <span class="fu">plot</span>(.x)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"/>  })</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<div id="fig-overfitting-mnist-test" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="../Images/79ad6250a77b4c2a00a0d16af75da4eb.png" class="quarto-discovered-preview-image img-fluid figure-img" alt="Thirty-two digits, standing upright." data-original-src="https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/images/overfitting-mnist-test.png"/></p>
<p/><figcaption class="figure-caption">Figure 16.1: MNIST: The first thirty-two images in the test set.</figcaption><p/>
</figure>
</div>
<p>Now, we use the training set to experiment with data augmentation. Like the <code>dogs_vs_cats_dataset()</code> we made use of in the last chapter – in fact, like all <code>torchvision</code> datasets – <code>mnist_dataset()</code> takes a <code>transform</code> argument, allowing you to pass in arbitrary transformations to be performed on the input images. Of the four transformations that appear in the code snippet below, one we have already seen: <code>transform_to_tensor()</code>, used to convert from R <code>double</code>s to <code>torch</code> tensors. The other three, that all share the infix <code>_random_</code>, each trigger non-deterministic data augmentation: be it by flipping the image horizontally (<code>transform_random_horizontal_flip()</code>) or vertically (<code>transform_random_vertical_flip()</code>), or through rotations and translations(<code>transform_random_affine()</code>). In all cases, the amount of distortion is configurable.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"/>train_ds <span class="ot">&lt;-</span> <span class="fu">mnist_dataset</span>(</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"/>  dir,</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"/>  <span class="at">download =</span> <span class="cn">TRUE</span>,</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"/>  <span class="at">transform =</span> . <span class="sc">%&gt;%</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"/>    <span class="fu">transform_to_tensor</span>() <span class="sc">%&gt;%</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"/>    <span class="co"># flip horizontally, with a probability of 0.5</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"/>    <span class="fu">transform_random_horizontal_flip</span>(<span class="at">p =</span> <span class="fl">0.5</span>) <span class="sc">%&gt;%</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"/>    <span class="co"># flip vertically, with a probability of 0.5</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"/>    <span class="fu">transform_random_vertical_flip</span>(<span class="at">p =</span> <span class="fl">0.5</span>) <span class="sc">%&gt;%</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"/>    <span class="co"># (1) rotate to the left or the right,</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"/>    <span class="co">#     up to respective angles of 45 degrees</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"/>    <span class="co"># (2) translate vertically or horizontally,</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"/>    <span class="co">#     not exceeding 10% of total image width/height</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"/>    <span class="fu">transform_random_affine</span>(</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"/>      <span class="at">degrees =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">45</span>, <span class="dv">45</span>),</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"/>      <span class="at">translate =</span> <span class="fu">c</span>(<span class="fl">0.1</span>, <span class="fl">0.1</span>)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"/>    )</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"/>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>Again, let’s look at a sample result (<a href="#fig-overfitting-mnist-transforms">fig. <span>16.2</span></a>).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"/>train_dl <span class="ot">&lt;-</span> <span class="fu">dataloader</span>(</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"/>  train_ds,</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"/>  <span class="at">batch_size =</span> <span class="dv">128</span>,</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"/>  <span class="at">shuffle =</span> <span class="cn">TRUE</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"/>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"/></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"/>train_images <span class="ot">&lt;-</span> coro<span class="sc">::</span><span class="fu">collect</span>(</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"/>  train_dl, <span class="dv">1</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"/>)[[<span class="dv">1</span>]]<span class="sc">$</span>x[<span class="dv">1</span><span class="sc">:</span><span class="dv">32</span>, <span class="dv">1</span>, , ] <span class="sc">%&gt;%</span> <span class="fu">as.array</span>()</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"/></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"/><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">4</span>, <span class="dv">8</span>), <span class="at">mar =</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">4</span>), <span class="at">mai =</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">4</span>))</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"/>train_images <span class="sc">%&gt;%</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"/>  purrr<span class="sc">::</span><span class="fu">array_tree</span>(<span class="dv">1</span>) <span class="sc">%&gt;%</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"/>  purrr<span class="sc">::</span><span class="fu">map</span>(as.raster) <span class="sc">%&gt;%</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"/>  purrr<span class="sc">::</span><span class="fu">iwalk</span>(<span class="sc">~</span> {</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"/>    <span class="fu">plot</span>(.x)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"/>  })</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<div id="fig-overfitting-mnist-transforms" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="../Images/2a62dbce6a0744a2b3bfe19a8e0c826e.png" class="img-fluid figure-img" alt="Thirty-two digits, standing upright, rotated, translated, and/or flipped horizontally." data-original-src="https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/images/overfitting-mnist-transforms.png"/></p>
<p/><figcaption class="figure-caption">Figure 16.2: MNIST, with random rotations, translations, and flips.</figcaption><p/>
</figure>
</div>
<p>The effects are clearly visible, and the ranges chosen for rotations and translations seem sensible. But let’s think about the flips. Does it actually make <em>sense</em> to include them?</p>
<p>In general, this would depend on the dataset, and more even, on the task. Think of a cat, comfortably residing on a fluffy sofa. If the cat were looking to the right instead of to the left, it would still be a cat; if it was positioned upside-down, we’d probably assume the image had been loaded incorrectly. Neither transformation would affect its “catness”. It’s different with digits, though. A flipped <span class="math inline">\(1\)</span> is not the same as a <span class="math inline">\(1\)</span>, at least not in a default context. Thus, for MNIST, I’d rather go with rotations and translations only:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"/>train_ds <span class="ot">&lt;-</span> <span class="fu">mnist_dataset</span>(</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"/>  dir,</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"/>  <span class="at">download =</span> <span class="cn">TRUE</span>,</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"/>  <span class="at">transform =</span> . <span class="sc">%&gt;%</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"/>    <span class="fu">transform_to_tensor</span>() <span class="sc">%&gt;%</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"/>    <span class="fu">transform_random_affine</span>(</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"/>      <span class="at">degrees =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">45</span>, <span class="dv">45</span>), <span class="at">translate =</span> <span class="fu">c</span>(<span class="fl">0.1</span>, <span class="fl">0.1</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"/>    )</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"/>)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"/></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"/>train_dl <span class="ot">&lt;-</span> <span class="fu">dataloader</span>(train_ds,</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"/>  <span class="at">batch_size =</span> <span class="dv">128</span>,</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"/>  <span class="at">shuffle =</span> <span class="cn">TRUE</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"/>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>Now, to compare what happens with and without augmentation, you’d separately train a model for both an augmented and a non-augmented version of the training set. Here is an example setup:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"/>convnet <span class="ot">&lt;-</span> <span class="fu">nn_module</span>(</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"/>  <span class="st">"convnet"</span>,</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"/>  <span class="at">initialize =</span> <span class="cf">function</span>() {</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"/>    <span class="co"># nn_conv2d(in_channels, out_channels, kernel_size, stride)</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"/>    self<span class="sc">$</span>conv1 <span class="ot">&lt;-</span> <span class="fu">nn_conv2d</span>(<span class="dv">1</span>, <span class="dv">32</span>, <span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"/>    self<span class="sc">$</span>conv2 <span class="ot">&lt;-</span> <span class="fu">nn_conv2d</span>(<span class="dv">32</span>, <span class="dv">64</span>, <span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"/>    self<span class="sc">$</span>conv3 <span class="ot">&lt;-</span> <span class="fu">nn_conv2d</span>(<span class="dv">64</span>, <span class="dv">128</span>, <span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"/>    self<span class="sc">$</span>conv4 <span class="ot">&lt;-</span> <span class="fu">nn_conv2d</span>(<span class="dv">128</span>, <span class="dv">256</span>, <span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"/>    self<span class="sc">$</span>conv5 <span class="ot">&lt;-</span> <span class="fu">nn_conv2d</span>(<span class="dv">256</span>, <span class="dv">10</span>, <span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"/>  },</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"/>  <span class="at">forward =</span> <span class="cf">function</span>(x) {</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"/>    x <span class="sc">%&gt;%</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"/>      self<span class="sc">$</span><span class="fu">conv1</span>() <span class="sc">%&gt;%</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"/>      <span class="fu">nnf_relu</span>() <span class="sc">%&gt;%</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"/>      self<span class="sc">$</span><span class="fu">conv2</span>() <span class="sc">%&gt;%</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"/>      <span class="fu">nnf_relu</span>() <span class="sc">%&gt;%</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"/>      self<span class="sc">$</span><span class="fu">conv3</span>() <span class="sc">%&gt;%</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"/>      <span class="fu">nnf_relu</span>() <span class="sc">%&gt;%</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"/>      self<span class="sc">$</span><span class="fu">conv4</span>() <span class="sc">%&gt;%</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"/>      <span class="fu">nnf_relu</span>() <span class="sc">%&gt;%</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"/>      self<span class="sc">$</span><span class="fu">conv5</span>() <span class="sc">%&gt;%</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"/>      <span class="fu">torch_squeeze</span>()</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"/>  }</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"/>)</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"/></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"/>fitted <span class="ot">&lt;-</span> convnet <span class="sc">%&gt;%</span></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"/>  <span class="fu">setup</span>(</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"/>    <span class="at">loss =</span> <span class="fu">nn_cross_entropy_loss</span>(),</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"/>    <span class="at">optimizer =</span> optim_adam,</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"/>    <span class="at">metrics =</span> <span class="fu">list</span>(</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"/>      <span class="fu">luz_metric_accuracy</span>()</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"/>    )</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"/>  ) <span class="sc">%&gt;%</span></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"/>  <span class="fu">fit</span>(train_dl, <span class="at">epochs =</span> <span class="dv">5</span>, <span class="at">valid_data =</span> valid_dl)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>With MNIST, we have at our disposition a huge training set, and at the same time, we’re dealing with a very homogeneous domain. Those are exactly the conditions where we don’t expect to see much overfitting<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p>
<p>However, even with MNIST, you’ll notice that with augmentation, as well as the other “overfitting antidotes” to be introduced, it takes a lot longer to achieve better performance on the training than on the test set (if ever there <em>is</em> better performance on the training set!). For example, in the setup described above, with no data augmentation applied, training-set accuracy surpassed that on the test set from the third epoch onwards; whereas with augmentation, there was no sign of overfitting during all five epochs I trained for.</p>
<p>Next – still in the realm of data augmentation – we look at a technique that is domain-independent; that is, it can be applied to all kinds of data, not just images.</p>
</section>
<section id="mixup" class="level3" data-number="16.2.2">
<h3 data-number="16.2.2" class="anchored" data-anchor-id="mixup"><span class="header-section-number">16.2.2</span> Mixup</h3>
<p>Classic data augmentation, whatever it may be doing to the entities involved – move them, distort them, blur them – it leaves them <em>intact</em>. A rotated cat is still a cat. <em>Mixup</em> <span class="citation" data-cites="2017arXiv171009412Z">(<a href="references.html#ref-2017arXiv171009412Z" role="doc-biblioref">Zhang et al. 2017</a>)</span>, on the other hand, takes two entities and “mixes them together”. With mixup, we may have something that’s half-cat and half-squirrel. Or rather, in practice, with strongly unequal <em>mixing weights</em> used, ninety percent squirrel and ten percent cat.</p>
<p>As an idea, mixup generalizes to any domain. We could mix time series, say, or categorical data of any kind. We could also mix numerical data, although it’s not clear why we would do it. After all, mixup is nothing else but linear combination: take two values <span class="math inline">\(x1\)</span> and <span class="math inline">\(x2\)</span>, and construct <span class="math inline">\(x3 = w1 x1 + w2 x2\)</span>, where <span class="math inline">\(w1\)</span> and <span class="math inline">\(w2\)</span> are weights summing to one. In neural networks, linear combination of numerical values (“automatic mixup”) happens all the time, so that normally, we wouldn’t expect “manual mixup” to add much value.</p>
<p>For visual demonstration, however, images are still best. Starting from MNIST’s test set, we can apply mixup with different weight patterns – equal, very unequal, and somewhere in-between – and see what happens. <code>luz</code> has a function, called <code>nnf_mixup()</code>, that lets you play around with this.</p>
<p>(By the way, I’m introducing this function just so you can picture (literally!) what is going on. To actually <em>use</em> mixup, all that is required is to pass the appropriate callback to <code>fit()</code>, and let <code>setup()</code> know which loss function you want to use.)</p>
<p>Besides the input and target batches, <code>nnf_mixup()</code> expects to be passed the <em>mixing weights</em>, one value per batch item. We start with the most “tame” variant: with weights that are very unequal between classes. Every resultant image will be composed, to ninety percent, of the original item at that position, and to ten percent, of a randomly-chosen different one (<a href="#fig-overfitting-mnist-mixup-0.9">fig. <span>16.3</span></a>):</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"/>first_batch <span class="ot">&lt;-</span> coro<span class="sc">::</span><span class="fu">collect</span>(valid_dl, <span class="dv">1</span>)[[<span class="dv">1</span>]]</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"/></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"/>mixed <span class="ot">&lt;-</span> <span class="fu">nnf_mixup</span>(<span class="at">x =</span> first_batch<span class="sc">$</span>x,</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"/>                   <span class="at">y =</span> first_batch<span class="sc">$</span>y,</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"/>                   <span class="at">weight =</span> <span class="fu">torch_tensor</span>(<span class="fu">rep</span>(<span class="fl">0.9</span>, <span class="dv">128</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<div id="fig-overfitting-mnist-mixup-0.9" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="../Images/13fe1d809dfb70b857eddea76e713ea1.png" class="img-fluid figure-img" alt="Thirty-two bold and clearly visible digits, each overlayed with another digit, which is barely visible." data-original-src="https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/images/overfitting-mnist-mixup-0.9.png"/></p>
<p/><figcaption class="figure-caption">Figure 16.3: Mixing up MNIST, with mixing weights of 0.9.</figcaption><p/>
</figure>
</div>
<p>Do you agree that the mixed-in digits are just barely visible, if at all? Still, the callback’s default configuration results in mixing ratios pretty close to this one. For MNIST, this probably is too cautious a choice. But think of datasets where the objects are less shape-like, less sharp-edged. Mixing two landscapes, at an equal-ish ratio, would result in total gibberish. And here, too, the task plays a role; not just the dataset per se. Mixing apples and oranges one-to-one can make sense if we’re looking for a higher-level concept – a superset, of sorts. But if all we’re looking for is to correctly discern oranges that look a bit like an apple, or apples that have something “orange-y” in them, then a ratio such as 9:1 might be just fine.</p>
<p>To develop an idea of what would happen for other proportions, let’s successively make the mixing ratio more equal.</p>
<p>First, here (<a href="#fig-overfitting-mnist-mixup-0.7">fig. <span>16.4</span></a>) is 0.7:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"/>mixed <span class="ot">&lt;-</span> <span class="fu">nnf_mixup</span>(<span class="at">x =</span> first_batch<span class="sc">$</span>x,</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"/>                   <span class="at">y =</span> first_batch<span class="sc">$</span>y,</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"/>                   <span class="at">weight =</span> <span class="fu">torch_tensor</span>(<span class="fu">rep</span>(<span class="fl">0.7</span>, <span class="dv">128</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<div id="fig-overfitting-mnist-mixup-0.7" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="../Images/f71ee51a740fe049bf798162290e797e.png" class="img-fluid figure-img" alt="Thirty-two bold and rather clearly visible digits, each overlayed with another digit, which is visible but not as easy to classify." data-original-src="https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/images/overfitting-mnist-mixup-0.7.png"/></p>
<p/><figcaption class="figure-caption">Figure 16.4: Mixing up MNIST, with mixing weights of 0.7.</figcaption><p/>
</figure>
</div>
<p>And here (<a href="#fig-overfitting-mnist-mixup-0.5">fig. <span>16.5</span></a>), 0.5:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"/>mixed <span class="ot">&lt;-</span> <span class="fu">nnf_mixup</span>(<span class="at">x =</span> first_batch<span class="sc">$</span>x,</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"/>                   <span class="at">y =</span> first_batch<span class="sc">$</span>y,</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"/>                   <span class="at">weight =</span> <span class="fu">torch_tensor</span>(<span class="fu">rep</span>(<span class="fl">0.5</span>, <span class="dv">128</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<div id="fig-overfitting-mnist-mixup-0.5" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="../Images/7321e050b5d985b0bd1f4e18c6a71f06.png" class="img-fluid figure-img" alt="Thirty-two digits, each overlayed with another digit, which is displayed with the same intensity." data-original-src="https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/images/overfitting-mnist-mixup-0.5.png"/></p>
<p/><figcaption class="figure-caption">Figure 16.5: Mixing up MNIST, with mixing weights of 0.5.</figcaption><p/>
</figure>
</div>
<p>To use mixup while training, all you need to do is:</p>
<ol type="1">
<li><p>Add <code>luz_callback_mixup()</code> to the list of callbacks passed to <code>luz::fit()</code>. The callback takes an optional parameter, <code>alpha</code>, used in determining the mixing ratios. I’d recommend starting with the default, though, and start tweaking from there.</p></li>
<li><p>Wrap the loss you’re using for the task (here, cross entropy) in <code>nn_mixup_loss()</code>.</p></li>
<li><p>Use the loss, not the accuracy metric, for monitoring training progress, since accuracy in this case is not well defined.</p></li>
</ol>
<p>Here is an example:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"/><span class="co"># redefine the training set to not use augmentation</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"/>train_ds <span class="ot">&lt;-</span> <span class="fu">mnist_dataset</span>(</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"/>  dir,</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"/>  <span class="at">download =</span> <span class="cn">TRUE</span>,</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"/>  <span class="at">transform =</span> transform_to_tensor</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"/>)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"/></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"/>train_dl <span class="ot">&lt;-</span> <span class="fu">dataloader</span>(train_ds,</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"/>  <span class="at">batch_size =</span> <span class="dv">128</span>,</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"/>  <span class="at">shuffle =</span> <span class="cn">TRUE</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"/>)</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"/></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"/>fitted <span class="ot">&lt;-</span> convnet <span class="sc">%&gt;%</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"/>  <span class="fu">setup</span>(</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"/>    <span class="at">loss =</span> <span class="fu">nn_mixup_loss</span>(torch<span class="sc">::</span><span class="fu">nn_cross_entropy_loss</span>()),</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"/>    <span class="at">optimizer =</span> optim_adam</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"/>  ) <span class="sc">%&gt;%</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"/>  <span class="fu">fit</span>(</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"/>    train_dl,</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"/>    <span class="at">epochs =</span> <span class="dv">5</span>,</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"/>    <span class="at">valid_data =</span> valid_dl,</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"/>    <span class="at">callbacks =</span> <span class="fu">list</span>(<span class="fu">luz_callback_mixup</span>())</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"/>  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>Mixup is an appealing technique that makes a lot of intuitive sense. If you feel like, go ahead and experiment with it on different tasks and different types of data.</p>
<p>Next, we move on to the next stage in the workflow: model definition.</p>
</section>
</section>
<section id="modeling-stage-dropout-and-regularization" class="level2" data-number="16.3">
<h2 data-number="16.3" class="anchored" data-anchor-id="modeling-stage-dropout-and-regularization"><span class="header-section-number">16.3</span> Modeling stage: dropout and regularization</h2>
<p>Inside a neural network, there are two kinds of “data”: <em>activations</em> – tensors propagated from one layer to the next – and <em>weights</em>, tensors associated with individual layers. From the two techniques we’ll look at in this section, one (<em>dropout</em>) affects the former; the other (<em>regularization</em>), the latter.</p>
<section id="dropout" class="level3" data-number="16.3.1">
<h3 data-number="16.3.1" class="anchored" data-anchor-id="dropout"><span class="header-section-number">16.3.1</span> Dropout</h3>
<p>Dropout <span class="citation" data-cites="10.5555/2627435.2670313">(<a href="references.html#ref-10.5555/2627435.2670313" role="doc-biblioref">Srivastava et al. 2014</a>)</span> happens during training only. At each forward pass, individual activations – single values in the tensors being passed on – are <em>dropped</em> (meaning: set to zero), with configurable probability. Due to randomness, actual positions of zeroed-out values in a tensor vary from pass to pass.</p>
<p>Put differently: Dynamically and reversibly, individual inter-neuron connections are “cut off”. Why would this help to avoid overfitting?</p>
<p>If different connections between neurons could be dropped out at unforeseeable times, the network as a whole had better not get too dependent on cooperation between individual units. But it is just this kind of inter-individual cooperation that results in strong memorization of examples presented during training. If that is made impossible, the network as a whole has to focus on more general features, ones that that emerge from more distributed, more random cooperation. Put differently, we’re introducing <em>randomness</em>, or <em>noise</em>, and no hyper-specialized model will be able to deal with that.</p>
<p>Application of this technique in <code>torch</code> is very straightforward. A dedicated layer takes care of it: <code>nn_dropout()</code>. By “takes care” I mean:</p>
<ul>
<li><p>In its <code>forward()</code> method, the layer checks whether we’re in training or test mode. If the latter, nothing happens.</p></li>
<li><p>If we’re training, it uses the dropout probability <span class="math inline">\(p\)</span> it’s been initialized with to zero out parts of the input tensor.</p></li>
<li><p>The partly-zeroed tensor is scaled up by the inverse of <span class="math inline">\(1-p\)</span>, to keep the overall magnitude of the tensor unchanged. The result is then passed on to the next layer.</p></li>
</ul>
<p>Here is our convnet from above, with a few dropout layers interspersed:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"/>convnet <span class="ot">&lt;-</span> <span class="fu">nn_module</span>(</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"/>  <span class="st">"convnet"</span>,</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"/>  </span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"/>  <span class="at">initialize =</span> <span class="cf">function</span>() {</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"/>    <span class="co"># nn_conv2d(in_channels, out_channels, kernel_size, stride)</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"/>    self<span class="sc">$</span>conv1 <span class="ot">&lt;-</span> <span class="fu">nn_conv2d</span>(<span class="dv">1</span>, <span class="dv">32</span>, <span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"/>    self<span class="sc">$</span>conv2 <span class="ot">&lt;-</span> <span class="fu">nn_conv2d</span>(<span class="dv">32</span>, <span class="dv">64</span>, <span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"/>    self<span class="sc">$</span>conv3 <span class="ot">&lt;-</span> <span class="fu">nn_conv2d</span>(<span class="dv">64</span>, <span class="dv">128</span>, <span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"/>    self<span class="sc">$</span>conv4 <span class="ot">&lt;-</span> <span class="fu">nn_conv2d</span>(<span class="dv">128</span>, <span class="dv">256</span>, <span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"/>    self<span class="sc">$</span>conv5 <span class="ot">&lt;-</span> <span class="fu">nn_conv2d</span>(<span class="dv">256</span>, <span class="dv">10</span>, <span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"/>    </span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"/>    self<span class="sc">$</span>drop1 <span class="ot">&lt;-</span> <span class="fu">nn_dropout</span>(<span class="at">p =</span> <span class="fl">0.2</span>)</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"/>    self<span class="sc">$</span>drop2 <span class="ot">&lt;-</span> <span class="fu">nn_dropout</span>(<span class="at">p =</span> <span class="fl">0.2</span>)</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"/>  },</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"/>  <span class="at">forward =</span> <span class="cf">function</span>(x) {</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"/>    x <span class="sc">%&gt;%</span> </span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"/>      self<span class="sc">$</span><span class="fu">conv1</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"/>      <span class="fu">nnf_relu</span>() <span class="sc">%&gt;%</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"/>      self<span class="sc">$</span><span class="fu">conv2</span>() <span class="sc">%&gt;%</span></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"/>      <span class="fu">nnf_relu</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"/>      self<span class="sc">$</span><span class="fu">drop1</span>() <span class="sc">%&gt;%</span></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"/>      self<span class="sc">$</span><span class="fu">conv3</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"/>      <span class="fu">nnf_relu</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"/>      self<span class="sc">$</span><span class="fu">conv4</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"/>      <span class="fu">nnf_relu</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"/>      self<span class="sc">$</span><span class="fu">drop2</span>() <span class="sc">%&gt;%</span></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"/>      self<span class="sc">$</span><span class="fu">conv5</span>() <span class="sc">%&gt;%</span></span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"/>      <span class="fu">torch_squeeze</span>()</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"/>  }</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"/>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>As always, experimentation will help in determining a good dropout rate, as well as the number of dropout layers introduced. You may be wondering, though – how does this technique go together with the data-related ones we presented before?</p>
<p>In practice (in the “real world”), you would basically always use data augmentation. As to dropout, it probably is <em>the</em> go-to technique in this area. A priori, there is no reason to <em>not</em> use them together – all the more since both are configurable. One way to see it is like this: You have a fixed budget of randomness; every technique you use that adds randomness will take up some of that budget. How do you know if you’ve exceeded it? By seeing no (or insufficient) progress on the training set. There is a clear ranking of priorities here: It’s no use worrying about generalization to the test set as long as the model is not learning at all. The number one requirement always is to get the model to learn in the first place.</p>
</section>
<section id="regularization" class="level3" data-number="16.3.2">
<h3 data-number="16.3.2" class="anchored" data-anchor-id="regularization"><span class="header-section-number">16.3.2</span> Regularization</h3>
<p>Both dropout and regularization affect how the model’s inner workings, but they are very different in spirit.</p>
<p>Dropout introduces randomness. Looking for analogies in machine learning overall, it has something in common with ensemble modeling. (By the way, the idea of ensembling is as applicable, in theory, to neural networks, as to other algorithms. It’s just not that popular because training a neural network takes a long time.)</p>
<p>Regularization, on the other hand, is similar to – regularization. If you know what is meant by this term in machine learning in general, you know what is meant in deep learning. In deep learning, though, it is often referred to by a different name: “weight decay”. Personally, I find this a little misleading. “Decay” seems to hint at some sort of <em>temporal</em> development; in fact, this is exactly its meaning in “learning rate decay”, a training strategy that makes use of decreasing learning rates over time.</p>
<p>In weight decay, or regularization, however, there’s no dynamics involved at all. Instead, we follow a fixed rule. That rule is: When computing the loss, add to it a quantity proportional to the aggregate size of the weights. The idea is to keep the weights small and homogeneous, to prevent sharp cliffs and canyons in the loss function.</p>
<p>In deep learning, regularization as a strategy is nowhere as central as data augmentation or dropout, which is why I’m not going to go into great detail. If you want to learn more, check out one of the many great introductions to statistical learning around.</p>
<p>Among data scientists, regularization probably is most often associated with different variants of linear regression. Variants differ in what they understand by a penalty “proportional” to the weights. In <em>ridge regression</em>, this quantity will be a fraction of the sum of the <em>squared</em> weights; in the <em>Lasso</em>, their absolute values. It is only the former algorithm that is implemented in <code>torch</code>.</p>
<p>Although semantically, regularization forms part of the “business logic” – which is why I’m listing it in the “model” section – it technically is implemented as part of an optimizer object. All of the classic optimizers – SGD, RMSProp, Adam, and relatives – take a <code>weight_decay</code> argument used to indicate what fraction of the sum of squared weights you’d like to have added to the loss.</p>
<p>In our example, you’d pass this argument to <code>luz::set_opt_hparams()</code>, like so:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"/>fitted <span class="ot">&lt;-</span> convnet <span class="sc">%&gt;%</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"/>  <span class="fu">setup</span>(</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"/>    <span class="at">loss =</span> <span class="fu">nn_cross_entropy_loss</span>(),</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"/>    <span class="at">optimizer =</span> optim_adam,</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"/>    <span class="at">metrics =</span> <span class="fu">list</span>(<span class="fu">luz_metric_accuracy</span>())</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"/>    ) <span class="sc">%&gt;%</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"/>  <span class="fu">set_opt_hparams</span>(<span class="at">weight_decay =</span> <span class="fl">0.00001</span>) <span class="sc">%&gt;%</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"/>  <span class="fu">fit</span>(train_dl, <span class="at">epochs =</span> <span class="dv">5</span>, <span class="at">valid_data =</span> valid_dl)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>As already hinted at above, regularization is not seen that often in the context of neural networks. Nevertheless, given the wide range of problems deep learning is applied to, it’s good to be aware of its availability.</p>
</section>
</section>
<section id="training-stage-early-stopping" class="level2" data-number="16.4">
<h2 data-number="16.4" class="anchored" data-anchor-id="training-stage-early-stopping"><span class="header-section-number">16.4</span> Training stage: Early stopping</h2>
<p>We conclude this chapter with an example of an anything-but-sophisticated, but very effective training technique: early stopping. So far, in our running example, we’ve always let the model learn for a pre-determined number of epochs. Thanks to the existence of callbacks – those “hooks” into the training process that let you modify “almost everything” dynamically – however, training duration does not have to be fixed from the outset.</p>
<p>One of several callbacks related to the learning rate – besides, e.g., <code>luz_callback_lr_scheduler()</code> , that allows you to dynamically adjust learning rate while training – <code>luz_callback_early_stopping()</code> will trigger an early exit once some configurable condition is satisfied.</p>
<p>Called without parameters, it will monitor loss on the validation set, and once validation loss stops decreasing, it will immediately cause training to stop. However, less strict policies are possible. For example, <code>luz_callback_early_stopping(patience = 2)</code> will allow for two consecutive epochs without improvement before triggering an exit.</p>
<p>To make use of <code>luz_callback_early_stopping()</code>, you add it to the callbacks list in <code>fit()</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"/>fitted <span class="ot">&lt;-</span> convnet <span class="sc">%&gt;%</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"/>  <span class="fu">setup</span>(</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"/>    <span class="at">loss =</span> <span class="fu">nn_cross_entropy_loss</span>(),</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"/>    <span class="at">optimizer =</span> optim_adam,</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"/>    <span class="at">metrics =</span> <span class="fu">list</span>(<span class="fu">luz_metric_accuracy</span>())</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"/>  ) <span class="sc">%&gt;%</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"/>  <span class="fu">fit</span>(train_dl,</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"/>    <span class="at">epochs =</span> <span class="dv">5</span>,</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"/>    <span class="at">valid_data =</span> valid_dl,</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"/>    <span class="at">callbacks =</span> <span class="fu">list</span>(</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"/>      <span class="fu">luz_callback_early_stopping</span>()</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"/>    )</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"/>  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>In deep learning, early stopping is ubiquitous; it’s hard to imagine why one would <em>not</em> want to use it.</p>
<p>Having discussed overfitting, we now go on to a complementary aspect of model training: True, we want our models to generalize; but we also want them to learn fast. That’s what the next chapter is dedicated to.</p>


<div id="refs" class="references csl-bib-body hanging-indent" role="list" style="display: none">
<div id="ref-10.5555/2627435.2670313" class="csl-entry" role="listitem">
Srivastava, Nitish, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. <span>“Dropout: A Simple Way to Prevent Neural Networks from Overfitting.”</span> <em>J. Mach. Learn. Res.</em> 15 (1): 1929–58.
</div>
<div id="ref-2017arXiv171009412Z" class="csl-entry" role="listitem">
Zhang, Hongyi, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. 2017. <span>“<span class="nocase">mixup: Beyond Empirical Risk Minimization</span>.”</span> <em>arXiv e-Prints</em>, October, arXiv:1710.09412. <a href="https://arxiv.org/abs/1710.09412">https://arxiv.org/abs/1710.09412</a>.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr/>
<ol>
<li id="fn1"><p>Meaning: “overly fitting” the model to the training data.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>I chose MNIST for this overview because it allows to easily discern the effects of various augmentation techniques (especially the next one we’re going to discuss).<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

    
</body>
</html>