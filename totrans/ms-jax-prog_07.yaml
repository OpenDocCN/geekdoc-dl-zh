- en: 'Chapter 6: Advanced Deep Learning Techniques with `Jax`'
  id: totrans-0
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: 第六章：使用 `Jax` 的高级深度学习技术
- en: '* * *'
  id: totrans-1
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '* * *'
- en: Welcome to the realm of advanced deep learning techniques with `Jax`, where
    we'll boost your neural networks to new heights. In this chapter, we explore tactics
    like regularization, dropout, batch normalization, and early stopping to fortify
    your models. Get ready to fine-tune your networks for unparalleled performance.
  id: totrans-2
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 欢迎来到使用 `Jax` 的高级深度学习技术的领域，我们将使你的神经网络达到新的高度。在本章中，我们探索正则化、dropout、批归一化和提前停止等策略，以增强模型。准备好为无与伦比的性能微调您的网络。
- en: 6.1 Exploring Regularization Techniques
  id: totrans-3
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 6.1 探索正则化技术
- en: Overfitting, a common challenge in machine learning, especially in neural networks,
    occurs when a model becomes too attuned to the training data, hindering its ability
    to generalize to new, unseen data. Regularization techniques step in as the superheroes,
    introducing constraints to prevent the model from memorizing the training data
    excessively and encouraging it to learn more broadly applicable patterns.
  id: totrans-4
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 过拟合是机器学习中常见的挑战，特别是在神经网络中，当模型过于适应训练数据时，会妨碍其泛化到新的、未见过的数据。正则化技术则如超级英雄般登场，引入约束条件，防止模型过度记忆训练数据，并鼓励其学习更广泛适用的模式。
- en: Common Regularization Techniques
  id: totrans-5
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 常见的正则化技术
- en: '1\. L1 and L2 Regularization: These techniques act as gatekeepers for the weights
    in the neural network. L1 regularization adds the absolute value of weights to
    the loss function, while L2 regularization squares the weights. This prompts the
    model to favor smaller weights, curbing complexity and preventing overfitting.'
  id: totrans-6
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '1\. L1 和 L2 正则化: 这些技术作为神经网络权重的守门员。L1 正则化将权重的绝对值加入损失函数，而 L2 正则化则对权重进行平方。这促使模型偏好较小的权重，减少复杂性并防止过拟合。'
- en: 'def `l1_regularization`(weights, alpha):'
  id: totrans-7
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '定义 `l1_regularization`(weights, alpha):'
- en: return `alpha * jnp.sum(jnp.abs(weights))`
  id: totrans-8
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 返回 `alpha * jnp.sum(jnp.abs(weights))`
- en: 'def `l2_regularization`(weights, beta):'
  id: totrans-9
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '定义 `l2_regularization`(weights, beta):'
- en: return `beta * jnp.sum(weights2)`
  id: totrans-10
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 返回 `beta * jnp.sum(weights2)`
- en: '2\. Dropout: Enter Dropout, the disruptor of routines. It randomly sidelines
    neurons during training, pushing the remaining neurons to learn robust representations
    less reliant on individual ones.'
  id: totrans-11
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '2\. Dropout: 进入 Dropout，打破常规。它在训练过程中随机使部分神经元失效，推动其余神经元学习更强大的表示，减少对个别神经元的依赖。'
- en: 'def `dropout_layer`(x, dropout_prob, is_training):'
  id: totrans-12
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '定义 `dropout_layer`(x, dropout_prob, is_training):'
- en: 'if `is_training`:'
  id: totrans-13
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '如果 `is_training`:'
- en: '`mask = jax.random.bernoulli(jax.random.PRNGKey(0), dropout_prob, x.shape)`'
  id: totrans-14
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`mask = jax.random.bernoulli(jax.random.PRNGKey(0), dropout_prob, x.shape)`'
- en: return `x * mask / (1 - dropout_prob)`
  id: totrans-15
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 返回 `x * mask / (1 - dropout_prob)`
- en: 'else:'
  id: totrans-16
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'else:'
- en: return `x`
  id: totrans-17
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 返回 `x`
- en: '3\. Early Stopping: Acting as a vigilant guardian, Early Stopping keeps tabs
    on the model''s performance on a validation set, bringing training to a halt when
    the model''s prowess starts to wane.'
  id: totrans-18
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '3\. Early Stopping: 作为一名警惕的守护者，Early Stopping 监视模型在验证集上的表现，当模型表现开始下降时，会终止训练。'
- en: 'def `early_stopping`(validation_loss, patience, best_loss, counter):'
  id: totrans-19
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '定义 `early_stopping`(validation_loss, patience, best_loss, counter):'
- en: 'if `validation_loss < best_loss`:'
  id: totrans-20
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '如果 `validation_loss < best_loss`:'
- en: best_loss = validation_loss
  id: totrans-21
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: best_loss = validation_loss
- en: counter = 0
  id: totrans-22
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: counter = 0
- en: 'else:'
  id: totrans-23
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'else:'
- en: counter += 1
  id: totrans-24
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: counter += 1
- en: return `best_loss, counter, counter >= patience`
  id: totrans-25
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 返回 `best_loss, counter, counter >= patience`
- en: Implementation of Regularization Techniques
  id: totrans-26
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 正则化技术的实施
- en: '1\. L1 and L2 Regularization: `Jax` provides built-in functionality for L1
    and L2 regularization. When defining the loss function, add a regularization term
    penalizing the weights based on the chosen method.'
  id: totrans-27
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '1\. L1 和 L2 正则化: `Jax` 提供了内置的 L1 和 L2 正则化功能。在定义损失函数时，添加一个根据所选方法惩罚权重的正则化项。'
- en: '2\. Dropout: `Jax''s jax.nn.dropout` makes dropout implementation seamless.
    Apply dropout to specific layers by setting the `dropout_probability` argument.'
  id: totrans-28
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '2\. Dropout: `Jax` 的 `jax.nn.dropout` 使得 dropout 的实现变得无缝。通过设置 `dropout_probability`
    参数，可以将 dropout 应用于特定的层。'
- en: '3\. Early Stopping: Monitor model performance on the validation set using metrics
    like accuracy or loss. Stop training when validation performance falters.'
  id: totrans-29
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '3\. Early Stopping: 使用像准确率或损失这样的指标监控模型在验证集上的表现。当验证性能下降时停止训练。'
- en: Benefits of Regularization
  id: totrans-30
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 正则化的好处
- en: '1\. Improved Generalization: Regularization prevents overfitting, enhancing
    performance on unseen data.'
  id: totrans-31
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '1\. 改进的泛化能力: 正则化防止过拟合，提升对未见数据的性能。'
- en: '2\. Reduced Complexity: Regularization cuts model complexity, making it less
    prone to memorizing training data and more adept at learning generalizable patterns.'
  id: totrans-32
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`2. 减少复杂度：正则化削减模型复杂度，减少过度记忆训练数据，更擅长学习普遍适用的模式。`'
- en: 3\. 增强可解释性：通过减少重要权重来提升模型可解释性，提供对模型决策过程的洞察。
  id: totrans-33
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 3\. 增强可解释性：通过减少重要权重来提升模型可解释性，提供对模型决策过程的洞察。
- en: 正则化技术作为防止过拟合的坚定捍卫者，在神经网络中促进更好的泛化。通过采用L1和L2正则化、dropout和early stopping，您巩固了您的模型，确保它们在各种任务中表现出色。防范过拟合，让您的模型大放异彩！
  id: totrans-34
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 正则化技术作为防止过拟合的坚定捍卫者，在神经网络中促进更好的泛化。通过采用L1和L2正则化、dropout和early stopping，您巩固了您的模型，确保它们在各种任务中表现出色。防范过拟合，让您的模型大放异彩！
- en: '`6.2 Techniques for Neural Network Regularization`'
  id: totrans-35
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '`6.2 神经网络正则化技术`'
- en: 在深度学习的动态景观中，正则化崛起为英雄，对抗过拟合的大敌——即神经网络过于依赖训练数据，从而影响其在新数据上的表现。让我们探索三位坚定的守护者——dropout、批标准化和early
    stopping，它们运用有效策略抵御过拟合，增强神经网络的泛化能力。
  id: totrans-36
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在深度学习的动态景观中，正则化崛起为英雄，对抗过拟合的大敌——即神经网络过度依赖训练数据，从而影响其在新数据上的表现。让我们探索三位坚定的守护者——dropout、批标准化和early
    stopping，它们运用有效策略抵御过拟合，增强神经网络的泛化能力。
- en: '`Dropout`：打造稳健的表示'
  id: totrans-37
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Dropout`：打造稳健的表示'
- en: '`Dropout`，一个随机的奇迹，训练过程中随机剔除神经元。这迫使网络形成稳健的表示，不过度依赖单个神经元，从而减少过拟合并增强泛化能力。'
  id: totrans-38
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Dropout`，一个随机的奇迹，训练过程中随机剔除神经元。这迫使网络形成稳健的表示，不过度依赖单个神经元，从而减少过拟合并增强泛化能力。'
- en: 'def `apply_dropout`(x, dropout_prob, is_training):'
  id: totrans-39
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def `apply_dropout`(x, dropout_prob, is_training):'
- en: 如果正在训练：
  id: totrans-40
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 如果正在训练：
- en: '`mask = jax.random.bernoulli(jax.random.PRNGKey(0), dropout_prob, x.shape)`'
  id: totrans-41
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`mask = jax.random.bernoulli(jax.random.PRNGKey(0), dropout_prob, x.shape)`'
- en: 返回`x * mask / (1 - dropout_prob)`
  id: totrans-42
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 返回`x * mask / (1 - dropout_prob)`
- en: 否则：
  id: totrans-43
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 否则：
- en: 返回`x`
  id: totrans-44
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 返回`x`
- en: 批标准化：稳固前行
  id: totrans-45
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 批标准化：稳固前行
- en: 批标准化接管，规范化层激活以维持跨批次的稳定输入分布。这稳定训练，增强梯度流动，加速收敛，为卓越性能铺平道路。
  id: totrans-46
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 批标准化接管，规范化层激活以维持跨批次的稳定输入分布。这稳定训练，增强梯度流动，加速收敛，为卓越性能铺平道路。
- en: 'def `apply_batch_norm`(x, running_mean, running_var, is_training):'
  id: totrans-47
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def `apply_batch_norm`(x, running_mean, running_var, is_training):'
- en: '`mean, var = jnp.mean(x, axis=0), jnp.var(x, axis=0)`'
  id: totrans-48
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`mean, var = jnp.mean(x, axis=0), jnp.var(x, axis=0)`'
- en: 如果正在训练：
  id: totrans-49
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 如果正在训练：
- en: '# 在训练期间更新运行统计信息'
  id: totrans-50
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '# 在训练期间更新运行统计信息'
- en: '`running_mean = momentum * running_mean + (1 - momentum) * mean`'
  id: totrans-51
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`running_mean = momentum * running_mean + (1 - momentum) * mean`'
- en: '`running_var = momentum * running_var + (1 - momentum) * var`'
  id: totrans-52
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`running_var = momentum * running_var + (1 - momentum) * var`'
- en: '`normalized_x = (x - mean) / jnp.sqrt(var + epsilon)`'
  id: totrans-53
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`normalized_x = (x - mean) / jnp.sqrt(var + epsilon)`'
- en: 返回`gamma * normalized_x + beta`
  id: totrans-54
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 返回`gamma * normalized_x + beta`
- en: '`Early Stopping`：泛化的守护者'
  id: totrans-55
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Early Stopping`：泛化的守护者'
- en: '`Early stopping`充当守卫，监控模型在验证集上的表现。一旦出现性能下降的迹象，训练停止。这种预见性的干预防止模型过度依赖训练数据，保持其泛化能力。'
  id: totrans-56
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Early stopping`充当守卫，监控模型在验证集上的表现。一旦出现性能下降的迹象，训练停止。这种预见性的干预防止模型过度依赖训练数据，保持其泛化能力。'
- en: 'def `early_stopping`(validation_loss, patience, best_loss, counter):'
  id: totrans-57
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def `early_stopping`(validation_loss, patience, best_loss, counter):'
- en: 如果`validation_loss < best_loss`：
  id: totrans-58
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 如果`validation_loss < best_loss`：
- en: '`best_loss = validation_loss`'
  id: totrans-59
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`best_loss = validation_loss`'
- en: '`counter = 0`'
  id: totrans-60
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`counter = 0`'
- en: 否则：
  id: totrans-61
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 否则：
- en: '`counter += 1`'
  id: totrans-62
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`counter += 1`'
- en: 返回`best_loss, counter, counter >= patience`
  id: totrans-63
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 返回`best_loss, counter, counter >= patience`
- en: 实现行动中
  id: totrans-64
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 实现行动中
- en: 1\. `Dropout`：使用Jax的`jax.nn.dropout`函数。设置dropout概率以定义要丢弃的神经元的百分比。
  id: totrans-65
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 1\. `Dropout`：使用Jax的`jax.nn.dropout`函数。设置dropout概率以定义要丢弃的神经元的百分比。
- en: 2\. 批标准化：利用`jax.nn.batch_norm`。提供输入张量和表示平均值和方差的张量元组，通常使用运行批次统计计算。
  id: totrans-66
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 2\. 批标准化：利用`jax.nn.batch_norm`。提供输入张量和表示平均值和方差的张量元组，通常使用运行批次统计计算。
- en: '`3. Early Stopping: Craft a callback monitoring validation performance. When
    the performance stagnates for a specified epoch count, the callback halts training.`'
  id: totrans-67
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`3. Early Stopping: 设计一个回调函数，监控验证性能。当性能在指定的时期内停滞时，回调函数停止训练。`'
- en: '`Benefits of the Techniques`'
  id: totrans-68
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`技术的好处`'
- en: '`1. Improved Generalization: These techniques elevate model performance on
    unseen data by curbing overfitting.`'
  id: totrans-69
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`1. 提升泛化能力：这些技术通过抑制过拟合，提高模型在未见数据上的表现。`'
- en: '`2. Reduced Complexity: Complexity reduction discourages over-memorization,
    steering models toward learning broadly applicable patterns.`'
  id: totrans-70
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`2. 减少复杂度：简化模型结构，减少过度记忆，使模型能够学习更广泛适用的模式。`'
- en: '`3. Enhanced Interpretability: Slimming down significant weights promotes model
    interpretability, unraveling decision-making processes.`'
  id: totrans-71
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`3. 增强可解释性：减少重要权重，提高模型可解释性，揭示决策过程。`'
- en: '`Dropout, batch normalization, and early stopping stand as formidable guardians
    against overfitting, elevating the generalization prowess of neural networks.`'
  id: totrans-72
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Dropout, batch normalization, and early stopping stand as formidable guardians
    against overfitting, elevating the generalization prowess of neural networks.`'
- en: '`6.3 Hyperparameter Tuning for Optimal Model Performance`'
  id: totrans-73
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '`6.3 超参数调整以实现最佳模型性能`'
- en: '`In neural networks, hyperparameters wield immense power, influencing a model''s
    performance. Tuning these parameters, like learning rate, regularization strength,
    and batch size acts as a conductor orchestrating a model''s success. Here''s a
    peek into how different techniques fine-tune these levers for optimal neural network
    performance.`'
  id: totrans-74
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`在神经网络中，超参数具有巨大的影响力，影响模型的性能。调整这些参数，如学习率、正则化强度和批大小，就像指挥一台乐器，指导模型的成功。这里展示了不同技术如何微调这些杠杆，以获得最佳的神经网络性能。`'
- en: '`Common Hyperparameter Tuning Techniques`'
  id: totrans-75
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`常见的超参数调整技术`'
- en: '`1. Grid Search: This meticulous method evaluates predefined hyperparameter
    values exhaustively. It selects the combination that shines brightest in terms
    of performance. However, its thoroughness comes with computational demands.`'
  id: totrans-76
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '-   1\. 网格搜索：这种细致的方法详尽地评估预定义的超参数值。它选择在性能上表现最好的组合。然而，它的彻底性带来了计算上的需求。'
- en: '`from sklearn.model_selection import GridSearchCV`'
  id: totrans-77
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from sklearn.model_selection import GridSearchCV`'
- en: '`from sklearn.svm import SVC`'
  id: totrans-78
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from sklearn.svm import SVC`'
- en: '`param_grid = {''C'': [0.1, 1, 10, 100], ''gamma'': [1, 0.1, 0.01, 0.001],
    ''kernel'': [''rbf'', ''linear'']}`'
  id: totrans-79
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`param_grid = {''C'': [0.1, 1, 10, 100], ''gamma'': [1, 0.1, 0.01, 0.001],
    ''kernel'': [''rbf'', ''linear'']}`'
- en: '`grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=2)`'
  id: totrans-80
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=2)`'
- en: '`grid.fit(X_train, y_train)`'
  id: totrans-81
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`grid.fit(X_train, y_train)`'
- en: '`print(grid.best_params_)`'
  id: totrans-82
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`print(grid.best_params_)`'
- en: '`2. Random Search: A less intensive approach, random search randomly explores
    hyperparameter values within a range. It embraces serendipity, opting for the
    best-found combo. Though less taxing, it might miss nuances in the parameter space.`'
  id: totrans-83
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`2\. Random Search: A less intensive approach, random search randomly explores
    hyperparameter values within a range. It embraces serendipity, opting for the
    best-found combo. Though less taxing, it might miss nuances in the parameter space.`'
- en: '`from sklearn.model_selection import RandomizedSearchCV`'
  id: totrans-84
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from sklearn.model_selection import RandomizedSearchCV`'
- en: '`from scipy.stats import uniform`'
  id: totrans-85
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from scipy.stats import uniform`'
- en: '`param_dist = {''C'': uniform(0, 4), ''gamma'': uniform(0, 0.1), ''kernel'':
    [''rbf'', ''linear'']}`'
  id: totrans-86
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`param_dist = {''C'': uniform(0, 4), ''gamma'': uniform(0, 0.1), ''kernel'':
    [''rbf'', ''linear'']}`'
- en: '`random_search = RandomizedSearchCV(SVC(), param_distributions=param_dist,
    n_iter=10, random_state=42)`'
  id: totrans-87
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`random_search = RandomizedSearchCV(SVC(), param_distributions=param_dist,
    n_iter=10, random_state=42)`'
- en: '`random_search.fit(X_train, y_train)`'
  id: totrans-88
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`random_search.fit(X_train, y_train)`'
- en: '`print(random_search.best_params_)`'
  id: totrans-89
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`print(random_search.best_params_)`'
- en: '`3. Bayesian Optimization: A sophisticated strategy that employs a probabilistic
    model to guide the search. It focuses on zones with higher performance potential,
    offering efficiency without compromising exploration depth.`'
  id: totrans-90
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '-   3\. 贝叶斯优化：一种复杂的策略，利用概率模型引导搜索。它专注于性能潜力更高的区域，提供高效率而不影响探索深度。'
- en: '`from skopt import BayesSearchCV`'
  id: totrans-91
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from skopt import BayesSearchCV`'
- en: '`param_bayes = {''C'': (0.1, 100), ''gamma'': (0.01, 1.0), ''kernel'': [''rbf'',
    ''linear'']}`'
  id: totrans-92
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`param_bayes = {''C'': (0.1, 100), ''gamma'': (0.01, 1.0), ''kernel'': [''rbf'',
    ''linear'']}`'
- en: '`bayesian_search = BayesSearchCV(SVC(), param_bayes, n_iter=50, random_state=42)`'
  id: totrans-93
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`bayesian_search = BayesSearchCV(SVC(), param_bayes, n_iter=50, random_state=42)`'
- en: '`bayesian_search.fit(X_train, y_train)`'
  id: totrans-94
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`bayesian_search.fit(X_train, y_train)`'
- en: '`print(bayesian_search.best_params_)`'
  id: totrans-95
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`print(bayesian_search.best_params_)`'
- en: '`Strategies for Hyperparameter Tuning`'
  id: totrans-96
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 超参数调优策略
- en: '`1. Define a Performance Metric: Handpick a performance metric like accuracy
    or loss to gauge the model''s prowess during hyperparameter tuning.`'
  id: totrans-97
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '-   1\. 定义性能度量：选择一个性能度量指标，如准确率或损失，来评估超参数调优过程中模型的表现。'
- en: 2\. Setting Tuning Goals: Clearly articulate the goals for tuning whether it's
    boosting accuracy, trimming loss, or refining generalization.
  id: totrans-98
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '-   2\. 设置调优目标：明确调优的目标，无论是提升准确率、减少损失还是优化泛化能力。'
- en: 3\. Selecting the Right Technique: Choose the hyperparameter tuning technique
    best suited to your resources and exploration objectives.
  id: totrans-99
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '-   3\. 选择合适的技术：选择最适合你资源和探索目标的超参数调优技术。'
- en: 4\. Evaluate and Enhance: Assess the model's performance across different hyperparameter
    combinations. Refine your strategy based on these evaluations.
  id: totrans-100
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '-   4\. 评估和改进：评估模型在不同超参数组合下的性能。根据这些评估结果优化你的策略。'
- en: Benefits of Hyperparameter Tuning
  id: totrans-101
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 超参数调优的好处
- en: 1\. Peak Model Performance: Optimizing hyperparameters can drastically elevate
    a model's performance by zeroing in on the best parameter combination.
  id: totrans-102
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '-   1\. 达到最佳模型性能：通过优化超参数，可以显著提升模型的性能，找到最佳的参数组合。'
- en: 2\. Trimmed Training Time: Tuning parameters not only improves performance but
    also speeds up the training process, boosting overall efficiency.
  id: totrans-103
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '-   2\. 缩短训练时间：调优参数不仅提升性能，还加快了训练过程，提高了整体效率。'
- en: 3\. Enhanced Generalization: Fine-tuning parameters augments a model's ability
    to generalize, rendering better performance on unseen data.
  id: totrans-104
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '-   3\. 提升泛化能力：微调参数可以增强模型对未见数据的泛化能力，从而在未知数据上表现更好。'
- en: Hyperparameter tuning emerges as the linchpin in optimizing neural networks.
    By implementing various tuning techniques, meticulously evaluating a model's performance,
    and strategically navigating hyperparameter spaces, you unearth the optimal parameter
    combination. This translates into supercharged models, aligned precisely with
    your performance objectives.
  id: totrans-105
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 调参作为优化神经网络的关键，通过实施各种调参技术，精确评估模型的表现，并在超参数空间中进行策略性导航，你能找到最优的参数组合。这将转化为性能目标完美契合的超级模型。
- en: Congratulations! You've journeyed through the realm of advanced Jax techniques.
    Now armed with tools against overfitting, and strategies like dropout and batch
    normalization, you're prepared to optimize models. The art of hyperparameter tuning
    is at your fingertips.
  id: totrans-106
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 祝贺！你已经穿越了高级 Jax 技术的境界。现在，你拥有了应对过拟合的工具，以及像 dropout 和批归一化这样的策略，你已经准备好优化模型了。调参艺术尽在你掌握之中。
