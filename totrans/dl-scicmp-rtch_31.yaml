- en: '24  Matrix computations: Least-squares problems'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/matrix_computations_leastsquares.html](https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/matrix_computations_leastsquares.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In this chapter and the next, we’ll explore what `torch` lets us do with matrices.
    Here, we take a look at various ways to solve least-squares problems. The intention
    is two-fold.
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, this subject often gets pretty technical, or rather, *computational*,
    very fast. Depending on your background (and goals), this may just be what you
    want; you either know well, or do not care about so much, the underlying concepts.
    But for some people, a purely technical presentation, one that does not also dwell
    on the *concepts*, the abstract ideas underlying the subject, may well fail to
    convey the fascination, the intellectual attraction it can exert. That’s why,
    in this chapter, I’ll try to present things in a way that the main ideas don’t
    get obscured by “computer-sciencey” details (details that are easily found in
    a number of excellent books, anyway).
  prefs: []
  type: TYPE_NORMAL
- en: 24.1 Five ways to do least squares
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'How do you compute linear least-squares regression? In R, using `lm()`; in
    `torch`, there is `linalg_lstsq()`. Where R, sometimes, hides complexity from
    the user, high-performance computation frameworks like `torch` tend to ask a bit
    more up-front effort, be it careful reading of documentation, or playing around
    some, or both. For example, here is the central piece of documentation for `linalg_lstsq()`,
    elaborating on the `driver` parameter to the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '`driver` chooses the LAPACK/MAGMA function that will be used.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For CPU inputs the valid values are ‘gels’, ‘gelsy’, ‘gelsd, ’gelss’.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For CUDA input, the only valid driver is ‘gels’, which assumes that A is full-rank.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'To choose the best driver on CPU consider:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'If A is well-conditioned (its condition number is not too large), or you do
    not mind some precision loss:'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For a general matrix: ‘gelsy’ (QR with pivoting) (default)'
  prefs:
  - PREF_BQ
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'If A is full-rank: ‘gels’ (QR)'
  prefs:
  - PREF_BQ
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'If A is not well-conditioned:'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ‘gelsd’ (tridiagonal reduction and SVD)
  prefs:
  - PREF_BQ
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'But if you run into memory issues: ‘gelss’ (full SVD).'
  prefs:
  - PREF_BQ
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether you’ll need to know this will depend on the problem you’re solving.
    But if you do, it certainly will help to have an idea what is being talked about
    there, if only in a high-level way.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example problem below, we’re going to be lucky. All drivers will return
    the same result – but only once we’ll have applied a “trick”, of sorts. Still,
    we’ll go on and dig deeper into the various methods used by `linalg_lstsq()`,
    as well as a few others of common use. Concretely, we’ll solve least squares:'
  prefs: []
  type: TYPE_NORMAL
- en: By means of the so-called *normal equations*, the most direct way, in the sense
    that it immediately results from a mathematical statement of the problem.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Again, starting from the normal equations, but making use of *Cholesky factorization*
    in solving them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Yet again, taking the normal equations for a point of departure, but proceeding
    by means of *LU* decomposition.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fourth, employing another type of factorization – *QR* – that, together with
    the final one, accounts for the vast majority of decompositions applied “in the
    real world”. With QR decomposition, the solution algorithm does not start from
    the normal equations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: And fifth and finally, making use of *Singular Value Decomposition* (SVD). Here,
    too, the normal equations are not needed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All methods will first be applied to a real-world dataset, and then, be tested
    on a benchmark problem well known for its lack of stability.
  prefs: []
  type: TYPE_NORMAL
- en: 24.2 Regression for weather prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The dataset we’ll use is available from the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/machine-learning-dAtAbases/00514/Bias_correction_ucl.csv).
    The way we’ll use it does not quite match the original purpose of collection;
    instead of forecasting temperature with machine learning, the original study (Cho
    et al. ([2020](references.html#ref-2019EA000740))) really was about bias correction
    of forecasts obtained from a numerical weather prediction model. But never mind
    – our focus here is on matrix methods, and the dataset lends itself very well
    to the kinds of explorations we’re going to do.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE1]'
  prefs: []
  type: TYPE_NORMAL
- en: The way we’re framing the task, basically everything in the dataset serves (or
    would serve, if we kept it – more on that below) as a predictor. As target, we’ll
    use `Next_Tmax`, the maximal temperature reached on the subsequent day. This means
    we need to remove `Next_Tmin` from the set of predictors, as it would make for
    too powerful of a clue. We’ll do the same for `station`, the weather station id,
    and `Date`. This leaves us with twenty-one predictors, including measurements
    of actual temperature (`Present_Tmax`, `Present_Tmin`), model forecasts of various
    variables (`LDAPS_*`), and auxiliary information (`lat`, `lon`, and `` `Solar
    radiation` ``, among others).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '*Note how, above, I’ve added a line to *standardize* the predictors. This is
    the “trick” I was alluding to above. We’ll talk about why we’re doing this soon.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For `torch`, we split up the data into two tensors: a matrix `A`, containing
    all predictors, and a vector `b` that holds the target.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE4]'
  prefs: []
  type: TYPE_NORMAL
- en: Now, first let’s determine the expected output.
  prefs: []
  type: TYPE_NORMAL
- en: '24.2.1 Least squares (I): Setting expectations with `lm()`'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If there’s a least squares implementation we “believe in”, it surely must be
    `lm()`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE6]'
  prefs: []
  type: TYPE_NORMAL
- en: 'With an explained variance of 78%, the forecast is working pretty well. This
    is the baseline we want to check all other methods against. To that purpose, we’ll
    store respective predictions and prediction errors (the latter being operationalized
    as root mean squared error, RMSE). For now, we just have entries for `lm()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE8]**  **### 24.2.2 Least squares (II): Using `linalg_lstsq()`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, for a moment let’s assume this was not about exploring different approaches,
    but getting a quick result. In `torch`, we have `linalg_lstsq()`, a function dedicated
    specifically to solving least-squares problems. (This is the function whose documentation
    I was citing, above.) Just like we did with `lm()`, we’d probably just go ahead
    and call it, making use of the default settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE10]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Predictions resemble those of `lm()` very closely – so closely, in fact, that
    we may guess those tiny differences are just due to numerical errors surfacing
    from deep down the respective call stacks. RMSE, thus, should be equal as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE12]'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is; and this is a satisfying outcome. However, it only really came about
    due to that “trick”: normalization. Of course, when I say “trick”, I don’t really
    mean it. Standardizing the data is a common operation, and especially with neural
    networks, it tends to get used routinely, to speed up training. The point I’d
    like to make is this: Frameworks for high-performance computation, like `torch`,
    will often presuppose more domain knowledge, or more up-front analysis, on the
    part of the user.'
  prefs: []
  type: TYPE_NORMAL
- en: 'I’ll explain.**  **### 24.2.3 Interlude: What if we hadn’t standardized the
    data?'
  prefs: []
  type: TYPE_NORMAL
- en: 'For quick comparison, let’s create an alternate matrix of predictors: *not*
    normalizing the data, this time.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '*To set our expectations, we again call `lm()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE15]'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we call `linalg_lstsq()`, using the default arguments just like we did
    before.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE17]'
  prefs: []
  type: TYPE_NORMAL
- en: Wow – what happened here? Thinking back of that piece of documentation I’ve
    cited, maybe the default arguments aren’t working out that well, this time. Let’s
    find out why.
  prefs: []
  type: TYPE_NORMAL
- en: 24.2.3.1 Investigating “the issue”
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To efficiently solve a linear least-squares problem, `torch` calls into LAPACK,
    a set of Fortran routines designed to efficiently and scaleably address the tasks
    most frequently found in linear algebra: solving linear systems of equations,
    computing eigenvectors and eigenvalues, and determining singular values.'
  prefs: []
  type: TYPE_NORMAL
- en: The allowed `driver`s in `linalg_lstsq()` correspond to different LAPACK procedures[¹](#fn1),
    and these procedures all apply different algorithms in order to solve the problem
    – analogously to what’ll we do ourselves, below.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, in investigating what is going on, step one is to determine which method
    gets used and why; analyse (if possible) why the result is unsatisfying; determine
    the LAPACK routine we’d like to be using instead, and check what happens if indeed
    we do. (Of course, given the little effort involved, we’d probably give all methods
    a try.)
  prefs: []
  type: TYPE_NORMAL
- en: The main concept involved here is the *rank* of a matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '24.2.3.2 Concepts (I): Rank of a matrix'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*“But wait!”* you may be thinking – from the above-cited piece of documentation,
    it seems like the first thing we should check is not rank, but *condition number*:
    whether the matrix is “well-conditioned”. Yes, the condition number certainly
    is important, and we’ll get back to it very soon. However, there is something
    even more fundamental at work here, something that does not really “jump to the
    eye”.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The central piece of information is found in that LAPACK piece of documentation
    we’re being referred to by `linalg_lstsq()`. Between the four routines GELS, GELSY,
    GELSD, and GELSS, differences are not restricted to implementation. The *goal*
    of optimization differs, as well. The rationale is the following. Throughout,
    let’s assume we’re working with a matrix that has more rows than columns (more
    observations than features, in the most-frequent case):'
  prefs: []
  type: TYPE_NORMAL
- en: If the matrix is full-rank – meaning, its columns are linearly independent –
    there is no “perfect” solution. The problem is over-determined. All we can do
    is find the best possible approximation. This is done by minimizing the prediction
    error – we’ll come back to that when discussing the normal equations. Minimize
    prediction error is what the GELS routine does, and it is GELS we should use when
    we have a full-rank matrix of predictors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If the matrix is not full-rank, the problem is under-determined; there is an
    infinite number of solutions. All the remaining routines – GELSY, GELSD, and GELSS
    – are suited to this situation. While they do proceed differently, they all pursue
    the same strategy, different from the one followed by GELS: In addition to the
    prediction error, they *also* minimize the vector of coefficients. This is called
    finding a minimum-norm least-squares solution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In sum, GELS (for full-rank matrices) and the three of GELSY, GELSD, and GELSS
    (for when the matrix is rank-deficient) intentionally follow different optimization
    criteria.
  prefs: []
  type: TYPE_NORMAL
- en: Now, as per the documentation for `linalg_lstsq()`, when no `driver` is passed
    explicitly, it is GELSY that gets called. That should be fine if our matrix is
    rank-deficient – but is it?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE19]'
  prefs: []
  type: TYPE_NORMAL
- en: The matrix has twenty-one columns; so if its rank is twenty-one, then it is
    full-rank for sure. We definitely want to be calling the GELS routine.*  *####
    24.2.3.3 Calling `linalg_lstsq()` the right way
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we know what to pass for `driver`, here is the modified call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE21]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the respective RMSE values are very close. You’ll be wondering, though:
    Why didn’t we have to specify the Fortran routine when working with the *standardized*
    matrix?*  *#### 24.2.3.4 Why did standardization help?'
  prefs: []
  type: TYPE_NORMAL
- en: 'For our matrix, what standardization did was reduce significantly the range
    spanned by the singular values. With `A`, the standardized matrix, the largest
    singular value is about ten times as large as the smallest one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE23]'
  prefs: []
  type: TYPE_NORMAL
- en: 'While with `A_alt`, it is a million times as large:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE25]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Why is this important? It’s here that we finally get back to the *condition
    number*.**  **#### 24.2.3.5 Concepts (II): Condition number'
  prefs: []
  type: TYPE_NORMAL
- en: The higher the so-called *condition number* of a matrix, the more likely we
    are to run into problems of numerical stability when computing with it. In torch,
    `linalg_cond()` is used to obtain the condition number. Let’s compare the condition
    numbers for `A` and `A_alt`, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE27]'
  prefs: []
  type: TYPE_NORMAL
- en: That is quite a difference! How does it arise?
  prefs: []
  type: TYPE_NORMAL
- en: 'The condition number is defined as the matrix norm of `A`, divided by the norm
    of its inverse. Different kinds of norm may be used; the default is the 2-norm.
    In this case, condition number can be computed from the matrix’s singular values:
    Namely, the 2-norm of `A` equals the largest singular value, while that of its
    inverse is given by the smallest one.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can verify this ourselves, using `linalg_svdvals()` as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE29]'
  prefs: []
  type: TYPE_NORMAL
- en: To reiterate, this is a substantial difference. Incidentally, do you remember
    that in the case of `A_alt`, RMSE was a tiny bit worse for `linalg_lstsq()` than
    for `lm()`, even when using the appropriate routine, GELS? Given that both essentially
    use the same algorithm (QR factorization, to be introduced very soon) this may
    very well have been due to numerical errors, arising from the high condition number
    of `A_alt`.
  prefs: []
  type: TYPE_NORMAL
- en: 'By now, I may have convinced you that with `torch`’s `linalg` component, it
    helps to know a bit about how the most-in-use least-squares algorithms work. Let’s
    get acquainted.*********  ***### 24.2.4 Least squares (III): The normal equations'
  prefs: []
  type: TYPE_NORMAL
- en: We start by stating the goal. Given a matrix, \(\mathbf{A}\), that holds features
    in its columns and observations in its rows, and a vector of observed outcomes,
    \(\mathbf{b}\), we want to find regression coefficients, one for each feature,
    that allow to approximate \(\mathbf{b}\) as well as possible. Call the vector
    of regression coefficients \(\mathbf{x}\). To obtain it, we need to solve a simultaneous
    system of equations, that in matrix notation appears as
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{Ax} = \mathbf{b} \]
  prefs: []
  type: TYPE_NORMAL
- en: If \(\mathbf{b}\) were a square, invertible matrix, the solution could directly
    be computed as \(\mathbf{x} = \mathbf{A}^{-1}\mathbf{b}\). This will hardly ever
    be possible, though; we’ll (hopefully) always have more observations than predictors.
    Another approach is needed. It directly starts from the problem statement.
  prefs: []
  type: TYPE_NORMAL
- en: When we use the columns of \(\mathbf{A}\) to approximate \(\mathbf{b}\), that
    approximation necessarily is in the column space of \(\mathbf{A}\). \(\mathbf{b}\),
    on the other hand, normally won’t be. We want those two to be as close as possible;
    in other words, we want to minimize the distance between them. Choosing the 2-norm
    for the distance, this yields the objective
  prefs: []
  type: TYPE_NORMAL
- en: \[ minimize \ ||\mathbf{Ax}-\mathbf{b}||^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: 'This distance is the (squared) length of the vector of prediction errors. That
    vector necessarily is orthogonal to \(\mathbf{A}\) itself. That is, when we multiply
    it with \(\mathbf{A}\), we get the zero vector:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{A}^T(\mathbf{Ax} - \mathbf{b}) = \mathbf{0} \]
  prefs: []
  type: TYPE_NORMAL
- en: 'A rearrangement of this equation yields the so-called *normal equations*:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{A}^T \mathbf{A} \mathbf{x} = \mathbf{A}^T \mathbf{b} \]
  prefs: []
  type: TYPE_NORMAL
- en: 'These may be solved for \(\mathbf{x}\), computing the inverse of \(\mathbf{A}^T\mathbf{A}\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{x} = (\mathbf{A}^T \mathbf{A})^{-1} \mathbf{A}^T \mathbf{b} \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\mathbf{A}^T\mathbf{A}\) is a square matrix. It still might not be invertible,
    in which case the so-called pseudoinverse would be computed instead. In our case,
    this will not be needed; we already know \(\mathbf{A}\) has full rank, and so
    does \(\mathbf{A}^T\mathbf{A}\).
  prefs: []
  type: TYPE_NORMAL
- en: Thus, from the normal equations we have derived a recipe for computing \(\mathbf{b}\).
    Let’s put it to use, and compare with what we got from `lm()` and `linalg_lstsq()`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE31]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Having confirmed that the direct way works, we may allow ourselves some sophistication.
    Four different matrix factorizations will make their appearance: Cholesky, LU,
    QR, and Singular Value Decomposition. The goal, in every case, is to avoid the
    expensive computation of the (pseudo-) inverse. That’s what all methods have in
    common. However, they do not differ “just” in the way the matrix is factorized,
    but also, in *which* matrix is. This has to do with the constraints the various
    methods impose. Roughly speaking, the order they’re listed in above reflects a
    falling slope of preconditions, or put differently, a rising slope of generality.
    Due to the constraints involved, the first two (Cholesky, as well as LU decomposition)
    will be performed on \(\mathbf{A}^T\mathbf{A}\), while the latter two (QR and
    SVD) operate on \(\mathbf{A}\) directly. With them, there never is a need to compute
    \(\mathbf{A}^T\mathbf{A}\).*  *### 24.2.5 Least squares (IV): Cholesky decomposition'
  prefs: []
  type: TYPE_NORMAL
- en: In Cholesky decomposition, a matrix is factored into two triangular matrices
    of the same size, with one being the transpose of the other. This commonly is
    written either
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{A} = \mathbf{L} \mathbf{L}^T \] or
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{A} = \mathbf{R}^T\mathbf{R} \]
  prefs: []
  type: TYPE_NORMAL
- en: Here symbols \(\mathbf{L}\) and \(\mathbf{R}\) denote lower-triangular and upper-triangular
    matrices, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: For Cholesky decomposition to be possible, a matrix has to be both symmetric
    and positive definite. These are pretty strong conditions, ones that will not
    often be fulfilled in practice. In our case, \(\mathbf{A}\) is not symmetric;
    this immediately implies we have to operate on \(\mathbf{A}^T\mathbf{A}\) instead.
    And since \(\mathbf{A}\) already is positive definite, we know that \(\mathbf{A}^T\mathbf{A}\)
    is, as well.
  prefs: []
  type: TYPE_NORMAL
- en: In `torch`, we obtain the Cholesky decomposition of a matrix using `linalg_cholesky()`.
    By default, this call will return \(\mathbf{L}\), a lower-triangular matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '*Let’s check that we can reconstruct \(\mathbf{A}\) from \(\mathbf{L}\):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE34]'
  prefs: []
  type: TYPE_NORMAL
- en: Here, I’ve computed the Frobenius norm of the difference between the original
    matrix and its reconstruction. The Frobenius norm individually sums up all matrix
    entries, and returns the square root. In theory, we’d like to see zero here; but
    in the presence of numerical errors, the result is sufficient to indicate that
    the factorization worked fine.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have \(\mathbf{L}\mathbf{L}^T\) instead of \(\mathbf{A}^T\mathbf{A}\),
    how does that help us? It’s here that the magic happens, and you’ll find the same
    type of magic at work in the remaining three methods. The idea is that due to
    some decomposition, a more performant way arises of solving the system of equations
    that constitute a given task.
  prefs: []
  type: TYPE_NORMAL
- en: 'With \(\mathbf{L}\mathbf{L}^T\), the point is that \(\mathbf{L}\) is triangular,
    and when that’s the case the linear system can be solved by simple substitution.
    That is best visible with a tiny example:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 0 & 0\\ 2 & 3 & 0\\ 3 & 4 & 1 \end{bmatrix} \begin{bmatrix}
    x1\\ x2\\ x3 \end{bmatrix} = \begin{bmatrix} 1\\ 11\\ 15 \end{bmatrix} \]
  prefs: []
  type: TYPE_NORMAL
- en: Starting in the top row, we immediately see that \(x1\) equals \(1\); and once
    we know *that* it is straightforward to calculate, from row two, that \(x2\) must
    be \(3\). The last row then tells us that \(x3\) must be \(0\).
  prefs: []
  type: TYPE_NORMAL
- en: In code, `torch_triangular_solve()` is used to efficiently compute the solution
    to a linear system of equations where the matrix of predictors is lower- or upper-triangular.
    An additional requirement is for the matrix to be symmetric – but that condition
    we already had to satisfy in order to be able to use Cholesky factorization.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, `torch_triangular_solve()` expects the matrix to be upper- (not
    lower-)triangular; but there is a function parameter, `upper`, that lets us correct
    that expectation. The return value is a list, and its first item contains the
    desired solution. To illustrate, here is `torch_triangular_solve()`, applied to
    the toy example we manually solved above:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE36]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Returning to our running example, the normal equations now look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{L}\mathbf{L}^T \mathbf{x} = \mathbf{A}^T \mathbf{b} \]
  prefs: []
  type: TYPE_NORMAL
- en: We introduce a new variable, \(\mathbf{y}\), to stand for \(\mathbf{L}^T \mathbf{x}\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{L}\mathbf{y} = \mathbf{A}^T \mathbf{b} \]
  prefs: []
  type: TYPE_NORMAL
- en: 'and compute the solution to *this* system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '*Now that we have \(y\), we look back at how it was defined:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{y} = \mathbf{L}^T \mathbf{x} \]
  prefs: []
  type: TYPE_NORMAL
- en: 'To determine \(\mathbf{x}\), we can thus again use `torch_triangular_solve()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '*And there we are.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, we compute the prediction error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE40]'
  prefs: []
  type: TYPE_NORMAL
- en: Now that you’ve seen the rationale behind Cholesky factorization – and, as already
    suggested, the idea carries over to all other decompositions – you might like
    to save yourself some work making use of a dedicated convenience function, `torch_cholesky_solve()`.
    This will render obsolete the two calls to `torch_triangular_solve()`.
  prefs: []
  type: TYPE_NORMAL
- en: The following lines yield the same output as the code above – but, of course,
    they *do* hide the underlying magic.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE42]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s move on to the next method – equivalently, to the next factorization.*******  ***###
    24.2.6 Least squares (V): LU factorization'
  prefs: []
  type: TYPE_NORMAL
- en: 'LU factorization is named after the two factors it introduces: a lower-triangular
    matrix, \(\mathbf{L}\), as well as an upper-triangular one, \(\mathbf{U}\). In
    theory, there are no restrictions on LU decomposition: Provided we allow for row
    exchanges, effectively turning \(\mathbf{A} = \mathbf{L}\mathbf{U}\) into \(\mathbf{A}
    = \mathbf{P}\mathbf{L}\mathbf{U}\) (where \(\mathbf{P}\) is a permutation matrix),
    we can factorize any matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: In practice, though, if we want to make use of `torch_triangular_solve()` ,
    the input matrix has to be symmetric. Therefore, here too we have to work with
    \(\mathbf{A}^T\mathbf{A}\), not \(\mathbf{A}\) directly. (And that’s why I’m showing
    LU decomposition right after Cholesky – they’re similar in what they make us do,
    though not at all similar in spirit.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Working with \(\mathbf{A}^T\mathbf{A}\) means we’re again starting from the
    normal equations. We factorize \(\mathbf{A}^T\mathbf{A}\), then solve two triangular
    systems to arrive at the final solution. Here are the steps, including the not-always-needed
    permutation matrix \(\mathbf{P}\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{aligned} \mathbf{A}^T \mathbf{A} \mathbf{x} &= \mathbf{A}^T \mathbf{b}
    \\ \mathbf{P} \mathbf{L}\mathbf{U} \mathbf{x} &= \mathbf{A}^T \mathbf{b} \\ \mathbf{L}
    \mathbf{y} &= \mathbf{P}^T \mathbf{A}^T \mathbf{b} \\ \mathbf{y} &= \mathbf{U}
    \mathbf{x} \end{aligned} \]
  prefs: []
  type: TYPE_NORMAL
- en: 'We see that when \(\mathbf{P}\) *is* needed, there is an additional computation:
    Following the same strategy as we did with Cholesky, we want to move \(\mathbf{P}\)
    from the left to the right. Luckily, what may look expensive – computing the inverse
    – is not: For a permutation matrix, its transpose reverses the operation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Code-wise, we’re already familiar with most of what we need to do. The only
    missing piece is `torch_lu()`. `torch_lu()` returns a list of two tensors, the
    first a compressed representation of the three matrices \(\mathbf{P}\), \(\mathbf{L}\),
    and \(\mathbf{U}\). We can uncompress it using `torch_lu_unpack()` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '*We move \(\mathbf{P}\) to the other side:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '*All that remains to be done is solve two triangular systems, and we are done:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE46]'
  prefs: []
  type: TYPE_NORMAL
- en: 'As with Cholesky decomposition, we can save ourselves the trouble of calling
    `torch_triangular_solve()` twice. `torch_lu_solve()` takes the decomposition,
    and directly returns the final solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE48]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we look at the two methods that don’t require computation of \(\mathbf{A}^T\mathbf{A}\).****  ***###
    24.2.7 Least squares (VI): QR factorization'
  prefs: []
  type: TYPE_NORMAL
- en: Any matrix can be decomposed into an orthogonal matrix, \(\mathbf{Q}\), and
    an upper-triangular matrix, \(\mathbf{R}\). QR factorization is probably the most
    popular approach to solving least-squares problems; it is, in fact, the method
    used by R’s `lm()`. In what ways, then, does it simplify the task?
  prefs: []
  type: TYPE_NORMAL
- en: 'As to \(\mathbf{R}\), we already know how it is useful: By virtue of being
    triangular, it defines a system of equations that can be solved step-by-step,
    by means of mere substitution. \(\mathbf{Q}\) is even better. An orthogonal matrix
    is one whose columns are orthogonal – meaning, mutual dot products are all zero
    – and have unit norm; and the nice thing about such a matrix is that its inverse
    equals its transpose. In general, the inverse is hard to compute; the transpose,
    however, is easy. Seeing how computation of an inverse – solving \(\mathbf{x}=\mathbf{A}^{-1}\mathbf{b}\)
    – is just the central task in least squares, it’s immediately clear how significant
    this is.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Compared to our usual scheme, this leads to a slightly shortened recipe. There
    is no “dummy” variable \(\mathbf{y}\) anymore. Instead, we directly move \(\mathbf{Q}\)
    to the other side, computing the transpose (which *is* the inverse). All that
    remains, then, is back-substitution. Also, since every matrix has a QR decomposition,
    we now directly start from \(\mathbf{A}\) instead of \(\mathbf{A}^T\mathbf{A}\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{aligned} \mathbf{A}\mathbf{x} &= \mathbf{b}\\ \mathbf{Q}\mathbf{R}\mathbf{x}
    &= \mathbf{b}\\ \mathbf{R}\mathbf{x} &= \mathbf{Q}^T\mathbf{b}\\ \end{aligned}
    \]
  prefs: []
  type: TYPE_NORMAL
- en: In `torch`, `linalg_qr()` gives us the matrices \(\mathbf{Q}\) and \(\mathbf{R}\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '*On the right side, we used to have a “convenience variable” holding \(\mathbf{A}^T\mathbf{b}\)
    ; here, we skip that step, and instead, do something “immediately useful”: move
    \(\mathbf{Q}\) to the other side.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '*The only remaining step now is to solve the remaining triangular system.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE52]'
  prefs: []
  type: TYPE_NORMAL
- en: 'By now, you’ll be expecting for me to end this section saying “there is also
    a dedicated solver in `torch`/`torch_linalg`, namely …”). Well, not literally,
    no; but effectively, yes. If you call `linalg_lstsq()` passing `driver = "gels"`,
    it is QR factorization that will be used.***  ***### 24.2.8 Least squares (VII):
    Singular Value Decomposition (SVD)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In true climactic order, the last factorization method we discuss is the most
    versatile, most diversely applicable, most semantically meaningful one: *Singular
    Value Decomposition (SVD)*. The third aspect, fascinating though it is, does not
    relate to our current task, so I won’t go into it here. Here, it is universal
    applicability that matters: Every matrix can be composed into components SVD-style.'
  prefs: []
  type: TYPE_NORMAL
- en: Singular Value Decomposition factors an input \(\mathbf{A}\) into two orthogonal
    matrices, called \(\mathbf{U}\) and \(\mathbf{V}^T\), and a diagonal one, named
    \(\symbf{\Sigma}\), such that \(\mathbf{A} = \mathbf{U} \symbf{\Sigma} \mathbf{V}^T\).
    Here \(\mathbf{U}\) and \(\mathbf{V}^T\) are the *left* and *right singular vectors*,
    and \(\symbf{\Sigma}\) holds the *singular values*.
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{aligned} \mathbf{A}\mathbf{x} &= \mathbf{b}\\ \mathbf{U}\symbf{\Sigma}\mathbf{V}^T\mathbf{x}
    &= \mathbf{b}\\ \symbf{\Sigma}\mathbf{V}^T\mathbf{x} &= \mathbf{U}^T\mathbf{b}\\
    \mathbf{V}^T\mathbf{x} &= \mathbf{y}\\ \end{aligned} \]
  prefs: []
  type: TYPE_NORMAL
- en: We start by obtaining the factorization, using `linalg_svd()` . The argument
    `full_matrices = FALSE` tells `torch` that we want a \(\mathbf{U}\) of dimensionality
    same as \(\mathbf{A}\), not expanded to 7588 x 7588.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE54]'
  prefs: []
  type: TYPE_NORMAL
- en: We move \(\mathbf{U}\) to the other side – a cheap operation, thanks to \(\mathbf{U}\)
    being orthogonal.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '*With both \(\mathbf{U}^T\mathbf{b}\) and \(\symbf{\Sigma}\) being same-length
    vectors, we can use element-wise multiplication to do the same for \(\symbf{\Sigma}\).
    We introduce a temporary variable, `y`, to hold the result.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '*Now left with the final system to solve, \(\mathbf{\mathbf{V}^T\mathbf{x}
    = \mathbf{y}}\), we again profit from orthogonality – this time, of the matrix
    \(\mathbf{V}^T\).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '*Wrapping up, let’s calculate predictions and prediction error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE59]'
  prefs: []
  type: TYPE_NORMAL
- en: That concludes our tour of important least-squares algorithms. Wrapping up the
    example, we take a quick look at performance.*****  ***### 24.2.9 Checking execution
    times
  prefs: []
  type: TYPE_NORMAL
- en: Like I said, the focus in this chapter is on concepts, not performance. But
    once you work with bigger datasets, you inevitably will care about speed. Also,
    it’s just interesting to see how fast those methods are! So, let’s do a quick
    performance benchmark. Just, please, don’t extrapolate from these results – instead,
    run analogous code on the data you care about.
  prefs: []
  type: TYPE_NORMAL
- en: 'To time them, we need all algorithms encapsulated in their respective functions.
    Here they are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '*We use the `bench` package to profile those methods. The `mark()` function
    does a lot more than just track time; however, here we just take a glance at the
    distributions of execution times ([fig. 24.1](#fig-least-squares-benchmark)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '*![Density plots of execution times, one row per method. Not going into detail
    because I just want to show how benchmarking can be done.](../Images/f4e746f40f968adcda1540fa0621b449.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 24.1: Timing least-squares algorithms, by example.'
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, we saw how different ways of factorizing a matrix can help in
    solving least squares problems. We also quickly showed a way to time those strategies;
    however, speed is not all that counts. We want the solution to be reliable, as
    well. The technical term here is *stability*.*************************  ***##
    24.3 A quick look at stability
  prefs: []
  type: TYPE_NORMAL
- en: We’ve already talked about condition numbers. The concept of stability is similar
    in spirit, but refers to an *algorithm* instead of a *matrix*. In both cases,
    the idea is that small changes in the input to a calculation should lead to small
    changes in the output. Whole books have been dedicated to this topic, so I’ll
    refrain from going into details[²](#fn2).
  prefs: []
  type: TYPE_NORMAL
- en: Instead, I’ll use an example of an ill-conditioned least-squares problem – meaning,
    the matrix is ill-conditioned – for us to form an idea about the stability of
    the algorithms we’ve discussed[³](#fn3).
  prefs: []
  type: TYPE_NORMAL
- en: 'The matrix of predictors is a 100 x 15 Vandermonde matrix, created like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '*Condition number is very high:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE64]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Even higher is the condition number obtained when we multiply it with its transpose
    – remember that some algorithms actually need to work with *this* matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE66]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we have the prediction target:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '*In our example above, we ended up with the same RMSE for all methods. It will
    be interesting to see what happens here. I’ll restrict myself to the “DIY” ones
    among the methods shown before. Here they are, listed again for convenience:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '*Let’s see, then!'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE70]'
  prefs: []
  type: TYPE_NORMAL
- en: This is pretty impressive! We clearly see how the normal equations, straightforward
    though they are, may not be the best option once problems cease to be well-conditioned.
    Cholesky as well as LU decomposition fare better; however, the clear “winners”
    are QR factorization and the SVD. No wonder those two (with two variants each)
    are the ones made use of by `linalg_lstsq()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cho, Dongjin, Cheolhee Yoo, Jungho Im, and Dong-Hyun Cha. 2020\. “Comparative
    Assessment of Various Machine Learning-Based Bias Correction Methods for Numerical
    Weather Prediction Model Forecasts of Extreme Air Temperatures in Urban Areas.”
    *Earth and Space Science* 7 (4): e2019EA000740\. https://doi.org/[https://doi.org/10.1029/2019EA000740](https://doi.org/10.1029/2019EA000740).Trefethen,
    Lloyd N., and David Bau. 1997\. *Numerical Linear Algebra*. SIAM.****** **** *
    *'
  prefs: []
  type: TYPE_NORMAL
- en: The documentation for `driver` cited above is basically an excerpt from the
    corresponding documentation in [LAPACK](https://www.netlib.org/lapack/lug/node27.html),
    as we can easily verify, since the page in question has conveniently been linked
    in the documentation for `linalg_lstsq()`.[↩︎](#fnref1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To learn more, consider consulting one of those books, for example, the widely-used
    (and concise) treatment by Trefethen and Bau ([1997](references.html#ref-Trefethen)).[↩︎](#fnref2)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The example is taken from the book by Trefethen and Bau referred to in the footnote
    above. Credits to Rachel Thomas, who brought this to my attention by virtue of
    using it in her [numerical linear algebra course](https://github.com/fastai/numerical-linear-algebra).[↩︎](#fnref3)******
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
