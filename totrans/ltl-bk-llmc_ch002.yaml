- en: The Book
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《书籍》
- en: '[PRE0]'
  id: totrans-1
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Chapter 1\. Orientation
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一章. 导航
- en: 1\. What *llm.c* Is
  id: totrans-3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1\. 什么是 *llm.c*
- en: Imagine you wanted to peek inside a modern AI model-not by reading thousands
    of lines of optimized C++ or CUDA hidden inside a giant framework, but by opening
    a small, neat folder and seeing the entire training pipeline laid out in front
    of you. That is what *llm.c* gives you.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你想要窥探一个现代 AI 模型的内部——不是通过阅读隐藏在巨大框架中的数千行优化后的 C++ 或 CUDA 代码，而是通过打开一个小巧的文件夹，看到整个训练流程展现在你面前。这正是
    *llm.c* 给你的。
- en: 'At its heart, *llm.c* is a reference implementation of how to train and run
    a GPT-2 style language model, written in pure C (and CUDA). The key word is *reference*:
    the code is meant to be minimal, readable, and educational. You don’t need to
    wade through abstraction layers or device-specific macros. Instead, you get a
    version that looks almost like pseudocode, but still compiles and runs on your
    computer.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，*llm.c* 是一个如何训练和运行 GPT-2 风格语言模型的参考实现，用纯 C（和 CUDA）编写。关键词是 *参考*：代码旨在最小化、可读性和教育性。你不需要通过抽象层或设备特定的宏。相反，你得到的是一个看起来几乎像是伪代码的版本，但仍然可以在你的计算机上编译和运行。
- en: Why This Project Exists
  id: totrans-6
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这个项目存在
- en: 'Deep learning frameworks like PyTorch and TensorFlow are amazing for getting
    models to work quickly, but they hide most of the actual mechanics. Under the
    hood, there’s a lot happening: tensors are allocated in memory, gradients are
    computed through backpropagation, optimizer states are updated, and schedulers
    adjust the learning rate. Most of us never see those details, because the framework
    handles them for us.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习框架如 PyTorch 和 TensorFlow 对于快速让模型工作非常出色，但它们隐藏了大部分实际机制。在底层，有很多事情在发生：张量在内存中分配，梯度通过反向传播计算，优化器状态更新，调度器调整学习率。我们大多数人从未看到这些细节，因为框架为我们处理了它们。
- en: '*llm.c* flips this around. It says: *what if we removed the black box and showed
    you exactly how a GPT-2 model is trained, line by line?* It’s not about speed
    or production deployment. It’s about clarity, education, and demystifying how
    large language models work.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*llm.c* 反转了这个概念。它说：*如果我们移除了黑盒，并逐行向你展示 GPT-2 模型的训练过程会怎样？* 这不是关于速度或生产部署。这是关于清晰度、教育和揭示大型语言模型的工作原理。'
- en: Key Characteristics
  id: totrans-9
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 关键特性
- en: 'Minimalism: The CPU version (`train_gpt2.c`) avoids complicated optimizations
    so that beginners can follow the logic. Even the CUDA version tries to stay simple,
    with only necessary calls to cuBLAS/cuDNN.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 极简主义：CPU 版本 (`train_gpt2.c`) 避免了复杂的优化，以便初学者可以理解逻辑。即使是 CUDA 版本也尽量保持简单，只调用必要的
    cuBLAS/cuDNN 调用。
- en: 'Self-contained: No external frameworks. The code defines its own tokenizer,
    dataloader, optimizer, and scheduler. Everything you need is in the repository.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自包含：无外部框架。代码定义了自己的分词器、数据加载器、优化器和调度器。你需要的一切都在仓库中。
- en: 'Parallels to PyTorch: Each function in the C/CUDA implementation has a counterpart
    in PyTorch. The repo even ships with Python test files to prove that the outputs
    match within tolerance.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与 PyTorch 的相似之处：C/CUDA 实现中的每个函数在 PyTorch 中都有一个对应函数。仓库甚至还附带 Python 测试文件，以证明输出在公差范围内匹配。
- en: 'Step-by-step scalability: You can start with a tiny model on CPU and, once
    you understand the basics, switch to GPU, multi-GPU, or even multi-node training.
    The structure remains the same, just faster.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步骤化可扩展性：你可以从 CPU 上的小型模型开始，一旦你理解了基础知识，就可以切换到 GPU、多 GPU 或甚至多节点训练。结构保持不变，只是更快。
- en: What You Can Do With It
  id: totrans-14
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 你可以用它做什么
- en: 'Train GPT-2 from scratch: Start with a small dataset (like Tiny Shakespeare)
    and see the model learn patterns in language.'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从头开始训练 GPT-2：从一个小的数据集（如 Tiny Shakespeare）开始，看看模型如何学习语言中的模式。
- en: 'Experiment with configurations: Change number of layers, sequence length, or
    hidden size, then watch how memory and training time scale.'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试不同的配置：改变层数、序列长度或隐藏大小，然后观察内存和训练时间如何缩放。
- en: 'Learn GPU training internals: Move from CPU to CUDA, and later to multi-GPU
    with MPI/NCCL, to see how real distributed training works under the hood.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 学习 GPU 训练内部机制：从 CPU 到 CUDA，再到使用 MPI/NCCL 的多 GPU，看看真正的分布式训练在底层是如何工作的。
- en: 'Profile performance: The repo includes profiling tools so you can measure FLOPs,
    memory bandwidth, and kernel execution times.'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 性能分析：仓库包括性能分析工具，因此你可以测量 FLOPs、内存带宽和内核执行时间。
- en: 'Reproduce big models: With enough hardware, you can actually retrain GPT-2
    124M or larger versions, using the exact same setup described in the README.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新训练大型模型：有了足够的硬件，你可以实际上使用与 README 中描述的完全相同的设置重新训练 GPT-2 124M 或更大的版本。
- en: Why You Should Care
  id: totrans-20
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 你为什么应该关心
- en: 'If you’ve ever used a large language model and thought *“this feels like magic”*,
    *llm.c* is your chance to peel back the curtain. You’ll see:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾经使用过大型语言模型，并想 *“这感觉就像魔法”*，*llm.c* 就是你的机会，揭开神秘的面纱。你会看到：
- en: How text is converted into numbers (tokenization).
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本是如何转换为数字的（分词）。
- en: How those numbers are fed into matrices and transformed.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 那些数字是如何输入到矩阵中并进行转换的。
- en: How attention actually looks in code.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力在代码中实际看起来是怎样的。
- en: How gradients are computed and weights updated.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度是如何计算和权重是如何更新的。
- en: How scaling from one GPU to many is managed.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何从单个 GPU 到多个 GPU 的扩展进行管理。
- en: It’s not magic anymore-it’s math plus code you can read.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 它不再是魔法——它是数学加上你可以阅读的代码。
- en: The Big Picture
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 整体图景
- en: 'Think of *llm.c* as a teaching lab for modern AI:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 将 *llm.c* 视为一个现代 AI 的教学实验室：
- en: On day one, you can run a small demo on CPU and see loss values dropping as
    the model learns.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第一天，你可以在 CPU 上运行一个小型演示，并看到损失值随着模型的学习而下降。
- en: On day ten, you might be profiling CUDA kernels or debugging NCCL synchronization
    across GPUs.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第十天，你可能会分析 CUDA 内核或调试跨 GPU 的 NCCL 同步。
- en: At every step, the code remains approachable because it avoids unnecessary complexity.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每一步，代码都易于理解，因为它避免了不必要的复杂性。
- en: 'This philosophy makes *llm.c* unique: it’s not trying to be the fastest or
    the most feature-rich. It’s trying to be the clearest window into how large language
    models actually work when stripped down to their essentials.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这种哲学使 *llm.c* 独特：它不是试图成为最快的或功能最丰富的。它试图成为了解大型语言模型实际工作原理的最清晰窗口。
- en: 'The Takeaway: *llm.c* is a minimal, transparent re-implementation of GPT-2
    training and inference, designed to teach you how language models operate under
    the hood. It’s not about production performance-it’s about giving you a guided
    tour through the machinery of modern AI, one C file at a time.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结**：*llm.c* 是 GPT-2 训练和推理的一个最小化、透明的重新实现，旨在教你如何理解语言模型在底层的工作原理。它并不关乎生产性能——而是通过逐个
    C 文件，带你参观现代 AI 的机器。'
- en: Would you like me to now expand Section 2 (“Repository Tour”) in the same friendly
    beginner tone, showing what’s inside each folder and file?
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 你希望我现在以同样的友好新手语气扩展第 2 节（“仓库巡游”），展示每个文件夹和文件的内容吗？
- en: 2\. Repository Tour
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2. 仓库巡游
- en: When you first open the *llm.c* repository, it may look like a random collection
    of `.c`, `.cu`, `.py`, and shell scripts. But there’s a clear structure hiding
    underneath. Each part has a purpose, and once you know where things live, navigating
    the repo feels natural. Let’s take a guided walk through the main pieces.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当你第一次打开 *llm.c* 仓库时，它可能看起来像是一堆随机的 `.c`、`.cu`、`.py` 和 shell 脚本。但下面隐藏着清晰的架构。每个部分都有其目的，一旦你知道了事物的位置，在仓库中导航就会感觉自然。让我们来一次有指导的漫步，看看主要的部分。
- en: 'Root Directory: The Entry Points'
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 根目录：入口点
- en: 'At the top level, you’ll find the core training programs. These are the files
    you actually compile and run:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶级目录中，你会找到核心训练程序。这些是你实际编译和运行的文件：
- en: '`train_gpt2.c` - The CPU reference implementation. This is the simplest, most
    readable version of GPT-2 training. It avoids special optimizations so you can
    follow the math and logic step by step.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_gpt2.c` - CPU 的参考实现。这是 GPT-2 训练最简单、最易读的版本。它避免了特殊优化，以便你可以逐步跟踪数学和逻辑。'
- en: '`train_gpt2.cu` - The CUDA implementation. Faster, uses GPU kernels, cuBLAS,
    and optional cuDNN FlashAttention. This is the version you’d use for serious training
    runs.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_gpt2.cu` - CUDA 的实现。更快，使用 GPU 内核、cuBLAS 和可选的 cuDNN FlashAttention。这是你用于严肃训练运行版本。'
- en: '`train_gpt2_fp32.cu` - A legacy CUDA path, using plain FP32 precision instead
    of mixed precision. It’s slower but useful as a debugging baseline.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_gpt2_fp32.cu` - 一个传统的 CUDA 路径，使用纯 FP32 精度而不是混合精度。它速度较慢，但作为调试基线很有用。'
- en: '`train_gpt2.py` - The PyTorch reference. This is the oracle: a tiny script
    in Python/PyTorch that trains the same GPT-2 so you can compare outputs and verify
    correctness.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_gpt2.py` - PyTorch 的参考实现。这是一个微型 Python/PyTorch 脚本，用于训练相同的 GPT-2，以便你可以比较输出并验证正确性。'
- en: 'Other important root-level files:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 其他重要的根级文件：
- en: '`Makefile` - Defines how to build different versions. Targets like `make train_gpt2`
    or `make train_gpt2cu` are your entry points.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Makefile` - 定义了如何构建不同版本。例如，`make train_gpt2` 或 `make train_gpt2cu` 是你的入口点。'
- en: '`README.md` - The main guide for running experiments, installing dependencies,
    and reproducing models.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`README.md` - 运行实验、安装依赖项和重现模型的主要指南。'
- en: '`llmc/` Directory: Utilities and Building Blocks'
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '`llmc/` 目录：工具和构建块'
- en: 'This folder holds reusable C utilities that the main training files include:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这个文件夹包含可重用的 C 工具，主训练文件包括：
- en: '`utils.h` - Safety wrappers (`fopenCheck`, `mallocCheck`) and helper functions.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`utils.h` - 安全包装（`fopenCheck`、`mallocCheck`）和辅助函数。'
- en: '`tokenizer.h` - Implements GPT-2’s tokenizer in C: encoding text into token
    IDs and decoding back to text.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer.h` - 使用 C 语言实现了 GPT-2 的分词器：将文本编码成标记 ID，并解码回文本。'
- en: '`dataloader.h` - Defines how training batches are loaded and served, handling
    dataset splits and iteration.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataloader.h` - 定义了如何加载和提供训练批次，处理数据集拆分和迭代。'
- en: '`rand.h` - Random number utilities, mirroring PyTorch’s `manual_seed` and normal
    distributions.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rand.h` - 随机数实用工具，类似于 PyTorch 的 `manual_seed` 和正态分布。'
- en: '`schedulers.h` - Learning rate scheduling, like cosine decay with warmup.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`schedulers.h` - 学习率调度，如带有预热周期的余弦衰减。'
- en: '`sampler.h` - Implements softmax sampling for text generation and helper RNG.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sampler.h` - 实现文本生成中的 softmax 抽样和辅助随机数生成器。'
- en: '`logger.h` - Minimal logging functionality for tracking progress.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logger.h` - 最小化日志功能，用于跟踪进度。'
- en: Think of `llmc/` as the library that keeps the main files clean and readable.
    Instead of cluttering `train_gpt2.c` with helpers, everything is modularized here.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `llmc/` 视为保持主要文件整洁和可读性的库。而不是在 `train_gpt2.c` 中添加辅助函数，这里一切都是模块化的。
- en: '`dev/` Directory: Scripts and Extras'
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '`dev/` 目录：脚本和附加内容'
- en: 'This folder is full of supporting tools that make experiments easier:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这个文件夹充满了支持工具，使实验更容易进行：
- en: '`dev/download_starter_pack.sh` - Fetches the GPT-2 124M weights, tokenizer,
    and datasets. This is the quickest way to get started.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dev/download_starter_pack.sh` - 获取 GPT-2 124M 权重、分词器和数据集。这是快速开始的最快方式。'
- en: '`dev/data/` - Contains scripts for preparing datasets like Tiny Shakespeare
    or OpenWebText in the binary format that *llm.c* expects.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dev/data/` - 包含准备数据集（如 Tiny Shakespeare 或 OpenWebText）的脚本，这些数据集是以二进制格式提供的，符合
    *llm.c* 的期望。'
- en: '`dev/cuda/` - A place for experimenting with standalone CUDA kernels. This
    is where you’d go if you want to tinker with custom GPU code beyond the main trainer.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dev/cuda/` - 这是一个用于实验独立 CUDA 内核的地方。如果你想对主训练器之外的定制 GPU 代码进行修改，你会去这里。'
- en: '`doc/` Directory: Learning Resources'
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '`doc/` 目录：学习资源'
- en: 'Documentation that digs deeper into specific topics. For example:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 深入特定主题的文档。例如：
- en: '`doc/layernorm/layernorm.md` - A tutorial-style explanation of Layer Normalization,
    complete with math and code. It helps you understand one of GPT-2’s core components
    before diving into the C implementation.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`doc/layernorm/layernorm.md` - 一篇教程风格的解释，包括数学和代码，帮助你理解 GPT-2 的核心组件之一，在深入 C
    语言实现之前。'
- en: This folder is a learning aid. Whenever a concept feels too dense, check here
    for a more gentle walkthrough.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这个文件夹是一个学习辅助工具。每当一个概念感觉过于复杂时，可以在这里找到更温和的讲解。
- en: Test Files
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 测试文件
- en: 'Testing is taken seriously in *llm.c*, because the goal is to prove that the
    C/CUDA implementation is correct compared to PyTorch:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *llm.c* 中，测试被认真对待，因为目标是证明 C/CUDA 实现与 PyTorch 相比是正确的：
- en: '`test_gpt2.c` - Runs forward passes and training steps on CPU and compares
    outputs to PyTorch.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`test_gpt2.c` - 在 CPU 上运行前向传递和训练步骤，并将输出与 PyTorch 进行比较。'
- en: '`test_gpt2cu.cu` - Same idea but for CUDA, including both FP32 and mixed-precision
    runs.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`test_gpt2cu.cu` - 与此类似，但针对 CUDA，包括 FP32 和混合精度运行。'
- en: 'These files keep everything honest: you can always verify that your build produces
    the same results as the canonical PyTorch model.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这些文件确保一切真实：你可以始终验证你的构建产生的结果与官方 PyTorch 模型相同。
- en: Profiling Tools
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 性能分析工具
- en: 'For performance deep dives:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 对于性能深入研究：
- en: '`profile_gpt2.cu` - A CUDA profiling harness that benchmarks kernels and measures
    throughput.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`profile_gpt2.cu` - 一个 CUDA 性能分析工具，用于基准测试内核并测量吞吐量。'
- en: '`profile_gpt2cu.py` - Python-side profiler for analyzing GPU utilization, memory
    bandwidth, and FLOPs.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`profile_gpt2cu.py` - Python 端的性能分析器，用于分析 GPU 利用率、内存带宽和浮点运算次数。'
- en: If you’re curious about where time is being spent in training, these files show
    you how to measure it.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你好奇训练过程中时间都花在哪里，这些文件会告诉你如何进行测量。
- en: Datasets and Artifacts
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集和工件
- en: 'When you run `download_starter_pack.sh`, you’ll get:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行 `download_starter_pack.sh` 时，你会得到：
- en: '`gpt2_tokenizer.bin` - GPT-2’s byte-pair encoding tokenizer, serialized in
    binary.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gpt2_tokenizer.bin` - GPT-2 的字节对编码分词器，以二进制序列化。'
- en: Dataset `.bin` files - Training and validation sets, tokenized and ready for
    the dataloader.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集 `.bin` 文件 - 训练和验证集，分词并准备好供数据加载器使用。
- en: These files are not in the repo by default but are downloaded or generated locally.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这些文件默认不在仓库中，但会下载或本地生成。
- en: Putting It Together
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 组装起来
- en: 'The repository is structured like a teaching lab:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 仓库的结构就像一个教学实验室：
- en: Root files are the main experiments.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根文件是主要实验。
- en: '`llmc/` is the library of building blocks.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`llmc/` 是构建模块的库。'
- en: '`dev/` provides extra tools and scripts.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dev/` 提供了额外的工具和脚本。'
- en: '`doc/` explains tricky concepts in tutorial form.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`doc/` 以教程形式解释复杂的概念。'
- en: Tests and profilers make sure everything matches PyTorch and runs efficiently.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试和性能分析器确保一切与 PyTorch 一致且运行高效。
- en: Once you see the pattern, the repo feels less intimidating. Every file has a
    role in telling the story of how a GPT-2 model is built from scratch in C and
    CUDA.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您看到这种模式，仓库看起来就不那么令人生畏了。每个文件都在讲述一个故事，说明如何从头开始使用 C 和 CUDA 构建一个 GPT-2 模型。
- en: 3\. Makefile Targets & Flags
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3. Makefile 目标和标志
- en: 'Every C or CUDA program needs a build system, and in *llm.c* that role is handled
    by a simple but powerful Makefile. If you’ve never used `make` before, think of
    it as a recipe book: you type `make <target>` in your terminal, and it follows
    the instructions for compiling the code into an executable. In *llm.c*, this file
    is your control center for choosing which trainer to build, whether to enable
    GPUs, and which optional features to turn on.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 C 或 CUDA 程序都需要一个构建系统，在 *llm.c* 中，这个角色由一个简单但强大的 Makefile 处理。如果您以前从未使用过 `make`，可以将其视为一本食谱书：您在终端中输入
    `make <target>`，然后它将遵循指令将代码编译成可执行文件。在 *llm.c* 中，此文件是您选择要构建哪个训练器、是否启用 GPU 以及要开启哪些可选功能的控制中心。
- en: Why a Makefile?
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么需要 Makefile？
- en: 'Instead of memorizing long `gcc` or `nvcc` compile commands with dozens of
    flags, the Makefile captures those instructions once and gives them a short name.
    For example, building the CPU trainer is as easy as:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是记住带有数十个标志的长 `gcc` 或 `nvcc` 编译命令，Makefile 一次捕获这些指令并给它们一个简短的名称。例如，构建 CPU 训练器就像这样：
- en: '[PRE1]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Behind the scenes, this calls `gcc`, sets optimization flags, includes the right
    headers, and links everything together. The same applies to CUDA builds with `nvcc`.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，这调用 `gcc`，设置优化标志，包含正确的头文件，并将所有内容链接在一起。CUDA 构建 `nvcc` 也适用同样的方法。
- en: Core Targets
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 核心目标
- en: 'Here are the most important build targets you’ll find:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是您会发现的最重要的构建目标：
- en: '`train_gpt2` - Builds the CPU-only reference trainer. Uses `gcc` (or `clang`)
    and links against OpenMP for parallel loops.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_gpt2` - 构建仅使用 CPU 的参考训练器。使用 `gcc`（或 `clang`）并链接 OpenMP 以实现并行循环。'
- en: '`train_gpt2cu` - Builds the CUDA trainer with mixed precision and optional
    cuDNN FlashAttention. Uses `nvcc`.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_gpt2cu` - 使用混合精度和可选的 cuDNN FlashAttention 构建 CUDA 训练器。使用 `nvcc`。'
- en: '`train_gpt2_fp32` - Builds the legacy CUDA trainer that stays in pure FP32
    (slower but simpler).'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_gpt2_fp32` - 构建保持纯 FP32（较慢但更简单）的遗留 CUDA 训练器。'
- en: '`test_gpt2` - Compiles the CPU test program to compare results against PyTorch.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`test_gpt2` - 编译 CPU 测试程序以与 PyTorch 进行结果比较。'
- en: '`test_gpt2cu` - Compiles the CUDA test program to check GPU parity with PyTorch.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`test_gpt2cu` - 编译 CUDA 测试程序以检查与 PyTorch 的 GPU 一致性。'
- en: '`profile_gpt2.cu` - Compiles the CUDA profiler harness, used to benchmark kernels
    and FLOPs.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`profile_gpt2.cu` - 编译 CUDA 性能分析工具包，用于基准测试内核和 FLOPs。'
- en: 'Each of these produces a binary you can run directly, for example:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这些中的每一个都会生成一个可以直接运行的二进制文件，例如：
- en: '[PRE2]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Key Flags You Can Toggle
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 您可以切换的关键标志
- en: 'The Makefile also exposes several switches that let you customize the build.
    You set them when running `make`, like this:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Makefile 还暴露了几个开关，允许您自定义构建。您在运行 `make` 时设置它们，如下所示：
- en: '[PRE3]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here are the most important flags:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是最重要的标志：
- en: '`USE_CUDNN` - Enables cuDNN FlashAttention if your system has cuDNN installed.
    This can give big speedups for attention, but it’s optional. By default, it’s
    off.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`USE_CUDNN` - 如果您的系统已安装 cuDNN，则启用 cuDNN FlashAttention。这可以为注意力机制提供很大的速度提升，但这是可选的。默认情况下，它是关闭的。'
- en: '`OMP=1` - Tells the CPU trainer to compile with OpenMP enabled. This allows
    multithreaded execution, making CPU runs much faster. Usually on by default if
    OpenMP is detected.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`OMP=1` - 告诉 CPU 训练器使用 OpenMP 启用编译。这允许多线程执行，使 CPU 运行速度大大提高。通常在检测到 OpenMP 时默认开启。'
- en: '`DEBUG=1` - Compiles with debugging symbols (`-g`) instead of maximum optimization.
    Useful when stepping through code in an IDE or using a debugger.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DEBUG=1` - 使用调试符号（`-g`）而不是最大优化进行编译。在 IDE 中逐步执行代码或使用调试器时很有用。'
- en: '`PROFILE=1` - Adds profiling hooks, helping you analyze execution time and
    performance.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PROFILE=1` - 添加性能分析钩子，帮助您分析执行时间和性能。'
- en: Optimization Choices
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优化选择
- en: 'The default build uses `-O3` optimization, which makes the code run fast but
    sometimes harder to debug. If you’re just learning and want clarity, you can switch
    to:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 默认构建使用 `-O3` 优化，这使得代码运行速度快，但有时更难调试。如果您正在学习并希望清晰，可以切换到：
- en: '[PRE4]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This creates a binary that runs slower but lets you step through line by line
    in a debugger. For performance benchmarking, stick with the optimized default.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这会创建一个运行速度较慢的二进制文件，但让你可以在调试器中逐行执行。对于性能基准测试，请坚持使用优化的默认设置。
- en: Multi-GPU and MPI Support
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多GPU和MPI支持
- en: When building the CUDA trainer, the Makefile can also link against MPI and NCCL
    if they’re installed. That’s what enables multi-GPU and multi-node training. You
    usually don’t need to change anything-the Makefile automatically detects these
    libraries and includes them if available.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建CUDA训练器时，如果已安装，Makefile也可以链接到MPI和NCCL。这就是实现多GPU和多节点训练的原因。通常你不需要更改任何东西——Makefile会自动检测这些库，并在可用时包含它们。
- en: Putting It All Together
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 整合所有内容
- en: 'Think of the Makefile as a switchboard for the whole project:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 将Makefile视为整个项目的交换板：
- en: Want to run the simple CPU demo? → `make train_gpt2`
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 想要运行简单的CPU演示吗？→ `make train_gpt2`
- en: Want to train faster on GPU? → `make train_gpt2cu`
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 想要在GPU上更快地训练吗？→ `make train_gpt2cu`
- en: Want to debug kernels? → `make train_gpt2cu DEBUG=1`
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 想要调试内核？→ `make train_gpt2cu DEBUG=1`
- en: Want to test parity with PyTorch? → `make test_gpt2` or `make test_gpt2cu`
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 想要测试与PyTorch的兼容性吗？→ `make test_gpt2` 或 `make test_gpt2cu`
- en: With just a few keystrokes, you control whether you’re running a beginner-friendly
    CPU demo, a high-performance GPU build, or a debugging session.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 只需几个按键，你就可以控制是运行一个适合初学者的CPU演示、高性能GPU构建还是调试会话。
- en: 'The takeaway: The Makefile is your control center. It abstracts away complicated
    compiler commands and gives you a clean menu of options: CPU vs GPU, FP32 vs mixed
    precision, debug vs optimized, and single vs multi-GPU. Mastering it is the first
    step to feeling comfortable experimenting inside *llm.c*.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 吸取的教训：Makefile是你的控制中心。它抽象了复杂的编译器命令，并为你提供了一个干净的选项菜单：CPU vs GPU、FP32 vs 混合精度、调试
    vs 优化、单GPU vs 多GPU。掌握它是舒适地在*llm.c*中进行实验的第一步。
- en: '4\. Quickstart: CPU Reference Path (`train_gpt2.c`)'
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4. 快速入门：CPU参考路径（`train_gpt2.c`）
- en: The simplest way to begin exploring *llm.c* is with the CPU-only reference implementation.
    This file, `train_gpt2.c`, is deliberately designed to be minimal, readable, and
    approachable. It doesn’t hide complexity behind libraries or macros. Instead,
    it shows you exactly how a GPT-2 model is trained, step by step, using plain C
    and a sprinkle of OpenMP for speed.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 开始探索 *llm.c* 的最简单方式是使用仅CPU的参考实现。这个文件，`train_gpt2.c`，被故意设计成最小化、可读和易于接近。它不会在库或宏后面隐藏复杂性。相反，它展示了如何一步一步地使用纯C和一点OpenMP来加速训练一个GPT-2模型。
- en: Why Start with CPU?
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么从CPU开始？
- en: 'Clarity first: GPUs add layers of complexity (CUDA kernels, memory transfers,
    cuBLAS). On CPU, you can focus on the core algorithm without distraction.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先清晰：GPU增加了复杂性层次（CUDA内核、内存传输、cuBLAS）。在CPU上，你可以专注于核心算法，不受干扰。
- en: 'Portability: Any machine with a C compiler can run it-no special hardware required.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可移植性：任何有C编译器的机器都可以运行它——不需要特殊硬件。
- en: 'Debuggability: Errors are easier to trace, and you can single-step through
    the code in an IDE.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可调试性：错误更容易追踪，你可以在IDE中单步执行代码。
- en: The CPU version is slower, but that’s a feature here-it forces you to really
    see what’s happening under the hood.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: CPU版本较慢，但在这里这是一个特性——它迫使你真正看到底层发生了什么。
- en: Building the CPU Trainer
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 构建CPU训练器
- en: 'From the root of the repository, you just type:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 从存储库的根目录，你只需输入：
- en: '[PRE5]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This compiles `train_gpt2.c` into an executable named `train_gpt2`. If your
    system has OpenMP, the Makefile will detect it and add the right flags.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这会将`train_gpt2.c`编译成一个名为`train_gpt2`的可执行文件。如果你的系统有OpenMP，Makefile会检测到它并添加正确的标志。
- en: Running Your First Training Run
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 运行你的第一次训练运行
- en: 'Before running, download the starter pack (tokenizer, dataset, configs):'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行之前，下载启动包（分词器、数据集、配置）：
- en: '[PRE6]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now launch training:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在开始训练：
- en: '[PRE7]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You’ll see output like:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到如下输出：
- en: '[PRE8]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Each line tells you:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 每一行都会告诉你：
- en: Model size and config (sequence length, vocabulary size, layers, heads, channels).
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型大小和配置（序列长度、词汇量大小、层、头、通道）。
- en: Dataset stats (how many batches for training and validation).
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集统计（训练和验证的批次数量）。
- en: Activation memory size (a measure of how big the intermediate states are).
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活内存大小（衡量中间状态大小的指标）。
- en: Training progress (step number, train loss, validation loss, time per step).
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练进度（步骤编号、训练损失、验证损失、每步时间）。
- en: Inside the Training Loop
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在训练循环内部
- en: 'Although you don’t need to dive into the code yet, here’s the high-level flow
    in `train_gpt2.c`:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然你目前不需要深入研究代码，但这里是在`train_gpt2.c`中的高级流程：
- en: Load tokenizer and dataset → turns text into tokens.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载分词器和数据集 → 将文本转换为标记。
- en: Initialize model parameters → embeddings, attention weights, MLPs, norms.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化模型参数 → 嵌入、注意力权重、MLP、归一化。
- en: 'For each batch:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个批次：
- en: Forward pass → compute logits and loss.
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前向传播 → 计算logits和损失。
- en: Backward pass → compute gradients.
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反向传播 → 计算梯度。
- en: Update parameters → optimizer step.
  id: totrans-157
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新参数 → 优化器步骤。
- en: Log progress → print losses, occasionally run validation.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记录进度 → 打印损失，偶尔运行验证。
- en: This mirrors exactly what happens in PyTorch, just spelled out in C.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这与PyTorch中发生的情况完全一致，只是用C语言表述出来。
- en: Performance Notes
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 性能注意事项
- en: 'On CPU, don’t expect speed. Training GPT-2 124M can take days or weeks. But
    that’s not the point. The CPU reference path is like a glass box: everything is
    visible, no shortcuts. You’ll use this to learn the mechanics and to verify that
    your GPU runs match the same results.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在CPU上，不要期望速度。训练GPT-2 124M可能需要几天或几周。但这不是重点。CPU参考路径就像一个玻璃盒子：一切尽收眼底，没有捷径。你将使用它来学习机制，并验证你的GPU运行是否与相同的结果匹配。
- en: 'If you want to speed things up slightly, you can:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要稍微加快速度，你可以：
- en: 'Increase OpenMP threads:'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加OpenMP线程：
- en: '[PRE9]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Use a smaller dataset (Tiny Shakespeare) to see faster progress.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用更小的数据集（Tiny Shakespeare）以看到更快的进展。
- en: Reduce model size by changing config values (fewer layers, smaller channels).
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过更改配置值（减少层、减小通道）来减小模型大小。
- en: When to Move On
  id: totrans-167
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 何时继续前进
- en: Once you’re comfortable with how training looks on CPU-loss values going down,
    checkpoints being written, logs appearing-you’ll be ready to graduate to the GPU
    version (`train_gpt2.cu`). That’s where performance and scaling come in, but the
    CPU run gives you the conceptual foundation.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你对CPU上的训练外观感到舒适——损失值下降、检查点写入、日志出现——你就可以准备过渡到GPU版本（`train_gpt2.cu`）。在那里，性能和扩展性变得重要，但CPU运行为你提供了概念基础。
- en: 'The takeaway: Running `train_gpt2.c` is your first hands-on encounter with
    GPT-2 training in *llm.c*. It’s slow, transparent, and designed for learning.
    You’ll see every piece of the model at work, one step at a time, before diving
    into the complexity of CUDA.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 吸取的教训：运行`train_gpt2.c`是你在`llm.c`中第一次亲身体验GPT-2训练。它速度较慢、透明度高，且旨在学习。在深入CUDA的复杂性之前，你将一步一步地看到模型中的每一部分是如何工作的。
- en: '5\. Quickstart: 1-GPU Legacy Path (`train_gpt2_fp32.cu`)'
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5. 快速入门：1-GPU旧版路径（`train_gpt2_fp32.cu`）
- en: Once you’ve seen the CPU trainer in action, the natural next step is to try
    training on a GPU. The file `train_gpt2_fp32.cu` is the simplest GPU entry point.
    It predates the more advanced mixed-precision trainer (`train_gpt2.cu`), and it
    runs everything in full 32-bit floating point (FP32) precision. That makes it
    easier to follow and debug, even though it’s slower than modern approaches. Think
    of it as the “training wheels” for GPU training in *llm.c*.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你看到了CPU训练器的实际运行，下一步自然的步骤就是尝试在GPU上训练。文件`train_gpt2_fp32.cu`是最简单的GPU入口点。它早于更高级的混合精度训练器（`train_gpt2.cu`），并且以全32位浮点数（FP32）精度运行所有内容。这使得它更容易跟踪和调试，尽管它比现代方法慢。把它看作是`llm.c`中GPU训练的“训练轮”。
- en: Why This Path Exists
  id: totrans-172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这条路径存在
- en: 'Modern GPU training almost always uses mixed precision (FP16/BF16 for speed
    and memory savings, FP32 for stability). But mixed precision introduces extra
    complexity: scaling losses, maintaining master weights, checking for overflows.
    For beginners, all that can be distracting.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现代GPU训练几乎总是使用混合精度（FP16/BF16以提高速度和节省内存，FP32以保证稳定性）。但混合精度引入了额外的复杂性：缩放损失、维护主权重、检查溢出。对于初学者来说，所有这些都可能分散注意力。
- en: 'The FP32 path avoids those complications:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: FP32路径避免了这些复杂性：
- en: Every tensor (activations, weights, gradients) is stored as 32-bit floats.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个张量（激活、权重、梯度）都存储为32位浮点数。
- en: No special handling of loss scaling is needed.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不需要特殊处理损失缩放。
- en: Debugging mismatches with PyTorch is straightforward.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PyTorch调试不匹配是直接的。
- en: The trade-off is performance-this version runs significantly slower and uses
    more memory.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 代价是性能——这个版本运行速度明显较慢，且使用更多内存。
- en: Building the FP32 CUDA Trainer
  id: totrans-179
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 构建FP32 CUDA训练器
- en: 'From the root of the repository:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 从存储库的根目录：
- en: '[PRE10]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This invokes `nvcc` (the NVIDIA CUDA compiler) and links against cuBLAS for
    matrix multiplications. The output is an executable named `train_gpt2_fp32`.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这将调用`nvcc`（NVIDIA CUDA编译器）并链接cuBLAS以进行矩阵乘法。输出是一个名为`train_gpt2_fp32`的可执行文件。
- en: Running It
  id: totrans-183
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 运行它
- en: 'Just like the CPU version, make sure you’ve downloaded the starter pack first:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 就像CPU版本一样，确保你首先下载了启动包：
- en: '[PRE11]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Then launch training on your GPU:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在你的GPU上启动训练：
- en: '[PRE12]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: If CUDA is installed correctly, the program will detect your GPU and start training.
    You’ll see logs that look similar to the CPU trainer’s, but with much shorter
    step times. For example, a training step that took ~2 seconds on CPU might take
    ~50 milliseconds on GPU.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如果CUDA安装正确，程序将检测到你的GPU并开始训练。你会看到类似于CPU训练器的日志，但步骤时间要短得多。例如，在CPU上花费约2秒的训练步骤可能在GPU上只需要约50毫秒。
- en: Under the Hood
  id: totrans-189
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 内部机制
- en: 'Although the training loop looks the same on the surface, a lot changes under
    the hood when running on GPU:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然从表面上看训练循环看起来相同，但在GPU上运行时，内部有很多变化：
- en: Tensors are allocated in GPU memory (not system RAM).
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 张量分配在GPU内存中（不是系统RAM）。
- en: Matrix multiplications (the core of attention and MLP layers) are executed by
    cuBLAS, NVIDIA’s high-performance linear algebra library.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 矩阵乘法（注意力和MLP层的核心）由cuBLAS执行，这是NVIDIA的高性能线性代数库。
- en: Kernels for elementwise operations (like adding residuals, applying softmax,
    or normalizing) are written in CUDA or use built-in primitives.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 元素级操作的内核（如添加残差、应用softmax或归一化）是用CUDA编写的或使用内置原语。
- en: Gradients and optimizer states are updated entirely on the device, with minimal
    CPU↔︎GPU transfers.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 梯度和优化器状态完全在设备上更新，CPU↔GPU传输最小化。
- en: This makes training dramatically faster, but the structure of the code is still
    recognizable compared to the CPU version.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得训练速度显著提高，但与CPU版本相比，代码结构仍然可识别。
- en: When to Use FP32 vs Mixed Precision
  id: totrans-196
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 何时使用FP32与混合精度
- en: 'Use FP32 (this path) when:'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当使用FP32（此路径）时：
- en: You’re learning how GPU training works step by step.
  id: totrans-198
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你正在逐步学习GPU训练的工作原理。
- en: You want a clean comparison with the CPU trainer.
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你想要与CPU训练器进行干净的比较。
- en: You’re debugging correctness issues without worrying about loss scaling.
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你在调试正确性问题时无需担心损失缩放。
- en: 'Use Mixed Precision (`train_gpt2.cu`) when:'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当使用混合精度（`train_gpt2.cu`）时：
- en: You want real performance (2–4× faster training).
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你想要真正的性能（2–4×更快的训练）。
- en: You’re training larger models (774M, 1.6B parameters) where memory efficiency
    matters.
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你正在训练更大的模型（774M，1.6B个参数），内存效率很重要。
- en: You’re aiming to reproduce published GPT-2 runs on modern GPUs.
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你旨在在现代GPU上重现已发布的GPT-2运行。
- en: Common Pitfalls
  id: totrans-205
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 常见陷阱
- en: CUDA not installed → If `nvcc` isn’t found, the Makefile will fail. You’ll need
    the CUDA Toolkit installed.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CUDA未安装 → 如果找不到`nvcc`，Makefile将失败。你需要安装CUDA Toolkit。
- en: Driver mismatch → Your NVIDIA driver must match the CUDA version.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 驱动程序不匹配 → 你的NVIDIA驱动程序必须与CUDA版本匹配。
- en: Out of memory errors → FP32 uses more GPU memory, so you may need to lower batch
    size if you’re on a smaller GPU.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 内存不足错误 → FP32使用更多的GPU内存，因此如果你在较小的GPU上，你可能需要降低批处理大小。
- en: Why This Step Matters
  id: totrans-209
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这一步很重要
- en: 'The FP32 trainer is like a bridge:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: FP32训练器就像一座桥梁：
- en: On one side is the CPU reference path, slow but crystal-clear.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一边是CPU参考路径，慢但清晰。
- en: On the other is the mixed-precision CUDA path, fast but more complex.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一边是混合精度CUDA路径，速度快但更复杂。
- en: By walking across this bridge, you learn how GPU acceleration works without
    being overwhelmed by optimizations.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 通过走过这座桥，你可以在不被优化所淹没的情况下学习GPU加速的工作原理。
- en: 'The takeaway: `train_gpt2_fp32.cu` is your first taste of real GPU training
    in *llm.c*. It skips advanced tricks and shows you a clean, one-GPU, full-precision
    implementation. It’s not the fastest, but it’s the friendliest way to understand
    how training moves from CPU to GPU.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 吸取教训：`train_gpt2_fp32.cu`是你在*llm.c*中第一次尝试真正的GPU训练。它跳过了高级技巧，并展示了干净、单GPU、全精度实现。它不是最快的，但它是理解训练如何从CPU迁移到GPU的最友好方式。
- en: '6\. Quickstart: Modern CUDA Path (`train_gpt2.cu`)'
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6. 快速入门：现代CUDA路径（`train_gpt2.cu`）
- en: This is the high-performance trainer most people use day to day. It runs on
    a single NVIDIA GPU (and also forms the basis for multi-GPU), uses mixed precision
    (FP16/BF16 where safe, FP32 where needed), and can optionally enable cuDNN FlashAttention
    for fast attention. Compared to the FP32 legacy path, it’s significantly faster
    and uses less memory, while keeping the training loop easy to follow.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这是大多数人日常使用的高性能训练器。它运行在单个NVIDIA GPU上（也是多GPU的基础），使用混合精度（在安全的情况下使用FP16/BF16，在需要时使用FP32），并可选择启用cuDNN
    FlashAttention以实现快速注意力。与FP32传统路径相比，它速度更快，内存使用更少，同时保持训练循环易于跟踪。
- en: What “mixed precision” means (in plain words)
  id: totrans-217
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: “混合精度”是什么意思（用简单的话说）
- en: 'Weights & activations: stored/processed in FP16 or BF16 for speed and lower
    memory.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重与激活：以FP16或BF16的速度和较低的内存存储/处理。
- en: 'Master weights: a FP32 copy of parameters kept for stable updates.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主权重：用于稳定更新的FP32参数副本。
- en: 'Loss scaling: multiply the loss before backward to avoid underflow; unscale
    the grads before the optimizer step.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失缩放：在反向之前乘以损失以避免下溢；在优化器步骤之前未缩放梯度。
- en: 'Autocast-like behavior: the code picks safe dtypes for each op (GEMMs in tensor
    cores, reductions in FP32, etc.).'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类似于Autocast的行为：代码为每个操作选择安全的dtype（张量核心中的GEMMs，FP32中的减少等）。
- en: You get 2–4× speedups on many GPUs, and the same final accuracy when configured
    properly.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在许多GPU上获得2-4倍的速度提升，并且当配置正确时，最终精度相同。
- en: Build the modern CUDA trainer
  id: totrans-223
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 构建现代CUDA训练器
- en: '[PRE13]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Common variants:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 常见变体：
- en: 'With cuDNN FlashAttention (if available):'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果有cuDNN FlashAttention（如果可用）：
- en: '[PRE14]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'With debug symbols (slower, but easier to step through):'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用调试符号（较慢，但更容易逐步执行）：
- en: '[PRE15]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This produces an executable named `train_gpt2cu`.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成一个名为`train_gpt2cu`的可执行文件。
- en: One-time data & artifacts
  id: totrans-231
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 一次性数据和工件
- en: 'If you haven’t already:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还没有：
- en: '[PRE16]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This fetches the tokenizer and a small dataset so you can run immediately.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这将获取分词器和一个小型数据集，以便您可以立即运行。
- en: Run your first GPU training session
  id: totrans-235
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 运行您的第一个GPU训练会话
- en: '[PRE17]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: You should see a config header (model dims, vocab, sequence length) followed
    by step-by-step loss prints. Step times will be much shorter than CPU and noticeably
    faster than FP32, especially on tensor-core GPUs (Turing and newer).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到一个配置标题（模型维度、词汇、序列长度），然后是逐步损失打印。步骤时间将比CPU短得多，并且比FP32明显快，尤其是在张量核心GPU（Turing和更新的）上。
- en: 'Speed tips right away:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 立即使用的速度提示：
- en: Use a larger global batch if memory allows-it improves GPU utilization.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果内存允许，使用更大的全局批量大小可以提高GPU利用率。
- en: 'Set environment threads for any CPU preprocessing:'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置任何CPU预处理的环境线程：
- en: '[PRE18]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: What’s different under the hood vs. FP32
  id: totrans-242
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 与FP32相比，底层有什么不同
- en: 'Tensor cores: GEMMs run in FP16/BF16 paths via cuBLAS/cuBLASLt for big throughput.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张量核心：GEMMs通过cuBLAS/cuBLASLt在FP16/BF16路径上运行，以实现大吞吐量。
- en: 'Scaled loss & unscale pass: Forward computes the loss, multiplies it by a scale
    factor; backward divides gradients by the same factor before updates.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缩放损失与未缩放传递：正向计算损失，乘以缩放因子；反向更新前，将梯度除以相同的因子。
- en: 'Master FP32 copy: Optimizer (AdamW) updates this copy, then casts back to low
    precision for the next forward.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主FP32复制：优化器（AdamW）更新此复制，然后将其转换为低精度以进行下一个正向操作。
- en: 'Fused/fast attention (optional): With `USE_CUDNN=1`, attention may route through
    cuDNN FlashAttention backends.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混合/快速注意力（可选）：通过`USE_CUDNN=1`，注意力可能通过cuDNN FlashAttention后端路由。
- en: 'You still recognize the same loop: load batch → forward → loss → backward →
    AdamW step → log.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 您仍然可以识别出相同的循环：加载批量 → 正向 → 损失 → 反向 → AdamW步骤 → 记录。
- en: Choosing FP16 vs. BF16
  id: totrans-248
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 选择FP16与BF16
- en: 'FP16: best speed, needs loss scaling; widely supported.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FP16：最佳速度，需要损失缩放；广泛支持。
- en: 'BF16: more numerically forgiving (often needs little/no scaling), requires
    hardware support (Ampere+); slightly larger memory than FP16 but often simpler.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BF16：更易于数值处理（通常需要少量/无缩放），需要硬件支持（Ampere+）；比FP16略大的内存，但通常更简单。
- en: The trainer picks what your GPU supports or what the code defaults to; you can
    expose a flag later if you want to force one.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 训练器选择您的GPU支持的或代码默认的；如果您想强制使用一个，稍后可以公开一个标志。
- en: Common command patterns
  id: totrans-252
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 常见命令模式
- en: 'Small GPU (less VRAM):'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小型GPU（较少的VRAM）：
- en: '[PRE19]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Faster warmup with cosine schedule:'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用余弦调度加快预热：
- en: '[PRE20]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Periodic eval to sanity-check:'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定期评估以进行合理性检查：
- en: '[PRE21]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: (Flag names above mirror typical patterns; adjust to match the binary’s printed
    help.)
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: （上面的标志名称反映了典型模式；根据二进制文件的打印帮助进行调整。）
- en: Validating correctness (highly recommended)
  id: totrans-260
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 验证正确性（强烈推荐）
- en: 'Run the CUDA test binary to compare against the PyTorch reference on small
    batches:'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行CUDA测试二进制文件，以比较PyTorch参考在小型批量上的表现：
- en: '[PRE22]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Check that logits/loss match within a small tolerance. If mismatches happen,
    recompile without optimizations or disable cuDNN fast paths (`USE_CUDNN=0`) to
    isolate the issue.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查logits/loss在小的公差范围内是否匹配。如果发生不匹配，请在不进行优化或禁用cuDNN快速路径（`USE_CUDNN=0`）的情况下重新编译，以隔离问题。
- en: Enabling FlashAttention (when available)
  id: totrans-264
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 启用FlashAttention（当可用时）
- en: '[PRE23]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Good signs: faster attention time and lower step latency. If you hit build/runtime
    errors, ensure your CUDA, cuDNN, and driver versions are compatible; fall back
    to `USE_CUDNN=0` while you sort it out.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 好兆头：更快的注意力时间和更低的步骤延迟。如果您遇到构建/运行时错误，请确保您的CUDA、cuDNN和驱动程序版本兼容；在解决问题时回退到`USE_CUDNN=0`。
- en: Memory & performance tuning checklist
  id: totrans-267
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 内存与性能调整清单
- en: 'Batching: Increase `micro_batch_size` until you reach ~90% GPU utilization
    without OOM.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批处理：增加`micro_batch_size`，直到达到约90%的GPU利用率而不发生内存不足。
- en: 'Sequence length: Longer sequences increase compute quadratically in attention;
    reduce `--seq_len` if memory is tight.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列长度：较长的序列在注意力中计算量呈二次增长；如果内存紧张，请减少`--seq_len`。
- en: 'Grad accumulation: Keep global batch size large by accumulating over multiple
    micro-batches.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度累积：通过多个微批次的累积来保持全局批次大小。
- en: 'Pinned host memory & async copies: Already used where sensible; keep CPU↔︎GPU
    transfers minimal.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 固定主机内存和异步复制：在合理的地方已经使用；尽量减少 CPU↔GPU 传输。
- en: 'Profiler: Once it runs, profile hotspots to confirm GEMMs dominate (as expected)
    and attention isn’t a bottleneck unless FlashAttention is off.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析器：一旦运行，分析热点以确认 GEMMs 占主导地位（如预期）并且除非 FlashAttention 关闭，否则注意力不是瓶颈。
- en: Troubleshooting
  id: totrans-273
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 故障排除
- en: '`cudaErrorNoKernelImageForDevice`: Toolkit too new/old for your GPU; rebuild
    with proper `-arch=` or update drivers.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cudaErrorNoKernelImageForDevice`：工具包对于你的 GPU 来说太新/太旧；使用正确的 `-arch=` 重建或更新驱动程序。'
- en: '`CUBLAS_STATUS_ALLOC_FAILED` / OOM: Lower batch size, sequence length, or switch
    to BF16 if supported.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CUBLAS_STATUS_ALLOC_FAILED` / OOM：降低批次大小、序列长度，或如果支持则切换到 BF16。'
- en: 'Diverging loss with FP16: Increase loss scale (if configurable) or try BF16;
    confirm master-weight updates are in FP32.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 FP16 的发散损失：增加损失缩放（如果可配置）或尝试 BF16；确认主权重更新是在 FP32 中。
- en: 'cuDNN errors: Rebuild without `USE_CUDNN` to verify the base path works, then
    revisit versions/paths.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: cuDNN 错误：不使用 `USE_CUDNN` 重建以验证基本路径是否正常工作，然后重新访问版本/路径。
- en: 'The takeaway: `train_gpt2.cu` is the practical, fast trainer: mixed precision,
    optional FlashAttention, and ready to scale. You keep the same readable training
    loop while tapping your GPU’s tensor cores for large speedups and much better
    memory efficiency.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 吸取的经验：`train_gpt2.cu` 是实用的快速训练器：混合精度，可选的 FlashAttention，并且可以扩展。你保持相同的可读训练循环，同时利用
    GPU 的张量核心实现大幅加速和更好的内存效率。
- en: 7\. Starter Artifacts & Data Prep (`dev/download_starter_pack.sh`, `dev/data/`)
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7. 入门工具和数据处理 (`dev/download_starter_pack.sh`, `dev/data/`)
- en: 'Before you can actually train or test a model in *llm.c*, you need a few essential
    artifacts: the tokenizer, a dataset, and a config. These files aren’t stored in
    the repo directly (they’re too large and often under different licenses), so the
    project provides scripts to fetch or generate them. This is where the `dev/` folder
    comes into play.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在你实际上可以在 *llm.c* 中训练或测试模型之前，你需要一些基本工具：分词器、数据集和配置文件。这些文件没有直接存储在存储库中（它们太大，并且通常有不同的许可证），因此项目提供了脚本来检索或生成它们。这就是
    `dev/` 文件夹发挥作用的地方。
- en: The Starter Pack Script
  id: totrans-281
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 入门套件脚本
- en: 'The easiest way to get going is with:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 开始的最简单方法是使用：
- en: '[PRE24]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This script pulls down a ready-made bundle containing:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 此脚本下载一个现成的捆绑包，其中包含：
- en: '`gpt2_tokenizer.bin` - The GPT-2 byte-pair encoding (BPE) tokenizer in binary
    format.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gpt2_tokenizer.bin` - GPT-2 字节对编码 (BPE) 分词器的二进制格式。'
- en: '`train.bin` / `val.bin` - Pre-tokenized training and validation datasets, often
    based on OpenWebText or Tiny Shakespeare for demos.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train.bin` / `val.bin` - 预分词的训练和验证数据集，通常基于 OpenWebText 或 Tiny Shakespeare
    用于演示。'
- en: Model configs - A JSON or header file that sets hyperparameters like layers,
    hidden size, and number of heads for GPT-2 124M.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型配置 - 一个 JSON 或头部文件，用于设置 GPT-2 124M 的超参数，如层数、隐藏大小和头数。
- en: 'Think of this as your “starter kit”: it contains just enough to run a demo
    and see training loss decreasing without setting up a full-scale dataset pipeline
    yourself.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 将其视为你的“入门套件”：它包含足够的资源来运行演示并看到训练损失下降，而无需自己设置完整规模的数据集管道。
- en: The Tokenizer File (`gpt2_tokenizer.bin`)
  id: totrans-289
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分词器文件 (`gpt2_tokenizer.bin`)
- en: This is a binary representation of GPT-2’s tokenizer vocabulary. It maps raw
    text (like `"Hello world"`) into integer token IDs, which are the actual inputs
    to the model.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 GPT-2 分词器词汇的二进制表示。它将原始文本（如 `"Hello world"`) 映射到整数标记 ID，这些 ID 是模型的实际输入。
- en: Why binary? It’s faster to load in C than parsing a text-based vocabulary.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么是二进制？在 C 中加载比解析基于文本的词汇更快。
- en: Size? ~500 KB, representing ~50,000 tokens.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大小？约 500 KB，代表约 50,000 个标记。
- en: Role in training? Used in both the dataloader (to prepare inputs) and the sampler
    (to decode outputs).
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练中的作用？既用于数据加载器（准备输入）也用于采样器（解码输出）。
- en: Without this file, the model can’t understand text at all-it would just be manipulating
    meaningless numbers.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 没有这个文件，模型根本无法理解文本——它只会操作无意义的数字。
- en: Dataset Files (`train.bin`, `val.bin`)
  id: totrans-295
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集文件 (`train.bin`, `val.bin`)
- en: 'Each dataset file is a binary blob containing:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 每个数据集文件都是一个包含二进制数据块，其中：
- en: A header (about 1 KB) describing sequence length, vocab size, and other metadata.
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个描述序列长度、词汇大小和其他元数据的头部（大约 1 KB）。
- en: A stream of token IDs (`uint16`), representing the text corpus already tokenized.
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一系列标记 ID (`uint16`)，代表已经分词的文本语料库。
- en: This design means the C dataloader can simply `fread()` chunks of tokens into
    memory, without needing to tokenize text on the fly. It’s fast and memory-efficient,
    perfect for a lean project like *llm.c*.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设计意味着C数据加载器可以简单地使用 `fread()` 将标记块读取到内存中，而无需在运行时对文本进行分词。它既快又节省内存，非常适合像 *llm.c*
    这样的精简项目。
- en: 'The script usually fetches two versions:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本通常会获取两个版本：
- en: Training set (`train.bin`)
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练集（`train.bin`）
- en: Validation set (`val.bin`)
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证集（`val.bin`）
- en: That way, the training loop can occasionally switch to validation mode and report
    a validation loss, helping you track overfitting.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，训练循环可以偶尔切换到验证模式并报告验证损失，帮助您跟踪过拟合。
- en: The `dev/data/` Folder
  id: totrans-304
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '`dev/data/` 文件夹'
- en: 'If you want to generate your own datasets, this is where you’ll find the tools:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想生成自己的数据集，您将在这里找到工具：
- en: Scripts for Tiny Shakespeare, OpenWebText, or other corpora.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于Tiny Shakespeare、OpenWebText或其他语料库的脚本。
- en: Utilities to tokenize text using the GPT-2 tokenizer and write out the `.bin`
    format.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用GPT-2分词器对文本进行分词并输出 `.bin` 格式的实用工具。
- en: Small Python snippets to check dataset statistics (like number of tokens or
    average sequence length).
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小型Python片段，用于检查数据集统计信息（如标记数或平均序列长度）。
- en: 'For example, if you wanted to try fine-tuning GPT-2 on your own text files,
    you’d:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果您想尝试在自己的文本文件上微调GPT-2，您会：
- en: Run a preprocessing script in `dev/data/` to tokenize and save your corpus.
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `dev/data/` 中运行预处理脚本以分词并保存您的语料库。
- en: Point `train_gpt2.c` or `train_gpt2.cu` to your new `train.bin` and `val.bin`.
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `train_gpt2.c` 或 `train_gpt2.cu` 指向您的 `train.bin` 和 `val.bin`。
- en: Kick off training as usual.
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照常规启动训练。
- en: Why Preprocessing Matters
  id: totrans-313
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么预处理很重要
- en: Tokenization and dataset preparation can be surprisingly heavy in Python, especially
    for large corpora. By precomputing everything into compact `.bin` files, *llm.c*
    keeps the runtime training loop as simple as possible-just reading arrays of integers
    and feeding them into the model.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，分词和数据集准备可能会非常耗时，尤其是对于大型语料库。通过将所有内容预计算到紧凑的 `.bin` 文件中，*llm.c* 使运行时训练循环尽可能简单——只需读取整数数组并将它们输入到模型中。
- en: This separation of concerns (preprocessing vs. training) is what makes the training
    code clean and focused.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 这种关注点的分离（预处理与训练）使得训练代码干净且专注。
- en: Quick Sanity Check
  id: totrans-316
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 快速检查
- en: 'After running `download_starter_pack.sh`, you should see these files in your
    working directory:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行 `download_starter_pack.sh` 之后，您应该在您的当前工作目录中看到这些文件：
- en: '[PRE25]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: If any are missing, re-run the script. Without them, the trainer will exit with
    a file-not-found error.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有任何缺失，重新运行脚本。没有它们，训练器将因找不到文件而退出。
- en: 'The takeaway: The starter pack is your ticket to running *llm.c* right away.
    It gives you a tokenizer and datasets in exactly the format the C code expects.
    Later, when you’re ready to train on your own text or scale up, the `dev/data/`
    folder shows you how to prepare custom datasets the same way.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 总结：入门包是您立即运行 *llm.c* 的通行证。它提供了C代码期望的格式化的分词器和数据集。稍后，当您准备好在自己的文本上训练或扩展时，`dev/data/`
    文件夹会向您展示如何以相同的方式准备自定义数据集。
- en: 8\. Debugging Tips & IDE Stepping (`-g`)
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8. 调试技巧与IDE逐步执行（`-g`）
- en: Even though *llm.c* is designed to be small and readable, training a transformer
    model is still a big program with lots of moving parts. When something goes wrong-whether
    it’s a segmentation fault, `NaN` losses, or unexpected results-you’ll want to
    be able to debug effectively. That’s where debug builds and IDE stepping come
    in.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管*llm.c*设计得小巧且易于阅读，但训练一个转换器模型仍然是一个包含许多部件的大程序。当出现问题时——无论是段错误、`NaN` 损失还是意外结果——您将希望能够有效地进行调试。这就是调试构建和IDE逐步执行发挥作用的地方。
- en: Why Debug Mode Exists
  id: totrans-323
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么存在调试模式
- en: 'By default, the Makefile compiles with heavy optimization (`-O3`). That makes
    the code run fast, but it also makes debugging harder:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Makefile使用重优化（`-O3`）进行编译。这使得代码运行得很快，但也使得调试更加困难：
- en: Variables may be optimized away.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变量可能会被优化掉。
- en: Functions might get inlined so you can’t step through them clearly.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 函数可能会被内联，因此您无法清楚地逐步执行它们。
- en: The debugger may jump around unpredictably.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调试器可能会不可预测地跳转。
- en: Adding the `-g` flag (enabled with `DEBUG=1`) tells the compiler to include
    extra information in the binary so you can see exactly what the code is doing
    at runtime.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 添加 `-g` 标志（通过 `DEBUG=1` 启用）告诉编译器在二进制文件中包含额外的信息，以便您可以在运行时看到代码的确切行为。
- en: Building a Debug Binary
  id: totrans-329
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 构建调试二进制文件
- en: 'To build with debug info:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建带有调试信息的程序：
- en: '[PRE26]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This produces a slower executable, but one that works seamlessly with tools
    like:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生一个较慢的可执行文件，但它可以无缝地与像以下工具一起工作：
- en: gdb - the classic GNU debugger.
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: gdb - 经典的GNU调试器。
- en: lldb - default on macOS.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: lldb - 默认在 macOS 上。
- en: VS Code / CLion / Xcode - IDEs with integrated debuggers and GUI interfaces.
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VS Code / CLion / Xcode - 集成调试器和 GUI 界面的 IDE。
- en: Using gdb on Linux/macOS
  id: totrans-336
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在 Linux/macOS 上使用 gdb
- en: 'Start your program under gdb:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在 gdb 下启动你的程序：
- en: '[PRE27]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Inside gdb:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在 gdb 内部：
- en: 'Run the program: `run`'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行程序：`run`
- en: 'Set a breakpoint at `main`: `break main`'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 `main` 处设置断点：`break main`
- en: 'Step through line by line: `step` or `next`'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逐行执行：`step` 或 `next`
- en: 'Inspect variables: `print loss`, `print i`'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查变量：`print loss`、`print i`
- en: 'Quit: `quit`'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 退出：`quit`
- en: This is the fastest way to see exactly where a crash happens.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 这是查看崩溃发生位置最快的方法。
- en: Using an IDE
  id: totrans-346
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 IDE
- en: 'If command-line debugging feels intimidating, you can use an IDE like VS Code
    or CLion:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 如果命令行调试让你感到害怕，你可以使用 VS Code 或 CLion 这样的 IDE：
- en: Open the project folder.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 打开项目文件夹。
- en: Configure the debugger (choose `gdb` or `lldb` backend).
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置调试器（选择 `gdb` 或 `lldb` 后端）。
- en: Add breakpoints by clicking next to line numbers.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过点击行号旁边添加断点。
- en: Run the debug build (`train_gpt2` with `DEBUG=1`).
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行调试构建（`train_gpt2` 并设置 `DEBUG=1`）。
- en: Step through forward pass, backward pass, or optimizer updates.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步过前向传递、反向传递或优化器更新。
- en: This way, you can visually watch variables update with each step.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，你可以通过每一步来直观地观察变量更新。
- en: Debugging CUDA Code
  id: totrans-354
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: CUDA 代码调试
- en: 'CUDA debugging is a bit trickier, but still possible:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 调试可能有点棘手，但仍然可行：
- en: '`cuda-gdb` - NVIDIA’s GPU debugger, works like gdb but supports stepping into
    kernels.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cuda-gdb` - NVIDIA 的 GPU 调试器，类似于 gdb，但支持进入内核。'
- en: Nsight Systems / Nsight Compute - graphical profilers/debuggers that let you
    trace kernel launches, memory transfers, and GPU utilization.
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nsight Systems / Nsight Compute - 图形化的分析器/调试器，允许你跟踪内核启动、内存传输和 GPU 利用率。
- en: If your CUDA code crashes with cryptic messages like `illegal memory access`,
    `cuda-gdb` can help pinpoint the kernel and even the exact line.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的 CUDA 代码崩溃并显示像 `非法内存访问`、`cuda-gdb` 这样的神秘信息，可以帮助定位内核甚至确切的行。
- en: Debugging Common Issues in llm.c
  id: totrans-359
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在 llm.c 中调试常见问题
- en: File not found → Make sure `gpt2_tokenizer.bin`, `train.bin`, and `val.bin`
    are downloaded.
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文件未找到 → 确保已下载 `gpt2_tokenizer.bin`、`train.bin` 和 `val.bin`。
- en: Segfault at malloc/fread → Check file paths and dataset sizes.
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 malloc/fread 处发生段错误 → 检查文件路径和数据集大小。
- en: Loss becomes NaN →
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 损失变为 NaN →
- en: 'On CPU: check for division by zero in normalization.'
  id: totrans-363
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 CPU 上：检查归一化中的除以零。
- en: 'On GPU: check loss scaling (mixed precision) or try FP32 path for comparison.'
  id: totrans-364
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 GPU 上：检查损失缩放（混合精度）或尝试 FP32 路径进行比较。
- en: Mismatch with PyTorch tests → Run `test_gpt2` or `test_gpt2cu` and compare outputs;
    this usually isolates whether the bug is in forward pass, backward pass, or optimizer.
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与 PyTorch 测试不匹配 → 运行 `test_gpt2` 或 `test_gpt2cu` 并比较输出；这通常可以隔离错误是否出现在前向传递、反向传递或优化器中。
- en: Logging & Sanity Checks
  id: totrans-366
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 日志记录与合理性检查
- en: 'When debugging, it helps to add extra logging. The repo already has a lightweight
    logger, but you can also sprinkle `printf`s (on CPU) or `cudaDeviceSynchronize();
    printf(...)` (on GPU) to track values. For example:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 在调试时，添加额外的日志记录很有帮助。仓库已经有一个轻量级的日志记录器，但你也可以在 CPU 上添加 `printf`s 或在 GPU 上添加 `cudaDeviceSynchronize();
    printf(...)` 来跟踪值。例如：
- en: '[PRE28]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Sometimes the quickest fix is just to print and see what’s going on.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 有时最快的修复方法就是打印出来看看发生了什么。
- en: Best Practices for Beginners
  id: totrans-370
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 初学者最佳实践
- en: Start with the CPU build when learning-it’s easier to debug than CUDA.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习时从 CPU 构建开始 - 它比 CUDA 更容易调试。
- en: Always keep a small dataset (like Tiny Shakespeare) for fast iteration.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总是保持一个小数据集（如 Tiny Shakespeare），以便快速迭代。
- en: Compare against the PyTorch reference for the same batch to catch subtle errors.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将其与同一批次的 PyTorch 参考进行比较以捕捉细微的错误。
- en: Use `DEBUG=1` whenever you hit strange behavior-you’ll trade speed for clarity,
    which is usually worth it when learning.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每当你遇到奇怪的行为时，使用 `DEBUG=1` - 你会牺牲速度以换取清晰度，这在学习时通常值得。
- en: 'The takeaway: Debug builds (`-g`) turn *llm.c* from a black box into a step-through
    learning tool. With gdb, lldb, or an IDE, you can pause at any line, inspect variables,
    and understand exactly how GPT-2 training works inside C or CUDA. It’s slower,
    but it’s the clearest way to learn and fix issues.'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 吸取的教训：调试构建（`-g`）将 *llm.c* 从一个黑盒变成了一个可以逐步学习的工具。使用 gdb、lldb 或 IDE，你可以在任何一行暂停，检查变量，并确切了解
    GPT-2 训练在 C 或 CUDA 中的工作方式。它速度较慢，但这是学习并修复问题的最清晰方式。
- en: 9\. Project Constraints & Readability Contract
  id: totrans-376
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9. 项目约束与可读性契约
- en: 'The *llm.c* project isn’t trying to be the fastest or most feature-rich GPT-2
    trainer. Instead, it has a very deliberate set of constraints-rules the author
    imposes on the codebase to keep it approachable and educational. You can think
    of these as the “contract” between the code and the reader: certain things are
    kept simple on purpose, even if they cost some performance.'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '*llm.c*项目并不是试图成为最快的或功能最丰富的GPT-2训练器。相反，它有一套非常明确的约束——作者对代码库施加的规则，以保持其可接近性和教育性。您可以将这些视为代码和读者之间的“契约”：某些事情故意保持简单，即使这会牺牲一些性能。'
- en: Minimalism over Optimizations
  id: totrans-378
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 简约胜于优化
- en: 'No processor-specific intrinsics: You won’t see AVX, NEON, or other hardware-tuned
    assembly calls in the CPU path.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有处理器特定的内联函数：您在CPU路径中不会看到AVX、NEON或其他硬件优化的汇编调用。
- en: 'No fancy template metaprogramming: Unlike in C++ frameworks, here you get plain
    C structs and functions.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有花哨的模板元编程：与C++框架不同，这里您得到的是普通的C结构和函数。
- en: 'No exotic libraries: Aside from cuBLAS/cuDNN for GPU acceleration, most functionality
    is implemented directly.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有奇特的库：除了用于GPU加速的cuBLAS/cuDNN外，大多数功能都是直接实现的。
- en: This means the code runs almost anywhere, and you don’t need to understand deep
    compiler tricks to follow what’s going on.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着代码几乎可以在任何地方运行，您不需要理解深度的编译器技巧就能理解正在发生的事情。
- en: Transparency over Abstraction
  id: totrans-383
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 透明度高于抽象
- en: Every operation is visible in the source. For example, instead of calling a
    framework function like `nn.CrossEntropyLoss`, you’ll find an explicit forward
    and backward pass coded in C.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个操作都在源代码中可见。例如，您不会调用像`nn.CrossEntropyLoss`这样的框架函数，而是会发现用C编写的显式前向和反向传递代码。
- en: Data loading, tokenization, optimizer steps, and schedulers are all implemented
    as separate, small modules in `llmc/`.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据加载、标记化、优化器步骤和调度器都在`llmc/`中作为独立的、小的模块实现。
- en: You don’t need to guess what’s happening-if you’re curious, you can open the
    corresponding `.h` file and see the exact code.
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您不需要猜测正在发生的事情——如果您好奇，您可以打开相应的`.h`文件并查看确切的代码。
- en: 'The guiding idea: if something is central to training GPT-2, you should be
    able to read and understand it.'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 指导思想：如果某事对训练GPT-2至关重要，您应该能够阅读和理解它。
- en: Performance Where It Matters (but No More)
  id: totrans-388
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在需要的地方提供性能（但不超过）
- en: OpenMP pragmas are allowed in CPU builds, because they give large speedups with
    minimal extra code.
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU构建允许使用OpenMP指令，因为它们可以提供大量的速度提升，而额外代码最少。
- en: cuBLAS/cuDNN are used for GPU matmuls and attention, because re-implementing
    them would be a distraction and would make the project impossibly large.
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: cuBLAS/cuDNN用于GPU矩阵乘法和注意力，因为重新实现它们会分散注意力，并使项目变得不可行。
- en: But the project avoids unnecessary complexity-no kernel fusion, no elaborate
    caching layers, no half-implemented “framework” abstractions.
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 但项目避免了不必要的复杂性——没有内核融合，没有复杂的缓存层，没有半实现的“框架”抽象。
- en: This balance ensures you can still run experiments at a reasonable speed, but
    the code never sacrifices readability.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 这种平衡确保您仍然可以以合理的速度运行实验，但代码永远不会牺牲可读性。
- en: Educational First
  id: totrans-393
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 教育优先
- en: 'The code is written to teach, not to win benchmarks. That means:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的编写是为了教学，而不是为了赢得基准测试。这意味着：
- en: Variable names are descriptive, not cryptic.
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变量名具有描述性，而不是晦涩难懂。
- en: Comments explain not just *what* happens, but also *why*.
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注释不仅解释了发生了什么，还解释了为什么。
- en: Files are kept small and focused, rather than sprawling across dozens of layers
    of abstraction.
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文件被保持得小而专注，而不是横跨数十层抽象。
- en: There’s a matching PyTorch reference implementation so you can always check
    your understanding against a familiar baseline.
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有一个匹配的PyTorch参考实现，这样您就可以始终将您的理解与熟悉的基线进行比较。
- en: Limitations You Should Expect
  id: totrans-399
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 您应该预期的限制
- en: Training is slower than PyTorch/XLA/JAX or DeepSpeed-tuned runs.
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练速度比PyTorch/XLA/JAX或DeepSpeed调优运行慢。
- en: Multi-GPU scaling is functional but not heavily optimized.
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多GPU扩展功能正常，但未进行大量优化。
- en: Only GPT-2 architectures are covered-don’t expect GPT-3 or transformer variants.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只涵盖GPT-2架构——不要期待GPT-3或transformer变体。
- en: Features like dataset streaming, checkpoint sharding, or advanced distributed
    tricks are intentionally left out.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 故意省略了像数据集流、检查点分片或高级分布式技巧这样的功能。
- en: These are not bugs-they’re conscious trade-offs to keep the codebase small,
    sharp, and didactic.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 这些不是错误——它们是有意识的权衡，以保持代码库小、锐利和教学性。
- en: Why This Matters for You
  id: totrans-405
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 这为什么对您很重要
- en: 'If you’re learning how transformers work, this contract is a gift:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在学习transformers的工作原理，这份契约是一份礼物：
- en: You won’t get lost in performance hacks.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您不会迷失在性能黑客中。
- en: You won’t fight through an abstraction jungle.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你不会在抽象丛林中挣扎。
- en: You’ll always know that what you’re reading is close to the “pure” algorithmic
    idea.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将始终知道你所阅读的内容接近“纯粹”的算法思想。
- en: On the flip side, if you’re aiming for production-grade speed, you’ll need to
    layer more on top. But that’s outside the mission of *llm.c*.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 从另一方面来看，如果你追求生产级速度，你需要在上面添加更多层。但这超出了*llm.c*的任务范围。
- en: 'The takeaway: *llm.c* is bound by a readability contract: clarity over raw
    speed, transparency over abstraction, minimalism over complexity. These constraints
    keep the project small enough to fit in your head, while still powerful enough
    to reproduce GPT-2 training. It’s a teaching lab, not a racing car-and that’s
    exactly why it’s valuable.'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 吸取的教训是：*llm.c*受限于可读性合约：清晰胜于原始速度，透明胜于抽象，简约胜于复杂。这些限制使项目足够小，可以装进你的头脑，同时仍然足够强大，可以重现GPT-2的训练。它是一个教学实验室，而不是赛车——这正是它的价值所在。
- en: 10\. Community, Discussions, and Learning Path
  id: totrans-412
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10. 社区、讨论和学习路径
- en: The last piece of the quickstart isn’t about code at all-it’s about the people
    and resources around the project. *llm.c* has grown into more than just a single
    repository; it has become a meeting point for learners, tinkerers, and researchers
    who want to strip large language models down to their essentials. Understanding
    this community layer is just as important as understanding the code itself.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 快速入门的最后一步根本不是关于代码，而是关于项目和周围的人及资源。“*llm.c*”已经不仅仅是一个单一存储库；它已成为想要将大型语言模型简化到其本质的学习者、修补者和研究者的聚会点。理解这一社区层与理解代码本身同样重要。
- en: Discussions and Issues on GitHub
  id: totrans-414
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: GitHub上的讨论和问题
- en: 'The project’s Discussions tab is full of valuable context:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 项目讨论标签页充满了有价值的背景信息：
- en: Developers asking about build errors on different platforms (Linux, macOS, Windows).
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 询问不同平台（Linux、macOS、Windows）上构建错误的开发者。
- en: Explorations of how to extend *llm.c* to train larger GPT-2 models (355M, 774M,
    1.6B).
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索如何将*llm.c*扩展到训练更大的GPT-2模型（355M、774M、1.6B）。
- en: Reports on multi-GPU and MPI runs, including troubleshooting NCCL hangs and
    performance bottlenecks.
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多GPU和MPI运行的报告，包括解决NCCL挂起和性能瓶颈。
- en: Debates on mixed precision vs FP32 vs BF16 stability.
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于混合精度与FP32与BF16稳定性的辩论。
- en: Reading these threads is like looking over the shoulders of hundreds of other
    learners. You’ll see not only the official answers but also the thought process
    of people solving problems in real time.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读这些帖子就像是在数百名其他学习者的肩膀上观察。你不仅会看到官方答案，还会看到人们实时解决问题的思维过程。
- en: Roadmap and Contributions
  id: totrans-421
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 路线图和贡献
- en: 'The README and issues sometimes hint at where the project might grow:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: README和问题有时会暗示项目可能的发展方向：
- en: Making the CUDA kernels more modular in `dev/cuda/`.
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`dev/cuda/`中使CUDA内核更模块化。
- en: Simplifying multi-GPU startup for clusters.
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简化集群的多GPU启动。
- en: Adding small tutorial-style docs (like the LayerNorm walkthrough).
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加小型教程风格的文档（如LayerNorm教程）。
- en: 'The project is open to contributions, but it follows the same minimalist philosophy.
    If you’re thinking of contributing, remember: the goal is clarity first, performance
    second.'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 该项目欢迎贡献，但遵循相同的极简主义哲学。如果你打算贡献，请记住：目标是清晰第一，性能第二。
- en: External Learning Resources
  id: totrans-427
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 外部学习资源
- en: 'While *llm.c* is self-contained, it pairs nicely with outside material:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然*llm.c*是自包含的，但它与外部材料搭配得很好：
- en: The PyTorch reference implementation in `train_gpt2.py` is your canonical oracle
    for correctness.
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_gpt2.py`中的PyTorch参考实现是正确性的权威来源。'
- en: The GPT-2 paper gives the architecture background.
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-2论文提供了架构背景。
- en: CUDA and cuBLAS/cuDNN docs explain the GPU APIs that the project calls into.
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA和cuBLAS/cuDNN文档解释了项目调用的GPU API。
- en: Community blog posts often walk through specific sections of the code in plain
    English, making it easier to digest.
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 社区博客文章通常用简单的英语解释代码的特定部分，这使得消化起来更容易。
- en: By combining the code, the paper, and these resources, you can triangulate a
    much deeper understanding.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合代码、论文和这些资源，你可以获得更深入的理解。
- en: A Suggested Learning Path
  id: totrans-434
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 建议的学习路径
- en: 'If you’re coming to *llm.c* as a beginner, here’s a natural progression:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你作为初学者来到*llm.c*，这里有一个自然的进展：
- en: Run the CPU trainer (`train_gpt2.c`) on Tiny Shakespeare. Watch the loss decrease.
  id: totrans-436
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Tiny Shakespeare上运行CPU训练器（`train_gpt2.c`），观察损失下降。
- en: Step through the code with `DEBUG=1`, confirming that you understand forward,
    backward, and optimizer steps.
  id: totrans-437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`DEBUG=1`逐步检查代码，确认你理解了正向、反向和优化步骤。
- en: Move to the FP32 CUDA trainer to see how the same loop runs on GPU.
  id: totrans-438
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到FP32 CUDA训练器，看看相同的循环如何在GPU上运行。
- en: Switch to the modern CUDA trainer (`train_gpt2.cu`) and learn how mixed precision
    works.
  id: totrans-439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 切换到现代CUDA训练器（`train_gpt2.cu`）并了解混合精度是如何工作的。
- en: Experiment with dataset scripts in `dev/data/`-try your own text corpus.
  id: totrans-440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`dev/data/`中的数据集脚本上进行实验，尝试你自己的文本语料库。
- en: Read the LayerNorm doc in `doc/` to deepen your theory-practice connection.
  id: totrans-441
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`doc/`目录中阅读LayerNorm文档，以加深你的理论与实践联系。
- en: Explore multi-GPU runs with MPI/NCCL if you have access to multiple GPUs.
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你有多块GPU，可以探索使用MPI/NCCL的多GPU运行。
- en: Follow GitHub Discussions for real-world debugging and scaling stories.
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关注GitHub讨论，了解实际的调试和扩展故事。
- en: Why This Matters
  id: totrans-444
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 这为什么重要
- en: Code alone is not enough. The community context, the discussions, and the learning
    path make *llm.c* a living project. By engaging with them, you avoid the feeling
    of learning in isolation. You’ll see others wrestling with the same challenges,
    and you’ll have a clearer sense of what to try next.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 仅代码是不够的。社区背景、讨论和学习路径使`llm.c`成为一个活生生的项目。通过参与其中，你可以避免孤立学习的感受。你会看到其他人正在努力克服相同的挑战，你将更清楚地了解下一步该尝试什么。
- en: 'The takeaway: Beyond the files and scripts, *llm.c* is a community-driven learning
    environment. GitHub issues, discussions, reference docs, and external tutorials
    all form part of the “extended classroom.” If the code is the lab bench, the community
    is the set of lab partners who help you figure things out along the way.'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 吸取的教训：除了文件和脚本之外，`llm.c`是一个社区驱动的学习环境。GitHub问题、讨论、参考文档和外部教程都构成了“扩展课堂”的一部分。如果代码是实验室的工作台，那么社区就是那些帮助你一路解决问题的实验室伙伴。
- en: Chapter 2\. Data, Tokenization, and Loaders
  id: totrans-447
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二章：数据、分词和加载器
- en: 11\. GPT-2 Tokenizer Artifacts (`gpt2_tokenizer.bin`)
  id: totrans-448
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11. GPT-2分词器工件（`gpt2_tokenizer.bin`）
- en: A language model like GPT-2 doesn’t directly understand English, Vietnamese,
    or any other natural language. Instead, it understands numbers. These numbers
    are called tokens. A tokenizer is the tool that translates between human text
    and tokens. In *llm.c*, the GPT-2 tokenizer is stored in a small file called `gpt2_tokenizer.bin`.
    This file is the key that lets the model read input text and produce output text
    that we can understand.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于GPT-2的语言模型并不直接理解英语、越南语或其他任何自然语言。相反，它理解数字。这些数字被称为标记。分词器是介于人类文本和标记之间的翻译工具。在`llm.c`中，GPT-2分词器存储在一个名为`gpt2_tokenizer.bin`的小文件中。这个文件是模型能够读取输入文本并产生我们可以理解的输出文本的关键。
- en: What This File Contains
  id: totrans-450
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 该文件包含的内容
- en: 'The file `gpt2_tokenizer.bin` is a binary version of GPT-2’s tokenizer. It
    includes:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 文件`gpt2_tokenizer.bin`是GPT-2分词器的二进制版本。它包括：
- en: '| Component | Purpose |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| 组件 | 目的 |'
- en: '| --- | --- |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Byte vocabulary (0–255) | Makes sure every possible character can be represented.
    |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| 字节词汇（0–255） | 确保每个可能的字符都可以表示。|'
- en: '| Merge rules (BPE) | Combines frequent sequences like “ing” or ” the” into
    single tokens for efficiency. |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| 合并规则（BPE） | 将“ing”或“the”等频繁出现的序列合并为单个标记以提高效率。|'
- en: '| Vocabulary size (~50,257) | Defines how many distinct tokens GPT-2 can work
    with. |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '| 词汇量大小（约50,257） | 定义了GPT-2可以处理的独特标记的数量。|'
- en: '| Mapping IDs ↔︎ text | Lets the program turn model outputs (numbers) back
    into human-readable strings. |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '| 映射ID与文本 | 允许程序将模型输出（数字）转换回人类可读的字符串。|'
- en: Instead of being written as JSON or text, the tokenizer is stored in binary
    form. This allows *llm.c* to load it very quickly using a simple file read, which
    keeps the code clean and fast.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 分词器不是以JSON或文本的形式编写，而是以二进制形式存储。这允许`llm.c`通过简单的文件读取快速加载它，从而保持代码简洁和快速。
- en: Where It Comes From
  id: totrans-459
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 它从何而来
- en: 'You don’t need to build this file by hand. The repository provides a script
    to download it, along with small training and validation datasets:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 你不需要手动构建这个文件。仓库提供了一个脚本用于下载它，以及一些小的训练和验证数据集：
- en: '[PRE29]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: After running the script, you should see `gpt2_tokenizer.bin`, `train.bin`,
    and `val.bin` in your working directory. If the tokenizer is missing, the program
    cannot run because it won’t know how to interpret text.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 运行脚本后，你应该在你的工作目录中看到`gpt2_tokenizer.bin`、`train.bin`和`val.bin`。如果分词器缺失，程序将无法运行，因为它不知道如何解释文本。
- en: How the Code Uses It
  id: totrans-463
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 代码如何使用它
- en: During training, the tokenizer is not active because the datasets (`train.bin`
    and `val.bin`) are already pre-tokenized into integers. This keeps the training
    loop fast and simple.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练期间，分词器不活跃，因为数据集（`train.bin`和`val.bin`）已经预先分词为整数。这使训练循环快速且简单。
- en: During sampling or evaluation, the tokenizer becomes important again. After
    the model predicts a sequence of token IDs, the tokenizer translates those numbers
    back into text that you can read on your screen.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 在采样或评估期间，tokenizer 再次变得重要。在模型预测一系列标记 ID 之后，tokenizer 将这些数字转换回你可以在屏幕上阅读的文本。
- en: 'The C API for the tokenizer, defined in `llmc/tokenizer.h`, provides just three
    main functions:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `llmc/tokenizer.h` 中定义的 tokenizer 的 C API 提供了三个主要功能：
- en: '[PRE30]'
  id: totrans-467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This is all you need: initialize the tokenizer from the file, decode tokens
    into text, and free memory when done.'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是你需要的：从文件中初始化 tokenizer，将标记解码为文本，并在完成后释放内存。
- en: Example Workflow in Practice
  id: totrans-469
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实践中的示例工作流程
- en: 'Initialize the tokenizer:'
  id: totrans-470
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化 tokenizer：
- en: '[PRE31]'
  id: totrans-471
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Decode a sequence of tokens back to text:'
  id: totrans-472
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将一系列标记解码回文本：
- en: '[PRE32]'
  id: totrans-473
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Clean up memory when you no longer need it:'
  id: totrans-474
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当你不再需要它时清理内存：
- en: '[PRE33]'
  id: totrans-475
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: This small cycle is enough to turn model outputs into readable sentences.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 这个小循环足以将模型输出转换为可读的句子。
- en: Why It Matters
  id: totrans-477
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: Without the tokenizer, the model cannot communicate. The tokenizer is like a
    shared dictionary between humans and the neural network. If you give the model
    text, the tokenizer converts it into numbers the model understands. When the model
    responds, the tokenizer converts its numbers back into text. If the tokenizer
    does not match the dataset, the model’s predictions will come out as gibberish.
    Keeping the tokenizer and dataset in sync is essential for correct training and
    evaluation.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 没有tokenizer，模型无法进行交流。tokenizer 就像是人类和神经网络之间共享的词典。如果你给模型文本，tokenizer 将其转换为模型能理解的数字。当模型响应时，tokenizer
    将其数字转换回文本。如果 tokenizer 与数据集不匹配，模型的预测将变成乱码。保持 tokenizer 和数据集同步对于正确的训练和评估至关重要。
- en: Try It Yourself
  id: totrans-479
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 试试看
- en: 'Here are a few small exercises you can do to understand the tokenizer better:'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些小练习可以帮助你更好地理解 tokenizer：
- en: 'Check that the file exists: After running the starter pack script, verify that
    `gpt2_tokenizer.bin` is in your directory. Try running the trainer without it
    and observe the error message.'
  id: totrans-481
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查文件是否存在：在运行启动包脚本后，验证 `gpt2_tokenizer.bin` 是否在你的目录中。尝试在没有它的情况下运行训练器，并观察错误信息。
- en: 'Inspect vocab size: Run the trainer and look for the line that prints `vocab_size:
    50257`. Compare this with `padded_vocab_size: 50304`. Why do you think padding
    helps GPUs?'
  id: totrans-482
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '检查词汇表大小：运行训练器，寻找打印 `vocab_size: 50257` 的行。将其与 `padded_vocab_size: 50304` 进行比较。你认为为什么填充有助于
    GPU？'
- en: 'Decode a sequence manually: Write a short C program that loads the tokenizer
    and decodes a fixed list of token IDs (for example `[464, 3290, 318]`). Observe
    what text you get.'
  id: totrans-483
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 手动解码一系列标记：编写一个简短的 C 程序，加载 tokenizer 并解码一系列固定的标记 ID（例如 `[464, 3290, 318]`）。观察你得到什么文本。
- en: 'Mismatch experiment: If you build your own dataset with a different tokenizer
    (say, a custom vocabulary), try decoding it with `gpt2_tokenizer.bin`. Notice
    how the output becomes meaningless, showing why consistency matters.'
  id: totrans-484
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不匹配实验：如果你使用不同的 tokenizer（比如自定义词汇表）构建自己的数据集，尝试用 `gpt2_tokenizer.bin` 解码它。注意输出变得毫无意义，这显示了为什么一致性很重要。
- en: 'Dataset + tokenizer link: Open `train.bin` in a hex viewer. You’ll see it’s
    just numbers. Use the tokenizer to decode the first few hundred tokens and see
    real text emerge.'
  id: totrans-485
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据集 + tokenizer 链接：在十六进制查看器中打开 `train.bin`。你会发现它只是数字。使用 tokenizer 解码前几百个标记，你会看到真实的文本出现。
- en: The Takeaway
  id: totrans-486
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸取的经验
- en: '`gpt2_tokenizer.bin` is a tiny but vital file. It is the bridge that allows
    the model and humans to speak the same language. Training is efficient because
    all data is pre-tokenized, and when you want to see what the model has written,
    the tokenizer turns raw numbers back into words. Without it, the entire system
    would be silent.'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '`gpt2_tokenizer.bin` 是一个微小但至关重要的文件。它是模型和人类能够使用同一种语言交流的桥梁。训练效率高，因为所有数据都是预先分词的，当你想查看模型写的内容时，tokenizer
    将原始数字转换回单词。没有它，整个系统将陷入沉默。'
- en: 12\. Binary Dataset Format (`train.bin` and `val.bin`)
  id: totrans-488
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12. 二进制数据集格式 (`train.bin` 和 `val.bin`)
- en: 'Just like the tokenizer turns text into numbers, the datasets in *llm.c* are
    stored as numbers too. Instead of reading plain text files like `.txt`, the training
    and validation data are kept in simple binary files: `train.bin` and `val.bin`.
    These files are the fuel for the training loop.'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 tokenizer 将文本转换为数字一样，*llm.c* 中的数据集也是以数字的形式存储的。与读取纯文本文件（如 `.txt`）不同，训练数据和验证数据被保存在简单的二进制文件中：`train.bin`
    和 `val.bin`。这些文件是训练循环的燃料。
- en: What These Files Look Like
  id: totrans-490
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 这些文件看起来是什么样子
- en: 'At first glance, `train.bin` and `val.bin` look like unreadable blobs if you
    open them in a text editor. That’s because they are not meant to be human-readable.
    They contain:'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 初看，如果你在文本编辑器中打开 `train.bin` 和 `val.bin`，它们看起来像不可读的块。这是因为它们不是为人类可读而设计的。它们包含：
- en: '| Part | Description |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
  zh: '| 部分 | 描述 |'
- en: '| --- | --- |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| A tiny header (about 1 KB) | Stores metadata such as sequence length and
    vocab size. |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
  zh: '| 一个微小的头部（约 1 KB） | 存储诸如序列长度和词汇大小等元数据。 |'
- en: '| A stream of token IDs (`uint16`) | Every tokenized word piece from the dataset,
    saved as 16-bit integers. |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
  zh: '| 一串标记 ID (`uint16`) | 数据集中的每个标记化词元，保存为 16 位整数。 |'
- en: Each integer represents one token from the tokenizer’s vocabulary. Since GPT-2
    has a vocabulary of about 50,000 tokens, 16-bit integers (`uint16_t`) are enough
    to store them all.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 每个整数代表来自标记器词汇表中的一个标记。由于 GPT-2 的词汇量约为 50,000 个标记，16 位整数（`uint16_t`）足以存储它们。
- en: Why Binary Format?
  id: totrans-497
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么选择二进制格式？
- en: 'Efficiency: Instead of re-tokenizing text every time, the data is pre-tokenized
    once and stored as numbers. The trainer just reads them directly.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 效率：每次不需要重新标记文本，数据只需预先标记一次并存储为数字。训练器只需直接读取它们。
- en: 'Speed: Reading integers from a file is faster than parsing and processing raw
    text.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 速度：从文件中读取整数比解析和处理原始文本要快。
- en: 'Simplicity: The training loop only has to deal with arrays of integers-no string
    handling, no parsing, no surprises.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单性：训练循环只需处理整数数组——没有字符串处理，没有解析，没有意外。
- en: This choice makes the training code in *llm.c* much cleaner and faster.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 这个选择使得 *llm.c* 中的训练代码更加简洁和快速。
- en: How the Dataloader Uses Them
  id: totrans-502
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据加载器如何使用它们
- en: 'When training starts, the dataloader reads chunks of numbers from `train.bin`.
    Each chunk corresponds to one batch of size B × T:'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练开始时，数据加载器从 `train.bin` 中读取数字块。每个块对应于大小为 B × T 的一个批次：
- en: B = batch size (number of examples in a batch).
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: B = 批大小（每个批次中的示例数）。
- en: T = sequence length (number of tokens per example).
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: T = 序列长度（每个示例中的标记数）。
- en: For example, if `B = 8` and `T = 1024`, the dataloader will read `8 × 1024 =
    8192` token IDs from the file, reshape them into sequences, and feed them to the
    model.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果 `B = 8` 且 `T = 1024`，数据加载器将从文件中读取 `8 × 1024 = 8192` 个标记 ID，将它们重塑为序列，并将它们喂给模型。
- en: The validation file (`val.bin`) works the same way but is only used occasionally
    during training to measure validation loss. This helps detect overfitting.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 验证文件（`val.bin`）的工作方式相同，但在训练过程中仅偶尔使用来衡量验证损失。这有助于检测过拟合。
- en: Workflow in Code
  id: totrans-508
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 代码中的工作流程
- en: 'Inside the repo, you’ll see functions like these in `llmc/dataloader.h`:'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 在仓库内部，你会在 `llmc/dataloader.h` 中看到这些函数：
- en: '[PRE34]'
  id: totrans-510
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Here’s what happens step by step:'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是逐步发生的过程：
- en: Initialize with the binary file and batch/sequence sizes.
  id: totrans-512
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用二进制文件和批/序列大小进行初始化。
- en: Next batch reads the next B × T tokens into an input array and a target array.
  id: totrans-513
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一个批次将下一个 B × T 个标记读入输入数组和目标数组。
- en: Reset allows re-reading from the beginning when you start a new epoch.
  id: totrans-514
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重置允许在开始新的纪元时从开头重新读取。
- en: Free cleans up resources when training ends.
  id: totrans-515
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练结束时，Free 会清理资源。
- en: The target array is simply the same sequence shifted by one token-because language
    modeling predicts the *next* token.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 目标数组只是将相同的序列向前移动一个标记——因为语言模型预测的是 *下一个* 标记。
- en: Why It Matters
  id: totrans-517
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 它为什么重要
- en: The dataset format is what makes *llm.c* practical. Without it, the code would
    need to handle messy text, encodings, and tokenization during every training step.
    By storing clean arrays of token IDs, the training loop becomes very short and
    easy to follow. It’s a design decision that keeps the project minimal yet faithful
    to real training pipelines.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集格式是使 *llm.c* 实用的原因。没有它，代码需要在每个训练步骤中处理混乱的文本、编码和标记化。通过存储干净的标记 ID 数组，训练循环变得非常短且易于跟踪。这是一个设计决策，使项目最小化，同时忠实于真实的训练流程。
- en: Try It Yourself
  id: totrans-519
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 尝试自己操作
- en: 'Check file size: Run `ls -lh train.bin` and notice how large it is compared
    to a plain `.txt` file. Why is it smaller or larger?'
  id: totrans-520
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查文件大小：运行 `ls -lh train.bin` 并注意它与普通 `.txt` 文件相比有多大。为什么它更小或更大？
- en: 'Peek inside: Use a hex viewer (`xxd train.bin | head`) to see raw numbers.
    They won’t look like text, but they are the tokens the model trains on.'
  id: totrans-521
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看内部：使用十六进制查看器（`xxd train.bin | head`）来查看原始数字。它们看起来不像文本，但它们是模型训练所用的标记。
- en: 'Count tokens: Write a short Python or C script to count how many token IDs
    are stored in `train.bin`. This gives you a sense of dataset size.'
  id: totrans-522
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计数标记：编写一个简短的 Python 或 C 脚本来计算存储在 `train.bin` 中的标记 ID 的数量。这让你对数据集的大小有一个概念。
- en: 'Mini-dataset: Try generating your own dataset from a small `.txt` file using
    the scripts in `dev/data/`. See how the `.bin` file is created.'
  id: totrans-523
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 小型数据集：尝试使用 `dev/data/` 中的脚本从小的 `.txt` 文件生成自己的数据集。看看 `.bin` 文件是如何创建的。
- en: 'Validation experiment: During training, reduce the validation set to only a
    few batches and observe how the validation loss stabilizes or fluctuates compared
    to training loss.'
  id: totrans-524
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证实验：在训练过程中，将验证集减少到只有几个批次，并观察验证损失与训练损失相比是如何稳定或波动的。
- en: The Takeaway
  id: totrans-525
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸收要点
- en: '`train.bin` and `val.bin` may look like gibberish, but they are carefully prepared
    binary files containing token IDs. They make training faster, simpler, and more
    reproducible. The dataloader in *llm.c* reads these numbers in neat chunks and
    serves them directly to the model, letting you focus on learning how transformers
    work instead of wrestling with raw text parsing.'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: '`train.bin` 和 `val.bin` 可能看起来像是乱码，但它们是精心准备的包含标记ID的二进制文件。它们使训练更快、更简单、更可重复。*llm.c*
    中的数据加载器以整洁的块读取这些数字，并直接将它们提供给模型，让你专注于学习transformers的工作原理，而不是与原始文本解析纠缠。'
- en: 13\. Dataset Scripts in `dev/data/`
  id: totrans-527
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13. `dev/data/` 中的数据集脚本
- en: The repository doesn’t just give you ready-made binary datasets like `train.bin`
    and `val.bin`. It also provides scripts inside the `dev/data/` folder that show
    you how to create your own. These scripts are important because they demonstrate
    how raw text gets transformed into the binary format that the dataloader in *llm.c*
    expects.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 仓库不仅提供像 `train.bin` 和 `val.bin` 这样的现成二进制数据集。它还在 `dev/data/` 文件夹内提供脚本，展示如何创建自己的数据集。这些脚本很重要，因为它们展示了原始文本是如何转换为
    *llm.c* 数据加载器所期望的二进制格式的。
- en: What’s Inside `dev/data/`
  id: totrans-529
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '`dev/data/` 内部内容'
- en: 'This folder contains small Python scripts that:'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 此文件夹包含小的Python脚本，它们：
- en: '| Script | Purpose |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
  zh: '| 脚本 | 目的 |'
- en: '| --- | --- |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `prepare_shakespeare.py` | Turns the Tiny Shakespeare dataset into `train.bin`
    and `val.bin`. |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
  zh: '| `prepare_shakespeare.py` | 将Tiny Shakespeare数据集转换为 `train.bin` 和 `val.bin`。
    |'
- en: '| `prepare_openwebtext.py` | Prepares a large-scale dataset similar to the
    one GPT-2 was trained on. |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '| `prepare_openwebtext.py` | 准备一个与GPT-2训练时类似的大型数据集。 |'
- en: '| Other helpers | Tokenize raw `.txt` files, split them into train/val, and
    save to binary. |'
  id: totrans-535
  prefs: []
  type: TYPE_TB
  zh: '| 其他辅助工具 | 将原始 `.txt` 文件标记化，分割成训练/验证集，并保存到二进制格式。 |'
- en: 'Each script follows the same basic recipe:'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 每个脚本都遵循相同的基本配方：
- en: Read raw text from a source file.
  id: totrans-537
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从源文件中读取原始文本。
- en: Apply the GPT-2 tokenizer to turn text into token IDs.
  id: totrans-538
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用GPT-2标记化器将文本转换为标记ID。
- en: Split the tokens into training and validation portions.
  id: totrans-539
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将标记分割成训练和验证部分。
- en: Write the IDs into binary files that *llm.c* can read directly.
  id: totrans-540
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将ID写入 *llm.c* 可以直接读取的二进制文件。
- en: Why Preprocessing Happens Outside C
  id: totrans-541
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么预处理发生在C语言之外
- en: In C, handling text files with Unicode, punctuation, and different encodings
    is messy. Instead, preprocessing is done once in Python, where tokenizers are
    easier to use. The results are saved in a simple binary format (`uint16` IDs).
    From then on, C only has to deal with arrays of integers-clean and efficient.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 在C语言中，处理包含Unicode、标点和不同编码的文本文件很混乱。相反，预处理在Python中只进行一次，那里使用标记化器更容易。结果以简单的二进制格式（`uint16`
    ID）保存。从那时起，C语言只需要处理整数数组——干净且高效。
- en: 'This design keeps the training loop minimal: no text parsing, no string handling,
    just numbers.'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设计使训练循环最小化：没有文本解析，没有字符串处理，只有数字。
- en: 'Example: Tiny Shakespeare'
  id: totrans-544
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例：Tiny Shakespeare
- en: 'One of the simplest datasets is Tiny Shakespeare, about 1 MB of text from Shakespeare’s
    plays. The script `prepare_shakespeare.py` will:'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 其中最简单的数据集之一是Tiny Shakespeare，大约有1MB的莎士比亚戏剧文本。脚本 `prepare_shakespeare.py` 将：
- en: Read `input.txt` (the raw text).
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 读取 `input.txt`（原始文本）。
- en: Use the GPT-2 tokenizer (`gpt2_tokenizer.bin`) to turn every word and symbol
    into token IDs.
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用GPT-2标记化器（`gpt2_tokenizer.bin`）将每个单词和符号转换为标记ID。
- en: Split 90% of the data into `train.bin` and 10% into `val.bin`.
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将90%的数据分割成 `train.bin`，10%分割成 `val.bin`。
- en: After running the script, you’ll have small binary files that let you train
    GPT-2 from scratch in minutes on CPU or GPU.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 运行脚本后，你将得到小的二进制文件，让你在CPU或GPU上几分钟内从头开始训练GPT-2。
- en: 'Example: OpenWebText'
  id: totrans-550
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例：OpenWebText
- en: The script `prepare_openwebtext.py` shows how to tokenize a much larger dataset,
    closer to what GPT-2 was originally trained on. This is heavier and requires more
    disk space, but it’s useful if you want to try scaling up training to bigger models.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本 `prepare_openwebtext.py` 展示了如何标记化一个更大的数据集，更接近GPT-2最初训练的数据集。这需要更多的磁盘空间，但如果你想要尝试将训练扩展到更大的模型，这很有用。
- en: Why It Matters
  id: totrans-552
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: 'These scripts are more than convenience tools-they are examples of how to adapt
    llm.c to your own data. If you have a collection of emails, poems, or programming
    code, you can:'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 这些脚本不仅是便利工具，它们还是如何将 llm.c 适配到你的数据的示例。如果你有一系列电子邮件、诗歌或编程代码，你可以：
- en: Put them into a single `.txt` file.
  id: totrans-554
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将它们放入一个单独的 `.txt` 文件中。
- en: Modify one of the scripts in `dev/data/`.
  id: totrans-555
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改 `dev/data/` 中的脚本之一。
- en: Generate new `train.bin` and `val.bin` files.
  id: totrans-556
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成新的 `train.bin` 和 `val.bin` 文件。
- en: Train GPT-2 on your own text.
  id: totrans-557
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的文本上训练 GPT-2。
- en: By separating dataset creation from training, *llm.c* keeps the C code small
    and makes experimentation flexible.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将数据集创建与训练分离，*llm.c* 保持 C 代码小巧，并使实验灵活。
- en: Try It Yourself
  id: totrans-559
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 尝试自己操作。
- en: 'Run the Shakespeare script:'
  id: totrans-560
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行莎士比亚脚本：
- en: '[PRE35]'
  id: totrans-561
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Then check that `train.bin` and `val.bin` were created.
  id: totrans-562
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后检查 `train.bin` 和 `val.bin` 是否已创建。
- en: Open the binary files with a hex viewer and confirm that they contain only numbers.
  id: totrans-563
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用十六进制查看器打开二进制文件，并确认它们只包含数字。
- en: Modify the script to tokenize a different text file (for example, your own writing).
  id: totrans-564
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改脚本以标记化不同的文本文件（例如，你自己的写作）。
- en: 'Compare dataset sizes: Tiny Shakespeare is tiny (MBs), OpenWebText is huge
    (GBs). Observe how training speed changes depending on dataset size.'
  id: totrans-565
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较数据集大小：Tiny Shakespeare 很小（MBs），OpenWebText 很大（GBs）。观察训练速度如何根据数据集大小而变化。
- en: Re-run training with your custom dataset and watch how the model starts generating
    text in your style.
  id: totrans-566
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用你自定义的数据集重新运行训练，并观察模型如何开始以你的风格生成文本。
- en: The Takeaway
  id: totrans-567
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸收要点
- en: The `dev/data/` scripts are the bridge between raw human text and the binary
    datasets used in training. They let you prepare small demo datasets or scale up
    to larger corpora. By experimenting with these scripts, you learn how to bring
    your own data into *llm.c* and train a GPT-style model on anything you like.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: '`dev/data/` 脚本是原始人类文本和训练中使用的二进制数据集之间的桥梁。它们允许你准备小型演示数据集或扩展到更大的语料库。通过实验这些脚本，你可以学习如何将你的数据带入
    `llm.c` 并在喜欢的任何事物上训练 GPT 风格的模型。'
- en: 14\. DataLoader Design (Batching, Strides, Epochs)
  id: totrans-569
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14. DataLoader 设计（批处理、步长、周期）
- en: 'Now that the datasets are prepared as `.bin` files, we need a way to feed them
    into the model during training. This is the job of the DataLoader in *llm.c*.
    You’ll find its interface in `llmc/dataloader.h`, and its purpose is very simple:
    take a big stream of token IDs from `train.bin` or `val.bin`, cut it into manageable
    chunks, and serve those chunks to the training loop as batches.'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据集已准备为 `.bin` 文件，我们需要一种方法在训练期间将它们喂入模型。这是 `llm.c` 中 DataLoader 的任务。你可以在 `llmc/dataloader.h`
    中找到其接口，其目的是非常简单的：从 `train.bin` 或 `val.bin` 中获取大量令牌 ID 流，将其切割成可管理的块，并将这些块作为批次提供给训练循环。
- en: The Core Idea
  id: totrans-571
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 核心思想
- en: 'Training a language model requires two arrays for every batch:'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 训练语言模型需要为每个批次两个数组：
- en: 'Inputs: a sequence of token IDs, like `[The, cat, sat, on]`'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入：令牌 ID 的序列，例如 `[The, cat, sat, on]`
- en: 'Targets: the same sequence shifted by one, like `[cat, sat, on, the]`'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标：与原始序列偏移一个位置的相同序列，例如 `[cat, sat, on, the]`
- en: The model learns to predict each next token in the sequence. The DataLoader
    automates slicing these arrays out of the giant dataset file.
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 模型学习预测序列中的每个下一个令牌。DataLoader 自动从巨大的数据集文件中切割这些数组。
- en: The Interface
  id: totrans-576
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 接口
- en: 'In the code you’ll see function declarations like these:'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，你会看到像这样的函数声明：
- en: '[PRE36]'
  id: totrans-578
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Here’s what each does:'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是每个函数的作用：
- en: '`dataloader_init`: opens the dataset file, remembers batch size `B` and sequence
    length `T`.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataloader_init`: 打开数据集文件，记住批大小 `B` 和序列长度 `T`。'
- en: '`dataloader_next_batch`: returns the next chunk of `B × T` tokens (inputs)
    and their shifted version (targets).'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataloader_next_batch`: 返回下一个 `B × T` 令牌（输入）及其偏移版本（目标）。'
- en: '`dataloader_reset`: rewinds to the start of the file when an epoch ends.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataloader_reset`: 当周期结束时，回滚到文件开头。'
- en: '`dataloader_free`: closes the file and releases memory.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataloader_free`: 关闭文件并释放内存。'
- en: 'This design keeps the training loop clean: just call `next_batch` and you get
    the data ready for forward/backward passes.'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设计使训练循环保持简洁：只需调用 `next_batch`，你就可以获得用于前向/反向传递的数据。
- en: B × T Explained
  id: totrans-585
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B × T 解释
- en: 'The two most important parameters are:'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的两个参数是：
- en: '| Symbol | Meaning | Example |'
  id: totrans-587
  prefs: []
  type: TYPE_TB
  zh: '| 符号 | 含义 | 示例 |'
- en: '| --- | --- | --- |'
  id: totrans-588
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| B | Batch size (how many sequences per step) | 16 |'
  id: totrans-589
  prefs: []
  type: TYPE_TB
  zh: '| B | 批大小（每步多少个序列） | 16 |'
- en: '| T | Sequence length (how many tokens per sequence) | 1024 |'
  id: totrans-590
  prefs: []
  type: TYPE_TB
  zh: '| T | 序列长度（每个序列多少个令牌） | 1024 |'
- en: So one batch contains `B × T` tokens. For example, with `B = 16` and `T = 1024`,
    each batch holds 16,384 tokens. The DataLoader simply reads that many numbers
    from the binary file and arranges them in memory.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个批次包含 `B × T` 个标记。例如，当 `B = 16` 和 `T = 1024` 时，每个批次包含 16,384 个标记。DataLoader
    简单地从二进制文件中读取这么多数字，并将它们排列在内存中。
- en: Strides Through the Dataset
  id: totrans-592
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在数据集中穿梭步长
- en: 'As you call `dataloader_next_batch`, the loader moves forward through the dataset
    by `B × T` tokens each time. When it reaches the end of the dataset file, it either:'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 当你调用 `dataloader_next_batch` 时，加载器会通过 `B × T` 个标记每次向前移动一次。当它到达数据集文件的末尾时，它要么：
- en: Resets back to the beginning (`dataloader_reset`), or
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重置到开始位置（`dataloader_reset`），或者
- en: Switches from training to validation, depending on the training loop’s needs.
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据训练循环的需求从训练切换到验证。
- en: 'This stride-based reading is efficient: no random access, just sequential reads
    from a file.'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 这种基于步长的读取方式效率很高：没有随机访问，只是从文件中进行顺序读取。
- en: Epochs and Shuffling
  id: totrans-597
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练轮次和洗牌
- en: 'In deep learning, an epoch means one full pass through the dataset. The DataLoader
    in *llm.c* is simple: it goes linearly from start to finish. It doesn’t shuffle
    data like PyTorch’s `DataLoader`. Why? Because language data is already very diverse,
    and the project values minimal code over extra features. If you want shuffling,
    you can preprocess the dataset differently before creating `.bin` files.'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，一个训练轮次意味着对数据集的一次完整遍历。*llm.c* 中的 DataLoader 很简单：它从开始到结束线性移动。它不像 PyTorch
    的 `DataLoader` 那样洗牌数据。为什么？因为语言数据已经非常多样化，项目更重视代码的简洁性而不是额外的功能。如果你需要洗牌，你可以在创建 `.bin`
    文件之前对数据集进行不同的预处理。
- en: Why It Matters
  id: totrans-599
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: The DataLoader is the quiet workhorse of training. It ensures that every step
    sees a fresh batch of token sequences, always with matching inputs and targets.
    By separating dataset reading from the training loop, the code stays clean and
    focused. This design also makes it easy to swap datasets-once you generate a `.bin`
    file, the loader doesn’t care where it came from.
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: DataLoader 是训练中的沉默的功臣。它确保每个步骤都能看到一个新的标记序列批次，始终与匹配的输入和目标相对应。通过将数据集读取与训练循环分离，代码保持简洁和专注。这种设计也使得数据集的交换变得容易——一旦生成了
    `.bin` 文件，加载器就不关心它来自哪里。
- en: Try It Yourself
  id: totrans-601
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 尝试自己操作
- en: 'Print the first batch: Modify the code to print the first 20 input tokens and
    their targets. See how each input token aligns with the next target token.'
  id: totrans-602
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印第一个批次：修改代码以打印前 20 个输入标记及其目标。看看每个输入标记是如何与下一个目标标记对齐的。
- en: 'Experiment with B and T: Set `B = 2` and `T = 8` and observe how the loader
    slices the dataset into tiny chunks. Then try larger values and see how memory
    usage changes.'
  id: totrans-603
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试 B 和 T 的不同值：将 `B = 2` 和 `T = 8` 设置为观察加载器如何将数据集切割成小块。然后尝试更大的值，看看内存使用如何变化。
- en: 'Check epoch length: Write a small loop to count how many batches you get before
    `dataloader_reset` is called. Does this match the total tokens divided by `B ×
    T`?'
  id: totrans-604
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查训练轮次长度：编写一个小循环来计算在调用 `dataloader_reset` 之前你能得到多少个批次。这是否与 `B × T` 除以总标记数相等？
- en: 'Validation check: Observe how often the training loop switches to `val.bin`.
    How does validation loss compare to training loss over time?'
  id: totrans-605
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证检查：观察训练循环切换到 `val.bin` 的频率。随着时间的推移，验证损失与训练损失是如何比较的？
- en: 'Custom stride: Modify the code so the DataLoader skips some tokens between
    batches. What effect does this have on training?'
  id: totrans-606
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自定义步长：修改代码，使 DataLoader 在批次之间跳过一些标记。这对训练有什么影响？
- en: The Takeaway
  id: totrans-607
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸收要点
- en: The DataLoader in *llm.c* is intentionally simple. It streams token IDs in fixed-sized
    batches, moves forward stride by stride, and resets when done. This straightforward
    design avoids complexity and keeps the focus on the model itself, while still
    teaching you the essential mechanics of batching and sequence handling in language
    model training.
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: '*llm.c* 中的 DataLoader 故意设计得很简单。它以固定大小的批次流式传输标记 ID，逐个步长前进，并在完成后重置。这种直接的设计避免了复杂性，同时将重点放在模型本身上，同时仍然教授你在语言模型训练中批处理和序列处理的基本机制。'
- en: 15\. EvalLoader and Validation Workflow
  id: totrans-609
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15. 评估加载器和验证流程
- en: Training a model isn’t just about watching the training loss go down. To know
    whether the model is actually learning patterns that generalize-and not just memorizing
    the training data-you need to run validation. In *llm.c*, validation is handled
    by a component called the EvalLoader, which works just like the DataLoader but
    reads from the validation dataset (`val.bin`) instead of the training dataset
    (`train.bin`).
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型不仅仅是观察训练损失下降。要知道模型是否实际上在学习可以推广的模式——而不仅仅是记住训练数据——你需要运行验证。在*llm.c*中，验证由一个名为EvalLoader的组件处理，它的工作方式与DataLoader相同，但读取的是验证数据集（`val.bin`）而不是训练数据集（`train.bin`）。
- en: Why We Need Validation
  id: totrans-611
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么我们需要验证
- en: Imagine teaching a student only by drilling them with the same math problems
    over and over. They might get really good at those problems, but fail completely
    when given new ones. Validation is like giving the student a pop quiz with unseen
    questions. If they do well, you know they’ve actually learned the concepts.
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，只通过反复用相同的数学问题来教学生。他们可能会在这些问题上变得非常擅长，但面对新的问题时可能会完全失败。验证就像给学生一个突然的测验，其中包含未见过的题目。如果他们做得好，你就知道他们实际上已经学会了这些概念。
- en: 'For language models, validation helps detect overfitting: when the training
    loss keeps improving but the validation loss stays flat or even gets worse.'
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 对于语言模型，验证有助于检测过拟合：当训练损失持续改进，但验证损失保持平稳或甚至变差时。
- en: How EvalLoader Works
  id: totrans-614
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: EvalLoader 的工作原理
- en: 'EvalLoader lives in the same code file as the DataLoader (`llmc/dataloader.h`),
    but it points to a different dataset file. Its workflow is nearly identical:'
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: EvalLoader 与 DataLoader（`llmc/dataloader.h`）位于同一代码文件中，但它指向不同的数据集文件。其工作流程几乎相同：
- en: Open `val.bin` and prepare for reading.
  id: totrans-616
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开`val.bin`并准备读取。
- en: Serve up batches of size `B × T` (batch size × sequence length).
  id: totrans-617
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供大小为`B × T`的批次（批量大小×序列长度）。
- en: Provide inputs and targets the same way as the training DataLoader.
  id: totrans-618
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以与训练DataLoader相同的方式提供输入和目标。
- en: Reset after one full pass through the file.
  id: totrans-619
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成一次完整遍历文件后重置。
- en: The training loop typically calls the EvalLoader at intervals-for example, every
    few hundred steps-so you get a snapshot of validation loss during training.
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 训练循环通常每隔一段时间调用EvalLoader，例如，每几百步一次，这样你就可以在训练期间获得验证损失的快照。
- en: What Happens During Validation
  id: totrans-621
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 验证过程中发生的事情
- en: 'When validation is triggered:'
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 当触发验证时：
- en: The current model parameters are frozen (no gradient updates).
  id: totrans-623
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当前模型参数被冻结（没有梯度更新）。
- en: A few batches are read from `val.bin`.
  id: totrans-624
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`val.bin`中读取几个批次。
- en: The model runs forward passes only, computing the loss on each batch.
  id: totrans-625
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型只运行前向传递，计算每个批次的损失。
- en: The losses are averaged and reported as validation loss.
  id: totrans-626
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 损失被平均并报告为验证损失。
- en: This doesn’t take long because it usually samples just a subset of the validation
    dataset, not the entire file.
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: 这不会花费太多时间，因为它通常只采样验证数据集的一个子集，而不是整个文件。
- en: Training Loop with Validation
  id: totrans-628
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 带验证的训练循环
- en: 'In pseudocode, the loop looks like this:'
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 在伪代码中，循环看起来像这样：
- en: '[PRE37]'
  id: totrans-630
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'This is simplified, but it shows the idea: the validation loop is nested inside
    the training loop, running occasionally instead of every step.'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个简化，但它展示了这个想法：验证循环嵌套在训练循环中，偶尔运行而不是每步都运行。
- en: Why It Matters
  id: totrans-632
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么它很重要
- en: 'Validation is the reality check of training. Without it, you could train forever
    and celebrate low training losses, only to discover that your model produces nonsense
    on new text. By tracking validation loss, you can:'
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: 验证是训练的现实检查。没有它，你可能会永远训练下去，并庆祝低训练损失，但最终发现你的模型在新文本上产生的是无意义的输出。通过跟踪验证损失，你可以：
- en: Detect overfitting early.
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 早期检测过拟合。
- en: Adjust hyperparameters (like learning rate or batch size).
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整超参数（如学习率或批量大小）。
- en: Know when training has plateaued and it’s time to stop.
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解何时训练停滞，是时候停止了。
- en: In professional setups, validation curves are often plotted live, but in *llm.c*,
    the minimalist approach is to just print numbers to the console.
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 在专业设置中，验证曲线通常实时绘制，但在*llm.c*中，极简主义的方法是只将数字打印到控制台。
- en: Try It Yourself
  id: totrans-638
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 试试你自己
- en: 'Watch val loss: Run training and note how validation loss compares to training
    loss. Do they both decrease together?'
  id: totrans-639
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察验证损失：运行训练并注意验证损失与训练损失的对比。它们是否同时下降？
- en: 'Overfitting demo: Train on a very tiny dataset (like 10 KB of text). Notice
    how training loss plummets but validation loss stalls or rises.'
  id: totrans-640
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 过拟合演示：在一个非常小的数据集（如10 KB的文本）上训练。注意训练损失急剧下降，但验证损失停滞或上升。
- en: 'Change eval interval: Reduce `eval_interval` so validation runs every step.
    How much slower does training feel?'
  id: totrans-641
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 改变评估间隔：减少`eval_interval`，使验证每步运行。训练感觉慢了多少？
- en: 'Change eval batches: Set `eval_batches` to 1 vs 100\. What difference does
    this make in the stability of the reported validation loss?'
  id: totrans-642
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更改评估批次：将 `eval_batches` 设置为 1 与 100。这会对报告的验证损失稳定性产生什么影响？
- en: 'Validation as stopping rule: Stop training when validation loss stops improving
    for many intervals. How does this affect final performance?'
  id: totrans-643
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证作为停止规则：当验证损失在多个间隔内停止改善时停止训练。这会如何影响最终性能？
- en: The Takeaway
  id: totrans-644
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸取的经验
- en: The EvalLoader is a twin of the DataLoader, but for validation. It feeds the
    model data it has never seen during training, and the resulting validation loss
    tells you whether your model is learning useful patterns or just memorizing. It’s
    the simplest safeguard against wasted compute, and it’s an essential part of every
    training loop-even in the stripped-down world of *llm.c*.
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: EvalLoader 是 DataLoader 的孪生兄弟，但用于验证。它为模型提供在训练期间从未见过的数据，而得到的验证损失告诉你模型是否在学习有用的模式，而不是仅仅记忆。这是防止浪费计算的最简单保障，也是每个训练循环的必要部分——即使在简化版的
    *llm.c* 世界中也是如此。
- en: 16\. Sequence Length and Memory Budgeting
  id: totrans-646
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16. 序列长度与内存预算
- en: When training GPT-2 in *llm.c*, one of the most important decisions you make
    is choosing the sequence length (often called T). This value determines how many
    tokens the model processes in a single forward pass. It might sound like just
    another parameter, but sequence length has a huge impact on what the model can
    learn, how much memory it uses, and how fast training runs.
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *llm.c* 中训练 GPT-2 时，你做出的最重要的决定之一是选择序列长度（通常称为 T）。这个值决定了模型在单次前向传递中处理多少个标记。这听起来可能只是另一个参数，但序列长度对模型能学习的内容、使用的内存以及训练速度有巨大影响。
- en: What Sequence Length Means
  id: totrans-648
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 序列长度意味着什么
- en: Sequence length is simply the number of tokens per training example. If `T =
    1024`, the model reads 1,024 tokens in a row (like words or subwords) and tries
    to predict the next token at each position.
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: 序列长度仅仅是每个训练示例中的标记数。如果 `T = 1024`，模型将按行读取 1,024 个标记（如单词或子词），并在每个位置尝试预测下一个标记。
- en: 'Think of it like this: if you give the model a paragraph of text, sequence
    length is how much of that paragraph it sees at once. Shorter lengths give the
    model less context, while longer lengths allow it to capture bigger patterns,
    like whole paragraphs or even multiple pages.'
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下：如果你给模型一段文本，序列长度就是它一次能看到的这段文本的多少。较短的长度给模型提供的上下文较少，而较长的长度则允许它捕捉到更大的模式，比如整个段落甚至多页内容。
- en: Where It Appears in the Code
  id: totrans-651
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 它在代码中的位置
- en: 'In the logs, you’ll often see lines like:'
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: 在日志中，你经常会看到如下行：
- en: '[PRE38]'
  id: totrans-653
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: This number is defined in the GPT-2 configuration and passed into the DataLoader.
    The DataLoader slices chunks of exactly `T` tokens from `train.bin` and `val.bin`.
    The model itself has fixed positional embeddings of size `T`, so it cannot process
    sequences longer than this maximum.
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数字在 GPT-2 配置中定义，并传递给 DataLoader。DataLoader 从 `train.bin` 和 `val.bin` 中切出恰好
    `T` 个标记的块。模型本身具有固定大小的 `T` 位置嵌入，因此它无法处理超过这个最大值的序列。
- en: Memory Costs of Longer Sequences
  id: totrans-655
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 较长序列的内存成本
- en: 'Transformers are powerful but expensive. The attention mechanism compares every
    token to every other token in the sequence. This means memory and compute scale
    with the square of sequence length:'
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 很强大，但成本高昂。注意力机制会将序列中的每个标记与其他每个标记进行比较。这意味着内存和计算与序列长度的平方成正比：
- en: '| Sequence Length (T) | Relative Attention Cost |'
  id: totrans-657
  prefs: []
  type: TYPE_TB
  zh: '| 序列长度 (T) | 相对注意力成本 |'
- en: '| --- | --- |'
  id: totrans-658
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 256 | 1× |'
  id: totrans-659
  prefs: []
  type: TYPE_TB
  zh: '| 256 | 1× |'
- en: '| 512 | 4× |'
  id: totrans-660
  prefs: []
  type: TYPE_TB
  zh: '| 512 | 4× |'
- en: '| 1024 | 16× |'
  id: totrans-661
  prefs: []
  type: TYPE_TB
  zh: '| 1024 | 16× |'
- en: '| 2048 | 64× |'
  id: totrans-662
  prefs: []
  type: TYPE_TB
  zh: '| 2048 | 64× |'
- en: So doubling `T` doesn’t just double the cost-it multiplies it by four. That’s
    why training at long context lengths requires a lot of GPU memory.
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，将 `T` 加倍不仅会加倍成本，还会将其乘以四。这就是为什么在长上下文长度下训练需要大量的 GPU 内存。
- en: Trade-offs
  id: totrans-664
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 权衡
- en: 'Shorter sequences: Faster, less memory, but limited context. Good for quick
    experiments or tiny datasets like Tiny Shakespeare.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 较短的序列：更快，内存更少，但上下文有限。适合快速实验或像微型莎士比亚这样的小型数据集。
- en: 'Longer sequences: More memory, slower, but the model can understand larger
    spans of text. Required for large-scale GPT-2 training.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 较长的序列：需要更多内存，速度较慢，但模型可以理解更大的文本范围。对于大规模 GPT-2 训练是必需的。
- en: 'You can think of sequence length as a dial: turning it up increases the model’s
    ability to “remember,” but it also makes training much heavier.'
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将序列长度视为一个旋钮：将其调高会增加模型“记忆”的能力，但也会使训练变得更加沉重。
- en: Practical Choices in *llm.c*
  id: totrans-668
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在 *llm.c* 中的实际选择
- en: 'Tiny Shakespeare example: often trained with `T = 64` or `128` for speed.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微型莎士比亚示例：通常为了速度，训练时使用 `T = 64` 或 `128`。
- en: 'GPT-2 small (124M): typically uses `T = 1024`, the same as the original paper.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-2 小型（124M）：通常使用 `T = 1024`，与原始论文相同。
- en: If your GPU has limited memory, you might need to shrink `T` and/or batch size
    `B`.
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你的 GPU 内存有限，你可能需要缩小 `T` 和/或批大小 `B`。
- en: Why It Matters
  id: totrans-672
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: Choosing sequence length is about balancing learning power against hardware
    limits. A too-small sequence length can prevent the model from capturing long-term
    dependencies. A too-large one can make training impossible on your hardware. Every
    run of *llm.c* is a negotiation between what you’d like the model to see and what
    your system can handle.
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: 选择序列长度是关于平衡学习能力和硬件限制。序列长度太小可能会阻止模型捕捉长期依赖关系。太大可能会使你的硬件上的训练变得不可能。*llm.c* 的每一次运行都是你希望模型看到的内容和你的系统可以处理的内容之间的协商。
- en: Try It Yourself
  id: totrans-674
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 尝试自己操作
- en: 'Short vs long: Train Tiny Shakespeare with `T = 64` and then `T = 256`. Compare
    both the speed and the coherence of generated text.'
  id: totrans-675
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 短与长：使用 `T = 64` 训练 Tiny Shakespeare，然后使用 `T = 256`。比较生成的文本的速度和连贯性。
- en: 'Memory test: Increase `T` step by step until you hit an out-of-memory (OOM)
    error. Note the maximum your GPU can handle.'
  id: totrans-676
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 内存测试：逐步增加 `T` 步骤，直到遇到内存不足（OOM）错误。注意 GPU 可以处理的最大值。
- en: 'Batch trade-off: Try reducing batch size `B` while increasing `T`. Can you
    keep GPU memory stable while giving the model more context?'
  id: totrans-677
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 批处理权衡：尝试在增加 `T` 的同时减少批大小 `B`。你能在保持 GPU 内存稳定的同时给模型提供更多上下文吗？
- en: 'Validation impact: Run with different `T` values and watch how validation loss
    behaves. Does longer context always help?'
  id: totrans-678
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证影响：使用不同的 `T` 值运行，观察验证损失如何变化。更长的上下文总是有帮助吗？
- en: 'Inspect embeddings: Print out the shape of the positional embeddings. Notice
    how they are always tied to `T`.'
  id: totrans-679
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查嵌入：打印出位置嵌入的形状。注意它们总是与 `T` 相关联。
- en: The Takeaway
  id: totrans-680
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 要点总结
- en: Sequence length (`T`) controls how much context the model sees. It directly
    determines the size of the positional embeddings, the structure of batches, and
    the memory required for attention. In *llm.c*, adjusting `T` is one of the fastest
    ways to explore the trade-offs between speed, memory, and model capability.
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: 序列长度 (`T`) 控制模型可以看到多少上下文。它直接决定了位置嵌入的大小、批次的结构和注意力所需的内存。在 *llm.c* 中，调整 `T` 是探索速度、内存和模型能力之间权衡的最快方式之一。
- en: 17\. Reproducibility and Seeding Across Runs
  id: totrans-682
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 17. 运行间的可重复性和种子设置
- en: When training machine learning models, it’s common to notice that two runs-using
    the same code and the same dataset-don’t produce exactly the same results. This
    happens because many parts of training involve randomness. In *llm.c*, reproducibility
    is controlled by random seeds. A seed is a starting point for a random number
    generator. If you always start from the same seed, the sequence of “random” numbers
    will be identical, and so will the training run.
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练机器学习模型时，通常会注意到，使用相同代码和相同数据集的两个运行不会产生完全相同的结果。这是因为训练的许多部分都涉及随机性。在 *llm.c* 中，可重复性由随机种子控制。种子是随机数生成器的起点。如果你总是从相同的种子开始，那么“随机”数字的序列将是相同的，训练运行也将是相同的。
- en: Where Randomness Appears
  id: totrans-684
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 随机性出现的地方
- en: 'Even in a small project like *llm.c*, randomness shows up in several places:'
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是在像 *llm.c* 这样的小项目中，随机性也会出现在几个地方：
- en: '| Component | Random Role |'
  id: totrans-686
  prefs: []
  type: TYPE_TB
  zh: '| 组件 | 随机角色 |'
- en: '| --- | --- |'
  id: totrans-687
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Weight initialization | The model’s parameters (like attention matrices)
    are set randomly at the start. |'
  id: totrans-688
  prefs: []
  type: TYPE_TB
  zh: '| 权重初始化 | 模型的参数（如注意力矩阵）在开始时随机设置。 |'
- en: '| Optimizer states | Some optimizers use random noise (though AdamW is mostly
    deterministic). |'
  id: totrans-689
  prefs: []
  type: TYPE_TB
  zh: '| 优化器状态 | 一些优化器使用随机噪声（尽管 AdamW 主要为确定性）。 |'
- en: '| Sampling outputs | When generating text, randomness decides which token to
    pick if probabilities are close. |'
  id: totrans-690
  prefs: []
  type: TYPE_TB
  zh: '| 样本输出 | 在生成文本时，如果概率接近，随机性决定选择哪个标记。 |'
- en: '| Parallelism | On GPU, threads may execute in slightly different orders, sometimes
    introducing small nondeterminism. |'
  id: totrans-691
  prefs: []
  type: TYPE_TB
  zh: '| 并行性 | 在 GPU 上，线程可能以略微不同的顺序执行，有时会引入微小的非确定性。 |'
- en: Without a fixed seed, every training run can drift apart, even if all settings
    look the same.
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: 没有固定的种子，每次训练运行都可能有所不同，即使所有设置看起来都相同。
- en: How *llm.c* Handles Seeds
  id: totrans-693
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*llm.c* 如何处理种子'
- en: 'The repository provides a small random utilities module: `llmc/rand.h`. Inside
    you’ll find functions such as:'
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: 仓库提供了一个小的随机实用工具模块：`llmc/rand.h`。在里面你可以找到如下函数：
- en: '[PRE39]'
  id: totrans-695
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '`manual_seed` sets the seed for the internal random number generator, ensuring
    reproducibility.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`manual_seed` 设置内部随机数生成器的种子，确保可重复性。'
- en: '`normal_` is used for initializing weights with Gaussian noise, similar to
    PyTorch’s `torch.nn.init.normal_`.'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`normal_` 用于使用高斯噪声初始化权重，类似于 PyTorch 的 `torch.nn.init.normal_`。'
- en: When you call `manual_seed(1337);`, the model weights will be initialized the
    same way every time.
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: 当您调用 `manual_seed(1337);` 时，模型权重将每次以相同的方式初始化。
- en: Why Seeds Don’t Guarantee Perfect Reproducibility
  id: totrans-699
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么种子不能保证完美的可重复性
- en: 'Even with a fixed seed, you may still see small differences:'
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: 即使有固定的种子，您仍然可能看到小的差异：
- en: GPU kernels sometimes use parallel algorithms that are not bitwise deterministic.
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU 内核有时会使用不是位确定性的并行算法。
- en: Floating-point math can produce slightly different rounding on different hardware.
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 浮点数数学在不同的硬件上可能会产生略微不同的舍入结果。
- en: Multi-GPU runs (via NCCL/MPI) may introduce nondeterministic reduce operations.
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多 GPU 运行（通过 NCCL/MPI）可能会引入非确定性 reduce 操作。
- en: These differences are usually tiny-validation loss might vary by 0.001-but they
    exist. For most educational purposes, *llm.c* seeds are enough to make experiments
    repeatable.
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: 这些差异通常很小-验证损失可能变化 0.001，但它们确实存在。对于大多数教育目的，*llm.c* 的种子足以使实验可重复。
- en: Typical Defaults
  id: totrans-705
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 典型默认值
- en: 'In many examples, you’ll see:'
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多示例中，您会看到：
- en: '[PRE40]'
  id: totrans-707
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: This “magic number” 1337 is just a convention. You can change it to any integer.
    Using the same seed across runs guarantees the same starting weights, which helps
    when comparing hyperparameters.
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: 这个“魔法数字” 1337 只是一个约定。您可以将其更改为任何整数。在运行中使用相同的种子保证相同的起始权重，这有助于比较超参数。
- en: Why It Matters
  id: totrans-709
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: 'Reproducibility is crucial in machine learning because it lets you:'
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，可重复性至关重要，因为它让您：
- en: 'Debug effectively: If a bug appears, you want it to appear consistently.'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有效调试：如果出现错误，您希望它始终如一地出现。
- en: 'Compare settings: You can test learning rates or batch sizes fairly by keeping
    everything else the same.'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较设置：通过保持其他所有设置不变，您可以公平地测试学习率或批量大小。
- en: 'Share results: Other people can run your exact setup and see the same outcomes.'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分享结果：其他人可以运行您确切的设置并看到相同的结果。
- en: Without seeds, it becomes hard to tell whether a difference came from your hyperparameter
    change or just random luck.
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 没有种子，就很难判断差异是来自您的超参数更改还是只是随机运气。
- en: Try It Yourself
  id: totrans-715
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 试试看
- en: 'Run twice with same seed: Train GPT-2 with `manual_seed(1337)` set. Do you
    get identical training loss curves?'
  id: totrans-716
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用相同的种子运行两次：使用 `manual_seed(1337)` 设置训练 GPT-2。您是否得到相同的训练损失曲线？
- en: 'Change the seed: Try `manual_seed(42)` and compare the loss curve. How similar
    are they? Do they converge to about the same final validation loss?'
  id: totrans-717
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 改变种子：尝试 `manual_seed(42)` 并比较损失曲线。它们有多相似？它们是否收敛到大约相同的最终验证损失？
- en: 'Remove seeding: Comment out the seed line and run again. Notice how runs diverge.'
  id: totrans-718
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移除种子：注释掉种子行并再次运行。注意运行如何发散。
- en: 'Sampling experiment: With a fixed seed, generate text multiple times. Then
    change the seed and generate again. See how outputs change.'
  id: totrans-719
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 样本实验：使用固定的种子多次生成文本。然后更改种子再次生成。看看输出如何变化。
- en: 'Multi-GPU test: If you have more than one GPU, run the same seed across devices.
    Do results stay exactly the same or only approximately?'
  id: totrans-720
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 多 GPU 测试：如果您有多个 GPU，请在设备上运行相同的种子。结果是否完全相同或只是近似相同？
- en: The Takeaway
  id: totrans-721
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸收要点
- en: 'Reproducibility in *llm.c* comes from setting seeds for random number generators.
    While floating-point quirks mean you can’t always get perfect bit-for-bit matches,
    seeds let you control the biggest source of randomness: weight initialization
    and sampling. With seeding, you can debug, compare, and share results confidently.'
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *llm.c* 中，可重复性来自为随机数生成器设置种子。虽然浮点数的怪癖意味着您不能总是得到完美的位对位匹配，但种子让您可以控制最大的随机源：权重初始化和采样。有了种子，您可以自信地进行调试、比较和分享结果。
- en: 18\. Error Surfaces from Bad Data (Bounds, Asserts)
  id: totrans-723
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 18. 来自不良数据的错误表面（界限，断言）
- en: When training a model in *llm.c*, everything depends on the quality and correctness
    of the data you feed in. If the dataset or batches contain mistakes, the training
    process can go off track quickly-sometimes by crashing outright, other times by
    producing strange loss values like `NaN`. To guard against this, the code uses
    bounds checks and asserts that catch problems early.
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *llm.c* 中训练模型时，一切取决于您输入的数据的质量和正确性。如果数据集或批次包含错误，训练过程可能会迅速偏离轨道——有时会直接崩溃，有时会产生像
    `NaN` 这样的奇怪损失值。为了防止这种情况，代码使用界限检查和断言来早期捕捉问题。
- en: What Can Go Wrong with Data
  id: totrans-725
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据可能出什么问题
- en: 'There are several common data issues:'
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个常见的数据问题：
- en: '| Problem | What Happens |'
  id: totrans-727
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | 发生的情况 |'
- en: '| --- | --- |'
  id: totrans-728
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Token ID out of range | The model expects IDs between 0 and `vocab_size-1`.
    A wrong ID can cause array indexing errors. |'
  id: totrans-729
  prefs: []
  type: TYPE_TB
  zh: '| 标记ID越界 | 模型期望ID在0到`vocab_size-1`之间。错误的ID可能导致数组索引错误。|'
- en: '| Empty or short dataset | The DataLoader may run out of tokens before filling
    a batch. |'
  id: totrans-730
  prefs: []
  type: TYPE_TB
  zh: '| 空或短数据集 | DataLoader可能在填充一个批次之前就耗尽了标记。|'
- en: '| Mismatched tokenizer | If you build a dataset with a different tokenizer,
    IDs may not correspond to the GPT-2 tokenizer in `gpt2_tokenizer.bin`. This produces
    nonsense outputs. |'
  id: totrans-731
  prefs: []
  type: TYPE_TB
  zh: '| 不匹配的分词器 | 如果你使用不同的分词器构建数据集，ID可能不会对应于`gpt2_tokenizer.bin`中的GPT-2分词器。这会产生无意义的输出。|'
- en: '| Corrupt `.bin` files | If files are incomplete or written incorrectly, the
    DataLoader might read garbage values. |'
  id: totrans-732
  prefs: []
  type: TYPE_TB
  zh: '| 损坏的`.bin`文件 | 如果文件不完整或写入错误，DataLoader可能会读取垃圾值。|'
- en: These errors show up as segfaults, invalid memory access, or exploding losses
    during training.
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: 这些错误在训练过程中表现为段错误、无效内存访问或损失爆炸。
- en: How *llm.c* Defends Against Bad Data
  id: totrans-734
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*llm.c* 如何防御不良数据'
- en: The repository makes heavy use of asserts-simple checks that stop the program
    immediately if something unexpected happens. For example, in `llmc/utils.h`, functions
    like `freadCheck` and `mallocCheck` ensure that file reads and memory allocations
    succeed. If not, they print an error message and abort instead of silently failing.
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: 仓库大量使用断言——如果发生意外情况，它们会立即停止程序。例如，在`llmc/utils.h`中，`freadCheck`和`mallocCheck`等函数确保文件读取和内存分配成功。如果不成功，它们会打印错误消息并终止，而不是静默失败。
- en: Inside the DataLoader, token IDs are often validated to make sure they fall
    inside the expected vocabulary range. If you try to access an invalid index in
    the embedding table, the program will crash quickly, which is better than continuing
    with corrupted values.
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: 在DataLoader内部，通常会对标记ID进行验证，以确保它们落在预期的词汇范围内。如果你尝试访问嵌入表中的无效索引，程序会迅速崩溃，这比继续使用损坏的值要好。
- en: 'Example: Vocab Range Check'
  id: totrans-737
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例：词汇范围检查
- en: During training, every input token is used to look up a row in the embedding
    matrix. If a token ID is too large, you’d access memory outside the matrix. This
    is why checking `0 <= id < vocab_size` is essential. In C, asserts provide this
    safety net.
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，每个输入标记都会用于在嵌入矩阵中查找一行。如果标记ID太大，你会访问矩阵外的内存。这就是为什么检查`0 <= id < vocab_size`是至关重要的。在C语言中，断言提供了这种安全网。
- en: '[PRE41]'
  id: totrans-739
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: This kind of check may look simple, but it saves hours of debugging mysterious
    crashes.
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: 这种检查可能看起来很简单，但它可以节省数小时的调试神秘崩溃的时间。
- en: Error Surfaces in Loss
  id: totrans-741
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 损失函数中的误差面
- en: 'Even if your program doesn’t crash, bad data can create “error surfaces” in
    the loss function:'
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你的程序没有崩溃，不良数据也可能在损失函数中创建“误差面”：
- en: 'NaNs: Appear when invalid values propagate through softmax, layernorm, or division
    operations.'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NaN值：当无效值通过softmax、layernorm或除法操作传播时出现。
- en: 'Flat loss: If the dataset is empty or repetitive, the model never improves.'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平坦损失：如果数据集为空或重复，模型永远不会改进。
- en: 'Mismatch behavior: Training loss decreases but validation loss stays high if
    training and validation sets use inconsistent tokenization.'
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不匹配行为：如果训练集和验证集使用不一致的分词，训练损失会下降，但验证损失会保持较高。
- en: These are signs that something is wrong with the dataset or preprocessing.
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是数据集或预处理有问题的一些迹象。
- en: Why It Matters
  id: totrans-747
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: C is a low-level language with very little safety by default. One out-of-range
    index can corrupt memory and cause unpredictable bugs. By aggressively checking
    assumptions (file sizes, vocab bounds, token IDs), *llm.c* turns hard-to-find
    errors into immediate, clear failures. For learners, this makes it much easier
    to understand what went wrong.
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: C是一种默认安全性很低的底层语言。一个越界的索引可以损坏内存并导致不可预测的错误。通过积极检查假设（文件大小、词汇边界、标记ID），*llm.c*将难以找到的错误转化为立即、清晰的失败。对于学习者来说，这使得理解出错原因变得容易得多。
- en: Try It Yourself
  id: totrans-749
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 尝试自己操作
- en: 'Corrupt a dataset: Open `train.bin` and delete a few bytes. Run training and
    see what error appears. Notice how quickly asserts catch it.'
  id: totrans-750
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 损坏数据集：打开`train.bin`并删除一些字节。运行训练并查看出现什么错误。注意断言如何快速捕捉到它。
- en: 'Force a bad ID: Modify the DataLoader to add `+100000` to a token. Does the
    model crash with an assertion?'
  id: totrans-751
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 强制一个不良ID：修改DataLoader以向标记添加`+100000`。模型会因断言而崩溃吗？
- en: 'Skip asserts: Temporarily disable checks and rerun. Compare how much harder
    it is to figure out what went wrong.'
  id: totrans-752
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 跳过断言：暂时禁用检查并重新运行。比较找出问题所在有多困难。
- en: 'Validation mismatch: Tokenize a file with a different tokenizer and save it
    as `val.bin`. Watch how the validation loss behaves compared to training loss.'
  id: totrans-753
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证不匹配：使用不同的分词器对文件进行分词并保存为`val.bin`。观察验证损失与训练损失的行为差异。
- en: 'Print debug info: Add logging to display the first 20 tokens of each batch.
    Can you spot bad data before it crashes?'
  id: totrans-754
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印调试信息：添加日志以显示每个批次的第一个20个标记。你能在它崩溃之前发现坏数据吗？
- en: The Takeaway
  id: totrans-755
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸取教训
- en: Bad data can silently sabotage training, but *llm.c* uses asserts and bounds
    checks to make errors loud and clear. This design choice helps learners focus
    on the real logic of transformers instead of chasing hidden bugs caused by corrupted
    or mismatched datasets. In machine learning, good data hygiene and strict validation
    are as important as the model itself.
  id: totrans-756
  prefs: []
  type: TYPE_NORMAL
  zh: 坏数据可能会无声地破坏训练，但*llm.c*使用断言和边界检查来使错误响亮且清晰。这种设计选择有助于学习者专注于transformers的真实逻辑，而不是追逐由损坏或不匹配的数据集引起的隐藏错误。在机器学习中，良好的数据卫生和严格的验证与模型本身一样重要。
- en: 19\. Tokenization Edge Cases (UNKs, EOS, BOS)
  id: totrans-757
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 19. Tokenization Edge Cases (UNKs, EOS, BOS)
- en: 'Tokenization looks simple at first: take text, split it into tokens, and assign
    each token an ID. But in practice, there are always tricky situations. *llm.c*
    inherits the quirks of the GPT-2 tokenizer, which is byte-level BPE (Byte Pair
    Encoding). This design mostly avoids “unknown” tokens, but it still has details
    you need to understand when preparing datasets or interpreting outputs.'
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: Tokenization看起来一开始很简单：取文本，将其分割成标记，并为每个标记分配一个ID。但在实践中，总会有一些棘手的情况。"llm.c"继承了GPT-2标记化器的怪癖，它是基于字节的BPE（字节对编码）。这种设计主要避免了“未知”标记，但在准备数据集或解释输出时，仍有一些细节需要你理解。
- en: No True “UNK” in GPT-2
  id: totrans-759
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: GPT-2中没有真正的“UNK”
- en: 'Some tokenizers, like those used in earlier NLP systems, include a special
    `UNK` (unknown) token for words that aren’t in the vocabulary. GPT-2 avoids this
    problem by working at the byte level:'
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: 一些标记化器，如早期NLP系统中使用的标记化器，包括一个特殊的`UNK`（未知）标记，用于不在词汇表中的单词。GPT-2通过在字节级别工作来避免这个问题：
- en: Every possible byte (0–255) is in the base vocabulary.
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个可能的字节（0-255）都在基本词汇表中。
- en: If the tokenizer doesn’t know how to split a character or word, it just falls
    back to raw bytes.
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果标记化器不知道如何分割一个字符或单词，它就回退到原始字节。
- en: That means you will never see an `UNK` token in *llm.c*. Any input text is always
    representable. This is one of the main reasons GPT-2’s tokenizer is so robust.
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着你永远不会在*llm.c*中看到`UNK`标记。任何输入文本总是可以表示的。这是GPT-2的标记化器如此健壮的主要原因之一。
- en: 'Special Tokens: EOS and BOS'
  id: totrans-764
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 特殊标记：EOS和BOS
- en: 'Even though GPT-2 doesn’t use `UNK`, it does use other special tokens:'
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管GPT-2不使用`UNK`，但它确实使用其他特殊标记：
- en: '| Token | ID | Purpose |'
  id: totrans-766
  prefs: []
  type: TYPE_TB
  zh: '| 标记 | ID | 用途 |'
- en: '| --- | --- | --- |'
  id: totrans-767
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| EOS (End of Sequence) | 50256 | Marks the end of a text segment. Used during
    training and sampling. |'
  id: totrans-768
  prefs: []
  type: TYPE_TB
  zh: '| EOS（序列结束） | 50256 | 标记文本段的结束。在训练和采样期间使用。|'
- en: '| BOS (Beginning of Sequence) | Not explicit in GPT-2 | GPT-2 doesn’t use a
    fixed BOS token. Instead, the model assumes generation starts at position 0. |'
  id: totrans-769
  prefs: []
  type: TYPE_TB
  zh: '| BOS（序列开始） | GPT-2中不明确 | GPT-2不使用固定的BOS标记。相反，模型假设生成从位置0开始。|'
- en: In *llm.c*, you’ll often see `EOS` at the end of training sequences or when
    sampling text. If you generate text and see strange endings, it’s usually because
    the model predicted `EOS`.
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
  zh: 在*llm.c*中，你经常会看到训练序列末尾或采样文本时的`EOS`。如果你生成文本并看到奇怪的结尾，通常是因为模型预测了`EOS`。
- en: Whitespace Quirks
  id: totrans-771
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 空白字符怪癖
- en: The tokenizer also handles whitespace in a slightly unusual way. For example,
    the word “hello” and the word ” hello” (with a leading space) map to different
    tokens. This is why generated text sometimes starts with a space-it’s part of
    the token definition.
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
  zh: 标记化器还以略微不寻常的方式处理空白字符。例如，单词“hello”和带前导空格的单词” hello”映射到不同的标记。这就是为什么生成的文本有时以空格开头的原因——它是标记定义的一部分。
- en: 'Example:'
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '`"hello"` → token ID 31373'
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"hello"` → 标记ID 31373'
- en: '`" hello"` → token ID 15496'
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"hello"` → 标记ID 15496'
- en: This is normal behavior for GPT-2\. It helps the model capture spacing and punctuation
    consistently.
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: 这对GPT-2来说是正常的行为。它有助于模型一致地捕捉间距和标点符号。
- en: Unicode and Rare Characters
  id: totrans-777
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Unicode和稀有字符
- en: Because it’s byte-level, GPT-2 can encode emojis, accented characters, or even
    binary junk data. But the BPE merges are optimized for English, so rare characters
    often get split into multiple byte tokens. That means sequences with lots of rare
    symbols (like Chinese or emojis) will use more tokens than plain English text.
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: 因为它是基于字节的，GPT-2可以编码表情符号、带重音的字符，甚至二进制垃圾数据。但BPE合并优化了英语，所以稀有字符经常被分割成多个字节标记。这意味着包含大量稀有符号（如中文或表情符号）的序列将比普通英语文本使用更多的标记。
- en: Why It Matters
  id: totrans-779
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: Edge cases in tokenization affect both dataset preparation and model outputs.
    If you see weird spacing or early `EOS` tokens, it’s not a bug-it’s just how the
    tokenizer works. Understanding these quirks helps you debug outputs and prepare
    datasets without surprises.
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: 分词中的边缘情况会影响数据集准备和模型输出。如果你看到奇怪的间距或早期的 `EOS` 标记，这不是错误——这只是分词器的工作方式。理解这些怪癖有助于你调试输出并准备数据集时没有意外。
- en: Try It Yourself
  id: totrans-781
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 试试看
- en: 'EOS inspection: Open `val.bin` with a hex viewer and look for token ID `50256`.
    These mark the ends of text segments.'
  id: totrans-782
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`EOS` 检查：使用十六进制查看器打开 `val.bin` 并查找标记ID `50256`。这些标记表示文本段落的结束。'
- en: 'Whitespace check: Use the tokenizer to encode `"hello"` and `" hello"`. Compare
    the token IDs.'
  id: totrans-783
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 空白字符检查：使用分词器将“hello”和“ hello”编码。比较标记ID。
- en: 'Emoji test: Encode a string with emojis (e.g., `"🙂🙂🙂"`) and see how many tokens
    it becomes.'
  id: totrans-784
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 表情符号测试：将包含表情符号的字符串（例如，“🙂🙂🙂”）编码，看看它变成了多少个标记。
- en: 'Rare character dataset: Create a small `.txt` file with accented characters
    and tokenize it. How many bytes does each character consume?'
  id: totrans-785
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 稀有字符数据集：创建一个包含带音标的字符的小 `.txt` 文件并对其进行分词。每个字符消耗多少字节？
- en: 'Sampling experiment: Generate text until you see the EOS token appear. Notice
    how the model “knows” to stop.'
  id: totrans-786
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 样本实验：生成文本，直到你看到 `EOS` 标记出现。注意模型“知道”何时停止。
- en: The Takeaway
  id: totrans-787
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸收要点
- en: Tokenization in GPT-2 is robust, but it has quirks. There are no unknown tokens
    thanks to byte-level encoding, but whitespace and special tokens like `EOS` play
    important roles. By experimenting with these edge cases, you’ll develop an intuition
    for how raw text is mapped into the numbers that drive training and generation
    in *llm.c*.
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2 中的分词是健壮的，但它有一些怪癖。由于字节级编码，没有未知标记，但空白字符和像 `EOS` 这样的特殊标记扮演着重要的角色。通过实验这些边缘情况，你会对原始文本如何映射到驱动
    *llm.c* 训练和生成的数字有更深的理解。
- en: 20\. Data Hygiene and Logging
  id: totrans-789
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 20. 数据卫生和日志记录
- en: When training with *llm.c*, having clean data is just as important as having
    the right model code. If the dataset contains errors, duplicates, or formatting
    issues, the model may waste capacity memorizing noise instead of learning useful
    patterns. This is where data hygiene comes in-making sure your training and validation
    sets are prepared properly. Alongside this, logging ensures you can monitor what’s
    happening during training and catch problems early.
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 *llm.c* 训练时，拥有干净的数据与拥有正确的模型代码一样重要。如果数据集包含错误、重复或格式问题，模型可能会浪费能力去记忆噪声而不是学习有用的模式。这就是数据卫生的重要性——确保你的训练和验证集被正确准备。与此同时，日志记录确保你可以在训练过程中监控发生的情况并及早发现问题。
- en: What Data Hygiene Means
  id: totrans-791
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据卫生的含义
- en: 'Data hygiene is about making sure your dataset is both valid and useful. For
    language models, this includes:'
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: 数据卫生是确保你的数据集既有效又有用。对于语言模型，这包括：
- en: '| Check | Why It Matters |'
  id: totrans-793
  prefs: []
  type: TYPE_TB
  zh: '| 检查 | 为什么它很重要 |'
- en: '| --- | --- |'
  id: totrans-794
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Correct tokenization | Must match the tokenizer (`gpt2_tokenizer.bin`), otherwise
    IDs won’t line up. |'
  id: totrans-795
  prefs: []
  type: TYPE_TB
  zh: '| 正确的分词 | 必须与分词器（`gpt2_tokenizer.bin`）匹配，否则ID不会对齐。|'
- en: '| No corrupt files | Binary `.bin` files must be complete; partial writes cause
    crashes. |'
  id: totrans-796
  prefs: []
  type: TYPE_TB
  zh: '| 无损坏文件 | 二进制 `.bin` 文件必须是完整的；部分写入会导致崩溃。|'
- en: '| Balanced splits | Training and validation sets should come from the same
    distribution. |'
  id: totrans-797
  prefs: []
  type: TYPE_TB
  zh: '| 平衡分割 | 训练和验证集应来自相同的分布。|'
- en: '| Reasonable size | Too small → overfitting. Too large → slow or infeasible.
    |'
  id: totrans-798
  prefs: []
  type: TYPE_TB
  zh: '| 合理的大小 | 过小 → 过拟合。过大 → 慢或不可行。|'
- en: '| Deduplication | Repeated passages (e.g., web scrapes) make models memorize
    instead of generalize. |'
  id: totrans-799
  prefs: []
  type: TYPE_TB
  zh: '| 去重 | 重复的段落（例如，网络爬取）会使模型记住而不是泛化。|'
- en: The scripts in `dev/data/` handle basic hygiene by tokenizing consistently and
    splitting into train/val sets. But if you bring your own dataset, you are responsible
    for cleaning it first.
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
  zh: '`dev/data/` 中的脚本通过一致地分词并将数据集分割成训练/验证集来处理基本卫生。但如果你带来了自己的数据集，你负责首先对其进行清理。'
- en: Logging During Training
  id: totrans-801
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练过程中的日志记录
- en: 'Once training starts, logging becomes your window into what’s happening. *llm.c*
    uses a minimal logging system (`llmc/logger.h`) to print progress to the console.
    Typical logs include:'
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦开始训练，日志就成为了你了解发生情况的窗口。*llm.c* 使用一个最小的日志系统（`llmc/logger.h`）将进度打印到控制台。典型的日志包括：
- en: '[PRE42]'
  id: totrans-803
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'These numbers let you track:'
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数字让你可以跟踪：
- en: 'Training loss: Is the model fitting the data?'
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练损失：模型是否拟合数据？
- en: 'Validation loss: Is it generalizing, or overfitting?'
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证损失：它是泛化还是过拟合？
- en: 'Step timing: How long each batch takes, useful for profiling.'
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步骤计时：每个批次花费的时间，对于分析很有用。
- en: Even in such a small project, this logging loop gives you most of what you need
    to debug runs.
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在这样的小项目中，这个日志循环也为你提供了调试运行所需的大部分信息。
- en: Why Hygiene and Logging Go Together
  id: totrans-809
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么卫生和日志记录是相辅相成的
- en: 'Bad data often reveals itself in the logs. For example:'
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
  zh: 坏数据通常会在日志中暴露出来。例如：
- en: If validation loss is much higher than training loss, your validation set may
    be mismatched.
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果验证损失远高于训练损失，你的验证集可能不匹配。
- en: If loss suddenly becomes `NaN`, your dataset might contain corrupt tokens.
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果损失突然变为 `NaN`，你的数据集中可能包含损坏的标记。
- en: If loss plateaus at a high value, you may have too little data or poor preprocessing.
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果损失在较高值处停滞不前，你可能数据太少或预处理不当。
- en: By keeping your data clean and watching logs closely, you can detect these issues
    early instead of wasting hours of compute.
  id: totrans-814
  prefs: []
  type: TYPE_NORMAL
  zh: 通过保持数据清洁并密切观察日志，你可以及早发现这些问题，而不是浪费数小时的计算时间。
- en: Try It Yourself
  id: totrans-815
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 尝试自己操作
- en: 'Dirty dataset test: Take a `.txt` file, add random symbols or binary junk,
    and prepare a `.bin` dataset. What happens to training loss?'
  id: totrans-816
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 污染数据集测试：取一个 `.txt` 文件，添加随机符号或二进制垃圾，并准备一个 `.bin` 数据集。训练损失会发生什么变化？
- en: 'Duplicate passages: Copy the same paragraph 100 times into a training file.
    Does validation loss improve, or does the model just memorize?'
  id: totrans-817
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复段落：将相同的段落复制 100 次到训练文件中。验证损失是否提高，或者模型只是记忆了？
- en: 'Log frequency: Modify the code to log every step instead of every N steps.
    How noisy are the results?'
  id: totrans-818
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 日志频率：修改代码以记录每一步而不是每 N 步。结果有多嘈杂？
- en: 'Custom logger: Extend the logger to also print gradient norms or learning rate
    values. Does this help you understand training dynamics better?'
  id: totrans-819
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自定义日志记录器：扩展日志记录器以打印梯度范数或学习率值。这有助于你更好地理解训练动态吗？
- en: 'Compare splits: Build two datasets with different train/val splits. Which one
    gives more stable validation losses?'
  id: totrans-820
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较分割：构建两个具有不同训练/验证分割的数据集。哪一个给出了更稳定的验证损失？
- en: The Takeaway
  id: totrans-821
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸取的教训
- en: Data hygiene ensures the model learns from clean, consistent input, while logging
    ensures you can see whether learning is actually happening. Together, they form
    the foundation of reliable experiments in *llm.c*. If you clean your data carefully
    and pay attention to the logs, you’ll catch most problems before they become serious.
  id: totrans-822
  prefs: []
  type: TYPE_NORMAL
  zh: 数据卫生确保模型从干净、一致输入中学习，而日志记录确保你可以看到学习是否真正发生。在 *llm.c* 中，它们共同构成了可靠实验的基础。如果你仔细清理数据并关注日志，你将在问题变得严重之前捕捉到大多数问题。
- en: Chapter 3\. Model Definition and Weights
  id: totrans-823
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3章 模型定义和权重
- en: '21\. GPT-2 Config: Vocab, Layers, Heads, Channels'
  id: totrans-824
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 21. GPT-2 配置：词汇量、层、头、通道
- en: Every GPT-2 model, no matter how large or small, is defined by a handful of
    configuration numbers. These numbers decide how big the model is, how much memory
    it needs, and how powerful it can become. In *llm.c*, these settings are stored
    in a simple config struct and printed at the start of training. They describe
    the “blueprint” of the transformer.
  id: totrans-825
  prefs: []
  type: TYPE_NORMAL
  zh: 任何 GPT-2 模型，无论大小，都是由一些配置数字定义的。这些数字决定了模型的大小、所需的内存以及它可能变得多强大。在 *llm.c* 中，这些设置存储在一个简单的配置结构体中，并在训练开始时打印出来。它们描述了变换器的“蓝图”。
- en: The Core Parameters
  id: totrans-826
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 核心参数
- en: 'Here are the most important values you’ll see in the logs:'
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是日志中你会看到的最重要的一些值：
- en: '| Parameter | Meaning | Example (GPT-2 Small) |'
  id: totrans-828
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 含义 | 示例（GPT-2 小型） |'
- en: '| --- | --- | --- |'
  id: totrans-829
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `vocab_size` | Number of distinct tokens (from tokenizer). | 50,257 |'
  id: totrans-830
  prefs: []
  type: TYPE_TB
  zh: '| `vocab_size` | 从分词器来的不同标记的数量。 | 50,257 |'
- en: '| `padded_vocab_size` | Vocab size rounded up to nearest multiple (for GPU
    efficiency). | 50,304 |'
  id: totrans-831
  prefs: []
  type: TYPE_TB
  zh: '| `padded_vocab_size` | 四舍五入到最接近的倍数的词汇量大小（为了 GPU 效率）。 | 50,304 |'
- en: '| `max_seq_len` | Longest sequence of tokens the model can handle. | 1,024
    |'
  id: totrans-832
  prefs: []
  type: TYPE_TB
  zh: '| `max_seq_len` | 模型可以处理的最长标记序列。 | 1,024 |'
- en: '| `num_layers` | Number of transformer blocks stacked on top of each other.
    | 12 |'
  id: totrans-833
  prefs: []
  type: TYPE_TB
  zh: '| `num_layers` | 堆叠在一起的变换器块的数量。 | 12 |'
- en: '| `num_heads` | Number of attention heads per block. | 12 |'
  id: totrans-834
  prefs: []
  type: TYPE_TB
  zh: '| `num_heads` | 每个块的注意力头数。 | 12 |'
- en: '| `channels` | Width of hidden states (embedding dimension). | 768 |'
  id: totrans-835
  prefs: []
  type: TYPE_TB
  zh: '| `channels` | 隐藏状态宽度（嵌入维度）。 | 768 |'
- en: '| `num_parameters` | Total trainable weights in the model. | ~124M |'
  id: totrans-836
  prefs: []
  type: TYPE_TB
  zh: '| `num_parameters` | 模型中的总可训练权重。 | ~124M |'
- en: Together, these values define both the structure and the capacity of the model.
  id: totrans-837
  prefs: []
  type: TYPE_NORMAL
  zh: 这些值共同定义了模型的结构和容量。
- en: What They Control
  id: totrans-838
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 它们控制的内容
- en: Vocabulary size connects the model to the tokenizer. Every input token ID must
    be less than `vocab_size`. The padded version makes GPU matrix multiplications
    easier.
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词汇量大小将模型与分词器连接起来。每个输入标记 ID 必须小于 `vocab_size`。填充版本使得 GPU 矩阵乘法更容易。
- en: Max sequence length fixes the size of the positional embeddings. If you set
    this to 1024, the model can’t read beyond 1024 tokens in one pass.
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大序列长度固定了位置嵌入的大小。如果你将其设置为 1024，则模型在一次遍历中不能读取超过 1024 个标记。
- en: Layers control model depth. Each layer contains an attention block and an MLP.
    More layers = more representational power.
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层数控制模型深度。每一层包含一个注意力块和一个 MLP。更多的层数意味着更强的表示能力。
- en: Heads divide attention into parallel “subspaces.” With 12 heads, the model can
    track different types of relationships in the text at the same time.
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 头数将注意力分割成并行的“子空间”。使用 12 个头，模型可以同时跟踪文本中的不同类型的关系。
- en: Channels set the dimensionality of embeddings and hidden vectors. Larger channels
    mean more expressive representations but also more computation.
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通道数设置嵌入和隐藏向量的维度。更大的通道数意味着更丰富的表示，但也需要更多的计算。
- en: Parameters are the sum of it all. This number tells you how heavy the model
    is to train and how much memory it will consume.
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数是所有这些的总和。这个数字告诉你模型训练的重量以及它将消耗多少内存。
- en: Configs Across GPT-2 Sizes
  id: totrans-845
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: GPT-2 不同规模的配置
- en: 'The original GPT-2 models come in several sizes:'
  id: totrans-846
  prefs: []
  type: TYPE_NORMAL
  zh: 原始 GPT-2 模型有几个不同的规模：
- en: '| Model | Layers | Heads | Channels | Parameters |'
  id: totrans-847
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 层数 | 头数 | 通道数 | 参数 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-848
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| GPT-2 Small | 12 | 12 | 768 | 124M |'
  id: totrans-849
  prefs: []
  type: TYPE_TB
  zh: '| GPT-2 小型 | 12 | 12 | 768 | 124M |'
- en: '| GPT-2 Medium | 24 | 16 | 1024 | 355M |'
  id: totrans-850
  prefs: []
  type: TYPE_TB
  zh: '| GPT-2 中型 | 24 | 16 | 1024 | 355M |'
- en: '| GPT-2 Large | 36 | 20 | 1280 | 774M |'
  id: totrans-851
  prefs: []
  type: TYPE_TB
  zh: '| GPT-2 大型 | 36 | 20 | 1280 | 774M |'
- en: '| GPT-2 XL | 48 | 25 | 1600 | 1.6B |'
  id: totrans-852
  prefs: []
  type: TYPE_TB
  zh: '| GPT-2 XL | 48 | 25 | 1600 | 1.6B |'
- en: '*llm.c* can scale between these by just changing a few numbers in the config
    struct.'
  id: totrans-853
  prefs: []
  type: TYPE_NORMAL
  zh: '*llm.c* 只需在配置结构体中更改几个数字就可以在这些规模之间进行扩展。'
- en: Where Config Appears in the Code
  id: totrans-854
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 配置在代码中的位置
- en: 'In `train_gpt2.c` and `train_gpt2.cu`, you’ll see something like:'
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `train_gpt2.c` 和 `train_gpt2.cu` 中，你会看到类似以下的内容：
- en: '[PRE43]'
  id: totrans-856
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Later, the model is initialized using this struct, and the log prints all the
    derived information (like `num_parameters`).
  id: totrans-857
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，模型使用此结构体进行初始化，日志打印所有派生信息（如 `num_parameters`）。
- en: Why It Matters
  id: totrans-858
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: The config is the contract between your dataset and your model.
  id: totrans-859
  prefs: []
  type: TYPE_NORMAL
  zh: 配置是您的数据集和模型之间的合同。
- en: If `vocab_size` doesn’t match your tokenizer, you’ll get crashes.
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 `vocab_size` 与您的分词器不匹配，您会得到崩溃。
- en: If `max_seq_len` is too small, you’ll lose context.
  id: totrans-861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 `max_seq_len` 太小，你会丢失上下文。
- en: If `num_layers` or `channels` are too large for your GPU, you’ll run out of
    memory.
  id: totrans-862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 `num_layers` 或 `channels` 对于您的 GPU 来说太大，您会耗尽内存。
- en: By tweaking the config, you decide whether you want a tiny model for learning
    or a massive one closer to GPT-2 XL.
  id: totrans-863
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调整配置，你可以决定是想要一个用于学习的微型模型，还是一个接近 GPT-2 XL 的巨型模型。
- en: Try It Yourself
  id: totrans-864
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 尝试自己操作
- en: 'Print config: Run the trainer and note the printed values. Compare them with
    the GPT-2 sizes in the table.'
  id: totrans-865
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印配置：运行训练器并注意打印的值。将它们与表中的 GPT-2 规模进行比较。
- en: 'Shrink the model: Change `num_layers = 4`, `num_heads = 4`, and `channels =
    256`. Train on Tiny Shakespeare and see how fast it runs.'
  id: totrans-866
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 缩小模型：将 `num_layers = 4`、`num_heads = 4` 和 `channels = 256` 进行更改。在 Tiny Shakespeare
    上进行训练，看看运行速度有多快。
- en: 'Increase sequence length: Try setting `max_seq_len = 2048`. Does your GPU still
    handle it, or do you get out-of-memory errors?'
  id: totrans-867
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 增加序列长度：尝试设置 `max_seq_len = 2048`。你的 GPU 是否还能处理，或者你会得到内存不足的错误？
- en: 'Parameter count check: Compute how many parameters your custom config has.
    Compare it to the reported `num_parameters`.'
  id: totrans-868
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 参数计数检查：计算您的自定义配置有多少参数。将其与报告的 `num_parameters` 进行比较。
- en: 'Tokenizer mismatch test: Intentionally set `vocab_size = 30000` and watch what
    error appears when loading the tokenizer.'
  id: totrans-869
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分词器不匹配测试：故意设置 `vocab_size = 30000`，并观察在加载分词器时出现什么错误。
- en: The Takeaway
  id: totrans-870
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 要点总结
- en: 'The GPT-2 config struct in *llm.c* is small but powerful. It defines everything
    about the model’s architecture: vocabulary, sequence length, depth, width, and
    total parameters. By adjusting just a few integers, you can scale from a toy model
    that runs on CPU to a billion-parameter giant (if your hardware allows it). Understanding
    these numbers is the first step to understanding how transformer capacity is controlled.'
  id: totrans-871
  prefs: []
  type: TYPE_NORMAL
  zh: '*llm.c* 中的 GPT-2 配置结构体虽小但功能强大。它定义了模型架构的所有内容：词汇表、序列长度、深度、宽度和总参数。只需调整几个整数，你就可以从在
    CPU 上运行的玩具模型扩展到具有数十亿参数的巨型模型（如果您的硬件允许的话）。理解这些数字是理解如何控制 Transformer 容量的第一步。'
- en: 22\. Parameter Tensors and Memory Layout
  id: totrans-872
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 22. 参数张量和内存布局
- en: Once the GPT-2 configuration is set, the next big step is to allocate the parameters
    of the model. These are the trainable numbers-weights and biases-that define how
    the model processes input tokens. In *llm.c*, parameters are stored in flat arrays
    of floats rather than in deeply nested objects like in PyTorch. This choice makes
    the code easier to read and keeps memory access predictable.
  id: totrans-873
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦设置好 GPT-2 配置，下一步重要的步骤就是分配模型的参数。这些是可训练的数字——权重和偏差——定义了模型如何处理输入标记。在 *llm.c* 中，参数以浮点数扁平数组的形式存储，而不是像
    PyTorch 中的深度嵌套对象。这种选择使得代码更容易阅读，并使内存访问可预测。
- en: What Are Parameters?
  id: totrans-874
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 什么是参数？
- en: 'Every part of the transformer has its own trainable weights:'
  id: totrans-875
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器的每个部分都有自己的可训练权重：
- en: 'Embedding tables: one for tokens and one for positions.'
  id: totrans-876
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入表：一个用于标记，一个用于位置。
- en: 'Attention layers: query, key, value, and output projections.'
  id: totrans-877
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力层：查询、键、值和输出投影。
- en: 'MLP layers: two linear layers plus their biases.'
  id: totrans-878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLP 层：两个线性层及其偏差。
- en: 'LayerNorms: scale (`gamma`) and shift (`beta`) values.'
  id: totrans-879
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层归一化：缩放（`gamma`）和偏移（`beta`）值。
- en: 'Final projection: maps hidden states back to vocab size for logits.'
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终投影：将隐藏状态映射回词汇大小以生成 logits。
- en: Together, these add up to hundreds of millions of numbers, even for GPT-2 Small.
  id: totrans-881
  prefs: []
  type: TYPE_NORMAL
  zh: 总共加起来有数亿个数字，即使是 GPT-2 小型也是如此。
- en: Flat Memory Design in *llm.c*
  id: totrans-882
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*llm.c* 中的扁平内存设计'
- en: Instead of allocating each parameter separately, *llm.c* stores all parameters
    in one contiguous block of memory. Each layer is given a slice of this big array.
  id: totrans-883
  prefs: []
  type: TYPE_NORMAL
  zh: '*llm.c* 不是单独分配每个参数，而是将所有参数存储在一个连续的内存块中。每个层都分配了这个大数组的一个切片。'
- en: 'This has two benefits:'
  id: totrans-884
  prefs: []
  type: TYPE_NORMAL
  zh: 这有两个好处：
- en: 'Simplicity: You only need one malloc (or cudaMalloc) for all parameters.'
  id: totrans-885
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 简单性：你只需要为所有参数分配一次 malloc（或 cudaMalloc）。
- en: 'Performance: Contiguous memory access is faster on both CPU and GPU.'
  id: totrans-886
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 性能：在 CPU 和 GPU 上，连续内存访问更快。
- en: To keep track of where each layer’s weights live inside the block, the code
    uses offsets.
  id: totrans-887
  prefs: []
  type: TYPE_NORMAL
  zh: 为了跟踪每个层的权重在块内的位置，代码使用偏移量。
- en: Example in Code
  id: totrans-888
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 代码示例
- en: 'In `train_gpt2.c`, parameters are packed into a single array:'
  id: totrans-889
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `train_gpt2.c` 中，参数被打包到一个单独的数组中：
- en: '[PRE44]'
  id: totrans-890
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Later, helper functions compute pointers into this array for each sub-module.
    For example, the token embedding weights are just the first slice:'
  id: totrans-891
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，辅助函数计算每个子模块的指针。例如，标记嵌入权重只是第一个切片：
- en: '[PRE45]'
  id: totrans-892
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Then the program moves forward, assigning chunks to positional embeddings, attention
    weights, and so on.
  id: totrans-893
  prefs: []
  type: TYPE_NORMAL
  zh: 然后程序继续前进，将块分配给位置嵌入、注意力权重等。
- en: Shapes of the Tensors
  id: totrans-894
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 张量的形状
- en: 'Even though parameters are stored in 1D memory, they conceptually form 2D or
    3D tensors. For example:'
  id: totrans-895
  prefs: []
  type: TYPE_NORMAL
  zh: 即使参数存储在 1D 内存中，它们在概念上形成 2D 或 3D 张量。例如：
- en: '| Parameter | Shape | Purpose |'
  id: totrans-896
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 形状 | 目的 |'
- en: '| --- | --- | --- |'
  id: totrans-897
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Token embeddings | `[vocab_size, channels]` | Maps token IDs to vectors.
    |'
  id: totrans-898
  prefs: []
  type: TYPE_TB
  zh: '| 标记嵌入 | `[vocab_size, channels]` | 将标记 ID 映射到向量。 |'
- en: '| Positional embeddings | `[max_seq_len, channels]` | Adds position info. |'
  id: totrans-899
  prefs: []
  type: TYPE_TB
  zh: '| 位置嵌入 | `[max_seq_len, channels]` | 添加位置信息。 |'
- en: '| Attention weights (Q, K, V, O) | `[channels, channels]` | Project hidden
    states. |'
  id: totrans-900
  prefs: []
  type: TYPE_TB
  zh: '| 注意力权重（Q, K, V, O） | `[channels, channels]` | 投影隐藏状态。 |'
- en: '| MLP layers | `[channels, 4×channels]` and `[4×channels, channels]` | Expand
    and contract hidden states. |'
  id: totrans-901
  prefs: []
  type: TYPE_TB
  zh: '| MLP 层 | `[channels, 4×channels]` 和 `[4×channels, channels]` | 扩展和收缩隐藏状态。
    |'
- en: '| LayerNorm scale/shift | `[channels]` | Normalize and rescale features. |'
  id: totrans-902
  prefs: []
  type: TYPE_TB
  zh: '| 层归一化缩放/偏移 | `[channels]` | 归一化和重新缩放特征。 |'
- en: 'When you look at the code, remember: these shapes are “virtual.” They’re just
    views into slices of the big 1D array.'
  id: totrans-903
  prefs: []
  type: TYPE_NORMAL
  zh: 当你看代码时，记住：这些形状是“虚拟”的。它们只是大 1D 数组切片的视图。
- en: Why This Layout Works Well
  id: totrans-904
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这种布局效果很好
- en: 'PyTorch or TensorFlow manage parameter tensors with lots of abstractions. *llm.c*
    strips this away: you see the raw memory, the exact number of parameters, and
    the order they’re laid out in. This makes it clear how large the model really
    is and why it uses so much RAM or VRAM.'
  id: totrans-905
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 或 TensorFlow 使用大量抽象来管理参数张量。*llm.c* 去掉了这些抽象：你看到的是原始内存、确切的参数数量以及它们布局的顺序。这使得模型的大小一目了然，以及为什么它使用这么多
    RAM 或 VRAM。
- en: It also means you can easily save and load checkpoints by writing or reading
    the flat array directly to disk. No need for complicated serialization formats.
  id: totrans-906
  prefs: []
  type: TYPE_NORMAL
  zh: 这也意味着你可以轻松地通过直接将扁平数组写入或读取磁盘来保存和加载检查点。无需复杂的序列化格式。
- en: Why It Matters
  id: totrans-907
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: 'Understanding parameter layout helps you:'
  id: totrans-908
  prefs: []
  type: TYPE_NORMAL
  zh: 理解参数布局有助于你：
- en: See how the model’s size explodes as you increase layers, heads, or channels.
  id: totrans-909
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 看看随着层数、头数或通道数的增加，模型的大小是如何爆炸的。
- en: Debug memory issues by checking how big each slice is.
  id: totrans-910
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过检查每个切片的大小来调试内存问题。
- en: Realize how much of training is just linear algebra on big arrays of floats.
  id: totrans-911
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 认识到训练中有多少只是大浮点数数组上的线性代数。
- en: 'This perspective is powerful because it demystifies deep learning: at its core,
    GPT-2 is just multiplying slices of one giant float array again and again.'
  id: totrans-912
  prefs: []
  type: TYPE_NORMAL
  zh: 这种视角很强大，因为它揭示了深度学习的神秘：在其核心，GPT-2只是反复乘以一个巨大浮点数数组的切片。
- en: Try It Yourself
  id: totrans-913
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 试试你自己
- en: 'Print parameter count: Add a line in the code to print `config.num_parameters`.
    Compare it with the table for GPT-2 Small/Medium/Large.'
  id: totrans-914
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印参数数量：在代码中添加一行以打印`config.num_parameters`。将其与GPT-2 Small/Medium/Large的表格进行比较。
- en: 'Inspect a slice: Print the first 10 numbers of the embedding table. They’ll
    look random (from initialization).'
  id: totrans-915
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查一个切片：打印嵌入表的前10个数字。它们看起来是随机的（来自初始化）。
- en: 'Change precision: Modify the code to allocate `half` (FP16) instead of `float`.
    How much memory do you save?'
  id: totrans-916
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改精度：修改代码以分配`half`（FP16）而不是`float`。你将节省多少内存？
- en: 'Checkpoint peek: Save a checkpoint, then open it in a hex viewer. It’s just
    raw floats-proof that parameters are stored flat.'
  id: totrans-917
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查点预览：保存一个检查点，然后在十六进制查看器中打开它。这只是参数存储平铺的原始浮点数证明。
- en: 'Parameter scaling: Double the number of layers and see how `num_parameters`
    changes. Can you predict the increase?'
  id: totrans-918
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 参数缩放：将层数加倍，看看`num_parameters`如何变化。你能预测增加量吗？
- en: The Takeaway
  id: totrans-919
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸收要点
- en: In *llm.c*, parameters are not hidden inside classes or objects. They live in
    one flat block of memory, sliced up by convention into embeddings, attention matrices,
    MLP weights, and norms. This design makes the relationship between model architecture
    and memory crystal clear-and reminds you that even a billion-parameter transformer
    is “just” a giant array of numbers.
  id: totrans-920
  prefs: []
  type: TYPE_NORMAL
  zh: 在`llm.c`中，参数不是隐藏在类或对象中。它们生活在一个扁平的内存块中，按照惯例被切割成嵌入、注意力矩阵、MLP权重和范数。这种设计使得模型架构与内存之间的关系非常清晰，并提醒你，即使是一个十亿参数的转换器也“只是”一个巨大的数字数组。
- en: '23\. Embedding Tables: Token + Positional'
  id: totrans-921
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 23. 嵌入表：标记 + 位置
- en: 'Before a transformer can reason about text, it first needs to turn tokens into
    vectors. In *llm.c*, this job is handled by the embedding tables: one for tokens,
    one for positions. These tables are the very first layer of GPT-2, and they transform
    plain integer IDs into continuous values that the neural network can process.'
  id: totrans-922
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个转换器能够对文本进行推理之前，它首先需要将标记转换为向量。在`llm.c`中，这项工作由嵌入表处理：一个用于标记，一个用于位置。这些表是GPT-2的第一层，它们将普通的整数ID转换为神经网络可以处理的连续值。
- en: Token Embedding Table
  id: totrans-923
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 标记嵌入表
- en: When you feed in a batch of token IDs, the model looks up their corresponding
    vectors in the token embedding table.
  id: totrans-924
  prefs: []
  type: TYPE_NORMAL
  zh: 当你输入一个标记ID批次时，模型会在标记嵌入表中查找它们对应的向量。
- en: 'Shape: `[vocab_size, channels]`'
  id: totrans-925
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 形状：`[vocab_size, channels]`
- en: '`vocab_size ≈ 50,257` (for GPT-2)'
  id: totrans-926
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size ≈ 50,257`（对于GPT-2）'
- en: '`channels = hidden size` (768 for GPT-2 Small)'
  id: totrans-927
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`channels = hidden size`（GPT-2 Small为768）'
- en: Each row corresponds to one token in the vocabulary.
  id: totrans-928
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每一行对应词汇表中的一个标记。
- en: Each row is a dense vector of size `channels`.
  id: totrans-929
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每行是一个大小为`channels`的密集向量。
- en: So if your input batch has size `(B, T)`, looking up embeddings gives you a
    tensor of shape `(B, T, channels)`.
  id: totrans-930
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果你的输入批次大小为`(B, T)`，查找嵌入将给出形状为`(B, T, channels)`的张量。
- en: 'In the code, this is implemented as an array slice from the flat parameter
    block:'
  id: totrans-931
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，这被实现为一个从平铺参数块中的数组切片：
- en: '[PRE46]'
  id: totrans-932
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: At runtime, token IDs index directly into this table.
  id: totrans-933
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行时，标记ID直接索引到这个表中。
- en: Positional Embedding Table
  id: totrans-934
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 位置嵌入表
- en: Transformers don’t inherently know about word order. That’s what positional
    embeddings are for.
  id: totrans-935
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器本身并不了解词序。这就是位置嵌入的作用。
- en: 'Shape: `[max_seq_len, channels]`'
  id: totrans-936
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 形状：`[max_seq_len, channels]`
- en: '`max_seq_len = 1024` in GPT-2 Small'
  id: totrans-937
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-2 Small中的`max_seq_len = 1024`
- en: Same channel dimension as token embeddings
  id: totrans-938
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与标记嵌入相同的通道维度
- en: Each position (0, 1, 2, …, 1023) has its own vector.
  id: totrans-939
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个位置（0，1，2，…，1023）都有自己的向量。
- en: During training, when the model sees token `i` at position `j`, it takes the
    token embedding vector and adds the positional embedding vector for `j`. This
    gives the model both word identity and word position.
  id: totrans-940
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，当模型在位置`j`看到标记`i`时，它将标记嵌入向量与位置嵌入向量`j`相加。这给模型提供了词身份和词位置。
- en: In *llm.c*, positional embeddings immediately follow the token embeddings in
    the flat parameter array.
  id: totrans-941
  prefs: []
  type: TYPE_NORMAL
  zh: 在`llm.c`中，位置嵌入紧随标记嵌入之后在平铺参数数组中。
- en: Adding Them Together
  id: totrans-942
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将它们相加
- en: 'The embedding layer’s forward pass is simple:'
  id: totrans-943
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入层的正向传播很简单：
- en: '[PRE47]'
  id: totrans-944
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: This results in a `(B, T, channels)` tensor that becomes the input to the first
    transformer block.
  id: totrans-945
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了一个形状为`(B, T, channels)`的张量，成为第一个转换器块的输入。
- en: Why This Matters
  id: totrans-946
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 这为什么重要
- en: 'Embeddings are the bridge between discrete tokens and continuous math. Without
    them, the model couldn’t use linear algebra to learn patterns. By adding positional
    embeddings, GPT-2 knows the difference between:'
  id: totrans-947
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入是离散令牌和连续数学之间的桥梁。没有它们，模型无法使用线性代数来学习模式。通过添加位置嵌入，GPT-2 能够知道以下区别：
- en: “dog bites man” → `dog` comes first, `man` comes last
  id: totrans-948
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “dog bites man” → `dog` 在前，`man` 在后
- en: “man bites dog” → same tokens, but swapped positions change the meaning
  id: totrans-949
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “man bites dog” → 相同的令牌，但交换位置会改变意义
- en: 'This small step is essential: order and identity must both be captured before
    attention can begin.'
  id: totrans-950
  prefs: []
  type: TYPE_NORMAL
  zh: 这一小步至关重要：在注意力开始之前，必须同时捕捉顺序和身份。
- en: Try It Yourself
  id: totrans-951
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 尝试自己操作
- en: 'Inspect shapes: Print the sizes of the token and positional embedding tables
    during initialization. Confirm they match `[vocab_size, channels]` and `[max_seq_len,
    channels]`.'
  id: totrans-952
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查形状：在初始化期间打印令牌和位置嵌入表的大小。确认它们匹配 `[vocab_size, channels]` 和 `[max_seq_len, channels]`。
- en: 'Look at first rows: Print the first 5 vectors of the token embedding table.
    They should look like small random floats from initialization.'
  id: totrans-953
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看前五行：打印令牌嵌入表的前5个向量。它们应该看起来像初始化时的小随机浮点数。
- en: 'Change max_seq_len: Double `max_seq_len` in the config. How does this change
    the size of the positional table? Does training still work?'
  id: totrans-954
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 改变 max_seq_len：在配置中将 `max_seq_len` 加倍。这如何改变位置表的大小？训练是否仍然有效？
- en: 'Overwrite embeddings: Try setting the token embedding table to all zeros. What
    happens to training loss?'
  id: totrans-955
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 覆盖嵌入：尝试将令牌嵌入表设置为全零。训练损失会发生什么变化？
- en: 'Sampling experiment: After training a few steps, decode outputs without adding
    positional embeddings. Do the results become nonsensical or repetitive?'
  id: totrans-956
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 采样实验：训练了几步之后，不添加位置嵌入解码输出。结果是否变得无意义或重复？
- en: The Takeaway
  id: totrans-957
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸收要点
- en: The embedding tables are the foundation of GPT-2\. Token embeddings give meaning
    to symbols, while positional embeddings give structure to sequences. In *llm.c*,
    they are just two slices of the flat parameter array, added together at the very
    start of the forward pass-but without them, the transformer would be blind to
    both words and order.
  id: totrans-958
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入表是 GPT-2 的基础。令牌嵌入赋予符号意义，而位置嵌入赋予序列结构。在 *llm.c* 中，它们只是平坦参数数组中的两个切片，在正向传递的非常开始时相加——但如果没有它们，transformer
    将对单词和顺序都视而不见。
- en: '24\. Attention Stack: QKV Projections and Geometry'
  id: totrans-959
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 24. 注意力栈：QKV 投影和几何
- en: 'After embeddings, the real magic of transformers begins: the attention mechanism.
    In GPT-2, every transformer block contains an attention stack. This is where the
    model learns how each token relates to others in the sequence-whether it’s paying
    attention to the previous word, the beginning of a sentence, or even punctuation
    marks far away.'
  id: totrans-960
  prefs: []
  type: TYPE_NORMAL
  zh: 在嵌入之后，transformers 的真正魔力开始了：注意力机制。在 GPT-2 中，每个 transformer 块都包含一个注意力栈。这就是模型学习每个令牌如何与序列中的其他令牌相关联的地方——无论是关注前面的单词、句子的开头，甚至是远处的标点符号。
- en: What Attention Does
  id: totrans-961
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意力做什么
- en: 'Attention lets the model answer the question:'
  id: totrans-962
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力让模型能够回答以下问题：
- en: “Given the current word, which other words in the context should I care about,
    and how much?”
  id: totrans-963
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “给定当前单词，我应该关注上下文中的哪些其他单词，以及关注程度如何？”
- en: Instead of treating words independently, the model uses attention to build connections
    across the sequence.
  id: totrans-964
  prefs: []
  type: TYPE_NORMAL
  zh: 与独立处理单词不同，模型使用注意力在序列中建立连接。
- en: The Q, K, V Projections
  id: totrans-965
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Q、K、V 投影
- en: 'Each attention block starts with three linear projections:'
  id: totrans-966
  prefs: []
  type: TYPE_NORMAL
  zh: 每个注意力块都以三个线性投影开始：
- en: '| Name | Shape | Purpose |'
  id: totrans-967
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 形状 | 目的 |'
- en: '| --- | --- | --- |'
  id: totrans-968
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Q (Query) | `[channels, channels]` | Represents what each token is *asking*
    about. |'
  id: totrans-969
  prefs: []
  type: TYPE_TB
  zh: '| Q (Query) | `[channels, channels]` | 代表每个令牌在询问什么。|'
- en: '| K (Key) | `[channels, channels]` | Represents how each token can be *recognized*.
    |'
  id: totrans-970
  prefs: []
  type: TYPE_TB
  zh: '| K (Key) | `[channels, channels]` | 代表每个令牌如何被 *识别*。|'
- en: '| V (Value) | `[channels, channels]` | Represents the actual *information*
    to pass along. |'
  id: totrans-971
  prefs: []
  type: TYPE_TB
  zh: '| V (Value) | `[channels, channels]` | 代表实际 *信息* 要传递的内容。|'
- en: 'Here’s the flow:'
  id: totrans-972
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是流程：
- en: Each input vector (from embeddings or previous block) is multiplied by these
    three matrices to produce Q, K, and V vectors.
  id: totrans-973
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个输入向量（来自嵌入或前一个块）都乘以这三个矩阵，以产生 Q、K 和 V 向量。
- en: Attention scores are computed by comparing Qs with Ks.
  id: totrans-974
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意力分数是通过比较 Q 和 K 来计算的。
- en: These scores are used to weight the Vs, mixing information from other tokens
    into the current one.
  id: totrans-975
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些分数用于加权 V，将来自其他令牌的信息混合到当前令牌中。
- en: Geometry of Attention
  id: totrans-976
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意力的几何
- en: 'Q and K define a similarity score: how well does this token match another one?'
  id: totrans-977
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Q 和 K 定义了一个相似度分数：这个令牌与另一个令牌匹配得有多好？
- en: V carries the actual features (like meaning, grammar cues).
  id: totrans-978
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: V 承载实际特征（如意义、语法提示）。
- en: 'The result is a weighted sum: tokens borrow information from others based on
    attention scores.'
  id: totrans-979
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果是一个加权总和：标记根据注意力分数从其他标记那里借用信息。
- en: 'In equations:'
  id: totrans-980
  prefs: []
  type: TYPE_NORMAL
  zh: 在方程中：
- en: '[PRE48]'
  id: totrans-981
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: The division by `sqrt(d_k)` normalizes scores so they don’t blow up as dimensions
    grow.
  id: totrans-982
  prefs: []
  type: TYPE_NORMAL
  zh: 通过除以 `sqrt(d_k)` 来对分数进行归一化，这样当维度增长时，分数不会爆炸。
- en: Multi-Head Attention
  id: totrans-983
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多头注意力
- en: 'GPT-2 doesn’t use just one attention projection-it uses many in parallel, called
    heads. Each head learns to focus on different types of relationships:'
  id: totrans-984
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2 不仅使用一个注意力投影——它并行使用多个，称为头部。每个头部都学会专注于不同类型的关系：
- en: One head might track subject–verb agreement.
  id: totrans-985
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个头部可能跟踪主语-谓语一致。
- en: Another might watch punctuation and quotes.
  id: totrans-986
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个可能关注标点符号和引号。
- en: Another might connect pronouns to their referents.
  id: totrans-987
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个可能将代词与其指代者联系起来。
- en: 'For GPT-2 Small:'
  id: totrans-988
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 GPT-2 小型：
- en: 12 heads per layer
  id: totrans-989
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每层 12 个头部
- en: Each head works on a reduced dimension (`channels / num_heads`)
  id: totrans-990
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个头部在一个减少的维度（`channels / num_heads`）上工作
- en: Outputs are concatenated and projected back to `channels`
  id: totrans-991
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出被连接并投影回 `channels`
- en: This setup is what gives transformers their flexibility.
  id: totrans-992
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设置是赋予转换器其灵活性的原因。
- en: Implementation in *llm.c*
  id: totrans-993
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在 *llm.c* 中的实现
- en: 'In the parameter array, each transformer block has slices for Q, K, V, and
    output projection (O). During forward pass:'
  id: totrans-994
  prefs: []
  type: TYPE_NORMAL
  zh: 在参数数组中，每个转换器块都有 Q、K、V 和输出投影（O）的切片。在正向传递期间：
- en: Multiply input by Q, K, V matrices.
  id: totrans-995
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入乘以 Q、K、V 矩阵。
- en: Reshape into heads.
  id: totrans-996
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新塑形为头部。
- en: Compute attention scores (masked to prevent looking forward).
  id: totrans-997
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算注意力分数（被掩码以防止向前看）。
- en: Apply softmax.
  id: totrans-998
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用 softmax。
- en: Multiply by V to get weighted values.
  id: totrans-999
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 乘以 V 以获得加权值。
- en: Concatenate heads and apply the O projection.
  id: totrans-1000
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 连接头部并应用 O 投影。
- en: All of this is done with plain matrix multiplications and softmax calls-no magic
    beyond linear algebra.
  id: totrans-1001
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些操作都是通过普通的矩阵乘法和 softmax 调用完成的——没有超出线性代数的魔法。
- en: Why It Matters
  id: totrans-1002
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: Attention is the beating heart of GPT-2\. It’s how the model captures dependencies
    across text, from short-term grammar to long-range coherence. Without QKV, embeddings
    would stay isolated, and the model could never build context-aware representations.
  id: totrans-1003
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力是 GPT-2 的核心。它是模型如何捕捉文本中从短期语法到长期连贯性的依赖关系的方式。没有 QKV，嵌入将保持孤立，模型永远无法构建上下文感知的表示。
- en: Try It Yourself
  id: totrans-1004
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 尝试自己操作
- en: 'Print shapes: Log the shapes of Q, K, V matrices in one layer. Confirm they
    match `[channels, channels]`.'
  id: totrans-1005
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印形状：记录一层中 Q、K、V 矩阵的形状。确认它们匹配 `[channels, channels]`。
- en: 'Visualize scores: After a forward pass, print the attention weights for one
    head. Do they concentrate on recent tokens or spread across the sequence?'
  id: totrans-1006
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化分数：在正向传递后，打印一个头部的注意力权重。它们是否集中在最近的标记上，还是分布在整个序列中？
- en: 'Reduce heads: Change `num_heads` from 12 to 4\. What happens to validation
    loss?'
  id: totrans-1007
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 减少头部：将 `num_heads` 从 12 更改为 4。验证损失会发生什么？
- en: 'Break symmetry: Initialize all Q, K, V matrices with zeros. Does training loss
    decrease at all?'
  id: totrans-1008
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打破对称性：用零初始化所有 Q、K、V 矩阵。训练损失是否有所下降？
- en: 'Mask experiment: Disable the causal mask (allow looking ahead). Does the model
    “cheat” by predicting future tokens perfectly?'
  id: totrans-1009
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遮蔽实验：禁用因果掩码（允许向前看）。模型是否通过完美预测未来标记来“作弊”？
- en: The Takeaway
  id: totrans-1010
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸取的经验教训
- en: The attention stack is where tokens stop being isolated and start talking to
    each other. Q, K, and V projections turn context into weighted relationships,
    and multi-head attention lets the model juggle many types of dependencies at once.
    In *llm.c*, this is implemented with straightforward linear algebra, making the
    most powerful idea in modern NLP visible and accessible.
  id: totrans-1011
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力堆栈是标记停止孤立并开始相互交流的地方。Q、K 和 V 投影将上下文转换为加权关系，多头注意力使模型能够同时处理多种类型的依赖关系。在 *llm.c*
    中，这是通过简单的线性代数实现的，使现代 NLP 中最强大的想法变得可见和可访问。
- en: '25\. MLP Block: Linear Layers + Activation'
  id: totrans-1012
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 25. MLP 块：线性层 + 激活
- en: 'After attention mixes information across tokens, GPT-2 applies a second transformation
    inside each block: the MLP (Multi-Layer Perceptron). This part doesn’t look at
    other tokens-it processes each position independently. But it’s just as important
    because it gives the model extra capacity to transform and refine the hidden features
    before passing them to the next layer.'
  id: totrans-1013
  prefs: []
  type: TYPE_NORMAL
  zh: 在注意力在标记之间混合信息之后，GPT-2 在每个块内部应用第二个转换：MLP（多层感知器）。这部分不查看其他标记——它独立处理每个位置。但它的作用同样重要，因为它给模型提供了额外的容量，在将隐藏特征传递到下一层之前进行转换和细化。
- en: What the MLP Looks Like
  id: totrans-1014
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: MLP 的样子
- en: 'Every transformer block contains an MLP with two linear layers and a nonlinear
    activation in between:'
  id: totrans-1015
  prefs: []
  type: TYPE_NORMAL
  zh: 每个transformer块都包含一个MLP，其中包含两个线性层和一个非线性激活层之间：
- en: '[PRE49]'
  id: totrans-1016
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: This structure expands the feature dimension and then compresses it back down,
    which lets the network learn richer representations.
  id: totrans-1017
  prefs: []
  type: TYPE_NORMAL
  zh: 这种结构扩展了特征维度，然后将其压缩回，这使得网络能够学习更丰富的表示。
- en: Shapes of the Layers
  id: totrans-1018
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 层的形状
- en: 'If the hidden size (channels) is `d_model`, the MLP works as follows:'
  id: totrans-1019
  prefs: []
  type: TYPE_NORMAL
  zh: 如果隐藏大小（通道数）是`d_model`，MLP的工作方式如下：
- en: '| Step | Shape | Purpose |'
  id: totrans-1020
  prefs: []
  type: TYPE_TB
  zh: '| 步骤 | 形状 | 目的 |'
- en: '| --- | --- | --- |'
  id: totrans-1021
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Input | `[B, T, d_model]` | Output of attention for each token. |'
  id: totrans-1022
  prefs: []
  type: TYPE_TB
  zh: '| 输入 | `[B, T, d_model]` | 每个标记的注意力输出。 |'
- en: '| Linear1 | `[d_model, 4 × d_model]` | Expands features 4× wider. |'
  id: totrans-1023
  prefs: []
  type: TYPE_TB
  zh: '| Linear1 | `[d_model, 4 × d_model]` | 将特征扩展4倍更宽。 |'
- en: '| GELU | elementwise | Introduces nonlinearity. |'
  id: totrans-1024
  prefs: []
  type: TYPE_TB
  zh: '| GELU | 元素级 | 引入非线性。 |'
- en: '| Linear2 | `[4 × d_model, d_model]` | Projects back to original size. |'
  id: totrans-1025
  prefs: []
  type: TYPE_TB
  zh: '| Linear2 | `[4 × d_model, d_model]` | 将特征投影回原始大小。 |'
- en: '| Output | `[B, T, d_model]` | Same shape as input, ready for residual add.
    |'
  id: totrans-1026
  prefs: []
  type: TYPE_TB
  zh: '| 输出 | `[B, T, d_model]` | 与输入相同的形状，准备进行残差添加。 |'
- en: For GPT-2 Small (`d_model = 768`), Linear1 expands to 3072 channels, then Linear2
    reduces back to 768.
  id: totrans-1027
  prefs: []
  type: TYPE_NORMAL
  zh: 对于GPT-2 Small（`d_model = 768`），Linear1扩展到3072个通道，然后Linear2将其缩减回768。
- en: 'Activation: GELU'
  id: totrans-1028
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 激活：GELU
- en: 'The activation function in GPT-2 is GELU (Gaussian Error Linear Unit). It’s
    smoother than ReLU, giving the model a more nuanced way of handling values around
    zero. In code, GELU looks like:'
  id: totrans-1029
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2中的激活函数是GELU（高斯误差线性单元）。它比ReLU更平滑，给模型提供了处理零周围值的一种更细腻的方式。在代码中，GELU看起来像：
- en: '[PRE50]'
  id: totrans-1030
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'This formula may look complicated, but the idea is simple: it smoothly squashes
    negative values toward zero and keeps positive values flowing through.'
  id: totrans-1031
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式看起来可能很复杂，但理念很简单：它平滑地将负值推向零，并保持正值流动。
- en: Why Expand and Shrink?
  id: totrans-1032
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么扩展和缩减？
- en: 'The expansion to `4 × d_model` may seem wasteful, but it’s deliberate:'
  id: totrans-1033
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展到`4 × d_model`可能看起来很浪费，但这是故意的：
- en: Expanding gives the model more capacity to represent patterns at each token.
  id: totrans-1034
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展给模型提供了更多表示每个标记中模式的能力。
- en: Shrinking keeps the overall parameter count manageable.
  id: totrans-1035
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缩放保持整体参数数量可管理。
- en: Together, they act like a bottleneck layer that forces the model to transform
    information more effectively.
  id: totrans-1036
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一起，它们就像一个瓶颈层，迫使模型更有效地转换信息。
- en: This “expand → activate → shrink” design is one of the main reasons transformers
    scale so well.
  id: totrans-1037
  prefs: []
  type: TYPE_NORMAL
  zh: 这种“扩展→激活→缩减”设计是transformer能够很好地扩展的主要原因之一。
- en: Implementation in *llm.c*
  id: totrans-1038
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在*llm.c*中的实现
- en: 'Just like attention, the MLP parameters live in the flat array of floats. Each
    block stores two weight matrices and two bias vectors. During forward pass:'
  id: totrans-1039
  prefs: []
  type: TYPE_NORMAL
  zh: 就像注意力一样，MLP参数位于浮点数的平面数组中。每个块存储两个权重矩阵和两个偏差向量。在正向传递过程中：
- en: Multiply input by `Linear1` weights, add bias.
  id: totrans-1040
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入乘以`Linear1`权重，并添加偏差。
- en: Apply GELU elementwise.
  id: totrans-1041
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 元素级应用GELU。
- en: Multiply by `Linear2` weights, add bias.
  id: totrans-1042
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其乘以`Linear2`权重，并添加偏差。
- en: Pass result through residual connection.
  id: totrans-1043
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过残差连接传递结果。
- en: Because each position is processed independently, the MLP is easy to parallelize
    across tokens.
  id: totrans-1044
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个位置都是独立处理的，因此MLP可以轻松地在标记之间并行化。
- en: Why It Matters
  id: totrans-1045
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: The MLP is the nonlinear refiner of transformer blocks. Attention spreads information,
    but MLPs transform it in-place, giving the model more expressive power. Without
    the MLP, the network would be mostly linear, limiting its ability to capture complex
    patterns in text.
  id: totrans-1046
  prefs: []
  type: TYPE_NORMAL
  zh: MLP是transformer块的非线性细化器。注意力传播信息，但MLPs在原地转换它，赋予模型更多的表达能力。没有MLP，网络将主要是线性的，限制其捕获文本中复杂模式的能力。
- en: Try It Yourself
  id: totrans-1047
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 试试你自己
- en: 'Print shapes: Log the dimensions of Linear1 and Linear2 weights in one block.
    Do they match `[768, 3072]` and `[3072, 768]` for GPT-2 Small?'
  id: totrans-1048
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印形状：记录Linear1和Linear2权重的维度在一个块中。它们是否与GPT-2 Small的`[768, 3072]`和`[3072, 768]`相匹配？
- en: 'Swap activation: Replace GELU with ReLU in the code. Does training still work?
    How does validation loss compare?'
  id: totrans-1049
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 交换激活：在代码中将GELU替换为ReLU。训练是否仍然有效？验证损失如何比较？
- en: 'Reduce expansion: Change expansion from 4× to 2× (`[768, 1536]`). What effect
    does this have on parameter count and performance?'
  id: totrans-1050
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 减少扩展：将扩展从4×改为2×（`[768, 1536]`）。这对参数数量和性能有什么影响？
- en: 'Zero out MLP: Set MLP weights to zero. Does the model still learn anything,
    or does performance collapse?'
  id: totrans-1051
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 清零MLP：将MLP权重设置为零。模型是否仍然学习到任何东西，或者性能是否崩溃？
- en: 'Compare speed: Measure training step time with and without the MLP enabled.
    How much slower is it?'
  id: totrans-1052
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较速度：测量启用和未启用MLP的训练步骤时间。慢了多少？
- en: The Takeaway
  id: totrans-1053
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 收获
- en: The MLP block in GPT-2 is a simple two-layer network with GELU activation, applied
    independently to each token. It expands, activates, and compresses features, giving
    the model nonlinear power to reshape hidden states. In *llm.c*, it’s implemented
    with basic matrix multiplications and a smooth GELU function, proving that even
    small building blocks can have a big impact on the model’s ability to learn language.
  id: totrans-1054
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2 中的 MLP 块是一个简单的两层网络，具有 GELU 激活，独立应用于每个标记。它扩展、激活和压缩特征，赋予模型非线性能力以重塑隐藏状态。在
    *llm.c* 中，它通过基本的矩阵乘法和平滑的 GELU 函数实现，证明了即使是小的构建块也可以对模型学习语言的能力产生重大影响。
- en: '26\. LayerNorm: Theory and Implementation (`doc/layernorm`)'
  id: totrans-1055
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 26. LayerNorm：理论实现（`doc/layernorm`）
- en: Deep neural networks often suffer from unstable training if activations drift
    too high or too low. To stabilize this, GPT-2 uses Layer Normalization (LayerNorm)
    inside every transformer block. In *llm.c*, LayerNorm is implemented directly
    in C, and there’s even a detailed explanation in the repo’s `doc/layernorm` file
    to help learners understand how it works.
  id: totrans-1056
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络在激活值过高或过低时往往会遭受不稳定的训练。为了稳定这一点，GPT-2 在每个 transformer 块内部使用 Layer Normalization（LayerNorm）。在
    *llm.c* 中，LayerNorm 直接用 C 语言实现，并在存储库的 `doc/layernorm` 文件中有详细的解释，以帮助学习者理解它是如何工作的。
- en: The Idea of Normalization
  id: totrans-1057
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 归一化的理念
- en: 'When you pass vectors through many layers, their values can become unbalanced-some
    features dominate while others shrink. Normalization fixes this by:'
  id: totrans-1058
  prefs: []
  type: TYPE_NORMAL
  zh: 当向量通过许多层时，它们的值可能会变得不平衡——一些特征占主导地位，而其他特征则缩小。归一化通过以下方式解决这个问题：
- en: 'Centering: subtracting the mean of the vector.'
  id: totrans-1059
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定中心：从向量均值中减去。
- en: 'Scaling: dividing by the standard deviation.'
  id: totrans-1060
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 缩放：除以标准差。
- en: This makes every feature vector have mean 0 and variance 1, improving stability.
  id: totrans-1061
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得每个特征向量都有均值为 0 和方差为 1，提高了稳定性。
- en: Why “Layer” Norm?
  id: totrans-1062
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么叫“层”归一化？
- en: 'There are different kinds of normalization (BatchNorm, InstanceNorm, etc.).
    LayerNorm is special because:'
  id: totrans-1063
  prefs: []
  type: TYPE_NORMAL
  zh: 存在着不同类型的归一化（BatchNorm、InstanceNorm 等）。LayerNorm 是特殊的，因为：
- en: It normalizes across the features of a single token (the “layer”), not across
    the batch.
  id: totrans-1064
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它在单个标记（“层”）的特征上归一化，而不是在整个批次上。
- en: This makes it independent of batch size, which is important for NLP where batch
    sizes can vary.
  id: totrans-1065
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这使其与批大小无关，这对于批大小可能变化的 NLP 来说非常重要。
- en: So if a hidden vector has 768 channels, LayerNorm computes the mean and variance
    over those 768 numbers for each token.
  id: totrans-1066
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果一个隐藏向量有 768 个通道，LayerNorm 会为每个标记计算这些 768 个数字的均值和方差。
- en: Trainable Parameters
  id: totrans-1067
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可训练参数
- en: 'LayerNorm isn’t just normalization-it also has two trainable vectors:'
  id: totrans-1068
  prefs: []
  type: TYPE_NORMAL
  zh: LayerNorm 不仅是一种归一化，它还具有两个可训练的向量：
- en: 'γ (gamma): scales each feature after normalization.'
  id: totrans-1069
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: γ（gamma）：在归一化后缩放每个特征。
- en: 'β (beta): shifts each feature after normalization.'
  id: totrans-1070
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: β（beta）：在归一化后移动每个特征。
- en: These allow the network to “undo” normalization when necessary, giving it flexibility.
  id: totrans-1071
  prefs: []
  type: TYPE_NORMAL
  zh: 这些允许网络在必要时“撤销”归一化，提供灵活性。
- en: Formula
  id: totrans-1072
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 公式
- en: 'For each input vector `x` of size `d`:'
  id: totrans-1073
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个大小为 `d` 的输入向量 `x`：
- en: '[PRE51]'
  id: totrans-1074
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Where `eps` is a tiny constant (like `1e-5`) to avoid dividing by zero.
  id: totrans-1075
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 `eps` 是一个非常小的常数（如 `1e-5`），以避免除以零。
- en: Implementation in *llm.c*
  id: totrans-1076
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在 *llm.c* 中的实现
- en: In the code, LayerNorm is implemented as a simple function that loops over features,
    computes mean and variance, and applies the formula above. It’s not hidden inside
    a framework-it’s right there in C, so you can step through it line by line.
  id: totrans-1077
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，LayerNorm 被实现为一个简单的函数，它遍历特征，计算均值和方差，并应用上述公式。它并没有隐藏在框架内部——它就在 C 语言中，因此你可以逐行进行调试。
- en: 'For example, the forward pass looks like this (simplified):'
  id: totrans-1078
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，前向传递看起来像这样（简化版）：
- en: '[PRE52]'
  id: totrans-1079
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: This is the kind of clear, low-level implementation that makes *llm.c* educational.
  id: totrans-1080
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种清晰、低级的实现，使得 *llm.c* 具有教育意义。
- en: Where It Fits in GPT-2
  id: totrans-1081
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 它在 GPT-2 中的位置
- en: 'Each transformer block contains two LayerNorms:'
  id: totrans-1082
  prefs: []
  type: TYPE_NORMAL
  zh: 每个transformer块包含两个LayerNorm：
- en: One before attention.
  id: totrans-1083
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在注意力之前。
- en: One before the MLP.
  id: totrans-1084
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 MLP 之前。
- en: 'GPT-2 uses Pre-LN architecture: inputs are normalized before each sublayer.
    This makes training more stable and gradients flow better.'
  id: totrans-1085
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2 使用 Pre-LN 架构：在每个子层之前对输入进行归一化。这使得训练更加稳定，梯度流动得更好。
- en: Why It Matters
  id: totrans-1086
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 它为什么很重要
- en: LayerNorm may look like a small detail, but without it, GPT-2 would fail to
    train reliably. It smooths out the flow of activations so attention and MLP layers
    can do their job. In practice, this is one of the critical “glue” components that
    makes deep transformers trainable at scale.
  id: totrans-1087
  prefs: []
  type: TYPE_NORMAL
  zh: LayerNorm 可能看起来是一个小细节，但没有它，GPT-2 就无法可靠地训练。它平滑了激活流的流动，以便注意力和 MLP 层可以完成它们的工作。在实践中，这是关键的“胶水”组件之一，使得深度变压器能够在规模上可训练。
- en: Try It Yourself
  id: totrans-1088
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 试试看
- en: 'Print statistics: After applying LayerNorm, print the mean and variance of
    the output. Do they stay close to 0 and 1?'
  id: totrans-1089
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印统计数据：在应用 LayerNorm 后，打印输出的均值和方差。它们是否接近 0 和 1？
- en: 'Remove γ and β: Force gamma to 1 and beta to 0\. Does the model still train?
    Compare losses.'
  id: totrans-1090
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移除 γ 和 β：将 gamma 强制设为 1，beta 设为 0。模型是否仍然可以训练？比较损失。
- en: 'Disable normalization: Comment out LayerNorm and train. How unstable does training
    become?'
  id: totrans-1091
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 禁用归一化：取消注释 LayerNorm 并训练。训练变得有多不稳定？
- en: 'Compare positions: Try switching to Post-LN (apply normalization after attention/MLP).
    Does this change convergence speed?'
  id: totrans-1092
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较位置：尝试切换到 Post-LN（在注意力/MLP 之后应用归一化）。这会改变收敛速度吗？
- en: 'Vary epsilon: Change `1e-5` to `1e-2` or `1e-8`. How sensitive is training?'
  id: totrans-1093
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 改变 epsilon：将 `1e-5` 改为 `1e-2` 或 `1e-8`。训练有多敏感？
- en: The Takeaway
  id: totrans-1094
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸收要点
- en: LayerNorm is the quiet stabilizer of GPT-2\. It makes sure each token’s features
    stay balanced, while γ and β keep flexibility. In *llm.c*, it’s implemented directly
    with clear C code, letting you see exactly how normalization is calculated. It’s
    a small but indispensable piece of the transformer puzzle.
  id: totrans-1095
  prefs: []
  type: TYPE_NORMAL
  zh: LayerNorm 是 GPT-2 的安静稳定器。它确保每个标记的特征保持平衡，而 γ 和 β 保持灵活性。在 *llm.c* 中，它通过清晰的 C 代码直接实现，让你看到归一化是如何计算的。它是
    Transformer 模式中的一个微小但不可或缺的部分。
- en: '27\. Residual Connections: Keeping the Signal Flowing'
  id: totrans-1096
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 27. 残差连接：保持信号流动
- en: Transformers like GPT-2 don’t just stack layers on top of each other blindly.
    They use residual connections-a trick that allows the input of a layer to be added
    back to its output. This simple addition helps signals flow through the network
    without vanishing or exploding, and it makes training deep models possible.
  id: totrans-1097
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2 这样的 Transformer 并不是盲目地堆叠层。它们使用残差连接——这是一种允许层的输入加回到其输出的技巧。这种简单的加法有助于信号在网络中流动，而不会消失或爆炸，并使得训练深度模型成为可能。
- en: The Basic Idea
  id: totrans-1098
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基本思想
- en: 'Imagine you have a function `F(x)` representing some transformation (like attention
    or an MLP). Instead of just computing:'
  id: totrans-1099
  prefs: []
  type: TYPE_NORMAL
  zh: 想象你有一个函数 `F(x)` 代表某种转换（如注意力或 MLP）。而不是仅仅计算：
- en: '[PRE53]'
  id: totrans-1100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'the transformer does:'
  id: totrans-1101
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 做的是：
- en: '[PRE54]'
  id: totrans-1102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: This means the layer learns only the *difference* it needs to add to the input,
    instead of replacing it entirely.
  id: totrans-1103
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着该层只学习需要添加到输入中的 *差异*，而不是完全替换它。
- en: Why This Helps
  id: totrans-1104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这有帮助
- en: 'Residuals solve two big problems in deep networks:'
  id: totrans-1105
  prefs: []
  type: TYPE_NORMAL
  zh: 残差解决了深度网络中的两个大问题：
- en: 'Gradient flow: During backpropagation, gradients can get smaller and smaller
    as they pass through many layers. Adding the input back ensures gradients always
    have a path straight through.'
  id: totrans-1106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 梯度流：在反向传播过程中，梯度在通过许多层时会变得越来越小。将输入加回确保梯度始终有一条直通路径。
- en: 'Information preservation: Even if `F(x)` distorts the signal, the original
    `x` is still there. This prevents the model from “forgetting” important information.'
  id: totrans-1107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 信息保留：即使 `F(x)` 扭曲了信号，原始的 `x` 仍然存在。这防止了模型“忘记”重要信息。
- en: 'Faster training: The network doesn’t have to re-learn identity mappings-it
    can just pass them through the skip connection.'
  id: totrans-1108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更快的训练：网络不必重新学习恒等映射——它只需通过跳跃连接传递它们。
- en: Implementation in *llm.c*
  id: totrans-1109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在 *llm.c* 中的实现
- en: 'Residuals in *llm.c* are implemented as a straightforward elementwise addition:'
  id: totrans-1110
  prefs: []
  type: TYPE_NORMAL
  zh: '*llm.c* 中的残差实现为一个简单的逐元素加法：'
- en: '[PRE55]'
  id: totrans-1111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Here:'
  id: totrans-1112
  prefs: []
  type: TYPE_NORMAL
  zh: 这里：
- en: '`inp1` is the output of the layer (like attention).'
  id: totrans-1113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inp1` 是层的输出（如注意力）。'
- en: '`inp2` is the original input.'
  id: totrans-1114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inp2` 是原始输入。'
- en: '`out` is the combined result.'
  id: totrans-1115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`out` 是组合结果。'
- en: This is done for every token position and feature channel.
  id: totrans-1116
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对每个标记位置和特征通道都进行的。
- en: Where Residuals Are Used
  id: totrans-1117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 残差的使用位置
- en: 'In GPT-2, every transformer block has two residuals:'
  id: totrans-1118
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GPT-2 中，每个 Transformer 模块包含两个残差连接：
- en: 'Attention residual: Adds the input of the attention layer to its output.'
  id: totrans-1119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意力残差：将注意力层的输入加到其输出上。
- en: 'MLP residual: Adds the input of the MLP to its output.'
  id: totrans-1120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: MLP 残差：将 MLP 的输入加到其输出上。
- en: So the data flowing through the network always carries both the new transformation
    and the original signal.
  id: totrans-1121
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过网络流动的数据始终携带新的转换和原始信号。
- en: Why It Matters
  id: totrans-1122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: Without residual connections, stacking 12–48 transformer blocks would be nearly
    impossible to train. Gradients would vanish, and the model would either stop learning
    or take forever to converge. Residuals let deep transformers scale smoothly.
  id: totrans-1123
  prefs: []
  type: TYPE_NORMAL
  zh: 没有残差连接，堆叠 12-48 个 Transformer 模块几乎不可能进行训练。梯度会消失，模型要么停止学习，要么永远无法收敛。残差让深度 Transformer
    能够平滑地扩展。
- en: 'They also add an intuitive interpretation: each block is like a “refinement
    step” rather than a full rewrite of the representation.'
  id: totrans-1124
  prefs: []
  type: TYPE_NORMAL
  zh: 他们还添加了一个直观的解释：每个块就像一个“细化步骤”，而不是对表示的全面重写。
- en: Try It Yourself
  id: totrans-1125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 试试看
- en: 'Remove residuals: Comment out the addition in the code. Does training collapse?'
  id: totrans-1126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移除剩余项：在代码中取消注释的添加。训练会崩溃吗？
- en: 'Scale residuals: Multiply the input by 0.5 before adding. Does this slow convergence?'
  id: totrans-1127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 缩放剩余项：在相加之前将输入乘以0.5。这会减慢收敛速度吗？
- en: 'Check loss curves: Compare training with and without residuals for the first
    500 steps.'
  id: totrans-1128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查损失曲线：比较前500步有和无剩余项的训练。
- en: 'Inspect outputs: Print the norms of `inp1`, `inp2`, and `out`. Are the scales
    balanced?'
  id: totrans-1129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查输出：打印 `inp1`、`inp2` 和 `out` 的范数。比例是否平衡？
- en: 'Deeper models: Increase the number of layers from 12 to 24\. Does the importance
    of residuals become more obvious?'
  id: totrans-1130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更深的模型：将层数从12增加到24。剩余项的重要性变得更加明显吗？
- en: The Takeaway
  id: totrans-1131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸收要点
- en: 'Residual connections are the “lifeline” of deep transformers. By simply adding
    inputs back into outputs, they make it possible to train very deep networks without
    losing gradients or information. In *llm.c*, the implementation is as simple as
    looping over arrays and adding them-but the effect is profound: residuals are
    what let GPT-2 go deep and still work.'
  id: totrans-1132
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余连接是深度变换器的“生命线”。通过简单地将输入加回到输出中，它们使得训练非常深的网络而不会丢失梯度或信息成为可能。在 *llm.c* 中，实现起来就像遍历数组并将它们相加一样简单——但效果是深远的：剩余项使得
    GPT-2 能够深入且仍然有效工作。
- en: '28\. Attention Masking: Enforcing Causality'
  id: totrans-1133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 28. 注意力掩码：强制因果性
- en: One of the defining traits of GPT-2 is that it’s a causal language model. That
    means it predicts the *next* token given all the tokens before it, but never cheats
    by looking ahead. To enforce this, GPT-2 applies an attention mask inside every
    attention layer.
  id: totrans-1134
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2 的一个定义特征是它是一个因果语言模型。这意味着它根据所有之前的标记预测下一个标记，但永远不会通过向前看而作弊。为了强制执行这一点，GPT-2
    在每个注意力层中应用一个注意力掩码。
- en: Why a Mask Is Needed
  id: totrans-1135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么需要掩码
- en: 'Without a mask, attention is free to connect any token to any other, including
    future ones. For example:'
  id: totrans-1136
  prefs: []
  type: TYPE_NORMAL
  zh: 没有掩码，注意力可以自由地将任何标记连接到任何其他标记，包括未来的标记。例如：
- en: 'Input: “The cat sat on the”'
  id: totrans-1137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入： “The cat sat on the”
- en: 'Target: “mat”'
  id: totrans-1138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标： “mat”
- en: If the model could peek at “mat” while computing attention, the task would be
    trivial-it could just copy the next word. That would break the training objective.
  id: totrans-1139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型在计算注意力时可以查看“mat”，那么任务将变得非常简单——它只需复制下一个单词即可。这将破坏训练目标。
- en: The mask forces the model to only use tokens at or before the current position
    when making predictions.
  id: totrans-1140
  prefs: []
  type: TYPE_NORMAL
  zh: 掩码迫使模型在做出预测时只使用当前位置或之前的标记。
- en: How the Mask Works
  id: totrans-1141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 掩码是如何工作的
- en: When computing attention scores (`Q × K^T / sqrt(d_k)`), the result is a matrix
    of size `[T, T]` where each row corresponds to one token attending to all others.
  id: totrans-1142
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算注意力分数（`Q × K^T / sqrt(d_k)`）时，结果是大小为 `[T, T]` 的矩阵，其中每一行对应一个标记正在关注所有其他标记。
- en: 'The mask modifies this matrix:'
  id: totrans-1143
  prefs: []
  type: TYPE_NORMAL
  zh: 掩码修改了这个矩阵：
- en: 'Allowed positions (past and present): keep scores as is.'
  id: totrans-1144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许的位置（过去和现在）：保持分数不变。
- en: 'Disallowed positions (future): set scores to `-inf`.'
  id: totrans-1145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不允许的位置（未来）：将分数设置为 `-inf`。
- en: After applying softmax, those `-inf` entries become zero probability, effectively
    blocking attention to the future.
  id: totrans-1146
  prefs: []
  type: TYPE_NORMAL
  zh: 应用 softmax 后，那些 `-inf` 条目变为零概率，有效地阻止了对未来的注意力。
- en: Implementation in *llm.c*
  id: totrans-1147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在 *llm.c* 中的实现
- en: 'The causal mask is applied during the attention forward pass. The code uses
    a loop to zero out invalid positions:'
  id: totrans-1148
  prefs: []
  type: TYPE_NORMAL
  zh: 因果掩码在注意力前向传递期间应用。代码使用循环将无效位置清零：
- en: '[PRE56]'
  id: totrans-1149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Here `T` is the sequence length. This ensures that token `t` can only attend
    to itself and earlier tokens.
  id: totrans-1150
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 `T` 是序列长度。这确保了标记 `t` 只能关注自身和更早的标记。
- en: Visualizing the Mask
  id: totrans-1151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可视化掩码
- en: 'Think of the mask as a triangular matrix:'
  id: totrans-1152
  prefs: []
  type: TYPE_NORMAL
  zh: 将掩码想象成一个三角形矩阵：
- en: '|  | 0 | 1 | 2 | 3 |'
  id: totrans-1153
  prefs: []
  type: TYPE_TB
  zh: '|  | 0 | 1 | 2 | 3 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-1154
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | ✓ |  |  |  |'
  id: totrans-1155
  prefs: []
  type: TYPE_TB
  zh: '| 0 | ✓ |  |  |  |'
- en: '| 1 | ✓ | ✓ |  |  |'
  id: totrans-1156
  prefs: []
  type: TYPE_TB
  zh: '| 1 | ✓ | ✓ |  |  |'
- en: '| 2 | ✓ | ✓ | ✓ |  |'
  id: totrans-1157
  prefs: []
  type: TYPE_TB
  zh: '| 2 | ✓ | ✓ | ✓ |  |'
- en: '| 3 | ✓ | ✓ | ✓ | ✓ |'
  id: totrans-1158
  prefs: []
  type: TYPE_TB
  zh: '| 3 | ✓ | ✓ | ✓ | ✓ |'
- en: Each row shows which past tokens a given position can look at. Future positions
    remain blank.
  id: totrans-1159
  prefs: []
  type: TYPE_NORMAL
  zh: 每行显示给定位置可以查看的过去标记。未来的位置保持空白。
- en: Why It Matters
  id: totrans-1160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: The mask is what makes GPT-2 a predictive model instead of a bidirectional encoder
    like BERT. Without it, the model could “cheat” and the training objective would
    no longer match how it’s used at inference time (generating text step by step).
  id: totrans-1161
  prefs: []
  type: TYPE_NORMAL
  zh: 掩码使得 GPT-2 成为一个预测模型，而不是像 BERT 这样的双向编码器。没有它，模型可能会“作弊”，训练目标将不再与推理时间（逐个生成文本）的使用相匹配。
- en: This small detail-just filling part of a matrix with `-inf`-is critical to making
    autoregressive text generation possible.
  id: totrans-1162
  prefs: []
  type: TYPE_NORMAL
  zh: 这个小细节——只是将矩阵的一部分填充为 `-inf`——对于使自回归文本生成成为可能至关重要。
- en: Try It Yourself
  id: totrans-1163
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 试试看
- en: 'Disable the mask: Comment out the masking code. Watch validation loss drop
    unrealistically, then notice that text generation produces garbage.'
  id: totrans-1164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 禁用掩码：取消注释掩码代码。观察验证损失不切实际地下降，然后注意文本生成产生了垃圾。
- en: 'Reverse the mask: Block the past and allow the future. Does the model still
    train? What does it predict?'
  id: totrans-1165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反转掩码：阻止过去并允许未来。模型是否仍然训练？它预测了什么？
- en: 'Partial mask: Only allow attention to the previous 5 tokens (a sliding window).
    How does this affect learning long-range structure?'
  id: totrans-1166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部分掩码：只允许关注前5个标记（滑动窗口）。这如何影响学习长距离结构？
- en: 'Print scores: Before and after masking, log a row of attention scores. Notice
    how future positions become huge negatives.'
  id: totrans-1167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印分数：在掩码前后记录一行注意力分数。注意未来位置变成了巨大的负数。
- en: 'Visualize: Write a small script to plot the attention mask as a matrix. It
    should look strictly lower-triangular.'
  id: totrans-1168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化：编写一个小脚本，将注意力掩码作为矩阵绘制。它应该看起来是严格下三角的。
- en: The Takeaway
  id: totrans-1169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸收要点
- en: Attention masking is a simple but essential trick. By filling future positions
    with `-inf` before softmax, GPT-2 ensures that each token can only attend to its
    past. In *llm.c*, this is implemented with just a couple of loops-but it’s what
    turns a generic transformer into a true causal language model.
  id: totrans-1170
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力掩码是一个简单但至关重要的技巧。通过在softmax之前用`-inf`填充未来位置，GPT-2确保每个标记只能关注其过去。在*llm.c*中，这通过几个循环实现——但它将一个通用的transformer转变为真正的因果语言模型。
- en: '29\. Output Head: From Hidden States to Vocabulary'
  id: totrans-1171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 29. 输出头：从隐藏状态到词汇表
- en: After tokens pass through embeddings, attention, MLPs, LayerNorm, and residuals,
    we end up with hidden states for every position in the sequence. But GPT-2’s final
    job is not to output vectors-it must predict the next token from the vocabulary.
    This is handled by the output head, the last stage of the model.
  id: totrans-1172
  prefs: []
  type: TYPE_NORMAL
  zh: 在标记通过嵌入、注意力、MLP、LayerNorm和残差后，我们得到了序列中每个位置的隐藏状态。但GPT-2的最终任务不是输出向量——它必须从词汇表中预测下一个标记。这是由输出头，模型的最后阶段来处理的。
- en: What the Output Head Does
  id: totrans-1173
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 输出头的作用
- en: The output head maps hidden states of shape `(B, T, channels)` into logits of
    shape `(B, T, vocab_size)`. Each logit represents the model’s “raw score” for
    how likely a particular token is at the next step.
  id: totrans-1174
  prefs: []
  type: TYPE_NORMAL
  zh: 输出头将形状为`(B, T, channels)`的隐藏状态映射到形状为`(B, T, vocab_size)`的logits。每个logit代表模型在下一步中特定标记可能性的“原始分数”。
- en: 'The pipeline looks like this:'
  id: totrans-1175
  prefs: []
  type: TYPE_NORMAL
  zh: 管道看起来像这样：
- en: '[PRE57]'
  id: totrans-1176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Logits: real numbers, one per token in the vocabulary.'
  id: totrans-1177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Logits：实数，每个词汇表中的标记一个。
- en: 'Softmax: converts logits into probabilities that sum to 1.'
  id: totrans-1178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Softmax：将logits转换为总和为1的概率。
- en: 'Predicted token: the token with the highest probability (or sampled from the
    distribution).'
  id: totrans-1179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测的标记：概率最高的标记（或从分布中采样）。
- en: Tied Weights with Embeddings
  id: totrans-1180
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 与嵌入绑定的权重
- en: 'In GPT-2, the token embedding table and the output head share weights. This
    means the same matrix is used both for:'
  id: totrans-1181
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPT-2中，标记嵌入表和输出头共享权重。这意味着相同的矩阵既用于：
- en: Mapping tokens to vectors at the start (embedding lookup).
  id: totrans-1182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在开始时将标记映射到向量（嵌入查找）。
- en: Mapping vectors back to tokens at the end (output head).
  id: totrans-1183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在最后将向量映射回标记（输出头）。
- en: Mathematically, this improves efficiency and helps align input and output representations.
  id: totrans-1184
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，这提高了效率并有助于对齐输入和输出表示。
- en: In *llm.c*, this is done by simply pointing both embedding and output head to
    the same parameter slice.
  id: totrans-1185
  prefs: []
  type: TYPE_NORMAL
  zh: 在*llm.c*中，这是通过简单地让嵌入和输出头指向相同的参数切片来实现的。
- en: '[PRE58]'
  id: totrans-1186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: When the model projects hidden states back to vocab space, it does a matrix
    multiply with this shared matrix.
  id: totrans-1187
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型将隐藏状态投影回词汇空间时，它使用这个共享矩阵进行矩阵乘法。
- en: Shapes in Action
  id: totrans-1188
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 形状的实际应用
- en: 'For GPT-2 Small:'
  id: totrans-1189
  prefs: []
  type: TYPE_NORMAL
  zh: 对于GPT-2 Small：
- en: 'Hidden states: `[B, T, 768]`'
  id: totrans-1190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏状态：`[B, T, 768]`
- en: 'Output projection (embedding transpose): `[768, 50257]`'
  id: totrans-1191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出投影（嵌入转置）：`[768, 50257]`
- en: 'Logits: `[B, T, 50257]`'
  id: totrans-1192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Logits：`[B, T, 50257]`
- en: That’s more than 50k scores per position, one for each token in the vocabulary.
  id: totrans-1193
  prefs: []
  type: TYPE_NORMAL
  zh: 这比每个位置50k个分数还要多，每个词汇表中的标记一个。
- en: Why Weight Tying Helps
  id: totrans-1194
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么权重绑定有帮助
- en: 'Memory efficiency: You don’t need a separate giant matrix for the output head.'
  id: totrans-1195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 内存效率：您不需要为输出头单独使用一个巨大的矩阵。
- en: 'Better learning: The same vectors that represent tokens going in also represent
    them going out, which reinforces consistency.'
  id: totrans-1196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更好的学习：代表进入的标记的相同向量也代表它们出去，这加强了一致性。
- en: 'Simpler code: Just reuse the same parameter slice.'
  id: totrans-1197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 简单的代码：只需重用相同的参数切片。
- en: This trick is why GPT-2 can scale vocab sizes without blowing up parameter counts
    too much.
  id: totrans-1198
  prefs: []
  type: TYPE_NORMAL
  zh: 这个技巧是为什么GPT-2可以在不过度增加参数计数的情况下扩展词汇大小。
- en: Why It Matters
  id: totrans-1199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: The output head is where everything comes together. For each position, the model
    collapses its hidden representation into a distribution over possible next tokens.
    This is how GPT-2 generates text one step at a time. Without this step, you’d
    only have abstract hidden states-useful internally, but not something you can
    read.
  id: totrans-1200
  prefs: []
  type: TYPE_NORMAL
  zh: 输出头是所有东西汇集的地方。对于每个位置，模型将其隐藏表示折叠成可能的下一个标记的分布。这就是 GPT-2 逐个步骤生成文本的方式。如果没有这一步，你将只有抽象的隐藏状态——虽然对内部有用，但不是你可以阅读的内容。
- en: Try It Yourself
  id: totrans-1201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 尝试自己操作
- en: 'Print logits: After a forward pass, print the logits for the last token. Do
    they look like random floats at initialization?'
  id: totrans-1202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印 logits：在正向传播之后，打印最后一个标记的 logits。它们在初始化时看起来像是随机的浮点数吗？
- en: 'Check probability sum: Apply softmax to logits and verify the probabilities
    sum to 1.'
  id: totrans-1203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查概率和：对 logits 应用 softmax 并验证概率之和是否为 1。
- en: 'Untie weights: Make the output head its own matrix instead of reusing embeddings.
    Does training still work? How does the parameter count change?'
  id: totrans-1204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解绑权重：使输出头成为自己的矩阵而不是重用嵌入。训练是否仍然有效？参数计数如何变化？
- en: 'Top-k sampling: Modify sampling to keep only the top 5 logits before softmax.
    What kind of text does this produce?'
  id: totrans-1205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Top-k 样本采样：修改采样以保留 softmax 之前的 top 5 logits。这会产生什么样的文本？
- en: 'Greedy vs random: Compare greedy decoding (argmax) vs random sampling from
    probabilities. Which one gives more interesting outputs?'
  id: totrans-1206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 贪婪解码与随机采样：比较贪婪解码（argmax）与从概率中进行随机采样的结果。哪一个会产生更有趣的输出？
- en: The Takeaway
  id: totrans-1207
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸收要点
- en: The output head is the final bridge between hidden vectors and actual words.
    By reusing the token embedding matrix, GPT-2 projects hidden states back into
    vocabulary space and produces logits for every possible token. In *llm.c*, this
    step is just another matrix multiplication-but it’s the one that turns internal
    math into real text predictions.
  id: totrans-1208
  prefs: []
  type: TYPE_NORMAL
  zh: 输出头是隐藏向量与实际单词之间的最终桥梁。通过重用标记嵌入矩阵，GPT-2 将隐藏状态重新投影到词汇空间，并为每个可能的标记生成 logits。在 *llm.c*
    中，这一步只是另一个矩阵乘法——但它将内部数学转化为实际的文本预测。
- en: '30\. Loss Function: Cross-Entropy over Vocabulary'
  id: totrans-1209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 30. 损失函数：词汇表上的交叉熵
- en: Training GPT-2 means teaching it to predict the next token in a sequence. To
    measure how well it’s doing, we need a loss function that compares the model’s
    predicted probabilities with the true token IDs. In *llm.c*, this is done with
    the cross-entropy loss-a standard choice for classification tasks.
  id: totrans-1210
  prefs: []
  type: TYPE_NORMAL
  zh: 训练 GPT-2 意味着教会它预测序列中的下一个标记。为了衡量其表现的好坏，我们需要一个损失函数，该函数将模型的预测概率与真实标记 ID 进行比较。在
    *llm.c* 中，这是通过交叉熵损失来完成的——这是分类任务的标准选择。
- en: From Logits to Probabilities
  id: totrans-1211
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 从 logits 到概率
- en: 'After the output head, we have logits of shape `(B, T, vocab_size)`. These
    are raw scores. To turn them into probabilities:'
  id: totrans-1212
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出头之后，我们得到形状为 `(B, T, vocab_size)` 的 logits。这些是原始分数。要将它们转换为概率：
- en: '[PRE59]'
  id: totrans-1213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Softmax ensures two things:'
  id: totrans-1214
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax 确保两件事：
- en: All values are between 0 and 1.
  id: totrans-1215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有值都在 0 到 1 之间。
- en: They sum to 1 across the vocabulary.
  id: totrans-1216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们在词汇表中加起来为 1。
- en: So for each position, you get a probability distribution over all possible next
    tokens.
  id: totrans-1217
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于每个位置，你都会得到一个覆盖所有可能下一个标记的概率分布。
- en: Cross-Entropy Definition
  id: totrans-1218
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 交叉熵定义
- en: 'Cross-entropy compares the predicted distribution `p` with the true distribution
    `q`. For language modeling:'
  id: totrans-1219
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵比较预测分布 `p` 与真实分布 `q`。对于语言建模：
- en: '`q` is a one-hot vector (all zeros, except 1 at the true token index).'
  id: totrans-1220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`q` 是一个 one-hot 向量（除了在真实标记索引处的 1 以外，其余都是 0）。'
- en: '`p` is the probability vector from softmax.'
  id: totrans-1221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`p` 是 softmax 的概率向量。'
- en: 'The formula for one token:'
  id: totrans-1222
  prefs: []
  type: TYPE_NORMAL
  zh: 单个标记的公式：
- en: '[PRE60]'
  id: totrans-1223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: For a batch, you average across all tokens in all sequences.
  id: totrans-1224
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个批次，你会在所有序列的所有标记上平均。
- en: Implementation in *llm.c*
  id: totrans-1225
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在 *llm.c* 中的实现
- en: 'In C, this boils down to:'
  id: totrans-1226
  prefs: []
  type: TYPE_NORMAL
  zh: 在 C 语言中，这归结为：
- en: '[PRE61]'
  id: totrans-1227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: This snippet shows how *llm.c* explicitly computes softmax and cross-entropy
    in loops. No black boxes-just raw math.
  id: totrans-1228
  prefs: []
  type: TYPE_NORMAL
  zh: 这个片段展示了 *llm.c* 如何在循环中显式地计算 softmax 和交叉熵。没有黑盒——只有原始数学。
- en: Intuition
  id: totrans-1229
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 直觉
- en: If the model assigns high probability to the correct token → loss is small.
  id: totrans-1230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果模型将高概率分配给正确标记 → 损失较小。
- en: If the model assigns low probability to the correct token → loss is large.
  id: totrans-1231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果模型将低概率分配给正确标记 → 损失较大。
- en: Minimizing loss means pushing probability mass toward the right answers.
  id: totrans-1232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小化损失意味着将概率质量推向正确答案。
- en: Why Cross-Entropy Works for Language
  id: totrans-1233
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么交叉熵适用于语言
- en: 'Language modeling is essentially a huge multi-class classification problem:
    at each step, which word comes next? Cross-entropy is perfect here because it
    directly penalizes wrong predictions proportional to how confident the model was.'
  id: totrans-1234
  prefs: []
  type: TYPE_NORMAL
  zh: 语言建模本质上是一个巨大的多类分类问题：在每一步，下一个单词是什么？交叉熵在这里是完美的，因为它直接按模型自信程度成比例惩罚错误预测。
- en: Why It Matters
  id: totrans-1235
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: The loss function is the only signal the model gets about how well it’s doing.
    Everything else-parameter updates, weight tuning, learning dynamics-flows from
    this single number. A well-implemented cross-entropy ensures training is stable
    and meaningful.
  id: totrans-1236
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数是模型关于其表现如何的唯一信号。所有其他——参数更新、权重调整、学习动态——都从这个单一数字中流出。一个实现良好的交叉熵确保训练稳定且有意义。
- en: Try It Yourself
  id: totrans-1237
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 尝试自己操作
- en: 'Check values: Print the loss after the first few steps. It should be close
    to `log(vocab_size)` (≈10.8 for 50k vocab) before training.'
  id: totrans-1238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查值：打印前几步后的损失。在训练之前，它应该接近 `log(vocab_size)`（对于50k词汇量约为10.8）。
- en: 'Overfit tiny batch: Train on just one sequence. Does the loss go near 0 after
    enough steps?'
  id: totrans-1239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 过拟合小批量：仅在一个序列上训练。经过足够的步骤后，损失是否接近0？
- en: 'Change target: Replace the true token with a random one. Does the loss increase
    immediately?'
  id: totrans-1240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 改变目标：用随机的一个替换真实标记。损失是否会立即增加？
- en: 'Compare vocab sizes: Train with a smaller vocabulary (e.g., 100 tokens). Does
    initial loss drop to `log(100) ≈ 4.6`?'
  id: totrans-1241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较词汇量大小：使用较小的词汇量（例如，100个标记）进行训练。初始损失是否下降到`log(100) ≈ 4.6`？
- en: 'Inspect probabilities: For one token, print the top 5 predicted probabilities.
    Does the true token climb to the top as training progresses?'
  id: totrans-1242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查概率：对于单个标记，打印前5个预测概率。随着训练的进行，真实标记是否上升到顶部？
- en: The Takeaway
  id: totrans-1243
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸收要点
- en: The cross-entropy loss is the compass guiding GPT-2 during training. It turns
    raw logits into probabilities and measures how well the model predicts the correct
    next token. In *llm.c*, it’s implemented with explicit loops and math, letting
    you see exactly how probabilities and losses are computed. Without this step,
    the model would have no way to learn from its mistakes.
  id: totrans-1244
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵损失是指导GPT-2训练的指南。它将原始logits转换为概率，并衡量模型预测正确下一个标记的能力。在`llm.c`中，它通过显式循环和数学实现，让你看到概率和损失是如何计算的。没有这一步，模型将无法从其错误中学习。
- en: Chapter 4\. CPU Inference (Forward Only)
  id: totrans-1245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第四章. CPU推理（仅前向）
- en: 31\. Forward Pass Walkthrough
  id: totrans-1246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 31. 前向传递概述
- en: When we talk about the *forward pass* in GPT-2, we mean the process of turning
    an input sentence (like “The cat sat on the”) into predictions for the next word.
    In simple terms, it’s how the model “thinks” before giving an answer. In `train_gpt2.c`,
    this happens inside the function `gpt2_forward`. Let’s walk through it slowly,
    step by step, so you can see how numbers flow through the model and transform
    along the way.
  id: totrans-1247
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论GPT-2中的*前向传递*时，我们指的是将输入句子（如“猫坐在”）转换为下一个单词预测的过程。简单来说，这是模型在给出答案之前“思考”的方式。在`train_gpt2.c`中，这发生在`gpt2_forward`函数内部。让我们一步一步地慢慢走，这样你可以看到数字是如何通过模型并在这个过程中转换的。
- en: 1\. From Words to Numbers
  id: totrans-1248
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1. 从单词到数字
- en: 'Computers don’t understand words like *cat* or *sat*. They only understand
    numbers. Before the forward pass starts, text is already tokenized into IDs (integers).
    For example:'
  id: totrans-1249
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机不理解像*猫*或*坐*这样的单词。它们只理解数字。在前向传递开始之前，文本已经被标记化为ID（整数）。例如：
- en: '[PRE62]'
  id: totrans-1250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Each number is a token ID. The model doesn’t yet know what “464” means in plain
    English-it just knows it’s a number that points into a table.
  id: totrans-1251
  prefs: []
  type: TYPE_NORMAL
  zh: 每个数都是一个标记ID。模型目前还不知道“464”在普通英语中的含义——它只知道它是一个指向表的数字。
- en: '2\. Embedding: Giving Words Meaning'
  id: totrans-1252
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2. 嵌入：赋予单词意义
- en: The first real step in the forward pass is embedding lookup. Imagine we have
    a huge dictionary, but instead of definitions in English, each word ID points
    to a long vector of numbers (say, 768 numbers for GPT-2 small).
  id: totrans-1253
  prefs: []
  type: TYPE_NORMAL
  zh: 前向传递的第一步是嵌入查找。想象我们有一个巨大的字典，但每个单词ID指向一个由数字组成的长向量（例如，GPT-2小型为768个数字）。
- en: 'Word embeddings (`wte`): Each token ID becomes a vector that captures the meaning
    of the word.'
  id: totrans-1254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词嵌入（`wte`）：每个标记ID变成一个向量，它捕捉单词的意义。
- en: 'Position embeddings (`wpe`): Each token also gets a vector for its position:
    first word, second word, third word, etc.'
  id: totrans-1255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置嵌入（`wpe`）：每个标记也为其位置获得一个向量：第一个单词、第二个单词、第三个单词等。
- en: The model adds these two vectors together. This way, it knows not just what
    the word is, but also where it is in the sentence.
  id: totrans-1256
  prefs: []
  type: TYPE_NORMAL
  zh: 模型将这两个向量相加。这样，它不仅知道单词是什么，还知道它在句子中的位置。
- en: 'For example:'
  id: totrans-1257
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: '| Token | Word | Word Embedding (shortened) | Position Embedding (shortened)
    | Combined Vector |'
  id: totrans-1258
  prefs: []
  type: TYPE_TB
  zh: '| 标记 | 词 | 词嵌入（缩短） | 位置嵌入（缩短） | 合成向量 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-1259
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 464 | “The” | [0.2, -0.5, 0.1, …] | [0.0, 0.1, -0.3, …] | [0.2, -0.4, -0.2,
    …] |'
  id: totrans-1260
  prefs: []
  type: TYPE_TB
  zh: '| 464 | “The” | [0.2, -0.5, 0.1, …] | [0.0, 0.1, -0.3, …] | [0.2, -0.4, -0.2,
    …] |'
- en: '| 3290 | “cat” | [0.9, -0.2, 0.4, …] | [0.1, -0.1, -0.2, …] | [1.0, -0.3, 0.2,
    …] |'
  id: totrans-1261
  prefs: []
  type: TYPE_TB
  zh: '| 3290 | “cat” | [0.9, -0.2, 0.4, …] | [0.1, -0.1, -0.2, …] | [1.0, -0.3, 0.2,
    …] |'
- en: Now every token is a vector with both meaning and position built in.
  id: totrans-1262
  prefs: []
  type: TYPE_NORMAL
  zh: 现在每一个标记都是一个内置了意义和位置的向量。
- en: '3\. Transformer Layers: The Thinking Steps'
  id: totrans-1263
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3. 变换层：思考步骤
- en: 'GPT-2 has multiple identical layers stacked on top of each other. Each layer
    has two big parts: attention and MLP (feed-forward network).'
  id: totrans-1264
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2 有多个相同的层堆叠在一起。每一层有两个主要部分：注意力和 MLP（前馈网络）。
- en: 'Attention (looking around):'
  id: totrans-1265
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力（环顾四周）：
- en: 'Each word asks: “Which other words should I pay attention to right now?”'
  id: totrans-1266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个词都会问：“我现在应该关注哪些其他词？”
- en: For “sat,” attention might focus heavily on “cat,” because those words are related.
  id: totrans-1267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于“sat”，注意力可能会高度集中在“cat”上，因为这两个词相关。
- en: The code computes *queries*, *keys*, and *values* for every word, then does
    dot-products, softmax, and weighted sums to mix information.
  id: totrans-1268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码为每个词计算 *queries*、*keys* 和 *values*，然后进行点积、softmax 和加权求和以混合信息。
- en: 'MLP (processing deeply):'
  id: totrans-1269
  prefs: []
  type: TYPE_NORMAL
  zh: MLP（深度处理）：
- en: After attention, each token passes through a mini neural network (two matrix
    multiplications with a nonlinear GELU function in between).
  id: totrans-1270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力之后，每个标记通过一个微型神经网络（两个矩阵乘法，中间有一个非线性 GELU 函数）。
- en: This helps each word refine its understanding, even if it doesn’t directly attend
    to another word.
  id: totrans-1271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这有助于每个词细化其理解，即使它没有直接关注另一个词。
- en: 'Both blocks have residual connections: the input is added back to the output,
    like keeping the original notes while adding new insights. This prevents information
    loss.'
  id: totrans-1272
  prefs: []
  type: TYPE_NORMAL
  zh: 两个块都有残差连接：输入被加回到输出中，就像在保留原有笔记的同时添加新的见解。这防止了信息丢失。
- en: '4\. Normalization: Keeping Numbers Stable'
  id: totrans-1273
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4. 归一化：保持数字稳定
- en: At many points, the model normalizes vectors so they don’t explode in size or
    shrink too small. This is called LayerNorm. It ensures training is stable, like
    making sure your cooking pot doesn’t boil over or dry out.
  id: totrans-1274
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多点上，模型会规范化向量，以防止它们尺寸爆炸或变得太小。这被称为 LayerNorm。它确保训练稳定，就像确保你的烹饪锅不会溢出或烧干一样。
- en: 5\. The Final Prediction Layer
  id: totrans-1275
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5. 最终预测层
- en: 'After all layers, the model produces a final vector for each position. Then:'
  id: totrans-1276
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有层之后，模型为每个位置生成一个最终向量。然后：
- en: It multiplies those vectors by the embedding table again (but transposed).
  id: totrans-1277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它将这些向量再次乘以嵌入表（但转置）。
- en: 'This gives logits: raw scores for each word in the vocabulary (about 50k options).'
  id: totrans-1278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这给出了 logits：词汇表中每个词的原始分数（大约 50k 个选项）。
- en: 'Example: for the last token “on the,” the logits might be:'
  id: totrans-1279
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：对于最后一个标记“on the”，logits可能如下：
- en: '| Word | Logit | Probability (after softmax) |'
  id: totrans-1280
  prefs: []
  type: TYPE_TB
  zh: '| 词 | Logit | 概率（softmax 后） |'
- en: '| --- | --- | --- |'
  id: totrans-1281
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| “mat” | 7.2 | 0.85 |'
  id: totrans-1282
  prefs: []
  type: TYPE_TB
  zh: '| “mat” | 7.2 | 0.85 |'
- en: '| “dog” | 5.1 | 0.10 |'
  id: totrans-1283
  prefs: []
  type: TYPE_TB
  zh: '| “dog” | 5.1 | 0.10 |'
- en: '| “car” | 3.0 | 0.05 |'
  id: totrans-1284
  prefs: []
  type: TYPE_TB
  zh: '| “car” | 3.0 | 0.05 |'
- en: The highest probability is “mat.”
  id: totrans-1285
  prefs: []
  type: TYPE_NORMAL
  zh: 最高概率的是“mat”。
- en: '6\. Softmax: Turning Scores into Probabilities'
  id: totrans-1286
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6. Softmax：将分数转换为概率
- en: 'The logits are big numbers, but they don’t mean much until we apply softmax.
    Softmax makes them into probabilities that sum to 1\. This way, we can interpret
    them as chances: “There’s an 85% chance the next word is *mat*.”'
  id: totrans-1287
  prefs: []
  type: TYPE_NORMAL
  zh: logits 是大数字，但它们没有多少意义，直到我们应用 softmax。softmax 将它们转换为概率，这些概率之和为 1。这样，我们可以将它们解释为机会：“下一个词是
    *mat* 的概率是 85%。”
- en: '7\. Cross-Entropy Loss: Measuring Mistakes'
  id: totrans-1288
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7. 交叉熵损失：衡量错误
- en: If we’re training, we also give the model the correct next word. The model checks
    how much probability it gave to that word. If it gave it high probability, the
    loss is low. If it gave it low probability, the loss is high.
  id: totrans-1289
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在训练，我们也会给模型正确的下一个词。模型检查它给了这个词多少概率。如果它给了高概率，损失就低。如果它给了低概率，损失就高。
- en: 'Correct: “mat” (probability 0.85 → loss ≈ 0.16, small).'
  id: totrans-1290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确： “mat”（概率 0.85 → 损失 ≈ 0.16，较小）。
- en: 'Wrong: “car” (probability 0.05 → loss ≈ 3.0, large).'
  id: totrans-1291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 错误： “car”（概率 0.05 → 损失 ≈ 3.0，较大）。
- en: This loss is averaged across all tokens, and it’s the signal that tells the
    backward pass how to update the model.
  id: totrans-1292
  prefs: []
  type: TYPE_NORMAL
  zh: 这个损失是所有标记的平均值，它是告诉反向传播如何更新模型的信号。
- en: 8\. Why It Matters
  id: totrans-1293
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8. 它为什么重要
- en: The forward pass is the part of GPT-2 that generates predictions. Without it,
    the model can’t “think” or make sense of input. It’s like the brain processing
    sensory input before deciding what to do. In `train_gpt2.c`, the forward pass
    is written with plain C loops, which makes the math crystal clear instead of hidden
    inside deep learning libraries.
  id: totrans-1294
  prefs: []
  type: TYPE_NORMAL
  zh: 前向传递是 GPT-2 生成预测的部分。没有它，模型就无法“思考”或理解输入。它就像大脑在决定做什么之前处理感官输入。在 `train_gpt2.c`
    中，前向传递是用纯 C 循环编写的，这使得数学运算清晰可见，而不是隐藏在深度学习库中。
- en: 9\. Try It Yourself
  id: totrans-1295
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9. 尝试自己操作
- en: 'Print embeddings: Modify the code to print the vector for the first token.
    See how it’s just numbers, but those numbers are the “meaning” of the word.'
  id: totrans-1296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印嵌入：修改代码以打印第一个标记的向量。看看它只是数字，但那些数字是“意义”。
- en: 'Inspect probabilities: After the forward pass, print the softmax probabilities
    for one position. They should sum to 1.0.'
  id: totrans-1297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查概率：在前向传递之后，打印一个位置的 softmax 概率。它们应该总和为 1.0。
- en: 'Change sequence length: Increase `T` from 64 to 128\. Notice how validation
    slows down, because attention compares all tokens with all others (`T²` scaling).'
  id: totrans-1298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 改变序列长度：将 `T` 从 64 增加到 128。注意验证速度会减慢，因为注意力机制需要比较所有标记与其他所有标记（`T²` 缩放）。
- en: 'Baseline loss: Before training, measure the loss. It should be around `log(vocab_size)`
    (≈10.8 for GPT-2 small). That’s the loss of random guessing.'
  id: totrans-1299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基准损失：在训练之前，测量损失。它应该大约是 `log(vocab_size)`（对于 GPT-2 小型模型约为 10.8）。这是随机猜测的损失。
- en: 'Mask experiment: Temporarily remove the causal mask in attention. The model
    will “cheat” by looking ahead, and loss will drop unrealistically.'
  id: totrans-1300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遮蔽实验：暂时移除注意力中的因果遮蔽。模型将通过向前看“作弊”，损失将不切实际地下降。
- en: The Takeaway
  id: totrans-1301
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸收要点
- en: The forward pass is like the thought process of GPT-2\. Input words become vectors,
    vectors mix through attention and MLPs, everything gets normalized, and finally
    the model produces probabilities for the next word. It’s a carefully choreographed
    dance of math operations, all coded in plain C loops in `train_gpt2.c`. Once you
    understand this flow, you can follow exactly how GPT-2 turns raw tokens into intelligent
    predictions.
  id: totrans-1302
  prefs: []
  type: TYPE_NORMAL
  zh: 前向传递就像 GPT-2 的思维过程。输入单词变成向量，向量通过注意力和 MLP 混合，一切都被归一化，最后模型为下一个单词产生概率。这是一场精心编排的数学运算舞蹈，所有代码都在
    `train_gpt2.c` 中的纯 C 循环中编写。一旦你理解了这个流程，你就可以精确地了解 GPT-2 如何将原始标记转换为智能预测。
- en: 32\. Token and Positional Embedding Lookup
  id: totrans-1303
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 32. 标记和位置嵌入查找
- en: Before GPT-2 can do anything intelligent with text, it needs to turn raw numbers
    (token IDs) into vectors that capture meaning and context. This is the role of
    embeddings. In `train_gpt2.c`, this step is handled by the function `encoder_forward`.
    Let’s take a closer look at how it works and why it matters.
  id: totrans-1304
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GPT-2 能够对文本进行任何智能操作之前，它需要将原始数字（标记 ID）转换为包含意义和上下文的向量。这是嵌入的作用。在 `train_gpt2.c`
    中，这一步由函数 `encoder_forward` 处理。让我们更详细地看看它是如何工作的以及为什么它很重要。
- en: Tokens Are Just Numbers
  id: totrans-1305
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 标记只是数字
- en: 'Suppose you type:'
  id: totrans-1306
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你输入：
- en: '[PRE63]'
  id: totrans-1307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'After tokenization, this sentence might look like:'
  id: totrans-1308
  prefs: []
  type: TYPE_NORMAL
  zh: 在标记化之后，这个句子可能看起来像：
- en: '[PRE64]'
  id: totrans-1309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: These are just IDs. The model doesn’t inherently know that `3290` means “cat.”
    It only knows it needs to use these numbers to fetch vectors from a table.
  id: totrans-1310
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是 ID。模型本身并不知道 `3290` 代表“猫”。它只知道它需要使用这些数字从表中获取向量。
- en: The Embedding Tables
  id: totrans-1311
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 嵌入表
- en: 'The model has two important tables stored in memory:'
  id: totrans-1312
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在内存中存储了两个重要的表：
- en: Word Token Embeddings (`wte`)
  id: totrans-1313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 词标记嵌入 (`wte`)
- en: 'Size: `(V, C)` where `V` is vocab size (~50,000 for GPT-2 small) and `C` is
    channels (768).'
  id: totrans-1314
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大小：`(V, C)` 其中 `V` 是词汇量（对于 GPT-2 小型模型约为 50,000）和 `C` 是通道（768）。
- en: Each row corresponds to a token ID.
  id: totrans-1315
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每一行对应一个标记 ID。
- en: 'Example: row 3290 might be `[0.12, -0.45, 0.88, …]`.'
  id: totrans-1316
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例：行 3290 可能是 `[0.12, -0.45, 0.88, …]`。
- en: Positional Embeddings (`wpe`)
  id: totrans-1317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 位置嵌入 (`wpe`)
- en: 'Size: `(maxT, C)` where `maxT` is the maximum sequence length (e.g. 1024).'
  id: totrans-1318
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大小：`(maxT, C)` 其中 `maxT` 是最大序列长度（例如 1024）。
- en: 'Each row corresponds to a position index: 0 for the first token, 1 for the
    second, etc.'
  id: totrans-1319
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每一行对应一个位置索引：0 对应第一个标记，1 对应第二个，等等。
- en: 'Example: position 2 might be `[0.07, 0.31, -0.22, …]`.'
  id: totrans-1320
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例：位置 2 可能是 `[0.07, 0.31, -0.22, …]`。
- en: Both tables are filled with trainable values. At the start, they’re random.
    As training progresses, the optimizer updates them so they encode useful patterns.
  id: totrans-1321
  prefs: []
  type: TYPE_NORMAL
  zh: 两个表都填充了可训练的值。一开始，它们是随机的。随着训练的进行，优化器会更新它们，以便它们能够编码有用的模式。
- en: Adding Them Together
  id: totrans-1322
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将它们相加
- en: 'For each token at position `t`:'
  id: totrans-1323
  prefs: []
  type: TYPE_NORMAL
  zh: 对于位置 `t` 的每个标记：
- en: Look up its word vector from `wte`.
  id: totrans-1324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 `wte` 中查找其词向量。
- en: Look up its position vector from `wpe`.
  id: totrans-1325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 `wpe` 中查找其位置向量。
- en: Add them elementwise.
  id: totrans-1326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将它们逐元素相加。
- en: This gives a final vector of size `C` that represents what the token is and
    where it is.
  id: totrans-1327
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了一个大小为 `C` 的最终向量，它代表了标记是什么以及它在哪。
- en: 'Example with simplified numbers:'
  id: totrans-1328
  prefs: []
  type: TYPE_NORMAL
  zh: 使用简化数字的示例：
- en: '| Token ID | Word | Word Embedding | Position | Combined |'
  id: totrans-1329
  prefs: []
  type: TYPE_TB
  zh: '| 标记ID | 单词 | 单词嵌入 | 位置 | 合并 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-1330
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 464 | The | [0.1, -0.2, 0.3] | [0.2, 0.0, -0.1] | [0.3, -0.2, 0.2] |'
  id: totrans-1331
  prefs: []
  type: TYPE_TB
  zh: '| 464 | The | [0.1, -0.2, 0.3] | [0.2, 0.0, -0.1] | [0.3, -0.2, 0.2] |'
- en: '| 3290 | cat | [0.4, 0.5, -0.3] | [0.0, 0.1, 0.2] | [0.4, 0.6, -0.1] |'
  id: totrans-1332
  prefs: []
  type: TYPE_TB
  zh: '| 3290 | cat | [0.4, 0.5, -0.3] | [0.0, 0.1, 0.2] | [0.4, 0.6, -0.1] |'
- en: Now the vector doesn’t just mean “cat,” it means “cat at position 1.”
  id: totrans-1333
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这个向量不仅仅意味着“猫”，它意味着“位置1的猫”。
- en: Why Position Matters
  id: totrans-1334
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么位置很重要
- en: 'Without positions, the model would treat:'
  id: totrans-1335
  prefs: []
  type: TYPE_NORMAL
  zh: 没有位置，模型将处理：
- en: “The cat sat”
  id: totrans-1336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “The cat sat”
- en: “Sat cat the”
  id: totrans-1337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “Sat cat the”
- en: as identical, because they use the same tokens. But word order is essential
    in language. By adding positional embeddings, GPT-2 knows the difference between
    “dog bites man” and “man bites dog.”
  id: totrans-1338
  prefs: []
  type: TYPE_NORMAL
  zh: 因为它们使用相同的标记，所以它们看起来是相同的。但在语言中，单词顺序至关重要。通过添加位置嵌入，GPT-2能够区分“dog bites man”和“man
    bites dog”。
- en: Inside the Code
  id: totrans-1339
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 代码内部
- en: 'The embedding lookup is written explicitly with loops in C:'
  id: totrans-1340
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入查找是用C语言中的循环明确编写的：
- en: '[PRE65]'
  id: totrans-1341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'What’s happening here:'
  id: totrans-1342
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生了什么：
- en: Loop over batches (`b`) and sequence positions (`t`).
  id: totrans-1343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 遍历批次（`b`）和序列位置（`t`）。
- en: Find the token ID `ix`.
  id: totrans-1344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到标记ID `ix`。
- en: Fetch its embedding `wte_ix`.
  id: totrans-1345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取其嵌入 `wte_ix`。
- en: Fetch its position embedding `wpe_t`.
  id: totrans-1346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取其位置嵌入 `wpe_t`。
- en: Add them element by element.
  id: totrans-1347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逐元素相加。
- en: The result, `out_bt`, is the vector for this token at this position.
  id: totrans-1348
  prefs: []
  type: TYPE_NORMAL
  zh: 结果 `out_bt` 是这个位置这个标记的向量。
- en: Analogy
  id: totrans-1349
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 类比
- en: 'Think of it like name tags at a conference:'
  id: totrans-1350
  prefs: []
  type: TYPE_NORMAL
  zh: 想象它就像会议上的名牌：
- en: 'The word embedding is your name: “Alice.”'
  id: totrans-1351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词嵌入是你的名字：“Alice。”
- en: 'The position embedding is your table number: “Table 7.”'
  id: totrans-1352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置嵌入是你的桌子号：“桌子7。”
- en: Together, they tell the conference staff who you are and where you are seated.
  id: totrans-1353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一起，它们告诉会议工作人员你是谁以及你坐在哪里。
- en: Without the table number, they might know who you are but not where to find
    you. Without your name, they just know there’s someone at Table 7 but not who.
    Both are needed for proper context.
  id: totrans-1354
  prefs: []
  type: TYPE_NORMAL
  zh: 没有表格号，他们可能知道你是谁，但不知道在哪里找到你。没有你的名字，他们只知道桌子7号有一个人，但不知道是谁。两者都是正确上下文所必需的。
- en: Why It Matters
  id: totrans-1355
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: Embeddings are the foundation of the whole model. If this step is wrong, everything
    else collapses. They transform meaningless IDs into rich vectors that carry semantic
    and positional information. This is the entry point where language starts becoming
    something a neural network can reason about.
  id: totrans-1356
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入是整个模型的基础。如果这一步出错，其他所有步骤都会崩溃。它们将无意义的ID转换成携带语义和位置信息的丰富向量。这是语言开始变得神经网络可以推理的地方。
- en: Try It Yourself
  id: totrans-1357
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 尝试自己操作
- en: 'Print a token embedding: Modify the code to print out `wte_ix` for a specific
    token ID like “cat.” You’ll see a vector of floats, the learned representation.'
  id: totrans-1358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印标记嵌入：修改代码以打印特定标记ID（如“猫”）的 `wte_ix`。你会看到一个浮点向量，这是学习到的表示。
- en: 'Print a position embedding: Do the same for `wpe_t` at position 0, 1, 2… Notice
    how positions have unique but consistent patterns.'
  id: totrans-1359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印位置嵌入：对位置0、1、2等处的 `wpe_t` 做同样的操作，注意位置有独特但一致的规律。
- en: 'Check the sum: Verify that `out_bt[i] = wte_ix[i] + wpe_t[i]`. This is literally
    how word and position are fused.'
  id: totrans-1360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查总和：验证 `out_bt[i] = wte_ix[i] + wpe_t[i]`。这正是单词和位置融合的方式。
- en: 'Shuffle words: Try feeding “cat sat” vs. “sat cat.” The embeddings will differ
    because the position vectors change, even though the words are the same.'
  id: totrans-1361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 混洗单词：尝试输入“cat sat”与“sat cat。” 嵌入将不同，因为位置向量改变了，尽管单词是相同的。
- en: 'Observe growth during training: After some training steps, dump the embeddings
    again. You’ll notice they stop being random and start showing structure.'
  id: totrans-1362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察训练过程中的增长：经过一些训练步骤后，再次导出嵌入。你会注意到它们不再随机，而是开始显示结构。
- en: The Takeaway
  id: totrans-1363
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸取的经验
- en: The embedding lookup is the very first step of the forward pass. It takes raw
    numbers and makes them meaningful by combining token identity and position. This
    prepares the input for the deeper transformer layers. Even though the C code looks
    simple-a few nested loops-it’s doing the crucial work of giving words a mathematical
    shape the model can understand.
  id: totrans-1364
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入查找是前向传递的第一步。它通过结合标记身份和位置将原始数字变得有意义，为更深的transformer层准备输入。尽管C代码看起来简单——几个嵌套循环——但它正在做给单词赋予模型可以理解的数学形状的关键工作。
- en: '33\. Attention: Matmuls, Masking, and Softmax on CPU'
  id: totrans-1365
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 33. 注意：CPU上的Matmuls、掩码和Softmax
- en: The attention mechanism is the heart of GPT-2\. It’s where each word in the
    input sequence decides which other words to look at when forming its representation.
    In `train_gpt2.c`, this happens inside the `attention_forward` function, which
    implements multi-head self-attention using plain C loops and matrix multiplications.
    Let’s break it down carefully, step by step, so even an absolute beginner can
    follow the flow.
  id: totrans-1366
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制是 GPT-2 的核心。它决定了输入序列中的每个单词在形成其表示时，要查看哪些其他单词。在 `train_gpt2.c` 中，这发生在 `attention_forward`
    函数内部，该函数使用纯 C 循环和矩阵乘法实现多头自注意力。让我们仔细分解，一步一步，以便即使是完全的初学者也能理解流程。
- en: The Big Idea of Attention
  id: totrans-1367
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意力的核心思想
- en: 'Imagine you’re reading:'
  id: totrans-1368
  prefs: []
  type: TYPE_NORMAL
  zh: 想象你正在阅读：
- en: “The cat sat on the mat.”
  id: totrans-1369
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “The cat sat on the mat.”
- en: When the model is trying to understand the word *sat*, it doesn’t just look
    at *sat* by itself. It wants to consider other words like *cat* (the subject)
    and *mat* (likely the object). Attention gives each token a way to “consult” earlier
    tokens and decide how important they are.
  id: totrans-1370
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型试图理解单词 *sat* 时，它不仅仅关注 *sat* 本身。它还想要考虑其他单词，如 *cat*（主题）和 *mat*（可能是宾语）。注意力机制为每个标记提供了一种“咨询”更早标记并决定它们重要性的方式。
- en: 'This is done mathematically by projecting each token into three roles: Query
    (Q), Key (K), and Value (V).'
  id: totrans-1371
  prefs: []
  type: TYPE_NORMAL
  zh: 这通过将每个标记投影到三个角色：查询（Q）、键（K）和值（V）来数学上完成。
- en: 'Query (Q): “What am I looking for?”'
  id: totrans-1372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询（Q）：“我在寻找什么？”
- en: 'Key (K): “What do I offer?”'
  id: totrans-1373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 键（K）：“我提供什么？”
- en: 'Value (V): “What information do I carry?”'
  id: totrans-1374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值（V）：“我携带什么信息？”
- en: 'Step 1: Creating Q, K, and V'
  id: totrans-1375
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第 1 步：创建 Q、K 和 V
- en: For every input vector of size `C` (e.g., 768), the code performs three separate
    linear projections (matrix multiplications). These produce Q, K, and V vectors
    of smaller size, divided among attention heads.
  id: totrans-1376
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个大小为 `C`（例如，768）的输入向量，代码执行三个独立的线性投影（矩阵乘法）。这些产生 Q、K 和 V 向量，其尺寸较小，分配给注意力头。
- en: 'In the code:'
  id: totrans-1377
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中：
- en: '[PRE66]'
  id: totrans-1378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Here:'
  id: totrans-1379
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里：
- en: '`acts.ln1` is the normalized input from the previous step.'
  id: totrans-1380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`acts.ln1` 是前一步的归一化输入。'
- en: '`params.wq`, `params.wk`, `params.wv` are the weight matrices.'
  id: totrans-1381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`params.wq`、`params.wk`、`params.wv` 是权重矩阵。'
- en: The output shapes are `(B, T, C)`.
  id: totrans-1382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出形状是 `(B, T, C)`。
- en: 'So each token now has three new representations: Q, K, and V.'
  id: totrans-1383
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，每个标记现在有三个新的表示：Q、K 和 V。
- en: 'Step 2: Computing Attention Scores'
  id: totrans-1384
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第 2 步：计算注意力分数
- en: For each token at position `t`, we want to know how much it should pay attention
    to every earlier token (including itself). This is done with a dot product between
    its Query and all Keys.
  id: totrans-1385
  prefs: []
  type: TYPE_NORMAL
  zh: 对于位置 `t` 的每个标记，我们想知道它应该对每个更早的标记（包括它自己）关注多少。这是通过其查询与所有键之间的点积来完成的。
- en: 'Mathematically:'
  id: totrans-1386
  prefs: []
  type: TYPE_NORMAL
  zh: 数学上：
- en: '[PRE67]'
  id: totrans-1387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '`t` = current token.'
  id: totrans-1388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`t` = 当前标记。'
- en: '`u` = another token at or before `t`.'
  id: totrans-1389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`u` = 位置 `t` 或之前的另一个标记。'
- en: '`sqrt(dk)` is a scaling factor (dk = size of each head) to keep values stable.'
  id: totrans-1390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sqrt(dk)` 是一个缩放因子（dk = 每个头的尺寸），以保持值稳定。'
- en: In the code, these dot products are done explicitly in loops.
  id: totrans-1391
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，这些点积在循环中显式完成。
- en: 'Step 3: Applying the Causal Mask'
  id: totrans-1392
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第 3 步：应用因果掩码
- en: 'GPT-2 is an autoregressive model, meaning it only predicts the future from
    the past, not the other way around. To enforce this, the attention matrix is masked:'
  id: totrans-1393
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2 是一个自回归模型，这意味着它只能从过去预测未来，而不是反过来。为了强制执行这一点，注意力矩阵被掩码：
- en: Token at position `t` can only look at positions `≤ t`.
  id: totrans-1394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置 `t` 的标记只能查看位置 `≤ t`。
- en: Anything beyond `t` is set to a very negative value (`-1e9`), which becomes
    effectively zero after softmax.
  id: totrans-1395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何超过 `t` 的内容都被设置为非常负的值（`-1e9`），在 softmax 后变为实际上为零。
- en: This ensures, for example, that when predicting the 3rd word, the model doesn’t
    cheat by looking at the 4th.
  id: totrans-1396
  prefs: []
  type: TYPE_NORMAL
  zh: 这确保了例如，当预测第 3 个单词时，模型不会通过查看第 4 个单词来作弊。
- en: 'Step 4: Turning Scores into Probabilities'
  id: totrans-1397
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第 4 步：将分数转换为概率
- en: 'The scores are raw numbers that can be large and unstable. To convert them
    into meaningful weights, the code applies softmax:'
  id: totrans-1398
  prefs: []
  type: TYPE_NORMAL
  zh: 分数是原始数字，可能很大且不稳定。为了将它们转换为有意义的权重，代码应用了 softmax：
- en: '[PRE68]'
  id: totrans-1399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: This makes all weights positive and ensures they sum to 1\. Now each token has
    a probability distribution over earlier tokens.
  id: totrans-1400
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得所有权重都为正，并确保它们的总和为 1。现在每个标记都有一个关于更早标记的概率分布。
- en: 'Example for the word *sat*:'
  id: totrans-1401
  prefs: []
  type: TYPE_NORMAL
  zh: '*sat* 这个单词的例子：'
- en: '| Attended Token | Raw Score | After Softmax |'
  id: totrans-1402
  prefs: []
  type: TYPE_TB
  zh: '| 关注的标记 | 原始分数 | 经过 Softmax |'
- en: '| --- | --- | --- |'
  id: totrans-1403
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| The | 1.2 | 0.10 |'
  id: totrans-1404
  prefs: []
  type: TYPE_TB
  zh: '| The | 1.2 | 0.10 |'
- en: '| cat | 3.4 | 0.80 |'
  id: totrans-1405
  prefs: []
  type: TYPE_TB
  zh: '| cat | 3.4 | 0.80 |'
- en: '| sat (itself) | 0.7 | 0.10 |'
  id: totrans-1406
  prefs: []
  type: TYPE_TB
  zh: '| sat (自身) | 0.7 | 0.10 |'
- en: '| on | masked | 0.00 |'
  id: totrans-1407
  prefs: []
  type: TYPE_TB
  zh: '| on | 遮蔽 | 0.00 |'
- en: Clearly, *sat* focuses most strongly on *cat*.
  id: totrans-1408
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，*sat* 最强烈地关注 *cat*。
- en: 'Step 5: Weighted Sum of Values'
  id: totrans-1409
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第 5 步：值的加权和
- en: 'Once the attention weights are computed, the model uses them to take a weighted
    sum of the Value vectors:'
  id: totrans-1410
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦计算了注意力权重，模型就使用它们对值向量取加权求和：
- en: '[PRE69]'
  id: totrans-1411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: This produces a new representation for each token that blends in information
    from others.
  id: totrans-1412
  prefs: []
  type: TYPE_NORMAL
  zh: 这为每个标记产生一个新的表示，其中融合了来自其他标记的信息。
- en: For *sat*, its new vector will be mostly influenced by *cat*, but also a little
    by *The* and itself.
  id: totrans-1413
  prefs: []
  type: TYPE_NORMAL
  zh: 对于*sat*，其新向量将主要受*cat*影响，但也受*The*和它自己的一小部分影响。
- en: 'Step 6: Multi-Head Attention'
  id: totrans-1414
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第6步：多头注意力
- en: In practice, attention is split into multiple heads (12 for GPT-2 small). Each
    head works on smaller chunks of the vector (C/heads).
  id: totrans-1415
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，注意力被分成多个头部（GPT-2小的12个）。每个头部处理向量的小块（C/heads）。
- en: Head 1 might focus on subject–verb relationships.
  id: totrans-1416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第1个头部可能专注于主谓关系。
- en: Head 2 might track distances (like “how far back was this token?”).
  id: totrans-1417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第2个头部可能跟踪距离（例如“这个标记有多远？”）。
- en: Head 3 might specialize in punctuation.
  id: totrans-1418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第3个头部可能专门处理标点符号。
- en: After all heads compute their outputs, the results are concatenated and projected
    back into size `C` with another matrix multiply.
  id: totrans-1419
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有头部计算完它们的输出后，结果通过另一个矩阵乘法连接并投影回大小`C`。
- en: 'Step 7: Residual Connection'
  id: totrans-1420
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第7步：残差连接
- en: Finally, the output of the attention block is added back to the original input
    (residual connection). This keeps the original signal flowing, even if the attention
    introduces distortions.
  id: totrans-1421
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将注意力块的输出加回到原始输入（残差连接）上。这保持了原始信号流动，即使注意力引入了扭曲。
- en: '[PRE70]'
  id: totrans-1422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: This ensures information isn’t lost and gradients flow smoothly during training.
  id: totrans-1423
  prefs: []
  type: TYPE_NORMAL
  zh: 这确保了信息不会丢失，并且在训练过程中梯度流动顺畅。
- en: Why It Matters
  id: totrans-1424
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: Attention is the mechanism that lets GPT-2 capture relationships between words.
    Without it, the model would treat each token independently, losing context. By
    explicitly computing “who should I look at?” for every token, GPT-2 learns patterns
    like subject–verb agreement, long-distance dependencies, and even stylistic nuances.
  id: totrans-1425
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力是让GPT-2捕捉词语之间关系的机制。没有它，模型将独立处理每个标记，丢失上下文。通过为每个标记显式计算“我应该看谁？”，GPT-2学习到主谓一致、长距离依赖甚至风格细微差别等模式。
- en: Try It Yourself
  id: totrans-1426
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 试试你自己
- en: 'Inspect the attention mask: Print out the scores before and after masking.
    Notice how future tokens are set to huge negative values.'
  id: totrans-1427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查注意力掩码：打印出掩码前后的分数。注意未来标记被设置为巨大的负值。
- en: 'Visualize weights: Run attention on a short sentence and plot the weights.
    You’ll see which words attend to which.'
  id: totrans-1428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化权重：在简短的句子上运行注意力并绘制权重。你会看到哪些词关注哪些词。
- en: 'Change sequence length: Try increasing `T` and observe how computation grows
    quadratically (`T²`). Attention is expensive!'
  id: totrans-1429
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 改变序列长度：尝试增加`T`并观察计算如何呈二次方增长（`T²`）。注意力是昂贵的！
- en: 'Experiment with heads: Force the model to use only 1 head instead of 12\. See
    how this limits the diversity of patterns it can capture.'
  id: totrans-1430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试使用头部：强制模型只使用1个头部而不是12个。看看这如何限制它所能捕捉到的模式多样性。
- en: 'Check sum of weights: For one token, verify that all attention weights add
    up to 1.0 after softmax.'
  id: totrans-1431
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查权重总和：对于单个标记，验证所有注意力权重在softmax后加起来等于1.0。
- en: The Takeaway
  id: totrans-1432
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸取的经验
- en: Attention is what makes transformers powerful. It allows each word to dynamically
    decide which other words matter for understanding its role in a sentence. In `train_gpt2.c`,
    this process is spelled out with explicit loops and matrix multiplications, so
    you can follow every step of the math. Understanding this section gives you the
    key to why GPT-2-and all modern LLMs-work so well.
  id: totrans-1433
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力是使transformers强大的原因。它允许每个词动态决定哪些其他词对其在句子中的作用很重要。在`train_gpt2.c`中，这个过程通过显式的循环和矩阵乘法来表述，因此你可以跟随数学的每一步。理解这一部分给你提供了理解为什么GPT-2以及所有现代LLM工作得如此之好的关键。
- en: '34\. MLP: GEMMs and Activation Functions'
  id: totrans-1434
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 34. MLP：GEMMs和激活函数
- en: After the attention block lets tokens “talk to each other,” GPT-2 applies a
    second kind of transformation called the MLP block (multi-layer perceptron). Unlike
    attention, which mixes information between tokens, the MLP processes each token
    independently, enriching its internal representation. Even though it looks simpler
    than attention, the MLP is essential for capturing complex relationships in language.
  id: totrans-1435
  prefs: []
  type: TYPE_NORMAL
  zh: 在注意力块让标记“互相交谈”之后，GPT-2应用第二种称为MLP块（多层感知器）的转换。与混合标记之间信息的注意力不同，MLP独立处理每个标记，丰富其内部表示。尽管它看起来比注意力简单，但MLP对于捕捉语言中的复杂关系至关重要。
- en: What the MLP Does
  id: totrans-1436
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: MLP的功能
- en: 'Every token’s vector (size `C`, e.g., 768 for GPT-2 small) goes through:'
  id: totrans-1437
  prefs: []
  type: TYPE_NORMAL
  zh: 每个标记的向量（大小`C`，例如GPT-2小的768）都会经过：
- en: 'Linear expansion: project from size `C` to size `4C` (3072 in GPT-2 small).'
  id: totrans-1438
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 线性扩展：从大小 `C` 投影到大小 `4C`（GPT-2 小型中的 3072）。
- en: 'Nonlinear activation: apply the GELU function, which adds flexibility.'
  id: totrans-1439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 非线性激活：应用 GELU 函数，增加了灵活性。
- en: 'Linear projection back: reduce size from `4C` back to `C`.'
  id: totrans-1440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 线性投影回：将大小从 `4C` 减少到 `C`。
- en: 'Residual connection: add the input vector back to the output, keeping the original
    signal intact.'
  id: totrans-1441
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 残差连接：将输入向量加回到输出中，保持原始信号完整。
- en: This allows the model to not only share information between tokens (via attention)
    but also refine how each token represents itself.
  id: totrans-1442
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得模型不仅可以在标记之间共享信息（通过注意力），还可以细化每个标记如何代表自己。
- en: 'Step 1: Expanding with a Matrix Multiply'
  id: totrans-1443
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第 1 步：使用矩阵乘法扩展
- en: 'The first step is to expand each token’s vector from 768 to 3072 dimensions.
    This is done with a general matrix multiply (GEMM):'
  id: totrans-1444
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是将每个标记的向量从 768 维扩展到 3072 维。这是通过一般矩阵乘法（GEMM）完成的：
- en: '[PRE71]'
  id: totrans-1445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '`acts.ln2`: the normalized input from the previous residual.'
  id: totrans-1446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`acts.ln2`：来自先前残差的归一化输入。'
- en: '`params.wfc`: the weight matrix of size `(C, 4C)`.'
  id: totrans-1447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`params.wfc`：大小为 `(C, 4C)` 的权重矩阵。'
- en: '`params.bfc`: bias vector of size `(4C)`.'
  id: totrans-1448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`params.bfc`：大小为 `(4C)` 的偏置向量。'
- en: '`acts.mlp_in`: the result, shape `(B, T, 4C)`.'
  id: totrans-1449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`acts.mlp_in`：结果，形状 `(B, T, 4C)`。'
- en: Think of it like stretching a rubber band-suddenly, the token has much more
    room to express richer features.
  id: totrans-1450
  prefs: []
  type: TYPE_NORMAL
  zh: 想象它就像拉伸橡皮筋——突然，标记有更多的空间来表达更丰富的特征。
- en: 'Step 2: GELU Activation'
  id: totrans-1451
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第 2 步：GELU 激活
- en: After expansion, each number passes through GELU (Gaussian Error Linear Unit).
  id: totrans-1452
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展后，每个数字都通过 GELU（高斯误差线性单元）。
- en: 'The formula:'
  id: totrans-1453
  prefs: []
  type: TYPE_NORMAL
  zh: 公式：
- en: '[PRE72]'
  id: totrans-1454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'This looks complicated, but the key idea is:'
  id: totrans-1455
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来很复杂，但关键思想是：
- en: For small negative numbers, output ≈ 0 (ignore weak signals).
  id: totrans-1456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于小的负数，输出 ≈ 0（忽略弱信号）。
- en: For large positive numbers, output ≈ x (pass strong signals).
  id: totrans-1457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于大的正数，输出 ≈ x（传递强信号）。
- en: For numbers in between, it smoothly blends.
  id: totrans-1458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于中间的数字，它平滑地混合。
- en: Unlike ReLU, which just chops off negatives, GELU lets small signals through
    in a probabilistic way. This makes it better for language, where even small hints
    matter.
  id: totrans-1459
  prefs: []
  type: TYPE_NORMAL
  zh: 与 ReLU 不同，ReLU 只切断负数，GELU 以概率方式让小信号通过。这使得它在语言处理中更好，因为即使是微小的提示也很重要。
- en: 'Analogy: Imagine you’re grading homework. If an answer is completely wrong,
    you give 0 points (ReLU style). If it’s perfect, you give full credit. But if
    it’s partially right, you give partial credit. GELU behaves like that-soft, nuanced
    grading.'
  id: totrans-1460
  prefs: []
  type: TYPE_NORMAL
  zh: 类比：想象你在批改作业。如果一个答案完全错误，你给 0 分（ReLU 风格）。如果它完美无缺，你给满分。但如果它部分正确，你给部分分数。GELU 就像那样——软性、细微的评分。
- en: 'Step 3: Projecting Back Down'
  id: totrans-1461
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第 3 步：向下投影
- en: 'Once the token vector has been expanded and passed through GELU, it’s projected
    back to the original size `C`:'
  id: totrans-1462
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦标记向量被扩展并通过 GELU，它就被投影回原始大小 `C`：
- en: '[PRE73]'
  id: totrans-1463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '`params.wproj`: the projection weights, size `(4C, C)`.'
  id: totrans-1464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`params.wproj`：投影权重，大小 `(4C, C)`。'
- en: '`params.bproj`: bias, size `(C)`.'
  id: totrans-1465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`params.bproj`：偏置，大小 `(C)`。'
- en: '`acts.mlp_out`: result, shape `(B, T, C)`.'
  id: totrans-1466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`acts.mlp_out`：结果，形状 `(B, T, C)`。'
- en: Now each token has gone through a non-linear “thinking step,” mixing and reshaping
    features.
  id: totrans-1467
  prefs: []
  type: TYPE_NORMAL
  zh: 现在每一个标记都已经经历了一个非线性的“思考步骤”，混合和重塑特征。
- en: 'Step 4: Residual Connection'
  id: totrans-1468
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第 4 步：残差连接
- en: 'Just like with attention, the MLP output is added back to the input:'
  id: totrans-1469
  prefs: []
  type: TYPE_NORMAL
  zh: 就像注意力一样，MLP 输出被加回到输入中：
- en: '[PRE74]'
  id: totrans-1470
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: This means the token keeps its old representation while adding the new refinements.
    If the MLP makes a mistake early in training, the residual ensures the token doesn’t
    lose all its meaning.
  id: totrans-1471
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着标记在添加新的改进的同时保留了其旧的表达方式。如果在训练早期 MLP 出现错误，残差确保标记不会失去所有意义。
- en: 'Inside the Code: Simplicity'
  id: totrans-1472
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 代码内部：简洁性
- en: 'Even though MLPs in deep learning libraries like PyTorch are one-liners (`nn.Linear`
    + `nn.GELU` + `nn.Linear`), here in C you see every step spelled out:'
  id: totrans-1473
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在深度学习库（如 PyTorch）中的 MLP 是一行代码（`nn.Linear` + `nn.GELU` + `nn.Linear`），但在 C
    中你可以看到每个步骤都明确写出：
- en: First GEMM expands to 4C.
  id: totrans-1474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一次 GEMM 扩展到 4C。
- en: Loop applies GELU element by element.
  id: totrans-1475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 循环逐元素应用 GELU。
- en: Second GEMM projects back to C.
  id: totrans-1476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二次 GEMM 将其投影回 C。
- en: Residual adds input and output.
  id: totrans-1477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 残差将输入和输出相加。
- en: It’s like watching a magician reveal the trick instead of just seeing the final
    illusion.
  id: totrans-1478
  prefs: []
  type: TYPE_NORMAL
  zh: 这就像观看魔术师揭示魔术技巧而不是只看到最终的幻象。
- en: Why the Expansion Matters
  id: totrans-1479
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么扩展很重要
- en: 'You might ask: why expand to 4C and then shrink back? Why not just keep the
    size the same?'
  id: totrans-1480
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问：为什么扩展到 4C 然后再缩小？为什么不保持大小不变？
- en: The expansion allows the model to capture more complicated combinations of features.
    By spreading information out, applying a nonlinear transformation, and then compressing
    it again, the model can discover patterns that wouldn’t fit in the smaller space.
  id: totrans-1481
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展允许模型捕捉更复杂的特征组合。通过分散信息，应用非线性变换，然后再压缩，模型可以发现不适合较小空间的模式。
- en: Think of it like brainstorming on a huge whiteboard. You spread out all your
    ideas (4C), reorganize them, and then condense the best ones into a neat summary
    (C).
  id: totrans-1482
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下在巨大的白板上头脑风暴。你将所有想法（4C）展开，重新组织它们，然后将最好的想法浓缩成一个整洁的总结（C）。
- en: Example Walkthrough
  id: totrans-1483
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例演示
- en: Let’s say we’re processing the token “cat” in the sentence *The cat sat*.
  id: totrans-1484
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们在处理句子“The cat sat”中的标记“cat”。
- en: 'Input vector (size 768): `[0.12, -0.08, 0.33, …]`'
  id: totrans-1485
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入向量（大小 768）： `[0.12, -0.08, 0.33, …]`
- en: 'After first matrix multiply: expanded to `[1.2, -0.9, 0.5, …]` (size 3072).'
  id: totrans-1486
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一次矩阵乘法后：扩展到 `[1.2, -0.9, 0.5, …]`（大小 3072）。
- en: 'After GELU: `[1.1, -0.0, 0.4, …]` (smooth nonlinearity).'
  id: totrans-1487
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GELU 之后： `[1.1, -0.0, 0.4, …]`（平滑非线性）。
- en: 'After projection: back to `[0.15, -0.02, 0.27, …]` (size 768).'
  id: totrans-1488
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 投影后：回到 `[0.15, -0.02, 0.27, …]`（大小 768）。
- en: 'Add back original input: `[0.27, -0.10, 0.60, …]`.'
  id: totrans-1489
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加回原始输入： `[0.27, -0.10, 0.60, …]`。
- en: Now “cat” has been enriched with new internal features that help the model predict
    what comes next.
  id: totrans-1490
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，“cat”已经通过新的内部特征得到了丰富，这些特征有助于模型预测接下来会发生什么。
- en: Why It Matters
  id: totrans-1491
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: The MLP is the part of GPT-2 that lets each token refine itself. Attention gives
    context from neighbors, but the MLP deepens the representation of the token itself.
    Without it, the model would lack the ability to detect fine-grained patterns.
  id: totrans-1492
  prefs: []
  type: TYPE_NORMAL
  zh: MLP 是 GPT-2 中允许每个标记自我精炼的部分。注意力从邻居那里提供上下文，但 MLP 深化标记自身的表示。没有它，模型将缺乏检测细粒度模式的能力。
- en: Try It Yourself
  id: totrans-1493
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 尝试自己操作
- en: 'Print intermediate sizes: Add debug prints to see how token vectors grow to
    4C and shrink back to C.'
  id: totrans-1494
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印中间大小：添加调试打印以查看标记向量如何增长到 4C 并缩小回 C。
- en: 'Swap activation: Replace GELU with ReLU in the code and train. Compare losses-you’ll
    notice GPT-2 prefers GELU.'
  id: totrans-1495
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 交换激活：在代码中将 GELU 替换为 ReLU 并进行训练。比较损失，你会注意到 GPT-2 更偏好 GELU。
- en: 'Disable residual: Temporarily remove the residual add. Watch how the model
    struggles to learn, because it can’t preserve information.'
  id: totrans-1496
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 禁用残差：暂时移除残差添加。观察模型如何努力学习，因为它无法保留信息。
- en: 'Visualize values: Track how many values are near 0 before and after GELU. You’ll
    see GELU softly zeroes out weak signals.'
  id: totrans-1497
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化值：追踪在 GELU 之前和之后接近 0 的值的数量。你会看到 GELU 软化地消除了弱信号。
- en: 'Smaller expansion: Try changing 4C to 2C in the code. You’ll save memory but
    lose accuracy, since the MLP has less expressive power.'
  id: totrans-1498
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更小的扩展：尝试在代码中将 4C 改为 2C。你会节省内存，但会失去精度，因为 MLP 的表达能力更弱。
- en: The Takeaway
  id: totrans-1499
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸取的教训
- en: The MLP block is a token’s personal deep thinker. It stretches the representation
    wide, filters it through GELU, compresses it again, and then adds it back to the
    original. While attention handles the conversation between words, the MLP ensures
    each word processes and refines its own role. Together, they create the layered
    reasoning ability that makes GPT-2 so powerful.
  id: totrans-1500
  prefs: []
  type: TYPE_NORMAL
  zh: MLP 块是标记的个人深度思考者。它将表示拉伸得很宽，通过 GELU 过滤，再次压缩，然后添加回原始值。当注意力处理单词之间的对话时，MLP 确保每个单词处理和细化其自身的角色。共同，它们创造了使
    GPT-2 非常强大的分层推理能力。
- en: 35\. LayerNorm on CPU (Step-by-Step)
  id: totrans-1501
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 35. CPU 上的 LayerNorm（逐步）
- en: One of the most important but often overlooked ingredients in GPT-2 is Layer
    Normalization, or LayerNorm for short. While attention and MLPs are the big stars,
    LayerNorm is like the stage crew keeping everything running smoothly behind the
    scenes. It ensures the numbers flowing through the network stay stable and balanced,
    preventing explosions or collapses that could make training impossible. In `train_gpt2.c`,
    LayerNorm is implemented with explicit loops so you can see every calculation.
    Let’s walk through it carefully.
  id: totrans-1502
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2 中最重要但往往被忽视的成分之一是层归一化，或简称 LayerNorm。虽然注意力和 MLP 是大明星，但 LayerNorm 更像是幕后保持一切顺利的舞台工作人员。它确保通过网络流动的数字保持稳定和平衡，防止爆炸或崩溃，这可能导致训练无法进行。在
    `train_gpt2.c` 中，LayerNorm 通过显式循环实现，这样你可以看到每个计算。让我们仔细地走一遍。
- en: Why Do We Need Normalization?
  id: totrans-1503
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么我们需要归一化？
- en: Imagine a classroom where every student talks at different volumes. Some whisper,
    some shout. If you try to listen to all of them at once, the loud voices drown
    out the quiet ones.
  id: totrans-1504
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个教室，每个学生说话的音量都不同。有些人低声细语，有些人高声喊叫。如果你试图同时听所有人的声音，大声的声音会淹没安静的声音。
- en: Neural networks face a similar problem. The outputs of layers can have wildly
    different scales. If one dimension of a vector is much larger than the others,
    it dominates. Training becomes unstable, and gradients may vanish or explode.
  id: totrans-1505
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络面临类似的问题。层的输出可以具有截然不同的尺度。如果向量的一个维度远大于其他维度，它将主导。训练变得不稳定，梯度可能消失或爆炸。
- en: 'LayerNorm fixes this by ensuring that, for each token at each layer, the vector
    has:'
  id: totrans-1506
  prefs: []
  type: TYPE_NORMAL
  zh: LayerNorm通过确保对于每个层的每个标记，向量都有：
- en: Mean = 0 (centered around zero)
  id: totrans-1507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均值 = 0（围绕零中心）
- en: Variance = 1 (consistent spread of values)
  id: totrans-1508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方差 = 1（值的一致分布）
- en: After that, trainable parameters scale and shift the result so the model can
    still learn flexible transformations.
  id: totrans-1509
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，可训练参数缩放和偏移结果，以便模型仍然可以学习灵活的转换。
- en: The Math Behind LayerNorm
  id: totrans-1510
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LayerNorm背后的数学
- en: 'For a given token vector `x` of size `C` (e.g., 768):'
  id: totrans-1511
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的大小为 `C` 的标记向量 `x`（例如，768）：
- en: 'Compute mean:'
  id: totrans-1512
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算均值：
- en: '[PRE75]'
  id: totrans-1513
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Compute variance:'
  id: totrans-1514
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算方差：
- en: '[PRE76]'
  id: totrans-1515
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Normalize:'
  id: totrans-1516
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正则化：
- en: '[PRE77]'
  id: totrans-1517
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: where `ε` is a tiny constant (like 1e-5) to avoid division by zero.
  id: totrans-1518
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 `ε` 是一个非常小的常数（如1e-5），以避免除以零。
- en: 'Scale and shift with trainable weights `g` (gamma) and `b` (beta):'
  id: totrans-1519
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用可训练权重 `g`（gamma）和 `b`（beta）进行缩放和偏移：
- en: '[PRE78]'
  id: totrans-1520
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: So the final output has controlled statistics, but still enough flexibility
    for the model to adjust.
  id: totrans-1521
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最终输出具有受控的统计信息，但仍然足够灵活，以便模型可以调整。
- en: The Code in `train_gpt2.c`
  id: totrans-1522
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 代码在 `train_gpt2.c` 中
- en: 'Here’s a simplified version from the repository:'
  id: totrans-1523
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个来自存储库的简化版本：
- en: '[PRE79]'
  id: totrans-1524
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Notice the structure:'
  id: totrans-1525
  prefs: []
  type: TYPE_NORMAL
  zh: 注意结构：
- en: Outer loops over batch `B` and sequence length `T`.
  id: totrans-1526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外层循环遍历批次 `B` 和序列长度 `T`。
- en: Inner loops compute mean, variance, and then apply normalization per token vector
    of length `C`.
  id: totrans-1527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内层循环计算均值、方差，然后对每个长度为 `C` 的标记向量应用归一化。
- en: '`weight` and `bias` are the learnable gamma and beta.'
  id: totrans-1528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weight` 和 `bias` 是可学习的gamma和beta。'
- en: 'This is exactly what LayerNorm means: normalize *each layer’s inputs per token*.'
  id: totrans-1529
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是LayerNorm的含义：对每个标记的每个层的输入进行归一化。
- en: Example Walkthrough
  id: totrans-1530
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例说明
- en: Suppose we have a single token vector (C=4) = `[2.0, -1.0, 3.0, 0.0]`.
  id: totrans-1531
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个单个标记向量（C=4）= `[2.0, -1.0, 3.0, 0.0]`。
- en: 'Mean: `(2 - 1 + 3 + 0)/4 = 1.0`'
  id: totrans-1532
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 均值：`(2 - 1 + 3 + 0)/4 = 1.0`
- en: 'Variance: `((2-1)² + (-1-1)² + (3-1)² + (0-1)²)/4 = (1 + 4 + 4 + 1)/4 = 2.5`'
  id: totrans-1533
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 方差：`((2-1)² + (-1-1)² + (3-1)² + (0-1)²)/4 = (1 + 4 + 4 + 1)/4 = 2.5`
- en: 'Normalize: subtract mean and divide by sqrt(2.5):'
  id: totrans-1534
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正则化：减去均值并除以sqrt(2.5)：
- en: '[PRE80]'
  id: totrans-1535
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Scale and shift (say weight=[1,1,1,1], bias=[0,0,0,0]):'
  id: totrans-1536
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尺度和偏移（例如 weight=[1,1,1,1]，bias=[0,0,0,0]）：
- en: '[PRE81]'
  id: totrans-1537
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: Now the vector has mean 0, variance 1, and is ready for the next layer.
  id: totrans-1538
  prefs: []
  type: TYPE_NORMAL
  zh: 现在向量具有均值0，方差1，并准备好进入下一层。
- en: Analogy
  id: totrans-1539
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 类比
- en: Think of LayerNorm like a baking recipe. If one ingredient is way too strong
    (like adding five times too much salt), the whole dish is ruined. LayerNorm tastes
    the mixture, balances all the flavors, and then lets you adjust the seasoning
    with learnable gamma (scale) and beta (shift).
  id: totrans-1540
  prefs: []
  type: TYPE_NORMAL
  zh: 将LayerNorm想象成一个烘焙食谱。如果一个成分太强（比如加了五倍的盐），整个菜肴就会被毁了。LayerNorm尝了尝混合物，平衡了所有的味道，然后让你通过可学习的gamma（缩放）和beta（偏移）调整调味。
- en: Why It Matters
  id: totrans-1541
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: 'Without LayerNorm, the model would quickly become unstable:'
  id: totrans-1542
  prefs: []
  type: TYPE_NORMAL
  zh: 没有LayerNorm，模型会迅速变得不稳定：
- en: Some tokens would dominate, while others fade.
  id: totrans-1543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些标记会主导，而其他标记会减弱。
- en: Gradients could explode, making loss jump wildly.
  id: totrans-1544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度可能会爆炸，导致损失剧烈波动。
- en: Training would be inconsistent between batches.
  id: totrans-1545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练会在批次之间不一致。
- en: With LayerNorm, each layer works with clean, normalized inputs. This allows
    deeper stacks of attention and MLP blocks to learn reliably.
  id: totrans-1546
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LayerNorm，每个层都使用干净、归一化的输入。这允许更深的注意力堆叠和MLP块可靠地学习。
- en: Try It Yourself
  id: totrans-1547
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 试试你自己
- en: 'Print statistics: Add debug code to check the mean and variance before and
    after LayerNorm. Before: mean ≠ 0, variance ≠ 1\. After: mean ≈ 0, variance ≈
    1.'
  id: totrans-1548
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印统计数据：添加调试代码以检查LayerNorm前后均值和方差。之前：mean ≠ 0，variance ≠ 1。之后：mean ≈ 0，variance
    ≈ 1。
- en: 'Remove LayerNorm: Comment out LayerNorm in the code. Watch training collapse-loss
    will not decrease properly.'
  id: totrans-1549
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移除LayerNorm：在代码中注释掉LayerNorm。观察训练崩溃-损失不会正确减少。
- en: 'Change epsilon: Try making `ε = 1e-1` or `ε = 1e-12`. See how too large or
    too small values can break stability.'
  id: totrans-1550
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 改变epsilon：尝试将 `ε = 1e-1` 或 `ε = 1e-12`。看看过大或过小的值如何破坏稳定性。
- en: 'Observe gamma and beta: Initialize gamma=1, beta=0\. During training, watch
    how these parameters drift, fine-tuning normalization.'
  id: totrans-1551
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察gamma和beta：初始化gamma=1，beta=0。在训练过程中，观察这些参数如何漂移，微调归一化。
- en: 'Experiment with batch norm: Replace LayerNorm with BatchNorm (not typical for
    transformers). You’ll see it doesn’t work well, because transformers process variable-length
    sequences where per-batch statistics vary too much.'
  id: totrans-1552
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试批量归一化：用BatchNorm替换LayerNorm（对于transformer来说不典型）。你会发现它效果不佳，因为transformer处理可变长度的序列，而每批次的统计数据变化太大。
- en: The Takeaway
  id: totrans-1553
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 总结
- en: 'LayerNorm is the quiet but critical stabilizer in GPT-2\. It ensures every
    token vector is balanced, centered, and scaled before moving into attention or
    MLP. In `train_gpt2.c`, you see exactly how it works: compute mean, compute variance,
    normalize, then scale and shift. Even though it’s just a few lines of C code,
    it’s one of the main reasons deep transformers can stack dozens of layers without
    breaking.'
  id: totrans-1554
  prefs: []
  type: TYPE_NORMAL
  zh: LayerNorm是GPT-2中安静但至关重要的稳定器。它确保每个标记向量在进入注意力或MLP之前都是平衡的、居中的和缩放的。在 `train_gpt2.c`
    中，你可以看到它的工作方式：计算均值、计算方差、归一化，然后缩放和偏移。尽管这只是几行C代码，但它是不深度transformer能够堆叠数十层而不崩溃的主要原因之一。
- en: 36\. Residual Adds and Signal Flow
  id: totrans-1555
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 36. 残差添加和信号流
- en: 'Once embeddings, attention, and MLP blocks are computed, there’s still one
    piece left to keep the whole network stable and effective: residual connections.
    In `train_gpt2.c`, these appear in functions like `residual_forward`, where outputs
    of a layer are added back to their inputs. This simple-looking step is one of
    the key reasons GPT-2 and other deep transformer models can stack many layers
    without collapsing.'
  id: totrans-1556
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦计算了嵌入、注意力和MLP块，仍然还有一个部分可以保持整个网络稳定和有效：残差连接。在 `train_gpt2.c` 中，这些出现在像 `residual_forward`
    这样的函数中，其中层的输出被添加回它们的输入。这个看似简单的步骤是GPT-2和其他深度transformer模型能够堆叠许多层而不崩溃的关键原因之一。
- en: The Core Idea
  id: totrans-1557
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 核心思想
- en: 'A residual connection says:'
  id: totrans-1558
  prefs: []
  type: TYPE_NORMAL
  zh: 残差连接表示：
- en: '[PRE82]'
  id: totrans-1559
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: Instead of replacing the old representation with the new one, the model adds
    them together. That way, the original signal always survives, even if the new
    transformation is noisy or imperfect.
  id: totrans-1560
  prefs: []
  type: TYPE_NORMAL
  zh: 与用新表示替换旧表示不同，模型将它们相加。这样，原始信号总是得以保留，即使新的转换有噪声或不完美。
- en: Think of it like taking lecture notes. Each time the teacher explains more,
    you don’t throw away your old notes. You add new details next to them. That way,
    you preserve everything learned so far, while layering new insights on top.
  id: totrans-1561
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下像做课堂笔记。每次老师讲解更多，你不会丢弃你的旧笔记。你会在旁边添加新的细节。这样，你就能保留到目前为止所学的一切，同时在上面叠加新的见解。
- en: Why Residuals Are Crucial
  id: totrans-1562
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么残差至关重要
- en: 'Preventing information loss: If you only applied transformations, some features
    might vanish forever. Adding the input back ensures no information is lost.'
  id: totrans-1563
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 防止信息丢失：如果你只应用了转换，一些特征可能会永远消失。添加输入确保没有信息丢失。
- en: 'Helping gradients flow: During backpropagation, gradients must travel backward
    through many layers. Without shortcuts, they can vanish or explode. Residuals
    create direct paths for gradients, making learning stable.'
  id: totrans-1564
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 帮助梯度流动：在反向传播过程中，梯度必须通过许多层向后传播。没有捷径，它们可能会消失或爆炸。残差为梯度创建直接路径，使学习稳定。
- en: 'Improving training speed: With residuals, deeper networks converge faster because
    the model can “skip” bad transformations while still using the identity mapping.'
  id: totrans-1565
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提高训练速度：有了残差，更深的网络收敛得更快，因为模型可以在使用恒等映射的同时“跳过”不良的转换。
- en: The Code in `train_gpt2.c`
  id: totrans-1566
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '`train_gpt2.c` 中的代码'
- en: 'Here’s the implementation of residual addition:'
  id: totrans-1567
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是残差添加的实现：
- en: '[PRE83]'
  id: totrans-1568
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'It’s deceptively simple:'
  id: totrans-1569
  prefs: []
  type: TYPE_NORMAL
  zh: 它看起来很简单：
- en: '`inp1` is the original input.'
  id: totrans-1570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inp1` 是原始输入。'
- en: '`inp2` is the new transformation (from attention or MLP).'
  id: totrans-1571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inp2` 是新的转换（来自注意力或MLP）。'
- en: '`out` stores the sum.'
  id: totrans-1572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`out` 存储总和。'
- en: '`N` is the total number of floats (`B * T * C`).'
  id: totrans-1573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`N` 是浮点数的总数（`B * T * C`）。'
- en: Even though it’s just a single line inside a loop, this is what makes stacking
    12+ transformer blocks possible.
  id: totrans-1574
  prefs: []
  type: TYPE_NORMAL
  zh: 即使它只是循环内部的一行，这也是为什么能够堆叠12+个transformer块的原因。
- en: Example Walkthrough
  id: totrans-1575
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例说明
- en: Suppose we’re processing the token “cat.”
  id: totrans-1576
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在处理标记“猫”。
- en: 'Input vector (simplified, size=3): `[0.5, -0.3, 0.7]`'
  id: totrans-1577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入向量（简化，大小=3）：`[0.5, -0.3, 0.7]`
- en: 'After attention block: `[0.2, 0.1, -0.4]`'
  id: totrans-1578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在注意力块之后：`[0.2, 0.1, -0.4]`
- en: 'Residual addition:'
  id: totrans-1579
  prefs: []
  type: TYPE_NORMAL
  zh: 残差添加：
- en: '[PRE84]'
  id: totrans-1580
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: Now the representation of “cat” contains both the original signal and the contextual
    information from attention.
  id: totrans-1581
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，“猫”的表示既包含原始信号，也包含来自注意力的上下文信息。
- en: 'Later, after the MLP:'
  id: totrans-1582
  prefs: []
  type: TYPE_NORMAL
  zh: 在MLP之后：
- en: 'Input again: `[0.7, -0.2, 0.3]`'
  id: totrans-1583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 再次输入：`[0.7, -0.2, 0.3]`
- en: 'MLP output: `[0.1, -0.5, 0.4]`'
  id: totrans-1584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLP 输出：`[0.1, -0.5, 0.4]`
- en: 'Residual: `[0.8, -0.7, 0.7]`'
  id: totrans-1585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 残差：`[0.8, -0.7, 0.7]`
- en: Step by step, the vector grows richer without losing its foundation.
  id: totrans-1586
  prefs: []
  type: TYPE_NORMAL
  zh: 逐步地，向量变得更加丰富，同时不失其基础。
- en: Analogy
  id: totrans-1587
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 类比
- en: Residual connections are like building layers in Photoshop. Each new layer adds
    adjustments, but you always keep the original photo underneath. If a new adjustment
    is bad, you can still see the original. This makes the final composition stronger
    and safer to experiment with.
  id: totrans-1588
  prefs: []
  type: TYPE_NORMAL
  zh: 残差连接就像在Photoshop中构建图层。每个新图层添加调整，但你总是保留原始照片在下面。如果新的调整不好，你仍然可以看到原始的。这使得最终的合成更强，并且更容易进行实验。
- en: Residuals Across the Model
  id: totrans-1589
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型中的残差
- en: 'In GPT-2’s forward pass, residuals appear in two main places inside each transformer
    block:'
  id: totrans-1590
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPT-2的前向传递中，残差在每个变压器块内部出现两个主要位置：
- en: 'After attention:'
  id: totrans-1591
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在注意力之后：
- en: '[PRE85]'
  id: totrans-1592
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'After MLP:'
  id: totrans-1593
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: MLP之后：
- en: '[PRE86]'
  id: totrans-1594
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Together with LayerNorm before each block, these form the backbone of the transformer
    architecture:'
  id: totrans-1595
  prefs: []
  type: TYPE_NORMAL
  zh: 与每个块之前的LayerNorm一起，这些构成了变压器架构的核心：
- en: '[PRE87]'
  id: totrans-1596
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: Why It Matters
  id: totrans-1597
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: Without residual connections, GPT-2 would struggle to train past a few layers.
    Deeper stacks would lose track of the original signal, gradients would vanish,
    and performance would stall. Residuals are the glue that holds the whole architecture
    together, enabling models with billions of parameters to train effectively.
  id: totrans-1598
  prefs: []
  type: TYPE_NORMAL
  zh: 没有残差连接，GPT-2将难以训练超过几层。更深层的堆叠会失去原始信号，梯度会消失，性能会停滞。残差是整个架构的粘合剂，使得具有数十亿参数的模型能够有效地训练。
- en: Try It Yourself
  id: totrans-1599
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 试试看
- en: 'Remove residuals: Temporarily comment out the `residual_forward` calls. Training
    will quickly fail-the loss won’t decrease properly.'
  id: totrans-1600
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移除残差：暂时注释掉`residual_forward`调用。训练将很快失败——损失不会适当减少。
- en: 'Print before/after: Inspect a token’s vector before and after residual add.
    Notice how the numbers change smoothly rather than being overwritten.'
  id: totrans-1601
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印前/后：检查残差添加前后的标记向量。注意数字是如何平滑变化而不是被覆盖的。
- en: 'Experiment with scaling: Try replacing `out[i] = inp1[i] + inp2[i];` with `out[i]
    = inp1[i] + 0.1 * inp2[i];`. This reduces the impact of the new transformation-sometimes
    used in advanced architectures.'
  id: totrans-1602
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试扩展：尝试将`out[i] = inp1[i] + inp2[i];`替换为`out[i] = inp1[i] + 0.1 * inp2[i];`。这减少了新转换的影响——有时在高级架构中使用。
- en: 'Compare to skip-less RNNs: Research how older recurrent networks without residuals
    had trouble scaling deep. You’ll see why residuals are a game changer.'
  id: totrans-1603
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与无跳过的RNN比较：研究没有残差的旧循环网络在扩展深度时遇到的困难。你会看到为什么残差是一个变革者。
- en: 'Chain of signals: Track how a single token’s vector evolves across all 12 layers.
    You’ll notice it keeps its core identity while absorbing new context.'
  id: totrans-1604
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 信号链：跟踪单个标记的向量如何在所有12层中演变。你会注意到它保持了其核心身份，同时吸收了新的上下文。
- en: The Takeaway
  id: totrans-1605
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸取的经验教训
- en: 'Residual connections may look like a simple addition, but they’re the key to
    deep learning’s success in transformers. They preserve information, stabilize
    training, and allow GPT-2 to stack many layers without falling apart. In `train_gpt2.c`,
    this idea is laid bare: a few lines of C code implementing one of the most powerful
    tricks in modern neural networks.'
  id: totrans-1606
  prefs: []
  type: TYPE_NORMAL
  zh: 残差连接看起来像简单的加法，但它们是深度学习在变压器中取得成功的关键。它们保留信息，稳定训练，并允许GPT-2堆叠多层而不会崩溃。在`train_gpt2.c`中，这一想法被明确地表达出来：几行C代码实现了现代神经网络中最强大的技巧之一。
- en: 37\. Cross-Entropy Loss on CPU
  id: totrans-1607
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 37. CPU上的交叉熵损失
- en: After embeddings, attention, MLPs, LayerNorm, and residuals have done their
    job, the model produces logits-raw scores for every word in the vocabulary at
    every position in the sequence. But logits alone don’t tell us if the model is
    “good” or “bad” at predicting the right word. To measure performance and guide
    learning, GPT-2 uses the cross-entropy loss function. In `train_gpt2.c`, this
    is implemented in the function `crossentropy_forward`.
  id: totrans-1608
  prefs: []
  type: TYPE_NORMAL
  zh: 在嵌入、注意力、MLP、LayerNorm和残差完成工作后，模型生成logits——序列中每个位置的每个词汇的原始分数。但仅凭logits本身并不能告诉我们模型在预测正确单词方面是“好”还是“坏”。为了衡量性能并指导学习，GPT-2使用交叉熵损失函数。在`train_gpt2.c`中，这通过`crossentropy_forward`函数实现。
- en: What Are Logits?
  id: totrans-1609
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 什么是logits？
- en: 'At the final stage of the forward pass, each token position has a vector of
    length `V` (vocabulary size, ~50k). For example, the model might produce these
    logits for a tiny vocabulary of 3 words:'
  id: totrans-1610
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向传递的最后阶段，每个标记位置都有一个长度为`V`（词汇量大小，约50k）的向量。例如，模型可能为3个单词的小型词汇表生成以下logits：
- en: '[PRE88]'
  id: totrans-1611
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: Logits are just numbers-bigger means “more likely,” smaller means “less likely”-but
    they aren’t probabilities yet.
  id: totrans-1612
  prefs: []
  type: TYPE_NORMAL
  zh: logits只是数字——更大的数字意味着“更有可能”，较小的数字意味着“不太可能”——但它们还不是概率。
- en: 'Step 1: Softmax – Turning Scores Into Probabilities'
  id: totrans-1613
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第1步：Softmax – 将分数转换为概率
- en: 'To compare predictions with the true target, we first convert logits into probabilities.
    The tool for this is the softmax function:'
  id: totrans-1614
  prefs: []
  type: TYPE_NORMAL
  zh: 要比较预测与真实目标，我们首先将 logits 转换为概率。用于此的工具是 softmax 函数：
- en: '[PRE89]'
  id: totrans-1615
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Softmax has two important effects:'
  id: totrans-1616
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax 有两个重要的影响：
- en: It makes all values positive.
  id: totrans-1617
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它使所有值都为正。
- en: It normalizes them so they sum to 1, forming a probability distribution.
  id: totrans-1618
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它将它们正规化，使它们的总和为 1，形成一个概率分布。
- en: 'Example:'
  id: totrans-1619
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: 'Logits: `[5.2, 1.1, -2.7]`'
  id: totrans-1620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Logits: `[5.2, 1.1, -2.7]`'
- en: Subtract max (5.2) for stability → `[0.0, -4.1, -7.9]`
  id: totrans-1621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减去最大值（5.2）以提高稳定性 → `[0.0, -4.1, -7.9]`
- en: Exponentiate → `[1.0, 0.017, 0.0004]`
  id: totrans-1622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指数化 → `[1.0, 0.017, 0.0004]`
- en: Normalize → `[0.98, 0.017, 0.0004]`
  id: totrans-1623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则化 → `[0.98, 0.017, 0.0004]`
- en: 'Now the model is saying:'
  id: totrans-1624
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模型说的是：
- en: 'Word 0: 98% chance'
  id: totrans-1625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词 0：98% 的机会
- en: 'Word 1: 1.7% chance'
  id: totrans-1626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词 1：1.7% 的机会
- en: 'Word 2: 0.04% chance'
  id: totrans-1627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词 2：0.04% 的机会
- en: 'Step 2: Cross-Entropy – Measuring Mistakes'
  id: totrans-1628
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第 2 步：交叉熵 – 测量错误
- en: Cross-entropy compares the predicted probability for the correct word against
    the ideal case (probability = 1).
  id: totrans-1629
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵比较预测的正确单词的概率与理想情况（概率 = 1）。
- en: 'Formula:'
  id: totrans-1630
  prefs: []
  type: TYPE_NORMAL
  zh: 公式：
- en: '[PRE90]'
  id: totrans-1631
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: If the model assigns high probability to the correct word, loss is small.
  id: totrans-1632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果模型为正确单词分配高概率，损失就小。
- en: If the model assigns low probability, loss is large.
  id: totrans-1633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果模型分配低概率，损失就大。
- en: 'Example:'
  id: totrans-1634
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: Correct word = Word 0, probability = 0.98 → loss = -log(0.98) ≈ 0.02 (excellent).
  id: totrans-1635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确单词 = 单词 0，概率 = 0.98 → 损失 = -log(0.98) ≈ 0.02 (优秀)。
- en: Correct word = Word 1, probability = 0.017 → loss = -log(0.017) ≈ 4.1 (bad).
  id: totrans-1636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确单词 = 单词 1，概率 = 0.017 → 损失 = -log(0.017) ≈ 4.1 (差)。
- en: 'Step 3: Averaging Over the Batch'
  id: totrans-1637
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第 3 步：批处理平均
- en: In practice, we don’t train on just one word, but a batch of sequences. The
    code loops over every token in every batch, collects their losses, and averages
    them.
  id: totrans-1638
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们不是只在一个单词上训练，而是在一系列序列上训练。代码遍历每个批次中的每个标记，收集它们的损失，并取平均值。
- en: 'From `train_gpt2.c`:'
  id: totrans-1639
  prefs: []
  type: TYPE_NORMAL
  zh: 从 `train_gpt2.c`：
- en: '[PRE91]'
  id: totrans-1640
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'What’s happening here:'
  id: totrans-1641
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生的事情：
- en: For each token, find the max logit (`logit_max`) to improve numerical stability.
  id: totrans-1642
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个标记，找到最大的 logit (`logit_max`) 以提高数值稳定性。
- en: Compute softmax denominator (`sum`).
  id: totrans-1643
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算softmax分母（`sum`）。
- en: Calculate log probability of the correct token.
  id: totrans-1644
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算正确标记的对数概率。
- en: Accumulate losses across all tokens.
  id: totrans-1645
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在所有标记上累积损失。
- en: Divide by total tokens (`B*T`) to get the average.
  id: totrans-1646
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除以总标记数（`B*T`）以获得平均值。
- en: Numerical Stability Tricks
  id: totrans-1647
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数值稳定性技巧
- en: Without subtracting `logit_max`, `exp(logits)` can overflow. For example, `exp(1000)`
    is infinite. By subtracting the max, the largest logit becomes 0, so its exponential
    is 1, and all others are ≤ 1\. This keeps numbers manageable while preserving
    the probability ratios.
  id: totrans-1648
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不减去 `logit_max`，`exp(logits)` 可能溢出。例如，`exp(1000)` 是无穷大。通过减去最大值，最大的 logit 变为
    0，因此其指数为 1，其他所有值都 ≤ 1。这保持了数字的可管理性，同时保留了概率比率。
- en: Example With a Sentence
  id: totrans-1649
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 带有句子的示例
- en: 'Sentence: *The cat sat on the mat.*'
  id: totrans-1650
  prefs: []
  type: TYPE_NORMAL
  zh: 句子：*猫坐在垫子上。*
- en: 'Suppose the model predicts probabilities for the last token:'
  id: totrans-1651
  prefs: []
  type: TYPE_NORMAL
  zh: 假设模型预测最后一个标记的概率：
- en: '“mat”: 0.85'
  id: totrans-1652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “垫子”：0.85
- en: '“dog”: 0.10'
  id: totrans-1653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “狗”：0.10
- en: '“car”: 0.05'
  id: totrans-1654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “车”：0.05
- en: Correct word = “mat.”
  id: totrans-1655
  prefs: []
  type: TYPE_NORMAL
  zh: 正确单词 = “垫子。”
- en: Loss = `-log(0.85) ≈ 0.16`.
  id: totrans-1656
  prefs: []
  type: TYPE_NORMAL
  zh: 损失 = `-log(0.85) ≈ 0.16`。
- en: If instead the model guessed “dog” with 0.10, loss = `-log(0.10) ≈ 2.3`. Higher
    penalty for being wrong.
  id: totrans-1657
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型猜测“狗”的概率为 0.10，损失 = `-log(0.10) ≈ 2.3`。错误惩罚更高。
- en: Analogy
  id: totrans-1658
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 类比
- en: Cross-entropy is like grading multiple-choice exams. If the student picks the
    right answer confidently (high probability), they lose almost no points. If they’re
    hesitant or wrong, they lose more points. Over many questions (tokens), you calculate
    their average score-the training loss.
  id: totrans-1659
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵就像批改多项选择题。如果学生自信地选择了正确答案（高概率），他们几乎不会丢分。如果他们犹豫不决或答错，他们会丢更多分。在许多问题（标记）中，你计算他们的平均得分——训练损失。
- en: Why It Matters
  id: totrans-1660
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: 'Cross-entropy loss is the guiding signal for the entire training process. It
    tells the optimizer:'
  id: totrans-1661
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵损失是整个训练过程的指导信号。它告诉优化器：
- en: “Increase probability for the right words.”
  id: totrans-1662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “增加正确单词的概率。”
- en: “Decrease probability for the wrong words.”
  id: totrans-1663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “降低错误单词的概率。”
- en: Without it, GPT-2 would have no way of knowing whether its predictions are improving.
  id: totrans-1664
  prefs: []
  type: TYPE_NORMAL
  zh: 没有它，GPT-2 就无法知道其预测是否在改进。
- en: Try It Yourself
  id: totrans-1665
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 试试你自己
- en: 'Check baseline loss: Before training, print the loss. It should be close to
    `log(vocab_size)` (~10.8 for GPT-2 small), which corresponds to random guessing.'
  id: totrans-1666
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查基线损失：在训练之前打印损失。它应该接近 `log(vocab_size)` (~10.8 for GPT-2 small)，这对应于随机猜测。
- en: 'Inspect softmax sums: For one token, sum all probabilities. It should equal
    ~1.0.'
  id: totrans-1667
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查 softmax 求和：对于单个标记，求和所有概率。它应该等于 ~1.0。
- en: 'Force the wrong answer: Temporarily change the target to an incorrect word.
    Watch how the loss shoots up.'
  id: totrans-1668
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 强制错误答案：暂时将目标更改为一个错误的单词。观察损失如何急剧上升。
- en: 'Observe loss during training: Print loss every step. It should steadily decrease
    as the model learns.'
  id: totrans-1669
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练过程中观察损失：每一步打印损失。随着模型的学习，损失应该稳步下降。
- en: 'Compare with accuracy: Track how often the model’s top prediction matches the
    target. Loss and accuracy will move together, but loss is smoother and more informative.'
  id: totrans-1670
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与准确度比较：跟踪模型最高预测与目标匹配的频率。损失和准确度将一起移动，但损失更平滑且更有信息量。
- en: The Takeaway
  id: totrans-1671
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸收要点
- en: Cross-entropy loss turns raw model scores into a clear training signal. It penalizes
    wrong predictions, rewards confident correct ones, and ensures the optimizer knows
    exactly how to adjust weights. In `train_gpt2.c`, you see this implemented explicitly,
    without any library shortcuts-just loops, exponentials, and logs. Understanding
    this section is key to understanding how GPT-2 learns from its mistakes.
  id: totrans-1672
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵损失将原始模型得分转换为清晰的训练信号。它惩罚错误预测，奖励自信的正确预测，并确保优化器确切知道如何调整权重。在`train_gpt2.c`中，你可以看到这是明确实现的，没有任何库快捷方式——只有循环、指数和对数。理解这一部分是理解GPT-2如何从错误中学习的关键。
- en: '38\. Putting It All Together: The `gpt2_forward` Function'
  id: totrans-1673
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 38. 将所有内容整合：`gpt2_forward`函数
- en: 'Up to this point, we’ve explored the forward pass piece by piece - embeddings,
    attention, feed-forward layers, layer normalization, residual connections, and
    finally the loss. But a model doesn’t live as disconnected pieces; they all come
    together in a single function that drives inference: `gpt2_forward`. This function
    is where the code actually executes the story we’ve been telling. Let’s walk through
    it carefully so you can see how every building block plugs into the whole picture.'
  id: totrans-1674
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经逐个探讨了前向传递的各个部分——嵌入、注意力、前馈层、层归一化、残差连接，最后是损失。但一个模型并不是作为独立的部件存在的；它们都在一个单一的功能中汇集在一起，驱动推理：`gpt2_forward`。这个函数是代码实际执行我们一直在讲述的故事的地方。让我们仔细地走一遍，这样你就可以看到每个构建块是如何融入整个图画的。
- en: The role of `gpt2_forward`
  id: totrans-1675
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '`gpt2_forward`的作用'
- en: 'Think of `gpt2_forward` as the director of the play. The actors (embeddings,
    attention, MLP, layernorm, etc.) already know their roles. The director calls
    them on stage in the right order and makes sure they hand the script off smoothly
    to the next actor. In our case:'
  id: totrans-1676
  prefs: []
  type: TYPE_NORMAL
  zh: 将`gpt2_forward`想象成戏剧的导演。演员（嵌入、注意力、MLP、层归一化等）已经知道他们的角色。导演按照正确的顺序将他们召唤到舞台上，并确保他们顺利地将剧本传递给下一个演员。在我们的情况下：
- en: Tokens come in as integers (word IDs).
  id: totrans-1677
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标记以整数（词ID）的形式输入。
- en: They’re turned into embeddings (token + position).
  id: totrans-1678
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它们被转换为嵌入（标记+位置）。
- en: Each transformer block processes the sequence through attention, MLP, layernorm,
    and residuals.
  id: totrans-1679
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个Transformer模块通过注意力、MLP、层归一化和残差处理序列。
- en: The final hidden states are mapped back into vocabulary space.
  id: totrans-1680
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终隐藏状态被映射回词汇空间。
- en: If labels are provided, a loss is computed.
  id: totrans-1681
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果提供了标签，则计算损失。
- en: Code skeleton
  id: totrans-1682
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 代码框架
- en: 'Here’s a simplified excerpt of the real function from `train_gpt2.c` (slightly
    shortened for readability):'
  id: totrans-1683
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是`train_gpt2.c`中真实函数的简化摘录（为了可读性略有缩短）：
- en: '[PRE92]'
  id: totrans-1684
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: Don’t worry if this looks intimidating - we’ll decode each part in plain language.
  id: totrans-1685
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这看起来令人生畏，请不要担心——我们将用通俗易懂的语言解码每个部分。
- en: 'Step 1: Embedding lookup'
  id: totrans-1686
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第1步：嵌入查找
- en: 'Before the model can reason about words, it has to map token IDs into continuous
    vectors. That’s where embedding tables come in:'
  id: totrans-1687
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型能够对单词进行推理之前，它必须将标记ID映射到连续向量。这就是嵌入表的作用所在：
- en: '`token_embedding` converts each integer token ID into a dense vector of size
    `C` (the channel dimension).'
  id: totrans-1688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_embedding`将每个整数标记ID转换为大小为`C`（通道维度）的密集向量。'
- en: '`position_embedding` does the same for positions (0, 1, 2, …, T-1).'
  id: totrans-1689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_embedding`对位置（0，1，2，…，T-1）做同样的处理。'
- en: These two are added together, giving each token both a meaning (word identity)
    and a place in the sentence.
  id: totrans-1690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这两个被相加，给每个标记既赋予了意义（词身份）又在句子中的位置。
- en: 'Step 2: Transformer blocks'
  id: totrans-1691
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第2步：Transformer模块
- en: 'Each block is like a mini-pipeline that processes the sequence and passes it
    forward. Inside the loop:'
  id: totrans-1692
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模块就像一个微型管道，处理序列并将其传递下去。在循环内部：
- en: 'Attention: compares tokens with each other, weighted by learned Q/K/V projections.'
  id: totrans-1693
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意力：通过学习到的Q/K/V投影加权比较标记。
- en: 'MLP: expands each token vector, applies a nonlinear GELU activation, then projects
    back down.'
  id: totrans-1694
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: MLP：扩展每个标记向量，应用非线性GELU激活，然后将其投影回下。
- en: 'LayerNorm: normalizes values for stable training and inference.'
  id: totrans-1695
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 层归一化：对值进行归一化，以实现稳定的训练和推理。
- en: 'Residual: adds the input of the block back to its output to keep information
    flowing.'
  id: totrans-1696
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 残差：将块的输入加回到其输出中，以保持信息流动。
- en: This loop runs `n_layer` times - for GPT-2 124M, that’s 12 blocks.
  id: totrans-1697
  prefs: []
  type: TYPE_NORMAL
  zh: 这个循环运行 `n_layer` 次 - 对于 GPT-2 124M，那就是 12 个块。
- en: 'Step 3: Final normalization and logits'
  id: totrans-1698
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤 3：最终归一化和 logits
- en: After the last block, the sequence of token representations goes through a final
    layer normalization. Then, a large matrix multiplication (`lm_head`) projects
    each token’s hidden state into the size of the vocabulary (≈50,000 for GPT-2).
    The result is a tensor of shape `(B, T, vocab_size)` containing the raw prediction
    scores for each next token.
  id: totrans-1699
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一个块之后，标记表示的序列通过一个最终的层归一化。然后，一个大的矩阵乘法（`lm_head`）将每个标记的隐藏状态投影到词汇量的大小（GPT-2
    约为 50,000）。结果是包含每个下一个标记的原始预测分数的形状为 `(B, T, vocab_size)` 的张量。
- en: 'Step 4: Optional loss computation'
  id: totrans-1700
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤 4：可选的损失计算
- en: 'If you pass `labels` (the correct next tokens) into `gpt2_forward`, the function
    calls `crossentropy_forward`. This compares the predicted scores with the true
    tokens and outputs a single number: the loss. The loss tells you “how wrong” the
    model was, which is critical during training. But if you’re only doing inference,
    you don’t need this step.'
  id: totrans-1701
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将 `labels`（正确的下一个标记）传递给 `gpt2_forward`，该函数将调用 `crossentropy_forward`。这比较预测分数与真实标记，并输出一个数字：损失。损失告诉你“模型有多错”，这在训练期间至关重要。但如果你只是进行推理，你不需要这一步。
- en: How the pieces connect
  id: totrans-1702
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 零部件如何连接
- en: 'Here’s a table that maps our earlier sections to the parts of `gpt2_forward`:'
  id: totrans-1703
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个表格，将我们之前的章节映射到 `gpt2_forward` 的部分：
- en: '| Code Step | Concept | Section Covered Earlier |'
  id: totrans-1704
  prefs: []
  type: TYPE_TB
  zh: '| 代码步骤 | 概念 | 之前章节涵盖的内容 |'
- en: '| --- | --- | --- |'
  id: totrans-1705
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Embeddings | token + positional vectors | 32 |'
  id: totrans-1706
  prefs: []
  type: TYPE_TB
  zh: '| 嵌入 | 标记 + 位置向量 | 32 |'
- en: '| Attention | QKV projections, masking, softmax | 33 |'
  id: totrans-1707
  prefs: []
  type: TYPE_TB
  zh: '| 注意力 | QKV 投影、掩码、softmax | 33 |'
- en: '| MLP | feed-forward expansion and compression | 34 |'
  id: totrans-1708
  prefs: []
  type: TYPE_TB
  zh: '| MLP | 前馈扩展和压缩 | 34 |'
- en: '| LayerNorm | normalization for stability | 35 |'
  id: totrans-1709
  prefs: []
  type: TYPE_TB
  zh: '| 层归一化 | 稳定性归一化 | 35 |'
- en: '| Residual | skip connections for signal flow | 36 |'
  id: totrans-1710
  prefs: []
  type: TYPE_TB
  zh: '| 残差 | 用于信号流的跳过连接 | 36 |'
- en: '| CrossEntropy | comparing predictions with labels | 37 |'
  id: totrans-1711
  prefs: []
  type: TYPE_TB
  zh: '| 交叉熵 | 比较预测与标签 | 37 |'
- en: So `gpt2_forward` is really just a neat orchestration of everything you’ve already
    learned.
  id: totrans-1712
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，`gpt2_forward` 实际上只是对已经学到的所有内容的巧妙编排。
- en: Why it matters
  id: totrans-1713
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: Understanding `gpt2_forward` gives you the complete mental picture of inference.
    It shows how embeddings, attention, MLP, normalization, and residuals work together
    in code to turn a batch of tokens into predictions. Without this integration step,
    the model would just be a collection of disconnected parts.
  id: totrans-1714
  prefs: []
  type: TYPE_NORMAL
  zh: 理解 `gpt2_forward` 给你完整的推理心理图。它展示了嵌入、注意力、MLP、归一化和残差如何在代码中协同工作，将一批标记转换为预测。没有这个集成步骤，模型将只是一个脱节的部件集合。
- en: Try it yourself
  id: totrans-1715
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 亲自尝试
- en: 'Print shapes: Add `printf` statements inside `gpt2_forward` to print tensor
    shapes after embeddings, after each block, and after logits. This helps you see
    the data flow.'
  id: totrans-1716
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印形状：在 `gpt2_forward` 中添加 `printf` 语句，以打印嵌入后的张量形状、每个块之后和 logits 之后。这有助于你看到数据流。
- en: 'Use a single block: Change the loop to run only 1 transformer block instead
    of all 12\. Watch how the outputs degrade - the model loses depth of reasoning.'
  id: totrans-1717
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用单个块：将循环更改为只运行 1 个 transformer 块而不是所有 12 个。观察输出如何退化 - 模型失去了推理的深度。
- en: 'Disable position embeddings: Comment out the line that adds `position_embedding`.
    Try running inference. You’ll notice the model becomes worse at handling word
    order.'
  id: totrans-1718
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 禁用位置嵌入：取消注释添加 `position_embedding` 的行。尝试运行推理。你会注意到模型在处理词序方面变得更差。
- en: 'Loss vs no loss: Call `gpt2_forward` with and without labels. Compare the difference
    - with labels you get a scalar loss, without labels you just get logits.'
  id: totrans-1719
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 损失与无损失：带有和没有标签调用 `gpt2_forward`。比较差异 - 带有标签你得到一个标量损失，没有标签你只得到 logits。
- en: 'Smaller vocab: Try using a toy tokenizer with a small vocabulary and rerun
    the projection step. You’ll see the logits shrink to `(B, T, tiny_vocab_size)`.'
  id: totrans-1720
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 较小的词汇量：尝试使用具有小词汇量的玩具标记器并重新运行投影步骤。你会看到 logits 缩小到 `(B, T, tiny_vocab_size)`。
- en: The takeaway
  id: totrans-1721
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸取的经验
- en: '`gpt2_forward` is where GPT-2 inference really happens. It ties together every
    concept - embeddings, attention, feed-forward layers, normalization, residuals,
    and the final projection into vocabulary space. Once you understand this function,
    you don’t just know the pieces of GPT-2, you know how they actually work together
    to produce predictions. It’s the “main stage” of inference, and mastering it means
    you can confidently say you understand how a transformer runs forward on CPU.'
  id: totrans-1722
  prefs: []
  type: TYPE_NORMAL
  zh: '`gpt2_forward` 是GPT-2推理真正发生的地方。它将每个概念——嵌入、注意力、前馈层、归一化、残差以及最终投影到词汇空间——联系在一起。一旦你理解了这个函数，你不仅知道GPT-2的各个部分，还知道它们实际上是如何协同工作以产生预测的。它是推理的“主舞台”，掌握它意味着你可以自信地说你理解了transformer在CPU上正向运行的方式。'
- en: 39\. OpenMP Pragmas for Parallel Loops
  id: totrans-1723
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 39. 并行循环的OpenMP Pragmas
- en: CPU training in `train_gpt2.c` is intentionally “plain C,” but it still squeezes
    out a lot of speed by adding a few OpenMP pragmas (`#pragma omp …`) in the hottest
    loops. OpenMP lets the compiler split a loop’s iterations across multiple CPU
    cores-no threads to create by hand, no locks to manage. If you compile without
    OpenMP support, these pragmas are simply ignored and the code still runs (just
    slower).
  id: totrans-1724
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `train_gpt2.c` 中的CPU训练故意是“纯C”，但它仍然通过在热点循环中添加几个OpenMP Pragmas（`#pragma omp
    …`）来挤出很多速度。OpenMP允许编译器将循环的迭代分配到多个CPU核心——无需手动创建线程，无需管理锁。如果你没有OpenMP支持地编译，这些Pragma将被简单地忽略，代码仍然可以运行（只是速度较慢）。
- en: Below we’ll (1) show exactly where OpenMP is used, (2) explain why those loops
    are good candidates, and (3) offer practical tips to get solid speedups on your
    machine.
  id: totrans-1725
  prefs: []
  type: TYPE_NORMAL
  zh: 下面我们将（1）展示OpenMP的确切使用位置，（2）解释为什么这些循环是很好的候选者，（3）提供一些实用的技巧，以在您的机器上获得稳定的加速效果。
- en: 'OpenMP in this file: where it appears and why'
  id: totrans-1726
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 本文件中的 OpenMP：出现的位置和原因
- en: '| Location / Function | Pragma used | What’s parallelized | Why it’s a great
    fit |'
  id: totrans-1727
  prefs: []
  type: TYPE_TB
  zh: '| 位置 / 功能 | 使用的Pragma | 并行化什么 | 为什么它非常适合 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-1728
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| `matmul_forward_naive` | `#pragma omp parallel for collapse(2)` | Outer loops
    over `b` (batch) and `t` (time) | Each `(b,t)` row computes an independent output
    vector; no write conflicts. Large, regular work = easy scaling. |'
  id: totrans-1729
  prefs: []
  type: TYPE_TB
  zh: '| `matmul_forward_naive` | `#pragma omp parallel for collapse(2)` | 对 `b`（批次）和
    `t`（时间）进行外层循环 | 每个 `(b,t)` 行计算一个独立的输出向量；没有写冲突。大量、规则的工作 = 容易扩展。 |'
- en: '| `matmul_forward` (tiled) | `#pragma omp parallel for` | Collapsed `B*T` loop
    in tiles of `LOOP_UNROLL` | Heaviest compute in the model; tiling + per-thread
    tiles keep caches warm. |'
  id: totrans-1730
  prefs: []
  type: TYPE_TB
  zh: '| `matmul_forward`（分块）| `#pragma omp parallel for` | 在分块中的折叠 `B*T` 循环 | 模型中最重的计算；分块
    + 每线程分块保持缓存活跃。 |'
- en: '| `matmul_backward` (part 1) | `#pragma omp parallel for collapse(2)` | Backprop
    into `inp` over `(b,t)` | Each `(b,t)` reads weights and `dout`, writes a private
    slice of `dinp` → no overlap. |'
  id: totrans-1731
  prefs: []
  type: TYPE_TB
  zh: '| `matmul_backward`（第一部分）| `#pragma omp parallel for collapse(2)` | 在 `(b,t)`
    上反向传播到 `inp` | 每个 `(b,t)` 读取权重和 `dout`，写入 `dinp` 的私有切片 → 没有重叠。 |'
- en: '| `matmul_backward` (part 2) | `#pragma omp parallel for` | Backprop into `weight`/`bias`
    over `o` (output channel) | Each thread owns one output channel’s gradient row,
    avoiding atomics. |'
  id: totrans-1732
  prefs: []
  type: TYPE_TB
  zh: '| `matmul_backward`（第二部分）| `#pragma omp parallel for` | 在 `o`（输出通道）上反向传播到 `weight`/`bias`
    | 每个线程拥有一个输出通道的梯度行，避免了原子操作。 |'
- en: '| `softmax_forward` | `#pragma omp parallel for collapse(2)` | Over `(b,t)`
    positions | Each softmax is independent; perfect “embarrassingly parallel” loop.
    |'
  id: totrans-1733
  prefs: []
  type: TYPE_TB
  zh: '| `softmax_forward` | `#pragma omp parallel for collapse(2)` | 在 `(b,t)` 位置上
    | 每个softmax是独立的；完美的“令人尴尬的并行”循环。 |'
- en: '| `attention_forward` | `#pragma omp parallel for collapse(3)` | Over `(b,
    t, h)` = batch, time, head | Per `(b,t,h)` head’s work is independent; big 3-D
    grid parallelizes extremely well. |'
  id: totrans-1734
  prefs: []
  type: TYPE_TB
  zh: '| `attention_forward` | `#pragma omp parallel for collapse(3)` | 在 `(b, t,
    h)` 上 = 批次、时间、头 | 每个 `(b,t,h)` 头的工作是独立的；大3-D网格并行化效果极佳。 |'
- en: 'A few key patterns to notice:'
  id: totrans-1735
  prefs: []
  type: TYPE_NORMAL
  zh: 几个需要注意的关键模式：
- en: Collapse clauses (`collapse(2)` / `collapse(3)`) fuse nested loops into one
    big iteration space so the scheduler can distribute more, smaller chunks-great
    for load-balancing when `B`, `T`, or `NH` are modest.
  id: totrans-1736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 合并子句（`collapse(2)` / `collapse(3)`）将嵌套循环合并为一个大的迭代空间，以便调度器可以分配更多、更小的块——当 `B`、`T`
    或 `NH` 适中时，非常适合负载均衡。
- en: Parallelizing along independent dimensions avoids race conditions. For example,
    in `matmul_backward` the pass that writes `dinp[b,t,:]` is parallelized over `(b,t)`
    so no two threads update the same memory.
  id: totrans-1737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 沿独立维度并行化可以避免竞争条件。例如，在 `matmul_backward` 中，写入 `dinp[b,t,:]` 的操作在 `(b,t)` 上并行化，因此没有两个线程更新相同的内存。
- en: 'Own-your-row strategy: when accumulating `dweight`, the loop goes over `o`
    (output channels) so each thread writes its own gradient row `dweight[o,:]`. No
    atomics needed.'
  id: totrans-1738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自主行策略：当累积 `dweight` 时，循环遍历 `o`（输出通道），因此每个线程都写入自己的梯度行 `dweight[o,:]`。不需要原子操作。
- en: 'Quick refresher: what OpenMP is doing'
  id: totrans-1739
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 快速复习：OpenMP 在做什么
- en: 'A typical pattern looks like:'
  id: totrans-1740
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的模式如下：
- en: '[PRE93]'
  id: totrans-1741
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: When compiled with OpenMP, the compiler creates a team of threads and divides
    the iteration space (`B*T` in this example) among them. Each thread executes its
    assigned iterations; when the loop finishes, threads sync at an implicit barrier.
  id: totrans-1742
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 OpenMP 编译时，编译器创建一个线程团队，并将迭代空间（本例中的 `B*T`）分配给它们。每个线程执行其分配的迭代；当循环结束时，线程在隐式屏障处同步。
- en: Because each `(b,t)` (or `(b,t,h)`) writes to a disjoint slice of the output
    arrays, there’s no need for locks or atomics. This is why these loops scale cleanly
    across cores.
  id: totrans-1743
  prefs: []
  type: TYPE_NORMAL
  zh: 因为每个 `(b,t)`（或 `(b,t,h)`）都写入输出数组的一个不重叠的切片，所以不需要锁或原子操作。这就是为什么这些循环可以干净地扩展到核心。
- en: Enabling OpenMP, safely
  id: totrans-1744
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 安全启用 OpenMP
- en: 'The source guards the OpenMP header with:'
  id: totrans-1745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源代码使用以下方式保护 OpenMP 头部：
- en: '[PRE94]'
  id: totrans-1746
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'so you can define `OMP` in your build and add your compiler switch. Example
    (GCC/Clang):'
  id: totrans-1747
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 因此您可以在构建中定义 `OMP` 并添加您的编译器开关。示例（GCC/Clang）：
- en: '[PRE95]'
  id: totrans-1748
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: If you forget `-fopenmp` (or your platform’s equivalent), the pragmas are ignored
    and the program runs single-threaded.
  id: totrans-1749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您忘记了 `-fopenmp`（或您平台的等效项），则省略了这些指令，程序以单线程运行。
- en: 'You can control threads at runtime:'
  id: totrans-1750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以在运行时控制线程：
- en: '[PRE96]'
  id: totrans-1751
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: A good rule of thumb is to start with the number of physical cores on your CPU.
  id: totrans-1752
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个好的经验法则是从您 CPU 上的物理核心数开始。
- en: Why these loops benefit the most
  id: totrans-1753
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这些循环受益最大
- en: Matrix multiplies dominate runtime. `matmul_forward`/`matmul_backward` consume
    the bulk of CPU time. Parallelizing them yields the largest end-to-end speedups.
  id: totrans-1754
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 矩阵乘法主导运行时间。`matmul_forward`/`matmul_backward` 消耗了大部分 CPU 时间。并行化它们可以获得最大的端到端加速。
- en: Softmax is independent per position. Each `(b,t)` softmax computes a max, then
    exponentials and a sum-no cross-talk between positions.
  id: totrans-1755
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Softmax 每个位置都是独立的。每个 `(b,t)` softmax 计算一个最大值，然后是指数和求和，位置之间没有交叉干扰。
- en: Attention splits across batch/time/head. The triple loop over `(b,t,h)` has
    lots of work per iteration (Q·K, softmax, weighted sum), making thread overhead
    negligible compared to useful compute.
  id: totrans-1756
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意力在批处理/时间/头之间分散。对 `(b,t,h)` 的三重循环在每次迭代中有很多工作（Q·K，softmax，加权求和），使得线程开销与有用计算相比可以忽略不计。
- en: Minimal synchronization and no atomics. By choosing iteration spaces that own
    exclusive output slices, we avoid costly synchronization.
  id: totrans-1757
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最小化同步和原子操作。通过选择拥有独家输出切片的迭代空间，我们避免了昂贵的同步。
- en: Practical tips for better scaling
  id: totrans-1758
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 更好扩展的实际技巧
- en: Set `OMP_NUM_THREADS` to your CPU. Too many threads can hurt (oversubscription).
    Start with physical cores, then experiment.
  id: totrans-1759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 `OMP_NUM_THREADS` 设置为您的 CPU。线程过多可能会造成损害（过度订阅）。从物理核心开始，然后进行实验。
- en: Pin threads (optional, advanced). Some OpenMP runtimes support `OMP_PROC_BIND=close`
    to improve cache locality.
  id: totrans-1760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 锚定线程（可选，高级）。一些 OpenMP 运行时支持 `OMP_PROC_BIND=close` 以提高缓存局部性。
- en: Mind memory bandwidth. On wide CPUs, GEMMs may become bandwidth-bound. Bigger
    `B`/`T` improves arithmetic intensity; tiny batches underutilize cores.
  id: totrans-1761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意内存带宽。在宽 CPU 上，GEMMs 可能会变成带宽限制。更大的 `B`/`T` 提高了算术强度；微批次未能充分利用核心。
- en: Warm caches with tiling. The “tiled” `matmul_forward` keeps small accumulators
    in registers and reuses loaded weights across `LOOP_UNROLL` inner iterations.
  id: totrans-1762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用分块填充缓存。分块 `matmul_forward` 将小累加器保留在寄存器中，并在 `LOOP_UNROLL` 内部迭代中重用加载的权重。
- en: Avoid hidden sharing. If you add new parallel loops, ensure each thread writes
    to unique memory regions. If you must accumulate to the same place, restructure
    (like “own-your-row”) or use per-thread scratch buffers then reduce.
  id: totrans-1763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免隐藏共享。如果您添加新的并行循环，请确保每个线程写入唯一的内存区域。如果您必须累积到同一位置，请重新结构（如“自主行”）或使用每个线程的临时缓冲区然后进行汇总。
- en: 'Micro-walkthrough: why `collapse` helps'
  id: totrans-1764
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 微型概述：为什么 `collapse` 有帮助
- en: 'Consider `softmax_forward`:'
  id: totrans-1765
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑 `softmax_forward`：
- en: '[PRE97]'
  id: totrans-1766
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: If `B=4`, `T=64`, that’s 256 independent softmaxes. With `collapse(2)`, OpenMP
    sees a single loop of 256 iterations to distribute evenly; without `collapse`,
    it might chunk by `b` first (only 4 big chunks), which can load-imbalance.
  id: totrans-1767
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `B=4`，`T=64`，那么就有 256 个独立的 softmax。使用 `collapse(2)`，OpenMP 会看到 256 次迭代的单个循环来均匀分配；如果没有
    `collapse`，它可能会首先按 `b` 块（只有 4 个大块），这可能会导致负载不平衡。
- en: Common pitfalls (and how this code avoids them)
  id: totrans-1768
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 常见陷阱（以及此代码如何避免它们）
- en: 'Race conditions: Two threads writing the same `out[i]`. *Avoided by design:*
    each parallel loop writes distinct slices (e.g., per `(b,t)` or per `o`).'
  id: totrans-1769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 竞态条件：两个线程写入相同的`out[i]`。*设计上避免：*每个并行循环写入不同的切片（例如，每个`(b,t)`或每个`o`)。
- en: 'False sharing: Threads write adjacent memory locations on the same cache line.
    It’s minimized by the large, contiguous slices per thread (entire rows/tiles),
    but if you extend the code with fine-grained parallelism, keep this in mind.'
  id: totrans-1770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假共享：线程在相同缓存行上的相邻内存位置写入。通过每个线程的大连续切片（整个行/瓦片）来最小化，但如果你在代码中添加了细粒度并行性，请记住这一点。
- en: 'Tiny loops: Overhead can exceed work. The file parallelizes only large, hot
    loops (GEMMs, attention, softmax), not small scalar ops.'
  id: totrans-1771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小循环：开销可能超过工作量。该文件仅并行化大循环（GEMMs、注意力、softmax），而不是小的标量操作。
- en: Try it yourself
  id: totrans-1772
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 试试看
- en: 'Change thread count: Run with `OMP_NUM_THREADS=1,2,4,8,…` and log step time.
    Plot speedup vs. threads.'
  id: totrans-1773
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 改变线程数：使用`OMP_NUM_THREADS=1,2,4,8,…`运行并记录步骤时间。绘制速度与线程的关系图。
- en: 'Toggle a pragma: Comment out `#pragma omp` in `matmul_forward` only. Measure
    the slowdown; you’ll see where most time goes.'
  id: totrans-1774
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 切换指令：仅在`matmul_forward`中注释掉`#pragma omp`。测量减速；你会看到大部分时间在哪里。
- en: 'Experiment with `collapse`: Remove `collapse(2)` in `softmax_forward`. On small
    `B`, you’ll likely see worse scaling.'
  id: totrans-1775
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试`collapse`：在`softmax_forward`中移除`collapse(2)`。在小`B`的情况下，你可能会看到更差的扩展性。
- en: 'Per-layer profiling: Print elapsed time around `matmul_forward`, `attention_forward`,
    and `softmax_forward` to see which benefits most on your CPU.'
  id: totrans-1776
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每层分析：在`matmul_forward`、`attention_forward`和`softmax_forward`周围打印经过的时间，以查看哪个在你的CPU上受益最大。
- en: 'Schedule policy (advanced): Try `#pragma omp parallel for schedule(static)`
    vs. `dynamic` on a heavy loop to see if it changes load balance (defaults are
    usually fine here).'
  id: totrans-1777
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调度策略（高级）：尝试在重循环上使用`#pragma omp parallel for schedule(static)`与`dynamic`，看看是否改变了负载平衡（默认值通常在这里是合适的）。
- en: The takeaway
  id: totrans-1778
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸取的经验教训
- en: A handful of well-placed OpenMP pragmas deliver big wins on CPU by parallelizing
    the most expensive loops (GEMMs, attention, softmax) across cores-without complicating
    the code. The design ensures each thread works on independent slices, so there’s
    no locking, no atomics, and very little overhead. If you compile with OpenMP enabled,
    you get fast, multi-core training; if not, you still have a clean, readable reference
    implementation.
  id: totrans-1779
  prefs: []
  type: TYPE_NORMAL
  zh: 几个放置得当的OpenMP指令可以显著提高CPU性能，通过并行化最昂贵的循环（GEMMs、注意力、softmax）跨核心，而不使代码复杂化。该设计确保每个线程独立工作在独立的切片上，因此没有锁定、没有原子操作，并且开销非常小。如果你使用启用了OpenMP的编译器编译，你将获得快速的多核训练；如果没有，你仍然有一个干净、可读的参考实现。
- en: 40\. CPU Memory Footprint and Performance
  id: totrans-1780
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 40. CPU内存占用和性能
- en: 'When you train GPT-2 on your CPU using `train_gpt2.c`, two big questions usually
    pop up almost immediately: *how much memory is this going to take?* and *how fast
    will it run?* Let’s walk through both of these in a beginner-friendly way, so
    you understand not just what happens in the code, but why it behaves the way it
    does.'
  id: totrans-1781
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用`train_gpt2.c`在你的CPU上训练GPT-2时，两个大问题通常几乎立即出现：*这将占用多少内存？*和*它将运行得多快？*让我们以初学者友好的方式逐一探讨这两个问题，这样你不仅了解代码中发生了什么，而且了解为什么它以这种方式表现。
- en: 'Memory: where does it all go?'
  id: totrans-1782
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 内存：它都去哪里了？
- en: 'Imagine training GPT-2 is like cooking a big meal in a small kitchen. You need
    space for ingredients, bowls for mixing, and counter space for preparing. Memory
    on your CPU is that kitchen. GPT-2 needs several “bowls” to hold different parts
    of the computation:'
  id: totrans-1783
  prefs: []
  type: TYPE_NORMAL
  zh: 想象训练GPT-2就像在一个小厨房里做一顿大餐。你需要空间放食材，混合用的碗，以及准备用的台面。你的CPU内存就是那个厨房。GPT-2需要几个“碗”来存放计算的不同部分：
- en: Parameters (the weights of the model). These are the “fixed recipe” - the actual
    numbers the network learns. They come from the checkpoint file you load at the
    start. For GPT-2 124M, this is about 124 million floating-point numbers. Each
    one takes 4 bytes, so just the weights are around 500 MB.
  id: totrans-1784
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 参数（模型的权重）。这些是“固定配方”——网络实际学习的数字。它们来自你开始时加载的检查点文件。对于GPT-2 124M，这大约是1.24亿个浮点数。每个数占用4字节，所以仅仅是权重就有大约500
    MB。
- en: 'Optimizer state (AdamW). Training doesn’t just adjust weights blindly; it keeps
    track of two extra moving averages for each weight, called *m* and *v*. That means
    for every single parameter, you store three numbers: the weight, m, and v. So
    memory for optimizer state is often double the size of the weights themselves.
    For GPT-2 124M, that’s about 1 GB more.'
  id: totrans-1785
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 优化器状态（AdamW）。训练不仅仅是盲目地调整权重；它还跟踪每个权重两个额外的移动平均值，称为*m*和*v*。这意味着对于每个单独的参数，你存储三个数字：权重、m和v。因此，优化器状态的内存通常是权重本身大小的两倍。对于GPT-2
    124M，这大约是1 GB更多。
- en: Gradients. Every time we run a backward pass, we store how much each weight
    should change. That’s another buffer roughly the same size as the weights - another
    500 MB.
  id: totrans-1786
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 梯度。每次我们运行反向传递时，我们都会存储每个权重应该改变多少。这是另一个与权重大小大致相同的缓冲区——另一个500 MB。
- en: Activations (intermediate results). This is the sneaky one. Every forward pass
    produces temporary tensors like embeddings, attention maps, and feed-forward outputs.
    Their size depends on batch size (B) and sequence length (T). If B=4 and T=64,
    activations are a few hundred MB. If B=32 and T=1024, they can balloon to many
    gigabytes.
  id: totrans-1787
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 激活（中间结果）。这是隐藏的一个。每次前向传递都会产生临时张量，如嵌入、注意力图和前馈输出。它们的大小取决于批量大小（B）和序列长度（T）。如果B=4且T=64，激活大约是几百MB。如果B=32且T=1024，它们可以膨胀到几个GB。
- en: 'Here’s a rough mental budget for GPT-2 124M with a small setup (B=4, T=64):'
  id: totrans-1788
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是GPT-2 124M在小型设置（B=4，T=64）下的粗略心理预算：
- en: 'Parameters: ~500 MB'
  id: totrans-1789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数：~500 MB
- en: 'Optimizer state: ~1 GB'
  id: totrans-1790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化器状态：~1 GB
- en: 'Gradients: ~500 MB'
  id: totrans-1791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度：~500 MB
- en: 'Activations: ~200–300 MB Total: ~2–2.5 GB'
  id: totrans-1792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活：~200–300 MB 总计：~2–2.5 GB
- en: Even for the “tiny” GPT-2, you already need a couple gigabytes of RAM to train.
    On a laptop, this can quickly push you to the limit.
  id: totrans-1793
  prefs: []
  type: TYPE_NORMAL
  zh: 即使对于“小巧”的GPT-2，你也需要几GB的RAM来训练。在笔记本电脑上，这可以迅速把你推到极限。
- en: 'Performance: where does time go?'
  id: totrans-1794
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 性能：时间都去哪了？
- en: 'Now let’s talk speed. When you run `train_gpt2.c` on CPU, you’ll see lines
    like:'
  id: totrans-1795
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们谈谈速度。当你用CPU运行`train_gpt2.c`时，你会看到如下类似的行：
- en: '[PRE98]'
  id: totrans-1796
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'That “took X ms” tells you how long one step took. Why is it slow? Three main
    reasons:'
  id: totrans-1797
  prefs: []
  type: TYPE_NORMAL
  zh: “花费了X毫秒”告诉你一个步骤花费了多长时间。为什么它这么慢？主要有三个原因：
- en: Matrix multiplications (matmuls). These are the heart of neural networks. Every
    attention head and every MLP layer does them. On CPU, most of your step time is
    spent here. That’s why the code uses OpenMP pragmas (`#pragma omp`) to parallelize
    loops across cores.
  id: totrans-1798
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 矩阵乘法（matmuls）。这是神经网络的核心。每个注意力头和每个MLP层都执行这些操作。在CPU上，你大部分的时间都花在这里。这就是为什么代码使用OpenMP指令（`#pragma
    omp`）来并行化跨核心的循环。
- en: Attention softmax. Attention compares every token in a sequence with every other
    token. If your sequence length is 1024, that’s over a million comparisons per
    head per layer. On CPU, this quadratic growth is painful.
  id: totrans-1799
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意力softmax。注意力将序列中的每个标记与序列中的每个其他标记进行比较。如果你的序列长度是1024，那么每个头每个层的比较次数超过一百万。在CPU上，这种二次增长是痛苦的。
- en: Memory bandwidth. CPUs can only move numbers from RAM to cores so fast. Even
    if you had infinite FLOPs, you’d still be slowed down by how quickly you can fetch
    and store these huge tensors.
  id: totrans-1800
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 内存带宽。CPU只能以如此快的速度将数字从RAM移动到核心。即使你有无限的FLOPs，你仍然会受到如何快速检索和存储这些巨大张量的速度限制。
- en: A simple experiment
  id: totrans-1801
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 一个简单的实验
- en: 'You can see these effects yourself:'
  id: totrans-1802
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以自己看到这些效果：
- en: Change batch size (B). Run with B=1, then with B=8\. Notice how memory usage
    and step time scale up.
  id: totrans-1803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改变批量大小（B）。用B=1运行，然后用B=8运行。注意内存使用量和步骤时间是如何成比例增加的。
- en: Change sequence length (T). Try T=16, then T=256\. You’ll see attention costs
    grow dramatically.
  id: totrans-1804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改变序列长度（T）。尝试T=16，然后T=256。你会看到注意力成本显著增加。
- en: Change threads. Set `OMP_NUM_THREADS=1` versus `OMP_NUM_THREADS=8`. With more
    threads, you’ll often see speedups, but only up to the number of physical cores
    your CPU has.
  id: totrans-1805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改变线程数。设置`OMP_NUM_THREADS=1`与`OMP_NUM_THREADS=8`。随着线程数的增加，你通常会看到速度提升，但提升的幅度仅限于CPU的物理核心数。
- en: Why this matters
  id: totrans-1806
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: 'For beginners, CPU runs are perfect for learning:'
  id: totrans-1807
  prefs: []
  type: TYPE_NORMAL
  zh: 对于初学者来说，CPU运行非常适合学习：
- en: You can debug with small batches and short sequences.
  id: totrans-1808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以用小批量和短序列进行调试。
- en: You can step into functions with a debugger and watch tensors being created.
  id: totrans-1809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以用调试器进入函数并观察张量的创建。
- en: You don’t need a GPU just to understand how training works.
  id: totrans-1810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你不需要GPU就能理解训练是如何工作的。
- en: But when it comes to *serious* training - larger GPT-2 models or even long sequences
    - CPU is simply too slow. What takes seconds on GPU may take minutes on CPU. That’s
    why in practice, people use CPUs for learning and testing, and GPUs for large-scale
    training.
  id: totrans-1811
  prefs: []
  type: TYPE_NORMAL
  zh: 但是当涉及到*严肃*的训练——更大的GPT-2模型或甚至长序列时——CPU简单地太慢了。在GPU上只需几秒就能完成的事情，在CPU上可能需要几分钟。这就是为什么在实践中，人们使用CPU进行学习和测试，而使用GPU进行大规模训练。
- en: The takeaway
  id: totrans-1812
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸取的经验
- en: Training GPT-2 on CPU is like practicing piano on a small keyboard. It’s slower,
    limited, and you can’t play the biggest pieces, but it’s great for learning the
    fundamentals. Memory usage comes from weights, optimizer state, gradients, and
    activations, and performance is dominated by matmuls and attention. Once you understand
    where the resources go, you can adjust batch size, sequence length, and threads
    to find the sweet spot for your machine.
  id: totrans-1813
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CPU 上训练 GPT-2 就像在小键盘上练习钢琴。它速度较慢，功能有限，无法演奏最大的作品，但对于学习基础知识来说非常棒。内存使用来自权重、优化器状态、梯度和激活，性能主要由矩阵乘法和注意力操作主导。一旦你了解了资源去向，你就可以调整批处理大小、序列长度和线程，以找到适合你机器的最佳点。
- en: Chapter 5\. Training Loop (CPU Path)
  id: totrans-1814
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 5 章\. 训练循环（CPU 路径）
- en: 41\. Backward Pass Walkthrough
  id: totrans-1815
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 41\. 反向传播遍历
- en: Up until now, we’ve spent all our time looking at the forward pass. That’s the
    part of the model that takes tokens, pushes them through embeddings, attention,
    feed-forward layers, and finally produces logits or a loss. For inference, forward
    pass alone is enough. But if you want to train a model, forward is only half the
    story.
  id: totrans-1816
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在关注正向传播。这是模型中处理标记的部分，它将标记推过嵌入、注意力、前馈层，最终产生 logits 或损失。对于推理，正向传播就足够了。但如果你想训练一个模型，正向传播只是故事的一半。
- en: Training means adjusting the weights of the model so that its predictions become
    better over time. To do this, we need a way to figure out how wrong each weight
    was and in what direction it should move to reduce the loss. That’s the job of
    the backward pass.
  id: totrans-1817
  prefs: []
  type: TYPE_NORMAL
  zh: 训练意味着调整模型的权重，使其预测随时间变得更好。为此，我们需要一种方法来确定每个权重有多错误，以及它应该朝哪个方向移动以减少损失。这就是反向传播的任务。
- en: 'The backward pass is also called backpropagation. It’s the algorithm that moves
    information in reverse through the network: from the loss, back through the final
    logits, through every transformer block, down to the embeddings. Along the way,
    it calculates gradients - small numbers that tell us how much each weight contributed
    to the error.'
  id: totrans-1818
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播也称为反向传播。它是将信息反向传递到网络中的算法：从损失，通过最终的 logits，通过每个 Transformer 块，直到嵌入。在这个过程中，它计算梯度——这些小数字告诉我们每个权重对误差的贡献程度。
- en: 'The big idea: chain rule in action'
  id: totrans-1819
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 主要思想：链式法则的实际应用
- en: 'At the heart of backpropagation is something very familiar from calculus: the
    chain rule. If the output of the network depends on many functions stacked together
    (embedding → attention → MLP → … → loss), then the derivative of the loss with
    respect to an early parameter is a product of partial derivatives through the
    entire chain.'
  id: totrans-1820
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播的核心是来自微积分的一个非常熟悉的概念：链式法则。如果网络的输出依赖于许多堆叠在一起的功能（嵌入 → 注意力 → MLP → … → 损失），那么损失相对于早期参数的导数是整个链中偏导数的乘积。
- en: Instead of writing long formulas, the code in `train_gpt2.c` simply calls each
    layer’s backward function in reverse order. The gradient flows backward, step
    by step, and each layer computes its own contribution using local rules.
  id: totrans-1821
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `train_gpt2.c` 代码中，为了避免编写冗长的公式，简单地以相反的顺序调用每一层的反向函数。梯度逐层反向流动，每层使用局部规则计算自己的贡献。
- en: 'Think of it like a relay race, but run backwards: the loss hands a “blame baton”
    to the output head, which hands it back to the last transformer block, and so
    on, until it reaches the very first embedding table.'
  id: totrans-1822
  prefs: []
  type: TYPE_NORMAL
  zh: 想象它就像接力赛，但方向相反：损失将一个“责任接力棒”传给输出头，然后输出头将其传给最后一个 Transformer 块，以此类推，直到它到达第一个嵌入表。
- en: Walking through `gpt2_backward`
  id: totrans-1823
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 遍历 `gpt2_backward`
- en: 'Here’s a simplified sketch of how the backward function looks in the code (names
    shortened for readability):'
  id: totrans-1824
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是代码中反向函数的简化草图（为了可读性，名称已缩短）：
- en: '[PRE99]'
  id: totrans-1825
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: Let’s unpack this line by line.
  id: totrans-1826
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐行分析这一行。
- en: 'Step 1: Starting from the loss'
  id: totrans-1827
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第 1 步：从损失开始
- en: The journey begins with the loss function. In training, the most common loss
    is cross-entropy. Its backward function compares the predicted probabilities with
    the true labels and produces a gradient for the logits.
  id: totrans-1828
  prefs: []
  type: TYPE_NORMAL
  zh: 旅程从损失函数开始。在训练中，最常用的损失是交叉熵。其反向函数比较预测概率与真实标签，并为 logits 生成梯度。
- en: If the model predicted “cat” with high confidence and the true label was “dog,”
    the gradient will push the logits away from “cat” and toward “dog.”
  id: totrans-1829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果模型以高置信度预测“猫”，而真实标签是“狗”，则梯度会将 logits 推离“猫”并朝向“狗”。
- en: This gradient is the starting signal that propagates backward through the entire
    network.
  id: totrans-1830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个梯度是传播整个网络的反向传播的起始信号。
- en: 'Step 2: Back through the output head'
  id: totrans-1831
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第 2 步：反向通过输出头
- en: 'After the loss, the next stop is the final linear projection (`lm_head`). This
    is just a big matrix multiply that turns hidden states into vocabulary logits.
    Its backward function computes two things:'
  id: totrans-1832
  prefs: []
  type: TYPE_NORMAL
  zh: 损失之后，下一个目的地是最终的线性投影（`lm_head`）。这只是一个将隐藏状态转换为词汇对数的大矩阵乘法。它的反向函数计算两件事：
- en: The gradient with respect to the weights of `lm_head`.
  id: totrans-1833
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与`lm_head`权重相关的梯度。
- en: The gradient with respect to the hidden states that fed into it.
  id: totrans-1834
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与其输入的隐藏状态相关的梯度。
- en: This hidden-state gradient is then passed back to the last transformer block.
  id: totrans-1835
  prefs: []
  type: TYPE_NORMAL
  zh: 这个隐藏状态梯度随后被传递回最后一个变换器块。
- en: 'Step 3: Transformer blocks in reverse'
  id: totrans-1836
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第3步：反向的变换器块
- en: Here comes the heavy lifting. Each block has multiple components, and their
    backward functions are called in the exact opposite order of the forward pass.
  id: totrans-1837
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是重头戏。每个块都有多个组件，它们的反向函数以与正向传播完全相反的顺序调用。
- en: 'Residual backward: the skip connection splits the gradient into two paths -
    one flowing back into the transformed output, one flowing back into the original
    input.'
  id: totrans-1838
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 剩余反向传播：跳跃连接将梯度分为两条路径——一条流向转换后的输出，一条流向原始输入。
- en: 'LayerNorm backward: computes gradients with respect to its scale (`gamma`)
    and shift (`beta`), and also passes gradients back to the normalized input.'
  id: totrans-1839
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LayerNorm反向传播：计算与其缩放（`gamma`）和偏移（`beta`）相关的梯度，并将梯度传递回归一化输入。
- en: 'MLP backward: applies the chain rule to the two linear layers and the GELU
    activation. The code reuses temporary values from the forward pass (like activations)
    to make this efficient.'
  id: totrans-1840
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: MLP反向传播：将链式法则应用于两个线性层和GELU激活。代码重用了正向传播中的临时值（如激活），以提高效率。
- en: 'Attention backward: this is the trickiest. It computes gradients for Q, K,
    and V projections, as well as for the softmaxed attention weights. It has to apply
    the causal mask again to ensure no illegal gradient flows.'
  id: totrans-1841
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意力反向传播：这是最棘手的。它计算Q、K和V投影的梯度，以及softmax后的注意力权重。它必须再次应用因果掩码，以确保没有非法的梯度流动。
- en: This loop continues until all transformer blocks have been processed.
  id: totrans-1842
  prefs: []
  type: TYPE_NORMAL
  zh: 这个循环会一直持续到所有变换器块都被处理。
- en: 'Step 4: Back to embeddings'
  id: totrans-1843
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第4步：回到嵌入
- en: Finally, the gradient reaches the embedding tables. This is where the model
    first looked up vectors for tokens and positions. Now it calculates how much each
    embedding contributed to the error. These gradients are added into the embedding
    matrices, telling the optimizer how to update them.
  id: totrans-1844
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，梯度到达嵌入表。这是模型首次查找标记和位置向量的地方。现在它计算每个嵌入对误差的贡献程度。这些梯度被添加到嵌入矩阵中，告诉优化器如何更新它们。
- en: Why this matters
  id: totrans-1845
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 这为什么重要
- en: The backward pass is what makes learning possible. Without it, the model would
    forever output the same predictions, never improving. By flowing “blame” backwards,
    each parameter learns how to nudge itself so that the next forward pass is a little
    bit better.
  id: totrans-1846
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播是使学习成为可能的原因。没有它，模型将永远输出相同的预测，永远不会改进。通过将“责任”向后传递，每个参数学习如何微调自己，以便下一次正向传播变得更好。
- en: 'Even though the code looks like a lot of function calls, the principle is simple:
    start from the loss, step backward through each layer, apply the chain rule locally,
    and collect gradients.'
  id: totrans-1847
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管代码看起来像很多函数调用，但原理很简单：从损失开始，逐层反向传播，局部应用链式法则，并收集梯度。
- en: Try it yourself
  id: totrans-1848
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 试试看
- en: 'Print gradient norms: Add a `printf` to see the average gradient magnitude
    at each layer. Notice how they change - sometimes exploding, sometimes vanishing.'
  id: totrans-1849
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印梯度范数：添加一个`printf`来查看每层的平均梯度幅度。注意它们是如何变化的——有时爆炸，有时消失。
- en: 'Freeze a layer: Comment out `mlp_backward` for one block and see how the model
    fails to update properly.'
  id: totrans-1850
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 冻结一层：注释掉一个块的`mlp_backward`，看看模型如何无法正确更新。
- en: 'Inspect embeddings: After training a few steps, dump a few rows of the token
    embedding matrix. You’ll see the numbers changing because of gradient updates.'
  id: totrans-1851
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查嵌入：训练了几步之后，输出几行标记嵌入矩阵。你会看到数字因为梯度更新而改变。
- en: 'Tiny dataset experiment: Train on a very small dataset (like a 10-word corpus)
    and watch how the backward pass quickly pushes embeddings to memorize it.'
  id: totrans-1852
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 小数据集实验：在一个非常小的数据集（如10词语料库）上训练，并观察反向传播如何迅速推动嵌入来记住它。
- en: 'Check symmetry: Compare the order of calls in `gpt2_forward` with `gpt2_backward`.
    They’re exact opposites - forward builds, backward unbuilds.'
  id: totrans-1853
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查对称性：比较`gpt2_forward`和`gpt2_backward`中的调用顺序。它们是相反的——正向构建，反向拆解。
- en: The takeaway
  id: totrans-1854
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 要点
- en: Backpropagation is the learning engine of neural networks. In `llm.c`, the backward
    pass is written out explicitly, showing how gradients flow from the loss, through
    the output head, back through every transformer block, and finally into embeddings.
    Once you understand this flow, you can see how training stitches forward and backward
    together to slowly shape a random model into a working language model.
  id: totrans-1855
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播是神经网络的学习引擎。在 `llm.c` 中，反向传播被明确写出，展示了梯度如何从损失，通过输出头部，反向通过每个转换器块，最终进入嵌入层。一旦你理解了这个流程，你就可以看到训练是如何将正向和反向步骤结合在一起，逐渐将一个随机的模型塑造成一个有效的语言模型。
- en: 42\. Skeleton of Training Loop
  id: totrans-1856
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 42. 训练循环的骨架
- en: 'The backward pass gave us gradients, but gradients by themselves don’t train
    a model. Training requires a loop: a cycle that repeatedly runs forward, backward,
    and update steps over and over until the model improves. This cycle is called
    the training loop, and it is the heartbeat of every deep learning program. In
    `train_gpt2.c`, the loop is written explicitly in C, which means you can see every
    piece instead of it being hidden away in a framework.'
  id: totrans-1857
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播为我们提供了梯度，但仅仅梯度本身并不能训练模型。训练需要一个循环：一个反复运行正向、反向和更新步骤的循环，直到模型改进。这个循环被称为训练循环，它是每个深度学习程序的心跳。在
    `train_gpt2.c` 中，循环是用 C 明确编写的，这意味着你可以看到每一部分，而不是它被隐藏在框架中。
- en: The basic rhythm
  id: totrans-1858
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基本节奏
- en: 'Every training step follows the same rhythm:'
  id: totrans-1859
  prefs: []
  type: TYPE_NORMAL
  zh: 每个训练步骤都遵循相同的节奏：
- en: Get a batch of data (input tokens and their labels).
  id: totrans-1860
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取一批数据（输入标记及其标签）。
- en: Run the forward pass to compute predictions and loss.
  id: totrans-1861
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行正向传播来计算预测和损失。
- en: Run the backward pass to compute gradients.
  id: totrans-1862
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行反向传播来计算梯度。
- en: Update weights using an optimizer like AdamW.
  id: totrans-1863
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 AdamW 等优化器更新权重。
- en: Log progress and, occasionally, validate.
  id: totrans-1864
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记录进度，偶尔进行验证。
- en: This rhythm repeats thousands or millions of times. With each repetition, the
    weights shift slightly, nudging the model toward lower loss and better predictions.
  id: totrans-1865
  prefs: []
  type: TYPE_NORMAL
  zh: 这种节奏会重复数千或数百万次。每次重复，权重都会略有变化，推动模型朝着更低的损失和更好的预测方向前进。
- en: How the loop looks in code
  id: totrans-1866
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 循环在代码中的样子
- en: 'Here’s a simplified sketch from `train_gpt2.c` (with some details omitted for
    clarity):'
  id: totrans-1867
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是 `train_gpt2.c`（为了清晰起见省略了一些细节）的简化草图：
- en: '[PRE100]'
  id: totrans-1868
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'This loop captures the full training lifecycle: data, forward, backward, update,
    and monitoring.'
  id: totrans-1869
  prefs: []
  type: TYPE_NORMAL
  zh: 这个循环捕捉了完整的训练生命周期：数据、正向传播、反向传播、更新和监控。
- en: 'Step 1: Batching data'
  id: totrans-1870
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤 1：批量数据
- en: The dataloader feeds the loop with small chunks of tokens. Instead of sending
    the whole dataset at once, it breaks it down into batches of size `B` (number
    of sequences per batch) and length `T` (number of tokens per sequence).
  id: totrans-1871
  prefs: []
  type: TYPE_NORMAL
  zh: 数据加载器将小块标记喂入循环。它不是一次性发送整个数据集，而是将其分解为大小为 `B`（每个批次的序列数）和长度为 `T`（每个序列的标记数）的批次。
- en: 'Example: if `B=4` and `T=128`, each batch is 512 tokens long.'
  id: totrans-1872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例：如果 `B=4` 且 `T=128`，则每个批次包含 512 个标记。
- en: Each sequence has a matching set of labels, which are simply the same tokens
    shifted one position ahead (so the model always predicts the *next* word).
  id: totrans-1873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个序列都有一个匹配的标签集，这些标签只是将相同的标记向前移动一个位置（因此模型总是预测*下一个*单词）。
- en: This batching keeps memory use manageable and helps the model see many small
    samples instead of a few giant ones.
  id: totrans-1874
  prefs: []
  type: TYPE_NORMAL
  zh: 这种批量处理使内存使用保持可控，并帮助模型看到许多小样本而不是少数大样本。
- en: 'Step 2: Forward pass'
  id: totrans-1875
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤 2：正向传播
- en: The forward pass computes predictions for all tokens in the batch and calculates
    the loss. This is the “evaluation” step - how well did the model do on this batch?
    The result is stored in `model.loss`.
  id: totrans-1876
  prefs: []
  type: TYPE_NORMAL
  zh: 正向传播计算批次中所有标记的预测并计算损失。这是“评估”步骤——模型在这个批次上做得怎么样？结果是存储在 `model.loss` 中。
- en: 'Step 3: Zeroing gradients'
  id: totrans-1877
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤 3：梯度归零
- en: 'Before calculating new gradients, the old ones must be cleared out. If you
    skip this step, gradients from previous batches would accumulate and corrupt the
    update. In frameworks like PyTorch you’d call `optimizer.zero_grad()`. Here it’s
    a plain C function:'
  id: totrans-1878
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算新的梯度之前，必须清除旧的梯度。如果你跳过这一步，先前批次的梯度会累积并破坏更新。在 PyTorch 等框架中，你会调用 `optimizer.zero_grad()`。这里它是一个普通的
    C 函数：
- en: '[PRE101]'
  id: totrans-1879
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: It walks through all parameters and resets their gradient buffers to zero.
  id: totrans-1880
  prefs: []
  type: TYPE_NORMAL
  zh: 它遍历所有参数并将它们的梯度缓冲区重置为零。
- en: 'Step 4: Backward pass'
  id: totrans-1881
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤 4：反向传播
- en: Now the backward function is called. It pushes gradients back through the network,
    computing how each weight influenced the error. At this point, every parameter
    has an associated gradient stored in memory.
  id: totrans-1882
  prefs: []
  type: TYPE_NORMAL
  zh: 现在调用反向函数。它将梯度反向推回网络，计算每个权重如何影响误差。此时，每个参数都有一个存储在内存中的相关梯度。
- en: 'Step 5: Optimizer update'
  id: totrans-1883
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤 5：优化器更新
- en: 'With gradients ready, the optimizer (AdamW in this code) updates each parameter:'
  id: totrans-1884
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好梯度后，优化器（本代码中的AdamW）更新每个参数：
- en: '[PRE102]'
  id: totrans-1885
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: This step is what actually changes the model. Without it, the model would never
    learn - the forward and backward passes would just repeat the same results forever.
  id: totrans-1886
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步实际上改变了模型。没有它，模型将永远无法学习——正向和反向传递将永远重复相同的结果。
- en: 'Step 6: Logging and validation'
  id: totrans-1887
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第6步：记录和验证
- en: 'Every few steps, the loop prints out useful numbers: current step, loss, time
    taken, and sometimes throughput (tokens per second). This feedback is important
    to check whether training is actually working.'
  id: totrans-1888
  prefs: []
  type: TYPE_NORMAL
  zh: 每过几步，循环会打印出有用的数字：当前步骤、损失、所用时间，有时还有吞吐量（每秒的token数量）。这种反馈对于检查训练是否真正有效非常重要。
- en: Every few hundred or thousand steps, the loop also runs a validation pass on
    held-out data. This tells you whether the model is just memorizing training data
    or genuinely learning patterns that generalize.
  id: totrans-1889
  prefs: []
  type: TYPE_NORMAL
  zh: 每过几百或几千步，循环还会在保留的数据上运行一个验证过程。这告诉你模型是否只是在记忆训练数据，或者真正地学习了一般化的模式。
- en: Why the training loop matters
  id: totrans-1890
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么训练循环很重要
- en: 'The training loop is deceptively simple, but it is the engine room of machine
    learning. Every improvement in model performance happens because this loop runs
    many times. By writing it explicitly in C, `llm.c` exposes details that high-level
    frameworks usually hide: zeroing gradients, passing pointers to arrays, calling
    backward and optimizer functions directly.'
  id: totrans-1891
  prefs: []
  type: TYPE_NORMAL
  zh: 训练循环看似简单，但它是机器学习的引擎室。模型性能的每一次改进都是因为这个循环多次运行的结果。通过在C中明确编写它，`llm.c`揭示了高级框架通常隐藏的细节：归零梯度、传递数组指针、直接调用反向和优化器函数。
- en: 'This makes it a perfect learning tool. You can see clearly:'
  id: totrans-1892
  prefs: []
  type: TYPE_NORMAL
  zh: 这使其成为一个完美的学习工具。你可以清楚地看到：
- en: Where the data comes in,
  id: totrans-1893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据从哪里进来，
- en: Where predictions are made,
  id: totrans-1894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测在哪里进行，
- en: Where gradients are calculated,
  id: totrans-1895
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度在哪里计算，
- en: And where learning actually happens.
  id: totrans-1896
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 而学习实际上发生在这里。
- en: Try it yourself
  id: totrans-1897
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 亲自尝试
- en: 'Print the loss curve: Add a `printf` inside the loop and write the loss to
    a file. Plot it - you should see it decrease over time.'
  id: totrans-1898
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印损失曲线：在循环中添加`printf`并将损失写入文件。绘制它——你应该会看到它随时间下降。
- en: 'Change batch size: Set `B=1` vs. `B=8`. Notice how the loop becomes noisier
    with smaller batches but smoother with larger ones.'
  id: totrans-1899
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 改变批量大小：设置`B=1`与`B=8`。注意，随着批量大小的减小，循环变得更嘈杂，而批量大小时则更平滑。
- en: 'Skip backward: Comment out `gpt2_backward` and optimizer update. Run the loop.
    You’ll see the loss never decreases - a clear demonstration that forward alone
    doesn’t train.'
  id: totrans-1900
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 跳过反向：注释掉`gpt2_backward`和优化器更新。运行循环。你会看到损失永远不会下降——这是正向训练不有效的明显证明。
- en: 'Experiment with steps: Try `max_steps=10` vs. `max_steps=1000`. Short runs
    show no improvement; longer runs start to reduce the loss.'
  id: totrans-1901
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试不同的步数：尝试`max_steps=10`与`max_steps=1000`。短运行没有改进；长运行开始减少损失。
- en: 'Slow it down: Insert a `sleep(1);` inside the loop. This makes the rhythm visible
    step by step, so you can literally watch the model “breathe” as it trains.'
  id: totrans-1902
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 慢下来：在循环中插入`sleep(1);`。这使得节奏一步一步地变得可见，因此你可以真正地看到模型在训练时的“呼吸”。
- en: The takeaway
  id: totrans-1903
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸取的经验
- en: The skeleton of the training loop is the core cycle of learning. It feeds data
    into the model, computes predictions, finds errors, sends them backward, updates
    weights, and logs progress. Everything else - optimizers, schedulers, distributed
    training, mixed precision - is just an enhancement of this basic loop. If you
    understand how this loop works in `llm.c`, you understand the beating heart of
    deep learning training.
  id: totrans-1904
  prefs: []
  type: TYPE_NORMAL
  zh: 训练循环的骨架是学习的核心循环。它将数据输入模型，计算预测，找出错误，将它们反向传递，更新权重，并记录进度。其他所有东西——优化器、调度器、分布式训练、混合精度——只是对这个基本循环的增强。如果你理解了`llm.c`中这个循环的工作原理，你就理解了深度学习训练的跳动心脏。
- en: 43\. AdamW Implementation in C
  id: totrans-1905
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 43. C语言中的AdamW实现
- en: Training a neural network is about adjusting millions of parameters so that
    the model gradually becomes better at predicting text. The function `gpt2_update`
    in `train_gpt2.c` is responsible for this adjustment. It implements the AdamW
    optimizer, one of the most widely used algorithms in deep learning. Let’s walk
    through both the theory and the actual implementation.
  id: totrans-1906
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络就是调整数百万个参数，使模型逐渐变得擅长预测文本。`train_gpt2.c`中的`gpt2_update`函数负责这一调整。它实现了AdamW优化器，这是深度学习中应用最广泛的算法之一。让我们一起来探讨其理论和实际实现。
- en: From Gradient Descent to AdamW
  id: totrans-1907
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 从梯度下降到AdamW
- en: 'The most basic optimizer is gradient descent:'
  id: totrans-1908
  prefs: []
  type: TYPE_NORMAL
  zh: 最基本的优化器是梯度下降：
- en: '[PRE103]'
  id: totrans-1909
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'This approach works, but it has weaknesses. The step size (learning rate) must
    be tuned carefully: too small and training is slow, too large and training diverges.
    Moreover, all parameters use the same step size, even though some may need gentler
    updates.'
  id: totrans-1910
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有效，但它有弱点。步长（学习率）必须仔细调整：太小则训练缓慢，太大则训练发散。此外，所有参数使用相同的步长，尽管某些参数可能需要更温和的更新。
- en: AdamW improves this by keeping track of moving averages of gradients and scaling
    updates adaptively. It also introduces weight decay, which prevents parameters
    from growing too large and helps regularize the model.
  id: totrans-1911
  prefs: []
  type: TYPE_NORMAL
  zh: AdamW 通过跟踪梯度的移动平均值并自适应地缩放更新来改进这一点。它还引入了权重衰减，这防止参数变得过大，并有助于正则化模型。
- en: How AdamW Works
  id: totrans-1912
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: AdamW 的工作原理
- en: 'AdamW combines several techniques into a single update rule. First, it uses
    momentum: instead of relying only on the current gradient, it averages recent
    gradients. This smooths noisy updates. Second, it maintains a running estimate
    of the squared gradient, which scales down steps in directions where gradients
    are consistently large. These are sometimes called the first and second moments.'
  id: totrans-1913
  prefs: []
  type: TYPE_NORMAL
  zh: AdamW 将几种技术结合成一个单一的更新规则。首先，它使用动量：它不仅依赖于当前的梯度，还平均了最近的梯度。这平滑了噪声更新。其次，它维护一个平方梯度的运行估计，在梯度持续较大的方向上缩小步长。这些有时被称为第一和第二矩。
- en: Since both running averages start at zero, the algorithm applies bias correction
    during the first few steps. Without this, the early updates would be too small.
    Finally, AdamW applies weight decay directly in the update, shrinking parameter
    values slightly each step.
  id: totrans-1914
  prefs: []
  type: TYPE_NORMAL
  zh: 由于两个运行平均值都是从零开始的，算法在最初的几步中应用偏差校正。如果没有这个，早期的更新会太小。最后，AdamW 在更新中直接应用权重衰减，每次略微缩小参数值。
- en: 'Putting it together, each parameter update looks like this:'
  id: totrans-1915
  prefs: []
  type: TYPE_NORMAL
  zh: 将其组合，每个参数更新看起来像这样：
- en: '[PRE104]'
  id: totrans-1916
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: Here `m` is momentum, `v` is variance, `lr` is learning rate, `ε` is a small
    constant for stability, and `λ` is the weight decay factor.
  id: totrans-1917
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里 `m` 是动量，`v` 是方差，`lr` 是学习率，`ε` 是一个小的常数，用于稳定性，而 `λ` 是权重衰减因子。
- en: The Implementation in `train_gpt2.c`
  id: totrans-1918
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在 `train_gpt2.c` 中的实现
- en: '[PRE105]'
  id: totrans-1919
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: The first `if` block allocates memory for the moving averages `m` and `v` the
    first time the optimizer runs. Then, for each parameter, the code computes the
    new averages, applies bias correction, and finally updates the parameter with
    the AdamW formula.
  id: totrans-1920
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个 `if` 块在优化器第一次运行时为移动平均值 `m` 和 `v` 分配内存。然后，对于每个参数，代码计算新的平均值，应用偏差校正，并最终使用 AdamW
    公式更新参数。
- en: Example Walkthrough
  id: totrans-1921
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例演练
- en: 'Suppose we have a parameter `w = 0.5` with gradient `g = 0.2` on the first
    training step. Using β1 = 0.9 and β2 = 0.999:'
  id: totrans-1922
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们在第一次训练步骤中有一个参数 `w = 0.5` 和梯度 `g = 0.2`。使用 β1 = 0.9 和 β2 = 0.999：
- en: 'Momentum:'
  id: totrans-1923
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动量：
- en: '[PRE106]'
  id: totrans-1924
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'Variance:'
  id: totrans-1925
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方差：
- en: '[PRE107]'
  id: totrans-1926
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'Bias correction:'
  id: totrans-1927
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏差校正：
- en: '[PRE108]'
  id: totrans-1928
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'Final update (lr = 0.001, weight_decay = 0.01):'
  id: totrans-1929
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终更新（lr = 0.001，weight_decay = 0.01）：
- en: '[PRE109]'
  id: totrans-1930
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE109]'
- en: So the parameter becomes `w = 0.498995`.
  id: totrans-1931
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，参数变为 `w = 0.498995`。
- en: Intuition
  id: totrans-1932
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 直觉
- en: Think of a ball rolling down a slope. The gradient is the slope itself. Momentum
    makes the ball keep rolling even if the slope flattens briefly. The variance term
    makes the ball slow down on rocky ground where the slope changes rapidly. Bias
    correction ensures the ball doesn’t move too timidly at the start. Weight decay
    adds friction so the ball doesn’t roll out of control.
  id: totrans-1933
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个球沿着斜坡滚动。梯度就是斜坡本身。动量使球即使在斜坡短暂变平的情况下也能继续滚动。方差项使球在斜坡变化迅速的崎岖地面上减速。偏差校正确保球在开始时不会过于胆怯。权重衰减增加摩擦，使球不会失控滚动。
- en: Why It Matters
  id: totrans-1934
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: Optimizers are the difference between a model that trains smoothly and one that
    diverges or gets stuck. AdamW became popular because it combines stability with
    efficiency. It automatically adapts to each parameter’s scale, reduces the need
    for manual learning rate tuning, and includes weight decay in a principled way.
    For GPT-style models with hundreds of millions of parameters, these qualities
    make training feasible.
  id: totrans-1935
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器是模型平稳训练与发散或卡住之间的区别。AdamW 因其结合了稳定性和效率而变得流行。它自动适应每个参数的规模，减少了手动调整学习率的必要性，并以一种原则性的方式包括权重衰减。对于具有数亿参数的
    GPT 风格模型，这些特性使得训练变得可行。
- en: Try It Yourself
  id: totrans-1936
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 试试看
- en: Change the learning rate from `0.001` to `0.01` in the code and see how quickly
    the model diverges.
  id: totrans-1937
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在代码中将学习率从 `0.001` 改为 `0.01`，看看模型会多快发散。
- en: Set `weight_decay = 0` and compare validation loss after a few epochs. The model
    might overfit more quickly.
  id: totrans-1938
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `weight_decay` 设置为 `0` 并在几个epoch后比较验证损失。模型可能会更快地过拟合。
- en: Print out the first 10 values of `m_memory` and `v_memory` during training to
    watch how they evolve over steps.
  id: totrans-1939
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练期间打印出`m_memory`和`v_memory`的前10个值，以观察它们随步骤的变化。
- en: Replace AdamW with plain SGD (just `param -= lr * grad`) and compare training
    speed and stability.
  id: totrans-1940
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将AdamW替换为普通的SGD（只是`param -= lr * grad`）并比较训练速度和稳定性。
- en: Experiment with β1 = 0 (no momentum) or β2 = 0 (no variance smoothing) and see
    how noisy updates become.
  id: totrans-1941
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试β1 = 0（无动量）或β2 = 0（无方差平滑）并看看更新变得多嘈杂。
- en: The Takeaway
  id: totrans-1942
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**总结**'
- en: AdamW provides a balance of speed, stability, and generalization. In practice,
    it allows models like GPT-2 to train much more reliably than with vanilla gradient
    descent. The C implementation in `llm.c` demonstrates that beneath the math, it’s
    just a simple loop applying a few arithmetic operations for each parameter.
  id: totrans-1943
  prefs: []
  type: TYPE_NORMAL
  zh: AdamW在速度、稳定性和泛化能力之间提供了平衡。在实践中，它允许像GPT-2这样的模型比使用标准梯度下降训练得更加可靠。`llm.c`中的C实现表明，在数学之下，它只是一个简单的循环，为每个参数应用几个算术运算。
- en: 44\. Gradient Accumulation and Micro-Batching
  id: totrans-1944
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 44. 梯度累积与微批量
- en: 'Modern language models are enormous, and so are the batches of text we would
    like to feed them during training. But real hardware has limits: a single GPU
    or CPU may not have enough memory to process a large batch in one go. To solve
    this, training code often uses gradient accumulation and micro-batching. Both
    ideas allow us to simulate training with larger batches without requiring more
    memory than our hardware can provide.'
  id: totrans-1945
  prefs: []
  type: TYPE_NORMAL
  zh: 现代语言模型非常庞大，我们希望在训练期间提供给它们的文本批量也同样庞大。但实际硬件有限：单个GPU或CPU可能没有足够的内存一次性处理一个大批量。为了解决这个问题，训练代码通常使用梯度累积和微批量。这两个想法都允许我们在不要求比我们的硬件能提供的更多内存的情况下模拟使用较大批量的训练。
- en: What Problem Are We Solving?
  id: totrans-1946
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**我们正在解决什么问题？**'
- en: When you process a batch of data, you run forward and backward passes to calculate
    gradients. If your batch size is very large, you get smoother gradients (less
    noisy), which often helps the model converge better. But large batches may not
    fit in memory.
  id: totrans-1947
  prefs: []
  type: TYPE_NORMAL
  zh: 当您处理一个数据批量时，您会运行正向和反向传递来计算梯度。如果您的批量大小非常大，您会得到更平滑的梯度（更少噪声），这通常有助于模型更好地收敛。但是，大批量可能不适合内存。
- en: Imagine trying to train with a batch of 1024 sequences on a GPU that can only
    handle 128 sequences at once. Without tricks, you would be forced to use the smaller
    batch size and give up the benefits of larger batches. Gradient accumulation fixes
    this problem by letting you split the big batch into smaller micro-batches, process
    them one at a time, and accumulate the results as if you had processed the big
    batch all at once.
  id: totrans-1948
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，在一个只能一次处理128个序列的GPU上，尝试用包含1024个序列的批量进行训练。如果没有技巧，您将不得不使用较小的批量大小，从而放弃大批量的好处。梯度累积通过允许您将大批量拆分成更小的微批量，一次处理一个，并像一次性处理大批量一样累积结果，来解决这个问题。
- en: How It Works in Practice
  id: totrans-1949
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**实际应用中的工作原理**'
- en: 'Let’s say we want an effective batch size of 1024, but our hardware only supports
    128\. We split the big batch into 8 micro-batches of 128 each:'
  id: totrans-1950
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要一个有效的批量大小为1024，但我们的硬件只支持128。我们将大批量拆分成8个每个128的微批量：
- en: Run forward + backward on micro-batch 1, store the gradients.
  id: totrans-1951
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在微批量1上运行正向+反向，并存储梯度。
- en: Run forward + backward on micro-batch 2, add its gradients to the stored ones.
  id: totrans-1952
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在微批量2上运行正向+反向，并将其梯度添加到存储的梯度中。
- en: Repeat until all 8 micro-batches are processed.
  id: totrans-1953
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复直到处理完所有8个微批量。
- en: Once gradients for all 8 are accumulated, perform the optimizer update.
  id: totrans-1954
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦所有8个微批量的梯度都累积完毕，执行优化器更新。
- en: 'The important part is step 4: we only update the parameters once per effective
    batch, not after each micro-batch. This preserves the effect of training with
    a large batch.'
  id: totrans-1955
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是第4步：我们只在每个有效批量更新一次参数，而不是在每个微批量之后。这保留了使用大批量训练的效果。
- en: Pseudocode Example
  id: totrans-1956
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**伪代码示例**'
- en: 'Here’s how this might look in simplified pseudocode:'
  id: totrans-1957
  prefs: []
  type: TYPE_NORMAL
  zh: 这在简化的伪代码中可能看起来是这样的：
- en: '[PRE110]'
  id: totrans-1958
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: Notice how the optimizer only runs once per outer loop iteration, even though
    gradients were accumulated across multiple micro-batches.
  id: totrans-1959
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，尽管梯度是在多个微批量之间累积的，但优化器只在外部循环迭代一次运行。
- en: Why Gradient Accumulation Helps
  id: totrans-1960
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**为什么梯度累积有帮助**'
- en: 'Memory efficiency: You can train with larger effective batch sizes without
    needing more hardware.'
  id: totrans-1961
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内存效率**：您可以在不需要更多硬件的情况下使用较大的有效批量大小进行训练。'
- en: 'Training stability: Larger batches reduce the variance of gradients, making
    training less noisy.'
  id: totrans-1962
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练稳定性：较大的批量减少了梯度的方差，使得训练更少噪声。
- en: 'Flexibility: You can scale effective batch size up or down depending on your
    needs without changing hardware.'
  id: totrans-1963
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灵活性**：根据您的需求，您可以调整有效的批量大小，而无需更改硬件。'
- en: Micro-Batching vs. Accumulation
  id: totrans-1964
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 微批处理与累积的比较
- en: 'Micro-batching refers to the act of splitting a batch into smaller parts. Gradient
    accumulation is what you do after micro-batching: sum up the gradients across
    those parts. Together, they allow you to simulate training with any batch size
    you want, within memory constraints.'
  id: totrans-1965
  prefs: []
  type: TYPE_NORMAL
  zh: 微批处理是指将批次分成更小部分的行为。梯度累积是在微批处理之后进行的：求和这些部分的梯度。一起，它们允许你在内存限制内模拟任何批大小的训练。
- en: Why It Matters
  id: totrans-1966
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: The quality of training often depends on batch size. If you can’t fit a large
    batch directly, gradient accumulation ensures you still reap the benefits. It’s
    one of those “engineering hacks” that makes training state-of-the-art models possible
    on limited resources.
  id: totrans-1967
  prefs: []
  type: TYPE_NORMAL
  zh: 训练质量通常取决于批大小。如果你不能直接放入一个大的批，梯度累积确保你仍然能获得好处。这是那些“工程技巧”之一，使得在有限的资源上训练最先进的模型成为可能。
- en: Try It Yourself
  id: totrans-1968
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 亲自尝试
- en: Run training with batch size = 16 and no accumulation. Watch how noisy the loss
    curve looks.
  id: totrans-1969
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用批大小 = 16 和无累积运行训练。观察损失曲线看起来有多嘈杂。
- en: Now set micro-batch size = 4 and accumulation_steps = 4\. This simulates batch
    size = 16, but in smaller chunks. Compare the loss curve.
  id: totrans-1970
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在设置微批大小 = 4 和 accumulation_steps = 4。这模拟了批大小 = 16，但以更小的块进行。比较损失曲线。
- en: Increase accumulation_steps to simulate batch size = 32\. Observe if training
    becomes smoother.
  id: totrans-1971
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 accumulation_steps 增加到模拟批大小 = 32。观察训练是否变得更加平滑。
- en: Experiment with turning accumulation off and on while keeping the same effective
    batch size. Notice how optimizer updates per epoch differ.
  id: totrans-1972
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在保持相同有效批大小的同时，尝试开启和关闭累积。注意每个epoch中优化器更新的差异。
- en: Print out how many times the optimizer is called. With accumulation, it should
    be fewer than the number of micro-batches.
  id: totrans-1973
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出优化器被调用的次数。使用累积，它应该少于微批次的数量。
- en: The Takeaway
  id: totrans-1974
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸取的经验
- en: 'Gradient accumulation and micro-batching are techniques that let you train
    with large effective batch sizes while staying within the limits of your hardware.
    They preserve the benefits of large batches-stability and smoother gradients-without
    demanding extra memory. In `llm.c`, the simplicity of the training loop means
    you can clearly see where accumulation fits: gradients are summed across micro-batches,
    and only then does the optimizer step in. This is a small adjustment in code but
    a huge enabler in practice.'
  id: totrans-1975
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度累积和微批处理是让你在硬件限制内使用大有效批大小进行训练的技术。它们保留了大型批次的优点——稳定性和更平滑的梯度——而不需要额外的内存。在`llm.c`中，训练循环的简单性意味着你可以清楚地看到累积是如何适应的：梯度在微批次之间求和，然后优化器才介入。这在代码上是一个小的调整，但在实践中是一个巨大的推动力。
- en: 45\. Logging and Progress Reporting
  id: totrans-1976
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 45. 日志和进度报告
- en: 'Every training loop needs a way to show what’s happening under the hood. Without
    logs, you wouldn’t know if the model is improving, if the code is running efficiently,
    or if something has silently gone wrong. In `train_gpt2.c`, logging is intentionally
    minimal but highly informative: each training step prints the step number, the
    current training loss, and how long that step took to run.'
  id: totrans-1977
  prefs: []
  type: TYPE_NORMAL
  zh: 每个训练循环都需要一种方式来显示底层正在发生的事情。没有日志，你就不知道模型是否在改进，代码是否运行高效，或者是否有什么问题悄然发生。在`train_gpt2.c`中，日志有意被保持最小化但高度信息丰富：每个训练步骤会打印出步骤编号、当前的训练损失以及该步骤运行所需的时间。
- en: The Real Code for Logging
  id: totrans-1978
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实际的日志代码
- en: 'Here’s the relevant snippet from `train_gpt2.c`:'
  id: totrans-1979
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是`train_gpt2.c`中的相关代码片段：
- en: '[PRE111]'
  id: totrans-1980
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: 'This small block accomplishes two things:'
  id: totrans-1981
  prefs: []
  type: TYPE_NORMAL
  zh: 这个小块完成了两件事：
- en: It measures how long the training step took using `clock_gettime`.
  id: totrans-1982
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它使用`clock_gettime`测量训练步骤的持续时间。
- en: It reports the step number, the loss, and the elapsed time in milliseconds.
  id: totrans-1983
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它报告了步骤编号、损失和经过的时间（以毫秒为单位）。
- en: 'The output looks like this when training:'
  id: totrans-1984
  prefs: []
  type: TYPE_NORMAL
  zh: 训练时的输出看起来是这样的：
- en: '[PRE112]'
  id: totrans-1985
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: Understanding What’s Reported
  id: totrans-1986
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 理解所报告的内容
- en: Step number (`step`) Tells you where you are in training. Since deep learning
    often runs for thousands of steps, this acts like a progress bar.
  id: totrans-1987
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步骤编号（`step`）告诉你训练的进度。由于深度学习通常需要运行数千步，这就像一个进度条。
- en: Training loss (`model.mean_loss`) Shows how well the model is fitting the training
    batch. A lower value generally means better predictions. Watching this number
    decrease over time is the main signal that learning is happening.
  id: totrans-1988
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练损失（`model.mean_loss`）显示了模型如何拟合训练批次。较低的值通常意味着更好的预测。随着时间的推移观察这个数字的下降是学习正在发生的最主要信号。
- en: Step duration (`time_elapsed_s * 1000`) Measures performance. If one step takes
    2000 ms, then 5000 steps would take about 3 hours. Monitoring this helps you estimate
    total training time and spot performance regressions (e.g., if a new change suddenly
    doubles the step time).
  id: totrans-1989
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步骤持续时间 (`time_elapsed_s * 1000`) 用于衡量性能。如果一步需要 2000 毫秒，那么 5000 步大约需要 3 个小时。监控这一点有助于你估计总训练时间并发现性能退化（例如，如果新的更改突然将步骤时间加倍）。
- en: Why It Matters
  id: totrans-1990
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: 'Logs are your window into the training process. If the loss goes down smoothly,
    training is healthy. If it suddenly spikes or stays flat, something is wrong-maybe
    the learning rate is too high, or the model has run out of capacity. Timing information
    also matters: you need to know whether the code is running efficiently or wasting
    cycles.'
  id: totrans-1991
  prefs: []
  type: TYPE_NORMAL
  zh: 日志是了解训练过程的窗口。如果损失平稳下降，则训练健康。如果它突然上升或保持平稳，则可能存在问题——可能是学习率太高，或者模型已经没有容量了。时间信息也很重要：你需要知道代码是否运行高效，或者是否在浪费周期。
- en: Try It Yourself
  id: totrans-1992
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 尝试自己操作
- en: Change the learning rate from `1e-4` to `1e-2` and watch how the loss behaves.
    If it jumps or becomes unstable, you’ll see it directly in the logs.
  id: totrans-1993
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将学习率从 `1e-4` 改为 `1e-2` 并观察损失的行为。如果它跳跃或变得不稳定，你将直接在日志中看到。
- en: Add validation logging by running the model on a held-out dataset every 100
    steps and printing `val_loss`. Compare it to `train_loss`.
  id: totrans-1994
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过每 100 步运行模型在保留的数据集上并打印 `val_loss` 来添加验证日志。将其与 `train_loss` 进行比较。
- en: 'Record the log output to a file with:'
  id: totrans-1995
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令将日志输出到文件：
- en: '[PRE113]'
  id: totrans-1996
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE113]'
- en: Then plot `train_loss` over steps in Python or Excel to visualize the curve.
  id: totrans-1997
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后在 Python 或 Excel 中绘制 `train_loss` 随步骤的变化曲线以可视化。
- en: 'Add throughput reporting: divide the batch size times sequence length (`B*T`)
    by the step time to print tokens per second. This gives a clearer sense of efficiency.'
  id: totrans-1998
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加吞吐量报告：通过将批量大小乘以序列长度 (`B*T`) 除以步骤时间来打印每秒的令牌数。这给出了更清晰的效率感。
- en: Try disabling `clock_gettime` and only print loss. Notice how much harder it
    becomes to judge performance without timing information.
  id: totrans-1999
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试禁用 `clock_gettime` 并只打印损失。注意没有时间信息时判断性能变得多么困难。
- en: The Takeaway
  id: totrans-2000
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸收要点
- en: 'Even the simplest logs can tell you a lot. With just a single line-step, loss,
    and duration-you know how fast training is, whether it’s converging, and how long
    it will take. In larger frameworks, this kind of information is often hidden behind
    dashboards and monitoring tools, but the core idea is the same: training is only
    useful if you can see and interpret its progress.'
  id: totrans-2001
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是最简单的日志也能告诉你很多信息。只需一行记录，包括步骤、损失和持续时间，你就能知道训练的速度如何，是否在收敛，以及需要多长时间。在更大的框架中，这类信息通常隐藏在仪表板和监控工具后面，但核心思想是相同的：只有当你能看见并解释其进度时，训练才有用。
- en: 46\. Validation Runs in the Training Loop
  id: totrans-2002
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 46. 验证运行在训练循环中
- en: When you train a model, it is not enough to look only at how well it does on
    the training data. The real test is whether the model has learned patterns that
    apply to new, unseen data. This is where validation comes in. Validation is like
    a quiz the model takes from time to time during training. It does not count toward
    learning-it is just a check to see how much the model has really understood.
  id: totrans-2003
  prefs: []
  type: TYPE_NORMAL
  zh: 当你训练一个模型时，仅仅看它在训练数据上的表现是不够的。真正的测试是模型是否学会了适用于新、未见数据的模式。这就是验证的作用。验证就像模型在训练过程中不时参加的测验。它不计入学习——它只是检查模型真正理解了多少。
- en: In `train_gpt2.c`, validation is built right into the training loop. Every so
    often, instead of updating weights, the program pauses and runs the model on a
    set of tokens it has never trained on. It then prints out the average validation
    loss. This number tells you if the model is actually generalizing, not just memorizing.
  id: totrans-2004
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `train_gpt2.c` 中，验证直接构建在训练循环中。每隔一段时间，程序会暂停更新权重，并在一组它从未训练过的令牌上运行模型。然后它会打印出平均验证损失。这个数字告诉你模型是否真正泛化，而不仅仅是记忆。
- en: How the validation code looks
  id: totrans-2005
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 验证代码的样貌
- en: 'Here is the actual block of code that handles validation:'
  id: totrans-2006
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是处理验证的实际代码块：
- en: '[PRE114]'
  id: totrans-2007
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: At first glance, this might look like just a few lines of C code. But behind
    it are several important ideas about how machine learning models are tested while
    they learn. Let’s go through this step by step.
  id: totrans-2008
  prefs: []
  type: TYPE_NORMAL
  zh: 初看，这看起来可能只是几行 C 语言代码。但背后是关于机器学习模型在训练过程中如何测试的几个重要思想。让我们一步步来看。
- en: Step-by-step explanation
  id: totrans-2009
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 逐步解释
- en: 'The first line checks whether it is time to run validation:'
  id: totrans-2010
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行检查是否是运行验证的时间：
- en: '[PRE115]'
  id: totrans-2011
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: This means that validation happens every 10 steps. The `%` operator is “modulo,”
    which returns the remainder of a division. If the step number is divisible by
    10 (like 0, 10, 20, 30), then the block runs. By spacing it out this way, validation
    does not slow training too much but still gives you regular updates.
  id: totrans-2012
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着验证每10步发生一次。`%` 运算符是“取模”，它返回除法的余数。如果步骤号能被10整除（如0、10、20、30），则执行该块。通过这种方式间隔执行，验证不会过多地减慢训练速度，但仍然会定期提供更新。
- en: 'Next, the code sets up a place to store the running total of the validation
    loss:'
  id: totrans-2013
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，代码设置了一个存储验证损失运行总量的地方：
- en: '[PRE116]'
  id: totrans-2014
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: 'Then it resets the validation dataloader:'
  id: totrans-2015
  prefs: []
  type: TYPE_NORMAL
  zh: 然后它重置验证数据加载器：
- en: '[PRE117]'
  id: totrans-2016
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: This makes sure the validation dataset starts from the beginning each time.
    That way, the results are consistent-you’re always checking the model on the same
    set of text, rather than starting from a random place.
  id: totrans-2017
  prefs: []
  type: TYPE_NORMAL
  zh: 这确保了验证数据集每次都是从开始处开始的。这样，结果是一致的——你总是在相同的文本集上检查模型，而不是从随机位置开始。
- en: 'Now comes the loop over validation batches:'
  id: totrans-2018
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是遍历验证批次的循环：
- en: '[PRE118]'
  id: totrans-2019
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: 'Here’s what’s happening inside:'
  id: totrans-2020
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在内部发生的事情：
- en: '`dataloader_next_batch` fetches the next chunk of tokens and labels from the
    validation set.'
  id: totrans-2021
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataloader_next_batch` 从验证集中获取下一批次的标记和标签。'
- en: '`gpt2_forward` runs the model forward on those tokens, predicting the next
    word for each one, and computes the loss against the true labels.'
  id: totrans-2022
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gpt2_forward` 在这些标记上运行模型，预测每个标记的下一个单词，并计算与真实标签的损失。'
- en: The loss from that batch is added to `val_loss`.
  id: totrans-2023
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 那个批次的损失被加到 `val_loss` 上。
- en: Notice that there is no call to `gpt2_zero_grad`, no `gpt2_backward`, and no
    `gpt2_update`. That is because validation does not train the model. It only measures
    performance.
  id: totrans-2024
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，没有调用 `gpt2_zero_grad`，没有 `gpt2_backward`，也没有 `gpt2_update`。这是因为验证并不训练模型。它只测量性能。
- en: 'Finally, the program averages the loss across the number of batches:'
  id: totrans-2025
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，程序平均了批次的损失：
- en: '[PRE119]'
  id: totrans-2026
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: 'And prints the result:'
  id: totrans-2027
  prefs: []
  type: TYPE_NORMAL
  zh: 并打印出结果：
- en: '[PRE120]'
  id: totrans-2028
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: This gives you a single number that summarizes how well the model is performing
    on unseen data at this point in training.
  id: totrans-2029
  prefs: []
  type: TYPE_NORMAL
  zh: 这为你提供了一个单一的数字，总结了模型在训练的这个点上对未见数据的性能。
- en: How to read validation loss
  id: totrans-2030
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何读取验证损失
- en: 'Imagine you are training and see logs like this:'
  id: totrans-2031
  prefs: []
  type: TYPE_NORMAL
  zh: 想象你在训练时看到这样的日志：
- en: '[PRE121]'
  id: totrans-2032
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: The training loss is printed every step, while the validation loss appears every
    10 steps. If both numbers are going down, that is a sign the model is genuinely
    learning. If training loss drops but validation loss stays the same or starts
    going up, the model is probably memorizing the training set-this is called overfitting.
  id: totrans-2033
  prefs: []
  type: TYPE_NORMAL
  zh: 每一步都会打印训练损失，而验证损失每10步出现一次。如果两个数字都在下降，这表明模型真正在学习。如果训练损失下降，但验证损失保持不变或开始上升，模型可能是在记忆训练集——这被称为过拟合。
- en: Why validation is important
  id: totrans-2034
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么验证很重要
- en: Without validation, you could be tricked into thinking the model is improving
    just because the training loss is going down. But that might only mean it has
    memorized the training data. Validation checks prevent this by showing you whether
    the model can handle data it has not seen before. It is like a student practicing
    with old exam papers (training) versus being tested with new problems (validation).
  id: totrans-2035
  prefs: []
  type: TYPE_NORMAL
  zh: 没有验证，你可能会被误导，以为模型正在改进，只是因为训练损失正在下降。但那可能只是意味着它已经记住了训练数据。验证检查通过显示模型是否可以处理之前未见过的数据来防止这种情况。这就像学生用旧试卷（训练）练习，与用新问题（验证）测试一样。
- en: Small details that matter
  id: totrans-2036
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 一些重要的细节
- en: The code averages validation loss over `val_num_batches`, which is set earlier
    to 5\. That means it only checks 5 batches, not the entire validation dataset.
    This is a shortcut-it makes validation much faster, at the cost of some accuracy
    in the measurement. But for training feedback, this is usually enough.
  id: totrans-2037
  prefs: []
  type: TYPE_NORMAL
  zh: 代码通过 `val_num_batches` 计算验证损失的均值，这个值之前设置为5。这意味着它只检查5个批次，而不是整个验证数据集。这是一个捷径——这使得验证更快，但牺牲了一些测量的准确性。但对于训练反馈来说，这通常足够。
- en: The batch size `B` and sequence length `T` for validation are the same as training.
    This keeps the loss comparable between training and validation.
  id: totrans-2038
  prefs: []
  type: TYPE_NORMAL
  zh: 验证中的批大小 `B` 和序列长度 `T` 与训练相同。这保持了训练和验证之间损失的可比性。
- en: Try it yourself
  id: totrans-2039
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 尝试自己操作
- en: 'You can experiment with the validation process to understand it better. Here
    are some ideas:'
  id: totrans-2040
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以尝试实验验证过程以更好地理解它。以下是一些想法：
- en: Change the frequency from every 10 steps to every 5 or even every step. You’ll
    see more validation updates, but training will slow down.
  id: totrans-2041
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将频率从每10步改为每5步或甚至每步。你会看到更多的验证更新，但训练会变慢。
- en: Increase `val_num_batches` to 20\. The validation loss will become less noisy,
    but each check will take longer.
  id: totrans-2042
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`val_num_batches`增加到20。验证损失将变得不那么嘈杂，但每次检查将花费更长的时间。
- en: Comment out the validation block and train again. Notice how you lose a sense
    of whether the model is really generalizing.
  id: totrans-2043
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注释掉验证块并再次训练。注意你将失去对模型是否真正泛化的感觉。
- en: Save validation loss values to a file and plot them. Compare the curve against
    the training loss curve. You’ll see how they move together or diverge.
  id: totrans-2044
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将验证损失值保存到文件并绘制它们。将曲线与训练损失曲线进行比较。你会看到它们是如何一起移动或分离的。
- en: Try using a very small validation dataset. Watch how the loss jumps around more
    compared to a larger, more stable dataset.
  id: totrans-2045
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试使用一个非常小的验证数据集。观察与更大的、更稳定的数据集相比，损失如何跳来跳去。
- en: The takeaway
  id: totrans-2046
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸取的教训
- en: Validation runs are short forward-only tests that give you confidence the model
    is learning patterns that apply to new text. They are easy to implement-a few
    lines of code in `train_gpt2.c`-but they are one of the most important tools for
    monitoring training. By checking validation loss regularly, you make sure your
    model is not just memorizing but actually becoming better at language modeling.
  id: totrans-2047
  prefs: []
  type: TYPE_NORMAL
  zh: 验证运行是简短的单向测试，可以让你有信心模型正在学习适用于新文本的模式。它们很容易实现——`train_gpt2.c`中的几行代码——但它们是监控训练的最重要工具之一。通过定期检查验证损失，你可以确保你的模型不仅仅是记忆，而是在语言建模方面变得更好。
- en: 47\. Checkpointing Parameters and Optimizer State
  id: totrans-2048
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 47. 检查点参数和优化器状态
- en: Training a model can take hours, days, or even weeks. If you stop the program
    halfway-whether by accident (a crash, a power cut) or on purpose (pausing to save
    compute)-you don’t want to start over from scratch. Checkpointing solves this
    problem by saving the model’s parameters and optimizer state to disk so you can
    resume training later.
  id: totrans-2049
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个模型可能需要数小时、数天甚至数周。如果你在程序中途停止——无论是意外（崩溃、断电）还是故意（暂停以节省计算）——你不想从头开始。通过将模型的参数和优化器状态保存到磁盘，检查点解决了这个问题，这样你就可以稍后恢复训练。
- en: What a checkpoint contains
  id: totrans-2050
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 检查点包含的内容
- en: 'A checkpoint is like a “save game” for machine learning. At a minimum, it needs:'
  id: totrans-2051
  prefs: []
  type: TYPE_NORMAL
  zh: 检查点就像是机器学习的“保存游戏”。至少需要以下内容：
- en: Model parameters – the actual weights of the neural network, stored as floating-point
    numbers in memory. These define what the model has learned so far.
  id: totrans-2052
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型参数 – 神经网络的实际权重，以内存中的浮点数形式存储。这些定义了模型迄今为止学到了什么。
- en: Optimizer state – for AdamW, this includes the running averages of gradients
    (`m_memory`) and squared gradients (`v_memory`). Without these, the optimizer
    would lose its “memory” of past updates, which could destabilize resumed training.
  id: totrans-2053
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 优化器状态 – 对于AdamW来说，这包括梯度的运行平均值（`m_memory`）和平方梯度的运行平均值（`v_memory`）。没有这些，优化器会失去对过去更新的“记忆”，这可能会使恢复的训练不稳定。
- en: Step counter – the number of steps completed so far. This matters for bias correction
    in AdamW and for scheduling the learning rate.
  id: totrans-2054
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 步数计数器 – 到目前为止完成的步数。这对于AdamW中的偏差校正和学习率调度很重要。
- en: Together, these three things capture the full training state.
  id: totrans-2055
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个要素共同捕捉了完整的训练状态。
- en: Saving a checkpoint
  id: totrans-2056
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 保存检查点
- en: 'Although `train_gpt2.c` is kept minimal and does not include full checkpointing
    code, the idea is straightforward. You allocate a file, write all parameters,
    optimizer buffers, and metadata, then close the file. In pseudocode, it looks
    like this:'
  id: totrans-2057
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然`train_gpt2.c`文件被保持最小化，并且不包括完整的检查点代码，但思路是直接的。你分配一个文件，写入所有参数、优化器缓冲区和元数据，然后关闭文件。在伪代码中，它看起来像这样：
- en: '[PRE122]'
  id: totrans-2058
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: This is a binary dump of the model and optimizer. Later, you can load the file
    back with `fread` calls into the same memory locations.
  id: totrans-2059
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个模型和优化器的二进制转储。稍后，你可以使用`fread`调用将文件加载回相同的内存位置。
- en: Loading a checkpoint
  id: totrans-2060
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 加载检查点
- en: 'Loading is the reverse:'
  id: totrans-2061
  prefs: []
  type: TYPE_NORMAL
  zh: 加载是相反的过程：
- en: '[PRE123]'
  id: totrans-2062
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: Once loaded, training can continue exactly where it left off.
  id: totrans-2063
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦加载，训练可以继续从上次停止的地方开始。
- en: Why optimizer state matters
  id: totrans-2064
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么优化器状态很重要
- en: It might seem enough to save only the model’s parameters. But AdamW depends
    on moving averages of past gradients. If you throw those away and restart with
    only the parameters, the optimizer will behave differently. Learning may suddenly
    become unstable, or the effective learning rate may feel wrong. That’s why saving
    both the parameters and optimizer state gives the most faithful restart.
  id: totrans-2065
  prefs: []
  type: TYPE_NORMAL
  zh: 可能看起来只保存模型的参数就足够了。但AdamW依赖于过去梯度的移动平均值。如果你丢弃这些值，只使用参数重新开始，优化器的行为将不同。学习可能会突然变得不稳定，或者有效学习率可能感觉不正确。这就是为什么保存参数和优化器状态可以提供最忠实的重启。
- en: Why checkpointing is essential
  id: totrans-2066
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么检查点很重要
- en: Training is rarely smooth. Servers reboot, experiments are interrupted, bugs
    are found. Without checkpoints, any interruption means wasted compute and lost
    progress. With checkpoints, you can pause and resume at will. They also let you
    archive important moments in training-for example, saving the model when validation
    loss is lowest, not just at the end.
  id: totrans-2067
  prefs: []
  type: TYPE_NORMAL
  zh: 训练很少是平滑的。服务器重启，实验被中断，发现错误。没有检查点，任何中断都意味着计算资源的浪费和进度丢失。有了检查点，你可以随意暂停和恢复。它们还让你能够保存训练中的关键时刻——例如，当验证损失最低时保存模型，而不仅仅是结束时。
- en: Try it yourself
  id: totrans-2068
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 亲自尝试
- en: Write a small function that saves the model’s parameters after every 100 steps.
    Then kill the program midway and reload from the saved file. Confirm that resumed
    training picks up where it left off.
  id: totrans-2069
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个小的函数，在每 100 步后保存模型的参数。然后在中途终止程序并从保存的文件中重新加载。确认恢复训练从上次停止的地方继续。
- en: Try saving only parameters but not optimizer state. Resume training and compare
    loss curves. You’ll see that the run diverges from the original.
  id: totrans-2070
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试只保存参数而不保存优化器状态。恢复训练并比较损失曲线。你会看到运行与原始运行发生了偏差。
- en: Save checkpoints at multiple steps and later reload them to compare model generations
    (does the model produce more fluent text after 10 steps, 100 steps, 1000 steps?).
  id: totrans-2071
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在多个步骤中保存检查点，并在之后重新加载它们以比较模型生成（模型在 10 步、100 步、1000 步后是否产生了更流畅的文本？）。
- en: Intentionally corrupt part of a checkpoint file (flip a few bytes) and try reloading.
    This helps you understand why consistency checks or checksums are often added
    in real systems.
  id: totrans-2072
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 故意损坏检查点文件的一部分（翻转几个字节）并尝试重新加载。这有助于你理解为什么在实际系统中经常添加一致性检查或校验和。
- en: Store checkpoints in a versioned way (e.g., `checkpoint_step100.bin`, `checkpoint_step200.bin`)
    so you can roll back if a later training phase degrades performance.
  id: totrans-2073
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以版本化的方式存储检查点（例如，`checkpoint_step100.bin`，`checkpoint_step200.bin`），这样你就可以在后续的训练阶段性能下降时回滚。
- en: The takeaway
  id: totrans-2074
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 要点
- en: 'Checkpointing is what makes long-running training practical. By saving parameters,
    optimizer state, and the step counter, you preserve not just what the model knows
    but how it is learning. In real projects, checkpoints are the bridge between experiments
    and production: they let you stop, resume, compare, and deploy models without
    ever starting from scratch. Even though `llm.c` does not fully implement it, the
    concept is simple and invaluable.'
  id: totrans-2075
  prefs: []
  type: TYPE_NORMAL
  zh: 检查点使得长时间运行的训练变得可行。通过保存参数、优化器状态和步骤计数器，你不仅保留了模型所知道的内容，还保留了它的学习过程。在实际项目中，检查点是实验和生产之间的桥梁：它们让你能够停止、恢复、比较和部署模型，而无需从头开始。即使
    `llm.c` 并没有完全实现它，这个概念简单而宝贵。
- en: 48\. Reproducibility and Small Divergences
  id: totrans-2076
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 48. 可复现性和小偏差
- en: When training deep learning models, two runs that look identical on the surface
    can still behave differently. One run might converge quickly, another might take
    longer, and sometimes losses diverge even though you used the same dataset and
    code. This happens because of the way randomness and numerical precision interact
    during training. Reproducibility is about controlling these factors so that results
    are consistent and meaningful.
  id: totrans-2077
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练深度学习模型时，表面上看起来相同的两次运行仍可能表现出不同的行为。一次运行可能快速收敛，另一次可能需要更长的时间，有时即使使用了相同的 dataset
    和代码，损失也可能发散。这是因为随机性和数值精度在训练过程中的交互方式。可复现性是关于控制这些因素，以确保结果一致且有意义。
- en: Sources of randomness
  id: totrans-2078
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 随机性的来源
- en: 'There are several places where randomness sneaks into training:'
  id: totrans-2079
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程中有多个地方会引入随机性：
- en: 'Data order: if batches are shuffled differently, the model sees tokens in a
    new sequence. Early steps can influence the trajectory of training.'
  id: totrans-2080
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据顺序：如果批次以不同的方式打乱，模型将看到新的序列中的标记。早期步骤可以影响训练的轨迹。
- en: 'Weight initialization: initial parameters are usually set randomly. Different
    seeds lead to slightly different starting points.'
  id: totrans-2081
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重初始化：初始参数通常设置为随机。不同的种子导致略微不同的起始点。
- en: 'Dropout and sampling: while `train_gpt2.c` is minimal and doesn’t include dropout
    layers, many neural networks do. Dropout randomly disables activations during
    training.'
  id: totrans-2082
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dropout 和采样：虽然 `train_gpt2.c` 最小化且不包含 dropout 层，但许多神经网络都包含。Dropout 在训练过程中随机禁用激活。
- en: 'Floating-point arithmetic: on CPUs and GPUs, the order of summations or parallel
    reductions can cause tiny rounding differences. Over many steps, these small changes
    accumulate.'
  id: totrans-2083
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 浮点运算：在 CPU 和 GPU 上，求和或并行归约的顺序可能导致微小的舍入差异。经过许多步骤，这些小的变化会累积。
- en: How llm.c handles reproducibility
  id: totrans-2084
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: llm.c 如何处理可复现性
- en: 'The repository includes functions like `manual_seed` and `random_f32` in `llmc/rand.h`.
    These are simple random number generators that can be seeded with a fixed value.
    For example:'
  id: totrans-2085
  prefs: []
  type: TYPE_NORMAL
  zh: 仓库中包含了`llmc/rand.h`中的`manual_seed`和`random_f32`等函数。这些是简单的随机数生成器，可以用一个固定的值来初始化。例如：
- en: '[PRE124]'
  id: totrans-2086
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: If you call this before training, the random number generator starts from the
    same state every run. That means weight initialization and sampling will be reproducible.
  id: totrans-2087
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在这之前调用，随机数生成器每次运行都会从相同的状态开始。这意味着权重初始化和采样将是可重复的。
- en: The dataloaders also have reproducibility options. When you initialize a `DataLoader`,
    you can decide whether it shuffles batches or not. Keeping this consistent ensures
    the model sees the same data order each run.
  id: totrans-2088
  prefs: []
  type: TYPE_NORMAL
  zh: 数据加载器也有可重复性选项。当你初始化一个`DataLoader`时，你可以决定是否对批次进行洗牌。保持这一致性确保模型每次运行都看到相同的数据顺序。
- en: Why small divergences happen anyway
  id: totrans-2089
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么仍然会发生小的偏差
- en: Even with fixed seeds, you might notice that two runs are not perfectly identical.
    On CPUs, differences often come from OpenMP parallel loops-threads may sum numbers
    in a different order, producing slightly different results. On GPUs, parallelism
    and library implementations (like cuBLAS or cuDNN) can do the same.
  id: totrans-2090
  prefs: []
  type: TYPE_NORMAL
  zh: 即使使用固定的种子，你也可能注意到两次运行并不完全相同。在CPU上，差异通常来自OpenMP并行循环——线程可能以不同的顺序求和数字，产生略微不同的结果。在GPU上，并行性和库实现（如cuBLAS或cuDNN）也能做到同样的事情。
- en: 'These differences are usually very small, but deep learning systems are chaotic:
    tiny changes in the early steps can grow into visible differences later. This
    doesn’t mean the code is wrong-it just means floating-point math has limits.'
  id: totrans-2091
  prefs: []
  type: TYPE_NORMAL
  zh: 这些差异通常非常小，但深度学习系统是混沌的：早期步骤中的微小变化可能会在后期产生可见的差异。这并不意味着代码是错误的——这仅仅意味着浮点数计算有局限性。
- en: Why reproducibility matters
  id: totrans-2092
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么可重复性很重要
- en: 'Reproducibility isn’t just about peace of mind. It has real uses:'
  id: totrans-2093
  prefs: []
  type: TYPE_NORMAL
  zh: 可重复性不仅仅是关于安心。它有实际的应用：
- en: 'Debugging: if a bug appears, you want to reproduce the exact same run to diagnose
    it.'
  id: totrans-2094
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调试：如果出现错误，你希望重现完全相同的运行来诊断它。
- en: 'Comparisons: when testing new optimizers, schedulers, or architectures, you
    want fair comparisons on identical conditions.'
  id: totrans-2095
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较：当测试新的优化器、调度器或架构时，你希望在相同条件下进行公平的比较。
- en: 'Science: reproducible results are essential for research papers and benchmarks.'
  id: totrans-2096
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 科学：可重复的结果对于研究论文和基准测试至关重要。
- en: 'At the same time, absolute bit-for-bit reproducibility is often unrealistic
    in large parallel systems. Instead, the goal is practical reproducibility: ensuring
    that runs are *similar enough* to reach the same conclusions.'
  id: totrans-2097
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，在大型的并行系统中，绝对位对位的可重复性通常是不切实际的。相反，目标是实际的可重复性：确保运行足够相似，以得出相同的结论。
- en: Example experiment
  id: totrans-2098
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例实验
- en: 'Suppose you seed training with `manual_seed(1337)` and use the same dataset.
    You might get a loss curve like this:'
  id: totrans-2099
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你使用`manual_seed(1337)`对训练进行种子化，并使用相同的数据集。你可能会得到一个像这样的损失曲线：
- en: '[PRE125]'
  id: totrans-2100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: The numbers are not identical, but they are close. The important part is that
    the model’s learning trajectory is stable and results are comparable.
  id: totrans-2101
  prefs: []
  type: TYPE_NORMAL
  zh: 数字并不完全相同，但它们很接近。重要的是模型的学习轨迹是稳定的，结果是可比较的。
- en: 'If you remove the seed and allow full randomness, you might get:'
  id: totrans-2102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你移除种子并允许完全随机，你可能会得到：
- en: '[PRE126]'
  id: totrans-2103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: Both are valid, but harder to compare.
  id: totrans-2104
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法都是有效的，但比较起来更困难。
- en: Try it yourself
  id: totrans-2105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 试试你自己
- en: Run training twice without setting a seed. Compare how training loss and validation
    loss differ at step 500.
  id: totrans-2106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不设置种子运行两次训练。比较在步骤500时训练损失和验证损失的不同。
- en: Set a fixed seed with `manual_seed(42)` before building the model. Run training
    twice and compare again. You should see closer numbers.
  id: totrans-2107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在构建模型之前，使用`manual_seed(42)`设置一个固定的种子。运行两次训练并再次比较。你应该会看到更接近的数字。
- en: Enable OpenMP with multiple threads and then run with a single thread. Notice
    how results differ slightly due to floating-point summation order.
  id: totrans-2108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用多线程启用OpenMP，然后以单线程运行。注意由于浮点数求和顺序的不同，结果会有轻微的差异。
- en: Save two checkpoints from runs with different seeds. Use the model to generate
    text and compare outputs. You’ll see different wording, but both grammatically
    plausible.
  id: totrans-2109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从具有不同种子的运行中保存两个检查点。使用该模型生成文本并比较输出。你会看到不同的措辞，但两者在语法上都是合理的。
- en: Increase the dataset size and check if differences between runs shrink. With
    more data, randomness matters less.
  id: totrans-2110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 增加数据集的大小并检查运行之间的差异是否缩小。随着数据的增加，随机性变得不那么重要。
- en: The takeaway
  id: totrans-2111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸取的教训
- en: Reproducibility in training is about controlling randomness where possible and
    accepting small divergences where not. In `llm.c`, reproducibility is made clear
    through simple seeding functions and deterministic dataloader options. Perfect
    bit-level reproducibility isn’t the point-the goal is to ensure results are stable,
    comparable, and scientifically sound, even if tiny numerical differences creep
    in.
  id: totrans-2112
  prefs: []
  type: TYPE_NORMAL
  zh: 训练的可重复性在于尽可能控制随机性，并在无法控制时接受小的偏差。在 `llm.c` 中，通过简单的种子函数和确定性的数据加载器选项，可重复性得到了明确。完美的位级可重复性并不是目标——目标是确保结果稳定、可比，并且科学合理，即使有微小的数值差异。
- en: 49\. Command-Line Flags and Defaults
  id: totrans-2113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 49. 命令行标志和默认值
- en: When you run a training program, you often want to change certain settings without
    editing the source code. For example, you might want to try a different batch
    size, adjust the learning rate, or train for more steps. Command-line flags make
    this possible. In `train_gpt2.c`, defaults are set inside the program, but it
    can also be compiled to accept arguments, giving you flexibility while keeping
    the code minimal.
  id: totrans-2114
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行训练程序时，你通常希望在不编辑源代码的情况下更改某些设置。例如，你可能想尝试不同的批处理大小，调整学习率，或者进行更多的训练步骤。命令行标志使得这一点成为可能。在
    `train_gpt2.c` 中，默认值在程序内部设置，但它也可以编译为接受参数，这样你在保持代码最小化的同时增加了灵活性。
- en: Why flags exist
  id: totrans-2115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 标志为什么存在
- en: Deep learning experiments are highly sensitive to hyperparameters-values like
    learning rate, batch size, sequence length, or number of steps. If every change
    required modifying source code, recompiling, and rerunning, experimentation would
    be slow and error-prone. Flags allow you to configure these parameters quickly
    at runtime.
  id: totrans-2116
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习实验对超参数非常敏感——如学习率、批处理大小、序列长度或步骤数等值。如果每次更改都需要修改源代码、重新编译和重新运行，实验将会很慢且容易出错。标志允许你在运行时快速配置这些参数。
- en: 'In many large frameworks (like PyTorch or TensorFlow), command-line arguments
    are parsed with helper libraries. In `llm.c`, the philosophy is simplicity: flags
    are either defined in code as constants, or you can extend `main` with standard
    C argument parsing to override defaults.'
  id: totrans-2117
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多大型框架（如 PyTorch 或 TensorFlow）中，命令行参数是通过辅助库解析的。在 `llm.c` 中，其哲学是简单：标志要么在代码中定义为常量，要么你可以通过标准的
    C 参数解析扩展 `main` 函数以覆盖默认值。
- en: Defaults in `train_gpt2.c`
  id: totrans-2118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '`train_gpt2.c` 中的默认值'
- en: 'Looking at the code, here are the main defaults hardcoded in the `main` function:'
  id: totrans-2119
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看代码，以下是 `main` 函数中硬编码的主要默认值：
- en: 'Batch size (`B`):'
  id: totrans-2120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '批处理大小 (`B`):'
- en: '[PRE127]'
  id: totrans-2121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE127]'
- en: 'Sequence length (`T`):'
  id: totrans-2122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '序列长度 (`T`):'
- en: '[PRE128]'
  id: totrans-2123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE128]'
- en: 'Validation batches:'
  id: totrans-2124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证批次：
- en: '[PRE129]'
  id: totrans-2125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE129]'
- en: 'Training steps:'
  id: totrans-2126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练步骤：
- en: '[PRE130]'
  id: totrans-2127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE130]'
- en: By default, only 40 steps are run in this example.
  id: totrans-2128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在此示例中，默认只运行 40 步。
- en: 'Optimizer hyperparameters (inside `gpt2_update` call):'
  id: totrans-2129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化器超参数（在 `gpt2_update` 调用中）：
- en: '[PRE131]'
  id: totrans-2130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE131]'
- en: Here the learning rate is `1e-4`, beta values for AdamW are `0.9` and `0.999`,
    epsilon is `1e-8`, and weight decay is `0.0`.
  id: totrans-2131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里学习率是 `1e-4`，AdamW 的 beta 值为 `0.9` 和 `0.999`，epsilon 是 `1e-8`，权重衰减是 `0.0`。
- en: These defaults are chosen to make the reference training loop run quickly and
    predictably, especially on small datasets like Tiny Shakespeare or Tiny Stories.
  id: totrans-2132
  prefs: []
  type: TYPE_NORMAL
  zh: 这些默认值被选择是为了使参考训练循环快速且可预测，尤其是在像 Tiny Shakespeare 或 Tiny Stories 这样的小数据集上。
- en: How to add flags
  id: totrans-2133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何添加标志
- en: 'If you want flexibility, you can extend `main` with argument parsing:'
  id: totrans-2134
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想增加灵活性，可以通过参数解析扩展 `main` 函数：
- en: '[PRE132]'
  id: totrans-2135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE132]'
- en: 'Now you can run:'
  id: totrans-2136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以运行：
- en: '[PRE133]'
  id: totrans-2137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE133]'
- en: This sets batch size to 8, sequence length to 128, and steps to 100, without
    changing source code.
  id: totrans-2138
  prefs: []
  type: TYPE_NORMAL
  zh: 这将批处理大小设置为 8，序列长度设置为 128，步骤设置为 100，而无需更改源代码。
- en: Why it matters
  id: totrans-2139
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: Command-line flags make experimentation far more efficient. You can try multiple
    configurations in one day without recompiling or editing the file repeatedly.
    This is especially useful when running jobs on clusters where you want scripts
    that launch many experiments automatically with different parameters.
  id: totrans-2140
  prefs: []
  type: TYPE_NORMAL
  zh: 命令行标志使得实验效率大大提高。你可以在一天之内尝试多个配置，而无需重新编译或反复编辑文件。这在运行集群上的作业时特别有用，你希望脚本能够自动以不同的参数启动许多实验。
- en: 'Defaults are equally important: they give you a safe, predictable starting
    point. Beginners can run the code without thinking about flags, while advanced
    users can override values as needed.'
  id: totrans-2141
  prefs: []
  type: TYPE_NORMAL
  zh: 默认值同样重要：它们为你提供了一个安全、可预测的起点。初学者可以在不思考标志的情况下运行代码，而高级用户可以根据需要覆盖值。
- en: Try it yourself
  id: totrans-2142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 亲自试试
- en: Keep the default batch size of 4 and sequence length of 64\. Run training and
    note the time per step.
  id: totrans-2143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保持默认的批处理大小为 4 和序列长度为 64。运行训练并注意每步的时间。
- en: Change batch size to 8 by editing the code. Observe how training speed changes
    and how memory usage increases.
  id: totrans-2144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过编辑代码将批量大小更改为8。观察训练速度如何变化以及内存使用如何增加。
- en: Modify the loop to train for 200 steps instead of 40\. Watch how loss decreases
    further.
  id: totrans-2145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改循环以训练200步而不是40步。观察损失如何进一步下降。
- en: Add argument parsing to accept learning rate as a flag. Experiment with `1e-3`
    vs. `1e-5` and see how quickly training diverges or stalls.
  id: totrans-2146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加参数解析以接受学习率作为标志。实验`1e-3`与`1e-5`，看看训练如何快速发散或停滞。
- en: Create a shell script that runs training multiple times with different values
    for `B` and `T`. Compare results.
  id: totrans-2147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个shell脚本，多次运行训练，并使用不同的`B`和`T`值。比较结果。
- en: The takeaway
  id: totrans-2148
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸取的经验教训
- en: Command-line flags and defaults balance simplicity with flexibility. Defaults
    make the code runnable out of the box, while flags let you scale experiments without
    constantly editing the source. In `train_gpt2.c`, this design keeps the training
    loop minimal but still adaptable, encouraging both clarity and experimentation.
  id: totrans-2149
  prefs: []
  type: TYPE_NORMAL
  zh: 命令行标志和默认值在简单性和灵活性之间取得平衡。默认值使得代码可以直接运行，而标志允许你在不经常编辑源代码的情况下扩展实验。在`train_gpt2.c`中，这种设计保持了训练循环的最小化，但仍然具有适应性，鼓励清晰性和实验性。
- en: 50\. Example CPU Training Logs and Outputs
  id: totrans-2150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 50. 示例CPU训练日志和输出
- en: 'One of the best ways to understand what a training loop is doing is by reading
    its logs. Logs are the program’s way of telling you how training is progressing:
    what the loss is, how fast it’s running, and whether validation checks are improving.
    In `train_gpt2.c`, logging is deliberately minimal so you can easily see the essentials
    without being overwhelmed.'
  id: totrans-2151
  prefs: []
  type: TYPE_NORMAL
  zh: 理解训练循环在做什么的最好方法之一是阅读其日志。日志是程序告诉你训练进展的方式：损失是多少，运行速度如何，以及验证检查是否在改善。在`train_gpt2.c`中，日志被故意设置为最小化，这样你可以轻松地看到关键信息而不会被淹没。
- en: What the logs look like
  id: totrans-2152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 日志的样式
- en: 'Here’s a snippet of output from running the CPU training loop on Tiny Shakespeare:'
  id: totrans-2153
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是运行Tiny Shakespeare上的CPU训练循环的输出片段：
- en: '[PRE134]'
  id: totrans-2154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE134]'
- en: 'Each part of this output has meaning:'
  id: totrans-2155
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的每一部分都有意义：
- en: 'Dataset sizes: how many training and validation batches are available.'
  id: totrans-2156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集大小：有多少训练和验证批次可用。
- en: 'Model config: confirmation that the GPT-2 model was loaded correctly (sequence
    length, vocab size, number of layers, etc.).'
  id: totrans-2157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型配置：确认GPT-2模型正确加载（序列长度、词汇量、层数等）。
- en: 'Validation loss: an average measure of how well the model is doing on unseen
    data.'
  id: totrans-2158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证损失：衡量模型在未见数据上的表现的平均指标。
- en: 'Training step logs: for each step, you see the training loss and how long the
    step took in milliseconds.'
  id: totrans-2159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练步骤日志：对于每个步骤，你都会看到训练损失以及该步骤花费的毫秒数。
- en: Understanding loss values
  id: totrans-2160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 理解损失值
- en: Loss is the number that tells us how far the model’s predictions are from the
    correct answers. Lower is better.
  id: totrans-2161
  prefs: []
  type: TYPE_NORMAL
  zh: 损失是告诉我们模型预测与正确答案之间距离的数字。越低越好。
- en: A loss around 5.3 means the model is essentially guessing.
  id: totrans-2162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大约5.3的损失意味着模型基本上是在猜测。
- en: As training progresses, you want to see this number slowly decrease.
  id: totrans-2163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着训练的进行，你希望看到这个数字缓慢下降。
- en: If the number gets stuck, or goes up, it can indicate problems with the learning
    rate, dataset, or implementation.
  id: totrans-2164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果数字停滞不前或上升，这可能表明学习率、数据集或实现存在问题。
- en: 'Think of it like a report card: at the beginning, the model is failing every
    test, but as it practices (trains), the grades (loss values) improve.'
  id: totrans-2165
  prefs: []
  type: TYPE_NORMAL
  zh: 想象它就像一份成绩单：一开始，模型在每次测试中都失败，但随着它练习（训练），成绩（损失值）会提高。
- en: Speed measurements
  id: totrans-2166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 速度测量
- en: The “took … ms” part shows how long each step took. On CPU, this is usually
    slow, sometimes a couple of seconds per step. On GPU, the same step might only
    take tens of milliseconds.
  id: totrans-2167
  prefs: []
  type: TYPE_NORMAL
  zh: “耗时…毫秒”部分显示了每个步骤花费的时间。在CPU上，这通常很慢，有时每个步骤可能需要几秒钟。在GPU上，相同的步骤可能只需要几十毫秒。
- en: 'Timing logs are useful because they help you:'
  id: totrans-2168
  prefs: []
  type: TYPE_NORMAL
  zh: 时间日志很有用，因为它们可以帮助你：
- en: Estimate how long full training will take.
  id: totrans-2169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 估计完整训练需要多长时间。
- en: Compare performance between machines.
  id: totrans-2170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较不同机器的性能。
- en: Spot problems if training suddenly slows down.
  id: totrans-2171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果训练突然变慢，要留意出现的问题。
- en: Occasional validation checks
  id: totrans-2172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 偶尔的验证检查
- en: 'Every few steps, the code switches to validation data and prints a `val loss`.
    This is crucial: training loss always goes down if the model memorizes the training
    set, but validation loss tells you if it is *actually learning patterns* that
    generalize.'
  id: totrans-2173
  prefs: []
  type: TYPE_NORMAL
  zh: 每过几步，代码就会切换到验证数据并打印一个`val loss`。这是至关重要的：如果模型记住了训练集，训练损失总是会下降，但验证损失会告诉你它是否实际上在学习可以推广的模式。
- en: If training loss goes down but validation loss stays high, that’s a sign of
    overfitting.
  id: totrans-2174
  prefs: []
  type: TYPE_NORMAL
  zh: 如果训练损失下降但验证损失保持较高，这是过拟合的迹象。
- en: Generated samples
  id: totrans-2175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 生成的样本
- en: 'At certain steps, the code also prints generated text like this:'
  id: totrans-2176
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些步骤中，代码还会打印出如下生成的文本：
- en: '[PRE135]'
  id: totrans-2177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE135]'
- en: Even though the text might look strange at first, it’s a powerful sign that
    the model is learning. At the beginning, output is pure gibberish, but as training
    continues, you start to see recognizable words and patterns.
  id: totrans-2178
  prefs: []
  type: TYPE_NORMAL
  zh: 即使文本一开始看起来很奇怪，这也是一个强有力的迹象，表明模型正在学习。一开始，输出完全是胡言乱语，但随着训练的继续，你开始看到可识别的单词和模式。
- en: Why it matters
  id: totrans-2179
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: Logs are your window into the training process. Without them, training would
    be a black box-you’d wait hours and have no idea if it was working. By watching
    loss curves, step times, and sample outputs, you can make informed adjustments
    and gain confidence that the model is on the right track.
  id: totrans-2180
  prefs: []
  type: TYPE_NORMAL
  zh: 日志是了解训练过程的窗口。没有它们，训练将是一个黑盒——你可能会等上几个小时，却不知道它是否在正常工作。通过观察损失曲线、步骤时间和样本输出，你可以做出明智的调整，并确信模型正在正确的轨道上。
- en: Try it yourself
  id: totrans-2181
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 试试看
- en: Run the training loop as-is and save the console output. Mark how loss changes
    between step 0 and step 40.
  id: totrans-2182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按原样运行训练循环并保存控制台输出。标记步骤0和步骤40之间损失的变化。
- en: Increase the number of steps to 200 and compare how the losses evolve.
  id: totrans-2183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将步数增加到200，并比较损失如何演变。
- en: Change the batch size from 4 to 8 and note both the training speed and the loss
    behavior.
  id: totrans-2184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将批处理大小从4改为8，并注意训练速度和损失行为。
- en: Edit the code to print validation loss every step instead of every 10 steps.
    Does the trend look smoother?
  id: totrans-2185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编辑代码以每步打印验证损失，而不是每10步打印一次。趋势看起来是否更平滑？
- en: Save the generated samples at steps 20 and 40\. Compare how the quality changes.
  id: totrans-2186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在步骤20和40时保存生成的样本。比较质量如何变化。
- en: The takeaway
  id: totrans-2187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸取的经验
- en: Training logs are like a diary of the model’s progress. They show you how quickly
    the model is learning, how well it generalizes, and how fast the computation runs.
    By reading and interpreting logs carefully, you can guide experiments, detect
    problems early, and appreciate the progress that’s happening inside the model.
  id: totrans-2188
  prefs: []
  type: TYPE_NORMAL
  zh: 训练日志是模型进度的日记。它们显示了模型学习速度有多快，泛化能力有多好，计算运行有多快。通过仔细阅读和解释日志，你可以指导实验，早期发现问题，并欣赏模型内部发生的进步。
- en: Chapter 6\. Testing and Profiling
  id: totrans-2189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第6章 测试和性能分析
- en: 51\. Debug State Structs and Their Role
  id: totrans-2190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 51. 调试状态结构及其作用
- en: When building and training a model as complex as GPT-2, you need ways to peek
    inside and check whether the values being passed around make sense. This is where
    debug state structs come in. In *llm.c*, the code is written in plain C, without
    the rich debugging utilities of frameworks like PyTorch. That means the developers
    had to create their own mechanism to store, inspect, and compare intermediate
    values.
  id: totrans-2191
  prefs: []
  type: TYPE_NORMAL
  zh: 当构建和训练像GPT-2这样复杂的模型时，你需要方法来窥视内部，检查传递的值是否有意义。这就是调试状态结构体发挥作用的地方。在 *llm.c* 中，代码是用纯C语言编写的，没有像PyTorch这样的框架提供的丰富调试工具。这意味着开发者必须创建自己的机制来存储、检查和比较中间值。
- en: 'A struct in C is just a container that groups related variables together. For
    debugging, you can think of a struct as a little notebook where the program writes
    down numbers as it computes them. These numbers might include:'
  id: totrans-2192
  prefs: []
  type: TYPE_NORMAL
  zh: C语言中的结构体只是一个将相关变量组合在一起的容器。对于调试，你可以将结构体想象成一个小的笔记本，程序在计算时会将数字记录下来。这些数字可能包括：
- en: The raw embeddings for tokens.
  id: totrans-2193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标记的原始嵌入。
- en: The attention scores before and after softmax.
  id: totrans-2194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在softmax前后注意力的分数。
- en: The outputs of each MLP block.
  id: totrans-2195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个MLP块的输出。
- en: The predicted probabilities for the next token.
  id: totrans-2196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下一个标记的预测概率。
- en: By saving these into a structured format, the program can later compare them
    to the outputs of a trusted reference implementation (usually PyTorch).
  id: totrans-2197
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将这些保存到结构化格式中，程序可以在以后将它们与受信任的参考实现（通常是PyTorch）的输出进行比较。
- en: How it works in practice
  id: totrans-2198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实际工作原理
- en: Inside *llm.c*, there are places where arrays of floats—like hidden states or
    logits—are copied into a debug state struct. Once stored, these values can be
    printed, dumped to a file, or checked against “golden” results from PyTorch.
  id: totrans-2199
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *llm.c* 中，有一些地方将浮点数数组（如隐藏状态或logits）复制到调试状态结构体中。一旦存储，这些值就可以打印出来，写入文件，或与PyTorch的“黄金”结果进行比较。
- en: Imagine you’re testing a tiny batch of input tokens. The forward pass runs as
    usual, but at specific checkpoints (say, right after attention or after the final
    linear projection), the program writes those arrays into a struct. Later, when
    running a PyTorch model with the same inputs and weights, the two outputs can
    be compared element by element.
  id: totrans-2200
  prefs: []
  type: TYPE_NORMAL
  zh: 想象你正在测试一小批输入标记。正向传播像往常一样运行，但在特定的检查点（比如，在注意力之后或最终线性投影之后），程序将这些数组写入结构体。稍后，当使用相同的输入和权重运行
    PyTorch 模型时，可以逐元素比较这两个输出。
- en: 'This is essential for catching subtle errors:'
  id: totrans-2201
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于捕捉细微的错误至关重要：
- en: A misplaced transpose in matrix multiplication.
  id: totrans-2202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵乘法中的错误转置。
- en: Forgetting to apply a mask in attention.
  id: totrans-2203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 忘记在注意力中应用掩码。
- en: A floating-point precision mismatch in softmax.
  id: totrans-2204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: softmax 中的浮点精度不匹配。
- en: Without the struct, you’d only know the model loss looks “off.” With the struct,
    you know *exactly* which step went wrong.
  id: totrans-2205
  prefs: []
  type: TYPE_NORMAL
  zh: 没有结构体，你只知道模型损失看起来“不正常”。有了结构体，你知道 *确切* 哪个步骤出了问题。
- en: Why it matters
  id: totrans-2206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: Debug structs bridge the gap between C and Python ecosystems. PyTorch has a
    decade of battle-tested layers, so it’s the gold standard for correctness. By
    saving intermediate activations in C and comparing them against PyTorch, developers
    ensure that every layer behaves identically. This builds confidence that the *llm.c*
    codebase isn’t just “roughly correct,” but precisely reproduces GPT-2’s math.
  id: totrans-2207
  prefs: []
  type: TYPE_NORMAL
  zh: 调试结构体架起了 C 和 Python 生态系统之间的桥梁。PyTorch 拥有十年的实战层，因此它是正确性的黄金标准。通过在 C 中保存中间激活并与之比较
    PyTorch，开发者确保每个层的行为完全相同。这增强了信心，即 *llm.c* 代码库不仅仅是“大致正确”，而是精确地重现了 GPT-2 的数学。
- en: For anyone modifying the code—for example, writing a new activation function
    or experimenting with quantization—the debug structs act like a safety net. You
    can quickly see if your change accidentally altered the outputs in a way that
    breaks parity with the original model.
  id: totrans-2208
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何修改代码的人来说——例如，编写新的激活函数或进行量化实验——调试结构体就像一个安全网。你可以快速查看你的更改是否意外地改变了输出，从而破坏了与原始模型的兼容性。
- en: Try it yourself
  id: totrans-2209
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 亲自尝试
- en: 'Trace a forward pass: Run the CPU version with a tiny batch and enable debug
    dumps. Look at the embeddings, attention scores, and final logits.'
  id: totrans-2210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 跟踪正向传播：使用 tiny batch 运行 CPU 版本并启用调试转储。查看嵌入、注意力分数和最终 logits。
- en: 'Cross-check with PyTorch: Run the same input through Hugging Face GPT-2\. Print
    the same tensors. Compare a few entries by hand—do they match closely?'
  id: totrans-2211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与 PyTorch 进行交叉检查：将相同的输入通过 Hugging Face GPT-2。打印相同的张量。手动比较几个条目——它们是否非常接近？
- en: 'Introduce a bug: Change the scaling factor in attention (e.g., remove the `1/√d`
    term). Run again and see how quickly the mismatch shows up in the debug struct.'
  id: totrans-2212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 引入一个错误：更改注意力中的缩放因子（例如，删除 `1/√d` 项）。再次运行并查看不匹配在调试结构体中显示得多快。
- en: 'Extend the struct: Add a new field for an intermediate step you care about,
    like LayerNorm outputs. Print it during debugging to see how normalization changes
    the activations.'
  id: totrans-2213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 扩展结构体：为关心的中间步骤添加一个新字段，例如 LayerNorm 输出。在调试期间打印它，以查看归一化如何改变激活。
- en: The takeaway
  id: totrans-2214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸取的经验教训
- en: Debug state structs are the microscope of *llm.c*. They allow you to pause the
    flow of numbers, record them, and compare them against a known-good model. Without
    them, development would feel like working blindfolded. With them, you can track
    down errors precisely, ensure parity with PyTorch, and confidently extend the
    system knowing you have a reliable safety net.
  id: totrans-2215
  prefs: []
  type: TYPE_NORMAL
  zh: 调试状态结构体是 *llm.c* 的显微镜。它们允许你暂停数字的流动，记录它们，并将它们与已知良好的模型进行比较。没有它们，开发就像蒙着眼睛工作。有了它们，你可以精确地追踪错误，确保与
    PyTorch 的兼容性，并且可以自信地扩展系统，因为你有一个可靠的安全网。
- en: '52\. `test_gpt2.c`: CPU vs PyTorch'
  id: totrans-2216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 52. `test_gpt2.c`：CPU 与 PyTorch
- en: Testing is one of the most important parts of the *llm.c* project. The file
    `test_gpt2.c` exists specifically to check whether the C implementation of GPT-2
    produces the same outputs as PyTorch, which is the trusted reference. Without
    this file, you would only know if the final training loss looked reasonable. With
    it, you can verify that every part of the forward pass matches.
  id: totrans-2217
  prefs: []
  type: TYPE_NORMAL
  zh: 测试是 *llm.c* 项目最重要的部分之一。文件 `test_gpt2.c` 存在的目的是检查 GPT-2 的 C 实现是否产生与 PyTorch 相同的输出，这是可信的参考。没有这个文件，你只能知道最终的训练损失是否合理。有了它，你可以验证正向传播的每个部分是否匹配。
- en: 'At its core, `test_gpt2.c` runs a very controlled experiment: it loads a GPT-2
    model checkpoint (exported from PyTorch), prepares a small batch of input tokens,
    executes a forward pass in C, and compares the outputs against the corresponding
    tensors from PyTorch. If everything matches within a tight numerical tolerance,
    you know the C code is correct. If not, you have a clear signal that something
    is wrong.'
  id: totrans-2218
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，`test_gpt2.c`运行一个非常受控的实验：它加载一个GPT-2模型检查点（从PyTorch导出），准备一小批输入标记，在C中执行前向传递，并将输出与PyTorch中的相应张量进行比较。如果一切在严格的数值公差范围内匹配，你就知道C代码是正确的。如果不匹配，你会有一个清晰的信号表明有问题。
- en: How the test works
  id: totrans-2219
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 测试是如何工作的
- en: Load the checkpoint The test begins by reading a binary checkpoint file, such
    as `gpt2_124M.bin`, which contains the weights of the GPT-2 model. These weights
    were originally trained in PyTorch and then exported into a binary format that
    *llm.c* can understand.
  id: totrans-2220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载检查点 测试首先读取一个二进制检查点文件，例如`gpt2_124M.bin`，其中包含GPT-2模型的权重。这些权重最初是在PyTorch中训练的，然后导出为*llm.c*可以理解的二进制格式。
- en: Prepare the inputs The test uses a known sequence of token IDs—sometimes from
    a dataset like Tiny Shakespeare, sometimes just a few hand-picked tokens. This
    ensures the same inputs can be run through both PyTorch and C implementations.
  id: totrans-2221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备输入 测试使用一个已知的标记ID序列——有时来自Tiny Shakespeare这样的数据集，有时只是几个手工挑选的标记。这确保了相同的输入可以通过PyTorch和C实现运行。
- en: Run the C forward pass The function `gpt2_forward` is executed on the CPU. All
    embeddings, attention layers, MLPs, and final logits are computed exactly as they
    would be during real inference or training.
  id: totrans-2222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行C前向传递 函数`gpt2_forward`在CPU上执行。所有嵌入、注意力层、MLP和最终logits的计算方式与实际推理或训练期间完全相同。
- en: Compare with PyTorch For each major tensor (e.g., hidden states, attention outputs,
    final logits), the values are compared against saved outputs from PyTorch. The
    comparison usually allows for very small differences, since floating-point math
    can vary slightly between libraries. A tolerance like `1e-5` is common.
  id: totrans-2223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与PyTorch比较 对于每个主要张量（例如，隐藏状态、注意力输出、最终logits），其值将与PyTorch保存的输出进行比较。由于浮点数数学在库之间可能略有差异，比较通常允许非常小的差异。`1e-5`这样的公差是常见的。
- en: Report mismatches If any element deviates beyond the allowed tolerance, the
    test reports the difference. Developers can then investigate where the divergence
    started, often by adding more debug dumps of intermediate states.
  id: totrans-2224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 报告差异 如果任何元素超出允许的公差范围，测试报告将显示差异。开发者可以随后调查差异开始的地方，通常是通过添加更多中间状态的调试输出。
- en: Why this test is crucial
  id: totrans-2225
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这个测试至关重要
- en: C code is low-level and unforgiving. A single indexing mistake, a wrong stride,
    or a missing scaling factor in attention can make outputs diverge wildly. Since
    GPT-2 has millions of parameters, such errors are almost impossible to spot by
    hand. By tying the implementation back to PyTorch, `test_gpt2.c` provides a ground
    truth check.
  id: totrans-2226
  prefs: []
  type: TYPE_NORMAL
  zh: C代码是低级的且不容忍错误。一个索引错误、错误的步长或注意力中缺少缩放因子都可能使输出剧烈偏离。由于GPT-2有数百万个参数，这样的错误几乎不可能手动发现。通过将实现与PyTorch联系起来，`test_gpt2.c`提供了一个基准检查。
- en: This also ensures scientific reproducibility. If someone else downloads *llm.c*
    and runs `test_gpt2.c`, they should see the same level of agreement with PyTorch.
    That way, they can trust that training runs, losses, and model outputs are not
    artifacts of a broken implementation.
  id: totrans-2227
  prefs: []
  type: TYPE_NORMAL
  zh: 这也确保了科学可重复性。如果其他人下载*llm.c*并运行`test_gpt2.c`，他们应该看到与PyTorch相同程度的协议。这样，他们可以相信训练运行、损失和模型输出不是损坏实现的产物。
- en: Example in action
  id: totrans-2228
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 动作中的示例
- en: 'Imagine you’ve just modified the attention code to optimize the matrix multiplication.
    You recompile and run `test_gpt2.c`. If you see an error like:'
  id: totrans-2229
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你刚刚修改了注意力代码以优化矩阵乘法。你重新编译并运行`test_gpt2.c`。如果你看到如下错误：
- en: '[PRE136]'
  id: totrans-2230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE136]'
- en: 'you know the two match within tolerance—everything is fine. But if you see:'
  id: totrans-2231
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道两者在公差范围内匹配——一切正常。但如果你看到：
- en: '[PRE137]'
  id: totrans-2232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE137]'
- en: that’s a red flag. It means the optimization introduced a bug. Without the test,
    you might not notice until much later when training fails to converge.
  id: totrans-2233
  prefs: []
  type: TYPE_NORMAL
  zh: 那是一个红旗。这意味着优化引入了一个错误。如果没有这个测试，你可能会在训练无法收敛的更晚时候才注意到。
- en: Why it matters
  id: totrans-2234
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: '`test_gpt2.c` is the guarantee that the C implementation is not just “close
    enough,” but *faithful*. It ensures that improvements, optimizations, or even
    rewrites don’t silently corrupt the model’s behavior. It’s a direct bridge between
    the experimental world of C internals and the well-established baseline of PyTorch.'
  id: totrans-2235
  prefs: []
  type: TYPE_NORMAL
  zh: '`test_gpt2.c`是保证C实现不仅仅是“足够接近”，而是*忠实*的依据。它确保改进、优化或甚至重写不会无声地破坏模型的行为。它是C内部实验世界和PyTorch稳固基线之间的直接桥梁。'
- en: Try it yourself
  id: totrans-2236
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 亲自试试
- en: Run `make test_gpt2` in the repository. Observe whether the outputs match PyTorch.
  id: totrans-2237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在仓库中运行`make test_gpt2`。观察输出是否与PyTorch匹配。
- en: Deliberately change one line of code in `gpt2_forward`—for example, remove the
    attention scaling factor. Run the test again and see how quickly it fails.
  id: totrans-2238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 故意更改`gpt2_forward`中的一行代码——例如，移除注意力缩放因子。再次运行测试，看看它会多快失败。
- en: Add your own print statements to show which tensors are being compared. Watch
    how the numbers line up almost exactly.
  id: totrans-2239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加自己的打印语句以显示正在比较哪些张量。观察数字几乎完全对齐。
- en: Try running with different checkpoints (e.g., 124M vs 355M) to see if parity
    holds across scales.
  id: totrans-2240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试使用不同的检查点（例如，124M vs 355M）来查看是否在各个尺度上保持一致性。
- en: The takeaway
  id: totrans-2241
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸取教训
- en: '`test_gpt2.c` is not just another file in the repository—it’s the truth meter.
    It reassures you that the complicated layers of GPT-2 have been implemented correctly
    in C and remain consistent with PyTorch. This confidence is what allows further
    work—whether training, profiling, or extending the model—to proceed on a solid
    foundation.'
  id: totrans-2242
  prefs: []
  type: TYPE_NORMAL
  zh: '`test_gpt2.c`不仅仅是仓库中的另一个文件——它是真相计。它让你确信GPT-2的复杂层已经在C中正确实现，并且与PyTorch保持一致。这种信心是允许进一步工作——无论是训练、分析还是扩展模型——在稳固基础上进行的前提。'
- en: 53\. Matching Outputs Within Tolerances
  id: totrans-2243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 53. 在公差内匹配输出
- en: Once you have a test like `test_gpt2.c` set up, the next challenge is figuring
    out how close the outputs need to be for the test to pass. Computers don’t always
    produce bit-for-bit identical results when doing floating-point math. The order
    of operations, the precision of instructions, and even the type of hardware (CPU
    vs GPU) can cause tiny differences.
  id: totrans-2244
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦设置了像`test_gpt2.c`这样的测试，下一个挑战就是确定输出需要多接近才能通过测试。计算机在进行浮点运算时并不总是产生位对位完全相同的结果。操作顺序、指令的精度，甚至硬件类型（CPU
    vs GPU）都可能导致微小的差异。
- en: If you demanded exact matches, most tests would fail even when the implementation
    is correct. That’s why *llm.c* uses tolerance-based comparison. Instead of asking
    “are these numbers exactly equal?”, the code asks “are these numbers *close enough*?”
  id: totrans-2245
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你要求完全匹配，即使实现是正确的，大多数测试也会失败。这就是为什么*llm.c*使用基于公差的比较。代码不是询问“这些数字是否完全相等？”，而是询问“这些数字*是否足够接近*？”
- en: Absolute vs relative tolerance
  id: totrans-2246
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 绝对公差与相对公差
- en: 'There are two common ways to define “close enough”:'
  id: totrans-2247
  prefs: []
  type: TYPE_NORMAL
  zh: 定义“足够接近”有两种常见方式：
- en: 'Absolute tolerance: check that the difference between two numbers is smaller
    than a threshold. For example,'
  id: totrans-2248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绝对公差：检查两个数字之间的差值是否小于一个阈值。例如，
- en: '[PRE138]'
  id: totrans-2249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE138]'
- en: This works well for values near zero.
  id: totrans-2250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这对于接近零的值来说效果很好。
- en: 'Relative tolerance: check that the difference is small relative to the size
    of the numbers. For example,'
  id: totrans-2251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相对公差：检查差值相对于数字大小的相对大小。例如，
- en: '[PRE139]'
  id: totrans-2252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE139]'
- en: Even though the absolute difference is 0.1, that’s tiny compared to the scale
    of 1000.
  id: totrans-2253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 虽然绝对差值为0.1，但与1000的规模相比，这非常小。
- en: In practice, the code often combines both. It passes if the difference is smaller
    than either the absolute tolerance or the relative tolerance.
  id: totrans-2254
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，代码通常结合了两者。如果差值小于绝对公差或相对公差，则通过测试。
- en: Why tolerances are necessary
  id: totrans-2255
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么需要公差
- en: 'Imagine you run the forward pass on CPU in C and in PyTorch. PyTorch might
    use fused kernels or higher-precision accumulations. If you compare the final
    logits, you may see values like:'
  id: totrans-2256
  prefs: []
  type: TYPE_NORMAL
  zh: 想象你在C和PyTorch中在CPU上运行前向传递。PyTorch可能会使用融合内核或更高精度的累加。如果你比较最终的logits，你可能会看到如下值：
- en: 'PyTorch: `-3.4521234`'
  id: totrans-2257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'PyTorch: `-3.4521234`'
- en: 'C: `-3.4521255`'
  id: totrans-2258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'C: `-3.4521255`'
- en: The difference is just `0.0000021`. For practical purposes, they’re the same.
    Without tolerance, this tiny difference would fail the test. With tolerance, you
    can safely say both implementations agree.
  id: totrans-2259
  prefs: []
  type: TYPE_NORMAL
  zh: 差值仅为`0.0000021`。从实用角度来说，它们是相同的。没有公差，这个微小的差异将无法通过测试。有了公差，你可以安全地说两种实现是一致的。
- en: Example from debugging
  id: totrans-2260
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 调试示例
- en: 'Suppose you compare the probabilities after softmax. You might get:'
  id: totrans-2261
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你在softmax之后比较概率。你可能会得到：
- en: 'PyTorch: `0.3333333, 0.3333333, 0.3333333`'
  id: totrans-2262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'PyTorch: `0.3333333, 0.3333333, 0.3333333`'
- en: 'C: `0.3333334, 0.3333333, 0.3333333`'
  id: totrans-2263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'C: `0.3333334, 0.3333333, 0.3333333`'
- en: Here, the first value differs in the last decimal place. The tolerance rule
    says that’s fine, since the absolute error is smaller than `1e-7`.
  id: totrans-2264
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，第一个值在最后一位小数上有所不同。容差规则说这没关系，因为绝对误差小于`1e-7`。
- en: 'But if you saw something like:'
  id: totrans-2265
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果你看到了类似的情况：
- en: 'PyTorch: `0.3333333, 0.3333333, 0.3333333`'
  id: totrans-2266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'PyTorch: `0.3333333, 0.3333333, 0.3333333`'
- en: 'C: `0.5000000, 0.2500000, 0.2500000`'
  id: totrans-2267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'C: `0.5000000, 0.2500000, 0.2500000`'
- en: the mismatch is huge—no tolerance rule would allow it. That’s a clear bug.
  id: totrans-2268
  prefs: []
  type: TYPE_NORMAL
  zh: 不匹配非常严重——没有任何容差规则会允许这种情况。这是一个明显的错误。
- en: Why it matters
  id: totrans-2269
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: Matching within tolerance isn’t just a technical detail; it’s about trust. It
    lets you say with confidence that the implementation is mathematically faithful
    to the reference. You don’t waste time chasing harmless decimal noise, but you
    also don’t miss real mistakes.
  id: totrans-2270
  prefs: []
  type: TYPE_NORMAL
  zh: 在容差范围内匹配不仅仅是一个技术细节；它关乎信任。它让你有信心地说，实现是数学上忠实于参考的。你不会浪费时间追踪无害的十进制噪声，但你也不会错过真正的错误。
- en: This approach is also what makes cross-platform development possible. The same
    *llm.c* code can be run on Linux, macOS, or even inside different compilers, and
    as long as results fall within tolerance, you know the model behavior is preserved.
  id: totrans-2271
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法也是使跨平台开发成为可能的原因。相同的`*llm.c*`代码可以在Linux、macOS上运行，甚至可以在不同的编译器中运行，只要结果在容差范围内，你就知道模型行为得到了保留。
- en: Try it yourself
  id: totrans-2272
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 试试看
- en: Run `test_gpt2.c` and look at the output logs. Notice how many decimal places
    match.
  id: totrans-2273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行`test_gpt2.c`并查看输出日志。注意有多少位小数匹配。
- en: Change the tolerance threshold in the code from `1e-5` to something stricter
    like `1e-8`. See if the test starts failing due to harmless floating-point noise.
  id: totrans-2274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将代码中的容差阈值从`1e-5`改为更严格的值，例如`1e-8`。看看测试是否因为无害的浮点噪声而开始失败。
- en: Add a deliberate bug—for example, skip dividing by `√d` in the attention code—and
    rerun. The mismatches will be far larger than the tolerance, proving the bug is
    real.
  id: totrans-2275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个故意的错误——例如，在注意力代码中跳过分母`√d`——然后重新运行。不匹配将远大于容差，证明错误是真实的。
- en: Compare CPU results with PyTorch, then recompile with different compiler flags
    (like `-O0` vs `-O3`) and check if results still fall within tolerance.
  id: totrans-2276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将CPU结果与PyTorch进行比较，然后使用不同的编译器标志（如`-O0`与`-O3`）重新编译，并检查结果是否仍在容差范围内。
- en: The takeaway
  id: totrans-2277
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸取的教训
- en: Tolerance-based testing is what allows *llm.c* to be both rigorous and realistic.
    It ensures that differences are only flagged when they matter, while ignoring
    the harmless quirks of floating-point math. This makes the test suite a reliable
    tool for catching true errors without overwhelming you with false alarms.
  id: totrans-2278
  prefs: []
  type: TYPE_NORMAL
  zh: 基于容差的测试使得`*llm.c*`既严格又现实。它确保只有当差异重要时才会标记差异，同时忽略浮点数学的无害怪癖。这使得测试套件成为捕捉真正错误的可靠工具，而不会让你被虚假警报淹没。
- en: 54\. Profiling with `profile_gpt2.c`
  id: totrans-2279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 54. 使用`profile_gpt2.c`进行性能分析
- en: 'After verifying that the outputs match PyTorch, the next big question is: how
    fast is the code running? Correctness is essential, but performance is what makes
    a minimal C implementation like *llm.c* worthwhile. That’s where `profile_gpt2.c`
    comes in. It is a small program that runs controlled forward passes through GPT-2
    and measures the time they take, helping you understand where the bottlenecks
    are.'
  id: totrans-2280
  prefs: []
  type: TYPE_NORMAL
  zh: 在验证输出与PyTorch匹配后，下一个重大问题是：代码运行得有多快？正确性是必要的，但性能是使像`*llm.c*`这样的最小C实现值得做的关键。这就是`profile_gpt2.c`的作用。它是一个小的程序，通过GPT-2运行受控的前向传递并测量它们所需的时间，帮助你了解瓶颈在哪里。
- en: What profiling means
  id: totrans-2281
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 性能分析的含义
- en: 'Profiling is the act of measuring the performance of a program, not just its
    correctness. Instead of asking “does this number match PyTorch?”, profiling asks:'
  id: totrans-2282
  prefs: []
  type: TYPE_NORMAL
  zh: 性能分析是衡量程序性能的行为，而不仅仅是其正确性。而不是问“这个数字与PyTorch匹配吗？”，性能分析会问：
- en: How many milliseconds does one forward pass take?
  id: totrans-2283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个前向传递需要多少毫秒？
- en: Which part of the model consumes the most time?
  id: totrans-2284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型中哪一部分消耗的时间最多？
- en: Does using OpenMP threads actually speed things up?
  id: totrans-2285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用OpenMP线程实际上能加快速度吗？
- en: How does batch size affect runtime?
  id: totrans-2286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批处理大小如何影响运行时间？
- en: By answering these, you can make informed decisions about optimization.
  id: totrans-2287
  prefs: []
  type: TYPE_NORMAL
  zh: 通过回答这些问题，你可以做出关于优化的明智决策。
- en: How `profile_gpt2.c` works
  id: totrans-2288
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '`profile_gpt2.c`是如何工作的'
- en: The profiling program is structured like a simplified inference loop.
  id: totrans-2289
  prefs: []
  type: TYPE_NORMAL
  zh: 性能分析程序的结构类似于简化的推理循环。
- en: Model setup It loads a GPT-2 checkpoint (e.g., `gpt2_124M.bin`) into memory
    and allocates space for activations.
  id: totrans-2290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型设置它将GPT-2检查点（例如，`gpt2_124M.bin`）加载到内存中，并为激活分配空间。
- en: Dummy input Instead of using real text, it creates random token IDs. That way,
    the cost measured comes purely from the computation, not from data loading or
    tokenization.
  id: totrans-2291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 虚拟输入而不是使用真实文本，它创建随机的标记ID。这样，测量的成本纯粹来自计算，而不是来自数据加载或标记化。
- en: Timing with clock functions Before and after each forward pass, it records timestamps
    with `clock_gettime(CLOCK_MONOTONIC, &start)` and `&end`. The difference gives
    the runtime in seconds, which is usually converted into milliseconds.
  id: totrans-2292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用时钟函数计时在每个前向传递之前和之后，使用`clock_gettime(CLOCK_MONOTONIC, &start)`和`&end`记录时间戳。差异给出以秒为单位的运行时间，这通常被转换为毫秒。
- en: Looping for stability A single run can be noisy due to background processes
    on your computer. To smooth things out, `profile_gpt2.c` runs the forward pass
    multiple times and averages the results.
  id: totrans-2293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 循环以稳定结果由于你的计算机上的后台进程，单次运行可能会很嘈杂。为了使结果平滑，`profile_gpt2.c`多次运行前向传递并平均结果。
- en: Reporting results Finally, it prints the average time per forward pass. Sometimes
    it also estimates FLOPs (floating-point operations per second), giving you a rough
    idea of efficiency compared to the theoretical peak of your CPU.
  id: totrans-2294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 报告结果最后，它打印出每次前向传递的平均时间。有时它还会估计FLOPs（每秒浮点运算数），这给你一个与CPU理论峰值相比效率的大致概念。
- en: What you can learn from profiling
  id: totrans-2295
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 从性能分析中你可以学到什么
- en: 'Running `profile_gpt2.c` on a CPU gives insights like:'
  id: totrans-2296
  prefs: []
  type: TYPE_NORMAL
  zh: 在CPU上运行`profile_gpt2.c`可以得到如下见解：
- en: The attention blocks dominate runtime, because they involve large matrix multiplications.
  id: totrans-2297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力块主导了运行时间，因为它们涉及大矩阵乘法。
- en: Increasing batch size makes the runtime longer, but not proportionally—sometimes
    bigger batches use hardware more efficiently.
  id: totrans-2298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加批大小会使运行时间更长，但不是成比例的——有时更大的批次使用硬件更有效率。
- en: OpenMP can speed things up when there are multiple CPU cores available, but
    scaling may flatten out after a certain number of threads.
  id: totrans-2299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当有多个CPU核心可用时，OpenMP可以加快速度，但线程数量达到一定程度后，扩展可能会趋于平坦。
- en: This helps decide where to spend effort. For example, if LayerNorm takes 2%
    of the runtime but attention takes 70%, you know optimization should focus on
    the attention code.
  id: totrans-2300
  prefs: []
  type: TYPE_NORMAL
  zh: 这有助于决定在哪里投入精力。例如，如果LayerNorm占运行时间的2%，而注意力层占70%，你知道优化应该集中在注意力代码上。
- en: Why it matters
  id: totrans-2301
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: Profiling isn’t just about numbers. It’s about guiding your development choices.
    Without profiling, you might spend weeks hand-optimizing LayerNorm, only to discover
    it barely affects overall runtime. With profiling, you see immediately where the
    slowdowns are and can focus on the real bottlenecks.
  id: totrans-2302
  prefs: []
  type: TYPE_NORMAL
  zh: 性能分析不仅仅是关于数字。它关乎指导你的开发选择。没有性能分析，你可能会花费数周手动优化LayerNorm，结果发现它几乎不影响整体运行时间。有了性能分析，你可以立即看到哪里有减速，并可以专注于真正的瓶颈。
- en: It also provides baseline performance numbers. If you change something in the
    implementation, re-running `profile_gpt2.c` will tell you if it sped things up
    or slowed them down. This feedback loop is essential for optimization work.
  id: totrans-2303
  prefs: []
  type: TYPE_NORMAL
  zh: 它还提供了基线性能数据。如果你在实现中做了改变，重新运行`profile_gpt2.c`会告诉你是否加快了速度或减慢了速度。这个反馈循环对于优化工作至关重要。
- en: Try it yourself
  id: totrans-2304
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 尝试自己操作
- en: Compile and run `profile_gpt2.c` with a small model checkpoint. Note the reported
    runtime per forward pass.
  id: totrans-2305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用小型模型检查点编译并运行`profile_gpt2.c`。注意每次前向传递报告的运行时间。
- en: Change the batch size `B` and sequence length `T`, then re-run. Watch how runtime
    scales with larger inputs.
  id: totrans-2306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 改变批大小`B`和序列长度`T`，然后重新运行。观察运行时间如何随着更大的输入而扩展。
- en: Set `OMP_NUM_THREADS=1` to disable threading and compare it against `OMP_NUM_THREADS=4`
    or higher. How much faster is it with multiple cores?
  id: totrans-2307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置`OMP_NUM_THREADS=1`以禁用多线程，并与`OMP_NUM_THREADS=4`或更高值进行比较。多核心能提高多少速度？
- en: Modify the code to time individual layers (embeddings, attention, MLP). This
    gives even more precise insight into which parts dominate computation.
  id: totrans-2308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改代码以计时单个层（嵌入层、注意力层、MLP层）。这能提供更精确的洞察，了解哪些部分主导了计算。
- en: The takeaway
  id: totrans-2309
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸取的经验
- en: '`profile_gpt2.c` turns performance from a guess into hard data. It tells you
    exactly how long a forward pass takes, how much threading helps, and where the
    real bottlenecks are. With it, you can track progress as you optimize the code,
    ensuring that your changes make the model not only correct but also efficient.'
  id: totrans-2310
  prefs: []
  type: TYPE_NORMAL
  zh: '`profile_gpt2.c`将性能从猜测变成了硬数据。它告诉你前向传递确切需要多长时间，多少线程有帮助，以及真正的瓶颈在哪里。有了它，你可以在优化代码时跟踪进度，确保你的更改不仅使模型正确，而且高效。'
- en: 55\. Measuring FLOPs and CPU Performance
  id: totrans-2311
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 55. 测量FLOPs和CPU性能
- en: 'When talking about performance in deep learning, it’s not enough to say “this
    run took 200 milliseconds.” To really understand efficiency, we need a measure
    that’s independent of hardware and input size. That’s where FLOPs come in: floating-point
    operations. A FLOP is a single numerical calculation involving real numbers—like
    an addition, multiplication, or division.'
  id: totrans-2312
  prefs: []
  type: TYPE_NORMAL
  zh: 在谈论深度学习的性能时，仅仅说“这次运行花费了200毫秒”是不够的。为了真正理解效率，我们需要一个与硬件和输入大小无关的度量。这就是FLOPs的作用：浮点运算。一个FLOP是一个涉及实数的单一数值计算——比如加法、乘法或除法。
- en: By counting how many FLOPs a model requires and comparing it to how many the
    computer can perform per second, you can evaluate how close your implementation
    is to the theoretical maximum speed of your CPU.
  id: totrans-2313
  prefs: []
  type: TYPE_NORMAL
  zh: 通过计算模型所需的FLOPs并与计算机每秒可以执行的数量进行比较，您可以评估您的实现与CPU理论最大速度的接近程度。
- en: What FLOPs mean in practice
  id: totrans-2314
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实际中FLOPs的含义
- en: 'Every layer of GPT-2—embeddings, attention, feed-forward—can be broken down
    into a sequence of multiplications and additions. For example:'
  id: totrans-2315
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2的每一层—嵌入、注意力、前馈—都可以分解为一系列乘法和加法。例如：
- en: A matrix multiplication between two matrices of size `M × K` and `K × N` requires
    `2 × M × K × N` FLOPs.
  id: totrans-2316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个大小为`M × K`和`K × N`的矩阵之间的矩阵乘法需要`2 × M × K × N` FLOPs。
- en: A softmax across a vector of size `d` requires about `3d` FLOPs (exponentials,
    sums, and divisions).
  id: totrans-2317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一个大小为`d`的向量上应用softmax需要大约`3d` FLOPs（指数、求和和除法）。
- en: An MLP block with hidden size `h` and intermediate size `4h` requires multiple
    matrix multiplications, adding up to billions of FLOPs per training step.
  id: totrans-2318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有隐藏大小`h`和中间大小`4h`的MLP块需要多次矩阵乘法，每个训练步骤累计数十亿FLOPs。
- en: When you sum these across all layers, even a small GPT-2 model like 124M parameters
    involves several gigaFLOPs (billions of operations) for one forward pass.
  id: totrans-2319
  prefs: []
  type: TYPE_NORMAL
  zh: 当您将它们跨所有层求和时，即使是像124M参数的小型GPT-2模型，在单次正向传播中也涉及数吉FLOPs（数十亿操作）。
- en: How `profile_gpt2.c` estimates FLOPs
  id: totrans-2320
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '`profile_gpt2.c`如何估计FLOPs'
- en: 'The profiling code doesn’t literally count every multiplication. Instead, it
    uses formulas derived from matrix dimensions:'
  id: totrans-2321
  prefs: []
  type: TYPE_NORMAL
  zh: 分析代码并不真正计算每一次乘法。相反，它使用从矩阵维度推导出的公式：
- en: The configuration struct (`model.config`) gives the number of layers, heads,
    embedding size, and sequence length.
  id: totrans-2322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置结构体(`model.config`)提供了层数、头数、嵌入大小和序列长度。
- en: For each block (attention + MLP), the code applies standard FLOPs formulas for
    matrix multiplication and softmax.
  id: totrans-2323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个块（注意力+MLP），代码应用了矩阵乘法和softmax的标准FLOPs公式。
- en: These counts are added up to estimate the total FLOPs for a forward pass.
  id: totrans-2324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些计数被加起来以估计正向传播的总FLOPs。
- en: Dividing the total FLOPs by the measured runtime (in seconds) gives FLOPs/second,
    also known as throughput.
  id: totrans-2325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将总FLOPs除以测量的运行时间（以秒为单位）得到FLOPs/second，也称为吞吐量。
- en: For example, if a forward pass takes 0.1 seconds and involves 20 billion FLOPs,
    then throughput is about 200 GFLOPs/s.
  id: totrans-2326
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果正向传播需要0.1秒并涉及2000亿FLOPs，那么吞吐量约为200 GFLOPs/s。
- en: Why this is useful
  id: totrans-2327
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很有用
- en: 'Compare hardware: You can test the same model on a laptop CPU and a server
    CPU, then compare FLOPs/s to see how much faster the server is.'
  id: totrans-2328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较硬件：您可以在笔记本电脑CPU和服务器CPU上测试相同型号，然后比较FLOPs/s以查看服务器有多快。
- en: 'Compare implementations: If you modify attention to use a different algorithm,
    the FLOPs count won’t change, but if runtime decreases, throughput increases—showing
    the optimization worked.'
  id: totrans-2329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较实现：如果您修改注意力以使用不同的算法，FLOPs计数不会改变，但如果运行时间减少，吞吐量增加—这表明优化是有效的。
- en: 'Know your limits: CPUs often achieve only a fraction of their theoretical peak
    FLOPs due to memory bottlenecks. Profiling shows how close you’re getting in practice.'
  id: totrans-2330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 了解你的限制：由于内存瓶颈，CPU通常只能达到其理论峰值FLOPs的一小部分。分析显示了您在实践中接近的程度。
- en: Example
  id: totrans-2331
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例
- en: 'Let’s say:'
  id: totrans-2332
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来说：
- en: 'Forward pass FLOPs: 15 billion'
  id: totrans-2333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正向传播FLOPs：150亿
- en: 'Runtime: 0.2 seconds'
  id: totrans-2334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行时间：0.2秒
- en: 'Throughput: 75 GFLOPs/s'
  id: totrans-2335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吞吐量：75 GFLOPs/s
- en: If your CPU’s datasheet says the peak is 200 GFLOPs/s, then you’re at about
    37% efficiency. That gap might be due to memory latency, cache misses, or lack
    of vectorization.
  id: totrans-2336
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的CPU数据表显示峰值是200 GFLOPs/s，那么您的效率大约是37%。这个差距可能是由内存延迟、缓存未命中或缺乏向量化引起的。
- en: Try it yourself
  id: totrans-2337
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 试试看
- en: Run `profile_gpt2.c` and note the reported FLOPs and runtime.
  id: totrans-2338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行`profile_gpt2.c`并注意报告的FLOPs和运行时间。
- en: Change the sequence length `T` and observe how FLOPs scale linearly with it.
  id: totrans-2339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 改变序列长度`T`并观察FLOPs如何与其成线性关系。
- en: Increase the number of layers in the model configuration—watch FLOPs rise accordingly.
  id: totrans-2340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 增加模型配置中的层数—观察FLOPs相应增加。
- en: Compare your measured FLOPs/s with the theoretical maximum listed for your CPU.
    How close are you?
  id: totrans-2341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将你测量的 FLOPs/s 与你 CPU 列出的理论最大值进行比较。你有多接近？
- en: The takeaway
  id: totrans-2342
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸取的教训
- en: FLOPs turn performance from “this feels fast” into hard numbers you can compare
    across runs, machines, and implementations. By knowing both the operation count
    and the achieved throughput, you gain a clear picture of how efficient the *llm.c*
    code really is and where further optimizations might pay off.
  id: totrans-2343
  prefs: []
  type: TYPE_NORMAL
  zh: FLOPs 将性能从“感觉很快”转化为可以跨运行、机器和实现进行比较的硬数值。通过了解操作数和实际吞吐量，你可以清楚地了解 *llm.c* 代码的真正效率以及进一步优化的可能收益。
- en: 56\. Capturing Memory Usage on CPU
  id: totrans-2344
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 56. 在 CPU 上捕获内存使用情况
- en: While FLOPs tell us how much raw computation a model needs, performance isn’t
    just about speed—it’s also about memory usage. Modern language models are enormous,
    and on CPUs with limited RAM, memory can become the true bottleneck. That’s why
    *llm.c* also emphasizes monitoring how much memory is used during inference and
    training.
  id: totrans-2345
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 FLOPs 告诉我们模型需要多少原始计算，但性能不仅仅是关于速度——它还关乎内存使用。现代语言模型非常庞大，在有限的 RAM 的 CPU 上，内存可能成为真正的瓶颈。这就是为什么
    *llm.c* 也强调在推理和训练过程中监控内存使用量。
- en: What consumes memory in GPT-2
  id: totrans-2346
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: GPT-2 中消耗内存的内容
- en: 'There are several key components that take up space:'
  id: totrans-2347
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个关键组件会占用空间：
- en: 'Parameters (weights): GPT-2 124M has about 124 million parameters. Each is
    stored as a 32-bit float (4 bytes). That alone is roughly 500 MB.'
  id: totrans-2348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 参数（权重）：GPT-2 124M 大约有 1240 万个参数。每个参数都存储为 32 位浮点数（4 字节）。仅此一项就大约是 500 MB。
- en: 'Gradients: During training, gradients for each parameter are stored. That doubles
    the memory usage.'
  id: totrans-2349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 梯度：在训练过程中，每个参数的梯度都会被存储。这会使内存使用量加倍。
- en: 'Optimizer states: AdamW requires two additional memory slots per parameter
    (`m` and `v`), which doubles it again. With parameters, gradients, and optimizer
    states combined, training can require 4× the parameter size in memory.'
  id: totrans-2350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 优化器状态：AdamW 每个参数需要两个额外的内存槽（`m` 和 `v`），这又将其加倍。结合参数、梯度和优化器状态，训练可能需要内存中参数大小的 4
    倍。
- en: 'Activations: These are the intermediate outputs of each layer (attention scores,
    MLP results, normalized states). For backpropagation, activations from the forward
    pass must be kept until gradients are computed. Depending on batch size and sequence
    length, activations can rival parameter memory.'
  id: totrans-2351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 激活：这些是每一层的中间输出（注意力分数、MLP 结果、归一化状态）。对于反向传播，前向传递的激活必须保留，直到计算梯度。根据批处理大小和序列长度，激活可能和参数内存相媲美。
- en: Measuring memory in practice
  id: totrans-2352
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实际测量内存使用
- en: 'On CPU, memory usage can be inspected in several ways:'
  id: totrans-2353
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CPU 上，可以通过几种方式检查内存使用情况：
- en: 'Operating system tools: `top`, `htop`, or Activity Monitor show total memory
    used by the program.'
  id: totrans-2354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作系统工具：`top`、`htop` 或活动监视器显示程序使用的总内存。
- en: 'Manual accounting in code: *llm.c* knows how many parameters, gradients, and
    optimizer states exist. By multiplying their counts by 4 bytes, it can estimate
    usage precisely.'
  id: totrans-2355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码中的手动会计：*llm.c* 知道有多少参数、梯度和优化器状态存在。通过将它们的计数乘以 4 字节，它可以精确地估计使用量。
- en: 'Instrumentation during profiling: you can add checkpoints that print memory
    usage at different stages of the forward or backward pass.'
  id: totrans-2356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析期间的仪器：你可以在前向或反向传递的不同阶段添加检查点，以打印内存使用情况。
- en: 'For example:'
  id: totrans-2357
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: 'Parameters only: ~500 MB.'
  id: totrans-2358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅参数：约 ~500 MB。
- en: 'Parameters + gradients: ~1 GB.'
  id: totrans-2359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数 + 梯度：约 ~1 GB。
- en: 'Parameters + gradients + optimizer: ~2 GB.'
  id: totrans-2360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数 + 梯度 + 优化器：约 ~2 GB。
- en: 'Adding activations: 2.5–3 GB, depending on batch size and sequence length.'
  id: totrans-2361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加激活：2.5–3 GB，具体取决于批处理大小和序列长度。
- en: Why memory matters on CPU
  id: totrans-2362
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么内存对 CPU 的重要性
- en: On a CPU, you don’t just care about “can it fit in RAM?” You also care about
    cache efficiency. Modern CPUs have multiple levels of cache (L1, L2, L3), which
    are much faster than main RAM. If activations or weights don’t fit well in cache,
    performance can suffer even if you technically have enough RAM.
  id: totrans-2363
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CPU 上，你不仅关心“它能否适应 RAM？”你还关心缓存效率。现代 CPU 有多个级别的缓存（L1、L2、L3），它们的速度比主 RAM 快得多。如果激活或权重不适合缓存，即使技术上你有足够的
    RAM，性能也可能受到影响。
- en: Memory footprint also limits experiment flexibility. For example, increasing
    sequence length from 64 to 1024 multiplies activation storage by 16\. A run that
    fits at `T=64` may crash or swap at `T=1024`.
  id: totrans-2364
  prefs: []
  type: TYPE_NORMAL
  zh: 内存占用也限制了实验的灵活性。例如，将序列长度从 64 增加到 1024 会将激活存储量乘以 16。在 `T=64` 时可以运行的程序可能在 `T=1024`
    时崩溃或交换。
- en: Example scenario
  id: totrans-2365
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例场景
- en: 'Suppose you run GPT-2 124M with:'
  id: totrans-2366
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你使用 GPT-2 124M：
- en: Batch size `B=4`
  id: totrans-2367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批处理大小 `B=4`
- en: Sequence length `T=64`
  id: totrans-2368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列长度 `T=64`
- en: This might use ~2.5 GB of memory for training. If you raise `T` to 512, suddenly
    activations balloon, and total usage may exceed 10 GB. On a laptop with 8 GB RAM,
    this simply won’t work.
  id: totrans-2369
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能需要 ~2.5 GB 的内存进行训练。如果你将 `T` 提高到 512，激活量会突然膨胀，总使用量可能超过 10 GB。在 8 GB RAM 的笔记本电脑上，这根本无法工作。
- en: By monitoring memory carefully, you can avoid mysterious crashes and plan runs
    realistically.
  id: totrans-2370
  prefs: []
  type: TYPE_NORMAL
  zh: 通过仔细监控内存，你可以避免神秘的崩溃，并现实地规划运行。
- en: Try it yourself
  id: totrans-2371
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 试试你自己
- en: Run training with a small batch (`B=2`, `T=64`) and check memory usage with
    `htop`.
  id: totrans-2372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用小批量（`B=2`，`T=64`）进行训练，并使用 `htop` 检查内存使用情况。
- en: Increase `T` step by step (128, 256, 512) and record the growth. Watch how activations
    dominate beyond a certain length.
  id: totrans-2373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 逐步增加 `T` 步（128，256，512）并记录增长。观察激活量如何超过一定长度后主导。
- en: 'Calculate parameter memory manually: `num_params × 4 bytes`. Compare it to
    what the OS reports. The difference comes from activations and optimizer states.'
  id: totrans-2374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 手动计算参数内存：`num_params × 4 bytes`。将其与操作系统报告的值进行比较。差异来自激活量和优化器状态。
- en: Modify the code to print memory allocations explicitly when arrays are created.
    This gives an internal log of usage at each step.
  id: totrans-2375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改代码，在创建数组时显式打印内存分配。这会在每个步骤提供使用情况的内部日志。
- en: The takeaway
  id: totrans-2376
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸取的经验教训
- en: 'Memory is the silent partner of FLOPs: you need both to train and run models
    efficiently. Profiling without tracking memory is incomplete—you might have a
    model that’s fast but impossible to run on your machine. By capturing and understanding
    memory usage, you gain the ability to scale responsibly, balance batch size and
    sequence length, and keep your experiments stable.'
  id: totrans-2377
  prefs: []
  type: TYPE_NORMAL
  zh: 内存是 FLOPs 的无声伙伴：你需要两者来高效地训练和运行模型。没有跟踪内存的配置文件是不完整的——你可能有一个运行速度快但无法在你的机器上运行的模型。通过捕获和理解内存使用情况，你能够负责任地扩展，平衡批量大小和序列长度，并保持你的实验稳定。
- en: 57\. Reproducing Known Loss Curves (CPU-only)
  id: totrans-2378
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 57. 仅 CPU 的已知损失曲线再现
- en: Once the model is correct and its performance is measured, the next important
    step is to check whether it learns in the way we expect. In deep learning, we
    usually monitor this with a loss curve—a graph that shows how the training loss
    decreases over time as the model sees more data.
  id: totrans-2379
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型正确并且其性能得到测量，下一步重要的步骤是检查它是否以我们期望的方式学习。在深度学习中，我们通常通过损失曲线来监控这一点——一个显示随着模型看到更多数据，训练损失随时间下降的图表。
- en: For GPT-2 and other language models, the standard loss function is cross-entropy,
    which measures how well the predicted probability distribution over tokens matches
    the actual next token in the dataset. If the implementation is right, the loss
    should fall in a predictable way when trained on text like Tiny Shakespeare or
    Tiny Stories.
  id: totrans-2380
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 GPT-2 和其他语言模型，标准的损失函数是交叉熵，它衡量预测的标记概率分布与数据集中实际下一个标记匹配得有多好。如果实现正确，当在 Tiny Shakespeare
    或 Tiny Stories 等文本上训练时，损失应该以可预测的方式下降。
- en: What a “known” loss curve looks like
  id: totrans-2381
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 一个“已知”损失曲线看起来像什么
- en: 'The community has already run countless GPT-2 experiments in PyTorch, so we
    know roughly what the curve should look like:'
  id: totrans-2382
  prefs: []
  type: TYPE_NORMAL
  zh: 社区已经在 PyTorch 中运行了无数 GPT-2 实验，所以我们大致知道曲线应该是什么样子：
- en: At the very beginning, the loss is high (around 5–6) because the model is basically
    guessing.
  id: totrans-2383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一开始，损失很高（大约 5–6），因为模型基本上是在猜测。
- en: After a few hundred steps, the loss starts dropping steadily.
  id: totrans-2384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经过几百步之后，损失开始稳步下降。
- en: For Tiny Shakespeare, loss often goes down toward ~2.0 with a small GPT-2 model
    (124M parameters).
  id: totrans-2385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 Tiny Shakespeare，使用一个小型的 GPT-2 模型（124M 参数）时，损失通常下降到 ~2.0。
- en: The exact numbers can vary, but the shape—a downward trend with small fluctuations—is
    consistent.
  id: totrans-2386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具体的数字可能会有所不同，但形状——一个向下趋势，波动很小——是一致的。
- en: If the C implementation produces a similar curve, that’s a strong sign the forward
    pass, backward pass, and optimizer are all working correctly.
  id: totrans-2387
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 C 实现产生类似的曲线，那么这是一个强烈的迹象，表明前向传递、反向传递和优化器都在正确工作。
- en: How reproduction is tested in practice
  id: totrans-2388
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实际操作中如何测试再现性
- en: Train with a small dataset The test usually uses Tiny Shakespeare or Tiny Stories
    since they are small enough to run quickly on CPU.
  id: totrans-2389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用小数据集进行训练 测试通常使用 Tiny Shakespeare 或 Tiny Stories，因为它们足够小，可以在 CPU 上快速运行。
- en: 'Log the loss per step Each training step prints something like:'
  id: totrans-2390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记录每步的损失 每个训练步骤会打印出类似的内容：
- en: '[PRE140]'
  id: totrans-2391
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE140]'
- en: Plot the curve Save the loss values and make a simple plot with step on the
    x-axis and loss on the y-axis.
  id: totrans-2392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制曲线 保存损失值，并使用步骤作为 x 轴，损失作为 y 轴制作简单的图表。
- en: Compare against PyTorch If you train the same model in PyTorch with the same
    hyperparameters, the loss curve should look almost identical. Small differences
    are normal due to random seeds or floating-point math.
  id: totrans-2393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与PyTorch比较 如果你在PyTorch中使用相同的超参数训练相同的模型，损失曲线应该几乎相同。由于随机种子或浮点数数学的差异，小的差异是正常的。
- en: Why this is important
  id: totrans-2394
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: 'Reproducing a known loss curve is more than just a sanity check. It tells you:'
  id: totrans-2395
  prefs: []
  type: TYPE_NORMAL
  zh: 重现已知的损失曲线不仅仅是进行合理性检查。它告诉你：
- en: 'The math is right: gradients, optimizer updates, and scheduler logic are functioning.'
  id: totrans-2396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数学是正确的：梯度、优化器更新和调度器逻辑正在正常工作。
- en: 'The data pipeline is correct: tokens are being fed in properly and batches
    are consistent.'
  id: totrans-2397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据管道是正确的：标记被正确地输入，批次是一致的。
- en: 'Nothing is silently broken: without this, a bug might go unnoticed until much
    later in training.'
  id: totrans-2398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有什么是默默出错的：没有这个，一个错误可能直到训练的后期才被发现。
- en: This is especially important on CPU, because training is slower and you may
    only run a few hundred steps. If the curve starts to dip in the expected way,
    you know you’re on the right track.
  id: totrans-2399
  prefs: []
  type: TYPE_NORMAL
  zh: 这在CPU上尤为重要，因为训练较慢，你可能只能运行几百步。如果曲线以预期的方式开始下降，你就知道你走在正确的道路上。
- en: Example scenario
  id: totrans-2400
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例场景
- en: Suppose you train GPT-2 124M with `B=4` and `T=64` on Tiny Shakespeare. The
    loss starts around 4.9, and by step 200 it falls to around 3.2\. If PyTorch shows
    a similar trajectory, then your implementation is validated.
  id: totrans-2401
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你使用`B=4`和`T=64`在Tiny Shakespeare上训练GPT-2 124M。损失开始大约在4.9，到第200步时下降到大约3.2。如果PyTorch显示出类似的轨迹，那么你的实现就得到了验证。
- en: But if your loss stays flat, say at 5.0 for hundreds of steps, that’s a red
    flag. It could mean gradients are not flowing, the optimizer isn’t updating weights,
    or your data loader is feeding the same batch repeatedly.
  id: totrans-2402
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果你的损失保持平坦，比如在几百步中保持在5.0，那么这是一个红旗。这可能意味着梯度没有流动，优化器没有更新权重，或者你的数据加载器反复提供相同的批次。
- en: Try it yourself
  id: totrans-2403
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 亲自尝试
- en: Train for 200 steps on Tiny Shakespeare with the C code and save the printed
    losses.
  id: totrans-2404
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用C代码在Tiny Shakespeare上训练200步，并保存打印出的损失。
- en: Train the same setup in PyTorch. Plot both curves together and compare.
  id: totrans-2405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在PyTorch中用相同的设置进行训练。将两条曲线一起绘制并比较。
- en: Intentionally break something—for example, comment out the optimizer update
    step—and observe how the loss no longer decreases.
  id: totrans-2406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 故意破坏某些东西——例如，注释掉优化器更新步骤——并观察损失不再下降。
- en: Experiment with learning rates. Too high may cause the curve to bounce up and
    down, while too low will make it drop very slowly.
  id: totrans-2407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试调整学习率。过高可能会导致曲线上下跳动，而过低会使它下降得很慢。
- en: The takeaway
  id: totrans-2408
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸取的经验
- en: Reproducing known loss curves is the ultimate integration test. It proves that
    the entire pipeline—data, model, training loop, optimizer—works together in harmony.
    When your loss curve matches the reference, you can trust that your C implementation
    of GPT-2 is not only correct in theory but also effective in practice.
  id: totrans-2409
  prefs: []
  type: TYPE_NORMAL
  zh: 重现已知的损失曲线是最终的集成测试。它证明了整个管道——数据、模型、训练循环、优化器——协同工作。当你的损失曲线与参考匹配时，你可以相信你的GPT-2的C实现不仅在理论上正确，而且在实践中也是有效的。
- en: 58\. Debugging Numerical Stability (NaNs, Infs)
  id: totrans-2410
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 58. 调试数值稳定性（NaNs，Infs）
- en: 'Even if the model produces correct outputs most of the time, there’s a hidden
    danger in deep learning: numerical instability. This happens when floating-point
    numbers inside the computation blow up to infinity (`Inf`) or collapse into “not
    a number” (`NaN`). When this occurs, training usually grinds to a halt—loss becomes
    undefined, gradients explode, and parameters no longer update meaningfully.'
  id: totrans-2411
  prefs: []
  type: TYPE_NORMAL
  zh: 即使模型大多数时候都能产生正确的输出，深度学习中仍存在一个潜在的危险：数值不稳定性。这种情况发生在计算中的浮点数膨胀到无穷大（`Inf`）或崩溃成“不是一个数字”（`NaN`）。当这种情况发生时，训练通常会停止——损失变得未定义，梯度爆炸，参数不再有意义地更新。
- en: Why NaNs and Infs happen
  id: totrans-2412
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么会出现NaN和Inf
- en: Neural networks involve many multiplications, exponentials, and divisions. On
    paper, all of these are fine. But computers store numbers with limited precision
    (32-bit floats in this case). When values get too large or too small, they can
    no longer be represented correctly.
  id: totrans-2413
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络涉及许多乘法、指数和除法。在纸上，所有这些都是好的。但计算机使用有限精度（在这种情况下是32位浮点数）来存储数字。当值变得过大或过小时，它们就不再能被正确表示。
- en: 'Common sources include:'
  id: totrans-2414
  prefs: []
  type: TYPE_NORMAL
  zh: 常见原因包括：
- en: 'Softmax overflow: computing `exp(x)` on large positive numbers leads to `Inf`.'
  id: totrans-2415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Softmax溢出：在大的正数上计算`exp(x)`会导致`Inf`。
- en: 'Division by very small numbers: for example, dividing by `sqrt(v_hat) + eps`
    in AdamW can produce instability if `eps` is too small.'
  id: totrans-2416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除以非常小的数字：例如，在AdamW中除以`sqrt(v_hat) + eps`可能会在`eps`太小的情况下产生不稳定性。
- en: 'Exploding gradients: during backpropagation, errors compound across many layers,
    producing extremely large values.'
  id: totrans-2417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度爆炸：在反向传播过程中，错误会在多个层中累积，产生极大的值。
- en: 'Improper initialization or learning rates: weights that are too large or step
    sizes that are too aggressive can push activations outside a stable range.'
  id: totrans-2418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不当的初始化或学习率：过大的权重或过于激进的学习率步长可能会将激活推到不稳定范围内。
- en: How to detect instability in *llm.c*
  id: totrans-2419
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何检测 *llm.c* 中的不稳定性
- en: 'Because the code is written in C without automatic checks, NaNs and Infs can
    spread silently unless you look for them. Some useful strategies include:'
  id: totrans-2420
  prefs: []
  type: TYPE_NORMAL
  zh: 由于代码是用 C 编写的，没有自动检查，NaNs 和 Infs 除非你寻找它们，否则会无声地传播。一些有用的策略包括：
- en: 'Assertions: insert `assert(!isnan(value) && !isinf(value));` inside loops to
    catch bad values immediately.'
  id: totrans-2421
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 断言：在循环中插入 `assert(!isnan(value) && !isinf(value));` 以立即捕获坏值。
- en: 'Debug prints: log sample values from activations or gradients each step to
    see if they drift toward extremely large numbers.'
  id: totrans-2422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调试打印：每步记录激活或梯度的样本值，以查看它们是否漂移到极大的数字。
- en: 'Check the loss: if the loss suddenly becomes `nan` or `inf`, that’s a strong
    signal something went wrong upstream.'
  id: totrans-2423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查损失：如果损失突然变为 `nan` 或 `inf`，这表明上游可能出现了问题。
- en: 'Small runs: testing on tiny sequences and batches makes it easier to inspect
    values directly.'
  id: totrans-2424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 小规模运行：在微小的序列和批次上进行测试，更容易直接检查值。
- en: How to fix instability
  id: totrans-2425
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何修复不稳定性
- en: 'Several practical techniques help keep numbers stable:'
  id: totrans-2426
  prefs: []
  type: TYPE_NORMAL
  zh: 几种实用技术有助于保持数值稳定：
- en: 'Add an epsilon: in divisions or square roots, add a small constant (like `1e-8`)
    to prevent division by zero.'
  id: totrans-2427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加一个 epsilon：在除法或平方根中，添加一个小的常数（如 `1e-8`）以防止除以零。
- en: 'Rescale before softmax: subtract the maximum value in the vector before computing
    exponentials. This keeps values in a safe range.'
  id: totrans-2428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 softmax 之前重新缩放：在计算指数之前从向量中减去最大值。这保持值在安全范围内。
- en: 'Gradient clipping: cap gradients so they cannot exceed a certain norm. This
    stops runaway updates.'
  id: totrans-2429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度裁剪：限制梯度，使其不超过一定的范数。这可以阻止更新失控。
- en: 'Adjust learning rate: if training diverges, lowering the learning rate often
    restores stability.'
  id: totrans-2430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整学习率：如果训练发散，降低学习率通常可以恢复稳定性。
- en: 'Check data: corrupted inputs or unexpected tokens can inject extreme values
    into the model.'
  id: totrans-2431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查数据：损坏的输入或意外的标记可能会将极端值注入模型。
- en: Example scenario
  id: totrans-2432
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例场景
- en: 'Suppose you’re training GPT-2 on Tiny Shakespeare. The first few steps look
    fine:'
  id: totrans-2433
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你在 Tiny Shakespeare 上训练 GPT-2。最初的几步看起来正常：
- en: '[PRE141]'
  id: totrans-2434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE141]'
- en: This sudden jump to `nan` suggests instability. Checking the gradients reveals
    extremely large values in the attention weights. The fix might be lowering the
    learning rate from `1e-4` to `5e-5` or enabling gradient clipping.
  id: totrans-2435
  prefs: []
  type: TYPE_NORMAL
  zh: 这种突然跳到 `nan` 的现象表明不稳定。检查梯度会发现在注意力权重中存在极端大的值。修复方法可能是将学习率从 `1e-4` 降低到 `5e-5` 或启用梯度裁剪。
- en: Try it yourself
  id: totrans-2436
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 亲自尝试
- en: Train with a very high learning rate (`1e-2`) and watch how quickly NaNs appear.
  id: totrans-2437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用非常高的学习率 (`1e-2`) 进行训练，并观察 NaNs 出现的速度。
- en: Add a debug check inside `gpt2_forward` that prints when any activation exceeds
    `1e6`. Run a few steps and observe if values explode.
  id: totrans-2438
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `gpt2_forward` 中添加一个调试检查，当任何激活超过 `1e6` 时打印。运行几步并观察值是否爆炸。
- en: Modify the softmax code to omit subtracting the max. Compare stability before
    and after.
  id: totrans-2439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改 softmax 代码以省略减去最大值。比较修改前后的稳定性。
- en: Add a gradient clipping routine and measure whether it prevents loss from diverging.
  id: totrans-2440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加梯度裁剪例程并测量它是否可以防止损失发散。
- en: The takeaway
  id: totrans-2441
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸取的经验
- en: Numerical stability is the difference between a model that trains smoothly and
    one that collapses after a few steps. By anticipating where NaNs and Infs can
    arise, adding checks, and applying stabilizing tricks, you make *llm.c* robust.
    This ensures that experiments are reliable and that debugging focuses on real
    algorithmic issues rather than avoidable numerical traps.
  id: totrans-2442
  prefs: []
  type: TYPE_NORMAL
  zh: 数值稳定性是模型平稳训练与训练几步后崩溃之间的区别。通过预测 NaNs 和 Infs 可能出现的地方，添加检查，并应用稳定技巧，使 *llm.c* 变得健壮。这确保了实验的可靠性，并且调试重点放在真正的算法问题上，而不是可避免的数值陷阱。
- en: 59\. From Unit Test to Full Training Readiness
  id: totrans-2443
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 59. 从单元测试到完整训练准备
- en: 'Unit tests are the first line of defense: they check whether small, isolated
    parts of the code—like embeddings, attention, or softmax—produce the correct outputs.
    But passing unit tests isn’t the same as being ready for full training. The transition
    from “this layer works” to “the whole system learns correctly over thousands of
    steps” involves additional challenges.'
  id: totrans-2444
  prefs: []
  type: TYPE_NORMAL
  zh: 单元测试是第一道防线：它们检查代码的小部分、孤立的部分——如嵌入、注意力和softmax——是否产生正确的输出。但通过单元测试并不意味着为全面训练做好准备。从“这一层工作”到“整个系统在数千步中正确学习”的转变涉及额外的挑战。
- en: The gap between unit tests and training
  id: totrans-2445
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 单元测试与训练之间的差距
- en: 'Unit tests check correctness: for example, verifying that `gpt2_forward` produces
    the same logits as PyTorch on a single batch.'
  id: totrans-2446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单元测试检查正确性：例如，验证`gpt2_forward`在单个批次上产生的logits与PyTorch相同。
- en: 'Training readiness checks robustness: making sure the model can run repeatedly
    for thousands of steps without crashing, diverging, or leaking memory.'
  id: totrans-2447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练准备性检查鲁棒性：确保模型可以反复运行数千步而不会崩溃、发散或泄漏内存。
- en: Think of it like testing a car. Unit tests are like checking the brakes, headlights,
    and steering individually. Training readiness is taking the car on a 500-mile
    road trip and making sure nothing overheats, rattles loose, or fails under stress.
  id: totrans-2448
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下测试一辆车。单元测试就像单独检查刹车、车灯和转向。训练准备性就像开车进行500英里的长途旅行，确保没有任何部件过热、松动或在压力下失效。
- en: What needs to be validated for training readiness
  id: totrans-2449
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 需要验证的训练准备性
- en: Loss curve behavior Run the training loop for several hundred steps. The training
    loss should steadily decrease, matching known curves from PyTorch. If it stagnates
    or spikes, something is wrong in gradients or the optimizer.
  id: totrans-2450
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 损失曲线行为 运行训练循环数百步。训练损失应该稳步下降，与PyTorch的已知曲线匹配。如果停滞或激增，则梯度或优化器中存在问题。
- en: Validation runs Regularly measure validation loss during training. If it decreases
    at first and then stabilizes, that shows the model is generalizing. If it decreases
    too quickly and then shoots up, that suggests overfitting.
  id: totrans-2451
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证运行 在训练期间定期测量验证损失。如果最初下降然后稳定，这表明模型正在泛化。如果下降得太快然后突然上升，这表明可能过拟合。
- en: Memory stability Training uses more memory than inference because of gradients
    and optimizer states. A memory leak—forgetting to free arrays or reallocating
    without release—will cause the program to crash after many steps.
  id: totrans-2452
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 内存稳定性 训练使用的内存比推理多，因为梯度优化器状态。内存泄漏——忘记释放数组或未释放就重新分配——会导致程序在许多步骤后崩溃。
- en: Optimizer state updates Check that AdamW accumulates `m` and `v` correctly over
    many iterations. If bias correction is missing, loss curves will diverge from
    expected baselines.
  id: totrans-2453
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 优化器状态更新 检查AdamW在多次迭代中正确累积`m`和`v`。如果缺少偏差校正，损失曲线将偏离预期的基线。
- en: Reproducibility With the same random seed, two runs should produce nearly identical
    loss curves. Small differences are normal, but major deviations suggest nondeterministic
    bugs.
  id: totrans-2454
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可重现性 使用相同的随机种子，两次运行应产生几乎相同的损失曲线。小差异是正常的，但主要偏差表明存在非确定性错误。
- en: Why it matters
  id: totrans-2455
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: Without this step, you might believe your implementation is complete after unit
    tests, only to discover that training silently fails at step 500\. Training readiness
    ensures the system is not only mathematically correct in small pieces but also
    practically usable for long-running experiments.
  id: totrans-2456
  prefs: []
  type: TYPE_NORMAL
  zh: 没有这一步，你可能会在单元测试后认为你的实现已经完成，但最终发现训练在500步时默默地失败了。训练准备性确保系统不仅在数学上在小部分正确，而且在长时间运行的实验中实际可用。
- en: This is also where confidence in deploying the code comes from. Passing training
    readiness means others can clone the repository, run the scripts, and expect stable
    training without mysterious crashes.
  id: totrans-2457
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是代码部署的信心来源。通过训练准备性意味着其他人可以克隆仓库，运行脚本，并期望稳定训练而不会出现神秘的崩溃。
- en: Example scenario
  id: totrans-2458
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例场景
- en: You run `test_gpt2.c` and all outputs match PyTorch within tolerance—great.
    Then you launch training for 5,000 steps. After step 600, the loss becomes `nan`.
    Investigation reveals that `gpt2_update` wasn’t applying bias correction properly,
    so the optimizer went unstable. That’s a bug you’d never catch with a one-batch
    unit test, but training readiness exposes it.
  id: totrans-2459
  prefs: []
  type: TYPE_NORMAL
  zh: 你运行`test_gpt2.c`，所有输出都在公差范围内与PyTorch匹配——太好了。然后你启动训练5000步。在第600步后，损失变为`nan`。调查发现`gpt2_update`没有正确应用偏差校正，所以优化器变得不稳定。这是一个你用单个批次的单元测试永远无法捕捉到的错误，但训练准备性揭示了它。
- en: Try it yourself
  id: totrans-2460
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 试试你自己
- en: Run training for 1,000 steps on Tiny Shakespeare and log the loss every 10 steps.
    Check that it decreases smoothly.
  id: totrans-2461
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Tiny Shakespeare上运行1000步的训练，并每10步记录一次损失。检查它是否平稳下降。
- en: Add validation runs every 100 steps. Watch for the classic gap between train
    loss (lower) and validation loss (slightly higher).
  id: totrans-2462
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每隔100步添加验证运行。注意训练损失（较低）和验证损失（略高）之间的经典差距。
- en: Use `htop` or similar tools to monitor memory usage during training. Confirm
    that it stays steady rather than creeping upward.
  id: totrans-2463
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`htop`或类似工具监控训练期间的内存使用情况。确认它保持稳定，而不是缓慢上升。
- en: Run the same training twice with the same seed. Compare the two loss curves—are
    they nearly identical?
  id: totrans-2464
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用相同的种子运行相同的训练两次。比较两个损失曲线——它们是否几乎相同？
- en: The takeaway
  id: totrans-2465
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸取的经验教训
- en: Unit tests prove the pieces are correct. Training readiness proves the whole
    system works under real conditions. Both are necessary. Together, they give you
    the confidence that *llm.c* isn’t just a collection of working parts, but a functioning
    engine capable of training GPT-2 models end to end.
  id: totrans-2466
  prefs: []
  type: TYPE_NORMAL
  zh: 单元测试证明各个部分是正确的。训练准备性证明整个系统在真实条件下能够正常工作。这两者都是必要的。共同之处在于，它们让你有信心，*llm.c*不仅仅是一系列工作的部件，而是一个能够从头到尾训练GPT-2模型的完整引擎。
- en: 60\. Limitations of CPU Testing
  id: totrans-2467
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 60. CPU测试的局限性
- en: Testing GPT-2 on CPU is invaluable for verifying correctness, but it comes with
    clear limits. Understanding these limitations helps you interpret the results
    properly and know when it’s time to move on to GPU-based experiments.
  id: totrans-2468
  prefs: []
  type: TYPE_NORMAL
  zh: 在CPU上测试GPT-2对于验证正确性非常有价值，但它有明显的限制。了解这些限制有助于你正确地解释结果，并知道何时是时候转向基于GPU的实验。
- en: Speed constraints
  id: totrans-2469
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 速度限制
- en: 'The most obvious limitation is speed. CPUs are optimized for general-purpose
    tasks, not the massive parallelism that neural networks demand. A single forward
    and backward pass on GPT-2 124M can take seconds or even minutes on CPU, while
    a GPU might handle it in milliseconds. This makes:'
  id: totrans-2470
  prefs: []
  type: TYPE_NORMAL
  zh: 最明显的限制是速度。CPU针对通用任务进行了优化，而不是神经网络所要求的巨大并行性。在GPT-2 124M上单次正向和反向传递可能需要几秒钟甚至几分钟，而GPU可能只需毫秒就能处理。这使得：
- en: 'Full-scale training impractical: training GPT-2 124M to convergence could take
    weeks or months on CPU.'
  id: totrans-2471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全规模训练不切实际：在CPU上训练GPT-2 124M以达到收敛可能需要数周或数月。
- en: 'Experiment cycles slower: testing new optimizations or debugging is slowed
    because each run takes longer.'
  id: totrans-2472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实验周期较慢：测试新的优化或调试会变慢，因为每次运行都需要更长的时间。
- en: For this reason, CPU testing is best suited for small-scale sanity checks, not
    full training runs.
  id: totrans-2473
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，CPU测试最适合进行小规模的健康检查，而不是完整的训练运行。
- en: Memory overhead
  id: totrans-2474
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 内存开销
- en: CPU memory is typically more abundant than GPU VRAM, but slower. The bottleneck
    often isn’t “do we have enough RAM?” but “how quickly can we move data in and
    out of memory?” As sequence length `T` grows, the activations balloon, and cache
    efficiency drops. This makes even medium-sized runs sluggish.
  id: totrans-2475
  prefs: []
  type: TYPE_NORMAL
  zh: CPU内存通常比GPU VRAM更丰富，但速度较慢。瓶颈往往不是“我们是否有足够的RAM？”而是“我们能否快速将数据移动到内存中或从内存中移除？”随着序列长度`T`的增长，激活量激增，缓存效率下降。这使得即使是中等规模的运行也会变得缓慢。
- en: Limited realism
  id: totrans-2476
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 有限的现实感
- en: 'Although CPU runs confirm that the math is correct, they don’t always reflect
    the realities of GPU execution. For example:'
  id: totrans-2477
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然CPU运行确认了数学是正确的，但它们并不总是反映GPU执行的现实。例如：
- en: CUDA kernels have different numerical characteristics (fused operations, different
    rounding).
  id: totrans-2478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA内核具有不同的数值特性（融合操作，不同的舍入）。
- en: GPU memory layouts can expose bugs that CPU arrays hide.
  id: totrans-2479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU内存布局可能会暴露CPU数组隐藏的bug。
- en: Parallel execution may create timing or synchronization issues that never appear
    on CPU.
  id: totrans-2480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行执行可能会产生在CPU上从未出现的时间或同步问题。
- en: So while CPU parity with PyTorch is necessary, it isn’t sufficient. You must
    repeat testing once CUDA code is introduced.
  id: totrans-2481
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，尽管与PyTorch的CPU兼容性是必要的，但并不足够。一旦引入CUDA代码，就必须重新进行测试。
- en: Loss of scale insights
  id: totrans-2482
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 缺乏规模洞察力
- en: A CPU test can prove correctness for a few batches, but it doesn’t tell you
    how the code scales under heavy load. On GPU, you learn about kernel efficiency,
    memory throughput, and distributed training. CPU tests simply don’t expose those
    concerns.
  id: totrans-2483
  prefs: []
  type: TYPE_NORMAL
  zh: CPU测试可以证明几个批次的正确性，但它不会告诉你代码在重负载下的扩展性。在GPU上，你了解内核效率、内存吞吐量和分布式训练。CPU测试根本不会暴露这些担忧。
- en: Why it matters
  id: totrans-2484
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: 'CPU testing is the foundation: it proves the algorithm is implemented correctly,
    step by step, without relying on specialized hardware. But if you stop there,
    you’ll miss the bigger picture of performance and scalability. CPU results should
    be treated as a green light to proceed, not the final word on readiness.'
  id: totrans-2485
  prefs: []
  type: TYPE_NORMAL
  zh: CPU测试是基础：它证明了算法是正确实现的，一步一步地，不依赖于专用硬件。但如果你就止步于此，你将错过性能和可扩展性的更大图景。CPU结果应被视为继续前进的绿灯，而不是准备就绪的最终结论。
- en: Example scenario
  id: totrans-2486
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例场景
- en: Suppose you run 500 training steps on Tiny Shakespeare. The loss curve drops
    exactly as expected—success. But training on CPU is so slow that finishing an
    epoch takes several hours. This validates correctness, but makes it obvious that
    GPUs are required for meaningful experiments.
  id: totrans-2487
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你在Tiny Shakespeare上运行了500个训练步骤。损失曲线如预期般精确下降——成功。但CPU上的训练速度如此之慢，完成一个epoch需要几个小时。这验证了正确性，但很明显，进行有意义的实验需要GPU。
- en: Try it yourself
  id: totrans-2488
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 亲自试试
- en: Train GPT-2 124M on CPU for 100 steps and record the time per step. Extrapolate
    how long it would take to run 100k steps.
  id: totrans-2489
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在CPU上训练GPT-2 124M 100步，并记录每步的时间。外推运行100k步需要多长时间。
- en: Increase sequence length from 64 to 512 and observe how memory access times
    affect throughput.
  id: totrans-2490
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将序列长度从64增加到512，并观察内存访问时间如何影响吞吐量。
- en: Compare your CPU loss curve with a GPU run from PyTorch. Notice they align in
    shape but differ dramatically in speed.
  id: totrans-2491
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将你的CPU损失曲线与PyTorch的GPU运行结果进行比较。注意它们的形状一致，但速度差异很大。
- en: Use profiling tools (`perf`, `valgrind`, or `gprof`) to see which CPU functions
    dominate runtime.
  id: totrans-2492
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用分析工具（`perf`、`valgrind`或`gprof`）来查看哪些CPU函数主导了运行时间。
- en: The takeaway
  id: totrans-2493
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸取的经验
- en: CPU testing is the safe laboratory where you validate correctness, catch numerical
    errors, and reproduce known loss curves. But its limitations—slow speed, reduced
    realism, and lack of scaling insights—mean it’s only a first step. Once CPU testing
    passes, the journey continues with GPU testing, profiling, and multi-device scaling.
  id: totrans-2494
  prefs: []
  type: TYPE_NORMAL
  zh: CPU测试是安全实验室，你可以在这里验证正确性，捕捉数值错误，并重现已知的损失曲线。但其局限性——速度慢、现实感降低和缺乏可扩展性见解——意味着它只是一个第一步。一旦CPU测试通过，旅程将继续进行GPU测试、分析和多设备扩展。
- en: Chapter 7\. CUDA Training (`train_gpt2.cu`)
  id: totrans-2495
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第7章. CUDA训练（`train_gpt2.cu`）
- en: 61\. CUDA Architecture Overview (streams, kernels)
  id: totrans-2496
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 61. CUDA架构概述（流、内核）
- en: When the CPU version of *llm.c* runs, it executes instructions one after another
    on your processor cores. This is fine for small models or debugging, but deep
    learning workloads—especially Transformers like GPT-2—demand an enormous number
    of floating-point operations. To handle that, *llm.c* also includes CUDA versions
    of the training loop that shift computation to NVIDIA GPUs.
  id: totrans-2497
  prefs: []
  type: TYPE_NORMAL
  zh: 当*llm.c*的CPU版本运行时，它会在你的处理器核心上依次执行指令。这对于小型模型或调试来说是可行的，但对于深度学习工作负载——特别是像GPT-2这样的Transformer——需要大量的浮点运算。为了处理这一点，*llm.c*还包括了CUDA版本的训练循环，将计算转移到NVIDIA
    GPU上。
- en: At a high level, CUDA is NVIDIA’s programming model that lets developers write
    code to run directly on the GPU. Unlike CPUs, which might have a few cores optimized
    for general-purpose tasks, GPUs contain thousands of simpler cores designed to
    process large batches of data in parallel. CUDA provides the tools to organize
    work so that those cores can stay busy.
  id: totrans-2498
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，CUDA是NVIDIA的编程模型，允许开发者编写代码直接在GPU上运行。与可能只有几个针对通用任务优化的核心的CPU不同，GPU包含成千上万个设计用于并行处理大量数据的简单核心。CUDA提供了组织工作的工具，以便这些核心保持忙碌。
- en: 'Kernels: Small Programs That Run on the GPU'
  id: totrans-2499
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 内核：在GPU上运行的程序
- en: In CUDA, a *kernel* is a function that runs on the GPU. When you launch a kernel,
    you don’t call it once like a normal C function—you launch thousands of copies
    at the same time. Each copy handles a different piece of the data. For example,
    if you want to multiply two vectors of a million elements, you can launch a million
    GPU threads, each multiplying one pair of numbers.
  id: totrans-2500
  prefs: []
  type: TYPE_NORMAL
  zh: 在CUDA中，*内核*是运行在GPU上的函数。当你启动一个内核时，你不会像调用普通C函数那样只调用一次——你同时启动成千上万的副本。每个副本处理数据的不同部分。例如，如果你想乘以两个包含一百万个元素的向量，你可以启动一百万个GPU线程，每个线程乘以一对数字。
- en: 'In *llm.c*, kernels are used for operations that can be expressed in terms
    of lots of small, independent tasks. Examples include:'
  id: totrans-2501
  prefs: []
  type: TYPE_NORMAL
  zh: 在*llm.c*中，内核用于可以表示为大量小型、独立任务的运算。例如包括：
- en: Applying the GeLU activation function elementwise to a big tensor.
  id: totrans-2502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将GeLU激活函数逐元素应用于大张量。
- en: Adding residual connections across every dimension.
  id: totrans-2503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个维度上添加残差连接。
- en: Normalizing values in LayerNorm.
  id: totrans-2504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在LayerNorm层中归一化值。
- en: For bigger, structured operations like matrix multiplications (GEMMs), the CUDA
    code often relies on specialized libraries such as cuBLAS or cuBLASLt, which are
    highly tuned for NVIDIA GPUs.
  id: totrans-2505
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像矩阵乘法（GEMMs）这样的大规模结构化操作，CUDA代码通常依赖于如cuBLAS或cuBLASLt这样的专用库，这些库针对NVIDIA GPU进行了高度优化。
- en: 'Streams: Overlapping Work'
  id: totrans-2506
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 流：重叠工作
- en: 'A GPU has the ability to handle multiple tasks at once. CUDA introduces the
    idea of *streams*, which are sequences of operations that run in order relative
    to each other, but can overlap with operations in other streams. This means:'
  id: totrans-2507
  prefs: []
  type: TYPE_NORMAL
  zh: GPU能够同时处理多个任务。CUDA引入了*流*的概念，这些流是按照相对顺序运行的操作的序列，但可以与其他流中的操作重叠。这意味着：
- en: While one kernel is executing, another can start transferring data between CPU
    and GPU.
  id: totrans-2508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当一个内核正在执行时，另一个内核可以开始将数据在CPU和GPU之间传输。
- en: Computation and communication can overlap, reducing idle time.
  id: totrans-2509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算和通信可以重叠，减少空闲时间。
- en: In the training loop of *llm.c*, streams let you schedule batches of work so
    that data preparation and model computation can proceed side by side. This is
    crucial for keeping the GPU saturated with useful work instead of waiting on the
    CPU.
  id: totrans-2510
  prefs: []
  type: TYPE_NORMAL
  zh: 在*llm.c*的训练循环中，流允许你安排工作批次，以便数据准备和模型计算可以并行进行。这对于保持GPU充满有用的工作而不是等待CPU至关重要。
- en: The Memory Hierarchy
  id: totrans-2511
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 内存层次结构
- en: 'CUDA programming is also shaped by the GPU memory hierarchy:'
  id: totrans-2512
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA编程也受到GPU内存层次结构的影响：
- en: 'Registers: Fastest, private to each thread.'
  id: totrans-2513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 寄存器：最快的，每个线程私有的。
- en: 'Shared Memory: Small chunks of memory shared among threads in a block; much
    faster than global memory.'
  id: totrans-2514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共享内存：在块中的线程之间共享的小块内存；比全局内存快得多。
- en: 'Global Memory: Large, but slower. This is where tensors like weights, activations,
    and gradients usually live.'
  id: totrans-2515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全局内存：大，但较慢。这是张量如权重、激活和梯度通常所在的地方。
- en: 'Host Memory (CPU RAM): Separate from GPU memory; transferring between them
    can be slow and should be minimized.'
  id: totrans-2516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主内存（CPU RAM）：与GPU内存分开；在它们之间传输可能会很慢，应该最小化。
- en: For example, in the attention kernel, partial results might be stored in registers
    or shared memory while processing a block of the sequence, before writing the
    final result back to global memory.
  id: totrans-2517
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在注意力内核中，在处理序列的块之前，部分结果可能存储在寄存器或共享内存中，然后再将最终结果写回全局内存。
- en: How This Fits Into *llm.c*
  id: totrans-2518
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 这如何适用于*llm.c*
- en: 'In `train_gpt2.cu`, most of the heavy lifting is done by calls into cuBLAS/cuBLASLt
    and cuDNN for matrix multiplications and attention. But understanding the CUDA
    model—kernels, streams, and memory—helps explain:'
  id: totrans-2519
  prefs: []
  type: TYPE_NORMAL
  zh: 在`train_gpt2.cu`中，大部分繁重的工作是通过cuBLAS/cuBLASLt和cuDNN对矩阵乘法和注意力进行调用完成的。但了解CUDA模型——内核、流和内存——有助于解释：
- en: Why we batch operations the way we do.
  id: totrans-2520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么我们以这种方式进行批量操作。
- en: Why minimizing data transfers between CPU and GPU is so important.
  id: totrans-2521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么最小化CPU和GPU之间的数据传输如此重要。
- en: How GPU kernels map naturally to the kinds of tensor operations GPT-2 requires.
  id: totrans-2522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU内核如何自然地映射到GPT-2所需的张量操作类型。
- en: Why It Matters
  id: totrans-2523
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: Without CUDA, training GPT-2 would be painfully slow, even on a powerful CPU.
    CUDA gives access to thousands of cores working in parallel, but it also requires
    careful programming to avoid bottlenecks. Knowing about kernels, streams, and
    memory hierarchy is the foundation for understanding later sections where we dive
    into matrix multiplication, attention, and optimization strategies.
  id: totrans-2524
  prefs: []
  type: TYPE_NORMAL
  zh: 没有CUDA，训练GPT-2将会非常缓慢，即使在功能强大的CPU上也是如此。CUDA提供了访问数千个并行工作的核心，但它也要求进行仔细的编程以避免瓶颈。了解内核、流和内存层次结构是理解后面章节的基础，在这些章节中我们将深入探讨矩阵乘法、注意力和优化策略。
- en: Try It Yourself
  id: totrans-2525
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 尝试自己动手做
- en: Write a simple CUDA kernel that adds two arrays elementwise. Compare its performance
    to a CPU loop.
  id: totrans-2526
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个简单的CUDA内核，用于逐元素相加两个数组。将其性能与CPU循环进行比较。
- en: Modify the kernel to use shared memory and see if it improves performance for
    larger arrays.
  id: totrans-2527
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改内核以使用共享内存，并查看它是否提高了大型数组的性能。
- en: 'Create two CUDA streams: one for computing a kernel, and another for copying
    data. Measure whether the operations overlap in time.'
  id: totrans-2528
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建两个CUDA流：一个用于计算内核，另一个用于复制数据。测量操作是否在时间上重叠。
- en: Use `nvprof` or `nsys` to profile a CUDA program and observe how kernels and
    memory transfers appear on the timeline.
  id: totrans-2529
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`nvprof`或`nsys`来分析CUDA程序，并观察内核和内存传输在时间线上的表现。
- en: Think about how you would split a big matrix multiplication across thousands
    of threads—each thread computing one row, one column, or one element? What are
    the tradeoffs?
  id: totrans-2530
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑你将如何将一个大矩阵乘法分配给数千个线程——每个线程计算一行、一列或一个元素？有哪些权衡？
- en: The Takeaway
  id: totrans-2531
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸收要点
- en: CUDA is not just about writing code for GPUs—it’s about rethinking computation
    as thousands of small tasks that can run side by side. Kernels handle the per-thread
    work, streams let you schedule and overlap operations, and the memory hierarchy
    dictates how to organize data for maximum speed. All of these ideas come together
    in *llm.c*’s CUDA implementation, making training feasible for models like GPT-2.
  id: totrans-2532
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 不仅仅是为 GPU 编写代码——它关于重新思考计算，将其视为成千上万的可以并行运行的小任务。内核处理每个线程的工作，流允许你调度和重叠操作，而内存层次结构决定了如何组织数据以实现最大速度。所有这些想法都在
    *llm.c* 的 CUDA 实现中汇聚，使得 GPT-2 等模型的可训练性成为可能。
- en: 62\. Matrix Multiplication via cuBLAS/cuBLASLt
  id: totrans-2533
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 62. 通过 cuBLAS/cuBLASLt 进行矩阵乘法
- en: 'Matrix multiplication—often called GEMM (General Matrix-Matrix Multiply)—is
    the beating heart of deep learning. In GPT-2, most of the computation comes from
    multiplying large matrices: projecting embeddings into query, key, and value vectors,
    applying attention weights, and processing the MLP feed-forward layers. On the
    CPU, we saw this done with nested loops and mild optimizations. On the GPU, however,
    we need far more efficient approaches. That’s where cuBLAS and cuBLASLt come in.'
  id: totrans-2534
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘法——通常称为 GEMM（通用矩阵-矩阵乘法）——是深度学习的核心。在 GPT-2 中，大部分计算都来自于乘以大矩阵：将嵌入投影到查询、键和值向量中，应用注意力权重，以及处理
    MLP 前馈层。在 CPU 上，我们看到了使用嵌套循环和轻微优化的实现。然而，在 GPU 上，我们需要更高效的途径。这就是 cuBLAS 和 cuBLASLt
    发挥作用的地方。
- en: Why Matrix Multiplication Is So Central
  id: totrans-2535
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么矩阵乘法如此核心
- en: 'Almost every step of a Transformer involves multiplying two big matrices:'
  id: totrans-2536
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎每个 Transformer 的步骤都涉及到乘以两个大矩阵：
- en: Embedding lookup can be seen as a matrix multiply between one-hot token vectors
    and the embedding table.
  id: totrans-2537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入查找可以看作是 one-hot 令牌向量和嵌入表之间的矩阵乘法。
- en: The attention mechanism computes dot products between queries and keys, followed
    by a weighted sum of values.
  id: totrans-2538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力机制计算查询和键之间的点积，然后是一个加权求和的值。
- en: The MLP applies two fully connected layers, each of which is essentially a GEMM.
  id: totrans-2539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLP 应用两个全连接层，每个层本质上都是一个 GEMM。
- en: If you profile GPT-2, you’ll find that GEMM operations dominate the runtime.
    That’s why NVIDIA’s libraries devote enormous effort to making these multiplications
    as fast as possible.
  id: totrans-2540
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你分析 GPT-2，你会发现 GEMM 操作主导了运行时间。这就是为什么 NVIDIA 的库投入了巨大的努力来尽可能快地实现这些乘法。
- en: 'cuBLAS: The Classic Workhorse'
  id: totrans-2541
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: cuBLAS：经典的工作马
- en: 'cuBLAS is NVIDIA’s GPU-accelerated version of the BLAS (Basic Linear Algebra
    Subprograms) library. It provides highly optimized implementations of GEMM and
    related routines. Under the hood, cuBLAS:'
  id: totrans-2542
  prefs: []
  type: TYPE_NORMAL
  zh: cuBLAS 是 NVIDIA 的 GPU 加速版本的 BLAS（基本线性代数子程序）库。它提供了 GEMM 和相关例程的高度优化实现。在底层，cuBLAS：
- en: Splits large matrices into tiles that fit into the GPU’s shared memory.
  id: totrans-2543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将大矩阵分割成适合 GPU 共享内存的瓦片。
- en: Schedules thousands of threads to compute different tiles in parallel.
  id: totrans-2544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调度成千上万的线程以并行计算不同的瓦片。
- en: Uses fused multiply-add (FMA) instructions for high throughput.
  id: totrans-2545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用融合乘加（FMA）指令以实现高吞吐量。
- en: Adapts to different GPU architectures to exploit Tensor Cores where available.
  id: totrans-2546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适应不同的 GPU 架构，以利用可用的 Tensor Cores。
- en: 'A typical call looks like this:'
  id: totrans-2547
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的调用看起来像这样：
- en: '[PRE142]'
  id: totrans-2548
  prefs: []
  type: TYPE_PRE
  zh: '[PRE142]'
- en: Here `A` and `B` are the input matrices, and `C` is the result. The function
    handles all the low-level scheduling.
  id: totrans-2549
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 `A` 和 `B` 是输入矩阵，`C` 是结果。该函数处理所有低级调度。
- en: 'cuBLASLt: The Flexible Successor'
  id: totrans-2550
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: cuBLASLt：灵活的后继者
- en: 'While cuBLAS is powerful, it’s somewhat rigid. cuBLASLt (Lightweight cuBLAS)
    is a newer API that adds:'
  id: totrans-2551
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 cuBLAS 功能强大，但有些僵化。cuBLASLt（轻量级 cuBLAS）是一个较新的 API，它增加了：
- en: Better support for mixed precision (e.g., FP16 or BF16 inputs with FP32 accumulation).
  id: totrans-2552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好的混合精度支持（例如，FP16 或 BF16 输入与 FP32 累加）。
- en: More control over algorithm selection, so developers can tune for performance
    or memory usage.
  id: totrans-2553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更多的算法选择控制，以便开发者可以针对性能或内存使用进行调优。
- en: Features like epilogues, which let you fuse additional operations (e.g., bias
    addition, activation functions) directly into the GEMM, reducing memory transfers.
  id: totrans-2554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像尾随操作这样的功能，允许你将额外的操作（例如，偏置添加、激活函数）直接融合到 GEMM 中，减少内存传输。
- en: In practice, cuBLASLt often outperforms cuBLAS because it can exploit Tensor
    Cores more aggressively and fuse multiple steps into a single kernel call.
  id: totrans-2555
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，cuBLASLt 通常比 cuBLAS 表现更好，因为它可以更积极地利用 Tensor Cores，并将多个步骤融合到一个内核调用中。
- en: Precision and Tensor Cores
  id: totrans-2556
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 精度和 Tensor Cores
- en: On modern NVIDIA GPUs (Volta, Turing, Ampere, Hopper), Tensor Cores accelerate
    matrix multiplications dramatically when using FP16, BF16, or TF32\. These special
    hardware units can perform matrix-multiply-and-accumulate on small blocks of numbers
    in a single instruction.
  id: totrans-2557
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代NVIDIA GPU（Volta、Turing、Ampere、Hopper）上，使用FP16、BF16或TF32时，Tensor Cores可以显著加速矩阵乘法。这些特殊硬件单元可以在单个指令中执行小数字块的矩阵乘法和累加。
- en: 'For example:'
  id: totrans-2558
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: On CPUs, multiplying two 16×16 matrices is done with many scalar multiplications.
  id: totrans-2559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在CPU上，两个16×16矩阵的乘法是通过许多标量乘法完成的。
- en: On GPUs with Tensor Cores, the entire block can be computed in one fused operation.
  id: totrans-2560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在具有Tensor Cores的GPU上，整个块可以在一个融合操作中计算。
- en: In GPT-2 training, using FP16 with cuBLASLt enables much higher throughput,
    while keeping master weights in FP32 to preserve numerical stability.
  id: totrans-2561
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPT-2训练中，使用cuBLASLt的FP16可以显著提高吞吐量，同时保持主权重在FP32以保持数值稳定性。
- en: Practical Example in *llm.c*
  id: totrans-2562
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*llm.c*中的实际示例'
- en: 'In `train_gpt2.cu`, most of the calls to perform linear layers—such as projecting
    input activations into query, key, and value matrices—are implemented with cuBLAS/cuBLASLt.
    For instance:'
  id: totrans-2563
  prefs: []
  type: TYPE_NORMAL
  zh: 在`train_gpt2.cu`中，大多数执行线性层的调用——例如将输入激活投影到查询、键和值矩阵中——都是使用cuBLAS/cuBLASLt实现的。例如：
- en: Inputs `(B, T, C)` are multiplied by a weight matrix `(C, 3C)` to produce `(B,
    T, 3C)`.
  id: totrans-2564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入`(B, T, C)`与权重矩阵`(C, 3C)`相乘，以产生`(B, T, 3C)`。
- en: Later, the output of attention `(B, T, C)` is multiplied by another projection
    matrix `(C, C)`.
  id: totrans-2565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 之后，注意力输出`(B, T, C)`被乘以另一个投影矩阵`(C, C)`。
- en: Instead of writing custom kernels for each case, the code defers to cuBLAS/cuBLASLt,
    ensuring maximum performance across GPU architectures.
  id: totrans-2566
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是为每个情况编写自定义核，代码将委托给cuBLAS/cuBLASLt，确保在所有GPU架构上实现最大性能。
- en: Why It Matters
  id: totrans-2567
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: Matrix multiplications are so frequent and heavy that their performance directly
    determines how fast you can train GPT-2\. By leaning on cuBLAS/cuBLASLt, *llm.c*
    avoids reinventing the wheel and gets near-peak GPU efficiency. This makes the
    code clean, maintainable, and scalable to larger models.
  id: totrans-2568
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘法非常频繁且计算量大，其性能直接决定了你训练GPT-2的速度。通过依赖cuBLAS/cuBLASLt，*llm.c*避免了重新发明轮子，并接近GPU效率的峰值。这使得代码更加简洁、易于维护，并且可以扩展到更大的模型。
- en: Try It Yourself
  id: totrans-2569
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 尝试自己动手
- en: Write a small CUDA program that multiplies two matrices using naive kernels,
    and compare its performance to cuBLAS.
  id: totrans-2570
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个小型CUDA程序，使用原始内核乘以两个矩阵，并将其性能与cuBLAS进行比较。
- en: Experiment with FP32 versus FP16 inputs and observe the speedup when Tensor
    Cores are enabled.
  id: totrans-2571
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试FP32与FP16输入的对比，并观察启用Tensor Cores时的加速效果。
- en: Enable cuBLASLt’s epilogues to fuse bias addition into GEMM, and measure memory
    savings.
  id: totrans-2572
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用cuBLASLt的epilogues将偏置添加融合到GEMM中，并测量内存节省。
- en: Profile GPT-2 training with `nvprof` or `nsys` to see how much time is spent
    in GEMM calls.
  id: totrans-2573
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`nvprof`或`nsys`分析GPT-2的训练，以查看在GEMM调用中花费了多少时间。
- en: Try scaling up matrix sizes to simulate bigger models and note how performance
    grows relative to CPU implementations.
  id: totrans-2574
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试扩大矩阵大小以模拟更大的模型，并注意性能相对于CPU实现的增长。
- en: The Takeaway
  id: totrans-2575
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸收要点
- en: Matrix multiplication is the computational engine of GPT-2, and on GPUs it’s
    powered by cuBLAS and cuBLASLt. These libraries harness the GPU’s architecture—tiling,
    Tensor Cores, mixed precision—to squeeze out maximum efficiency. Understanding
    how they work gives insight into why the GPU version of *llm.c* runs so much faster
    than the CPU version, and sets the stage for attention kernels and other CUDA-accelerated
    components.
  id: totrans-2576
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘法是GPT-2的计算引擎，在GPU上由cuBLAS和cuBLASLt提供动力。这些库利用GPU的架构——分块、Tensor Cores、混合精度——以榨取最大效率。了解它们的工作原理可以揭示为什么*llm.c*的GPU版本比CPU版本运行得快得多，并为注意力内核和其他CUDA加速组件奠定了基础。
- en: '63\. Attention Kernels: cuDNN FlashAttention'
  id: totrans-2577
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 63. 注意力内核：cuDNN FlashAttention
- en: 'The attention mechanism is at the core of every Transformer. It allows the
    model to weigh different parts of the input sequence when producing an output.
    For GPT-2, this means that when generating the next token, the model doesn’t just
    look at the last word—it considers the entire sequence of words before it, adjusting
    how much each past token contributes. But attention is expensive. Naively, it
    scales quadratically with sequence length: for a sequence of 1024 tokens, you
    need to compute a 1024×1024 attention matrix. That’s more than a million entries,
    and each must be computed, normalized, and multiplied back into the value vectors.'
  id: totrans-2578
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制是每个 Transformer 的核心。它允许模型在生成输出时权衡输入序列的不同部分。对于 GPT-2 来说，这意味着在生成下一个标记时，模型不仅查看最后一个单词——它还考虑了它之前的整个单词序列，调整每个过去标记的贡献程度。但注意力是昂贵的。直观地，它与序列长度成平方关系：对于
    1024 个标记的序列，你需要计算一个 1024×1024 的注意力矩阵。这超过了一百万个条目，每个都必须计算、归一化并乘回值向量。
- en: 'On CPUs, we saw how this is implemented step by step: queries, keys, and values
    are projected with matrix multiplications, dot products between queries and keys
    are computed, softmax is applied, and the result is multiplied by values. On GPUs,
    we want to do the same thing, but much faster. That’s where cuDNN’s FlashAttention
    comes into play.'
  id: totrans-2579
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CPU 上，我们看到了它是如何一步一步实现的：查询、键和值通过矩阵乘法进行投影，计算查询和键之间的点积，应用 softmax，然后将结果乘以值。在
    GPU 上，我们想要做同样的事情，但更快。这就是 cuDNN 的 FlashAttention 发挥作用的地方。
- en: What Is FlashAttention?
  id: totrans-2580
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 什么是 FlashAttention？
- en: FlashAttention is an algorithm that rethinks how attention is computed. Instead
    of materializing the full attention matrix in memory, it computes softmax and
    the weighted sum in a streaming fashion. This reduces memory usage and improves
    cache efficiency.
  id: totrans-2581
  prefs: []
  type: TYPE_NORMAL
  zh: FlashAttention 是一种重新思考注意力计算方式的算法。它不是在内存中实现完整的注意力矩阵，而是以流式的方式计算 softmax 和加权求和。这减少了内存使用并提高了缓存效率。
- en: 'Normally, attention involves these steps:'
  id: totrans-2582
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，注意力涉及以下步骤：
- en: Compute scores = Q × Kᵀ (queries times keys transpose).
  id: totrans-2583
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算得分 = Q × Kᵀ（查询乘以键的转置）。
- en: Apply softmax over scores to get attention weights.
  id: totrans-2584
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对得分应用 softmax 以获得注意力权重。
- en: Multiply weights × V (values) to get the output.
  id: totrans-2585
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将权重乘以 V（值）以获得输出。
- en: 'The problem: step 1 produces a huge score matrix of size `(sequence_length
    × sequence_length)`. Storing and processing this full matrix becomes the bottleneck.'
  id: totrans-2586
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：步骤 1 生成一个大小为 `(sequence_length × sequence_length)` 的巨大得分矩阵。存储和处理这个完整矩阵成为瓶颈。
- en: FlashAttention avoids storing the full matrix by computing attention block by
    block. It processes tiles of queries and keys, applies the softmax incrementally,
    and accumulates results directly into the output. This drastically cuts memory
    bandwidth requirements, which is critical for GPUs.
  id: totrans-2587
  prefs: []
  type: TYPE_NORMAL
  zh: FlashAttention 通过分块计算注意力来避免存储完整矩阵。它处理查询和键的块，逐步应用 softmax，并将结果直接累加到输出中。这极大地减少了内存带宽需求，这对于
    GPU 来说是至关重要的。
- en: cuDNN FlashAttention in Practice
  id: totrans-2588
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: cuDNN FlashAttention 实践
- en: 'In *llm.c*’s CUDA training code, when `USE_CUDNN` is enabled, the code can
    take advantage of cuDNN’s implementation of FlashAttention. This means:'
  id: totrans-2589
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *llm.c* 的 CUDA 训练代码中，当 `USE_CUDNN` 被启用时，代码可以利用 cuDNN 对 FlashAttention 的实现。这意味着：
- en: The library handles the tiling and streaming automatically.
  id: totrans-2590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该库自动处理分块和流式处理。
- en: It can leverage Tensor Cores for mixed-precision computation (FP16/BF16 inputs
    with FP32 accumulation).
  id: totrans-2591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以利用 Tensor Cores 进行混合精度计算（FP16/BF16 输入与 FP32 累加）。
- en: It reduces memory use, which allows training longer sequences without running
    out of GPU memory.
  id: totrans-2592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它减少了内存使用，这使得在不会耗尽 GPU 内存的情况下训练更长的序列成为可能。
- en: From a developer’s point of view, enabling cuDNN FlashAttention usually involves
    passing specific descriptors and flags to cuDNN routines rather than writing custom
    kernels. Instead of manually managing loops and softmax stability tricks, you
    hand over the responsibility to cuDNN, which has a heavily optimized kernel.
  id: totrans-2593
  prefs: []
  type: TYPE_NORMAL
  zh: 从开发者的角度来看，启用 cuDNN FlashAttention 通常涉及向 cuDNN 例程传递特定的描述符和标志，而不是编写自定义内核。你不需要手动管理循环和
    softmax 稳定性技巧，而是将责任交给 cuDNN，它有一个高度优化的内核。
- en: Why This Is a Game-Changer
  id: totrans-2594
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 这为什么是一个变革者
- en: 'The quadratic cost of attention has long been a bottleneck in scaling Transformers.
    With FlashAttention, the bottleneck shifts. The computation is still O(N²), but
    because memory is handled so much more efficiently, the GPU spends less time waiting
    on memory loads and more time doing actual math. This means:'
  id: totrans-2595
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力的二次成本一直是扩展 Transformer 的瓶颈。随着 FlashAttention 的出现，瓶颈转移了。计算仍然是 O(N²)，但由于内存处理得更加高效，GPU
    在内存加载上的等待时间减少，有更多时间进行实际计算。这意味着：
- en: Training can be faster even at the same sequence length.
  id: totrans-2596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使在相同的序列长度下，训练速度也可以更快。
- en: You can push to larger sequence lengths (e.g., 2K or 4K tokens) without running
    out of GPU memory.
  id: totrans-2597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以将序列长度推得更大（例如，2K 或 4K 标记），而不会耗尽 GPU 内存。
- en: Energy efficiency improves because you avoid redundant reads/writes to global
    memory.
  id: totrans-2598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于避免了全局内存的冗余读写，能效得到了提升。
- en: 'Example: Why Memory Access Matters'
  id: totrans-2599
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例：为什么内存访问很重要
- en: Let’s imagine a toy example with 4 tokens. A naive implementation might build
    a 4×4 attention matrix, compute softmax, and multiply by values. That’s fine for
    4 tokens, but with 1024 tokens, you’d be juggling matrices of a million entries.
    Even if each entry is just 2 bytes (FP16), that’s megabytes of temporary storage
    per step. On a real GPU, constantly moving that in and out of global memory slows
    everything down.
  id: totrans-2600
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们想象一个有 4 个标记的玩具示例。一个原生实现可能会构建一个 4×4 的注意力矩阵，计算 softmax，然后乘以值。对于 4 个标记来说这没问题，但如果有
    1024 个标记，你将需要处理百万条记录的矩阵。即使每个条目只是 2 字节（FP16），每一步都会有兆字节的临时存储。在真实的 GPU 上，不断将数据在全局内存中移动会减慢一切。
- en: 'FlashAttention says: instead of storing the whole million entries, compute
    them in chunks, normalize them on the fly, and immediately use them to update
    the output. This way, only small temporary blocks live in memory, and global memory
    pressure drops dramatically.'
  id: totrans-2601
  prefs: []
  type: TYPE_NORMAL
  zh: FlashAttention 表示：与其存储全部一百万条记录，不如分块计算，即时归一化，并立即用于更新输出。这样，只有小块临时数据块存在于内存中，全局内存压力显著降低。
- en: How It Shows Up in GPT-2 Training
  id: totrans-2602
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 它如何在 GPT-2 训练中体现
- en: When GPT-2 processes a batch of sequences, each block of the model applies attention.
    In the CUDA version of *llm.c*, these attention calls can be routed through cuDNN
    FlashAttention. Practically, this means that the inner loop of training—the part
    that would otherwise grind on those giant attention matrices—becomes leaner and
    faster.
  id: totrans-2603
  prefs: []
  type: TYPE_NORMAL
  zh: 当 GPT-2 处理一批序列时，模型的每个块都会应用注意力。在 *llm.c* 的 CUDA 版本中，这些注意力调用可以通过 cuDNN FlashAttention
    进行路由。实际上，这意味着训练的内部循环——原本会在那些巨大的注意力矩阵上缓慢进行的部分——变得更加精简和快速。
- en: This matters even more as models grow. For GPT-2 124M (12 layers, 12 heads,
    1024 sequence length), attention is already expensive. For GPT-2 1.5B or LLaMA-style
    models with longer contexts, FlashAttention can be the difference between feasible
    training and “out of memory” errors.
  id: totrans-2604
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型变大时，这一点尤为重要。对于 GPT-2 124M（12 层，12 头，1024 序列长度），注意力已经非常昂贵。对于 GPT-2 1.5B 或具有更长上下文的
    LLaMA-style 模型，FlashAttention 可能是可行训练和“内存不足”错误之间的区别。
- en: Why It Matters
  id: totrans-2605
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 它为什么重要
- en: Attention is the defining operation of Transformers, but it’s also their Achilles
    heel. FlashAttention addresses the biggest inefficiency—memory bandwidth—without
    changing the model’s outputs. By using cuDNN’s optimized kernels, *llm.c* ensures
    it runs close to hardware peak performance while still producing correct results.
    For anyone learning about deep learning systems, this is a perfect example of
    how algorithmic innovations (streaming softmax) and hardware-level optimizations
    (Tensor Cores, tiling) combine to make state-of-the-art training practical.
  id: totrans-2606
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力是 Transformer 的定义性操作，但也是它们的阿喀琉斯之踵。FlashAttention 解决了最大的低效问题——内存带宽——而不改变模型的输出。通过使用
    cuDNN 的优化内核，*llm.c* 确保它在接近硬件峰值性能的同时，仍然产生正确的结果。对于任何学习深度学习系统的人来说，这是一个算法创新（流式 softmax）和硬件级优化（Tensor
    Cores，tiling）如何结合以实现最先进训练的完美示例。
- en: Try It Yourself
  id: totrans-2607
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 尝试自己动手
- en: Run GPT-2 training in *llm.c* with `USE_CUDNN=0` and then with `USE_CUDNN=1`.
    Compare training speed and GPU memory usage.
  id: totrans-2608
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 *llm.c* 中使用 `USE_CUDNN=0` 运行 GPT-2 训练，然后使用 `USE_CUDNN=1`。比较训练速度和 GPU 内存使用情况。
- en: Write a naive CUDA kernel that builds the full attention matrix, then benchmark
    it against cuDNN FlashAttention.
  id: totrans-2609
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个简单的 CUDA 内核来构建完整的注意力矩阵，然后将其与 cuDNN FlashAttention 进行基准测试。
- en: Vary sequence lengths (128, 512, 1024, 2048) and see how performance diverges
    between naive and FlashAttention implementations.
  id: totrans-2610
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 改变序列长度（128、512、1024、2048），看看原生实现和 FlashAttention 实现之间的性能差异。
- en: Examine how mixed precision interacts with FlashAttention—try FP16 versus BF16.
  id: totrans-2611
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查混合精度如何与 FlashAttention 交互——尝试 FP16 与 BF16。
- en: Explore the FlashAttention paper and compare its algorithmic explanation with
    what you see in practice using *llm.c*.
  id: totrans-2612
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 探索FlashAttention论文，并将其算法解释与你在使用*llm.c*实践中看到的内容进行比较。
- en: The Takeaway
  id: totrans-2613
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸取的教训
- en: Attention is expensive, but it doesn’t have to be crippling. FlashAttention
    shows that clever algorithm design plus hardware-aware implementation can shrink
    the memory bottleneck dramatically. By leaning on cuDNN’s implementation, *llm.c*
    can train GPT-2 models more efficiently, and learners get a real-world view of
    how deep learning libraries squeeze performance out of GPUs.
  id: totrans-2614
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力成本高昂，但不必造成致命打击。FlashAttention表明，巧妙的算法设计加上对硬件的感知实现可以显著减少内存瓶颈。通过依赖cuDNN的实现，*llm.c*可以更有效地训练GPT-2模型，学习者可以真正了解深度学习库如何从GPU中挤出性能。
- en: '64\. Mixed Precision: FP16/BF16 with Master FP32 Weights'
  id: totrans-2615
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 64. 混合精度：FP16/BF16与主FP32权重
- en: Training large models like GPT-2 involves multiplying and adding enormous amounts
    of numbers—billions of operations in every training step. GPUs can do this very
    quickly, but the type of numbers you use matters a lot. Traditionally, training
    is done in 32-bit floating point (FP32), which gives good precision but is heavy
    on memory and compute. Modern GPUs offer special hardware—Tensor Cores—that run
    much faster when using reduced precision, such as FP16 (half-precision floating
    point) or BF16 (bfloat16). This technique is called mixed precision training.
  id: totrans-2616
  prefs: []
  type: TYPE_NORMAL
  zh: 训练像GPT-2这样的大型模型涉及在每一步训练中乘以和添加大量的数字——每个训练步骤有数十亿个操作。GPU可以非常快速地完成这些操作，但你所使用的数字类型非常重要。传统上，训练是在32位浮点数（FP32）下进行的，这提供了良好的精度，但内存和计算成本很高。现代GPU提供了特殊的硬件——Tensor
    Cores，当使用降低精度（如FP16（半精度浮点数）或BF16（bfloat16））时，运行速度更快。这种技术称为混合精度训练。
- en: Why Mixed Precision Helps
  id: totrans-2617
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 混合精度为什么有帮助
- en: 'Using FP16 or BF16 has two main benefits:'
  id: totrans-2618
  prefs: []
  type: TYPE_NORMAL
  zh: 使用FP16或BF16有两个主要好处：
- en: 'Speed: GPUs can perform more FP16/BF16 operations per clock cycle than FP32\.
    For example, NVIDIA Tensor Cores are specifically designed to accelerate half-precision
    math, often delivering 2× or more throughput.'
  id: totrans-2619
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 速度：GPU每时钟周期可以执行比FP32更多的FP16/BF16操作。例如，NVIDIA Tensor Cores专门设计用于加速半精度数学，通常提供2倍或更高的吞吐量。
- en: 'Memory: FP16/BF16 values take half the storage of FP32\. That means you can
    fit larger batches or longer sequences into the same GPU memory, which is critical
    for scaling models.'
  id: totrans-2620
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 内存：FP16/BF16值占FP32存储空间的一半。这意味着你可以将更大的批次或更长的序列放入相同的GPU内存中，这对于模型扩展至关重要。
- en: 'But reduced precision comes with a tradeoff: it’s easier for numbers to underflow
    (become zero) or overflow (become infinity), which can destabilize training.'
  id: totrans-2621
  prefs: []
  type: TYPE_NORMAL
  zh: 但降低精度伴随着权衡：数字更容易下溢（变为零）或上溢（变为无穷大），这可能会破坏训练稳定性。
- en: Master Weights in FP32
  id: totrans-2622
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: FP32主权重
- en: 'The trick used in *llm.c* (and also in PyTorch and TensorFlow) is to keep a
    master copy of weights in FP32\. Here’s the process:'
  id: totrans-2623
  prefs: []
  type: TYPE_NORMAL
  zh: 在*llm.c*（以及PyTorch和TensorFlow）中使用的技巧是保留FP32权重的原始副本。以下是这个过程：
- en: During the forward pass, weights are cast to FP16/BF16 so the GPU can run the
    math on Tensor Cores.
  id: totrans-2624
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在前向传递过程中，权重被转换为FP16/BF16，以便GPU可以在Tensor Cores上运行数学运算。
- en: The gradients are computed in reduced precision as well.
  id: totrans-2625
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 梯度也在降低精度下计算。
- en: When it’s time to update parameters, the optimizer applies updates to the FP32
    master copy.
  id: totrans-2626
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当需要更新参数时，优化器将更新应用于FP32主副本。
- en: The updated master weights are cast back to FP16/BF16 for the next forward pass.
  id: totrans-2627
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新后的主权重被转换回FP16/BF16，以便进行下一次前向传递。
- en: This way, you get the speed and memory savings of mixed precision without fully
    losing the stability of FP32.
  id: totrans-2628
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，你可以在不完全失去FP32稳定性的情况下获得混合精度的速度和内存节省。
- en: FP16 vs. BF16
  id: totrans-2629
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: FP16与BF16的比较
- en: 'Both FP16 and BF16 use 16 bits, but they split the bits differently:'
  id: totrans-2630
  prefs: []
  type: TYPE_NORMAL
  zh: FP16和BF16都使用16位，但它们分割位的方式不同：
- en: '| Format | Exponent Bits | Mantissa Bits | Range | Precision |'
  id: totrans-2631
  prefs: []
  type: TYPE_TB
  zh: '| 格式 | 指数位 | 尾数位 | 范围 | 精度 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-2632
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| FP16 | 5 | 10 | Smaller | Higher precision for small numbers |'
  id: totrans-2633
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | 5 | 10 | 更小 | 小数的高精度 |'
- en: '| BF16 | 8 | 7 | Wider | Rougher precision, better range |'
  id: totrans-2634
  prefs: []
  type: TYPE_TB
  zh: '| BF16 | 8 | 7 | 更宽 | 粗糙精度，更好的范围 |'
- en: FP16 has better precision near zero but a narrower range, so it’s more prone
    to overflow.
  id: totrans-2635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FP16在零附近的精度更好，但范围较窄，因此更容易溢出。
- en: BF16 has the same exponent size as FP32, giving it a much wider range but less
    precision.
  id: totrans-2636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BF16与FP32具有相同的指数大小，因此它具有更宽的范围但精度较低。
- en: Modern NVIDIA GPUs (Ampere, Hopper) support both, but BF16 is often preferred
    for stability in very large models.
  id: totrans-2637
  prefs: []
  type: TYPE_NORMAL
  zh: 现代NVIDIA GPU（Ampere、Hopper）都支持这两种格式，但BF16通常因为在大模型中的稳定性而更受欢迎。
- en: Example in Practice
  id: totrans-2638
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实践中的例子
- en: Imagine training GPT-2 with a sequence length of 1024 and batch size of 32\.
    With FP32, the activations might take ~12 GB of GPU memory. Switching to FP16
    halves that to ~6 GB, leaving room for larger models or more sequences.
  id: totrans-2639
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下用序列长度为1024和批大小为32来训练GPT-2。使用FP32，激活可能需要大约12 GB的GPU内存。切换到FP16将其减半到大约6 GB，为更大的模型或更多的序列留出空间。
- en: 'In *llm.c*, enabling mixed precision means the forward pass can look something
    like this:'
  id: totrans-2640
  prefs: []
  type: TYPE_NORMAL
  zh: 在*llm.c*中，启用混合精度意味着前向传递可能看起来像这样：
- en: Cast embeddings, weights, and activations to FP16/BF16.
  id: totrans-2641
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将嵌入、权重和激活转换为FP16/BF16。
- en: Run matrix multiplications on Tensor Cores (very fast).
  id: totrans-2642
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Tensor Cores（非常快）上运行矩阵乘法。
- en: Compute gradients in reduced precision.
  id: totrans-2643
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在降低精度中计算梯度。
- en: Convert gradients back to FP32 for stable updates.
  id: totrans-2644
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将梯度转换回FP32以进行稳定的更新。
- en: This flow is invisible to the high-level code but handled internally in CUDA/cuBLAS/cuDNN
    calls.
  id: totrans-2645
  prefs: []
  type: TYPE_NORMAL
  zh: 这种流程对高级代码不可见，但在CUDA/cuBLAS/cuDNN调用中内部处理。
- en: Common Challenges
  id: totrans-2646
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 常见挑战
- en: 'Mixed precision introduces new wrinkles:'
  id: totrans-2647
  prefs: []
  type: TYPE_NORMAL
  zh: 混合精度引入了新的问题：
- en: 'Loss scaling: small gradients may underflow to zero in FP16\. The solution
    is to multiply the loss by a large factor during backpropagation, then divide
    gradients back later. This preserves information.'
  id: totrans-2648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失缩放：在FP16中，小的梯度可能下溢为零。解决方案是在反向传播期间将损失乘以一个大的因子，然后稍后除以梯度。这保留了信息。
- en: 'Debugging: NaNs and Infs become more common when switching to FP16\. Careful
    monitoring is required to catch these early.'
  id: totrans-2649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调试：当切换到FP16时，NaNs和Infs变得更加常见。需要仔细监控以捕捉这些早期情况。
- en: 'Performance tuning: Not all operations benefit equally from FP16\. For example,
    reductions (like summing a large array) may lose too much precision unless done
    in FP32.'
  id: totrans-2650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能调整：并非所有操作都能从FP16中获得同等的好处。例如，减少（如求和大型数组）可能会丢失太多精度，除非在FP32中执行。
- en: Why It Matters
  id: totrans-2651
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么它很重要
- en: Mixed precision is one of the key reasons modern Transformers can be trained
    efficiently on today’s hardware. Without it, many models would require double
    the GPU memory and much more time to train. By combining FP16/BF16 for speed and
    memory efficiency with FP32 master weights for stability, *llm.c* mirrors the
    strategy used in production frameworks. This shows how even a minimalist codebase
    can teach the cutting-edge tricks that power real-world large-scale training.
  id: totrans-2652
  prefs: []
  type: TYPE_NORMAL
  zh: 混合精度是现代Transformer能够在今天的硬件上高效训练的关键原因之一。没有它，许多模型将需要双倍的GPU内存和更多的时间来训练。通过结合FP16/BF16以实现速度和内存效率，以及FP32主权重以实现稳定性，*llm.c*反映了生产框架中使用的策略。这展示了即使是极简的代码库也能传授那些推动现实世界大规模训练的尖端技巧。
- en: Try It Yourself
  id: totrans-2653
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 尝试自己操作
- en: Train GPT-2 in *llm.c* with FP32 only, then repeat with FP16\. Compare memory
    usage (`nvidia-smi`) and runtime per step.
  id: totrans-2654
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在*llm.c*中使用FP32仅训练GPT-2，然后使用FP16重复训练。比较每步的内存使用情况（`nvidia-smi`）和运行时间。
- en: Experiment with FP16 vs. BF16 if your GPU supports both. Observe whether one
    is more stable.
  id: totrans-2655
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你的GPU同时支持FP16和BF16，请尝试比较它们。观察哪一个更稳定。
- en: Intentionally remove the FP32 master weights (update parameters in FP16 only)
    and see how quickly training diverges.
  id: totrans-2656
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 故意移除FP32主权重（仅在FP16中更新参数）并观察训练如何快速发散。
- en: Plot validation loss curves with FP32, FP16, and BF16 runs to see if the model
    quality differs.
  id: totrans-2657
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用FP32、FP16和BF16运行绘制验证损失曲线，以查看模型质量是否有所不同。
- en: Try scaling the batch size up with FP16 and note how much bigger a model you
    can fit into the same GPU.
  id: totrans-2658
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试使用FP16增加批大小，并注意你可以在同一GPU中放入多大的模型。
- en: The Takeaway
  id: totrans-2659
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸收要点
- en: 'Mixed precision combines the best of both worlds: the speed and memory efficiency
    of FP16/BF16 with the stability of FP32\. This technique has become a standard
    in deep learning, and *llm.c* demonstrates it in a clear, accessible way. It’s
    not just a neat optimization—it’s what makes training large language models on
    modern GPUs feasible at all.'
  id: totrans-2660
  prefs: []
  type: TYPE_NORMAL
  zh: 混合精度结合了两种技术的优点：FP16/BF16的速度和内存效率以及FP32的稳定性。这项技术已成为深度学习中的标准，*llm.c*以清晰、易于理解的方式展示了它。这不仅是一种整洁的优化——这是使在现代GPU上训练大型语言模型成为可能的关键。
- en: 65\. Loss Scaling in Mixed Precision Training
  id: totrans-2661
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 65. 混合精度训练中的损失缩放
- en: When training in mixed precision (FP16 or BF16), one of the biggest challenges
    is numerical underflow. Gradients can become so small that they round down to
    zero when represented in 16-bit format. If that happens too often, the optimizer
    stops receiving meaningful updates, and training can stagnate or collapse. To
    address this, frameworks introduce a technique called loss scaling.
  id: totrans-2662
  prefs: []
  type: TYPE_NORMAL
  zh: 在混合精度（FP16或BF16）训练时，最大的挑战之一是数值下溢。梯度可能变得非常小，以至于在16位格式中表示时四舍五入为零。如果这种情况发生得太频繁，优化器将停止接收有意义的更新，训练可能会停滞或崩溃。为了解决这个问题，框架引入了一种称为损失缩放的技术。
- en: The Core Idea
  id: totrans-2663
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 核心思想
- en: Loss scaling works by multiplying the loss value by a constant factor (called
    the scale factor) before starting backpropagation. Since gradients are proportional
    to the loss, this also multiplies all gradients by the same factor, making them
    larger and less likely to underflow when stored in FP16.
  id: totrans-2664
  prefs: []
  type: TYPE_NORMAL
  zh: 损失缩放通过在开始反向传播之前将损失值乘以一个常数因子（称为缩放因子）来实现。由于梯度与损失成正比，这也将所有梯度乘以相同的因子，使它们更大，在存储在FP16中时不太可能发生下溢。
- en: At the end of backpropagation, the gradients are divided by the same scale factor,
    restoring their correct values before the optimizer step.
  id: totrans-2665
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播结束时，将梯度除以相同的缩放因子，恢复它们在优化器步骤之前的正确值。
- en: 'Mathematically:'
  id: totrans-2666
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲：
- en: '`scaled_loss = loss × scale`'
  id: totrans-2667
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`scaled_loss = loss × scale`'
- en: Compute gradients of `scaled_loss` → produces `scaled_gradients`
  id: totrans-2668
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算`scaled_loss`的梯度→产生`scaled_gradients`
- en: '`true_gradients = scaled_gradients ÷ scale`'
  id: totrans-2669
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`true_gradients = scaled_gradients ÷ scale`'
- en: The optimizer then uses `true_gradients` to update the weights.
  id: totrans-2670
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器随后使用`true_gradients`来更新权重。
- en: Static vs. Dynamic Scaling
  id: totrans-2671
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 静态与动态缩放
- en: 'There are two common approaches:'
  id: totrans-2672
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种常见的方法：
- en: 'Static scaling: Use a fixed scale factor throughout training. For example,
    always multiply the loss by `1024`. This is simple but risky; if the scale is
    too high, gradients may overflow to infinity. If it’s too low, underflow still
    happens.'
  id: totrans-2673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 静态缩放：在整个训练过程中使用固定的缩放因子。例如，始终将损失乘以`1024`。这很简单，但风险很大；如果缩放因子过高，梯度可能会溢出到无穷大。如果过低，仍然会发生下溢。
- en: 'Dynamic scaling: Adjust the scale factor on the fly. If overflows (NaNs or
    Infs) are detected, the scale factor is reduced. If training proceeds smoothly,
    the scale factor is gradually increased. This balances stability and efficiency.'
  id: totrans-2674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态缩放：动态调整缩放因子。如果检测到溢出（NaNs或Infs），则减少缩放因子。如果训练顺利进行，则逐渐增加缩放因子。这平衡了稳定性和效率。
- en: In practice, dynamic scaling is the standard. Libraries like PyTorch’s `GradScaler`
    automatically handle this logic, so users don’t have to tweak values manually.
  id: totrans-2675
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，动态缩放是标准。像PyTorch的`GradScaler`这样的库会自动处理这种逻辑，因此用户不需要手动调整值。
- en: How It Appears in *llm.c*
  id: totrans-2676
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 它在*llm.c*中的表现
- en: 'The minimal design of *llm.c* doesn’t yet include automatic loss scaling, but
    the idea fits neatly into its training loop. Before calling `gpt2_backward`, you
    would scale the loss. After gradients are computed, you would unscale them before
    `gpt2_update`. Conceptually:'
  id: totrans-2677
  prefs: []
  type: TYPE_NORMAL
  zh: '*llm.c*的最小设计尚未包括自动损失缩放，但这个想法非常适合其训练循环。在调用`gpt2_backward`之前，你会缩放损失。在计算梯度之后，你会在`gpt2_update`之前取消缩放。从概念上讲：'
- en: '[PRE143]'
  id: totrans-2678
  prefs: []
  type: TYPE_PRE
  zh: '[PRE143]'
- en: This is not yet in the repository, but it’s how one could extend *llm.c* to
    support stable FP16 training.
  id: totrans-2679
  prefs: []
  type: TYPE_NORMAL
  zh: 这还不是存储库中的内容，但这是如何扩展*llm.c*以支持稳定的FP16训练的方法。
- en: Why It Matters
  id: totrans-2680
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: Without loss scaling, mixed precision can fail silently. Training might appear
    to run, but the gradients may be effectively zero for many parameters. This wastes
    GPU time and produces poor results. With loss scaling, FP16/BF16 training becomes
    both fast and reliable, combining the hardware speedups with numerical stability.
  id: totrans-2681
  prefs: []
  type: TYPE_NORMAL
  zh: 没有损失缩放，混合精度可能会无声失败。训练可能看起来在运行，但许多参数的梯度可能实际上为零。这浪费了GPU时间，并产生了较差的结果。有了损失缩放，FP16/BF16训练既快又可靠，结合了硬件加速和数值稳定性。
- en: Example Scenario
  id: totrans-2682
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例场景
- en: Suppose you are training GPT-2 with FP16 and notice that the validation loss
    barely decreases after several hundred steps. One possible reason is gradient
    underflow. By enabling loss scaling with a scale factor of 512 or 1024, you might
    suddenly see the loss curve behave normally again, matching the FP32 baseline.
  id: totrans-2683
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在使用FP16训练GPT-2，并注意到在几百步之后验证损失几乎没有减少。可能的原因之一是梯度下溢。通过启用缩放因子为512或1024的损失缩放，你可能会突然看到损失曲线恢复正常，与FP32基线相匹配。
- en: Try It Yourself
  id: totrans-2684
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 亲自尝试
- en: Train with FP16 but without loss scaling. Monitor whether the loss decreases
    meaningfully.
  id: totrans-2685
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用FP16进行训练但不进行损失缩放。监控损失是否显著减少。
- en: Add a static scale factor (like 512) and rerun. Observe improvements in stability.
  id: totrans-2686
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个静态缩放因子（如512）并重新运行。观察稳定性方面的改进。
- en: 'Implement a simple dynamic scaler: start with 128, double it if no NaNs appear
    for 100 steps, halve it if NaNs are detected.'
  id: totrans-2687
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现一个简单的动态缩放器：从128开始，如果100步内没有出现NaNs，则将其加倍，如果检测到NaNs，则将其减半。
- en: Compare training curves (FP32 vs. FP16 with and without scaling) to see the
    effect.
  id: totrans-2688
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较训练曲线（FP32与带缩放和不带缩放的FP16）以查看效果。
- en: Experiment with very large scale factors to trigger overflow intentionally,
    then watch how dynamic scaling recovers.
  id: totrans-2689
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试使用非常大的缩放因子来有意触发溢出，然后观察动态缩放如何恢复。
- en: The Takeaway
  id: totrans-2690
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸取的经验
- en: Loss scaling is the hidden ingredient that makes mixed precision training practical.
    By rescaling the loss and gradients, we protect tiny numbers from disappearing
    in FP16 while still enjoying the massive performance and memory benefits. Even
    in a minimal codebase like *llm.c*, understanding loss scaling bridges the gap
    between a model that trains poorly and one that matches FP32 performance at half
    the cost.
  id: totrans-2691
  prefs: []
  type: TYPE_NORMAL
  zh: 损失缩放是使混合精度训练成为可能的关键成分。通过重新缩放损失和梯度，我们保护了在 FP16 下消失的小数字，同时仍然享受巨大的性能和内存优势。即使在像
    *llm.c* 这样的最小代码库中，理解损失缩放也能弥合训练效果不佳的模型与在成本减半的情况下匹配 FP32 性能的模型之间的差距。
- en: 66\. Activation Checkpointing and Memory Tradeoffs
  id: totrans-2692
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 66. 激活检查点和内存权衡
- en: Training deep networks like GPT-2 involves storing a large number of activations—the
    intermediate outputs produced at every layer during the forward pass. These activations
    are needed later in the backward pass to compute gradients. The problem is that
    they take up a huge amount of GPU memory. For a 12-layer GPT-2 with long sequences
    and large batch sizes, activations can consume more memory than the model weights
    themselves.
  id: totrans-2693
  prefs: []
  type: TYPE_NORMAL
  zh: 训练像 GPT-2 这样的深度网络需要存储大量的激活——在正向传递的每一层产生的中间输出。这些激活在反向传递的后期需要用来计算梯度。问题是它们会占用大量的
    GPU 内存。对于一个具有长序列和大型批次的 12 层 GPT-2，激活可能比模型权重本身消耗更多的内存。
- en: Why Activations Matter
  id: totrans-2694
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么激活很重要
- en: Let’s say you have a batch size of 8, sequence length of 1024, hidden size of
    768, and 12 layers. Each layer produces an activation tensor of shape `(batch_size,
    sequence_length, hidden_size)`, or `8 × 1024 × 768`. That’s about 6.3 million
    numbers per layer. Multiply by 12 layers, and you have ~75 million numbers. At
    FP16, that’s around 150 MB per forward pass just for storing activations, and
    this grows with larger models.
  id: totrans-2695
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个批大小为 8，序列长度为 1024，隐藏大小为 768，和 12 层。每一层产生一个形状为 `(batch_size, sequence_length,
    hidden_size)` 的激活张量，即 `8 × 1024 × 768`。这大约是每层 630 万个数字。乘以 12 层，你就有大约 7500 万个数字。在
    FP16 下，这大约是每次正向传递仅存储激活的 150 MB，并且随着模型规模的增大而增长。
- en: If you scale up to GPT-2 Medium or GPT-2 XL, this number balloons quickly into
    gigabytes, which may not fit in GPU memory.
  id: totrans-2696
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你扩展到 GPT-2 Medium 或 GPT-2 XL，这个数字会迅速膨胀到几 GB，这可能不适合 GPU 内存。
- en: The Idea of Checkpointing
  id: totrans-2697
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 检查点（Checkpointing）的概念
- en: 'Activation checkpointing offers a tradeoff: instead of storing all activations,
    you only keep a small subset (the checkpoints) and recompute the rest during the
    backward pass.'
  id: totrans-2698
  prefs: []
  type: TYPE_NORMAL
  zh: 激活检查点提供了一种权衡：不是存储所有激活，你只保留一小部分（检查点），并在反向传递期间重新计算其余部分。
- en: 'During forward pass: save only checkpoints (for example, the activations at
    the end of each transformer block).'
  id: totrans-2699
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在前向传递期间：仅保存检查点（例如，每个转换器块末尾的激活）。
- en: 'During backward pass: when gradients for a layer are needed, recompute the
    missing activations by running part of the forward pass again.'
  id: totrans-2700
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在反向传递期间：当需要层的梯度时，通过再次运行部分前向传递来重新计算缺失的激活。
- en: This saves memory at the cost of extra computation.
  id: totrans-2701
  prefs: []
  type: TYPE_NORMAL
  zh: 这以额外的计算为代价节省了内存。
- en: How It Works in GPT-2
  id: totrans-2702
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 它在 GPT-2 中的工作方式
- en: 'A GPT-2 block has multiple steps: embedding lookup, attention, MLP, layer norm,
    residual connections. Normally, you’d store every output tensor. With checkpointing,
    you might only store the input to each block and discard the intermediate results.
    When backpropagation reaches that block, you rerun the forward pass locally to
    regenerate those results, then compute gradients.'
  id: totrans-2703
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2 块有多个步骤：嵌入查找、注意力、MLP、层归一化、残差连接。通常，你会存储每个输出的张量。使用检查点（checkpointing）时，你可能只存储每个块的输入并丢弃中间结果。当反向传播到达该块时，你会在本地重新运行前向传递以重新生成这些结果，然后计算梯度。
- en: This reduces memory usage almost linearly with the number of discarded activations,
    at the cost of roughly 30–40% more compute.
  id: totrans-2704
  prefs: []
  type: TYPE_NORMAL
  zh: 这几乎以丢弃的激活数量的线性方式减少内存使用，但代价是大约 30-40% 的额外计算。
- en: In *llm.c* Context
  id: totrans-2705
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在 *llm.c* 上下文中
- en: '*llm.c* doesn’t yet include activation checkpointing in its minimal implementation,
    but it’s a natural extension. In CUDA, this might be implemented by wrapping blocks
    of code with a “checkpoint” function that decides whether to save or discard activations.
    In PyTorch, the equivalent is `torch.utils.checkpoint`.'
  id: totrans-2706
  prefs: []
  type: TYPE_NORMAL
  zh: '*llm.c* 的最小实现中尚未包括激活检查点，但它是一个自然的扩展。在 CUDA 中，这可以通过使用一个决定是否保存或丢弃激活的“检查点”函数来包装代码块来实现。在
    PyTorch 中，等效的是 `torch.utils.checkpoint`。'
- en: If you train with longer sequences (e.g., 2048 tokens instead of 1024), checkpointing
    could mean the difference between fitting in memory or running into out-of-memory
    (OOM) errors.
  id: totrans-2707
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用更长的序列（例如，2048 个标记而不是 1024 个），检查点可能意味着在内存中适应或遇到内存不足（OOM）错误的区别。
- en: Why It Matters
  id: totrans-2708
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: 'Modern GPUs have enormous compute capacity, but memory remains the bottleneck.
    Checkpointing shifts the tradeoff: you spend a bit more compute (re-running some
    forward passes) in exchange for freeing up gigabytes of memory. This lets you:'
  id: totrans-2709
  prefs: []
  type: TYPE_NORMAL
  zh: 现代 GPU 具有巨大的计算能力，但内存仍然是瓶颈。检查点改变了权衡：你花费更多的计算（重新运行一些前向传递）以换取释放数 GB 的内存。这让你：
- en: Train larger models on the same hardware.
  id: totrans-2710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在相同的硬件上训练更大的模型。
- en: Use longer sequence lengths for better context handling.
  id: totrans-2711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用更长的序列长度以获得更好的上下文处理。
- en: Increase batch size for more stable gradients.
  id: totrans-2712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加批量大小以获得更稳定的梯度。
- en: In practice, this technique is used in nearly every large-scale Transformer
    training run today.
  id: totrans-2713
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，这种技术几乎被用于今天几乎所有的规模化 Transformer 训练运行中。
- en: Example Analogy
  id: totrans-2714
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 举例类比
- en: Think of it like studying for an exam. You could take detailed notes on every
    page of the textbook (storing all activations), but your notebook would get huge.
    Alternatively, you could only mark the chapter headings (checkpoints) and re-read
    sections when you need them during review (recomputation). It takes more time,
    but saves notebook space.
  id: totrans-2715
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下为考试做准备。你可以在教科书每一页上做详细的笔记（存储所有激活），但你的笔记本会变得很大。或者，你只需标记章节标题（检查点），并在复习时需要时重新阅读章节（重新计算）。这需要更多时间，但可以节省笔记本空间。
- en: Try It Yourself
  id: totrans-2716
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 尝试自己操作
- en: Run training on a GPU with long sequences until you hit an out-of-memory error.
  id: totrans-2717
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 GPU 上运行带有长序列的训练，直到遇到内存不足错误。
- en: Implement a simple checkpointing scheme where you only store activations every
    other layer.
  id: totrans-2718
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现一个简单的检查点方案，其中你只存储每隔一层激活。
- en: Measure how much memory usage decreases (using `nvidia-smi`) and how much runtime
    increases per step.
  id: totrans-2719
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `nvidia-smi` 测量内存使用量减少了多少，以及每步运行时间增加了多少。
- en: Experiment with different checkpointing frequencies—every layer, every 2 layers,
    every 4 layers—and find the balance between memory savings and compute overhead.
  id: totrans-2720
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试不同的检查点频率——每层、每 2 层、每 4 层——并找到内存节省和计算开销之间的平衡。
- en: Compare validation loss curves to confirm that checkpointing does not affect
    training quality (only runtime).
  id: totrans-2721
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将验证损失曲线进行比较，以确认检查点不会影响训练质量（只有运行时间）。
- en: The Takeaway
  id: totrans-2722
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸收教训
- en: Activation checkpointing is a clever strategy to bend the memory limits of GPUs.
    By discarding and recomputing activations on demand, you can fit models or sequence
    lengths that would otherwise be impossible. The tradeoff is extra computation,
    but with today’s hardware, compute is usually cheaper than memory. This technique
    is one of the quiet enablers behind scaling Transformers to billions of parameters.
  id: totrans-2723
  prefs: []
  type: TYPE_NORMAL
  zh: 激活检查点是弯曲 GPU 内存限制的一种巧妙策略。通过按需丢弃和重新计算激活，你可以适应模型或序列长度，否则将是不可能的。权衡是额外的计算，但以今天的硬件而言，计算通常比内存便宜。这项技术是扩展
    Transformer 到数十亿参数的幕后推动者之一。
- en: '67\. GPU Memory Planning: Parameters, Gradients, States'
  id: totrans-2724
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 67. GPU 内存规划：参数、梯度、状态
- en: When training a GPT-2 model on GPU, one of the most important practical challenges
    is managing memory. Every tensor—the model’s parameters, gradients, optimizer
    state, and activations—must fit into limited GPU RAM. Unlike CPUs, where you can
    often rely on swap space or large RAM pools, GPU memory is tight and unforgiving.
    If you exceed the limit, the program crashes with an out-of-memory error.
  id: totrans-2725
  prefs: []
  type: TYPE_NORMAL
  zh: 当在 GPU 上训练 GPT-2 模型时，最重要的实际挑战之一是管理内存。每个张量——模型的参数、梯度、优化器状态和激活——都必须适合有限的 GPU RAM。与
    CPU 不同，你通常可以依赖交换空间或大型的 RAM 池，GPU 内存紧张且不容忍。如果你超出限制，程序会因内存不足错误而崩溃。
- en: Breaking Down What Takes Memory
  id: totrans-2726
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分解占用内存的部分
- en: 'Parameters (Weights) These are the trainable values of the model: embeddings,
    attention projections, MLP weights, and so on. For GPT-2 124M, there are about
    124 million parameters.'
  id: totrans-2727
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 参数（权重）这些是模型的可训练值：嵌入、注意力投影、MLP 权重等。对于 GPT-2 124M，大约有 124 百万参数。
- en: In FP32, that’s roughly 500 MB.
  id: totrans-2728
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 FP32 中，这大约是 500 MB。
- en: In FP16/BF16, it’s about 250 MB. Larger GPT-2 models (355M, 774M, 1.5B) scale
    this up proportionally.
  id: totrans-2729
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 FP16/BF16 中，这大约是 250 MB。更大的 GPT-2 模型（355M、774M、1.5B）按比例增加。
- en: Gradients For each parameter, the backward pass produces a gradient tensor of
    the same size. If parameters take 500 MB, gradients also take ~500 MB in FP32\.
    Mixed precision can halve this.
  id: totrans-2730
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 梯度对于每个参数，反向传播会产生一个同样大小的梯度张量。如果参数占用500 MB，梯度在FP32下也占用约500 MB。混合精度可以将其减半。
- en: 'Optimizer States Optimizers like AdamW don’t just store gradients—they also
    track moving averages (`m` and `v`). Each adds another full-sized tensor. With
    AdamW, you often end up with 3× the parameter size: weights + m + v.'
  id: totrans-2731
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 优化器状态像AdamW这样的优化器不仅存储梯度，还跟踪移动平均值（`m`和`v`）。每个都添加了一个全尺寸张量。使用AdamW，你通常会有参数大小的3倍：权重
    + m + v。
- en: Activations During the forward pass, every layer’s intermediate outputs must
    be stored for the backward pass. This is often the largest single consumer of
    memory. For a 12-layer GPT-2 with sequence length 1024 and batch size 8, activations
    can easily exceed several GB. Checkpointing (as discussed earlier) helps reduce
    this.
  id: totrans-2732
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 激活在正向传播过程中，每个层的中间输出必须存储以供反向传播使用。这通常是内存消耗最大的单个组件。对于一个12层的GPT-2，序列长度为1024，批次大小为8，激活可以轻松超过几个GB。检查点（如前所述）有助于减少这一点。
- en: A Simple Calculation Example
  id: totrans-2733
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 简单计算示例
- en: 'For GPT-2 124M in FP32:'
  id: totrans-2734
  prefs: []
  type: TYPE_NORMAL
  zh: 对于FP32下的GPT-2 124M：
- en: 'Parameters: ~500 MB'
  id: totrans-2735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数：约500 MB
- en: 'Gradients: ~500 MB'
  id: totrans-2736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度：约500 MB
- en: 'AdamW states: ~1 GB (two copies)'
  id: totrans-2737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AdamW状态：约1 GB（两份副本）
- en: 'Activations: 2–4 GB (depends on batch size and sequence length)'
  id: totrans-2738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活：2–4 GB（取决于批次大小和序列长度）
- en: 'Total: ~3–6 GB, which fits on most modern GPUs.'
  id: totrans-2739
  prefs: []
  type: TYPE_NORMAL
  zh: 总计：约3–6 GB，这适合大多数现代GPU。
- en: 'For GPT-2 774M in FP32:'
  id: totrans-2740
  prefs: []
  type: TYPE_NORMAL
  zh: 对于FP32下的GPT-2 774M：
- en: 'Parameters: ~3 GB'
  id: totrans-2741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数：约3 GB
- en: 'Gradients: ~3 GB'
  id: totrans-2742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度：约3 GB
- en: 'AdamW states: ~6 GB'
  id: totrans-2743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AdamW状态：约6 GB
- en: 'Activations: 8–12 GB'
  id: totrans-2744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活：8–12 GB
- en: 'Total: ~20+ GB—too large for many GPUs unless you use tricks like FP16 and
    checkpointing.'
  id: totrans-2745
  prefs: []
  type: TYPE_NORMAL
  zh: 总计：约20+ GB——除非使用FP16和检查点等技巧，否则对于许多GPU来说太大。
- en: Strategies for Memory Efficiency
  id: totrans-2746
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 内存效率策略
- en: Mixed Precision Using FP16/BF16 cuts parameter, gradient, and optimizer sizes
    in half. Instead of 20 GB, you may get by with ~10 GB.
  id: totrans-2747
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 混合精度使用FP16/BF16可以将参数、梯度和优化器的大小减半。而不是20 GB，你可能只需要约10 GB。
- en: Activation Checkpointing Store fewer activations and recompute them during backpropagation.
    This often saves multiple GB.
  id: totrans-2748
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 激活检查点存储较少的激活并在反向传播期间重新计算它们。这通常可以节省多个GB。
- en: Gradient Accumulation Instead of training with a huge batch at once, split it
    into smaller micro-batches and accumulate gradients across them. This reduces
    activation memory requirements.
  id: totrans-2749
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 梯度累积而不是一次性用一个大批次进行训练，将其分成更小的微批次，并在它们之间累积梯度。这减少了激活内存需求。
- en: Parameter Sharding (Advanced) In multi-GPU setups, parameters and optimizer
    states can be split across devices (e.g., ZeRO optimizer in DeepSpeed). While
    not in *llm.c*, it’s a common technique at scale.
  id: totrans-2750
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 参数分片（高级）在多GPU设置中，参数和优化器状态可以跨设备分割（例如，DeepSpeed中的ZeRO优化器）。虽然*llm.c*中没有实现，但这是一种常见的规模化技术。
- en: In *llm.c*
  id: totrans-2751
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在*llm.c*中
- en: 'The `GPT2` struct organizes memory into fields like `params_memory`, `grads_memory`,
    `m_memory`, and `v_memory`. These are allocated as flat arrays, making it easy
    to calculate their size. This minimal design highlights the reality: for every
    parameter, there’s at least one matching gradient and potentially two optimizer
    state values.'
  id: totrans-2752
  prefs: []
  type: TYPE_NORMAL
  zh: '`GPT2`结构将内存组织成`params_memory`、`grads_memory`、`m_memory`和`v_memory`等字段。这些作为平面数组分配，使得计算它们的大小变得容易。这种最小化设计突显了现实：对于每个参数，至少有一个匹配的梯度，可能还有两个优化器状态值。'
- en: This structure mirrors how full frameworks like PyTorch allocate memory, but
    *llm.c* exposes it transparently so you can see exactly what’s taking up space.
  id: totrans-2753
  prefs: []
  type: TYPE_NORMAL
  zh: 这种结构反映了像PyTorch这样的完整框架如何分配内存，但*llm.c*透明地暴露了它，这样你可以确切地看到占用空间的是什么。
- en: Why It Matters
  id: totrans-2754
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: When training models, memory is usually the first limit you hit, not compute.
    Even if your GPU is powerful enough to handle the math, if you run out of memory,
    you can’t proceed. Understanding how parameters, gradients, optimizer states,
    and activations interact helps you design training runs that actually fit.
  id: totrans-2755
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型时，内存通常是首先遇到的限制，而不是计算。即使你的GPU足够强大以处理数学运算，如果你内存不足，你无法继续。了解参数、梯度、优化器状态和激活之间的交互可以帮助你设计出实际可行的训练运行。
- en: Try It Yourself
  id: totrans-2756
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 亲自尝试
- en: Calculate memory usage for GPT-2 124M, 355M, and 774M using FP32 vs FP16\. Compare
    your numbers to your GPU’s memory size.
  id: totrans-2757
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用FP32与FP16计算GPT-2 124M、355M和774M的内存使用量。将你的数字与你的GPU内存大小进行比较。
- en: Run *llm.c* with increasing batch sizes until you hit an out-of-memory error.
    Record the exact point where it breaks.
  id: totrans-2758
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以递增的批次大小运行*llm.c*，直到遇到内存不足错误。记录它崩溃的确切点。
- en: Enable mixed precision and checkpointing to see how much further you can push
    sequence length or batch size.
  id: totrans-2759
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用混合精度和检查点，看看你可以将序列长度或批量大小推进多远。
- en: Write a script to print the sizes of `params_memory`, `grads_memory`, and optimizer
    states in *llm.c*. Compare this to `nvidia-smi` output during training.
  id: totrans-2760
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个脚本来打印*llm.c*中`params_memory`、`grads_memory`和优化器状态的大小。将其与训练期间的`nvidia-smi`输出进行比较。
- en: Experiment with reducing optimizer states (e.g., try SGD instead of AdamW) to
    see the memory difference.
  id: totrans-2761
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试减少优化器状态（例如，尝试使用SGD而不是AdamW）以查看内存差异。
- en: The Takeaway
  id: totrans-2762
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸收要点
- en: Training Transformers is not just about writing the forward and backward passes—it’s
    about planning memory carefully. Parameters, gradients, optimizer states, and
    activations all compete for limited GPU RAM. By understanding these categories
    and using techniques like mixed precision and checkpointing, you can fit bigger
    models or longer contexts on the same hardware. This balance between memory and
    compute is at the heart of scaling modern deep learning.
  id: totrans-2763
  prefs: []
  type: TYPE_NORMAL
  zh: 训练Transformer不仅仅是编写正向和反向传递——它还涉及到精心规划内存。参数、梯度、优化器状态和激活都会竞争有限的GPU RAM。通过理解这些类别并使用混合精度和检查点等技术，你可以在相同的硬件上适应更大的模型或更长的上下文。这种内存和计算之间的平衡是现代深度学习扩展的核心。
- en: 68\. Kernel Launch Configurations and Occupancy
  id: totrans-2764
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 68. 内核启动配置和占用率
- en: When writing CUDA code for training models like GPT-2, one of the most important
    yet subtle factors in performance is how kernels are launched. A kernel is just
    a function that runs on the GPU, but the way you configure it—the number of threads,
    blocks, and how work is divided—can make the difference between a GPU that runs
    at 10% efficiency and one that’s near peak utilization.
  id: totrans-2765
  prefs: []
  type: TYPE_NORMAL
  zh: 当为训练GPT-2等模型编写CUDA代码时，性能中最重要的微妙因素之一是内核的启动方式。内核只是在GPU上运行的一个函数，但你配置它的方式——线程数、块数以及工作如何划分——可以决定GPU在10%效率运行和接近峰值利用率之间的差异。
- en: Threads, Blocks, and Grids
  id: totrans-2766
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 线程、块和网格
- en: 'CUDA organizes computation hierarchically:'
  id: totrans-2767
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA以分层方式组织计算：
- en: 'Thread: the smallest unit of execution. Each thread runs the kernel code once.'
  id: totrans-2768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线程：执行的最小单元。每个线程运行一次内核代码。
- en: 'Block: a collection of threads that share fast, on-chip memory (called shared
    memory).'
  id: totrans-2769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 块：一组共享快速片上内存（称为共享内存）的线程集合。
- en: 'Grid: a collection of blocks. Together, the grid represents the entire kernel
    launch.'
  id: totrans-2770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网格：一组块。一起，网格代表整个内核启动。
- en: 'When you launch a kernel, you decide:'
  id: totrans-2771
  prefs: []
  type: TYPE_NORMAL
  zh: 当你启动内核时，你决定：
- en: '[PRE144]'
  id: totrans-2772
  prefs: []
  type: TYPE_PRE
  zh: '[PRE144]'
- en: 'For example:'
  id: totrans-2773
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: '[PRE145]'
  id: totrans-2774
  prefs: []
  type: TYPE_PRE
  zh: '[PRE145]'
- en: Here, `N` might be the total number of elements to process. The division ensures
    that all elements get covered.
  id: totrans-2775
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`N`可能是要处理的元素总数。这个除法确保所有元素都被覆盖。
- en: What Is Occupancy?
  id: totrans-2776
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 什么是占用率？
- en: Occupancy refers to how many threads are active on a GPU relative to the maximum
    possible. Higher occupancy usually means the GPU is better utilized, but it’s
    not the only factor—memory access patterns and instruction throughput also matter.
  id: totrans-2777
  prefs: []
  type: TYPE_NORMAL
  zh: 占用率指的是GPU上相对于最大可能的线程数量。更高的占用率通常意味着GPU得到了更好的利用，但这不是唯一因素——内存访问模式和指令吞吐量也很重要。
- en: Each GPU has a fixed number of Streaming Multiprocessors (SMs), and each SM
    can support only a certain number of threads and blocks at once. If your kernel
    launch doesn’t provide enough threads, the GPU will be underutilized. If you launch
    too many, they may compete for shared memory and registers, leading to inefficiency.
  id: totrans-2778
  prefs: []
  type: TYPE_NORMAL
  zh: 每个GPU都有固定数量的流多处理器（SM），每个SM一次只能支持一定数量的线程和块。如果你的内核启动没有提供足够的线程，GPU将会被低效利用。如果你启动太多，它们可能会竞争共享内存和寄存器，导致效率低下。
- en: 'Example: A Simple Vector Add'
  id: totrans-2779
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例：一个简单的向量加法
- en: Suppose you want to add two arrays of size `N = 1e6`.
  id: totrans-2780
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想要添加两个大小为`N = 1e6`的数组。
- en: If you use `threads_per_block = 32`, you’ll have many tiny blocks. This wastes
    parallelism because modern GPUs are designed to run hundreds of threads per SM.
  id: totrans-2781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你使用`threads_per_block = 32`，你将会有很多微小的块。这浪费了并行性，因为现代GPU被设计为每个SM运行数百个线程。
- en: If you use `threads_per_block = 1024`, you may hit the hardware limit but run
    very large blocks that restrict scheduling flexibility.
  id: totrans-2782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你使用`threads_per_block = 1024`，你可能会遇到硬件限制，但运行非常大的块，这会限制调度灵活性。
- en: A good balance might be 256 or 512 threads per block, which lets the GPU overlap
    computation and memory access effectively.
  id: totrans-2783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个好的平衡可能是每个块256或512个线程，这样可以让GPU有效地重叠计算和内存访问。
- en: In Transformer Training
  id: totrans-2784
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在Transformer训练中
- en: 'For GPT-2 in *llm.c*, most heavy lifting is done by:'
  id: totrans-2785
  prefs: []
  type: TYPE_NORMAL
  zh: 在*llm.c*中，GPT-2的大部分重负载由以下部分完成：
- en: Matrix multiplications (handled by cuBLAS/cuBLASLt). These libraries pick kernel
    launch parameters automatically.
  id: totrans-2786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵乘法（由cuBLAS/cuBLASLt处理）。这些库会自动选择内核启动参数。
- en: Attention and normalization kernels (custom or cuDNN). When written by hand,
    launch configuration becomes crucial.
  id: totrans-2787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力和归一化内核（自定义或cuDNN）。当手动编写时，启动配置变得至关重要。
- en: 'For example, in a softmax kernel over a sequence of 1024 tokens:'
  id: totrans-2788
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在一个包含1024个标记的softmax内核中：
- en: You might launch one block per sequence row, with 1024 threads per block (one
    per token).
  id: totrans-2789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可能需要为每个序列行启动一个块，每个块有1024个线程（每个线程处理一个标记）。
- en: Alternatively, you might launch multiple blocks per row, each handling a tile
    of tokens, if shared memory limits require it.
  id: totrans-2790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或者，如果需要，你可以为每行启动多个块，每个块处理一个标记块。
- en: Choosing wisely can double or triple performance.
  id: totrans-2791
  prefs: []
  type: TYPE_NORMAL
  zh: 选择得当可以加倍或三倍性能。
- en: Balancing Factors
  id: totrans-2792
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 平衡因素
- en: 'When configuring kernels, you balance:'
  id: totrans-2793
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置内核时，你需要平衡：
- en: 'Occupancy: Are enough threads active to use the GPU fully?'
  id: totrans-2794
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 占用率：是否有足够的线程活跃以充分利用GPU？
- en: 'Memory Coalescing: Do threads access memory in aligned, sequential chunks (which
    is fast) or scattered patterns (which is slow)?'
  id: totrans-2795
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 内存对齐：线程是否以对齐的、顺序的块访问内存（这很快）或以分散的模式访问（这很慢）？
- en: 'Shared Memory and Registers: Each block has limited resources. If your kernel
    uses too much shared memory, fewer blocks can fit per SM, reducing occupancy.'
  id: totrans-2796
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 共享内存和寄存器：每个块都有有限的资源。如果你的内核使用了过多的共享内存，每个SM中可以容纳的块数就会减少，从而降低占用率。
- en: 'Arithmetic Intensity: If the kernel does a lot of math per memory load, occupancy
    matters less; if it’s memory-bound, occupancy matters more.'
  id: totrans-2797
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运算强度：如果内核在每次内存加载时进行大量数学运算，则占用率不那么重要；如果是内存受限，则占用率更重要。
- en: Why It Matters
  id: totrans-2798
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: In GPU training, kernel launch decisions directly control how efficiently hardware
    is used. Two kernels implementing the same math can differ by 5–10× in runtime
    purely because of launch configuration. cuBLAS and cuDNN automate much of this
    for matrix-heavy ops, but understanding it is crucial when writing custom kernels.
  id: totrans-2799
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPU训练中，内核启动决策直接控制硬件的使用效率。两个实现相同数学运算的内核，仅因为启动配置的不同，其运行时间可能相差5-10倍。cuBLAS和cuDNN自动化了大多数矩阵密集型操作，但理解这一点在编写自定义内核时至关重要。
- en: Try It Yourself
  id: totrans-2800
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 尝试自己操作
- en: Write a simple CUDA kernel for vector addition with different `threads_per_block`
    values (32, 128, 256, 512, 1024). Measure runtime and see which is fastest.
  id: totrans-2801
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个简单的CUDA内核用于具有不同`threads_per_block`值的向量加法（32、128、256、512、1024）。测量运行时间并查看哪个最快。
- en: Use `nvprof` or `nsys` to inspect occupancy of kernels during GPT-2 training.
    Note which kernels run at <50% occupancy.
  id: totrans-2802
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`nvprof`或`nsys`在GPT-2训练期间检查内核的占用率。注意哪些内核的占用率低于<50%。
- en: 'Experiment with a softmax kernel: launch one block per row vs. multiple blocks
    per row. Compare performance and memory use.'
  id: totrans-2803
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试使用softmax内核：每行启动一个块与每行启动多个块进行比较。比较性能和内存使用。
- en: Explore how shared memory allocation per block affects occupancy by artificially
    increasing shared memory usage.
  id: totrans-2804
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过人为增加共享内存使用量来探索每个块共享内存分配如何影响占用率。
- en: Compare cuBLAS GEMM (matrix multiply) performance to a naive CUDA implementation
    and observe how kernel configuration explains the speed difference.
  id: totrans-2805
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将cuBLAS GEMM（矩阵乘法）性能与原始CUDA实现进行比较，并观察内核配置如何解释速度差异。
- en: The Takeaway
  id: totrans-2806
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸收要点
- en: Kernel launch configuration is the hidden lever of GPU performance. By adjusting
    how threads and blocks are assigned, you control how much of the GPU is kept busy,
    how well memory bandwidth is used, and how smoothly computations flow. For models
    like GPT-2, libraries handle most kernels, but knowing what’s happening under
    the hood is key to writing or debugging efficient CUDA code.
  id: totrans-2807
  prefs: []
  type: TYPE_NORMAL
  zh: 内核启动配置是GPU性能的隐藏杠杆。通过调整线程和块的分配方式，你可以控制GPU的忙碌程度，内存带宽的使用情况以及计算的流畅度。对于像GPT-2这样的模型，库处理大多数内核，但了解底层发生的事情对于编写或调试高效的CUDA代码至关重要。
- en: 69\. CUDA Error Handling and Debugging
  id: totrans-2808
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 69. CUDA错误处理和调试
- en: When writing or running CUDA code, one of the most frustrating parts is that
    errors often don’t show up immediately. Unlike CPU code, where an invalid pointer
    or division by zero can crash right away, CUDA launches kernels asynchronously.
    This means the host code (running on the CPU) queues up GPU work and moves on,
    while the GPU processes it in the background. If something goes wrong inside the
    kernel, the error might not be visible until later—sometimes only after you try
    to synchronize.
  id: totrans-2809
  prefs: []
  type: TYPE_NORMAL
  zh: 当编写或运行CUDA代码时，最令人沮丧的部分之一是错误通常不会立即出现。与CPU代码不同，无效的指针或除以零可以立即崩溃，CUDA启动内核是异步的。这意味着主机代码（在CPU上运行）将GPU工作排队并继续执行，而GPU在后台处理。如果在内核内部出现问题，错误可能不会立即可见——有时只有在尝试同步后才会出现。
- en: Common Error Sources
  id: totrans-2810
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 常见错误来源
- en: Out-of-bounds memory access A kernel thread tries to read or write past the
    end of an array. This can silently produce incorrect results or crash the program.
  id: totrans-2811
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 越界内存访问 内核线程试图读取或写入数组的末尾。这可能会静默地产生不正确的结果或导致程序崩溃。
- en: Invalid memory alignment Some CUDA operations require pointers to be aligned.
    Misaligned access can degrade performance or trigger errors.
  id: totrans-2812
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 无效的内存对齐 一些CUDA操作需要指针对齐。未对齐的访问可能会降低性能或触发错误。
- en: Illegal instruction or unsupported hardware feature Using Tensor Cores on an
    older GPU, or using instructions not supported by your GPU’s compute capability,
    can fail.
  id: totrans-2813
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 非法指令或不受支持的硬件功能 在较旧的GPU上使用Tensor Core，或使用不支持您GPU计算能力的指令，可能会失败。
- en: Out of memory (OOM) Allocating more GPU memory than available causes runtime
    errors. Unlike CPU memory, GPUs cannot “swap” to disk.
  id: totrans-2814
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 内存不足（OOM） 分配比可用的GPU内存更多的内存会导致运行时错误。与CPU内存不同，GPU不能“交换”到磁盘。
- en: Race conditions Threads within a block or across blocks accessing the same memory
    location without synchronization can corrupt results.
  id: totrans-2815
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 竞态条件 块内或跨块访问相同内存位置而没有同步的线程可以破坏结果。
- en: How CUDA Reports Errors
  id: totrans-2816
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何报告CUDA错误
- en: 'Every CUDA runtime API call returns an error code. For example:'
  id: totrans-2817
  prefs: []
  type: TYPE_NORMAL
  zh: 每个CUDA运行时API调用都返回一个错误代码。例如：
- en: '[PRE146]'
  id: totrans-2818
  prefs: []
  type: TYPE_PRE
  zh: '[PRE146]'
- en: 'Similarly, after launching a kernel, you should check:'
  id: totrans-2819
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，在启动内核后，你应该检查：
- en: '[PRE147]'
  id: totrans-2820
  prefs: []
  type: TYPE_PRE
  zh: '[PRE147]'
- en: This pattern ensures that if something fails, you see it quickly instead of
    later.
  id: totrans-2821
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模式确保了如果出现问题，你可以快速看到它，而不是之后。
- en: Debugging Tools
  id: totrans-2822
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 调试工具
- en: cuda-gdb A GPU-aware debugger. Lets you step through CUDA kernels much like
    gdb on CPU code.
  id: totrans-2823
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: cuda-gdb 一个具有GPU感知的调试器。允许您像在CPU代码上使用gdb一样逐步执行CUDA内核。
- en: cuda-memcheck Detects out-of-bounds accesses, race conditions, and misaligned
    memory operations. Essential when kernels produce “mysterious” wrong outputs.
  id: totrans-2824
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: cuda-memcheck 检测越界访问、竞态条件和未对齐的内存操作。当内核产生“神秘”的错误输出时，这是必不可少的。
- en: Nsight Systems / Nsight Compute Profiling tools that show kernel timelines,
    occupancy, memory throughput, and errors.
  id: totrans-2825
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Nsight Systems / Nsight Compute 分析工具，可以显示内核时间线、占用率、内存吞吐量和错误。
- en: Sanity checks in code Often, simply inserting assertions (`assert(i < N)`) or
    zero-initializing memory can catch problems earlier.
  id: totrans-2826
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码中的合理性检查 通常，只需简单地插入断言（`assert(i < N)`) 或将内存初始化为零，就可以在早期捕获问题。
- en: Debugging in *llm.c*
  id: totrans-2827
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在 *llm.c* 中调试
- en: In *llm.c*, most of the CUDA-heavy lifting is handled by cuBLAS and cuDNN. But
    when experimenting with custom kernels (e.g., softmax, masking, or layernorm),
    debugging becomes crucial. A small indexing mistake could make training diverge
    or crash with `nan` losses. By adding `cudaGetLastError()` checks after every
    kernel launch, you can catch issues right where they happen.
  id: totrans-2828
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *llm.c* 中，大部分CUDA繁重的工作由cuBLAS和cuDNN处理。但当你实验自定义内核（例如softmax、掩码或layernorm）时，调试变得至关重要。一个小小的索引错误可能会导致训练发散或使用
    `nan` 损失崩溃。通过在每次内核启动后添加 `cudaGetLastError()` 检查，你可以在问题发生的地方立即捕获问题。
- en: 'Example: A Softmax Bug'
  id: totrans-2829
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例：一个Softmax错误
- en: 'Imagine a kernel computing softmax across 1024 tokens per row. If one thread
    index accidentally runs past 1024, it may read garbage memory. Without error checking,
    you might just see “loss is NaN” 100 steps later. With `cuda-memcheck`, you’d
    immediately see:'
  id: totrans-2830
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个内核正在每行计算1024个标记的softmax。如果某个线程索引意外超过了1024，它可能会读取垃圾内存。如果没有错误检查，你可能会在100步后看到“损失是NaN”。使用
    `cuda-memcheck`，你会立即看到：
- en: '[PRE148]'
  id: totrans-2831
  prefs: []
  type: TYPE_PRE
  zh: '[PRE148]'
- en: Now you know exactly where to fix the bug.
  id: totrans-2832
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你确切地知道在哪里修复这个错误。
- en: Why It Matters
  id: totrans-2833
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: Training large models is expensive. A single bug in a CUDA kernel can waste
    hours of GPU time, produce invalid gradients, or silently corrupt weights. Robust
    error handling and debugging practices save not only frustration but also significant
    cost.
  id: totrans-2834
  prefs: []
  type: TYPE_NORMAL
  zh: 训练大型模型成本高昂。CUDA内核中的一个错误可能会浪费数小时的GPU时间，产生无效的梯度，或者静默地损坏权重。健壮的错误处理和调试实践不仅可以节省挫折，还可以节省显著的成本。
- en: Try It Yourself
  id: totrans-2835
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 试试你自己
- en: Write a CUDA kernel with an intentional bug (e.g., forget to check array bounds).
    Run it with and without `cuda-memcheck` to see the difference.
  id: totrans-2836
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个带有故意错误的 CUDA 核心函数（例如，忘记检查数组边界）。运行它并带有 `cuda-memcheck`，以查看差异。
- en: Add `cudaGetLastError()` after every kernel in a simple project and watch how
    it pinpoints issues earlier.
  id: totrans-2837
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在简单项目中，在每个核函数后添加 `cudaGetLastError()` 并观察它是如何更早地定位问题的。
- en: 'Experiment with Nsight Systems: run GPT-2 training and inspect kernel launches,
    checking for errors or unexpected stalls.'
  id: totrans-2838
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试使用 Nsight Systems：运行 GPT-2 训练并检查内核启动，检查错误或意外的停滞。
- en: Train with bad initialization (e.g., NaNs in inputs) and see how error checking
    reports failures.
  id: totrans-2839
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用不良初始化（例如，输入中的 NaN）进行训练，并查看错误检查如何报告失败。
- en: Introduce a race condition by having two threads update the same memory without
    `__syncthreads()`. Debug using `cuda-memcheck`.
  id: totrans-2840
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过两个线程在没有 `__syncthreads()` 的情况下更新相同的内存来引入竞态条件。使用 `cuda-memcheck` 进行调试。
- en: The Takeaway
  id: totrans-2841
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸收的经验
- en: CUDA’s asynchronous nature makes error handling less straightforward than CPU
    programming. But with the right tools—error codes, synchronization, cuda-memcheck,
    and debuggers—you can systematically catch and fix problems. In *llm.c*, this
    discipline ensures that CUDA kernels not only run fast but also run correctly,
    which is just as important when training large-scale models.
  id: totrans-2842
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 的异步特性使得错误处理不如 CPU 编程直接。但是，有了正确的工具——错误代码、同步、cuda-memcheck 和调试器——你可以系统地捕捉和修复问题。在
    *llm.c* 中，这种纪律确保 CUDA 核心不仅运行得快，而且运行得正确，这在训练大规模模型时同样重要。
- en: '70\. `dev/cuda/`: From Simple Kernels to High Performance'
  id: totrans-2843
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 70. `dev/cuda/`：从简单核函数到高性能
- en: Inside the *llm.c* repository, there is a folder called `dev/cuda/`. At first
    glance it may look like a side experiment, but it’s actually one of the most instructive
    parts of the project. The main training files (`train_gpt2.cu`, `train_gpt2_fp32.cu`)
    rely heavily on cuBLAS and cuDNN—optimized libraries that already deliver near-peak
    performance. But if you want to understand how CUDA really works under the hood,
    you have to look at how kernels are written from scratch, and that’s exactly what
    this folder shows.
  id: totrans-2844
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *llm.c* 仓库内部，有一个名为 `dev/cuda/` 的文件夹。乍一看，它可能看起来像是一个辅助实验，但实际上它是项目中最具教育意义的部分之一。主要的训练文件（`train_gpt2.cu`，`train_gpt2_fp32.cu`）严重依赖于
    cuBLAS 和 cuDNN——这些优化库已经提供了接近峰值性能。但如果你想要了解 CUDA 究竟是如何在底层工作的，你必须查看从零开始编写的核心，这正是这个文件夹所展示的。
- en: Why This Folder Exists
  id: totrans-2845
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这个文件夹存在
- en: 'The goal of `dev/cuda/` is not to replace cuBLAS or cuDNN. Instead, it acts
    as a sandbox for:'
  id: totrans-2846
  prefs: []
  type: TYPE_NORMAL
  zh: '`dev/cuda/` 的目标不是取代 cuBLAS 或 cuDNN。相反，它作为一个沙盒，用于：'
- en: Building intuition about how GPU kernels are structured.
  id: totrans-2847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立关于 GPU 核心结构的直觉。
- en: Experimenting with small-scale implementations of operations like vector addition,
    matrix multiplication, or normalization.
  id: totrans-2848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试小规模实现如向量加法、矩阵乘法或归一化等操作。
- en: Comparing naive CUDA implementations to highly optimized library calls.
  id: totrans-2849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较原始的 CUDA 实现与高度优化的库调用。
- en: Teaching developers how memory layout, thread synchronization, and shared memory
    affect performance.
  id: totrans-2850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 教育开发者如何内存布局、线程同步和共享内存影响性能。
- en: 'It’s a bridge: start simple with “hello world” style kernels, then step closer
    to the performance tricks used by NVIDIA’s professional libraries.'
  id: totrans-2851
  prefs: []
  type: TYPE_NORMAL
  zh: 它是一座桥梁：从简单的“hello world”风格核函数开始，然后逐步接近 NVIDIA 专业库使用的性能技巧。
- en: A Journey from Naive to Optimized
  id: totrans-2852
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 从原始到优化的旅程
- en: Simple Elementwise Kernels The first step is usually a kernel where each thread
    processes one element. For example, adding two vectors `C[i] = A[i] + B[i]`. This
    teaches indexing, memory coalescing, and the idea of grids and blocks.
  id: totrans-2853
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 简单的元素级核函数 第一步通常是一个核函数，其中每个线程处理一个元素。例如，添加两个向量 `C[i] = A[i] + B[i]`。这教会了索引、内存归约以及网格和块的概念。
- en: Reduction Kernels Next, you move to slightly harder tasks like summing an array.
    Now you need thread cooperation and synchronization (`__syncthreads()`), plus
    shared memory usage.
  id: totrans-2854
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 减少核函数接下来，你将转向稍微困难一些的任务，比如求和数组。现在你需要线程合作和同步 (`__syncthreads()`)，以及共享内存的使用。
- en: 'Matrix Multiplication (GEMM) A naive kernel might have each thread compute
    one output element by looping over the input dimension. It works, but is slow
    because it reloads data from global memory repeatedly. The optimized version uses
    tiling: load a tile of the matrix into shared memory, let threads reuse it many
    times, and then move to the next tile. This can speed up performance by 10× or
    more.'
  id: totrans-2855
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 矩阵乘法（GEMM）一个原始内核可能让每个线程通过遍历输入维度来计算一个输出元素。它可行，但速度很慢，因为它反复从全局内存中加载数据。优化版本使用瓦片：将矩阵的瓦片加载到共享内存中，让线程多次重用它，然后移动到下一个瓦片。这可以将性能提高10倍或更多。
- en: Advanced Optimizations Later examples may add warp-level primitives, vectorized
    loads, and Tensor Core usage. These bring performance closer to cuBLAS.
  id: totrans-2856
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 后续的高级优化中可能会添加warp级别的原语、向量化的加载和Tensor Core的使用。这些可以将性能提升到接近cuBLAS。
- en: Educational Value
  id: totrans-2857
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 教育价值
- en: 'Seeing these steps side by side makes the performance story very tangible:'
  id: totrans-2858
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些步骤并排放置，使性能故事非常具体：
- en: A naive GEMM kernel might achieve 1% of cuBLAS speed.
  id: totrans-2859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个原始的GEMM内核可能只达到cuBLAS速度的1%。
- en: A tiled shared-memory GEMM can jump to 30–40%.
  id: totrans-2860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 瓦片共享内存GEMM可以提升到30-40%。
- en: With careful warp scheduling, it can reach 60–70%.
  id: totrans-2861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过仔细的warp调度，它可以达到60-70%。
- en: cuBLAS goes further with hand-tuned assembly and Tensor Cores, pushing 90–95%.
  id: totrans-2862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: cuBLAS通过手动优化的汇编和Tensor Core进一步推进，达到90-95%。
- en: This teaches that optimization is not magic—it’s a sequence of logical improvements,
    each shaving off inefficiencies.
  id: totrans-2863
  prefs: []
  type: TYPE_NORMAL
  zh: 这说明优化不是魔法——它是一系列逻辑改进的序列，每次都减少了低效。
- en: Why It Matters for GPT-2 Training
  id: totrans-2864
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 它对GPT-2训练的重要性
- en: 'Even if you never plan to reimplement matrix multiplication yourself, understanding
    what happens in `dev/cuda/` helps explain why the main training loop in `train_gpt2.cu`
    is so fast. You see why cuBLAS/cuDNN kernels are black boxes of efficiency: because
    writing your own at that level is extremely hard.'
  id: totrans-2865
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你从未计划自己重新实现矩阵乘法，了解`dev/cuda/`中发生的事情也有助于解释为什么`train_gpt2.cu`中的主要训练循环如此之快。你看到为什么cuBLAS/cuDNN内核是效率的黑盒子：因为在这个级别上编写自己的代码极其困难。
- en: But this also means you’re better prepared to write custom kernels when you
    need them. For example, maybe you want to test a new activation function or a
    different attention mechanism. By borrowing patterns from the experimental kernels,
    you can build your own, test them, and compare to baselines.
  id: totrans-2866
  prefs: []
  type: TYPE_NORMAL
  zh: 但这也意味着当你需要时，你将更好地准备编写自定义内核。例如，你可能想测试一个新的激活函数或不同的注意力机制。通过从实验内核中借用模式，你可以构建自己的内核，测试它们，并与基线进行比较。
- en: 'Example: Vector Add Kernel'
  id: totrans-2867
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例：向量加法内核
- en: 'Here’s a simple kernel you might find in this folder:'
  id: totrans-2868
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个你可能在这个文件夹中找到的简单内核：
- en: '[PRE149]'
  id: totrans-2869
  prefs: []
  type: TYPE_PRE
  zh: '[PRE149]'
- en: It’s trivial compared to GPT-2’s attention, but this is where everyone starts.
    From here you scale up to 2D indexing for matrices, then to tiled shared-memory
    patterns.
  id: totrans-2870
  prefs: []
  type: TYPE_NORMAL
  zh: 与GPT-2的注意力机制相比，这很简单，但这是每个人开始的地方。从这里开始，你将扩展到矩阵的2D索引，然后到瓦片共享内存模式。
- en: Try It Yourself
  id: totrans-2871
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 试试看
- en: Run a kernel from `dev/cuda/` that does naive matrix multiplication. Compare
    its runtime to cuBLAS for the same dimensions.
  id: totrans-2872
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行`dev/cuda/`中的一个内核，该内核执行原始矩阵乘法。将其运行时间与相同维度的cuBLAS进行比较。
- en: Modify the naive GEMM to use tiling with shared memory. Measure how performance
    improves.
  id: totrans-2873
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将原始的GEMM修改为使用带有共享内存的瓦片。测量性能如何提升。
- en: Inspect PTX (the intermediate assembly) generated by NVCC for a simple kernel.
    Observe how memory loads are translated.
  id: totrans-2874
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查NVCC为简单内核生成的PTX（中间汇编）代码，观察内存加载是如何被转换的。
- en: Add timing code around kernels to see how much performance scales with different
    block sizes.
  id: totrans-2875
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在内核周围添加计时代码，以查看性能如何随着不同块大小的变化而扩展。
- en: Implement a new custom kernel (e.g., ReLU activation) and compare its speed
    to applying ReLU via cuDNN.
  id: totrans-2876
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现一个新的自定义内核（例如，ReLU激活函数）并将其速度与通过cuDNN应用ReLU的速度进行比较。
- en: The Takeaway
  id: totrans-2877
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸收
- en: The `dev/cuda/` folder is not about production training. It’s about learning
    and experimenting. It starts with the simplest CUDA kernels and builds up to performance-conscious
    designs. This progression mirrors how professional libraries achieve their speed.
    By studying and experimenting here, you gain a deeper appreciation of what it
    takes to make GPUs run at full tilt—and you gain the skills to write your own
    kernels when the libraries don’t provide what you need.
  id: totrans-2878
  prefs: []
  type: TYPE_NORMAL
  zh: '`dev/cuda/`文件夹不是关于生产训练的。它是关于学习和实验的。它从最简单的CUDA内核开始，逐步构建到注重性能的设计。这种进展反映了专业库如何实现其速度。通过在这里学习和实验，你将更深入地理解让GPU全速运行需要什么——并且你将获得编写自己内核的技能，当库没有提供你所需要的东西时。'
- en: Chapter 8\. Multi-GPU and Multi-node training
  id: totrans-2879
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第8章. 多GPU和多节点训练
- en: 71\. Data Parallelism in *llm.c*
  id: totrans-2880
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 71. *llm.c* 中的数据并行
- en: 'When you want to train a large model, a single GPU often isn’t enough. Either
    the model doesn’t fit in memory, or the training takes too long. One of the simplest
    and most widely used ways to scale training across multiple GPUs is data parallelism.
    The idea is conceptually simple: instead of giving all the training data to one
    GPU, you split it into smaller batches, send each GPU a piece, let them process
    it independently, and then combine their results.'
  id: totrans-2881
  prefs: []
  type: TYPE_NORMAL
  zh: 当你想训练大型模型时，单个 GPU 通常不足以满足需求。要么模型无法装入内存，要么训练时间过长。在多个 GPU 上扩展训练的最简单和最广泛使用的方法之一是数据并行。这个想法在概念上是简单的：不是将所有训练数据都给一个
    GPU，而是将其分成更小的批次，给每个 GPU 发送一部分，让它们独立处理，然后合并它们的结果。
- en: The Core Idea
  id: totrans-2882
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 核心思想
- en: 'Imagine you have a batch of 128 sequences and 4 GPUs. In data parallelism:'
  id: totrans-2883
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个包含 128 个序列和 4 个 GPU 的批次。在数据并行中：
- en: GPU 0 sees sequences 0–31
  id: totrans-2884
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU 0 处理序列 0–31
- en: GPU 1 sees sequences 32–63
  id: totrans-2885
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU 1 处理序列 32–63
- en: GPU 2 sees sequences 64–95
  id: totrans-2886
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU 2 处理序列 64–95
- en: GPU 3 sees sequences 96–127
  id: totrans-2887
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU 3 处理序列 96–127
- en: Each GPU runs the forward pass, computes the loss, and calculates gradients
    for its slice. At the end of the step, the gradients are averaged across GPUs,
    ensuring that all models stay synchronized. Every GPU holds a full copy of the
    model parameters, so they are always consistent after gradient averaging.
  id: totrans-2888
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 GPU 运行前向传播，计算损失，并为其切片计算梯度。在步骤结束时，梯度在 GPU 之间平均，确保所有模型保持同步。每个 GPU 都持有模型参数的完整副本，因此在梯度平均后始终一致。
- en: In *llm.c*
  id: totrans-2889
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在 *llm.c*
- en: 'The *llm.c* repository keeps things minimal, so there isn’t a full-fledged
    DeepSpeed or PyTorch DDP implementation. But the same principle applies:'
  id: totrans-2890
  prefs: []
  type: TYPE_NORMAL
  zh: '*llm.c* 仓库保持最小化，因此没有完整的 DeepSpeed 或 PyTorch DDP 实现。但原理是相同的：'
- en: Each GPU gets a copy of the GPT-2 model.
  id: totrans-2891
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个 GPU 都获得 GPT-2 模型的一个副本。
- en: The batch is split across devices.
  id: totrans-2892
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批次在设备之间分割。
- en: After the backward pass, gradients from all GPUs must be synchronized.
  id: totrans-2893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在反向传播之后，所有 GPU 的梯度必须同步。
- en: This synchronization is usually done using NCCL all-reduce (covered in the next
    section), but the design remains data parallel at heart.
  id: totrans-2894
  prefs: []
  type: TYPE_NORMAL
  zh: 这种同步通常使用 NCCL 全量减少（在下节中介绍），但设计本质上仍然是数据并行。
- en: Why Data Parallelism Works
  id: totrans-2895
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么数据并行有效
- en: The forward and backward passes are embarrassingly parallel across different
    data samples. A token in sequence A doesn’t need to know about a token in sequence
    B when computing gradients. As long as all GPUs agree on parameter updates after
    each step, splitting the batch is perfectly valid.
  id: totrans-2896
  prefs: []
  type: TYPE_NORMAL
  zh: 前向和反向传播在不同数据样本之间是令人尴尬的并行。在计算梯度时，序列 A 中的一个标记不需要知道序列 B 中的一个标记。只要所有 GPU 在每一步后都同意参数更新，分割批次就是完全有效的。
- en: Example Walkthrough
  id: totrans-2897
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例说明
- en: Let’s say we’re training GPT-2 on TinyStories with batch size `B = 32` and sequence
    length `T = 64`.
  id: totrans-2898
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们在 TinyStories 上使用批大小 `B = 32` 和序列长度 `T = 64` 训练 GPT-2。
- en: On a single GPU, the forward pass computes embeddings, attention, MLP, and loss
    for all 32 sequences.
  id: totrans-2899
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在单个 GPU 上，前向传播计算所有 32 个序列的嵌入、注意力、MLP 和损失。
- en: With 2 GPUs, we set `B = 16` on each. Each GPU processes 16 sequences in parallel.
  id: totrans-2900
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 2 个 GPU，我们在每个 GPU 上设置 `B = 16`。每个 GPU 并行处理 16 个序列。
- en: After backpropagation, both GPUs hold gradients for their half of the batch.
    Before applying the optimizer, the gradients are averaged so that the weight update
    is equivalent to training with the full batch of 32.
  id: totrans-2901
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反向传播后，两个 GPU 都持有其一半批次的梯度。在应用优化器之前，梯度被平均，以便权重更新等效于使用 32 个完整批次进行训练。
- en: From the model’s perspective, it’s as if nothing changed—it just sees gradients
    from the whole batch.
  id: totrans-2902
  prefs: []
  type: TYPE_NORMAL
  zh: 从模型的角度来看，它就像没有任何变化一样——它只看到整个批次的梯度。
- en: Memory and Speed Benefits
  id: totrans-2903
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 内存和速度优势
- en: 'Memory: Each GPU stores only the activations for its local batch. This reduces
    per-GPU memory use, making it possible to train with larger global batches.'
  id: totrans-2904
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存：每个 GPU 只存储其本地批次的激活。这减少了每个 GPU 的内存使用，使得使用更大的全局批次进行训练成为可能。
- en: 'Speed: Training steps finish faster because multiple GPUs share the work. For
    example, doubling the number of GPUs often cuts training time per step nearly
    in half, though communication overhead prevents perfect scaling.'
  id: totrans-2905
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 速度：由于多个 GPU 分享工作，训练步骤完成得更快。例如，将 GPU 数量加倍通常可以将每步训练时间几乎减半，尽管通信开销阻止了完美的扩展。
- en: Limitations
  id: totrans-2906
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 局限性
- en: Communication Overhead Synchronizing gradients across GPUs can become expensive,
    especially with large models or when running across multiple nodes.
  id: totrans-2907
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通信开销跨 GPU 同步梯度可能会变得昂贵，特别是对于大型模型或在多个节点上运行时。
- en: I/O Bottlenecks Feeding data to multiple GPUs fast enough requires efficient
    dataloaders and prefetching.
  id: totrans-2908
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: I/O瓶颈将数据快速喂给多个GPU需要高效的dataloader和预取。
- en: Optimizer State Replication With AdamW, each GPU also needs to store optimizer
    states (m and v). This means memory scales with the number of GPUs instead of
    shrinking.
  id: totrans-2909
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用AdamW优化器，每个GPU还需要存储优化器状态（m和v）。这意味着内存随着GPU数量的增加而扩展，而不是缩小。
- en: Why It Matters
  id: totrans-2910
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: Data parallelism is the workhorse of deep learning scaling. It’s conceptually
    easy to understand, straightforward to implement, and works well even for large
    models. In practice, nearly all large-scale GPT training begins with data parallelism,
    often enhanced by techniques like gradient accumulation or mixed precision.
  id: totrans-2911
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并行性是深度学习扩展的功臣。它在概念上容易理解，实现简单，即使对于大型模型也表现良好。在实践中，几乎所有大规模GPT训练都是从数据并行性开始的，通常通过梯度累积或混合精度等技术进行增强。
- en: Try It Yourself
  id: totrans-2912
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 试试你自己
- en: Train GPT-2 in *llm.c* on a single GPU, then split the batch across two GPUs
    using `CUDA_VISIBLE_DEVICES`. Compare throughput and loss curves.
  id: totrans-2913
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在*llm.c*上使用单个GPU训练GPT-2，然后使用`CUDA_VISIBLE_DEVICES`将批处理分割到两个GPU上。比较吞吐量和损失曲线。
- en: Experiment with increasing global batch size while keeping per-GPU batch size
    fixed. Notice how validation loss behaves.
  id: totrans-2914
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在保持每个GPU批大小不变的同时，尝试增加全局批大小。注意验证损失如何变化。
- en: Simulate gradient averaging by writing a simple script that averages arrays
    from two processes. Connect this idea back to how NCCL all-reduce works.
  id: totrans-2915
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过编写一个简单的脚本来平均两个进程的数组来模拟梯度平均。将这个想法与NCCL all-reduce的工作方式联系起来。
- en: Measure the difference in memory usage per GPU when training with 1 vs 2 GPUs.
  id: totrans-2916
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测量使用1个和2个GPU训练时每个GPU内存使用量的差异。
- en: Run a small experiment with different numbers of GPUs (1, 2, 4) and plot how
    training time per step changes.
  id: totrans-2917
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用不同数量的GPU（1、2、4）进行小实验，并绘制每步训练时间的变化。
- en: The Takeaway
  id: totrans-2918
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸取的经验
- en: Data parallelism splits the workload across GPUs by dividing the batch. Each
    GPU trains a full model replica on part of the data, then synchronizes gradients
    so that updates are consistent. It’s simple but powerful, forming the foundation
    of scaling strategies in *llm.c* and in most deep learning frameworks. Without
    it, training GPT-2 and larger models on modern datasets would be impractical.
  id: totrans-2919
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并行性通过分割批处理将工作负载分配到GPU上。每个GPU在数据的一部分上训练完整的模型副本，然后同步梯度，以确保更新一致。这很简单但很强大，是*llm.c*和大多数深度学习框架中扩展策略的基础。没有它，在现代数据集上训练GPT-2和更大的模型将是不切实际的。
- en: 72\. MPI Process Model and GPU Affinity
  id: totrans-2920
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 72. MPI进程模型和GPU亲和性
- en: 'When you scale training beyond a single GPU, you need a way to manage multiple
    processes and devices. In the *llm.c* codebase, the minimalist approach relies
    on MPI (Message Passing Interface), a library that has been around for decades
    in high-performance computing. MPI provides a simple abstraction: you launch multiple
    processes, each assigned a rank (an ID number), and they can communicate with
    each other by sending and receiving messages.'
  id: totrans-2921
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将训练扩展到单个GPU之外时，你需要一种方式来管理多个进程和设备。在*llm.c*代码库中，最小化方法依赖于MPI（消息传递接口），这是一个在高性能计算中存在了数十年的库。MPI提供了一个简单的抽象：你启动多个进程，每个进程分配一个排名（一个ID号），并且它们可以通过发送和接收消息相互通信。
- en: In distributed deep learning, MPI typically works alongside NCCL (NVIDIA Collective
    Communications Library). MPI handles process management—spawning workers, assigning
    GPUs, setting up environment variables—while NCCL handles the actual gradient
    synchronization.
  id: totrans-2922
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式深度学习中，MPI通常与NCCL（NVIDIA集体通信库）一起工作。MPI处理进程管理——启动工作进程、分配GPU、设置环境变量——而NCCL处理实际的梯度同步。
- en: MPI Processes and Ranks
  id: totrans-2923
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: MPI进程和排名
- en: 'Suppose you want to train on 4 GPUs. MPI will start 4 processes. Each process:'
  id: totrans-2924
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想在4个GPU上训练。MPI将启动4个进程。每个进程：
- en: Loads the same GPT-2 model code.
  id: totrans-2925
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载相同的GPT-2模型代码。
- en: Initializes CUDA on one GPU.
  id: totrans-2926
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一个GPU上初始化CUDA。
- en: Reads a shard of the training data or the same dataset, depending on setup.
  id: totrans-2927
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据设置读取训练数据的一部分或相同的数据集。
- en: 'Each process gets a rank:'
  id: totrans-2928
  prefs: []
  type: TYPE_NORMAL
  zh: 每个进程都会得到一个排名：
- en: Rank 0 → GPU 0
  id: totrans-2929
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排名0 → GPU 0
- en: Rank 1 → GPU 1
  id: totrans-2930
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排名1 → GPU 1
- en: Rank 2 → GPU 2
  id: totrans-2931
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排名2 → GPU 2
- en: Rank 3 → GPU 3
  id: totrans-2932
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排名3 → GPU 3
- en: Ranks are important because they determine roles. For example, rank 0 often
    acts as the “master,” printing logs or handling checkpoints, while the others
    focus purely on computation.
  id: totrans-2933
  prefs: []
  type: TYPE_NORMAL
  zh: 排名很重要，因为它们决定了角色。例如，排名0通常充当“主进程”，打印日志或处理检查点，而其他进程则专注于计算。
- en: GPU Affinity
  id: totrans-2934
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: GPU亲和性
- en: If you don’t explicitly map processes to GPUs, they can all try to use the same
    device. That leads to oversubscription—multiple processes fighting for one GPU
    while the others sit idle. To prevent this, you set GPU affinity.
  id: totrans-2935
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有明确地将进程映射到 GPU，它们都可能尝试使用相同的设备。这会导致过度订阅——多个进程争夺一个 GPU，而其他 GPU 则闲置。为了防止这种情况，你需要设置
    GPU 亲和性。
- en: 'The environment variable `CUDA_VISIBLE_DEVICES` is the simplest way to do this.
    For example:'
  id: totrans-2936
  prefs: []
  type: TYPE_NORMAL
  zh: 环境变量 `CUDA_VISIBLE_DEVICES` 是实现这一点的最简单方法。例如：
- en: '[PRE150]'
  id: totrans-2937
  prefs: []
  type: TYPE_PRE
  zh: '[PRE150]'
- en: MPI automatically assigns process 0 to GPU 0, process 1 to GPU 1, and so on.
    Inside the code, you can confirm this by calling `cudaSetDevice(rank)`.
  id: totrans-2938
  prefs: []
  type: TYPE_NORMAL
  zh: MPI 自动将进程 0 分配给 GPU 0，进程 1 分配给 GPU 1，依此类推。在代码内部，你可以通过调用 `cudaSetDevice(rank)`
    来确认这一点。
- en: On multi-node clusters, GPU affinity also needs to consider network topology.
    You want each process close to its GPU and ideally aligned with the node’s network
    card for faster NCCL communication.
  id: totrans-2939
  prefs: []
  type: TYPE_NORMAL
  zh: 在多节点集群上，GPU 亲和性还需要考虑网络拓扑。你希望每个进程靠近其 GPU，并且理想情况下与节点的网络卡对齐，以便更快地进行 NCCL 通信。
- en: Synchronization and Communication
  id: totrans-2940
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 同步和通信。
- en: After each forward and backward pass, each MPI process has its local gradients.
    These must be averaged across processes to keep the model weights consistent.
    MPI itself offers collective operations like `MPI_Allreduce`, but in practice,
    *llm.c* uses NCCL for GPU-to-GPU communication because it is faster and topology-aware.
    MPI sets up the group, NCCL does the heavy lifting.
  id: totrans-2941
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次前向和反向传递之后，每个 MPI 进程都有其局部梯度。这些梯度必须在进程之间平均，以保持模型权重的连续性。MPI 本身提供了集体操作，如 `MPI_Allreduce`，但在实践中，*llm.c*
    使用 NCCL 进行 GPU 到 GPU 的通信，因为它更快且具有拓扑感知性。MPI 设置组，NCCL 执行繁重的工作。
- en: Example Workflow
  id: totrans-2942
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例工作流程。
- en: 'Launch: `mpirun -np 4 ./train_gpt2` starts 4 processes.'
  id: totrans-2943
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动：`mpirun -np 4 ./train_gpt2` 启动 4 个进程。
- en: 'Initialization: Each process determines its rank and sets its GPU with `cudaSetDevice(rank)`.'
  id: totrans-2944
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化：每个进程确定其排名并使用 `cudaSetDevice(rank)` 设置其 GPU。
- en: 'Training loop:'
  id: totrans-2945
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练循环：
- en: Forward pass on each process’s GPU.
  id: totrans-2946
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个进程的 GPU 上进行前向传递。
- en: Backward pass to compute gradients.
  id: totrans-2947
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反向传递以计算梯度。
- en: Gradients averaged with NCCL All-Reduce.
  id: totrans-2948
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 NCCL All-Reduce 平均梯度。
- en: 'Update: Every process updates its copy of the weights.'
  id: totrans-2949
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新：每个进程都会更新其权重的副本。
- en: 'Sync: At the next step, all model replicas are identical.'
  id: totrans-2950
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同步：在下一步中，所有模型副本都是相同的。
- en: Debugging GPU Affinity Issues
  id: totrans-2951
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 调试 GPU 亲和性问题。
- en: 'If you accidentally misconfigure GPU affinity, symptoms include:'
  id: totrans-2952
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你意外地错误配置了 GPU 亲和性，症状包括：
- en: Two processes trying to use the same GPU → out-of-memory errors.
  id: totrans-2953
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个进程尝试使用相同的 GPU → 内存不足错误。
- en: GPUs left idle because no process is assigned.
  id: totrans-2954
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于没有进程分配，空闲的 GPU。
- en: Slowdowns because processes are spread inefficiently across sockets or PCIe
    lanes.
  id: totrans-2955
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于进程在套接字或 PCIe 通道上分布不均导致的减速。
- en: 'A quick way to debug is to print the rank and the GPU ID at startup:'
  id: totrans-2956
  prefs: []
  type: TYPE_NORMAL
  zh: 一种快速调试的方法是在启动时打印排名和 GPU ID：
- en: '[PRE151]'
  id: totrans-2957
  prefs: []
  type: TYPE_PRE
  zh: '[PRE151]'
- en: Why It Matters
  id: totrans-2958
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要。
- en: MPI and GPU affinity might feel like low-level plumbing, but they are critical
    for scaling. If you don’t get this right, training may run at a fraction of expected
    speed or crash outright. For small setups (2–4 GPUs), it may feel like overkill,
    but for larger clusters with 8, 16, or 64 GPUs, careful mapping is the difference
    between success and wasted compute time.
  id: totrans-2959
  prefs: []
  type: TYPE_NORMAL
  zh: MPI 和 GPU 亲和性可能感觉像是低级管道，但对于扩展来说至关重要。如果你没有正确设置，训练可能只能达到预期速度的一小部分，或者直接崩溃。对于小型设置（2-4
    个 GPU），这可能感觉像是过度配置，但对于具有 8、16 或 64 个 GPU 的大型集群，仔细映射是成功与浪费计算时间的区别。
- en: Try It Yourself
  id: totrans-2960
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 尝试自己操作。
- en: Train GPT-2 with `mpirun -np 2` and verify each process prints a different GPU
    ID.
  id: totrans-2961
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `mpirun -np 2` 训练 GPT-2 并验证每个进程打印不同的 GPU ID。
- en: Intentionally misconfigure `CUDA_VISIBLE_DEVICES` so both processes map to GPU
    0, then observe the OOM error.
  id: totrans-2962
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 故意错误配置 `CUDA_VISIBLE_DEVICES`，使两个进程都映射到 GPU 0，然后观察 OOM 错误。
- en: On a multi-GPU machine, experiment with running processes pinned to different
    GPUs. Measure training throughput.
  id: totrans-2963
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在多 GPU 机器上，尝试在不同的 GPU 上运行固定进程。测量训练吞吐量。
- en: Use `nvidia-smi topo -m` to view the PCIe topology of your GPUs. Try to align
    MPI ranks with nearby GPUs for better performance.
  id: totrans-2964
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `nvidia-smi topo -m` 查看你的 GPU 的 PCIe 拓扑。尝试将 MPI 排名与附近的 GPU 对齐以获得更好的性能。
- en: Print the time spent in all-reduce with different mappings to see how GPU affinity
    affects communication overhead.
  id: totrans-2965
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印不同映射的全局减少操作花费的时间，以查看 GPU 亲和性如何影响通信开销。
- en: The Takeaway
  id: totrans-2966
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸取的经验。
- en: MPI is the backbone for managing multiple processes in distributed training,
    and GPU affinity ensures each process has exclusive access to the right device.
    Together, they lay the groundwork for efficient multi-GPU training in *llm.c*.
    Get these details right, and scaling is smooth; get them wrong, and you run into
    memory crashes, idle GPUs, or bottlenecked communication.
  id: totrans-2967
  prefs: []
  type: TYPE_NORMAL
  zh: MPI是分布式训练中管理多个进程的骨干，GPU亲和力确保每个进程都有权独占访问正确的设备。它们共同为*llm.c*中的高效多GPU训练奠定基础。正确处理这些细节，扩展就会顺利；处理不当，就会遇到内存崩溃、空闲GPU或通信瓶颈。
- en: 73\. NCCL All-Reduce for Gradient Sync
  id: totrans-2968
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 73. NCCL All-Reduce用于梯度同步
- en: Once each GPU finishes its forward and backward pass, it has a set of gradients
    that reflect only its portion of the training data. To keep the model parameters
    consistent across all GPUs, these gradients must be synchronized. The standard
    way to do this in modern deep learning systems is with All-Reduce, and NVIDIA’s
    NCCL (NVIDIA Collective Communications Library, pronounced “Nickel”) provides
    the optimized implementation.
  id: totrans-2969
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦每个GPU完成其正向和反向传播，它就有一组仅反映其训练数据部分的梯度。为了保持所有GPU上的模型参数一致，这些梯度必须同步。在现代深度学习系统中，这种同步的标准方式是使用All-Reduce，NVIDIA的NCCL（NVIDIA集体通信库，发音为“Nickel”）提供了优化的实现。
- en: What All-Reduce Does
  id: totrans-2970
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: All-Reduce的作用
- en: All-Reduce is a collective communication operation. Every process (GPU) starts
    with its own local buffer of values—here, the gradients—and the operation combines
    them (using a reduction, usually summation) and distributes the result back to
    all processes.
  id: totrans-2971
  prefs: []
  type: TYPE_NORMAL
  zh: All-Reduce是一种集体通信操作。每个进程（GPU）从自己的本地缓冲区开始，这里是指梯度，操作将它们（通常使用归约，通常是求和）组合起来，并将结果分发给所有进程。
- en: 'Mathematically, if GPU 0 has `g0`, GPU 1 has `g1`, GPU 2 has `g2`, and GPU
    3 has `g3`, then after All-Reduce, each GPU has the same result:'
  id: totrans-2972
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，如果GPU 0有`g0`，GPU 1有`g1`，GPU 2有`g2`，GPU 3有`g3`，那么在All-Reduce之后，每个GPU都有相同的结果：
- en: '[PRE152]'
  id: totrans-2973
  prefs: []
  type: TYPE_PRE
  zh: '[PRE152]'
- en: The division by 4 is optional—it depends whether you want a sum or an average—but
    averaging is common for gradient updates.
  id: totrans-2974
  prefs: []
  type: TYPE_NORMAL
  zh: 除以4是可选的——这取决于你是否想要总和还是平均值——但平均对于梯度更新来说是常见的。
- en: This ensures that every GPU applies the same weight update and stays synchronized
    with the others.
  id: totrans-2975
  prefs: []
  type: TYPE_NORMAL
  zh: 这确保了每个GPU都应用相同的权重更新，并与其他GPU保持同步。
- en: Why NCCL?
  id: totrans-2976
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么选择NCCL？
- en: 'While MPI provides an All-Reduce primitive, NCCL is specifically optimized
    for GPUs. It knows about PCIe, NVLink, NVSwitch, and Infiniband topologies and
    arranges communication to maximize bandwidth and minimize latency. Some of its
    key strategies include:'
  id: totrans-2977
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然MPI提供了All-Reduce原语，但NCCL专门针对GPU进行了优化。它了解PCIe、NVLink、NVSwitch和Infiniband拓扑，并安排通信以最大化带宽和最小化延迟。其一些关键策略包括：
- en: 'Ring All-Reduce: GPUs are arranged in a ring. Each GPU sends its data to the
    next while receiving from the previous, accumulating partial sums as the data
    flows. This scales well with many GPUs.'
  id: totrans-2978
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环形All-Reduce：GPU按环形排列。每个GPU将其数据发送到下一个GPU，同时从上一个GPU接收数据，随着数据流动累积部分总和。这很好地扩展到许多GPU。
- en: 'Tree All-Reduce: Organizes communication as a tree, reducing depth (latency)
    at the cost of bandwidth.'
  id: totrans-2979
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 树形All-Reduce：将通信组织成树形，以带宽为代价减少深度（延迟）。
- en: 'Hybrid schemes: NCCL dynamically chooses strategies depending on GPU count
    and topology.'
  id: totrans-2980
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混合方案：NCCL根据GPU数量和拓扑动态选择策略。
- en: By exploiting topology awareness, NCCL can saturate available communication
    channels.
  id: totrans-2981
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用拓扑感知，NCCL可以饱和可用的通信通道。
- en: Example in *llm.c* Training
  id: totrans-2982
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*llm.c* 训练中的示例'
- en: 'In the CPU-only training loop, gradients are updated directly without communication.
    In the multi-GPU CUDA path, after backpropagation (`gpt2_backward`), each GPU
    has its local gradients in memory. At this point:'
  id: totrans-2983
  prefs: []
  type: TYPE_NORMAL
  zh: 在仅CPU的训练循环中，梯度直接更新而不进行通信。在多GPU CUDA路径中，在反向传播（`gpt2_backward`）之后，每个GPU在内存中都有自己的本地梯度。在这个时候：
- en: '[PRE153]'
  id: totrans-2984
  prefs: []
  type: TYPE_PRE
  zh: '[PRE153]'
- en: After this call, `model.grads_memory` on every GPU contains the summed gradients
    across all GPUs. Dividing by the number of GPUs turns it into the average.
  id: totrans-2985
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个调用之后，每个GPU上的`model.grads_memory`包含所有GPU的汇总梯度。除以GPU数量将其转换为平均值。
- en: Why Gradient Sync Matters
  id: totrans-2986
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么梯度同步很重要
- en: Without gradient synchronization, each GPU would drift apart, updating weights
    independently. This would be equivalent to training multiple smaller models rather
    than one unified model. Synchronization makes sure all replicas behave like a
    single large-batch training job.
  id: totrans-2987
  prefs: []
  type: TYPE_NORMAL
  zh: 没有梯度同步，每个GPU都会偏离，独立更新权重。这将相当于训练多个较小的模型，而不是一个统一的模型。同步确保所有副本的行为像一个单一的大批量训练作业。
- en: Memory and Performance Considerations
  id: totrans-2988
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 内存和性能考虑
- en: 'Bandwidth-bound: Gradient synchronization often dominates runtime as model
    size grows. For GPT-2 774M, gradients alone can be several GB per step.'
  id: totrans-2989
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 带宽限制：随着模型大小的增长，梯度同步通常占主导地位。对于 GPT-2 774M，每一步的梯度本身就可以达到几个 GB。
- en: 'Overlapping Communication with Compute: Advanced systems overlap gradient exchange
    with backward computation. While later layers are computing gradients, earlier
    layers are already being synchronized.'
  id: totrans-2990
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通信与计算的叠加：高级系统将梯度交换与反向计算叠加。当较晚的层正在计算梯度时，较早的层已经正在同步。
- en: 'Precision: Gradients can be synchronized in FP16/BF16 to cut communication
    bandwidth in half. This is called gradient compression.'
  id: totrans-2991
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 精度：梯度可以以 FP16/BF16 的精度同步，以将通信带宽减半。这被称为梯度压缩。
- en: Analogy
  id: totrans-2992
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 类比
- en: Think of four chefs cooking the same dish in separate kitchens. Each chef tastes
    their own version and suggests adjustments (gradients). If they don’t talk, their
    recipes diverge. With All-Reduce, the chefs share notes, average their adjustments,
    and apply the same changes—so all four kitchens end up cooking the same dish.
  id: totrans-2993
  prefs: []
  type: TYPE_NORMAL
  zh: 想象四个厨师在各自的厨房里烹饪相同的菜肴。每个厨师品尝自己的版本并提出调整（梯度）。如果他们不交流，他们的食谱就会发散。有了 All-Reduce，厨师们分享笔记，平均他们的调整，并应用相同的更改——所以四个厨房最终都烹饪了相同的菜肴。
- en: Try It Yourself
  id: totrans-2994
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 尝试自己动手做
- en: Run training on 2 GPUs with and without gradient synchronization (by commenting
    out All-Reduce). Watch how quickly the models diverge in loss.
  id: totrans-2995
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 2 个 GPU 上运行训练，有和无梯度同步（通过注释掉 All-Reduce）。观察模型在损失上的快速发散。
- en: Use NCCL’s `NCCL_DEBUG=INFO` environment variable to print communication patterns.
    Observe the chosen ring/tree strategies.
  id: totrans-2996
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 NCCL 的 `NCCL_DEBUG=INFO` 环境变量来打印通信模式。观察选择的环形/树形策略。
- en: Experiment with FP32 vs FP16 gradient synchronization and measure bandwidth
    savings.
  id: totrans-2997
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试 FP32 与 FP16 梯度同步，并测量带宽节省。
- en: Profile training with `nsys` or `nvprof` to see how much time is spent in All-Reduce.
  id: totrans-2998
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `nsys` 或 `nvprof` 分析训练，以查看在 All-Reduce 中花费了多少时间。
- en: Scale from 2 GPUs to 4 or 8 and measure how synchronization overhead grows.
  id: totrans-2999
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 2 个 GPU 扩展到 4 或 8 个 GPU，并测量同步开销的增长。
- en: The Takeaway
  id: totrans-3000
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸收要点
- en: NCCL All-Reduce is the backbone of multi-GPU training in *llm.c*. It ensures
    that gradients computed on separate GPUs are combined into a single, consistent
    update. By leveraging topology-aware algorithms like ring and tree reductions,
    NCCL keeps synchronization efficient even as models and GPU counts scale up. Without
    it, distributed training would produce inconsistent, drifting models rather than
    a unified one.
  id: totrans-3001
  prefs: []
  type: TYPE_NORMAL
  zh: NCCL All-Reduce 是 *llm.c* 中多 GPU 训练的骨干。它确保在单独的 GPU 上计算的梯度被组合成一个单一、一致的更新。通过利用拓扑感知算法，如环形和树形减少，NCCL
    即使在模型和 GPU 数量增加的情况下也能保持同步效率。没有它，分布式训练将产生不一致、漂移的模型，而不是统一的模型。
- en: 74\. Building and Running Multi-GPU Trainers
  id: totrans-3002
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 74. 构建和运行多 GPU 训练器
- en: Getting multiple GPUs to cooperate isn’t automatic—you need to set up the environment,
    initialize communication, and make sure each process knows which GPU to use. In
    *llm.c*, the design is intentionally minimalist, but it still has to integrate
    with MPI (Message Passing Interface) and NCCL to allow training across several
    GPUs.
  id: totrans-3003
  prefs: []
  type: TYPE_NORMAL
  zh: 让多个 GPU 协同工作不是自动的——你需要设置环境，初始化通信，并确保每个进程都知道要使用哪个 GPU。在 *llm.c* 中，设计故意简约，但它仍然需要与
    MPI（消息传递接口）和 NCCL 集成，以允许跨多个 GPU 进行训练。
- en: 'Step 1: MPI Launch'
  id: totrans-3004
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第 1 步：MPI 启动
- en: 'Multi-GPU training starts with MPI. You don’t run the program once; you launch
    it with `mpirun` or `mpiexec`, which spawns one process per GPU. For example:'
  id: totrans-3005
  prefs: []
  type: TYPE_NORMAL
  zh: 多 GPU 训练从 MPI 开始。你不需要一次性运行程序；你需要使用 `mpirun` 或 `mpiexec` 来启动它，这将为每个 GPU 启动一个进程。例如：
- en: '[PRE154]'
  id: totrans-3006
  prefs: []
  type: TYPE_PRE
  zh: '[PRE154]'
- en: Here, `-np 4` starts four processes. Each process will attach itself to one
    GPU.
  id: totrans-3007
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`-np 4` 启动了四个进程。每个进程将连接到一个 GPU。
- en: 'MPI provides:'
  id: totrans-3008
  prefs: []
  type: TYPE_NORMAL
  zh: MPI 提供：
- en: 'Rank: a unique ID for each process (0, 1, 2, 3).'
  id: totrans-3009
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排名：每个进程的唯一 ID（0，1，2，3）。
- en: 'World size: the total number of processes (here, 4).'
  id: totrans-3010
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 世界大小：进程总数（此处为 4）。
- en: Each process knows who it is and how many peers it has.
  id: totrans-3011
  prefs: []
  type: TYPE_NORMAL
  zh: 每个进程都知道自己的身份和有多少对等进程。
- en: 'Step 2: GPU Assignment'
  id: totrans-3012
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第 2 步：GPU 分配
- en: 'Once MPI assigns ranks, each process must select a GPU. This is often done
    with:'
  id: totrans-3013
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 MPI 分配了排名，每个进程必须选择一个 GPU。这通常是通过以下方式完成的：
- en: '[PRE155]'
  id: totrans-3014
  prefs: []
  type: TYPE_PRE
  zh: '[PRE155]'
- en: So process 0 gets GPU 0, process 1 gets GPU 1, and so on. Without this step,
    processes might all pile onto the same GPU, leading to chaos.
  id: totrans-3015
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，进程 0 获取 GPU 0，进程 1 获取 GPU 1，依此类推。如果没有这一步，进程可能会都堆在同一个 GPU 上，导致混乱。
- en: 'Step 3: NCCL Communicator'
  id: totrans-3016
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第 3 步：NCCL 通信器
- en: 'Next, the code creates an NCCL communicator. Think of it as a “conference call”
    between all GPUs. NCCL sets up the communication paths (rings, trees) across devices.
    A typical setup looks like:'
  id: totrans-3017
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，代码创建一个 NCCL 通信器。把它想象成所有 GPU 之间的“电话会议”。NCCL 在设备之间设置通信路径（环、树）。典型的设置看起来像：
- en: '[PRE156]'
  id: totrans-3018
  prefs: []
  type: TYPE_PRE
  zh: '[PRE156]'
- en: 'Here:'
  id: totrans-3019
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里：
- en: '`world_size` is the number of GPUs.'
  id: totrans-3020
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`world_size` 是 GPU 的数量。'
- en: '`nccl_id` is a shared identifier obtained via MPI (all processes must use the
    same one).'
  id: totrans-3021
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nccl_id` 是通过 MPI 获得的共享标识符（所有进程都必须使用相同的标识符）。'
- en: '`rank` is the local ID.'
  id: totrans-3022
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rank` 是本地 ID。'
- en: Now the GPUs can talk to each other.
  id: totrans-3023
  prefs: []
  type: TYPE_NORMAL
  zh: 现在 GPU 可以互相通信。
- en: 'Step 4: Training Loop Integration'
  id: totrans-3024
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第 4 步：训练循环集成
- en: 'Once communication is established, the training loop doesn’t look dramatically
    different. Each GPU:'
  id: totrans-3025
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦建立了通信，训练循环看起来并没有太大不同。每个 GPU：
- en: Loads its own batch of data (so the dataset is divided across GPUs).
  id: totrans-3026
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载它自己的数据批次（因此数据集被分配到 GPU 上）。
- en: Runs the forward pass.
  id: totrans-3027
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行前向传递。
- en: Runs the backward pass.
  id: totrans-3028
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行反向传递。
- en: Calls NCCL All-Reduce to sync gradients.
  id: totrans-3029
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用 NCCL All-Reduce 来同步梯度。
- en: Updates parameters.
  id: totrans-3030
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新参数。
- en: The only new ingredient is step 4\. Without it, each GPU would wander off with
    its own gradients.
  id: totrans-3031
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的新成分是第 4 步。没有它，每个 GPU 都会带着自己的梯度四处游荡。
- en: Example Command
  id: totrans-3032
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例命令
- en: 'Suppose you have 2 GPUs on your machine. You can train with:'
  id: totrans-3033
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你的机器上有 2 个 GPU。你可以用以下方式训练：
- en: '[PRE157]'
  id: totrans-3034
  prefs: []
  type: TYPE_PRE
  zh: '[PRE157]'
- en: Each GPU trains on 8 sequences of 128 tokens. Combined, it’s like training with
    batch size 16, but split across GPUs.
  id: totrans-3035
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 GPU 在 128 个标记的 8 个序列上训练。结合起来，就像批量大小为 16 的训练，但分布在 GPU 上。
- en: Common Pitfalls
  id: totrans-3036
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 常见陷阱
- en: 'Forgetting to set device by rank: all processes fight for GPU 0.'
  id: totrans-3037
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 忘记按排名设置设备：所有进程都在争夺 GPU 0。
- en: 'Mismatched NCCL IDs: communicator fails to initialize.'
  id: totrans-3038
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不匹配的 NCCL ID：通信器初始化失败。
- en: 'MPI vs NCCL versions: some builds are picky, and you may need to recompile
    with matching CUDA/NCCL.'
  id: totrans-3039
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MPI 与 NCCL 版本：一些构建很挑剔，你可能需要使用匹配的 CUDA/NCCL 重新编译。
- en: 'Networking issues: on multi-node setups, firewalls or missing InfiniBand drivers
    can block communication.'
  id: totrans-3040
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络问题：在多节点设置中，防火墙或缺少 InfiniBand 驱动器可能会阻止通信。
- en: Why It Matters
  id: totrans-3041
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: Building a multi-GPU trainer is the gateway to scaling. A single GPU may take
    weeks to train a large model, but spreading the work across 4, 8, or 16 GPUs cuts
    the time dramatically. The simplicity of *llm.c* shows that distributed training
    doesn’t require a massive framework—just careful use of MPI and NCCL.
  id: totrans-3042
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个多 GPU 训练器是扩展的门户。单个 GPU 可能需要几周时间来训练一个大模型，但将工作分散到 4、8 或 16 个 GPU 上可以显著减少时间。*llm.c*
    的简单性表明，分布式训练不需要庞大的框架——只需谨慎使用 MPI 和 NCCL。
- en: Try It Yourself
  id: totrans-3043
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 亲自尝试
- en: Launch training with 1 GPU and then 2 GPUs, keeping the global batch size the
    same. Compare training speed.
  id: totrans-3044
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 1 个 GPU 和然后 2 个 GPU 启动训练，保持全局批量大小相同。比较训练速度。
- en: Launch with 2 GPUs but forget All-Reduce. Notice how validation loss behaves
    differently on each GPU.
  id: totrans-3045
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 2 个 GPU 启动，但忘记 All-Reduce。注意验证损失在每个 GPU 上的行为如何不同。
- en: Use `NCCL_DEBUG=INFO` to see how NCCL sets up communication.
  id: totrans-3046
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `NCCL_DEBUG=INFO` 来查看 NCCL 如何设置通信。
- en: Try deliberately mismatching ranks and devices—observe the crash to understand
    why assignment matters.
  id: totrans-3047
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试故意不匹配排名和设备——观察崩溃以了解为什么分配很重要。
- en: Measure GPU utilization with `nvidia-smi` during training to confirm both GPUs
    are working.
  id: totrans-3048
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练期间使用 `nvidia-smi` 测量 GPU 利用率，以确认两个 GPU 都在工作。
- en: The Takeaway
  id: totrans-3049
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸收要点
- en: 'Multi-GPU trainers in *llm.c* are built around three pillars: MPI to manage
    processes, NCCL to synchronize gradients, and CUDA to run the math. Once these
    are in place, the training loop remains familiar, but the computation spreads
    across GPUs seamlessly. This design keeps the code minimal while still unlocking
    significant scaling power.'
  id: totrans-3050
  prefs: []
  type: TYPE_NORMAL
  zh: '*llm.c* 中的多 GPU 训练器建立在三个支柱之上：MPI 用于管理进程，NCCL 用于同步梯度，CUDA 用于运行数学运算。一旦这些就位，训练循环仍然熟悉，但计算无缝地分布在
    GPU 上。这种设计在保持代码最小化的同时，仍然释放了显著的扩展能力。'
- en: 75\. Multi-Node Bootstrapping with MPI
  id: totrans-3051
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 75. 使用 MPI 进行多节点引导
- en: 'So far, running across multiple GPUs on a single machine is relatively straightforward:
    every process talks through shared memory or high-speed interconnects like NVLink.
    Things become more interesting when training has to scale across multiple machines
    (often called “nodes”)—for example, when you want to run on 2 servers, each with
    4 GPUs, to make a total of 8.'
  id: totrans-3052
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在单台机器上跨多个 GPU 运行相对简单：每个进程通过共享内存或高速互连（如 NVLink）进行通信。当训练需要跨多台机器（通常称为“节点”）扩展时，事情变得更有趣——例如，当你想在
    2 台服务器上运行，每台服务器有 4 个 GPU，总共 8 个 GPU 时。
- en: The MPI World
  id: totrans-3053
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: MPI 世界
- en: 'MPI was designed for this. When you run:'
  id: totrans-3054
  prefs: []
  type: TYPE_NORMAL
  zh: MPI 是为此而设计的。当你运行：
- en: '[PRE158]'
  id: totrans-3055
  prefs: []
  type: TYPE_PRE
  zh: '[PRE158]'
- en: '`-np 8` says you want 8 processes.'
  id: totrans-3056
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-np 8` 表示你想要 8 个进程。'
- en: '`-hostfile myhosts` lists the machines (and how many processes to run on each).'
  id: totrans-3057
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-hostfile myhosts`列出机器（以及每个机器上要运行的进程数）。'
- en: MPI then launches processes across nodes and assigns each one a rank. From the
    program’s perspective, it doesn’t matter if two ranks are on the same machine
    or different machines—they all see a global communicator of size 8.
  id: totrans-3058
  prefs: []
  type: TYPE_NORMAL
  zh: MPI然后在节点间启动进程并为每个进程分配一个排名。从程序的角度来看，两个排名是否在同一台机器或不同机器上并不重要——它们都看到一个大小为8的全局通信者。
- en: Setting Up NCCL Across Nodes
  id: totrans-3059
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在节点间设置NCCL
- en: 'NCCL doesn’t know how to find other machines by itself. It relies on MPI to
    exchange a unique NCCL ID. The typical flow is:'
  id: totrans-3060
  prefs: []
  type: TYPE_NORMAL
  zh: NCCL无法自己找到其他机器。它依赖于MPI来交换一个唯一的NCCL ID。典型的流程是：
- en: Rank 0 creates a new NCCL ID.
  id: totrans-3061
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 排名0创建一个新的NCCL ID。
- en: Rank 0 broadcasts the ID to all other ranks using MPI.
  id: totrans-3062
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 排名0使用MPI将ID广播到所有其他排名。
- en: Each process calls `ncclCommInitRank` with the shared ID, total world size,
    and its own rank.
  id: totrans-3063
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个进程都调用`ncclCommInitRank`，使用共享ID、总世界大小和自己的排名。
- en: This ensures all GPUs, even across different machines, join the same “conference
    call.”
  id: totrans-3064
  prefs: []
  type: TYPE_NORMAL
  zh: 这确保了所有GPU，即使在不同机器上，也能加入同一个“电话会议”。
- en: Networking Considerations
  id: totrans-3065
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 网络考虑事项
- en: 'When scaling across nodes, networking becomes critical:'
  id: totrans-3066
  prefs: []
  type: TYPE_NORMAL
  zh: 当跨节点扩展时，网络变得至关重要：
- en: 'Ethernet vs InfiniBand: Standard Ethernet works but can be slow. High-performance
    clusters use InfiniBand for much higher bandwidth and lower latency.'
  id: totrans-3067
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以太网与InfiniBand：标准以太网可以工作但可能较慢。高性能集群使用InfiniBand以获得更高的带宽和更低的延迟。
- en: 'Firewall rules: NCCL needs open ports to connect nodes. Firewalls or strict
    security settings can block communication.'
  id: totrans-3068
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 防火墙规则：NCCL需要开放的端口来连接节点。防火墙或严格的安全设置可能会阻止通信。
- en: 'Environment variables: Variables like `NCCL_SOCKET_IFNAME` (to pick the right
    network interface) often need to be set. For example:'
  id: totrans-3069
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境变量：例如`NCCL_SOCKET_IFNAME`（选择正确的网络接口）等变量通常需要设置。例如：
- en: '[PRE159]'
  id: totrans-3070
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE159]'
- en: Example Hostfile
  id: totrans-3071
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例主机文件
- en: 'A simple `myhosts` file could look like this:'
  id: totrans-3072
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的`myhosts`文件可能看起来像这样：
- en: '[PRE160]'
  id: totrans-3073
  prefs: []
  type: TYPE_PRE
  zh: '[PRE160]'
- en: This says node1 and node2 each have 4 GPUs. MPI will launch 4 processes on each,
    totaling 8.
  id: totrans-3074
  prefs: []
  type: TYPE_NORMAL
  zh: 这表示节点1和节点2各有4个GPU。MPI将在每个节点上启动4个进程，总共8个。
- en: Synchronization Across Nodes
  id: totrans-3075
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 节点间的同步
- en: 'Because communication now spans machines, synchronization overhead becomes
    more visible. Gradient All-Reduce has to move data not only between GPUs in one
    server but also across the network. Efficient scaling depends on:'
  id: totrans-3076
  prefs: []
  type: TYPE_NORMAL
  zh: 因为现在通信跨越了机器，同步开销变得更加明显。梯度全归约不仅要在一台服务器的GPU之间移动数据，还要通过网络。有效的扩展取决于：
- en: Large enough batch sizes (so compute time outweighs communication).
  id: totrans-3077
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 足够大的批量大小（计算时间超过通信时间）。
- en: Overlapping communication with computation (advanced optimization).
  id: totrans-3078
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与计算重叠的通信（高级优化）。
- en: Fast interconnects between machines.
  id: totrans-3079
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器间快速互连。
- en: Why It Matters
  id: totrans-3080
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: Training large models rarely happens on a single machine. Multi-node training
    is how researchers and companies scale models to billions of parameters. By showing
    how to bootstrap MPI and NCCL across nodes, *llm.c* demonstrates the foundation
    of distributed AI training systems, but in a minimal and transparent way.
  id: totrans-3081
  prefs: []
  type: TYPE_NORMAL
  zh: 训练大型模型很少在单台机器上进行。多节点训练是研究人员和公司扩展模型到数十亿参数的方法。通过展示如何在节点间启动MPI和NCCL，*llm.c*展示了分布式AI训练系统的基础，但以最小化和透明的方式。
- en: Try It Yourself
  id: totrans-3082
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 尝试自己操作
- en: Prepare two machines with CUDA and NCCL installed, connected via the same network.
  id: totrans-3083
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备两台安装了CUDA和NCCL的机器，通过相同的网络连接。
- en: Write a hostfile listing both machines, then launch with `mpirun`.
  id: totrans-3084
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个包含两台机器的主机文件，然后使用`mpirun`启动。
- en: Set `NCCL_DEBUG=INFO` to watch how NCCL connects across nodes.
  id: totrans-3085
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置`NCCL_DEBUG=INFO`以观察NCCL如何在节点间连接。
- en: Compare throughput between single-node and two-node runs with the same number
    of GPUs.
  id: totrans-3086
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较具有相同GPU数量的单节点和双节点运行的吞吐量。
- en: Experiment with environment variables like `NCCL_SOCKET_IFNAME` or `NCCL_IB_DISABLE=1`
    to see how network choices affect speed.
  id: totrans-3087
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试环境变量，如`NCCL_SOCKET_IFNAME`或`NCCL_IB_DISABLE=1`，以查看网络选择如何影响速度。
- en: The Takeaway
  id: totrans-3088
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸收要点
- en: Bootstrapping multi-node training is about extending the same principles as
    single-node multi-GPU training, but with networking in the mix. MPI handles process
    management, NCCL sets up communication, and CUDA runs the math. With just a few
    lines of setup, *llm.c* can stretch from one GPU on your laptop to dozens of GPUs
    spread across multiple servers.
  id: totrans-3089
  prefs: []
  type: TYPE_NORMAL
  zh: 启动多节点训练是扩展单节点多GPU训练的相同原则，但加入了网络。MPI处理进程管理，NCCL设置通信，CUDA运行数学运算。只需几行设置，*llm.c*就可以从笔记本电脑上的一个GPU扩展到多个服务器上分布的数十个GPU。
- en: 76\. SLURM and PMIx Caveats
  id: totrans-3090
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 76. SLURM和PMIx注意事项
- en: On many research clusters or supercomputers, you don’t launch jobs manually
    with `mpirun` and a hostfile. Instead, you interact with a job scheduler, most
    commonly SLURM. SLURM takes care of allocating resources, starting processes across
    nodes, and enforcing quotas. While this saves you from manually managing hostfiles,
    it introduces its own set of details that you need to understand.
  id: totrans-3091
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多研究集群或超级计算机上，你不需要手动使用`mpirun`和主机文件来启动作业。相反，你与作业调度器交互，最常见的是SLURM。SLURM负责分配资源、在节点间启动进程以及执行配额。虽然这可以让你免于手动管理主机文件，但它引入了自己的一套需要你理解的细节。
- en: SLURM Basics
  id: totrans-3092
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: SLURM基础知识
- en: 'In SLURM, you typically request GPUs and nodes using a script or command like:'
  id: totrans-3093
  prefs: []
  type: TYPE_NORMAL
  zh: 在SLURM中，你通常使用脚本或命令来请求GPU和节点，例如：
- en: '[PRE161]'
  id: totrans-3094
  prefs: []
  type: TYPE_PRE
  zh: '[PRE161]'
- en: '`-N 2` asks for 2 nodes.'
  id: totrans-3095
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-N 2`请求2个节点。'
- en: '`-G 8` requests 8 GPUs (across those nodes).'
  id: totrans-3096
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-G 8`请求8个GPU（跨这些节点）。'
- en: '`--time=01:00:00` sets a one-hour time limit.'
  id: totrans-3097
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--time=01:00:00`设置了一个一小时的时限。'
- en: 'Once the job starts, SLURM sets environment variables such as:'
  id: totrans-3098
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦作业开始，SLURM会设置环境变量，例如：
- en: '`SLURM_NTASKS`: total number of tasks (processes).'
  id: totrans-3099
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SLURM_NTASKS`：任务总数（进程）。'
- en: '`SLURM_PROCID`: the rank of the current process.'
  id: totrans-3100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SLURM_PROCID`：当前进程的排名。'
- en: '`SLURM_NODEID`: which node this process is running on.'
  id: totrans-3101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SLURM_NODEID`：表示此进程正在哪个节点上运行。'
- en: MPI implementations (OpenMPI, MPICH) and NCCL can use these to bootstrap communication
    automatically.
  id: totrans-3102
  prefs: []
  type: TYPE_NORMAL
  zh: MPI实现（OpenMPI、MPICH）和NCCL可以使用这些来自动启动通信。
- en: PMIx Integration
  id: totrans-3103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: PMIx集成
- en: 'Modern SLURM often works with PMIx (Process Management Interface for Exascale).
    PMIx allows MPI and other runtimes to query process information directly from
    SLURM without relying on older launchers. In practice, this means:'
  id: totrans-3104
  prefs: []
  type: TYPE_NORMAL
  zh: 现代SLURM通常与PMIx（用于Exascale的进程管理接口）一起工作。PMIx允许MPI和其他运行时直接从SLURM查询进程信息，而不依赖于较旧的启动器。在实践中，这意味着：
- en: You might not use `mpirun` at all. Instead, SLURM provides `srun`.
  id: totrans-3105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可能根本不会使用`mpirun`。相反，SLURM提供了`srun`。
- en: 'For example:'
  id: totrans-3106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如：
- en: '[PRE162]'
  id: totrans-3107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE162]'
- en: Here `-n 8` launches 8 tasks across your allocated nodes. SLURM/PMIx handles
    the rank assignments.
  id: totrans-3108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里`-n 8`会在你的分配节点上启动8个任务。SLURM/PMIx处理排名分配。
- en: Common Pitfalls
  id: totrans-3109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 常见错误
- en: MPI version mismatch If your cluster has multiple MPI libraries installed, you
    may accidentally compile with one and run with another. Always confirm that the
    `mpicc` and `mpirun` you’re using match the library your job is linking against.
  id: totrans-3110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: MPI版本不匹配 如果你的集群安装了多个MPI库，你可能会不小心用其中一个编译，用另一个运行。始终确认你使用的`mpicc`和`mpirun`与你的作业链接的库相匹配。
- en: Environment variable propagation NCCL relies on environment variables like `NCCL_DEBUG`,
    `NCCL_SOCKET_IFNAME`, and `NCCL_IB_HCA`. Sometimes SLURM doesn’t forward these
    to all nodes unless you configure it to. Using `--export=ALL` or adding exports
    in your job script can fix this.
  id: totrans-3111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 环境变量传播 NCCL依赖于环境变量如`NCCL_DEBUG`、`NCCL_SOCKET_IFNAME`和`NCCL_IB_HCA`。有时SLURM不会将这些变量转发到所有节点，除非你进行配置。使用`--export=ALL`或在作业脚本中添加导出可以解决这个问题。
- en: GPU visibility SLURM manages GPU allocation via `CUDA_VISIBLE_DEVICES`. Each
    process only “sees” the GPUs it was assigned. If your code assumes a global view
    of all GPUs, it may break. In *llm.c*, the mapping between rank and GPU ID needs
    to respect this.
  id: totrans-3112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GPU可见性 SLURM通过`CUDA_VISIBLE_DEVICES`管理GPU分配。每个进程只能“看到”分配给它的GPU。如果你的代码假设了一个全局的GPU视图，它可能会出错。在*llm.c*中，排名和GPU
    ID之间的映射需要遵守这一点。
- en: Network fabric mismatches On big clusters, you may have multiple network fabrics
    (Ethernet, InfiniBand). If NCCL picks the wrong one, performance plummets. Explicitly
    setting `NCCL_SOCKET_IFNAME` or `NCCL_IB_DISABLE` can solve this.
  id: totrans-3113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络布线不匹配 在大型集群中，你可能有多达多个网络布线（以太网、InfiniBand）。如果NCCL选择了错误的一个，性能会急剧下降。显式设置`NCCL_SOCKET_IFNAME`或`NCCL_IB_DISABLE`可以解决这个问题。
- en: Why It Matters
  id: totrans-3114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: Learning to run across nodes with SLURM is essential if you want to scale training
    beyond a single server. While local `mpirun` commands work for development, almost
    all serious training runs—whether academic or industrial—happen under SLURM or
    a similar workload manager. Understanding the quirks of SLURM and PMIx ensures
    that your code scales smoothly without mysterious hangs or slowdowns.
  id: totrans-3115
  prefs: []
  type: TYPE_NORMAL
  zh: 学习使用SLURM跨节点运行对于你想要将训练扩展到单个服务器之外至关重要。虽然本地的`mpirun`命令适用于开发，但几乎所有严肃的训练运行——无论是学术还是工业——都是在SLURM或类似的作业管理器下进行的。了解SLURM和PMIx的怪癖可以确保你的代码平稳扩展，不会出现神秘的挂起或减速。
- en: Try It Yourself
  id: totrans-3116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 尝试自己操作
- en: Write a small SLURM job script that requests 2 GPUs for 10 minutes and runs
    a dummy *llm.c* training loop.
  id: totrans-3117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个小的SLURM作业脚本，请求2个GPU，运行10分钟，并运行一个模拟的*llm.c*训练循环。
- en: Use `srun` to launch the program and print out the `SLURM_PROCID` and `SLURM_NODEID`
    for each process.
  id: totrans-3118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`srun`启动程序，并打印出每个进程的`SLURM_PROCID`和`SLURM_NODEID`。
- en: Set `NCCL_DEBUG=INFO` in your job script and observe how NCCL initializes communication.
  id: totrans-3119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的作业脚本中设置 `NCCL_DEBUG=INFO` 并观察 NCCL 如何初始化通信。
- en: Experiment with `srun --ntasks-per-node` to control how many processes land
    on each node.
  id: totrans-3120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过实验 `srun --ntasks-per-node` 来控制每个节点上落地的进程数量。
- en: Intentionally misconfigure `CUDA_VISIBLE_DEVICES` to see how it affects rank-to-GPU
    mapping.
  id: totrans-3121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 故意配置错误的 `CUDA_VISIBLE_DEVICES` 来查看它如何影响排名到 GPU 的映射。
- en: The Takeaway
  id: totrans-3122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸取的经验
- en: SLURM and PMIx streamline distributed training on large clusters, but they add
    another layer of complexity. The principles remain the same—MPI ranks, NCCL communicators,
    and CUDA kernels—but the scheduler decides how processes are placed and how environments
    are set up. With a bit of practice, these tools allow *llm.c* to move from simple
    multi-GPU experiments to scalable cluster-wide training runs.
  id: totrans-3123
  prefs: []
  type: TYPE_NORMAL
  zh: SLURM 和 PMIx 在大型集群上简化了分布式训练，但它们增加了另一层复杂性。原则仍然是相同的——MPI 排名、NCCL 通信者和 CUDA 内核，但调度器决定进程的放置方式和环境的设置。经过一些实践，这些工具允许
    *llm.c* 从简单的多 GPU 实验转移到可扩展的集群级训练运行。
- en: 77\. Debugging Multi-GPU Hangs and Stalls
  id: totrans-3124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 77. 调试多 GPU 挂起和停滞
- en: When training on multiple GPUs, one of the most frustrating experiences is a
    job that simply hangs — no errors, no crashes, just frozen processes. In distributed
    deep learning, hangs are almost always related to synchronization mismatches.
    Every GPU worker is supposed to “meet up” at communication points (like gradient
    all-reduce), and if even one process gets lost, the whole group stalls.
  id: totrans-3125
  prefs: []
  type: TYPE_NORMAL
  zh: 在多个 GPU 上训练时，最令人沮丧的体验之一就是作业简单地挂起——没有错误，没有崩溃，只是冻结的进程。在分布式深度学习中，挂起几乎总是与同步不匹配有关。每个
    GPU 工作者都应该在通信点（如梯度全归约）处“会合”，如果任何一个进程丢失，整个组就会停滞。
- en: Common Causes of Hangs
  id: totrans-3126
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 挂起的常见原因
- en: Mismatched Collective Calls If one rank calls `ncclAllReduce` while another
    rank skips it or calls `ncclBroadcast`, the system deadlocks. All GPUs wait forever
    because they’re not speaking the same “language” at that step.
  id: totrans-3127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 集体调用不匹配 如果一个排名调用 `ncclAllReduce`，而另一个排名跳过它或调用 `ncclBroadcast`，系统就会死锁。所有 GPU
    将永远等待，因为在这个步骤中它们没有使用相同的“语言”。
- en: Uneven Batch Sizes If the training data isn’t perfectly divisible across GPUs,
    one process might run out of data earlier than others. The code tries to sync
    gradients, but some ranks never reach that point.
  id: totrans-3128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 批次大小不均 如果训练数据不能完美地分配到 GPU 上，一个进程可能会比其他进程更早耗尽数据。代码试图同步梯度，但某些排名永远不会达到那个点。
- en: CUDA Errors Silently Ignored A kernel launch failure on one GPU can prevent
    it from reaching synchronization. If error checks are missing, you won’t see the
    failure until the program is stuck.
  id: totrans-3129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 静默忽略 CUDA 错误 一个 GPU 上的内核启动失败可能会阻止它达到同步。如果没有错误检查，你将不会看到失败直到程序挂起。
- en: Networking Issues NCCL depends on reliable network connections. If one node
    has a bad InfiniBand card, firewall rule, or misconfigured interface, communication
    halts.
  id: totrans-3130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络问题 NCCL 依赖于可靠的网络连接。如果一个节点有坏的 InfiniBand 卡、防火墙规则或配置错误的接口，通信就会中断。
- en: Debugging Strategies
  id: totrans-3131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 调试策略
- en: 'Enable NCCL Debugging Set:'
  id: totrans-3132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启用 NCCL 调试 设置：
- en: '[PRE163]'
  id: totrans-3133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE163]'
- en: This produces logs showing when each rank enters and leaves collective operations.
    By comparing ranks, you can see who got stuck.
  id: totrans-3134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这会产生日志，显示每个排名何时进入和离开集体操作。通过比较排名，你可以看到谁卡住了。
- en: 'Check CUDA Errors Always wrap CUDA calls with error checks, or run with:'
  id: totrans-3135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总是检查 CUDA 错误，或者运行：
- en: '[PRE164]'
  id: totrans-3136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE164]'
- en: This detects invalid memory access or kernel failures that might lead to stalls.
  id: totrans-3137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这可以检测无效的内存访问或可能导致停滞的内核失败。
- en: Simplify the Setup Start with 2 GPUs on a single node. If it works, increase
    to 4 GPUs, then expand to multiple nodes. This isolates whether the bug is in
    GPU logic or network communication.
  id: totrans-3138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简化设置 从单个节点上的 2 个 GPU 开始。如果它工作，增加到 4 个 GPU，然后扩展到多个节点。这可以隔离错误是在 GPU 逻辑还是在网络通信中。
- en: Timeouts and Watchdogs NCCL provides environment variables like `NCCL_TIMEOUT`
    that can help detect when a collective is stalled. Although it won’t fix the hang,
    it prevents wasting hours waiting for nothing.
  id: totrans-3139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超时和看门狗 NCCL 提供了如 `NCCL_TIMEOUT` 这样的环境变量，可以帮助检测集体操作何时停滞。虽然它不能修复挂起，但它可以防止浪费数小时等待无果。
- en: Why It Matters
  id: totrans-3140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: In multi-GPU training, hangs are not rare — they are part of the debugging journey.
    Understanding that hangs usually mean “one rank is out of sync” helps you approach
    the problem methodically. By checking logs, validating batch sizes, and carefully
    testing collective calls, you avoid endless frustration and wasted GPU hours.
  id: totrans-3141
  prefs: []
  type: TYPE_NORMAL
  zh: 在多 GPU 训练中，挂起并不罕见——它是调试旅程的一部分。理解挂起通常意味着“一个 rank 失去同步”有助于你系统地解决问题。通过检查日志、验证批量大小和仔细测试集体调用，你可以避免无尽的挫败感和浪费
    GPU 时间。
- en: Try It Yourself
  id: totrans-3142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 尝试自己动手做
- en: Run a 2-GPU training job and intentionally misconfigure the code so only one
    rank calls `gpt2_backward`. Observe how the system hangs.
  id: totrans-3143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行一个 2-GPU 训练作业，故意配置错误代码，使只有一个 rank 调用 `gpt2_backward`。观察系统如何挂起。
- en: Enable `NCCL_DEBUG=INFO` and compare logs between the two ranks.
  id: totrans-3144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用 `NCCL_DEBUG=INFO` 并比较两个 rank 之间的日志。
- en: Modify the dataloader so that one GPU gets fewer batches than the other. Watch
    how training stalls at the first gradient sync.
  id: totrans-3145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改数据加载器，使一个 GPU 获得的批量比另一个 GPU 少。观察训练在第一次梯度同步时如何停滞。
- en: Experiment with `cuda-memcheck` to catch silent CUDA errors in a simple kernel.
  id: totrans-3146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试使用 `cuda-memcheck` 来捕获简单内核中的静默 CUDA 错误。
- en: Practice scaling up from 1 node to 2 nodes to see where hangs are more likely
    to appear.
  id: totrans-3147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 练习从 1 个节点扩展到 2 个节点，以了解挂起更可能出现在哪里。
- en: The Takeaway
  id: totrans-3148
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸取的经验
- en: Hangs in distributed training almost always trace back to mismatched synchronization,
    unbalanced workloads, or hidden errors. By using NCCL’s debug tools, adding error
    checks, and testing systematically, you can turn mysterious freezes into solvable
    problems. Multi-GPU training isn’t just about raw speed — it’s about learning
    to keep many moving parts in lockstep.
  id: totrans-3149
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式训练中的挂起几乎总是可以追溯到不匹配的同步、不平衡的工作负载或隐藏的错误。通过使用 NCCL 的调试工具、添加错误检查和系统性地测试，你可以将神秘的冻结变成可解决的问题。多
    GPU 训练不仅仅是关于原始速度，它是关于学习如何使许多移动部件同步。
- en: '78\. Scaling Stories: GPT-2 124M → 774M → 1.6B'
  id: totrans-3150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 78. 扩展故事：GPT-2 124M → 774M → 1.6B
- en: 'One of the most exciting parts of *llm.c* is that it doesn’t just stop at toy
    models. The same code that trains a small GPT-2 model on Tiny Shakespeare can
    be scaled up to much larger models, like GPT-2 774M and even 1.6B. But scaling
    isn’t just about making the numbers bigger — it changes almost everything about
    how you train: memory requirements, communication costs, optimizer stability,
    and even your workflow.'
  id: totrans-3151
  prefs: []
  type: TYPE_NORMAL
  zh: '*llm.c* 最令人兴奋的部分之一是它不仅仅停留在玩具模型上。在 Tiny Shakespeare 上训练小型 GPT-2 模型的相同代码可以扩展到更大的模型，如
    GPT-2 774M 甚至 1.6B。但扩展不仅仅是让数字变大——它几乎改变了你训练的方方面面：内存需求、通信成本、优化器稳定性，甚至你的工作流程。'
- en: 'Starting Small: GPT-2 124M'
  id: totrans-3152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 从小开始：GPT-2 124M
- en: 'The 124M parameter model is the “hello world” of GPT-2 training. It fits comfortably
    on a single modern GPU, and you can even run a trimmed-down version on CPU. At
    this size:'
  id: totrans-3153
  prefs: []
  type: TYPE_NORMAL
  zh: 124M 参数模型是 GPT-2 训练的“hello world”。它可以在单个现代 GPU 上舒适地运行，你甚至可以在 CPU 上运行一个精简版本。在这个规模下：
- en: Batch sizes can stay small (e.g., 4–8).
  id: totrans-3154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量大小可以保持较小（例如，4-8）。
- en: Memory requirements are modest — a few gigabytes of VRAM.
  id: totrans-3155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存需求适度——几 GB 的 VRAM。
- en: Training speed is relatively fast, so you can iterate quickly.
  id: totrans-3156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练速度相对较快，因此你可以快速迭代。
- en: 'Purpose: sanity checks, debugging kernels, verifying correctness.'
  id: totrans-3157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目的：进行合理性检查、调试内核、验证正确性。
- en: 'Think of 124M as the training wheels stage: you’re learning to balance, not
    yet racing.'
  id: totrans-3158
  prefs: []
  type: TYPE_NORMAL
  zh: 将 124M 视为训练轮阶段：你正在学习保持平衡，而不是还在比赛。
- en: Moving to GPT-2 774M
  id: totrans-3159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 移动到 GPT-2 774M
- en: 'At ~774M parameters, the picture changes:'
  id: totrans-3160
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ~774M 参数时，情况发生了变化：
- en: A single GPU can still *fit* the model, but training speed slows dramatically.
  id: totrans-3161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个 GPU 仍然可以 *容纳* 模型，但训练速度会显著减慢。
- en: Gradient synchronization across multiple GPUs becomes essential to get reasonable
    throughput.
  id: totrans-3162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在多个 GPU 之间同步梯度变得至关重要，以获得合理的吞吐量。
- en: 'Communication costs start to matter: an all-reduce of hundreds of megabytes
    per step stresses both PCIe and network bandwidth.'
  id: totrans-3163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通信成本开始变得重要：每一步数百兆字节的 all-reduce 都会压榨 PCIe 和网络带宽。
- en: 'Stability becomes more sensitive: learning rates and warmup schedules need
    more careful tuning.'
  id: totrans-3164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稳定性变得更加敏感：学习率和预热计划需要更仔细的调整。
- en: Here, training is less about “does the code run?” and more about “does the system
    scale?” This size is often used in academic replications of GPT-2 because it’s
    large enough to be interesting but not impossibly expensive.
  id: totrans-3165
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，训练更多地关乎“代码是否运行”，而不是“系统是否扩展？”这个规模常用于 GPT-2 的学术复制，因为它足够大以引起兴趣，但又不至于过于昂贵。
- en: 'GPT-2 1.6B: Scaling to the Edge'
  id: totrans-3166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: GPT-2 1.6B：扩展到边缘
- en: 'At 1.6B parameters, the model is too large for a single GPU to train efficiently.
    You need:'
  id: totrans-3167
  prefs: []
  type: TYPE_NORMAL
  zh: 在1.6B参数时，模型太大，单个GPU无法高效训练。你需要：
- en: Multi-GPU setups with NCCL all-reduce to share gradient updates.
  id: totrans-3168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用NCCL all-reduce的多GPU设置来共享梯度更新。
- en: Multi-node training on clusters when even 8 GPUs aren’t enough.
  id: totrans-3169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当即使是8个GPU也不够时，在集群上进行多节点训练。
- en: Careful optimizer tuning — without proper settings for AdamW and schedulers,
    the model may diverge.
  id: totrans-3170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仔细的优化器调优——如果没有为AdamW和调度器设置适当的设置，模型可能会发散。
- en: Memory tricks like mixed precision (FP16/BF16) and gradient checkpointing to
    fit activations in memory.
  id: totrans-3171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用混合精度（FP16/BF16）和梯度检查点等内存技巧来适应激活。
- en: Training GPT-2 1.6B is a significant engineering challenge, but it proves that
    *llm.c* is not just a toy project — it’s a minimal yet real implementation that
    can push to billion-parameter scale.
  id: totrans-3172
  prefs: []
  type: TYPE_NORMAL
  zh: 训练GPT-2 1.6B是一个重大的工程挑战，但它证明*llm.c*不仅仅是一个玩具项目——它是一个最小化但真实的实现，可以将规模推到十亿参数级别。
- en: Scaling Lessons
  id: totrans-3173
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 规模经验教训
- en: 'As you climb from 124M to 774M to 1.6B, several lessons emerge:'
  id: totrans-3174
  prefs: []
  type: TYPE_NORMAL
  zh: 当你从124M爬升到774M到1.6B时，会出现几个教训：
- en: Debug small, scale big — always test on 124M before attempting larger models.
  id: totrans-3175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 小规模调试，大规模扩展——在尝试更大模型之前，始终在124M上进行测试。
- en: Communication dominates — at 774M and beyond, time spent moving gradients often
    exceeds compute time.
  id: totrans-3176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通信占主导地位——在774M及以上，移动梯度所花费的时间通常超过计算时间。
- en: Hyperparameters evolve — a learning rate that works for 124M may explode the
    loss at 1.6B.
  id: totrans-3177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 超参数会演变——适用于124M的学习率在1.6B时可能会使损失爆炸。
- en: Infrastructure matters — GPUs, interconnects, and schedulers become as important
    as code.
  id: totrans-3178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基础设施很重要——GPU、互连和调度器与代码一样重要。
- en: Why It Matters
  id: totrans-3179
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: Scaling stories show that deep learning isn’t just about writing a clever algorithm
    — it’s about making that algorithm work under increasingly heavy loads. Each jump
    in size uncovers new bottlenecks and new engineering challenges. By following
    this path, you gain intuition for how large models are really trained in practice.
  id: totrans-3180
  prefs: []
  type: TYPE_NORMAL
  zh: 规模故事表明，深度学习不仅仅是编写一个巧妙的算法——它是关于让这个算法在越来越重的负载下工作。每次规模的跳跃都会揭示新的瓶颈和新的工程挑战。通过遵循这条路径，你可以获得对大型模型在实际中是如何训练的直觉。
- en: Try It Yourself
  id: totrans-3181
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 尝试自己操作
- en: Train GPT-2 124M on Tiny Shakespeare until the loss stabilizes. Record how long
    each step takes.
  id: totrans-3182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Tiny Shakespeare上训练GPT-2 124M，直到损失稳定。记录每一步需要多长时间。
- en: Attempt the same experiment on OpenWebText with 124M — watch how dataset size
    now becomes the limiting factor.
  id: totrans-3183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在OpenWebText上用124M尝试相同的实验——看看现在数据集大小是如何成为限制因素的。
- en: Scale up to GPT-2 355M or 774M if you have access to multiple GPUs. Measure
    how much time is spent in NCCL all-reduce compared to compute.
  id: totrans-3184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你有多个GPU的访问权限，将GPT-2 355M或774M扩展到GPT-2，并测量在NCCL all-reduce与计算相比花费的时间。
- en: If you have cluster access, try running 774M with `srun` or `mpirun` across
    nodes.
  id: totrans-3185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你有权访问集群，尝试使用`srun`或`mpirun`跨节点运行774M。
- en: Study published training logs for GPT-2 1.6B and compare them to your own —
    how does scaling change the shape of the loss curve?
  id: totrans-3186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 研究GPT-2 1.6B发布的训练日志，并将其与你的日志进行比较——规模如何改变损失曲线的形状？
- en: The Takeaway
  id: totrans-3187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸取的经验
- en: 'Scaling isn’t just “bigger models need bigger GPUs.” Each increase in model
    size reshapes the training process, introducing new bottlenecks and requiring
    new techniques. *llm.c* is valuable precisely because it makes these transitions
    transparent: you can start with a tiny model and gradually experience the real
    engineering hurdles of training state-of-the-art language models.'
  id: totrans-3188
  prefs: []
  type: TYPE_NORMAL
  zh: 规模不仅仅是“更大的模型需要更大的GPU”。模型规模的每次增加都会重塑训练过程，引入新的瓶颈并需要新的技术。*llm.c*之所以有价值，正是因为它使这些过渡变得透明：你可以从一个小型模型开始，并逐渐体验训练最先进语言模型的真正工程挑战。
- en: 79\. NCCL Tuning and Overlap Opportunities
  id: totrans-3189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 79. NCCL调优和重叠机会
- en: 'Once your training runs extend beyond a single GPU, communication overhead
    becomes a central challenge. Every training step requires gradients to be exchanged
    among GPUs so that the optimizer updates stay synchronized. This is where NCCL
    (NVIDIA Collective Communications Library) comes in. NCCL provides efficient implementations
    of collective operations like all-reduce, all-gather, and broadcast. But simply
    using NCCL isn’t enough: how you tune it, and how you overlap communication with
    computation, can make the difference between sluggish training and near-linear
    scaling.'
  id: totrans-3190
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你的训练运行扩展到单个GPU之外，通信开销就成为一个核心挑战。每个训练步骤都需要在GPU之间交换梯度，以便优化器更新保持同步。这就是NCCL（NVIDIA集体通信库）发挥作用的地方。NCCL提供了集体操作（如all-reduce、all-gather和broadcast）的高效实现。但仅仅使用NCCL是不够的：你如何调优它，以及你如何将通信与计算重叠，可以决定训练是缓慢的还是接近线性的扩展。
- en: How NCCL Works in Training
  id: totrans-3191
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: NCCL在训练中的工作原理
- en: In *llm.c*, when multiple GPUs train together, each GPU computes its local gradients
    during backpropagation. At the end of the backward pass, NCCL’s all-reduce combines
    gradients across GPUs so that every GPU ends up with the same values. Only then
    can the optimizer step forward.
  id: totrans-3192
  prefs: []
  type: TYPE_NORMAL
  zh: 在`llm.c`中，当多个GPU一起训练时，每个GPU在反向传播期间计算其局部梯度。在反向传播结束时，NCCL的全归约将跨GPU合并梯度，使得每个GPU最终具有相同的值。只有在这种情况下，优化器才能向前迈进。
- en: Without NCCL, you’d have to write custom point-to-point code with `cudaMemcpyPeer`
    or MPI, which would be both slower and harder to maintain. NCCL ensures the communication
    pattern is efficient for the underlying hardware — PCIe, NVLink, or InfiniBand.
  id: totrans-3193
  prefs: []
  type: TYPE_NORMAL
  zh: 没有NCCL，您将不得不使用`cudaMemcpyPeer`或MPI编写自定义点对点代码，这将既慢又难以维护。NCCL确保通信模式对底层硬件（PCIe、NVLink或InfiniBand）是高效的。
- en: Key NCCL Tuning Parameters
  id: totrans-3194
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 关键NCCL调整参数
- en: NCCL_DEBUG Setting `NCCL_DEBUG=INFO` helps you understand what NCCL is doing.
    For performance tuning, logs are essential.
  id: totrans-3195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: NCCL_DEBUG 设置`NCCL_DEBUG=INFO`有助于您了解NCCL正在做什么。对于性能调整，日志是必不可少的。
- en: NCCL_SOCKET_IFNAME On multi-node clusters, this decides which network interface
    NCCL binds to. Using the wrong interface (like Ethernet instead of InfiniBand)
    can slow training by orders of magnitude.
  id: totrans-3196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: NCCL_SOCKET_IFNAME 在多节点集群上，这决定了NCCL绑定到哪个网络接口。使用错误的接口（如以太网而不是InfiniBand）可能会以数量级降低训练速度。
- en: 'NCCL_ALGO Determines how collectives are executed:'
  id: totrans-3197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: NCCL_ALGO 决定了集体操作如何执行：
- en: '*Ring*: good for large message sizes, stable performance.'
  id: totrans-3198
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*环形*：适用于大消息大小，性能稳定。'
- en: '*Tree*: faster for small messages, less latency. Some training runs benefit
    from experimenting with both.'
  id: totrans-3199
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*树形*：对于小消息更快，延迟更低。一些训练运行可以从尝试两者中受益。'
- en: NCCL_IB_DISABLE Useful if you want to force NCCL to avoid InfiniBand and stick
    with TCP/IP, usually for debugging network issues.
  id: totrans-3200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: NCCL_IB_DISABLE 如果您想强制NCCL避免InfiniBand并坚持使用TCP/IP，通常用于调试网络问题。
- en: Overlapping Communication with Computation
  id: totrans-3201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通信与计算的叠加
- en: The backward pass doesn’t need to wait until all gradients are computed before
    starting to communicate. In fact, gradients for earlier layers can start their
    all-reduce while later layers are still computing gradients. This is called communication-computation
    overlap.
  id: totrans-3202
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播不需要等待所有梯度计算完毕才开始通信。实际上，较早层的梯度可以在较晚层仍在计算梯度时开始全归约。这被称为通信-计算叠加。
- en: 'For example:'
  id: totrans-3203
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: 'Without overlap: compute gradients for all layers → run NCCL all-reduce for
    all gradients → update parameters.'
  id: totrans-3204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不使用叠加：计算所有层的梯度 → 运行所有梯度全归约 → 更新参数。
- en: 'With overlap: while gradients for higher layers are still being computed, start
    the all-reduce for earlier layers.'
  id: totrans-3205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用叠加：当较高层的梯度仍在计算时，开始较早层的全归约。
- en: This reduces idle time and often leads to substantial throughput gains. Some
    frameworks (like PyTorch’s DistributedDataParallel) implement this automatically.
    In a low-level system like *llm.c*, this would require careful kernel launch ordering
    and stream management.
  id: totrans-3206
  prefs: []
  type: TYPE_NORMAL
  zh: 这减少了空闲时间，通常会导致显著的吞吐量提升。一些框架（如PyTorch的DistributedDataParallel）会自动实现这一点。在像`llm.c`这样的低级系统中，这需要仔细的内核启动顺序和流管理。
- en: Practical Example
  id: totrans-3207
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实际示例
- en: Imagine you’re training GPT-2 774M across 8 GPUs. Each backward pass produces
    ~3 GB of gradients. If you wait until all gradients are ready before syncing,
    the all-reduce might take 200 ms. If your compute step also takes 200 ms, then
    half of your training time is spent idle. With overlap, you can hide much of that
    communication inside compute time, potentially cutting step time almost in half.
  id: totrans-3208
  prefs: []
  type: TYPE_NORMAL
  zh: 想象您正在使用8个GPU训练GPT-2 774M。每次反向传播会产生约3 GB的梯度。如果您等待所有梯度都准备好后再同步，全归约可能需要200毫秒。如果您的计算步骤也花费200毫秒，那么您的一半训练时间将花费在空闲上。使用叠加，您可以将大部分通信隐藏在计算时间内，可能将步骤时间几乎减半。
- en: Why It Matters
  id: totrans-3209
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: As model sizes increase, communication costs can rival or exceed computation.
    Without tuning, GPUs spend more time waiting for data to arrive than actually
    training the model. By understanding NCCL and applying overlap techniques, you
    unlock the ability to scale efficiently to dozens or even hundreds of GPUs.
  id: totrans-3210
  prefs: []
  type: TYPE_NORMAL
  zh: 随着模型大小的增加，通信成本可以与计算相媲美或超过计算。如果没有调整，GPU将花费更多时间等待数据到达，而不是实际训练模型。通过理解NCCL并应用叠加技术，您可以解锁高效扩展到数十甚至数百个GPU的能力。
- en: Try It Yourself
  id: totrans-3211
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 尝试自己操作
- en: Run a multi-GPU training job with `NCCL_DEBUG=INFO` enabled and watch the communication
    patterns.
  id: totrans-3212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`NCCL_DEBUG=INFO`启用多GPU训练作业，并观察通信模式。
- en: Change `NCCL_ALGO` between `Ring` and `Tree` and measure the effect on step
    times.
  id: totrans-3213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `NCCL_ALGO` 之间切换 `Ring` 和 `Tree` 并测量对步骤时间的影响。
- en: Experiment with setting `CUDA_LAUNCH_BLOCKING=1` to remove overlap, then remove
    it again to see how communication and computation interleave.
  id: totrans-3214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试设置 `CUDA_LAUNCH_BLOCKING=1` 以消除重叠，然后再次移除以查看通信和计算如何交错。
- en: If you have a cluster, try forcing NCCL to use Ethernet instead of InfiniBand
    and compare bandwidth.
  id: totrans-3215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你有集群，尝试强制NCCL使用以太网而不是InfiniBand，并比较带宽。
- en: Profile a multi-GPU run using `nvprof` or Nsight Systems and check whether NCCL
    collectives overlap with kernel execution.
  id: totrans-3216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `nvprof` 或 Nsight Systems对多GPU运行进行性能分析，并检查NCCL集合是否与内核执行重叠。
- en: The Takeaway
  id: totrans-3217
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸取教训
- en: Efficient distributed training is not only about having more GPUs — it’s about
    keeping them busy. NCCL provides the communication backbone, but how you configure
    and overlap its operations determines whether you get close to linear scaling
    or waste resources. Mastering these details transforms multi-GPU training from
    “just working” into truly efficient large-scale computation.
  id: totrans-3218
  prefs: []
  type: TYPE_NORMAL
  zh: 高效的分布式训练不仅关于拥有更多的GPU——它关于保持它们忙碌。NCCL提供了通信骨干，但如何配置和重叠其操作决定了你是否接近线性扩展或浪费资源。掌握这些细节将多GPU训练从“仅仅工作”转变为真正高效的大规模计算。
- en: 80\. Common Multi-GPU Errors and Fixes
  id: totrans-3219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 80. 常见的多GPU错误和修复
- en: When running *llm.c* across multiple GPUs, errors can range from confusing hangs
    to cryptic NCCL messages. These problems are normal in distributed training, but
    they can eat up hours unless you recognize the patterns. The good news is that
    most errors fall into a handful of common categories, and once you learn the typical
    causes, they’re easier to diagnose and fix.
  id: totrans-3220
  prefs: []
  type: TYPE_NORMAL
  zh: 当在多个GPU上运行 *llm.c* 时，错误可能从令人困惑的挂起到神秘的NCCL消息。这些问题在分布式训练中很常见，但如果不识别模式，它们可能会消耗数小时。好消息是，大多数错误都落入几个常见的类别，一旦你了解了典型的原因，它们就更容易诊断和修复。
- en: 'Error Type 1: Process Hangs with No Output'
  id: totrans-3221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 错误类型 1：无输出挂起进程
- en: 'Symptom: Training starts but then freezes. No error message, no crash, just
    silence. Cause: Usually, one or more ranks are out of sync. This could mean:'
  id: totrans-3222
  prefs: []
  type: TYPE_NORMAL
  zh: 症状：训练开始但随后冻结。没有错误消息，没有崩溃，只有沉默。原因：通常，一个或多个排名不同步。这可能意味着：
- en: Different batch sizes on each rank (one rank has fewer tokens left).
  id: totrans-3223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个排名上的批处理大小不同（一个排名剩余的标记较少）。
- en: A mismatch in collective calls — for example, one GPU calls `all_reduce` while
    another skips it.
  id: totrans-3224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集合调用不匹配——例如，一个GPU调用 `all_reduce`，而另一个跳过它。
- en: 'A CUDA error in one process that prevents it from reaching synchronization.
    Fix:'
  id: totrans-3225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个进程中的CUDA错误阻止它达到同步。修复：
- en: Check that dataloaders feed the same number of steps to every rank.
  id: totrans-3226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确认数据加载器为每个排名提供相同数量的步骤。
- en: Add error checking to CUDA calls.
  id: totrans-3227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在CUDA调用中添加错误检查。
- en: Enable `NCCL_DEBUG=INFO` to trace which rank got stuck.
  id: totrans-3228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启用 `NCCL_DEBUG=INFO` 以跟踪哪个排名卡住。
- en: 'Error Type 2: `NCCL WARN Net: no interface found`'
  id: totrans-3229
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '错误类型 2：`NCCL WARN Net: 未找到接口`'
- en: 'Symptom: NCCL reports it can’t find a network interface, or training is extremely
    slow. Cause: NCCL can’t discover the correct interface to use for inter-node communication.
    By default, it may try Ethernet instead of InfiniBand. Fix:'
  id: totrans-3230
  prefs: []
  type: TYPE_NORMAL
  zh: 症状：NCCL报告找不到网络接口，或训练速度极慢。原因：NCCL无法发现用于节点间通信的正确接口。默认情况下，它可能会尝试使用以太网而不是InfiniBand。修复：
- en: 'Set `NCCL_SOCKET_IFNAME` to the right interface, e.g.:'
  id: totrans-3231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 `NCCL_SOCKET_IFNAME` 设置为正确的接口，例如：
- en: '[PRE165]'
  id: totrans-3232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE165]'
- en: Confirm with your sysadmin which network interfaces are available for high-performance
    GPU communication.
  id: totrans-3233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与你的系统管理员确认哪些网络接口可用于高性能GPU通信。
- en: 'Error Type 3: `CUDA_ERROR_OUT_OF_MEMORY`'
  id: totrans-3234
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 错误类型 3：`CUDA_ERROR_OUT_OF_MEMORY`
- en: 'Symptom: Processes crash when allocating model weights or during the backward
    pass. Cause:'
  id: totrans-3235
  prefs: []
  type: TYPE_NORMAL
  zh: 症状：在分配模型权重或反向传播过程中进程崩溃。原因：
- en: Model too large for the available GPU memory.
  id: totrans-3236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型太大，无法适应可用的GPU内存。
- en: Batch size too high.
  id: totrans-3237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批处理大小过高。
- en: 'Memory fragmentation from repeated allocations. Fix:'
  id: totrans-3238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复分配导致的内存碎片。修复：
- en: Reduce batch size `B` or sequence length `T`.
  id: totrans-3239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少批处理大小 `B` 或序列长度 `T`。
- en: Try mixed precision (FP16/BF16) if supported.
  id: totrans-3240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果支持，尝试混合精度（FP16/BF16）。
- en: Restart processes to clear memory fragmentation.
  id: totrans-3241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新启动进程以清除内存碎片。
- en: 'Error Type 4: `unhandled system error, NCCL version mismatch`'
  id: totrans-3242
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 错误类型 4：`未处理的系统错误，NCCL版本不匹配`
- en: 'Symptom: One process logs NCCL version `2.17` and another logs `2.14`. Training
    fails. Cause: Different NCCL libraries are being used across nodes. This happens
    when software environments aren’t identical. Fix:'
  id: totrans-3243
  prefs: []
  type: TYPE_NORMAL
  zh: 症状：一个进程记录NCCL版本 `2.17`，另一个记录 `2.14`。训练失败。原因：节点间使用不同的NCCL库。这发生在软件环境不相同的情况下。修复：
- en: Use the same container or module environment on all nodes.
  id: totrans-3244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在所有节点上使用相同的容器或模块环境。
- en: Confirm NCCL versions with `ldd` or `conda list`.
  id: totrans-3245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `ldd` 或 `conda list` 确认 NCCL 版本。
- en: 'Error Type 5: Validation Loss Diverges on Multi-GPU but Not on Single GPU'
  id: totrans-3246
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 错误类型 5：在多 GPU 上验证损失发散，但在单 GPU 上不会
- en: 'Symptom: Loss values explode only when running across multiple GPUs. Cause:
    Gradient synchronization may be broken — for example, only a subset of parameters
    are being all-reduced. Another possibility is using a different effective learning
    rate because of batch size scaling. Fix:'
  id: totrans-3247
  prefs: []
  type: TYPE_NORMAL
  zh: 症状：仅在跨多个 GPU 运行时损失值爆炸。原因：梯度同步可能已损坏——例如，只有参数子集正在执行 all-reduce。另一种可能性是由于批大小缩放而使用不同的有效学习率。修复：
- en: Confirm that all parameters participate in gradient synchronization.
  id: totrans-3248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确认所有参数都参与梯度同步。
- en: 'Scale the learning rate properly: if you double the global batch size by using
    more GPUs, you may need to adjust the learning rate.'
  id: totrans-3249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确缩放学习率：如果你通过使用更多 GPU 将全局批大小加倍，你可能需要调整学习率。
- en: Why It Matters
  id: totrans-3250
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: 'Multi-GPU training is powerful but unforgiving: even tiny mismatches in environment,
    data, or synchronization can cause errors. Instead of treating these as random
    mysteries, it’s helpful to recognize the patterns. Each error message or hang
    has a likely cause, and learning to map symptoms to fixes will make distributed
    training much smoother.'
  id: totrans-3251
  prefs: []
  type: TYPE_NORMAL
  zh: 多 GPU 训练功能强大但不容忍错误：即使是环境、数据或同步中的微小不匹配也可能导致错误。与其将这些视为随机神秘的事物，不如识别这些模式。每个错误信息或挂起都有可能的原因，学会将症状映射到解决方案将使分布式训练更加顺畅。
- en: Try It Yourself
  id: totrans-3252
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 试试看
- en: Intentionally reduce the dataset size so one rank runs out of data early — observe
    the hang.
  id: totrans-3253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 故意减少数据集大小，以便一个排名提前耗尽数据——观察挂起。
- en: Launch a multi-node run without setting `NCCL_SOCKET_IFNAME` and watch how performance
    collapses.
  id: totrans-3254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不设置 `NCCL_SOCKET_IFNAME` 就启动多节点运行，看看性能如何崩溃。
- en: Increase the batch size until you trigger `CUDA_ERROR_OUT_OF_MEMORY`, then reduce
    it step by step to see the limit.
  id: totrans-3255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 增加批大小，直到触发 `CUDA_ERROR_OUT_OF_MEMORY`，然后逐步减少以查看限制。
- en: Experiment with mismatched NCCL versions across nodes if you have access to
    multiple environments, then fix it by standardizing.
  id: totrans-3256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你有多个环境，尝试在节点之间进行不匹配的 NCCL 版本实验，然后通过标准化来修复它。
- en: Run a small model with different batch sizes per rank and study how the validation
    loss diverges.
  id: totrans-3257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用不同批量大小的模型运行小模型，并研究验证损失如何发散。
- en: The Takeaway
  id: totrans-3258
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸收要点
- en: Most multi-GPU errors aren’t mysterious once you understand what’s happening
    under the hood. They usually boil down to synchronization mismatches, network
    misconfigurations, memory limits, or environment inconsistencies. With the right
    debugging tools and a systematic mindset, you can fix these problems quickly and
    keep your training runs moving forward.
  id: totrans-3259
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你了解了底层发生的事情，大多数多 GPU 错误就不会神秘。它们通常归结为同步不匹配、网络配置错误、内存限制或环境不一致。有了正确的调试工具和系统性的思维方式，你可以快速修复这些问题，并使训练运行继续进行。
- en: Chapter 9\. Extending the codebase
  id: totrans-3260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 9 章：扩展代码库
- en: 81\. The `dev/cuda` Library for Custom Kernels
  id: totrans-3261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 81. 用于自定义内核的 `dev/cuda` 库
- en: 'So far, most of the CUDA logic in *llm.c* has relied on NVIDIA’s optimized
    libraries like cuBLAS and cuDNN. These libraries are extremely powerful and efficient,
    but sometimes you want more control: maybe you’re experimenting with a new attention
    mechanism, or maybe you want to fuse multiple operations into a single kernel
    to reduce memory traffic. That’s where the `dev/cuda` directory comes in. It’s
    the playground for custom kernels.'
  id: totrans-3262
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，*llm.c* 中的大多数 CUDA 逻辑都依赖于 NVIDIA 的优化库，如 cuBLAS 和 cuDNN。这些库非常强大且高效，但有时你想要更多的控制：也许你正在尝试新的注意力机制，或者你可能想将多个操作融合到一个内核中，以减少内存流量。这就是
    `dev/cuda` 目录的作用所在。它是自定义内核的游乐场。
- en: What Lives in `dev/cuda`
  id: totrans-3263
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '`dev/cuda` 中有什么'
- en: If you look at the repository structure, you’ll notice a folder named `dev/cuda`.
    This is not part of the minimal training path — you can train GPT-2 models without
    ever touching it. Instead, it contains experimental kernels that showcase how
    to move from simple CUDA examples toward more advanced, production-level implementations.
  id: totrans-3264
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看存储库结构，你会注意到一个名为 `dev/cuda` 的文件夹。这不是最小训练路径的一部分——你可以在不接触它的情况下训练 GPT-2 模型。相反，它包含实验性内核，展示了如何从简单的
    CUDA 示例过渡到更高级、生产级别的实现。
- en: 'Inside, you’ll typically find:'
  id: totrans-3265
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部，你通常会找到：
- en: 'Hello World kernels: basic examples like elementwise addition to get familiar
    with CUDA.'
  id: totrans-3266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hello World 内核：基本示例，如逐元素加法，以熟悉 CUDA。
- en: 'Fused operations: simple prototypes for combining steps like bias addition
    + activation.'
  id: totrans-3267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 融合操作：结合步骤如偏置添加+激活的简单原型。
- en: 'Benchmark code: small programs that measure kernel performance compared to
    cuBLAS/cuDNN.'
  id: totrans-3268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基准代码：测量内核性能与cuBLAS/cuDNN相比的小型程序。
- en: These files are not polished production code. They’re meant to be read, modified,
    and played with — like lab notebooks for CUDA development.
  id: totrans-3269
  prefs: []
  type: TYPE_NORMAL
  zh: 这些文件不是经过打磨的生产代码。它们旨在被阅读、修改和玩耍——就像CUDA开发的实验笔记本。
- en: Why Custom Kernels Matter
  id: totrans-3270
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么自定义内核很重要
- en: 'Libraries like cuBLAS are designed to cover a wide range of use cases, but
    they don’t always hit the sweet spot for your specific workload. Writing custom
    kernels allows you to:'
  id: totrans-3271
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于cuBLAS这样的库旨在覆盖广泛的使用场景，但它们并不总是能满足你特定工作负载的甜蜜点。编写自定义内核允许你：
- en: 'Fuse operations: Instead of launching separate kernels for bias addition, activation,
    and dropout, you can do all three in one kernel, saving time on memory reads/writes.'
  id: totrans-3272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 融合操作：而不是为偏置添加、激活和dropout分别启动内核，你可以在一个内核中完成所有这三个操作，从而在内存读写上节省时间。
- en: 'Experiment with new algorithms: If you invent a new type of attention or normalization,
    you can’t rely on cuDNN — you need to implement it yourself.'
  id: totrans-3273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试新的算法：如果你发明了一种新的注意力或归一化类型，你不能依赖于cuDNN——你需要自己实现它。
- en: 'Learn how GPUs actually work: Reading and writing custom kernels teaches you
    about thread blocks, memory hierarchy, and warp scheduling, all of which deepen
    your understanding of GPU programming.'
  id: totrans-3274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习GPU的实际工作原理：阅读和编写自定义内核教你关于线程块、内存层次结构和warp调度，所有这些都加深了你对于GPU编程的理解。
- en: 'Example: A Simple Elementwise Kernel'
  id: totrans-3275
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例：一个简单的逐元素内核
- en: 'Here’s a very small kernel from a toy example you might find in `dev/cuda`:'
  id: totrans-3276
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个来自`dev/cuda`目录中玩具示例的非常小的内核：
- en: '[PRE166]'
  id: totrans-3277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE166]'
- en: 'This kernel adds two arrays `a` and `b` elementwise. While trivial compared
    to attention mechanisms, it illustrates the GPU execution model:'
  id: totrans-3278
  prefs: []
  type: TYPE_NORMAL
  zh: 这个内核逐元素地添加两个数组`a`和`b`。与注意力机制相比，这微不足道，但它说明了GPU执行模型：
- en: Each thread handles one index `i`.
  id: totrans-3279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个线程处理一个索引`i`。
- en: Threads are grouped into blocks, and blocks form a grid.
  id: totrans-3280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线程被分组到块中，块形成一个网格。
- en: 'Memory access is explicit: you control exactly how `out` is written.'
  id: totrans-3281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存访问是显式的：你完全控制`out`的写入方式。
- en: Scaling this up to real workloads means adding more complexity — shared memory,
    warp shuffles, half-precision math — but the principles remain the same.
  id: totrans-3282
  prefs: []
  type: TYPE_NORMAL
  zh: 将其扩展到真实工作负载意味着增加更多复杂性——共享内存、warp洗牌、半精度数学——但原则保持不变。
- en: Why It Matters
  id: totrans-3283
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: The `dev/cuda` directory is not just for fun experiments. It’s a bridge between
    “using GPU libraries” and “designing GPU algorithms.” By learning to write kernels
    here, you gain the freedom to customize and optimize beyond what standard libraries
    provide. That skill becomes essential if you want to innovate in model architectures
    or squeeze the last bit of performance out of your hardware.
  id: totrans-3284
  prefs: []
  type: TYPE_NORMAL
  zh: '`dev/cuda`目录不仅仅是为了乐趣实验。它是“使用GPU库”和“设计GPU算法”之间的桥梁。通过在这里学习编写内核，你可以获得在标准库提供之外进行定制和优化的自由。如果你想在模型架构上进行创新或从硬件中榨取最后一丝性能，这项技能变得至关重要。'
- en: Try It Yourself
  id: totrans-3285
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 试试看
- en: Open one of the example `.cu` files in `dev/cuda` and compile it with `nvcc`.
  id: totrans-3286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`dev/cuda`目录中打开一个示例`.cu`文件，并用`nvcc`编译它。
- en: Modify the elementwise kernel so it performs `out[i] = a[i] * b[i]` instead
    of addition.
  id: totrans-3287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改逐元素内核，使其执行`out[i] = a[i] * b[i]`而不是加法。
- en: Benchmark your kernel against the equivalent cuBLAS call (e.g., `cublasSaxpy`)
    and compare performance.
  id: totrans-3288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将你的内核与等效的cuBLAS调用（例如，`cublasSaxpy`）进行基准测试并比较性能。
- en: Write a fused kernel that does bias addition followed by ReLU activation in
    one pass.
  id: totrans-3289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个融合内核，在一次遍历中执行偏置添加和ReLU激活。
- en: Use `nvprof` or Nsight Systems to measure how many kernel launches occur in
    a forward pass and imagine how custom fusion might reduce them.
  id: totrans-3290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`nvprof`或Nsight Systems来测量正向传递中发生的内核启动次数，并想象自定义融合如何减少它们。
- en: The Takeaway
  id: totrans-3291
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸收要点
- en: The `dev/cuda` library is your sandbox for learning and experimenting with CUDA.
    It’s not required for running GPT-2, but it’s where you build the skills to go
    beyond libraries and design your own GPU operations. Whether you’re optimizing
    for speed or testing new research ideas, this directory is where theory meets
    practice in GPU programming.
  id: totrans-3292
  prefs: []
  type: TYPE_NORMAL
  zh: '`dev/cuda`库是你在学习CUDA和实验CUDA时的沙盒。它不是运行GPT-2所必需的，但它是你构建超越库并设计自己的GPU操作技能的地方。无论你是优化速度还是测试新的研究想法，这个目录是GPU编程中理论与实践相遇的地方。'
- en: 82\. Adding New Dataset Pipelines (`dev/data/*`)
  id: totrans-3293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 82. 添加新的数据集管道（`dev/data/*`）
- en: 'Training a language model is not just about having a clever model — it’s equally
    about the data you feed it. In *llm.c*, datasets are handled in a very lightweight
    way compared to frameworks like PyTorch. Instead of complicated abstractions,
    the project keeps things simple: tokenize your text once, save it as a `.bin`
    file, and then stream batches of tokens into the model.'
  id: totrans-3294
  prefs: []
  type: TYPE_NORMAL
  zh: 训练语言模型不仅仅是拥有一个聪明的模型——同样重要的是你给它喂的数据。在 *llm.c* 中，与 PyTorch 等框架相比，数据集的处理方式非常轻量级。项目不使用复杂的抽象，而是保持简单：一次性标记你的文本，将其保存为
    `.bin` 文件，然后将标记批次流式传输到模型中。
- en: The `dev/data/` directory is where this happens. It contains scripts and utilities
    for preparing different datasets, from tiny toy corpora like Tiny Shakespeare
    to larger collections like TinyStories or OpenWebText subsets. Understanding how
    this directory works is the key to plugging in your own datasets.
  id: totrans-3295
  prefs: []
  type: TYPE_NORMAL
  zh: '`dev/data/` 目录是这里发生的地方。它包含用于准备不同数据集的脚本和实用程序，从小型玩具语料库如 Tiny Shakespeare 到更大的集合如
    TinyStories 或 OpenWebText 子集。理解这个目录的工作原理是插入您自己的数据集的关键。'
- en: How Data Pipelines Work in *llm.c*
  id: totrans-3296
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在 *llm.c* 中数据管道是如何工作的
- en: 'At a high level, the pipeline follows three steps:'
  id: totrans-3297
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，管道遵循三个步骤：
- en: Download or provide raw text data. For example, `tiny_shakespeare.txt` is just
    a plain text file with plays concatenated.
  id: totrans-3298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载或提供原始文本数据。例如，`tiny_shakespeare.txt` 只是一个简单的文本文件，其中包含了拼接在一起的剧本。
- en: Tokenize the data once using the GPT-2 tokenizer. The tokenizer converts text
    into integers according to `gpt2_tokenizer.bin`.
  id: totrans-3299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 GPT-2 标记化器一次性标记数据。标记化器根据 `gpt2_tokenizer.bin` 将文本转换为整数。
- en: Write the tokens to a binary file (`.bin`). This is a flat array of integers
    stored as 32-bit values, which makes it fast to memory-map and stream during training.
  id: totrans-3300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将标记写入二进制文件（`.bin`）。这是一个存储为 32 位值的整数平面数组，这使得在训练期间快速内存映射和流式传输变得很快。
- en: Once the `.bin` files exist, `dataloader_init` can open them, divide them into
    training and validation splits, and generate batches of shape `(B, T)` for the
    model.
  id: totrans-3301
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦生成了 `.bin` 文件，`dataloader_init` 可以打开它们，将它们分成训练和验证分割，并为模型生成形状为 `(B, T)` 的批次。
- en: What’s Inside `dev/data/`
  id: totrans-3302
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '`dev/data/` 中有什么'
- en: 'The folder contains small scripts like:'
  id: totrans-3303
  prefs: []
  type: TYPE_NORMAL
  zh: 文件夹包含如下小型脚本：
- en: '`download_starter_pack.sh` — downloads Tiny Shakespeare and TinyStories.'
  id: totrans-3304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`download_starter_pack.sh`——下载 Tiny Shakespeare 和 TinyStories。'
- en: Tokenization scripts — often small Python snippets that run the GPT-2 tokenizer
    over raw text.
  id: totrans-3305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标记化脚本——通常是运行在原始文本上的小型 Python 片段，使用 GPT-2 标记化器。
- en: Prebuilt `.bin` files — these are used in the quickstart so you don’t need to
    regenerate them yourself.
  id: totrans-3306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预建的 `.bin` 文件——这些在快速入门中使用，因此你不需要自己重新生成它们。
- en: 'The design choice here is minimalism: instead of a heavy dataset framework,
    you get plain files and short scripts. You can read and understand everything
    in a few minutes.'
  id: totrans-3307
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的设计选择是极简主义：而不是一个重量级的数据集框架，你得到的是普通文件和简短的脚本。你可以在几分钟内阅读和理解一切。
- en: Adding Your Own Dataset
  id: totrans-3308
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 添加您自己的数据集
- en: 'Suppose you want to train on your company’s support chat logs or a new dataset
    you’ve found. The process looks like this:'
  id: totrans-3309
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想在公司支持聊天记录或你找到的新数据集上进行训练。过程如下：
- en: Prepare raw text in a simple format (one text file is fine).
  id: totrans-3310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以简单格式准备原始文本（一个文本文件即可）。
- en: 'Run the tokenizer from *llm.c* on it:'
  id: totrans-3311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 *llm.c* 上运行标记化器：
- en: '[PRE167]'
  id: totrans-3312
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE167]'
- en: This produces a binary token file.
  id: totrans-3313
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将生成一个二进制标记文件。
- en: Drop the file into `dev/data/`. You might name it `my_corpus_train.bin` and
    `my_corpus_val.bin`.
  id: totrans-3314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文件放入 `dev/data/`。你可能将其命名为 `my_corpus_train.bin` 和 `my_corpus_val.bin`。
- en: 'Point the dataloader at it in your training code:'
  id: totrans-3315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的训练代码中将数据加载器指向它：
- en: '[PRE168]'
  id: totrans-3316
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE168]'
- en: That’s it — you now have a new dataset pipeline integrated with the same training
    loop.
  id: totrans-3317
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样——你现在有一个新的数据集管道与相同的训练循环集成。
- en: Why It Matters
  id: totrans-3318
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: 'Many frameworks hide data preprocessing behind layers of abstractions. *llm.c*
    takes the opposite approach: it makes the process transparent. You see exactly
    how text becomes tokens, how those tokens become batches, and how the model consumes
    them. This transparency makes it easier to debug, extend, and customize. Adding
    a new dataset is no longer a mystery — it’s just a matter of writing one file
    and updating a path.'
  id: totrans-3319
  prefs: []
  type: TYPE_NORMAL
  zh: 许多框架将数据预处理隐藏在多层抽象之后。*llm.c* 采取了相反的方法：它使过程变得透明。你可以看到文本如何变成标记，这些标记如何变成批次，以及模型如何消费它们。这种透明性使得调试、扩展和定制变得更加容易。添加新的数据集不再是神秘的事情——只需编写一个文件并更新路径即可。
- en: Try It Yourself
  id: totrans-3320
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 亲自尝试
- en: Explore the `dev/data/` directory and read through the provided scripts.
  id: totrans-3321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 探索 `dev/data/` 目录并阅读提供的脚本。
- en: Tokenize a new small dataset of your choice (a novel, a set of Wikipedia pages,
    or your own text).
  id: totrans-3322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标记化你选择的新小型数据集（一部小说、一组维基百科页面或你自己的文本）。
- en: Train a 124M model on your new dataset and observe the loss curve.
  id: totrans-3323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的新数据集上训练一个124M模型，并观察损失曲线。
- en: Compare validation loss between Tiny Shakespeare and your dataset — how does
    the model behave differently?
  id: totrans-3324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较Tiny Shakespeare和你的数据集之间的验证损失——模型的行为有何不同？
- en: Try increasing sequence length `T` to see how batching interacts with longer
    documents.
  id: totrans-3325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试增加序列长度 `T`，看看批处理如何与较长的文档交互。
- en: The Takeaway
  id: totrans-3326
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸收要点
- en: The `dev/data` folder is where you connect language models to the real world.
    It shows how raw text becomes training-ready binary files with almost no overhead.
    By learning to add your own pipelines, you gain the ability to train *llm.c* on
    any dataset — from classic literature to domain-specific corpora — while keeping
    the workflow fast and understandable.
  id: totrans-3327
  prefs: []
  type: TYPE_NORMAL
  zh: '`dev/data` 文件夹是您将语言模型连接到现实世界的地方。它展示了原始文本如何几乎无开销地成为训练就绪的二进制文件。通过学习添加自己的管道，您将能够以快速和可理解的方式在任意数据集上训练
    *llm.c* —— 从经典文学到特定领域的语料库。'
- en: 83\. Adding a New Optimizer to the Codebase
  id: totrans-3328
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 83. 将新的优化器添加到代码库中
- en: 'So far, *llm.c* has focused on AdamW, which is the workhorse optimizer for
    training transformer models. But deep learning is a fast-moving field: new optimizers
    appear, older ones sometimes resurface, and certain workloads benefit from alternatives.
    The simplicity of *llm.c* makes it a great environment to learn how to implement
    and experiment with optimizers.'
  id: totrans-3329
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，*llm.c* 专注于AdamW，这是训练转换器模型的工作马优化器。但深度学习是一个快速发展的领域：新的优化器出现，旧的优化器有时会重新出现，某些工作负载从替代方案中受益。*llm.c*
    的简单性使其成为学习如何实现和实验优化器的绝佳环境。
- en: Where Optimizers Live in *llm.c*
  id: totrans-3330
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优化器在 *llm.c* 中的位置
- en: In the CPU training path, the optimizer logic is implemented directly in C in
    the function `gpt2_update`. This function iterates through every parameter, applies
    AdamW’s moment updates, applies bias correction, and then modifies the parameter
    values in place.
  id: totrans-3331
  prefs: []
  type: TYPE_NORMAL
  zh: 在CPU训练路径中，优化器逻辑直接在 `gpt2_update` 函数中以C语言实现。这个函数遍历每个参数，应用AdamW的动量更新，应用偏差校正，然后就地修改参数值。
- en: 'Because the parameters, gradients, and optimizer states are all stored as contiguous
    arrays in memory (`params_memory`, `grads_memory`, `m_memory`, `v_memory`), adding
    a new optimizer usually means:'
  id: totrans-3332
  prefs: []
  type: TYPE_NORMAL
  zh: 因为参数、梯度和优化器状态都存储在内存中的连续数组中（`params_memory`、`grads_memory`、`m_memory`、`v_memory`），添加新的优化器通常意味着：
- en: Allocating any new state arrays you need.
  id: totrans-3333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分配任何需要的新的状态数组。
- en: Defining the update rule in the training loop.
  id: totrans-3334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练循环中定义更新规则。
- en: Adding function calls for your new optimizer, similar to `gpt2_update`.
  id: totrans-3335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加你的新优化器的函数调用，类似于 `gpt2_update`。
- en: 'Example: Implementing SGD with Momentum'
  id: totrans-3336
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例：实现带有动量的SGD
- en: 'Stochastic Gradient Descent (SGD) with momentum is much simpler than AdamW.
    The update rule looks like this:'
  id: totrans-3337
  prefs: []
  type: TYPE_NORMAL
  zh: 带有动量的随机梯度下降（SGD）比AdamW简单得多。更新规则如下：
- en: '[PRE169]'
  id: totrans-3338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE169]'
- en: Here, `m_memory` stores the velocity (the exponentially decayed average of past
    gradients). There’s no second moment estimate like in AdamW, so it’s leaner in
    both code and memory usage.
  id: totrans-3339
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`m_memory` 存储速度（过去梯度的指数衰减平均值）。没有像AdamW那样的第二阶矩估计，因此在代码和内存使用上更简洁。
- en: Comparing Optimizers
  id: totrans-3340
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 比较优化器
- en: 'Adding new optimizers lets you experiment and compare behaviors:'
  id: totrans-3341
  prefs: []
  type: TYPE_NORMAL
  zh: 添加新的优化器让你可以实验和比较行为：
- en: '| Optimizer | Strengths | Weaknesses | Memory Needs |'
  id: totrans-3342
  prefs: []
  type: TYPE_TB
  zh: '| 优化器 | 优点 | 缺点 | 内存需求 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-3343
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| SGD | Simple, stable, fewer hyperparameters | Slow convergence on large models
    | Low |'
  id: totrans-3344
  prefs: []
  type: TYPE_TB
  zh: '| SGD | 简单、稳定、超参数较少 | 大模型上收敛速度慢 | 低 |'
- en: '| SGD + Momentum | Faster convergence, smooths updates | Still less adaptive
    than Adam | Low |'
  id: totrans-3345
  prefs: []
  type: TYPE_TB
  zh: '| SGD + 动量 | 收敛速度更快，平滑更新 | 仍然不如Adam自适应 | 低 |'
- en: '| Adam | Adapts learning rates per parameter | Can overfit small datasets |
    Medium |'
  id: totrans-3346
  prefs: []
  type: TYPE_TB
  zh: '| Adam | 逐参数调整学习率 | 可能会过拟合小数据集 | 中等 |'
- en: '| AdamW | Same as Adam + correct weight decay | More complex | Medium |'
  id: totrans-3347
  prefs: []
  type: TYPE_TB
  zh: '| AdamW | 与Adam + 正确的权重衰减相同 | 更复杂 | 中等 |'
- en: '| Adagrad/RMSProp | Good for sparse features | Not always stable for transformers
    | Medium |'
  id: totrans-3348
  prefs: []
  type: TYPE_TB
  zh: '| Adagrad/RMSProp | 适用于稀疏特征 | 对于转换器来说不一定稳定 | 中等 |'
- en: In *llm.c*, each optimizer is just a loop over parameters with a few math operations.
    That makes it the perfect playground to see how different optimizers actually
    behave in practice.
  id: totrans-3349
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *llm.c* 中，每个优化器只是一个遍历参数的循环，其中包含一些数学运算。这使得它成为观察不同优化器在实际中如何表现的最佳游乐场。
- en: Why It Matters
  id: totrans-3350
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: Optimizers control how your model learns. While architectures like GPT-2 get
    a lot of attention, small changes in optimization can make the difference between
    a model that converges smoothly and one that diverges. By adding your own optimizers,
    you get a clearer understanding of this often “black box” part of deep learning.
  id: totrans-3351
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器控制你的模型如何学习。虽然像GPT-2这样的架构得到了很多关注，但优化中的小变化可能会在收敛平稳的模型和发散的模型之间产生差异。通过添加你自己的优化器，你可以更清楚地理解深度学习中的这个经常是“黑箱”的部分。
- en: Try It Yourself
  id: totrans-3352
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 亲自尝试
- en: Implement the SGD with momentum function shown above and swap it into the training
    loop instead of AdamW.
  id: totrans-3353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现上面显示的带有动量的SGD函数，并将其替换到训练循环中而不是AdamW。
- en: Run training on Tiny Shakespeare and compare how many steps it takes for the
    loss to reach 2.0.
  id: totrans-3354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Tiny Shakespeare上运行训练，比较损失达到2.0所需的步数。
- en: Modify the code to implement RMSProp (similar to Adam, but without momentum
    on the first moment).
  id: totrans-3355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改代码以实现RMSProp（类似于Adam，但第一动量没有使用）。
- en: 'Benchmark memory usage: notice how AdamW allocates both `m_memory` and `v_memory`,
    while SGD only uses one.'
  id: totrans-3356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基准内存使用：注意AdamW同时分配了`m_memory`和`v_memory`，而SGD只使用一个。
- en: Try running AdamW with a very small dataset versus SGD — does one overfit faster?
  id: totrans-3357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试使用非常小的数据集运行AdamW与SGD——哪一个更快地过拟合？
- en: The Takeaway
  id: totrans-3358
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸收要点
- en: Optimizers are just math on arrays. By writing and testing new ones inside *llm.c*,
    you’ll demystify how learning actually happens at the parameter level. This makes
    it easier to appreciate why AdamW became the default for transformers, and also
    gives you the tools to explore alternatives in a clean, transparent environment.
  id: totrans-3359
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器只是数组上的数学。通过在*llm.c*中编写和测试新的优化器，你会揭开学习在参数级别实际发生的神秘面纱。这使得更容易理解为什么AdamW成为transformers的默认选择，同时也为你提供了一个干净、透明环境中的探索替代方案的工具。
- en: 84\. Adding a New Scheduler (cosine, step, etc.)
  id: totrans-3360
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 84. 添加新的调度器（余弦、步长等）
- en: Training isn’t only about choosing an optimizer; the way you adjust the learning
    rate over time is just as important. A scheduler tells the optimizer *how fast
    to learn* at each step. Without a scheduler, you’d use a fixed learning rate,
    which often works poorly for large models. In *llm.c*, schedulers are kept intentionally
    simple so you can see exactly how they influence training.
  id: totrans-3361
  prefs: []
  type: TYPE_NORMAL
  zh: 训练不仅仅是选择一个优化器；你调整学习率的方式在时间上同样重要。调度器告诉优化器在每一步*如何快速学习*。没有调度器，你会使用一个固定的学习率，这对于大型模型通常效果不佳。在*llm.c*中，调度器被有意保持简单，这样你可以清楚地看到它们如何影响训练。
- en: Where Schedulers Fit
  id: totrans-3362
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 调度器的适用位置
- en: 'If you look at the training loop, every step calls the optimizer like this:'
  id: totrans-3363
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看训练循环，每一步都会调用优化器如下：
- en: '[PRE170]'
  id: totrans-3364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE170]'
- en: The `lr` here doesn’t have to be constant. Instead, a scheduler function can
    compute it based on the step number. In *llm.c*, this logic lives in `schedulers.h`
    and related helpers.
  id: totrans-3365
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的`lr`不必是常数。相反，调度器函数可以根据步数计算它。在*llm.c*中，这个逻辑位于`schedulers.h`和相关辅助函数中。
- en: Common Schedulers You Can Add
  id: totrans-3366
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 你可以添加的常见调度器
- en: Step Decay Reduce the learning rate by a fixed factor every N steps.
  id: totrans-3367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 步长衰减：每N步减少一个固定的因子。
- en: '[PRE171]'
  id: totrans-3368
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE171]'
- en: Cosine Decay Smoothly decrease the learning rate following a cosine curve.
  id: totrans-3369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 余弦衰减：平滑地按照余弦曲线降低学习率。
- en: '[PRE172]'
  id: totrans-3370
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE172]'
- en: Linear Warmup + Cosine Decay Start with a gradual increase (warmup) to avoid
    instability, then switch to cosine decay. This is the most common choice for transformers.
  id: totrans-3371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 线性预热 + 余弦衰减：从逐渐增加（预热）开始，以避免不稳定性，然后切换到余弦衰减。这是transformers中最常见的选择。
- en: 'Example: Cosine with Warmup'
  id: totrans-3372
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例：带有预热的余弦函数
- en: 'Here’s how you might implement cosine with warmup in *llm.c*:'
  id: totrans-3373
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何在*llm.c*中实现带有预热的余弦函数的示例：
- en: '[PRE173]'
  id: totrans-3374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE173]'
- en: 'This means:'
  id: totrans-3375
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着：
- en: 'Steps 0–`warmup_steps`: linearly scale from 0 → `base_lr`.'
  id: totrans-3376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步骤0–`warmup_steps`：从0线性缩放到`base_lr`。
- en: 'After warmup: smoothly decay the learning rate using cosine.'
  id: totrans-3377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 热身之后：使用余弦函数平滑地衰减学习率。
- en: Why It Matters
  id: totrans-3378
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: Schedulers help stabilize training. At the beginning, gradients can be very
    noisy, so warming up slowly prevents divergence. At the end, lowering the learning
    rate helps the model converge instead of bouncing around minima. Without schedulers,
    you’d often need more tuning to get the same results.
  id: totrans-3379
  prefs: []
  type: TYPE_NORMAL
  zh: 调度器有助于稳定训练。在开始时，梯度可能非常嘈杂，因此缓慢预热可以防止发散。在结束时，降低学习率有助于模型收敛而不是在最小值周围弹跳。没有调度器，你通常需要更多的调整才能得到相同的结果。
- en: Try It Yourself
  id: totrans-3380
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 亲自尝试
- en: Train with a constant learning rate on Tiny Shakespeare and record the loss
    curve.
  id: totrans-3381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Tiny Shakespeare上以恒定学习率进行训练，并记录损失曲线。
- en: Switch to a step decay scheduler and see if convergence improves.
  id: totrans-3382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 切换到步长衰减调度器，看看是否可以提高收敛性。
- en: Implement cosine decay with warmup and compare against constant LR — which reaches
    a lower validation loss?
  id: totrans-3383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现带有预热的余弦衰减并与恒定学习率（LR）进行比较——哪个达到更低的验证损失？
- en: Experiment with different warmup lengths (e.g., 10 steps vs 100 steps) and watch
    how training stability changes.
  id: totrans-3384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试使用不同的预热长度（例如，10步与100步）并观察训练稳定性如何变化。
- en: Try running the same experiment on TinyStories and see if dataset size affects
    which scheduler works best.
  id: totrans-3385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试在TinyStories上运行相同的实验，看看数据集大小是否会影响哪个调度器效果最好。
- en: The Takeaway
  id: totrans-3386
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 总结
- en: Schedulers are small pieces of code with big impact. They don’t change the model
    or the optimizer, but they control the *pace of learning*. By adding new schedulers
    to *llm.c*, you get a hands-on way to see why modern training recipes almost always
    combine warmup with a smooth decay schedule.
  id: totrans-3387
  prefs: []
  type: TYPE_NORMAL
  zh: 调度器是具有重大影响的小段代码。它们不会改变模型或优化器，但它们控制着学习的*节奏*。通过向*llm.c*添加新的调度器，你可以亲身体验为什么现代训练配方几乎总是将预热与平滑衰减调度相结合。
- en: 85\. Alternative Attention Mechanisms
  id: totrans-3388
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 85. 替代注意力机制
- en: Transformers became famous because of the self-attention mechanism, but “attention”
    is not a single fixed formula. Researchers have explored many alternatives that
    trade off memory use, speed, and accuracy. In *llm.c*, the default is scaled dot-product
    attention, but nothing prevents you from experimenting with new approaches.
  id: totrans-3389
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer因其自注意力机制而闻名，但“注意力”并非一个单一的固定公式。研究人员已经探索了许多权衡内存使用、速度和准确性的替代方案。在*llm.c*中，默认使用缩放点积注意力，但没有任何东西阻止你尝试新的方法。
- en: 'The Default: Scaled Dot-Product Attention'
  id: totrans-3390
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 默认：缩放点积注意力
- en: 'The standard attention formula looks like this:'
  id: totrans-3391
  prefs: []
  type: TYPE_NORMAL
  zh: 标准的注意力公式看起来是这样的：
- en: <semantics><mrow><mtext mathvariant="normal">Attention</mtext><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>Q</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi>V</mi><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>=</mo><mtext mathvariant="normal">softmax</mtext><mrow><mo
    stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo
    stretchy="true" form="postfix">)</mo></mrow><mi>V</mi></mrow> <annotation encoding="application/x-tex">\text{Attention}(Q,
    K, V) = \text{softmax}\!\left(\frac{QK^T}{\sqrt{d_k}}\right) V</annotation></semantics>
  id: totrans-3392
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><mtext mathvariant="normal">Attention</mtext><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>Q</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi>V</mi><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>=</mo><mtext mathvariant="normal">softmax</mtext><mrow><mo
    stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo
    stretchy="true" form="postfix">)</mo></mrow><mi>V</mi></mrow> <annotation encoding="application/x-tex">\text{Attention}(Q,
    K, V) = \text{softmax}\!\left(\frac{QK^T}{\sqrt{d_k}}\right) V</annotation></semantics>
- en: Q = queries
  id: totrans-3393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Q = 查询
- en: K = keys
  id: totrans-3394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K = 关键
- en: V = values
  id: totrans-3395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: V = 值
- en: <semantics><msub><mi>d</mi><mi>k</mi></msub><annotation encoding="application/x-tex">d_k</annotation></semantics>
    = key dimension (used for scaling)
  id: totrans-3396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <semantics><msub><mi>d</mi><mi>k</mi></msub><annotation encoding="application/x-tex">d_k</annotation></semantics>
    = 键维度（用于缩放）
- en: In *llm.c*, this is implemented using matrix multiplications and masking to
    enforce causality. It’s correct and faithful to GPT-2, but has quadratic cost
    in sequence length <semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics>.
  id: totrans-3397
  prefs: []
  type: TYPE_NORMAL
  zh: 在*llm.c*中，这是通过矩阵乘法和掩码来实现的，以强制执行因果性。它是正确且忠实于GPT-2的，但在序列长度<semantics><mi>T</mi><annotation
    encoding="application/x-tex">T</annotation></semantics>上有二次成本。
- en: Variants You Could Add
  id: totrans-3398
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 你可以添加的变体
- en: Sparse Attention Instead of attending to every token, restrict attention to
    a local window or a set of important positions.
  id: totrans-3399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 稀疏注意力而不是关注每个标记，将注意力限制在局部窗口或一组重要位置。
- en: Good for long sequences.
  id: totrans-3400
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适合长序列。
- en: Saves compute and memory.
  id: totrans-3401
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节省计算和内存。
- en: 'Example: “sliding window” attention where each token only looks back 128 steps.'
  id: totrans-3402
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例：“滑动窗口”注意力，其中每个标记只回顾128步。
- en: Linformer / Low-Rank Attention Approximate <semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation
    encoding="application/x-tex">QK^T</annotation></semantics> using low-rank projections.
  id: totrans-3403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Linformer / 低秩注意力近似 <semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation
    encoding="application/x-tex">QK^T</annotation></semantics> 使用低秩投影。
- en: Reduces memory from <semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>T</mi><mn>2</mn></msup><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(T^2)</annotation></semantics>
    to <semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>T</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(T)</annotation></semantics>.
  id: totrans-3404
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将内存从<semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>T</mi><mn>2</mn></msup><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(T^2)</annotation></semantics>减少到<semantics><mrow><mi>O</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>T</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">O(T)</annotation></semantics>。
- en: Works well when redundancy exists in sequences.
  id: totrans-3405
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当序列中存在冗余时表现良好。
- en: Performer (Linear Attention) Replace the softmax with kernel approximations
    so attention becomes linear in sequence length.
  id: totrans-3406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用线性注意力（Performer）用核近似替换softmax，使注意力在序列长度上呈线性。
- en: Trades exactness for scalability.
  id: totrans-3407
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以精确性换取可扩展性。
- en: Allows much longer sequences on the same hardware.
  id: totrans-3408
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在相同硬件上允许更长的序列。
- en: ALiBi (Attention with Linear Biases) Adds simple position-dependent biases instead
    of full positional embeddings.
  id: totrans-3409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ALiBi（带线性偏置的注意力）添加简单的位置相关偏置而不是完整的位置嵌入。
- en: Extremely efficient.
  id: totrans-3410
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 极其高效。
- en: Helps extrapolate to longer sequences than seen in training.
  id: totrans-3411
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有助于外推到比训练中看到的更长的序列。
- en: How to Experiment in *llm.c*
  id: totrans-3412
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在*llm.c*中如何进行实验
- en: 'The attention implementation lives inside the `attention_forward` and `attention_backward`
    routines in `train_gpt2.c` (and their CUDA equivalents). To try an alternative:'
  id: totrans-3413
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力实现位于`train_gpt2.c`中的`attention_forward`和`attention_backward`例程（以及它们的CUDA等效例程）。要尝试替代方案：
- en: Replace the part where <semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation
    encoding="application/x-tex">QK^T</annotation></semantics> is computed with your
    chosen method.
  id: totrans-3414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将计算<semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation
    encoding="application/x-tex">QK^T</annotation></semantics>的部分替换为你选择的方法。
- en: 'Keep the interface the same: given inputs Q, K, V, return outputs shaped `(B,
    T, C)`.'
  id: totrans-3415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保持接口不变：给定输入Q，K，V，返回形状为`(B, T, C)`的输出。
- en: Run unit tests (`test_gpt2.c`) against the baseline to make sure the outputs
    stay reasonable.
  id: totrans-3416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对基准进行单元测试（`test_gpt2.c`），以确保输出保持合理。
- en: Why It Matters
  id: totrans-3417
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: Attention is often the bottleneck in transformers. Quadratic time and memory
    usage limit how long your sequences can be. By experimenting with alternatives,
    you not only improve efficiency but also learn how new research ideas are implemented
    in practice. Many of today’s “efficient transformers” came from simple tweaks
    to this block.
  id: totrans-3418
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力通常是transformers的瓶颈。二次时间内存使用限制了你序列的长度。通过实验替代方案，你不仅可以提高效率，还可以了解新的研究想法是如何在实践中实现的。今天许多“高效的transformers”都源于对这个块的简单调整。
- en: Try It Yourself
  id: totrans-3419
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 尝试自己操作
- en: Modify attention so each token only attends to the last 16 tokens (a toy form
    of sparse attention). Train on Tiny Shakespeare and compare speed vs. accuracy.
  id: totrans-3420
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改注意力，使每个标记只关注最后16个标记（稀疏注意力的玩具形式）。在Tiny Shakespeare上训练，比较速度与准确率。
- en: Implement ALiBi by adding linear position-dependent bias terms and see if your
    model generalizes better to longer text.
  id: totrans-3421
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过添加线性位置相关偏置项实现ALiBi，看看你的模型是否更好地泛化到更长的文本。
- en: Benchmark the GPU memory footprint of standard attention vs. your custom version
    using Nsight Systems or `nvidia-smi`.
  id: totrans-3422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Nsight Systems或`nvidia-smi`比较标准注意力与你的自定义版本的GPU内存占用。
- en: Try removing the scaling factor <semantics><mrow><mn>1</mn><mi>/</mi><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mrow><annotation
    encoding="application/x-tex">1/\sqrt{d_k}</annotation></semantics> — does training
    become unstable?
  id: totrans-3423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试移除缩放因子<semantics><mrow><mn>1</mn><mi>/</mi><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mrow><annotation
    encoding="application/x-tex">1/\sqrt{d_k}</annotation></semantics>——训练是否变得不稳定？
- en: 'Replace softmax with a simple ReLU and see how the model behaves (hint: it
    usually diverges, but it teaches why softmax is important).'
  id: totrans-3424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将softmax替换为简单的ReLU，看看模型的表现如何（提示：通常发散，但它说明了softmax的重要性）。
- en: The Takeaway
  id: totrans-3425
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸收要点
- en: The attention block is where much of the magic happens in transformers, but
    it’s also the biggest bottleneck. By experimenting with alternatives in *llm.c*,
    you’ll gain a deeper understanding of why the standard formula works, what its
    weaknesses are, and how new ideas from research can be tested directly in code.
  id: totrans-3426
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力块是transformers中发生许多魔法的地方，但也是最大的瓶颈。通过在*llm.c*中实验替代方案，你将更深入地了解标准公式的原理，它的弱点，以及如何直接在代码中测试研究中的新想法。
- en: 86\. Profiling and Testing New Kernels
  id: totrans-3427
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 86. 分析和测试新内核
- en: 'When you start adding custom CUDA kernels or experimenting with new attention
    mechanisms, the next big question is: how do you know if they’re correct and efficient?
    That’s where profiling and testing come in. *llm.c* keeps this process minimal
    but transparent so you can see exactly how to validate both correctness and performance.'
  id: totrans-3428
  prefs: []
  type: TYPE_NORMAL
  zh: 当您开始添加自定义CUDA内核或尝试新的注意力机制时，下一个重大问题是：您如何知道它们是否正确且高效？这正是分析和测试的用武之地。*llm.c*使此过程最小化但透明，这样您可以确切地了解如何验证正确性和性能。
- en: 'Correctness First: Testing Against a Reference'
  id: totrans-3429
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 正确性优先：与参考进行测试
- en: 'Any new kernel you write should be compared against a known-good implementation.
    In *llm.c*, PyTorch usually serves as the “oracle.” For example:'
  id: totrans-3430
  prefs: []
  type: TYPE_NORMAL
  zh: 您编写的任何新内核都应该与已知良好的实现进行比较。在*llm.c*中，PyTorch通常作为“神谕”。例如：
- en: Generate random input tensors in both *llm.c* and PyTorch.
  id: totrans-3431
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在*llm.c*和PyTorch中生成随机的输入张量。
- en: Run your custom kernel in *llm.c*.
  id: totrans-3432
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在*llm.c*中运行您的自定义内核。
- en: Run the equivalent operation in PyTorch.
  id: totrans-3433
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在PyTorch中运行等效操作。
- en: Compare the outputs within a small tolerance (e.g., differences less than `1e-5`).
  id: totrans-3434
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在小范围内比较输出（例如，差异小于`1e-5`）。
- en: This ensures your kernel doesn’t silently compute the wrong thing. Without this
    step, you might train for hours before realizing your model is learning nonsense.
  id: totrans-3435
  prefs: []
  type: TYPE_NORMAL
  zh: 这确保了您的内核不会默默计算错误的结果。没有这一步，您可能训练数小时后才会意识到您的模型正在学习无意义的内容。
- en: 'Performance: Profiling the Kernels'
  id: totrans-3436
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 性能：内核分析
- en: 'Once correctness is established, the next step is performance. NVIDIA provides
    several tools:'
  id: totrans-3437
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦确立了正确性，下一步就是性能。NVIDIA提供了几个工具：
- en: 'nvprof (older): still widely used, easy to launch.'
  id: totrans-3438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: nvprof（较旧）：仍然广泛使用，易于启动。
- en: 'Nsight Systems / Nsight Compute (modern): more detailed, lets you see kernel
    timings, memory transfers, occupancy, and more.'
  id: totrans-3439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nsight Systems / Nsight Compute（现代）：更详细，让您可以看到内核计时、内存传输、占用率等更多信息。
- en: 'In practice:'
  id: totrans-3440
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上：
- en: Run your training loop with profiling enabled.
  id: totrans-3441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在启用分析的情况下运行您的训练循环。
- en: Identify which kernels take the most time.
  id: totrans-3442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别占用时间最多的内核。
- en: Check if your custom kernel is faster than the baseline (e.g., cuBLAS or cuDNN).
  id: totrans-3443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查您的自定义内核是否比基线（例如，cuBLAS或cuDNN）更快。
- en: Common Metrics to Watch
  id: totrans-3444
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 常见指标
- en: Kernel time (how long each launch takes).
  id: totrans-3445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内核时间（每次启动需要多长时间）。
- en: Occupancy (how many CUDA cores are active relative to maximum).
  id: totrans-3446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 占用率（相对于最大值有多少CUDA核心是活跃的）。
- en: Memory throughput (are you saturating memory bandwidth?).
  id: totrans-3447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存吞吐量（您是否饱和了内存带宽？）。
- en: Launch count (do you call your kernel too many times instead of fusing operations?).
  id: totrans-3448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启动计数（你是否调用内核次数过多而不是融合操作？）。
- en: Even a correct kernel can be slower than a library implementation if it doesn’t
    use the GPU efficiently.
  id: totrans-3449
  prefs: []
  type: TYPE_NORMAL
  zh: 即使内核是正确的，如果它没有高效地使用GPU，它也可能比库实现慢。
- en: Example Workflow
  id: totrans-3450
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例工作流程
- en: 'Suppose you write a fused bias + ReLU kernel. You can test it like this:'
  id: totrans-3451
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您编写了一个融合偏差 + ReLU内核。您可以像这样测试它：
- en: Generate a random tensor in C and in PyTorch.
  id: totrans-3452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在C和PyTorch中生成一个随机的张量。
- en: Apply your fused kernel vs. PyTorch’s separate `+` and `ReLU` ops.
  id: totrans-3453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将您的融合内核与PyTorch的单独`+`和`ReLU`操作进行比较。
- en: Compare results for correctness.
  id: totrans-3454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较结果的正确性。
- en: 'Profile both approaches: is your kernel faster? Did it reduce kernel launch
    overhead?'
  id: totrans-3455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析两种方法：您的内核是否更快？它是否减少了内核启动开销？
- en: Why It Matters
  id: totrans-3456
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: Custom kernels are fun to write, but without testing and profiling they’re just
    guesswork. Many research ideas look promising in theory but fall apart in practice
    because they run slower or break correctness. By learning to systematically test
    and profile, you can separate ideas that are genuinely useful from those that
    are just experiments.
  id: totrans-3457
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义内核编写起来很有趣，但没有测试和分析，它们只是猜测。许多研究想法在理论上看起来很有希望，但在实践中却失败了，因为它们运行得更慢或破坏了正确性。通过学习系统地测试和分析，您可以区分真正有用的想法和仅仅是实验的想法。
- en: Try It Yourself
  id: totrans-3458
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 尝试自己动手做
- en: Write a simple fused kernel for bias + ReLU. Compare it against PyTorch’s `x
    + bias` followed by `relu(x)`.
  id: totrans-3459
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个简单的融合内核用于偏差 + ReLU。将其与PyTorch的`x + bias`后跟`relu(x)`进行比较。
- en: Use `nvprof` to check how many kernel launches happen in each version.
  id: totrans-3460
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`nvprof`检查每个版本中发生多少内核启动。
- en: 'Run Nsight Systems and look at the timeline: do you see your fused kernel overlapping
    better with other GPU activity?'
  id: totrans-3461
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行Nsight Systems并查看时间线：您是否看到您的融合内核与其他GPU活动重叠得更好？
- en: Try scaling sequence length `T` to very large values — does your kernel still
    perform well?
  id: totrans-3462
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试将序列长度`T`扩展到非常大的值——您的内核是否仍然表现良好？
- en: Record memory usage before and after your kernel runs. Is there a difference
    compared to the unfused version?
  id: totrans-3463
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在内核运行前后记录内存使用情况。与未融合版本相比，是否有差异？
- en: The Takeaway
  id: totrans-3464
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸收要点
- en: Profiling and testing turn kernel hacking from random tinkering into real engineering.
    With a reference for correctness and tools for performance measurement, you can
    iterate confidently, knowing when your changes are truly improvements. This is
    how *llm.c* bridges the gap between a learning project and real GPU systems work.
  id: totrans-3465
  prefs: []
  type: TYPE_NORMAL
  zh: 性能分析和测试将内核黑客从随机的尝试变成真正的工程。有了正确性的参考和性能测量的工具，你可以自信地迭代，知道你的更改是否真正是改进。这就是 *llm.c*
    如何在学习项目和真正的 GPU 系统工作之间架起桥梁。
- en: 87\. Using PyTorch Reference as Oracle
  id: totrans-3466
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 87. 使用 PyTorch 参考作为 Oracle
- en: One of the guiding principles of *llm.c* is to stay small, readable, and minimal
    — but that doesn’t mean you’re left without a safety net. When implementing something
    as mathematically dense as a transformer, how do you know your C or CUDA code
    is doing the right thing? The answer is to use PyTorch as a reference implementation,
    often called an “oracle.”
  id: totrans-3467
  prefs: []
  type: TYPE_NORMAL
  zh: '*llm.c* 的一个指导原则是保持小巧、可读和最小化——但这并不意味着你没有任何安全网。当你实现像变压器这样数学密集型的东西时，你怎么知道你的 C
    或 CUDA 代码正在做正确的事情？答案是使用 PyTorch 作为参考实现，通常称为“Oracle”。'
- en: What Does “Oracle” Mean Here?
  id: totrans-3468
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 这里的“Oracle”是什么意思？
- en: 'An oracle is simply a trusted system you compare against. PyTorch is trusted
    because:'
  id: totrans-3469
  prefs: []
  type: TYPE_NORMAL
  zh: Oracle 简单来说是一个你可以与之比较的信任系统。PyTorch 被信任是因为：
- en: It’s widely used in production and research.
  id: totrans-3470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它在生产和研究中被广泛使用。
- en: Its operators (matrix multiply, attention, layernorm, etc.) have been thoroughly
    tested.
  id: totrans-3471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它的操作符（矩阵乘法、注意力、layernorm 等）已经过彻底测试。
- en: It gives you both CPU and GPU implementations with stable numerical behavior.
  id: totrans-3472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它为你提供具有稳定数值行为的 CPU 和 GPU 实现。
- en: If your *llm.c* forward or backward pass matches PyTorch’s within a small error
    tolerance, you can be confident that your implementation is correct.
  id: totrans-3473
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的 *llm.c* 前向或反向传递在小的误差容忍度内与 PyTorch 匹配，你可以有信心你的实现是正确的。
- en: How the Comparison Works
  id: totrans-3474
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 比较的工作方式
- en: 'The workflow usually looks like this:'
  id: totrans-3475
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流程通常如下所示：
- en: Set up the same model in both PyTorch and *llm.c* with identical weights.
  id: totrans-3476
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 PyTorch 和 *llm.c* 中使用相同的权重设置相同的模型。
- en: Feed the same inputs to both models.
  id: totrans-3477
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将相同的输入提供给两个模型。
- en: Compare outputs — logits, losses, or gradients.
  id: totrans-3478
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较输出——logits、损失或梯度。
- en: Allow for tiny differences due to floating point arithmetic, usually within
    `1e-5` to `1e-6`.
  id: totrans-3479
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 允许由于浮点运算而导致的微小差异，通常在 `1e-5` 到 `1e-6` 之间。
- en: For example, `test_gpt2.c` in the repository runs a forward pass in C and compares
    the logits to those produced by a PyTorch GPT-2 checkpoint.
  id: totrans-3480
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，存储库中的 `test_gpt2.c` 运行 C 中的前向传递并比较 logits 与 PyTorch GPT-2 检查点产生的 logits。
- en: Example
  id: totrans-3481
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例
- en: 'Suppose you’re testing the embedding lookup. In PyTorch you might write:'
  id: totrans-3482
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在测试嵌入查找。在 PyTorch 中，你可能编写：
- en: '[PRE174]'
  id: totrans-3483
  prefs: []
  type: TYPE_PRE
  zh: '[PRE174]'
- en: In *llm.c*, you’d run the same two tokens through the embedding lookup and compare
    the resulting vectors element by element. If they match, your embedding implementation
    is correct.
  id: totrans-3484
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *llm.c* 中，你会运行相同的两个标记通过嵌入查找并逐元素比较结果向量。如果它们匹配，你的嵌入实现是正确的。
- en: Why PyTorch is the Perfect Oracle
  id: totrans-3485
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么 PyTorch 是完美的 Oracle
- en: 'Transparency: it’s easy to extract weights from PyTorch checkpoints (`.bin`
    files).'
  id: totrans-3486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 透明度：从 PyTorch 检查点（`.bin` 文件）中提取权重很容易。
- en: 'Flexibility: you can test individual layers, not just the whole model.'
  id: totrans-3487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 灵活性：你可以测试单个层，而不仅仅是整个模型。
- en: 'Debuggability: if something goes wrong, you can isolate which layer diverges
    first.'
  id: totrans-3488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可调试性：如果出现问题，你可以隔离哪个层首先发散。
- en: This last point is crucial — instead of training for days only to find your
    loss curve diverges, you can catch mismatches immediately at the layer level.
  id: totrans-3489
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一点至关重要——而不是训练数天只发现你的损失曲线发散，你可以在层级别立即捕捉到不匹配。
- en: Why It Matters
  id: totrans-3490
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: Deep learning models are fragile. Even a tiny mistake in normalization, masking,
    or gradient flow can ruin training. By anchoring your work to PyTorch, you avoid
    “trusting your gut” and instead rely on a battle-tested baseline. This practice
    isn’t unique to *llm.c* — many professional frameworks (Megatron-LM, DeepSpeed)
    also validate against PyTorch during development.
  id: totrans-3491
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型很脆弱。即使在归一化、掩码或梯度流中的微小错误也可能破坏训练。通过将你的工作锚定在 PyTorch 上，你避免了“相信直觉”并转而依赖经过实战检验的基线。这种做法并不仅限于
    *llm.c* ——许多专业框架（Megatron-LM、DeepSpeed）在开发期间也会与 PyTorch 进行验证。
- en: Try It Yourself
  id: totrans-3492
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 亲自尝试
- en: Extract GPT-2 weights from Hugging Face Transformers and load them into *llm.c*.
  id: totrans-3493
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 Hugging Face Transformers 中提取 GPT-2 权重并将它们加载到 *llm.c* 中。
- en: Run a forward pass in both PyTorch and *llm.c* with the same input tokens. Compare
    outputs numerically.
  id: totrans-3494
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 PyTorch 和 *llm.c* 中使用相同的输入标记进行前向传递。从数值上比较输出。
- en: 'Focus on a single block: check whether the attention output matches PyTorch’s.'
  id: totrans-3495
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 专注于单个块：检查注意力输出是否与PyTorch的匹配。
- en: Modify a kernel (e.g., change softmax to ReLU) and watch how quickly the outputs
    diverge from PyTorch’s.
  id: totrans-3496
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改一个内核（例如，将softmax改为ReLU）并观察输出如何迅速偏离PyTorch的。
- en: Use PyTorch to verify gradients by calling `.backward()` and comparing with
    `gpt2_backward` in *llm.c*.
  id: totrans-3497
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用PyTorch通过调用`.backward()`来验证梯度，并与*llm.c*中的`gpt2_backward`进行比较。
- en: The Takeaway
  id: totrans-3498
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸收的要点
- en: PyTorch is your map and compass when navigating the dense jungle of transformer
    internals. By treating it as an oracle, you can move confidently from one layer
    to the next, knowing that your small, hand-written code matches the behavior of
    a full-featured deep learning framework. This practice turns *llm.c* into more
    than a toy project — it becomes a faithful, verifiable reimplementation of GPT-2.
  id: totrans-3499
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch是你在导航Transformer内部密集丛林时的指南针和罗盘。将其视为一个先知，你可以自信地从一层移动到下一层，知道你手写的少量代码与一个功能齐全的深度学习框架的行为相匹配。这种做法使*llm.c*不仅仅是一个玩具项目——它成为了一个忠实、可验证的GPT-2重实现。
- en: '88\. Exploring Beyond GPT-2: LLaMA Example'
  id: totrans-3500
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 88. 探索GPT-2之外的领域：LLaMA示例
- en: While *llm.c* focuses on GPT-2 for clarity, the same framework can extend to
    newer, larger, and more modern models such as LLaMA. LLaMA, released by Meta,
    uses many of the same building blocks as GPT-2 — embeddings, attention layers,
    MLPs, normalization, and residual streams — but with tweaks that improve efficiency
    and scaling. Looking at LLaMA through the lens of *llm.c* helps you see how language
    model designs evolve while still sharing the same DNA.
  id: totrans-3501
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然*llm.c*专注于GPT-2以提高清晰度，但相同的框架可以扩展到更新、更大、更现代的模型，如LLaMA。Meta发布的LLaMA使用了许多与GPT-2相同的构建块——嵌入、注意力层、MLP、归一化和残差流——但通过调整提高了效率和扩展性。通过*llm.c*的视角来看LLaMA，可以帮助你看到语言模型设计是如何演变的同时仍然保持相同的基因。
- en: What Stays the Same
  id: totrans-3502
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 保持不变的部分
- en: 'Token embeddings: both GPT-2 and LLaMA use lookup tables to turn token IDs
    into dense vectors.'
  id: totrans-3503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标记嵌入：GPT-2和LLaMA都使用查找表将标记ID转换为密集向量。
- en: 'Transformer blocks: the fundamental loop of attention → MLP → residuals is
    unchanged.'
  id: totrans-3504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer块：注意力→MLP→残差的基本循环保持不变。
- en: 'Autoregressive training: predict the next token given all previous ones, using
    causal masking.'
  id: totrans-3505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自回归训练：给定所有前面的标记预测下一个标记，使用因果掩码。
- en: This means much of the code in *llm.c* — dataloaders, embeddings, forward loops
    — would work with LLaMA almost unchanged.
  id: totrans-3506
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着*llm.c*中的大部分代码——数据加载器、嵌入、前向循环——几乎不需要修改就可以与LLaMA一起工作。
- en: What’s Different
  id: totrans-3507
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 不同之处
- en: Normalization
  id: totrans-3508
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 归一化
- en: GPT-2 uses LayerNorm before each block output.
  id: totrans-3509
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-2在每个块输出前使用LayerNorm。
- en: LLaMA uses RMSNorm, which normalizes using only the root mean square of activations
    (no mean subtraction).
  id: totrans-3510
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLaMA使用RMSNorm，它仅使用激活的根均方值进行归一化（没有均值减法）。
- en: This reduces compute slightly and improves stability.
  id: totrans-3511
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这略微减少了计算量并提高了稳定性。
- en: Positional Encoding
  id: totrans-3512
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 位置编码
- en: GPT-2 has learned positional embeddings.
  id: totrans-3513
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-2已经学习了位置嵌入。
- en: LLaMA uses Rotary Position Embeddings (RoPE), which rotate queries and keys
    in attention space to encode positions.
  id: totrans-3514
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLaMA使用旋转位置嵌入（RoPE），它在注意力空间中旋转查询和键来编码位置。
- en: RoPE scales better to longer contexts.
  id: totrans-3515
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: RoPE在更长的上下文中表现更好。
- en: Vocabulary
  id: totrans-3516
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 词汇量
- en: GPT-2’s vocab size is 50,257.
  id: totrans-3517
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-2的词汇量大小为50,257。
- en: LLaMA uses a different tokenizer (SentencePiece/BPE) with a larger vocabulary,
    closer to 32k for LLaMA-2.
  id: totrans-3518
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLaMA使用不同的分词器（SentencePiece/BPE），拥有更大的词汇量，接近32k的LLaMA-2。
- en: Model Scale
  id: totrans-3519
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型规模
- en: GPT-2 tops out at 1.6B parameters.
  id: totrans-3520
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-2的最大参数量为1.6B。
- en: LLaMA-2 and LLaMA-3 scale from 7B up to 70B+. This makes distributed training
    mandatory, with mixed precision and checkpointing as standard.
  id: totrans-3521
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLaMA-2和LLaMA-3的规模从70B+扩展到70B以上。这使得分布式训练成为强制性的，混合精度和检查点作为标准。
- en: Adapting *llm.c* to LLaMA
  id: totrans-3522
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将*llm.c*适配到LLaMA
- en: 'If you wanted to modify *llm.c* to approximate LLaMA, the main tasks would
    be:'
  id: totrans-3523
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要修改*llm.c*来近似LLaMA，主要任务将是：
- en: Replace LayerNorm with an RMSNorm implementation.
  id: totrans-3524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将LayerNorm替换为RMSNorm实现。
- en: Add RoPE into the attention mechanism. This means modifying the step where Q
    and K vectors are built, applying a rotation based on token positions.
  id: totrans-3525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将RoPE添加到注意力机制中。这意味着修改构建Q和K向量的步骤，根据标记位置应用旋转。
- en: Swap out the GPT-2 tokenizer with a SentencePiece tokenizer trained on the desired
    vocabulary.
  id: totrans-3526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用在所需词汇量上训练的SentencePiece分词器替换GPT-2分词器。
- en: The rest of the pipeline — optimizer, schedulers, dataloaders, multi-GPU support
    — would remain valid.
  id: totrans-3527
  prefs: []
  type: TYPE_NORMAL
  zh: 管道中的其余部分——优化器、调度器、数据加载器、多GPU支持——仍然有效。
- en: Why It Matters
  id: totrans-3528
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: By studying LLaMA in the context of GPT-2, you see that modern LLMs aren’t completely
    alien. They’re evolutionary improvements on the same transformer backbone. Recognizing
    these small architectural changes (RMSNorm, RoPE, scaling) helps demystify why
    newer models outperform older ones, and it shows you exactly what you’d need to
    tweak in *llm.c* to explore beyond GPT-2.
  id: totrans-3529
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在 GPT-2 的背景下研究 LLaMA，你会发现现代 LLMs 并非完全陌生。它们是在相同的 transformer 背骨上的进化改进。认识到这些小的架构变化（RMSNorm，RoPE，缩放）有助于揭示为什么新模型优于旧模型，并展示了你需要在
    *llm.c* 中进行哪些调整以探索 GPT-2 之外的内容。
- en: Try It Yourself
  id: totrans-3530
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 尝试自己动手做
- en: Implement RMSNorm in C by adapting the LayerNorm code in *llm.c*.
  id: totrans-3531
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 C 中通过修改 *llm.c* 中的 LayerNorm 代码实现 RMSNorm。
- en: Add a simplified version of RoPE to the attention kernel and run it on Tiny
    Shakespeare.
  id: totrans-3532
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在注意力内核中添加 RoPE 的简化版本，并在 Tiny Shakespeare 上运行它。
- en: Swap the GPT-2 tokenizer for a SentencePiece model and train a small LLaMA-like
    model on your own dataset.
  id: totrans-3533
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 GPT-2 的分词器替换为 SentencePiece 模型，并在自己的数据集上训练一个类似 LLaMA 的小型模型。
- en: Compare training stability between LayerNorm and RMSNorm — does the loss curve
    look different?
  id: totrans-3534
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较 LayerNorm 和 RMSNorm 之间的训练稳定性——损失曲线看起来是否不同？
- en: Study the memory use of GPT-2 vs. a small LLaMA-style variant and see how scaling
    behaves.
  id: totrans-3535
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 研究 GPT-2 与小型 LLaMA 风格变体的内存使用情况，并查看缩放行为。
- en: The Takeaway
  id: totrans-3536
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸收要点
- en: Exploring LLaMA through *llm.c* shows how flexible the codebase really is. With
    only a few targeted changes — normalization, positional encoding, tokenizer —
    you can shift from replicating GPT-2 to experimenting with the building blocks
    of modern LLMs. This makes *llm.c* not just a study tool for one model, but a
    foundation for understanding the entire lineage of transformers.
  id: totrans-3537
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 *llm.c* 探索 LLaMA 显示了代码库的真正灵活性。只需进行一些有针对性的更改——归一化、位置编码、分词器——你就可以从复制 GPT-2
    转变为实验现代 LLMs 的构建块。这使得 *llm.c* 不仅仅是一个模型的学习工具，而且是理解整个 transformer 血统的基础。
- en: '89\. Porting Playbook: C → Go/Rust/Metal'
  id: totrans-3538
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 89. 移植指南：C → Go/Rust/Metal
- en: The *llm.c* codebase is written in plain C for maximum readability and minimal
    dependencies. But in practice, many developers want to experiment with other languages
    or platforms — for example, writing a Go or Rust version for better tooling, or
    targeting Apple’s Metal API for GPU acceleration on Macs. Porting is not just
    a copy-paste exercise; it requires careful thinking about how low-level memory,
    math operations, and parallelism map across ecosystems.
  id: totrans-3539
  prefs: []
  type: TYPE_NORMAL
  zh: '*llm.c* 代码库是用纯 C 编写的，以实现最大程度的可读性和最小化依赖。但在实践中，许多开发者想要尝试其他语言或平台——例如，编写 Go 或 Rust
    版本以获得更好的工具支持，或者针对苹果的 Metal API 在 Mac 上实现 GPU 加速。移植不仅仅是复制粘贴的练习；它需要仔细思考如何在不同的生态系统中映射低级内存、数学运算和并行性。'
- en: Why Port at All?
  id: totrans-3540
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么需要移植？
- en: 'Go: strong concurrency model (goroutines, channels), good for building training
    services or distributed experiments.'
  id: totrans-3541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Go：强大的并发模型（goroutines，channels），适合构建训练服务或分布式实验。
- en: 'Rust: memory safety and performance without garbage collection, ideal for writing
    reliable numerical kernels.'
  id: totrans-3542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rust：无需垃圾回收的内存安全和性能，非常适合编写可靠的数值内核。
- en: 'Metal (Apple): GPU acceleration on macOS/iOS, a must if you want to train or
    run models efficiently on Apple Silicon.'
  id: totrans-3543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Metal（苹果）：在 macOS/iOS 上的 GPU 加速，如果你想在苹果硅上高效训练或运行模型，这是必须的。
- en: Each ecosystem has strengths that make *llm.c*’s concepts more approachable
    or more production-ready.
  id: totrans-3544
  prefs: []
  type: TYPE_NORMAL
  zh: 每个生态系统都有其优势，使得 *llm.c* 的概念更容易接近或更适用于生产。
- en: Mapping the Core Components
  id: totrans-3545
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 映射核心组件
- en: 'Let’s look at how the key pieces of *llm.c* translate:'
  id: totrans-3546
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 *llm.c* 的关键部分是如何翻译的：
- en: '| Component | C (llm.c) | Go Equivalent | Rust Equivalent | Metal Equivalent
    |'
  id: totrans-3547
  prefs: []
  type: TYPE_TB
  zh: '| 组件 | C (llm.c) | Go 等价 | Rust 等价 | Metal 等价 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-3548
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Memory allocation | `malloc`, `calloc` | `make`, slices, `unsafe.Pointer`
    | `Vec<T>`, `Box`, `unsafe` if needed | Buffers allocated on GPU |'
  id: totrans-3549
  prefs: []
  type: TYPE_TB
  zh: '| 内存分配 | `malloc`，`calloc` | `make`，切片，`unsafe.Pointer` | `Vec<T>`，`Box`，如果需要
    `unsafe` | 在 GPU 上分配的缓冲区 |'
- en: '| Math kernels | manual loops, OpenMP | loops or cgo bindings to BLAS | loops
    with iterators, Rayon for CPU | Metal compute shaders |'
  id: totrans-3550
  prefs: []
  type: TYPE_TB
  zh: '| 数学内核 | 手动循环，OpenMP | 循环或 cgo 绑定到 BLAS | 带迭代器的循环，Rayon 用于 CPU | Metal 计算着色器
    |'
- en: '| Tokenizer | `fread` binary file | standard file I/O, `encoding/json` | `serde`,
    binary read | Preprocessing on CPU, feed to GPU |'
  id: totrans-3551
  prefs: []
  type: TYPE_TB
  zh: '| 分词器 | `fread` 二进制文件 | 标准文件 I/O，`encoding/json` | `serde`，二进制读取 | 在 CPU 上进行预处理，输入到
    GPU |'
- en: '| Training loop | for-loops, structs | goroutines for dataloader + trainer
    | async tasks, channels | CPU driver, GPU kernels |'
  id: totrans-3552
  prefs: []
  type: TYPE_TB
  zh: '| 训练循环 | for 循环，结构体 | dataloader + trainer 的 goroutines | 异步任务，channels | CPU
    驱动程序，GPU 内核 |'
- en: '| Parallelism | `#pragma omp` | goroutines + sync primitives | Rayon or explicit
    threads | Warp/thread groups in Metal |'
  id: totrans-3553
  prefs: []
  type: TYPE_TB
  zh: '| 并行性 | `#pragma omp` | goroutines + 同步原语 | Rayon或显式线程 | Metal中的Warp/线程组 |'
- en: 'Example: LayerNorm in Rust'
  id: totrans-3554
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例：Rust中的LayerNorm
- en: 'Here’s a small Rust sketch of how a forward pass for LayerNorm might look:'
  id: totrans-3555
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个Rust草图，展示了LayerNorm的前向传播可能的样子：
- en: '[PRE175]'
  id: totrans-3556
  prefs: []
  type: TYPE_PRE
  zh: '[PRE175]'
- en: This looks strikingly similar to the C code, but benefits from Rust’s type safety
    and the absence of manual memory management bugs.
  id: totrans-3557
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来与C代码惊人地相似，但受益于Rust的类型安全和缺乏手动内存管理错误。
- en: 'Example: Attention Kernel in Metal'
  id: totrans-3558
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例：Metal中的注意力内核
- en: 'Metal would handle attention differently — you’d write a compute shader in
    `.metal` language:'
  id: totrans-3559
  prefs: []
  type: TYPE_NORMAL
  zh: Metal会以不同的方式处理注意力——你将使用`.metal`语言编写计算着色器：
- en: '[PRE176]'
  id: totrans-3560
  prefs: []
  type: TYPE_PRE
  zh: '[PRE176]'
- en: This isn’t a line-for-line port, but it shows how the concept — multiply queries
    with keys, apply softmax, weight values — remains the same while the implementation
    moves into GPU-land.
  id: totrans-3561
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是逐行移植，但它展示了概念——将查询与键相乘、应用softmax、加权值——在实现转移到GPU领域时保持不变。
- en: Challenges You’ll Face
  id: totrans-3562
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 你将面临的挑战
- en: 'Numerical libraries: C often leans on BLAS/LAPACK or just hand-written loops.
    In Go and Rust, you’ll either bind to these libraries or reimplement them.'
  id: totrans-3563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数值库：C通常依赖于BLAS/LAPACK或直接编写循环。在Go和Rust中，你将绑定到这些库或重新实现它们。
- en: 'Performance portability: getting code that runs fast both on CPU and GPU isn’t
    trivial. What works in C with OpenMP won’t directly translate.'
  id: totrans-3564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能可移植性：让代码在CPU和GPU上都能快速运行并不简单。在C中使用OpenMP的代码不会直接转换。
- en: 'Tokenizer compatibility: making sure tokenization matches byte-for-byte is
    essential. One mismatch can ruin training reproducibility.'
  id: totrans-3565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分词器兼容性：确保分词与字节对字节匹配是至关重要的。任何不匹配都可能破坏训练的可重复性。
- en: Why It Matters
  id: totrans-3566
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: Porting *llm.c* forces you to understand what each piece of the code is doing
    — you can’t just rely on `torch.nn.LayerNorm` or `torch.nn.MultiheadAttention`.
    This makes it an excellent exercise for truly learning transformers, while also
    giving you practical implementations in different environments.
  id: totrans-3567
  prefs: []
  type: TYPE_NORMAL
  zh: 移植`*llm.c*`迫使你理解代码的每一部分在做什么——你不能仅仅依赖于`torch.nn.LayerNorm`或`torch.nn.MultiheadAttention`。这使得它成为真正学习变换器的优秀练习，同时也为你提供了在不同环境中的实际实现。
- en: Try It Yourself
  id: totrans-3568
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 尝试自己动手做
- en: Reimplement one kernel (like LayerNorm or matmul) in Go or Rust. Test it against
    the C version.
  id: totrans-3569
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Go或Rust中重新实现一个内核（如LayerNorm或matmul）。与C版本进行测试。
- en: Write a minimal Metal kernel that adds two vectors, then extend it to matrix
    multiplication.
  id: totrans-3570
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个最简的Metal内核，用于向量加法，然后扩展到矩阵乘法。
- en: Build a Rust tokenizer reader that loads `gpt2_tokenizer.bin` and decodes IDs
    back into text.
  id: totrans-3571
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个Rust分词器读取器，它加载`gpt2_tokenizer.bin`并将ID解码回文本。
- en: 'Compare training speed: C with OpenMP vs. Rust with Rayon vs. Go with goroutines.'
  id: totrans-3572
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较训练速度：C与OpenMP、Rust与Rayon、Go与goroutines。
- en: Try porting just the forward pass first — inference is easier than training
    because you don’t need backprop.
  id: totrans-3573
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先尝试移植前向传播——推理比训练更容易，因为你不需要反向传播。
- en: The Takeaway
  id: totrans-3574
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸取的经验
- en: The *llm.c* design is portable by nature — it doesn’t hide behind opaque frameworks.
    Porting it to Go, Rust, or Metal is not just about performance or language preference.
    It’s about proving to yourself that the transformer algorithm is universal, and
    you can implement it anywhere once you truly understand it.
  id: totrans-3575
  prefs: []
  type: TYPE_NORMAL
  zh: '`*llm.c*`的设计本质上具有可移植性——它不会隐藏在模糊的框架后面。将其移植到Go、Rust或Metal不仅仅是关于性能或语言偏好。这是在真正理解了变换器算法之后，向自己证明该算法是通用的，并且你可以在任何地方实现它。'
- en: 90\. Keeping the Repo Minimal and Clean
  id: totrans-3576
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 90. 保持仓库最小化和清洁
- en: One of the defining features of *llm.c* is its minimalism. The repository avoids
    the sprawling complexity of large frameworks and instead sticks to a small, readable
    core. This design choice isn’t an accident—it’s a philosophy. By keeping the codebase
    small and clean, contributors can focus on understanding the fundamentals of transformers
    rather than navigating thousands of lines of boilerplate.
  id: totrans-3577
  prefs: []
  type: TYPE_NORMAL
  zh: '`*llm.c*`的一个定义性特征是其极简主义。仓库避免了大型框架的复杂扩展，而是坚持使用一个小巧、易读的核心。这种设计选择不是偶然的——它是一种哲学。通过保持代码库小而干净，贡献者可以专注于理解变换器的基本原理，而不是在数千行样板代码中导航。'
- en: The Philosophy of Minimalism
  id: totrans-3578
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 极简主义哲学
- en: 'Readability over performance: While production frameworks like PyTorch or TensorFlow
    optimize aggressively, *llm.c* intentionally trades some performance for clarity.
    A loop in plain C is easier to study than a chain of optimized CUDA calls hidden
    behind macros.'
  id: totrans-3579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可读性优于性能：虽然生产框架如PyTorch或TensorFlow会积极优化，但`*llm.c*`故意牺牲了一些性能以换取清晰度。一个普通的C循环比隐藏在宏后面的优化CUDA调用链更容易研究。
- en: 'Portability: A smaller codebase can be ported more easily to new environments
    (Go, Rust, Metal) without pulling in dozens of dependencies.'
  id: totrans-3580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可移植性：较小的代码库可以更容易地移植到新的环境（如Go、Rust、Metal）中，而无需引入数十个依赖项。
- en: 'Learning-first design: Every line of code has a clear purpose. There are no
    abstractions “just in case.”'
  id: totrans-3581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以学习为先的设计：每一行代码都有一个明确的目的。没有“以防万一”的抽象。
- en: This philosophy turns *llm.c* into both a working training framework and an
    educational resource.
  id: totrans-3582
  prefs: []
  type: TYPE_NORMAL
  zh: 这种理念使`llm.c`成为既是一个工作训练框架，也是一个教育资源。
- en: How Minimalism Is Enforced
  id: totrans-3583
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何强制执行简约主义
- en: 'Flat structure: The repository avoids deep directory hierarchies. Most files
    live directly under `llmc/` or `dev/`.'
  id: totrans-3584
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 平坦的结构：仓库避免了深层次的目录层次结构。大多数文件直接位于`llmc/`或`dev/`下。
- en: 'No external libraries unless critical: You’ll see OpenMP, cuBLAS, or cuDNN
    for performance, but not sprawling dependency chains.'
  id: totrans-3585
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除非是关键性的，否则不要使用外部库：你会看到OpenMP、cuBLAS或cuDNN用于性能，但不会出现庞大的依赖链。
- en: 'One feature, one file: Tokenization, dataloading, schedulers, and samplers
    each get their own small C file. This prevents “god files” where too much is lumped
    together.'
  id: totrans-3586
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个功能，一个文件：分词、数据加载、调度器和采样器各自有自己的小型C文件。这防止了“神文件”中内容过于集中。
- en: 'Consistent naming: Functions and structs use clear, descriptive names (e.g.,
    `dataloader_next_batch`, `tokenizer_decode`) so readers don’t get lost.'
  id: totrans-3587
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一致的命名：函数和结构体使用清晰、描述性的名称（例如，`dataloader_next_batch`、`tokenizer_decode`），这样读者就不会迷失方向。
- en: Practical Examples
  id: totrans-3588
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实际示例
- en: If you open `train_gpt2.c`, you’ll find that it builds the model, initializes
    dataloaders, and runs the training loop. It doesn’t try to handle every possible
    model configuration, dataset format, or distributed scenario. Those belong in
    specialized files or external tools.
  id: totrans-3589
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你打开`train_gpt2.c`，你会发现它构建模型、初始化数据加载器并运行训练循环。它不会尝试处理每个可能的模型配置、数据集格式或分布式场景。这些属于专门的文件或外部工具。
- en: 'Likewise, `llmc/utils.h` only defines the absolute essentials: safe file I/O
    wrappers (`fopenCheck`, `freadCheck`) and memory allocators. It’s not bloated
    with generic helpers unrelated to training GPT-2.'
  id: totrans-3590
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，`llmc/utils.h`仅定义了绝对必要的内容：安全的文件I/O包装器（`fopenCheck`、`freadCheck`）和内存分配器。它没有包含与训练GPT-2无关的通用辅助工具。
- en: Why It Matters
  id: totrans-3591
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: A minimal repo lowers the barrier to entry. Beginners can trace execution from
    `main()` all the way to the optimizer update without detours. Researchers can
    fork the code and modify it without worrying about breaking dozens of interconnected
    modules. Even advanced developers benefit because the simplicity forces clarity
    in reasoning about algorithms.
  id: totrans-3592
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化的仓库降低了入门门槛。初学者可以从`main()`函数开始追踪执行，直到优化器更新，而无需绕道。研究人员可以分叉代码并修改它，而无需担心破坏数十个相互连接的模块。即使是高级开发者也能从中受益，因为简单性迫使他们在算法推理上保持清晰。
- en: Try It Yourself
  id: totrans-3593
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 尝试自己动手做
- en: Pick any file in the repo and count how many lines it has. Most are under a
    few hundred. Compare this with a similar file in PyTorch or TensorFlow.
  id: totrans-3594
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在仓库中挑选任何文件并计算它的行数。大多数文件行数都在几百行以下。与PyTorch或TensorFlow中类似的文件相比。
- en: Try adding a new feature—say, a different activation function. Notice how easy
    it is to slot it in because the structure is clean.
  id: totrans-3595
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试添加一个新功能——比如，不同的激活函数。注意，由于结构清晰，将其插入是多么容易。
- en: Explore what happens if you make the repo “heavier.” Add too many helpers, abstractions,
    or configs. Does it make the code harder to read?
  id: totrans-3596
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 探索如果使仓库“更重”会发生什么。添加过多的辅助工具、抽象或配置。这会使代码更难阅读？
- en: Practice explaining the training loop to a friend. If the repo is simple, you
    should be able to walk them through without glossing over details.
  id: totrans-3597
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 练习向朋友解释训练循环。如果仓库简单，你应该能够引导他们了解细节。
- en: The Takeaway
  id: totrans-3598
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸收要点
- en: Keeping *llm.c* minimal is not just about saving lines of code. It’s about preserving
    clarity, ensuring reproducibility, and making the repository a place where anyone—from
    curious learners to experienced engineers—can open a file and *understand what’s
    happening*. The simplicity is the point, and that’s what makes *llm.c* a rare
    and valuable resource in a world of bloated ML frameworks.
  id: totrans-3599
  prefs: []
  type: TYPE_NORMAL
  zh: 保持`llm.c`最小化不仅仅是节省代码行数。它关乎保持清晰性，确保可重复性，并使仓库成为任何从好奇的学习者到经验丰富的工程师都能打开文件并理解正在发生什么的地方。简单性是关键，这也是为什么`llm.c`在臃肿的机器学习框架世界中成为罕见且宝贵的资源。
- en: Chapter 10\. Reproduction, community and roadmap
  id: totrans-3600
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第10章：重现、社区和路线图
- en: 91\. Reproducing GPT-2 124M on Single Node
  id: totrans-3601
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 91. 在单节点上重现GPT-2 124M
- en: The first major milestone for anyone exploring *llm.c* is to reproduce the training
    of GPT-2 124M, the smallest version of the GPT-2 family. This model has around
    124 million parameters, which is large enough to be interesting, but still small
    enough to train on a single modern GPU—or even slowly on CPU for demonstration
    purposes.
  id: totrans-3602
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何探索*llm.c*的人来说，第一个主要里程碑是复制GPT-2 124M的训练，这是GPT-2家族中最小的版本。这个模型大约有1240万个参数，足够有趣，但仍然足够小，可以在单个现代GPU上训练——或者甚至可以缓慢地在CPU上用于演示目的。
- en: Why Start with 124M?
  id: totrans-3603
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么从124M开始？
- en: 'GPT-2 comes in multiple sizes: 124M, 355M, 774M, and 1.6B parameters. Training
    the largest requires clusters of GPUs and serious compute budgets. The 124M version,
    however, fits comfortably on consumer-grade GPUs like an NVIDIA 3090, and can
    even be run on a laptop CPU if you’re patient. It’s the “hello world” of transformer
    reproduction: small, approachable, and still real.'
  id: totrans-3604
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2有多种尺寸：124M、355M、774M和1.6B参数。训练最大的版本需要GPU集群和大量的计算预算。然而，124M版本可以舒适地安装在消费级GPU上，例如NVIDIA
    3090，如果你有耐心，甚至可以在笔记本电脑CPU上运行。它是transformer复制的“hello world”：小巧、易于接近，而且仍然真实。
- en: What the Training Setup Looks Like
  id: totrans-3605
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练设置看起来像什么
- en: 'Training GPT-2 124M involves a few key steps:'
  id: totrans-3606
  prefs: []
  type: TYPE_NORMAL
  zh: 训练GPT-2 124M涉及几个关键步骤：
- en: Model Configuration The config for 124M is baked into *llm.c* with 12 layers,
    12 heads, hidden dimension of 768, and max sequence length of 1024.
  id: totrans-3607
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型配置124M的配置已经嵌入到*llm.c*中，具有12层、12个头、隐藏维度为768和最大序列长度为1024。
- en: '[PRE177]'
  id: totrans-3608
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE177]'
- en: Dataset You can train on small datasets like Tiny Shakespeare or Tiny Stories
    for quick runs. For more realistic reproduction, you need something closer to
    OpenWebText. The data is tokenized with the GPT-2 tokenizer (`gpt2_tokenizer.bin`)
    and stored in `.bin` files.
  id: totrans-3609
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据集你可以训练小型数据集，如Tiny Shakespeare或Tiny Stories进行快速运行。为了更真实地复制，你需要接近OpenWebText的数据。数据使用GPT-2标记器（`gpt2_tokenizer.bin`）进行标记化，并存储在`.bin`文件中。
- en: Batch Size and Sequence Length A common setting is `B = 8, T = 1024`, meaning
    8 sequences, each of length 1024 tokens. Adjust these based on available memory.
  id: totrans-3610
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 批大小和序列长度一个常见的设置是`B = 8, T = 1024`，这意味着8个序列，每个序列长度为1024个标记。根据可用内存进行调整。
- en: Optimizer AdamW with a learning rate around `3e-4` is the default. Warmup and
    cosine decay scheduling can be enabled to match published GPT-2 training curves.
  id: totrans-3611
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 默认的优化器是AdamW，学习率约为`3e-4`。可以启用预热和余弦衰减调度，以匹配已发布的GPT-2训练曲线。
- en: What to Expect in Practice
  id: totrans-3612
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实践中可以期待什么
- en: On CPU, training a single step may take several seconds. On a single GPU with
    CUDA, each step may take under 100 milliseconds. With 124M parameters, training
    from scratch on a dataset the size of OpenWebText still takes days, but you can
    reproduce key dynamics (loss curve, sample generations) on smaller datasets in
    hours.
  id: totrans-3613
  prefs: []
  type: TYPE_NORMAL
  zh: 在CPU上，训练单步可能需要几秒钟。在单个带有CUDA的GPU上，每步可能不到100毫秒。对于124M参数，从OpenWebText大小的数据集从头开始训练仍然需要几天，但你可以在更小的数据集上几小时内复制关键动态（损失曲线、样本生成）。
- en: 'As an example, here’s the type of log you might see:'
  id: totrans-3614
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你可能看到的日志类型如下：
- en: '[PRE178]'
  id: totrans-3615
  prefs: []
  type: TYPE_PRE
  zh: '[PRE178]'
- en: Even within a few hundred steps, the model begins to generate text resembling
    English rather than pure noise.
  id: totrans-3616
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在几百步之内，模型也开始生成类似英语的文本，而不是纯噪音。
- en: Why It Matters
  id: totrans-3617
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: Reproducing GPT-2 124M is a confidence check. If your setup is correct, your
    loss curves should match those in the original OpenAI paper or the PyTorch reference
    implementation. This validates that *llm.c* is a faithful reproduction, not just
    a toy. It also teaches you how much compute and data go into even the smallest
    GPT-2 model, building intuition about scaling laws.
  id: totrans-3618
  prefs: []
  type: TYPE_NORMAL
  zh: 复制GPT-2 124M是一个信心检查。如果你的设置正确，你的损失曲线应该与原始OpenAI论文或PyTorch参考实现中的曲线相匹配。这验证了*llm.c*是一个忠实复制品，而不仅仅是玩具。这也教会了你即使是最小的GPT-2模型也需要多少计算和数据，帮助你建立对扩展定律的直觉。
- en: Try It Yourself
  id: totrans-3619
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 试试看
- en: Train GPT-2 124M for 1000 steps on Tiny Shakespeare. Watch how the generated
    text improves.
  id: totrans-3620
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Tiny Shakespeare上训练GPT-2 124M 1000步。观察生成的文本是如何改进的。
- en: Change the batch size `B` from 8 to 4\. What happens to the speed and the stability
    of training?
  id: totrans-3621
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将批大小`B`从8改为4。训练速度和稳定性会发生什么变化？
- en: Run training on CPU vs. GPU. Compare how long each step takes.
  id: totrans-3622
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在CPU和GPU上运行训练。比较每个步骤所需的时间。
- en: Track both training and validation loss. Notice how they diverge when the model
    begins to overfit.
  id: totrans-3623
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 跟踪训练和验证损失。注意当模型开始过拟合时，它们是如何发散的。
- en: The Takeaway
  id: totrans-3624
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸收要点
- en: The GPT-2 124M run is more than just a demo—it’s your gateway into real LLM
    training. You see how data, model size, optimizer, and hardware come together.
    Once you’ve mastered this reproduction, you’re ready to push toward larger models
    and more complex setups. It’s the foundation on which everything else in *llm.c*
    builds.
  id: totrans-3625
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2 124M的运行不仅仅是演示——它是你进入真实LLM训练的门户。你看到数据、模型大小、优化器和硬件是如何结合在一起的。一旦你掌握了这种复制的技巧，你就可以向更大的模型和更复杂的设置迈进。这是*llm.c*中其他一切的基础。
- en: 92\. Reproducing GPT-2 355M (Constraints and Tricks)
  id: totrans-3626
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 92. 复制GPT-2 355M（约束和技巧）
- en: 'Once GPT-2 124M has been successfully reproduced, the next logical step is
    scaling up to GPT-2 355M. This version is roughly three times larger, with about
    355 million parameters. It introduces new challenges that don’t appear at the
    smaller scale: memory pressure, training stability, and compute cost.'
  id: totrans-3627
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦成功复制了GPT-2 124M，下一步合乎逻辑的步骤就是扩展到GPT-2 355M。这个版本大约大三倍，大约有355百万个参数。它引入了在较小规模下不会出现的新挑战：内存压力、训练稳定性和计算成本。
- en: Model Configuration
  id: totrans-3628
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型配置
- en: 'The 355M model still uses 1024 tokens as the maximum sequence length and the
    same GPT-2 tokenizer. The difference is in the depth and width of the network:'
  id: totrans-3629
  prefs: []
  type: TYPE_NORMAL
  zh: 355M模型仍然使用1024个token作为最大序列长度和相同的GPT-2分词器。区别在于网络的深度和宽度：
- en: 'Layers: 24 transformer blocks instead of 12'
  id: totrans-3630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层数：24个transformer块而不是12个
- en: 'Hidden dimension (channels): 1024 instead of 768'
  id: totrans-3631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏维度（通道）：1024个而不是768个
- en: 'Heads: 16 instead of 12'
  id: totrans-3632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 头数：16个而不是12个
- en: The total parameter count rises from ~124M to ~355M. That means not just three
    times more math per step, but also more memory needed for parameters, gradients,
    and optimizer state.
  id: totrans-3633
  prefs: []
  type: TYPE_NORMAL
  zh: 总参数数量从~124M增加到~355M。这意味着每一步不仅仅是数学量增加三倍，还需要更多的内存来存储参数、梯度和优化器状态。
- en: The Compute Challenge
  id: totrans-3634
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 计算挑战
- en: With 124M, a single GPU with 8–12 GB of VRAM is enough. For 355M, you need at
    least 16 GB to run comfortably with sequence length 1024 and batch size of 8\.
    On smaller GPUs, you’ll quickly hit “CUDA out of memory” errors.
  id: totrans-3635
  prefs: []
  type: TYPE_NORMAL
  zh: 对于124M，单个具有8-12GB VRAM的GPU就足够了。对于355M，你需要至少16GB才能舒适地运行，序列长度为1024，批大小为8。在较小的GPU上，你很快就会遇到“CUDA内存不足”的错误。
- en: One trick is to reduce batch size (B) or sequence length (T). For example, instead
    of training with `(B=8, T=1024)`, you might use `(B=4, T=512)`. This halves the
    memory footprint but still lets you test scaling dynamics.
  id: totrans-3636
  prefs: []
  type: TYPE_NORMAL
  zh: 一个技巧是减少批大小（B）或序列长度（T）。例如，你可能会使用`(B=4, T=512)`来代替`(B=8, T=1024)`进行训练。这减半了内存占用，但仍然允许你测试扩展动态。
- en: 'Another approach is to use gradient accumulation: simulate a larger batch size
    by running multiple small steps and accumulating gradients before updating.'
  id: totrans-3637
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是使用梯度累积：通过运行多个小步骤并累积梯度来模拟更大的批大小。
- en: Training Stability
  id: totrans-3638
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练稳定性
- en: 'Larger models are more sensitive to hyperparameters. The AdamW optimizer still
    works, but the learning rate schedule becomes more important. Many practitioners
    use:'
  id: totrans-3639
  prefs: []
  type: TYPE_NORMAL
  zh: 较大的模型对超参数更敏感。AdamW优化器仍然有效，但学习率调度变得更为重要。许多实践者使用：
- en: 'Learning rate: ~3e-4 peak'
  id: totrans-3640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率：峰值约为~3e-4
- en: 'Warmup steps: a few thousand'
  id: totrans-3641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预热步骤：几千步
- en: 'Cosine decay: to taper the learning rate gradually'
  id: totrans-3642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 余弦衰减：逐渐减小学习率
- en: If you skip warmup, the larger model may diverge early (loss exploding instead
    of decreasing).
  id: totrans-3643
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你跳过预热，较大的模型可能会早期发散（损失爆炸而不是减少）。
- en: Tricks for Feasibility
  id: totrans-3644
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可行性技巧
- en: 'Mixed Precision Training (FP16 or BF16): Cuts memory use nearly in half. Supported
    in CUDA paths of *llm.c*.'
  id: totrans-3645
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 混合精度训练（FP16或BF16）：内存使用量几乎减半。在*llm.c*的CUDA路径中受支持。
- en: 'Activation Checkpointing: Save memory by recomputing activations during backpropagation.
    Slower, but lets you fit bigger models.'
  id: totrans-3646
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 激活检查点：在反向传播期间重新计算激活以节省内存。速度较慢，但允许你拟合更大的模型。
- en: 'Smaller Dataset Runs: Train on Tiny Shakespeare or Tiny Stories to sanity-check
    the setup, then scale to OpenWebText-like data.'
  id: totrans-3647
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 小数据集运行：在Tiny Shakespeare或Tiny Stories上训练以验证设置，然后扩展到类似OpenWebText的数据。
- en: Example Logs
  id: totrans-3648
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例日志
- en: 'Running GPT-2 355M for a short demo might look like:'
  id: totrans-3649
  prefs: []
  type: TYPE_NORMAL
  zh: 短时间内运行GPT-2 355M可能看起来像：
- en: '[PRE179]'
  id: totrans-3650
  prefs: []
  type: TYPE_PRE
  zh: '[PRE179]'
- en: The loss drops more slowly than with 124M because the model has more capacity
    to learn, but also needs more data to generalize.
  id: totrans-3651
  prefs: []
  type: TYPE_NORMAL
  zh: 损失下降速度比124M慢，因为模型有更多的学习容量，但也需要更多的数据来泛化。
- en: Why It Matters
  id: totrans-3652
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: '355M is the first step into “medium-sized” LLMs. You start to feel the bottlenecks
    that dominate larger models: VRAM limits, training speed, and hyperparameter tuning.
    Solving these prepares you for the 774M and 1.6B experiments, where such problems
    become even more pronounced.'
  id: totrans-3653
  prefs: []
  type: TYPE_NORMAL
  zh: 355M是进入“中等规模”LLM的第一步。你开始感受到主导更大模型的瓶颈：VRAM限制、训练速度和超参数调整。解决这些问题将为你准备774M和1.6B实验，在这些实验中，这些问题变得更加明显。
- en: Try It Yourself
  id: totrans-3654
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 试试看
- en: Train GPT-2 355M with batch size 4 and sequence length 512\. Record how long
    each step takes.
  id: totrans-3655
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用批大小4和序列长度512来训练GPT-2 355M。记录每个步骤所需的时间。
- en: 'Experiment with warmup steps: run once with warmup=0, and once with warmup=2000\.
    Compare stability.'
  id: totrans-3656
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试不同的预热步骤：一次预热=0，一次预热=2000。比较稳定性。
- en: Enable mixed precision if you have a CUDA-capable GPU. Measure memory usage
    before and after.
  id: totrans-3657
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你有CUDA兼容的GPU，请启用混合精度。测量前后内存使用情况。
- en: Try training on Tiny Shakespeare vs. Tiny Stories. Does the model overfit faster
    on the smaller dataset?
  id: totrans-3658
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试在Tiny Shakespeare和Tiny Stories上训练。模型在较小的数据集上是否更快地过拟合？
- en: The Takeaway
  id: totrans-3659
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸收要点
- en: Reproducing GPT-2 355M is all about learning how to stretch limited resources.
    You’ll discover memory-saving tricks, the importance of learning rate schedules,
    and the role of data scale. It’s a practical exercise in resource management—just
    like the real challenges faced when training today’s billion-parameter models.
  id: totrans-3660
  prefs: []
  type: TYPE_NORMAL
  zh: 复现GPT-2 355M主要是学习如何扩展有限资源。你会发现内存节省技巧、学习率调度的重要性以及数据规模的作用。这是一次资源管理的实际练习——就像在训练今天的十亿参数模型时面临的真正挑战一样。
- en: 93\. Reproducing GPT-2 774M (Scaling Up)
  id: totrans-3661
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 93. 复现GPT-2 774M（扩展）
- en: The 774M parameter version of GPT-2 is often called “GPT-2 Medium.” This is
    the point where training transitions from a personal experiment to a small-scale
    research project. It’s about six times larger than the 124M baseline, and roughly
    twice the size of 355M. Running it requires careful planning of hardware, memory,
    and software tricks.
  id: totrans-3662
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2的774M参数版本通常被称为“GPT-2 Medium”。这是训练从个人实验转变为小型研究项目的转折点。它比124M基线大六倍左右，大约是355M的两倍。运行它需要仔细规划硬件、内存和软件技巧。
- en: Model Configuration
  id: totrans-3663
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型配置
- en: 'For 774M, the architecture expands again:'
  id: totrans-3664
  prefs: []
  type: TYPE_NORMAL
  zh: 对于774M，架构再次扩展：
- en: 'Layers (transformer blocks): 36'
  id: totrans-3665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层数（转换器块）：36
- en: 'Hidden size (channels): 1280'
  id: totrans-3666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏大小（通道）：1280
- en: 'Attention heads: 20'
  id: totrans-3667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力头：20
- en: 'Maximum sequence length: 1024 (unchanged)'
  id: totrans-3668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大序列长度：1024（未变）
- en: This jump in size increases both the parameter storage and the number of activations
    that must be kept during training. Optimizer states (AdamW’s *m* and *v* vectors)
    alone consume several gigabytes.
  id: totrans-3669
  prefs: []
  type: TYPE_NORMAL
  zh: 这种规模的增长增加了参数存储和训练过程中必须保持的激活数量。仅优化器状态（AdamW的*m*和*v*向量）就消耗了几个GB。
- en: Hardware Requirements
  id: totrans-3670
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 硬件要求
- en: Running GPT-2 774M from scratch generally requires GPUs with 24 GB VRAM or more
    (e.g., NVIDIA RTX 3090/4090, A100, or H100). With smaller cards, you’ll almost
    certainly hit memory errors unless you aggressively reduce batch size and use
    techniques like activation checkpointing.
  id: totrans-3671
  prefs: []
  type: TYPE_NORMAL
  zh: 从头开始运行GPT-2 774M通常需要24GB VRAM或更多的GPU（例如，NVIDIA RTX 3090/4090、A100或H100）。使用较小的卡，除非你积极降低批大小并使用激活检查点等技术，否则你几乎肯定会遇到内存错误。
- en: On CPUs, training is technically possible but far too slow to be practical—steps
    that take milliseconds on GPUs might take many seconds or even minutes.
  id: totrans-3672
  prefs: []
  type: TYPE_NORMAL
  zh: 在CPU上，从技术上讲可以进行训练，但速度太慢，不切实际——在GPU上只需毫秒的步骤可能需要几秒甚至几分钟。
- en: Practical Constraints
  id: totrans-3673
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实际限制
- en: 'Batch Size: In practice, you may need to lower `B` to 2 or even 1 with sequence
    length 1024.'
  id: totrans-3674
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 批大小：在实际应用中，你可能需要将`B`降低到2甚至1，以适应序列长度1024。
- en: 'Gradient Accumulation: A must-have to simulate larger batch sizes and stabilize
    training.'
  id: totrans-3675
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 梯度累积：模拟更大的批大小并稳定训练的必备条件。
- en: 'Mixed Precision: FP16 or BF16 reduces memory by about half, without hurting
    convergence much.'
  id: totrans-3676
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 混合精度：FP16或BF16可以将内存减少约一半，而不会对收敛造成太大影响。
- en: 'Checkpointing: Recomputes intermediate results instead of storing them, trading
    time for memory.'
  id: totrans-3677
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查点：重新计算中间结果而不是存储它们，以时间换取内存。
- en: Training Dynamics
  id: totrans-3678
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练动态
- en: 'The loss curve for GPT-2 774M drops more steadily and requires much more data
    to reach its potential. If you only train on Tiny Shakespeare or Tiny Stories,
    it will quickly overfit: the model is too large for such a small dataset. For
    meaningful reproduction, you need a dataset similar in scale to OpenWebText.'
  id: totrans-3679
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2 774M的损失曲线下降更加稳定，需要更多的数据才能达到其潜力。如果你只在Tiny Shakespeare或Tiny Stories上训练，模型会很快过拟合：对于如此小的数据集，模型太大。为了有意义地复现，你需要一个与OpenWebText规模相似的数据集。
- en: 'Training logs for a sanity check run might look like:'
  id: totrans-3680
  prefs: []
  type: TYPE_NORMAL
  zh: 精神检查运行的训练日志可能看起来像：
- en: '[PRE180]'
  id: totrans-3681
  prefs: []
  type: TYPE_PRE
  zh: '[PRE180]'
- en: Notice that losses start higher (due to the larger random initialization space)
    but decrease predictably once training gets underway.
  id: totrans-3682
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到损失开始较高（由于更大的随机初始化空间），但一旦开始训练，就会可预测地下降。
- en: Why It Matters
  id: totrans-3683
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: 'The 774M model is the sweet spot where scaling laws become obvious. Compared
    to 124M and 355M, it generalizes better, generates more fluent text, and demonstrates
    the benefits of parameter growth. But it also shows why infrastructure matters:
    without careful management, it’s nearly impossible to train this model on consumer-grade
    hardware.'
  id: totrans-3684
  prefs: []
  type: TYPE_NORMAL
  zh: 774M模型是规模法则变得明显的甜蜜点。与124M和355M相比，它具有更好的泛化能力，生成更流畅的文本，并展示了参数增长的好处。但它也显示了基础设施的重要性：如果没有仔细管理，几乎不可能在消费级硬件上训练此模型。
- en: This is also where distributed training (covered in Chapter 8) becomes relevant,
    because one GPU is often not enough for efficient scaling.
  id: totrans-3685
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是分布式训练（在第8章中介绍）变得相关的地方，因为单个GPU通常不足以进行有效的扩展。
- en: Try It Yourself
  id: totrans-3686
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 尝试自己操作
- en: Train GPT-2 774M with `(B=1, T=1024)` and gradient accumulation over 8 steps.
    Watch how it simulates a batch size of 8.
  id: totrans-3687
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`(B=1, T=1024)`和8步梯度累积训练GPT-2 774M。观察它是如何模拟8个批次的。
- en: Compare training with FP32 vs. mixed precision. Measure both memory use and
    speed.
  id: totrans-3688
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较FP32与混合精度训练。测量内存使用和速度。
- en: Run a short fine-tuning experiment on Tiny Stories. Observe how quickly the
    model memorizes the dataset.
  id: totrans-3689
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Tiny Stories上运行一个简短的微调实验。观察模型如何快速记住数据集。
- en: Plot the training and validation loss curves. Does the larger model overfit
    faster or slower than 355M?
  id: totrans-3690
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制训练和验证损失曲线。较大的模型是否比355M更快或更慢地过拟合？
- en: The Takeaway
  id: totrans-3691
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸收要点
- en: 'Reproducing GPT-2 774M is about scaling into the territory of real research
    workloads. You face serious memory constraints, dataset requirements, and compute
    costs. But if you succeed, you’ll see firsthand why the machine learning community
    kept pushing toward billion-parameter models: larger networks unlock noticeably
    stronger capabilities, even with the same architecture.'
  id: totrans-3692
  prefs: []
  type: TYPE_NORMAL
  zh: 复制GPT-2 774M是将扩展到真实研究工作负载的领域。你面临着严重的内存限制、数据集要求和计算成本。但如果你成功了，你将亲身体验为什么机器学习社区一直在推动向十亿参数模型发展：更大的网络解锁了明显更强的能力，即使使用相同的架构。
- en: 94\. Reproducing GPT-2 1.6B on 8×H100 (24h Run)
  id: totrans-3693
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 94. 在8×H100上复制GPT-2 1.6B（24小时运行）
- en: The largest GPT-2 model, with 1.6 billion parameters, represents the upper bound
    of the original GPT-2 family. Training this model from scratch is not something
    you can casually attempt on a single workstation. It demands cluster-scale resources,
    distributed training software, and careful tuning to keep everything stable. In
    this section, we’ll walk through what makes the 1.6B model special, what infrastructure
    is required, and how a full reproduction might look.
  id: totrans-3694
  prefs: []
  type: TYPE_NORMAL
  zh: 最大的GPT-2模型，拥有16亿个参数，代表了原始GPT-2家族的上限。从头开始训练此模型不是你可以在单个工作站上随意尝试的事情。它需要集群级资源、分布式训练软件和仔细调整以保持一切稳定。在本节中，我们将探讨1.6B模型的特殊之处、所需的基础设施以及完整复制的可能外观。
- en: Model Configuration
  id: totrans-3695
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型配置
- en: 'The jump from 774M to 1.6B doubles the parameter count and makes the network
    both deeper and wider:'
  id: totrans-3696
  prefs: []
  type: TYPE_NORMAL
  zh: 从774M到1.6B的跳跃将参数数量翻倍，使网络既更深又更宽：
- en: 'Layers (transformer blocks): 48'
  id: totrans-3697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层数（转换器块）：48
- en: 'Hidden size (channels): 1600'
  id: totrans-3698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏大小（通道）：1600
- en: 'Attention heads: 25'
  id: totrans-3699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力头：25
- en: 'Sequence length: still 1024'
  id: totrans-3700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列长度：仍然是1024
- en: With these dimensions, every forward and backward pass requires massive amounts
    of memory and compute. Just storing the parameters in FP32 takes around 6.4 GB.
    Once you add gradients, optimizer states (AdamW’s *m* and *v*), and activations,
    the memory footprint easily exceeds 100 GB.
  id: totrans-3701
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些维度下，每次正向和反向传递都需要大量的内存和计算。仅存储FP32参数就需要大约6.4 GB。一旦添加梯度、优化器状态（AdamW的*m*和*v*）和激活，内存占用很容易超过100
    GB。
- en: Hardware Setup
  id: totrans-3702
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 硬件设置
- en: To reproduce this model realistically, you need access to high-end accelerators
    such as NVIDIA A100s or H100s. A common baseline is 8 GPUs with 80 GB each. With
    this setup, it is possible to train GPT-2 1.6B in under 24 hours, assuming efficient
    utilization.
  id: totrans-3703
  prefs: []
  type: TYPE_NORMAL
  zh: 要真实地复制此模型，你需要访问高端加速器，如NVIDIA A100s或H100s。一个常见的基线是每个80 GB的8个GPU。使用这种设置，在24小时内训练GPT-2
    1.6B是可能的，前提是高效利用。
- en: Without multi-GPU, training is impractical. Even if you could somehow fit the
    model on one GPU, the runtime would be weeks or months.
  id: totrans-3704
  prefs: []
  type: TYPE_NORMAL
  zh: 没有多GPU，训练是不切实际的。即使你能够以某种方式将模型拟合到一个GPU上，运行时间也会是几周或几个月。
- en: Distributed Training
  id: totrans-3705
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分布式训练
- en: 'The main strategy is data parallelism: each GPU processes a different mini-batch
    of data, and gradients are averaged across all devices with NCCL all-reduce. The
    code paths in *llm.c* support this via MPI integration, so you can scale from
    single-GPU to multi-node setups.'
  id: totrans-3706
  prefs: []
  type: TYPE_NORMAL
  zh: 主要策略是数据并行：每个GPU处理不同的迷你批次数据，梯度通过NCCL的全归约平均到所有设备上。*llm.c*中的代码路径通过MPI集成支持这一点，因此你可以从单GPU扩展到多节点设置。
- en: The training loop looks nearly identical to smaller models, but behind the scenes,
    every parameter update is coordinated across devices.
  id: totrans-3707
  prefs: []
  type: TYPE_NORMAL
  zh: 训练循环看起来几乎与较小模型相同，但幕后，每个参数更新都是跨设备协调的。
- en: Training Dynamics
  id: totrans-3708
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练动态
- en: 'The loss curve for 1.6B smooths out compared to smaller models. With enough
    data, the model continues to improve where 774M starts to plateau. This was one
    of the key insights from the original GPT-2 paper: scaling laws hold, and performance
    improves predictably with size, data, and compute.'
  id: totrans-3709
  prefs: []
  type: TYPE_NORMAL
  zh: 与较小模型相比，1.6B的损失曲线更加平滑。有了足够的数据，模型会继续在774M开始平台期的地方改进。这是原始GPT-2论文中的一个关键见解：缩放定律成立，性能随着大小、数据和计算量的增加而可预测地提高。
- en: 'Logs from a distributed run might look like this:'
  id: totrans-3710
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式运行的日志可能看起来像这样：
- en: '[PRE181]'
  id: totrans-3711
  prefs: []
  type: TYPE_PRE
  zh: '[PRE181]'
- en: 'Notice the speed: each step still takes only a few hundred milliseconds despite
    the massive size, thanks to parallelism across multiple H100s.'
  id: totrans-3712
  prefs: []
  type: TYPE_NORMAL
  zh: 注意速度：每个步骤仍然只需要几百毫秒，这得益于多个H100之间的并行性。
- en: Why It Matters
  id: totrans-3713
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: Reproducing GPT-2 1.6B is less about training a useful model today and more
    about understanding the scaling challenges of large language models. This exercise
    demonstrates how compute, memory, and distributed infrastructure become the limiting
    factors as models grow. It also shows why modern research labs design entire pipelines
    around multi-GPU and multi-node scaling.
  id: totrans-3714
  prefs: []
  type: TYPE_NORMAL
  zh: 重现GPT-2 1.6B与其说是训练一个有用的模型，不如说是理解大型语言模型的扩展挑战。这个练习展示了随着模型的增长，计算、内存和分布式基础设施如何成为限制因素。它还说明了为什么现代研究实验室会围绕多GPU和多节点扩展设计整个管道。
- en: Try It Yourself
  id: totrans-3715
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 尝试自己操作
- en: Simulate a multi-GPU run with fewer resources by reducing the model size but
    using the same parallel training setup. For example, train GPT-2 124M across 2
    GPUs to practice the workflow.
  id: totrans-3716
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过减少模型大小但使用相同的并行训练设置，用更少的资源模拟多GPU运行。例如，在2个GPU上训练GPT-2 124M以练习工作流程。
- en: Experiment with gradient accumulation to mimic large global batch sizes even
    on smaller clusters.
  id: totrans-3717
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过梯度累积实验来模拟即使在较小的集群上也能模仿大型全局批次大小。
- en: Try enabling and disabling mixed precision. Watch how memory use drops dramatically
    with FP16/BF16.
  id: totrans-3718
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试启用和禁用混合精度。观察内存使用如何随着FP16/BF16而大幅下降。
- en: Plot validation loss curves for 124M, 355M, 774M, and 1.6B side by side. Notice
    how the larger models sustain improvements longer.
  id: totrans-3719
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 并排绘制124M、355M、774M和1.6B的验证损失曲线。注意较大模型如何更长时间地保持改进。
- en: The Takeaway
  id: totrans-3720
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸收要点
- en: 'The GPT-2 1.6B reproduction is the capstone project of *llm.c*. It forces you
    to combine everything you’ve learned: data pipelines, optimizers, schedulers,
    distributed training, and system-level debugging. While few people will actually
    train 1.6B themselves, understanding what it takes provides a window into the
    engineering behind state-of-the-art LLMs and prepares you to engage with even
    larger modern models.'
  id: totrans-3721
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2 1.6B的重现是*llm.c*的标志性项目。它迫使你结合你所学的所有内容：数据管道、优化器、调度器、分布式训练和系统级调试。尽管很少有人会真正训练1.6B，但了解这需要什么可以让你窥见最先进LLM背后的工程，并为你参与更大型的现代模型做好准备。
- en: 95\. CPU-only Fine-Tune Demo (Tiny Shakespeare)
  id: totrans-3722
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 95. 仅CPU微调演示（微型莎士比亚）
- en: Not everyone has access to powerful GPUs or large compute clusters. One of the
    strengths of *llm.c* is that it provides a clean, minimal CPU-only path that lets
    you run real experiments—even if they are small and slow. A practical way to explore
    this is by fine-tuning GPT-2 on a small dataset like Tiny Shakespeare, which has
    only about 1 MB of text.
  id: totrans-3723
  prefs: []
  type: TYPE_NORMAL
  zh: 并非每个人都能访问到强大的GPU或大型计算集群。*llm.c*的一个优势是它提供了一个干净、最小化的仅CPU路径，让你能够运行真正的实验——即使它们规模小且速度慢。探索这一点的实际方法是通过对小型数据集（如微型莎士比亚）进行微调，该数据集仅有大约1MB的文本。
- en: Why Tiny Shakespeare?
  id: totrans-3724
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么是微型莎士比亚？
- en: Tiny Shakespeare is a classic toy dataset in machine learning. It’s small enough
    to fit in memory, yet it contains a rich variety of words, characters, and structures.
    Fine-tuning GPT-2 on this dataset allows the model to mimic Shakespearean style
    in just a few thousand steps. It’s not about building a state-of-the-art model—it’s
    about seeing the training process work end-to-end on modest hardware.
  id: totrans-3725
  prefs: []
  type: TYPE_NORMAL
  zh: Tiny Shakespeare是机器学习中的一个经典玩具数据集。它足够小，可以放入内存，同时包含丰富的词汇、角色和结构。在这样一个数据集上微调GPT-2只需几千步就能模仿莎士比亚风格。这并不是关于构建最先进的模型——这是关于在适度硬件上看到训练过程从头到尾的工作。
- en: Setup
  id: totrans-3726
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 设置
- en: 'The fine-tuning process uses the same `train_gpt2.c` CPU path, but with fewer
    steps, smaller batch sizes, and lower sequence lengths to keep things fast. A
    typical setup looks like this:'
  id: totrans-3727
  prefs: []
  type: TYPE_NORMAL
  zh: 微调过程使用相同的`train_gpt2.c` CPU路径，但步骤更少、批大小更小、序列长度更低，以保持快速。一个典型的设置看起来像这样：
- en: '[PRE182]'
  id: totrans-3728
  prefs: []
  type: TYPE_PRE
  zh: '[PRE182]'
- en: 'The dataset is tokenized once using `gpt2_tokenizer.bin` and stored in binary
    `.bin` files:'
  id: totrans-3729
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集使用`gpt2_tokenizer.bin`进行一次标记，并存储在二进制的`.bin`文件中：
- en: '[PRE183]'
  id: totrans-3730
  prefs: []
  type: TYPE_PRE
  zh: '[PRE183]'
- en: These files are only a few megabytes, making them perfect for quick experiments.
  id: totrans-3731
  prefs: []
  type: TYPE_NORMAL
  zh: 这些文件只有几兆字节，非常适合快速实验。
- en: Training Dynamics
  id: totrans-3732
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练动态
- en: 'When you fine-tune on Tiny Shakespeare, the training logs may look like this:'
  id: totrans-3733
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在Tiny Shakespeare上进行微调时，训练日志可能看起来像这样：
- en: '[PRE184]'
  id: totrans-3734
  prefs: []
  type: TYPE_PRE
  zh: '[PRE184]'
- en: Within a few hundred steps, the loss drops rapidly. By the time you’ve run 1000
    steps, the model starts producing text that looks recognizably Shakespearean—complete
    with archaic words, unusual punctuation, and rhythmic patterns.
  id: totrans-3735
  prefs: []
  type: TYPE_NORMAL
  zh: 在几百步之内，损失迅速下降。当你运行到1000步时，模型开始生成看起来像是莎士比亚风格的文本——包括古词、不寻常的标点符号和韵律模式。
- en: Example Output
  id: totrans-3736
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例输出
- en: 'Here’s a short sample from a fine-tuned run:'
  id: totrans-3737
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个经过微调运行的简短样本：
- en: '[PRE185]'
  id: totrans-3738
  prefs: []
  type: TYPE_PRE
  zh: '[PRE185]'
- en: It isn’t perfect, but it captures the “feel” of Shakespeare, which is remarkable
    given the tiny dataset and limited compute.
  id: totrans-3739
  prefs: []
  type: TYPE_NORMAL
  zh: 它并不完美，但捕捉到了莎士比亚的“感觉”，考虑到数据集很小且计算有限，这是令人瞩目的。
- en: Why It Matters
  id: totrans-3740
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: The CPU-only Tiny Shakespeare demo proves that LLMs are not just for massive
    data centers. With a minimal setup, you can watch the model learn, generate text,
    and overfit to a dataset. This hands-on practice builds intuition about what training
    does, how loss curves behave, and why scaling up matters.
  id: totrans-3741
  prefs: []
  type: TYPE_NORMAL
  zh: 仅使用CPU的Tiny Shakespeare演示证明了LLMs不仅仅适用于大型数据中心。通过最小化的设置，你可以观察模型学习、生成文本，并对数据集进行过拟合。这种动手实践有助于理解训练的作用、损失曲线的行为以及为什么扩展很重要。
- en: Try It Yourself
  id: totrans-3742
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 亲自尝试
- en: Change the sequence length from 64 to 128\. How does training speed and loss
    change?
  id: totrans-3743
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将序列长度从64改为128。训练速度和损失如何变化？
- en: Reduce the number of training steps to 200\. Do you still see Shakespeare-like
    text in generation?
  id: totrans-3744
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练步骤减少到200。在生成中你还能看到莎士比亚风格的文本吗？
- en: Increase the batch size to 8\. Does the loss curve become smoother or noisier?
  id: totrans-3745
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将批大小增加到8。损失曲线会变得更平滑还是更嘈杂？
- en: Fine-tune again but initialize from scratch (random weights). Compare the results
    to fine-tuning from pretrained GPT-2 124M.
  id: totrans-3746
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次微调，但从头开始初始化（随机权重）。将结果与从预训练的GPT-2 124M微调的结果进行比较。
- en: The Takeaway
  id: totrans-3747
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸取的经验
- en: Fine-tuning GPT-2 on Tiny Shakespeare with CPU-only training is a simple yet
    powerful demonstration. You don’t need GPUs to understand the mechanics of transformers.
    Even a modest laptop can teach you how training works, why overfitting happens,
    and how LLMs adapt to new domains. It’s a reminder that the best way to learn
    machine learning is by rolling up your sleeves and running experiments—even small
    ones.
  id: totrans-3748
  prefs: []
  type: TYPE_NORMAL
  zh: 仅使用CPU进行训练的GPT-2在Tiny Shakespeare上进行微调是一个简单而强大的演示。你不需要GPU就能理解transformers的机制。即使是普通的笔记本电脑也能教你如何进行训练，为什么会出现过拟合，以及LLMs如何适应新领域。这是一个提醒，学习机器学习的最好方法是亲自动手进行实验——即使是小规模的实验。
- en: 96\. Cost and Time Estimation for Runs
  id: totrans-3749
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 96. 运行的成本和时间估算
- en: One of the most eye-opening parts of working with large language models is realizing
    how expensive and time-consuming training can be. While *llm.c* makes it possible
    to run models of all sizes with simple, minimal C code, the hardware requirements
    grow quickly as you scale from GPT-2 124M to GPT-2 1.6B. Understanding cost and
    time estimation helps set realistic expectations, whether you’re running on a
    laptop CPU, a single GPU, or a rented cluster of accelerators.
  id: totrans-3750
  prefs: []
  type: TYPE_NORMAL
  zh: 与大型语言模型一起工作的最令人大开眼界的部分之一是意识到训练是多么昂贵和耗时。虽然*llm.c*使得使用简单的、最小的C代码运行所有大小的模型成为可能，但随着从GPT-2
    124M扩展到GPT-2 1.6B，硬件需求迅速增长。理解成本和时间估算有助于设定现实的期望，无论你是在笔记本电脑CPU上运行，还是在租用的加速器集群上运行。
- en: Key Factors Affecting Training Time
  id: totrans-3751
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 影响训练时间的关键因素
- en: 'Several components determine how long training takes and how much it costs:'
  id: totrans-3752
  prefs: []
  type: TYPE_NORMAL
  zh: 几个因素决定了训练所需的时间和成本：
- en: 'Model size (parameters): Larger models mean more multiplications per forward/backward
    pass, and more memory for parameters, gradients, and optimizer states.'
  id: totrans-3753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型大小（参数）：更大的模型意味着每前向/反向传递需要更多的乘法运算，以及更多的参数、梯度和优化器状态内存。
- en: 'Batch size and sequence length: Increasing either multiplies the amount of
    work per step.'
  id: totrans-3754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批大小和序列长度：增加任一项都会使每步的工作量增加。
- en: 'Dataset size: Bigger datasets require more steps to complete one epoch.'
  id: totrans-3755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集大小：更大的数据集需要更多的步骤来完成一个 epoch。
- en: 'Hardware speed: CPUs are far slower than GPUs; high-end GPUs like H100s can
    be 100× faster than CPUs for this workload.'
  id: totrans-3756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬件速度：CPU 比GPU慢得多；对于这种工作负载，高端 GPU（如 H100s）可以比 CPU 快 100 倍。
- en: 'Parallelism: Multi-GPU or multi-node setups let you divide the work, reducing
    time per step.'
  id: totrans-3757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行性：多 GPU 或多节点设置让你可以分割工作，减少每步时间。
- en: Rough Time Estimates
  id: totrans-3758
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 大概时间估计
- en: 'Here’s a simplified view of how long it might take to train GPT-2 models depending
    on hardware and setup. These are very approximate, assuming full training on a
    dataset like OpenWebText:'
  id: totrans-3759
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个简化的视图，展示了根据硬件和设置，训练 GPT-2 模型可能需要多长时间。这些是非常粗略的估计，假设在像 OpenWebText 这样的数据集上完成全部训练：
- en: '| Model | Parameters | Hardware | Time per Step | Total Training Time |'
  id: totrans-3760
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 参数 | 硬件 | 每步时间 | 总训练时间 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-3761
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| GPT-2 124M | ~124M | Laptop CPU | 1–2 s | Months |'
  id: totrans-3762
  prefs: []
  type: TYPE_TB
  zh: '| GPT-2 124M | ~124M | 笔记本 CPU | 1–2 秒 | 数月 |'
- en: '| GPT-2 124M | ~124M | Single RTX 3090 | ~100 ms | ~2–4 weeks |'
  id: totrans-3763
  prefs: []
  type: TYPE_TB
  zh: '| GPT-2 124M | ~124M | 单个 RTX 3090 | ~100 ms | ~2–4 周 |'
- en: '| GPT-2 355M | ~355M | Single RTX 3090 | ~300 ms | ~6–8 weeks |'
  id: totrans-3764
  prefs: []
  type: TYPE_TB
  zh: '| GPT-2 355M | ~355M | 单个 RTX 3090 | ~300 ms | ~6–8 周 |'
- en: '| GPT-2 774M | ~774M | 2× A100 40GB | ~200 ms | ~4–6 weeks |'
  id: totrans-3765
  prefs: []
  type: TYPE_TB
  zh: '| GPT-2 774M | ~774M | 2× A100 40GB | ~200 ms | ~4–6 周 |'
- en: '| GPT-2 1.6B | ~1.6B | 8× H100 80GB | ~300 ms | ~24 hours |'
  id: totrans-3766
  prefs: []
  type: TYPE_TB
  zh: '| GPT-2 1.6B | ~1.6B | 8× H100 80GB | ~300 ms | ~24 小时 |'
- en: These numbers show why scaling matters. The larger models don’t just need more
    compute per step—they also need more data to reach their potential. That means
    the total cost balloons unless you have a cluster of top-tier GPUs.
  id: totrans-3767
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数字显示了为什么扩展很重要。更大的模型不仅需要每步更多的计算能力，还需要更多的数据来发挥其潜力。这意味着除非你有顶级 GPU 的集群，否则总成本会大幅增加。
- en: Cost in Cloud Environments
  id: totrans-3768
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 云环境中的成本
- en: 'If you run these experiments on cloud providers like AWS, GCP, or Azure, costs
    can add up quickly. For example:'
  id: totrans-3769
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在像 AWS、GCP 或 Azure 这样的云服务提供商上运行这些实验，成本可能会迅速增加。例如：
- en: An NVIDIA A100 40GB instance costs around $2–3 per hour (spot pricing can be
    cheaper).
  id: totrans-3770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NVIDIA A100 40GB 实例每小时大约花费 2–3 美元（spot 价格可能更便宜）。
- en: Training GPT-2 124M for a week might cost $500–1,000.
  id: totrans-3771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练 GPT-2 124M 一周可能需要 500–1,000 美元。
- en: Training GPT-2 1.6B for 24 hours on 8× H100s might cost $5,000–10,000, depending
    on the provider.
  id: totrans-3772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 8× H100s 上训练 GPT-2 1.6B 24 小时可能需要 5,000–10,000 美元，具体取决于提供商。
- en: This is why many researchers test code paths on small datasets and small models
    first, then only scale up when absolutely necessary.
  id: totrans-3773
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是为什么许多研究人员首先在小数据集和小模型上测试代码路径，然后在绝对必要时才进行扩展的原因。
- en: Why It Matters
  id: totrans-3774
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: Estimating cost and time prevents frustration and wasted money. It teaches you
    to prototype at small scale (CPU or 124M runs), validate your setup, and only
    then scale up to medium (355M, 774M) or large (1.6B) models. It also gives you
    a realistic appreciation for the engineering and budget that went into OpenAI’s
    original GPT-2 training runs.
  id: totrans-3775
  prefs: []
  type: TYPE_NORMAL
  zh: 估算成本和时间可以防止挫败感和金钱浪费。它教会你在小规模（CPU 或 124M 运行）上进行原型设计，验证你的设置，然后才扩展到中等（355M、774M）或大型（1.6B）模型。它还让你对
    OpenAI 原始 GPT-2 训练运行中投入的工程和预算有一个现实的了解。
- en: Try It Yourself
  id: totrans-3776
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 尝试自己操作
- en: Time how long a single training step takes on your hardware for GPT-2 124M.
    Multiply by 1000 to estimate training time for 1000 steps.
  id: totrans-3777
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测量你的硬件上 GPT-2 124M 单个训练步的时间。乘以 1000 来估计 1000 步的训练时间。
- en: Reduce the batch size by half. Does time per step decrease linearly, or not?
  id: totrans-3778
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将批大小减半。每步时间是否线性减少，还是不是？
- en: Run a short fine-tune (e.g., 200 steps) and measure the electricity cost if
    you’re on a home machine.
  id: totrans-3779
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你在家用机器上，运行一个短期的微调（例如，200 步）并测量电力成本。
- en: Use a cloud GPU for one hour. Compare the cost and speed to your local CPU.
  id: totrans-3780
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用云 GPU 一小时。比较其成本和速度与本地 CPU。
- en: The Takeaway
  id: totrans-3781
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸取的经验
- en: Training large language models is a balancing act between ambition, hardware,
    and budget. *llm.c* gives you the tools to explore everything from toy demos to
    billion-parameter reproductions, but you’ll quickly see why the field has shifted
    toward big labs and shared infrastructure. With careful planning, though, you
    can still learn a tremendous amount by running smaller experiments and scaling
    them up thoughtfully.
  id: totrans-3782
  prefs: []
  type: TYPE_NORMAL
  zh: 训练大型语言模型需要在雄心、硬件和预算之间取得平衡。*llm.c*为你提供了从玩具演示到百亿参数复制的所有工具，但你很快就会看到为什么该领域已经转向大型实验室和共享基础设施。然而，通过周密的规划，你仍然可以通过运行较小的实验并谨慎地扩展它们来学到大量的知识。
- en: 97\. Hyperparameter Sweeps (`sweep.sh`)
  id: totrans-3783
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 97. 超参数扫描（`sweep.sh`）
- en: 'Getting a model like GPT-2 to train well isn’t just about writing the code
    or having enough compute—it’s also about finding the right hyperparameters. These
    include the learning rate, batch size, weight decay, dropout rate, and scheduler
    configuration. A setting that works well for one dataset or model size might completely
    fail for another. That’s where hyperparameter sweeps come in: systematically trying
    different configurations to see which ones give the best results.'
  id: totrans-3784
  prefs: []
  type: TYPE_NORMAL
  zh: 让GPT-2这样的模型训练良好，不仅仅是编写代码或拥有足够的计算能力——还在于找到正确的超参数。这些包括学习率、批量大小、权重衰减、dropout率以及调度器配置。一个对某个数据集或模型大小有效的设置可能对另一个完全无效。这就是超参数扫描发挥作用的地方：系统地尝试不同的配置，看看哪些能给出最佳结果。
- en: The Role of `sweep.sh`
  id: totrans-3785
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '`sweep.sh`的作用'
- en: In *llm.c*, there’s a simple shell script called `sweep.sh` designed to automate
    hyperparameter testing. It’s not a complex experiment management system like Ray
    Tune or Optuna; instead, it’s lightweight, transparent, and easy to adapt. The
    script usually looks like a loop over different learning rates or batch sizes,
    running the training executable with each setting, and logging the output.
  id: totrans-3786
  prefs: []
  type: TYPE_NORMAL
  zh: 在*llm.c*中，有一个简单的shell脚本叫做`sweep.sh`，旨在自动化超参数测试。它不是一个像Ray Tune或Optuna那样的复杂实验管理系统；相反，它是轻量级、透明且易于适应的。脚本通常看起来像是一个循环，遍历不同的学习率或批量大小，使用每个设置运行训练可执行文件，并记录输出。
- en: 'A very simplified version might look like this:'
  id: totrans-3787
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常简化的版本可能看起来像这样：
- en: '[PRE186]'
  id: totrans-3788
  prefs: []
  type: TYPE_PRE
  zh: '[PRE186]'
- en: This way, you can launch multiple experiments with a single command and later
    compare validation losses to decide which hyperparameters are best.
  id: totrans-3789
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，你可以用一条命令启动多个实验，然后比较验证损失以决定哪些超参数最佳。
- en: Why Sweeps Are Important
  id: totrans-3790
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么扫描很重要
- en: 'Training a transformer is highly sensitive to hyperparameters. For example:'
  id: totrans-3791
  prefs: []
  type: TYPE_NORMAL
  zh: 训练Transformer对超参数非常敏感。例如：
- en: If the learning rate is too high, loss might explode.
  id: totrans-3792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果学习率过高，损失可能会爆炸。
- en: If it’s too low, training will be painfully slow.
  id: totrans-3793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果过低，训练将会非常缓慢。
- en: Too much weight decay can hurt performance, while too little can cause overfitting.
  id: totrans-3794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过多的权重衰减可能会损害性能，而太少的权重衰减可能会导致过拟合。
- en: The number of warmup steps can make the difference between stable convergence
    and failure in the first few hundred iterations.
  id: totrans-3795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预热步骤的数量可以在稳定收敛和前几百次迭代失败之间产生差异。
- en: Instead of guessing, sweeps let you see patterns. For instance, you might discover
    that `3e-4` is optimal for GPT-2 124M, but GPT-2 355M prefers `2e-4`.
  id: totrans-3796
  prefs: []
  type: TYPE_NORMAL
  zh: 与猜测不同，扫描可以帮助你看到模式。例如，你可能会发现`3e-4`对于GPT-2 124M是最佳选择，但GPT-2 355M更偏好`2e-4`。
- en: Example of a Sweep in Practice
  id: totrans-3797
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实践中的扫描示例
- en: 'Suppose you want to test three learning rates and two batch sizes. You can
    write a nested loop:'
  id: totrans-3798
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想测试三个学习率和两个批量大小。你可以写一个嵌套循环：
- en: '[PRE187]'
  id: totrans-3799
  prefs: []
  type: TYPE_PRE
  zh: '[PRE187]'
- en: Afterward, you could open the logs and compare validation loss at the end of
    each run. This gives you data-driven evidence about what works best.
  id: totrans-3800
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，你可以打开日志并比较每次运行结束时的验证损失。这为你提供了关于什么工作最好的数据驱动证据。
- en: Why It Matters
  id: totrans-3801
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么它很重要
- en: Hyperparameter sweeps are a cornerstone of practical machine learning. Even
    though *llm.c* is a minimalist project, the ability to quickly test and compare
    runs is essential. It transforms training from guesswork into an empirical process.
    You don’t just hope your model will converge—you verify it across multiple settings
    and pick the winner.
  id: totrans-3802
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数扫描是实际机器学习的基础。尽管*llm.c*是一个极简主义项目，但快速测试和比较运行的能力是至关重要的。它将训练从猜测转变为经验过程。你不仅仅希望你的模型会收敛——你需要在多个设置中验证它，并选择最佳方案。
- en: Try It Yourself
  id: totrans-3803
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 试试看
- en: Run a sweep over learning rates `[1e-3, 3e-4, 1e-4]` for GPT-2 124M on Tiny
    Shakespeare. Which one converges fastest?
  id: totrans-3804
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Tiny Shakespeare上对GPT-2 124M的`[1e-3, 3e-4, 1e-4]`学习率进行扫描。哪一个收敛得最快？
- en: Try sweeping over sequence length (`T=32, 64, 128`). How does it affect speed
    and loss?
  id: totrans-3805
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试对序列长度（`T=32, 64, 128`）进行扫描。它如何影响速度和损失？
- en: Compare runs with and without weight decay. Which generalizes better to the
    validation set?
  id: totrans-3806
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较有无权重衰减的运行。哪个对验证集的泛化更好？
- en: Extend the sweep to test different schedulers (cosine vs. step decay).
  id: totrans-3807
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将扫描扩展到测试不同的调度器（余弦衰减与步长衰减）。
- en: The Takeaway
  id: totrans-3808
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸收教训
- en: Hyperparameter sweeps are the “experimentation muscle” of training LLMs. They
    teach you that no single setting works everywhere and that systematic testing
    is far more effective than intuition alone. With a simple script like `sweep.sh`,
    you can explore dozens of setups in a reproducible way and build confidence that
    your model is training as well as it can.
  id: totrans-3809
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数扫描是训练LLM的“实验肌肉”。它们教会你没有单一设置适用于所有情况，并且系统测试比直觉本身更有效。使用简单的脚本如`sweep.sh`，你可以以可重复的方式探索数十种设置，并建立信心，相信你的模型训练得尽可能好。
- en: 98\. Validating Evaluation and Loss Curves
  id: totrans-3810
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 98. 验证评估和损失曲线
- en: Training logs—loss values printed after each step—are just numbers. To really
    understand whether your model is learning, you need to validate those numbers,
    plot them, and compare them across runs. This process of analyzing evaluation
    and loss curves is one of the most important skills in machine learning. It’s
    how you know whether your model is converging, overfitting, or failing entirely.
  id: totrans-3811
  prefs: []
  type: TYPE_NORMAL
  zh: 训练日志——每一步打印的损失值只是数字。要真正了解你的模型是否在学习，你需要验证这些数字，绘制它们，并在不同的运行中进行比较。分析评估和损失曲线的过程是机器学习中最重要技能之一。这是你了解模型是否收敛、过拟合或完全失败的方法。
- en: Training Loss vs. Validation Loss
  id: totrans-3812
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练损失与验证损失
- en: 'There are two kinds of loss curves to pay attention to:'
  id: totrans-3813
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种损失曲线需要关注：
- en: 'Training loss: computed on the batches the model actually sees during training.'
  id: totrans-3814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练损失：在模型实际看到的训练批次上计算。
- en: 'Validation loss: computed on a held-out dataset (like `*_val.bin` in *llm.c*).'
  id: totrans-3815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证损失：在保留的数据集（如*llm.c*中的`*_val.bin`）上计算。
- en: 'Training loss almost always goes down steadily. Validation loss is the real
    test: if it decreases alongside training loss, the model is learning useful patterns.
    If it stalls or increases while training loss keeps dropping, the model is overfitting.'
  id: totrans-3816
  prefs: []
  type: TYPE_NORMAL
  zh: 训练损失几乎总是稳步下降。验证损失是真正的测试：如果它与训练损失一起下降，则模型正在学习有用的模式。如果它在训练损失持续下降的同时停滞或增加，则模型正在过拟合。
- en: Plotting Loss Curves
  id: totrans-3817
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 绘制损失曲线
- en: 'Even though *llm.c* is a pure C project, you can redirect its training logs
    to a file and then use tools like Python + matplotlib to visualize. For example:'
  id: totrans-3818
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管llm.c是一个纯C项目，但你可以将其训练日志重定向到文件，然后使用Python + matplotlib等工具进行可视化。例如：
- en: '[PRE188]'
  id: totrans-3819
  prefs: []
  type: TYPE_PRE
  zh: '[PRE188]'
- en: 'Then parse `logs/run1.txt` in Python:'
  id: totrans-3820
  prefs: []
  type: TYPE_NORMAL
  zh: 然后用Python解析`logs/run1.txt`：
- en: '[PRE189]'
  id: totrans-3821
  prefs: []
  type: TYPE_PRE
  zh: '[PRE189]'
- en: 'Adding validation loss to the same plot makes it even more useful: you’ll see
    both curves and their relationship over time.'
  id: totrans-3822
  prefs: []
  type: TYPE_NORMAL
  zh: 将验证损失添加到同一图表中使其更加有用：你会看到两条曲线及其随时间的关系。
- en: What a Healthy Curve Looks Like
  id: totrans-3823
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 健康曲线的样子
- en: 'Early phase: Both training and validation loss drop quickly.'
  id: totrans-3824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 早期阶段：训练和验证损失都迅速下降。
- en: 'Middle phase: Training loss continues downward, validation loss drops more
    slowly.'
  id: totrans-3825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中期阶段：训练损失持续下降，验证损失下降较慢。
- en: 'Late phase: Training loss may keep decreasing, but validation loss stabilizes
    or begins to rise. That’s the point of overfitting.'
  id: totrans-3826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 晚期阶段：训练损失可能继续下降，但验证损失稳定或开始上升。这是过拟合的点。
- en: 'For example, a good curve might look like this:'
  id: totrans-3827
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一条好的曲线可能看起来像这样：
- en: '[PRE190]'
  id: totrans-3828
  prefs: []
  type: TYPE_PRE
  zh: '[PRE190]'
- en: Training loss keeps going down, but validation loss plateaus around step 500\.
    That’s a signal to stop training or adjust hyperparameters.
  id: totrans-3829
  prefs: []
  type: TYPE_NORMAL
  zh: 训练损失持续下降，但验证损失在步骤500左右停滞。这是停止训练或调整超参数的信号。
- en: Why It Matters
  id: totrans-3830
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: Loss curves are your window into model behavior. They reveal whether your model
    is underfitting (loss too high), overfitting (gap between train and val too large),
    or training stably (both losses decreasing together). Without them, you’re flying
    blind—just staring at numbers without context.
  id: totrans-3831
  prefs: []
  type: TYPE_NORMAL
  zh: 损失曲线是了解模型行为的窗口。它们揭示了你的模型是否欠拟合（损失过高）、过拟合（训练和验证之间的差距过大）或稳定训练（两个损失值同时下降）。没有它们，你就像盲人一样——只是盯着没有上下文的数字。
- en: Try It Yourself
  id: totrans-3832
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 试试看
- en: Train GPT-2 124M for 500 steps on Tiny Shakespeare and plot both training and
    validation loss.
  id: totrans-3833
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Tiny Shakespeare上训练GPT-2 124M 500步，并绘制训练和验证损失。
- en: Reduce the dataset size by half. Watch how validation loss worsens earlier due
    to overfitting.
  id: totrans-3834
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集大小减半。观察验证损失如何因过拟合而提前恶化。
- en: Run two experiments with different learning rates. Plot both curves and compare
    stability.
  id: totrans-3835
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进行两个不同学习率的实验。绘制两条曲线并比较其稳定性。
- en: Extend plotting to multiple runs (e.g., GPT-2 124M vs. 355M) to see scaling
    effects.
  id: totrans-3836
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将绘图扩展到多个运行（例如，GPT-2 124M vs. 355M）以查看缩放效应。
- en: The Takeaway
  id: totrans-3837
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吸收要点
- en: Validating evaluation and loss curves turns raw logs into insight. It helps
    you decide when to stop training, how to tune hyperparameters, and whether scaling
    up is worth it. In *llm.c*, even though the project is minimal, capturing and
    plotting these curves is the single most effective way to understand what’s happening
    inside your model.
  id: totrans-3838
  prefs: []
  type: TYPE_NORMAL
  zh: 验证评估和损失曲线将原始日志转化为洞察。这有助于你决定何时停止训练，如何调整超参数，以及是否扩大规模是值得的。在 *llm.c* 中，尽管项目最小化，捕捉和绘制这些曲线是理解模型内部发生情况的最有效方法。
- en: '99\. Future Work: Kernel Library, Less cuDNN Dependence'
  id: totrans-3839
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 99. 未来工作：内核库，减少对 cuDNN 的依赖
- en: 'The CUDA path in *llm.c* already uses cuBLAS and cuDNN, NVIDIA’s high-performance
    math libraries, to handle the heavy lifting of matrix multiplications and attention
    operations. These libraries are battle-tested and extremely fast, but they also
    act as a “black box”: you call into them, they do the work, and you get results
    without seeing what’s inside. While this is convenient, it limits flexibility
    and makes it harder to experiment with novel optimizations.'
  id: totrans-3840
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *llm.c* 中的 CUDA 路径已经使用了 cuBLAS 和 cuDNN，这是 NVIDIA 的高性能数学库，用于处理矩阵乘法和注意力操作的重负载。这些库经过实战检验且速度极快，但它们也充当了一个“黑盒”：你调用它们，它们完成工作，你得到结果却看不到里面的内容。虽然这很方便，但它限制了灵活性，并使得进行新颖优化的实验变得更加困难。
- en: That’s why one of the most exciting areas of future work is building a lightweight
    custom kernel library for *llm.c*. This would mean replacing parts of cuDNN with
    hand-written CUDA kernels for operations like attention, normalization, and activation
    functions.
  id: totrans-3841
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，未来工作中最令人兴奋的领域之一是为 *llm.c* 构建一个轻量级的自定义内核库。这意味着用手写的 CUDA 内核替换 cuDNN 的一部分，用于执行注意力、归一化和激活函数等操作。
- en: Why Reduce cuDNN Dependence?
  id: totrans-3842
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么减少对 cuDNN 的依赖？
- en: 'Transparency: With custom kernels, you see exactly how operations are implemented,
    which is great for learning and debugging.'
  id: totrans-3843
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 透明度：使用自定义内核，你可以确切地看到操作是如何实现的，这对于学习和调试非常有用。
- en: 'Flexibility: You can experiment with new ideas (e.g., alternative attention
    mechanisms, sparsity tricks) without waiting for cuDNN support.'
  id: totrans-3844
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 灵活性：你可以实验新的想法（例如，替代注意力机制、稀疏技巧），而无需等待 cuDNN 支持。
- en: 'Portability: cuDNN is NVIDIA-specific. A custom kernel library could make it
    easier to port *llm.c* to other backends, like AMD GPUs (HIP) or even Metal for
    Apple silicon.'
  id: totrans-3845
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可移植性：cuDNN 是 NVIDIA 特有的。一个自定义内核库可以使将 *llm.c* 移植到其他后端（如 AMD GPU 的 HIP 或甚至 Apple硅的
    Metal）变得更加容易。
- en: 'Performance Tuning: For small to medium models, hand-tuned kernels can sometimes
    outperform generic library calls because they’re tailored to the workload.'
  id: totrans-3846
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 性能调优：对于小型到中型模型，手动调优的内核有时可以超越通用库调用，因为它们是根据工作负载定制的。
- en: What a Kernel Library Might Include
  id: totrans-3847
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 内核库可能包含的内容
- en: 'A first version of a kernel library for *llm.c* might implement:'
  id: totrans-3848
  prefs: []
  type: TYPE_NORMAL
  zh: 为 *llm.c* 的内核库的第一个版本可能实现以下功能：
- en: Matrix multiplication (the workhorse of transformers) using tiling and shared
    memory.
  id: totrans-3849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用分块和共享内存的矩阵乘法（变换器的动力源泉）。
- en: Softmax kernels with numerical stability built in.
  id: totrans-3850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有内置数值稳定性的 softmax 内核。
- en: LayerNorm kernels for both forward and backward passes.
  id: totrans-3851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适用于正向和反向传递的 LayerNorm 内核。
- en: Attention kernels that integrate matmul + masking + softmax in one fused operation.
  id: totrans-3852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意将 matmul + masking + softmax 集成在一个融合操作中的注意力内核。
- en: Activation functions like GELU or ReLU in fused forms.
  id: totrans-3853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 融合形式中的激活函数，如 GELU 或 ReLU。
- en: 'For example, a very simplified CUDA kernel for vector addition might look like
    this:'
  id: totrans-3854
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个用于向量加法的非常简化的 CUDA 内核可能看起来像这样：
- en: '[PRE191]'
  id: totrans-3855
  prefs: []
  type: TYPE_PRE
  zh: '[PRE191]'
- en: 'While trivial, this illustrates the idea: instead of calling a library, you
    write the math yourself and control how threads are launched and memory is accessed.'
  id: totrans-3856
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然很简单，但这说明了这样一个观点：不是调用库，而是自己编写数学公式，并控制线程的启动和内存的访问方式。
- en: The Path Forward
  id: totrans-3857
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 前进的道路
- en: Developing a kernel library is a long-term effort. It requires profiling, benchmarking,
    and iterative tuning. At first, custom kernels may be slower than cuDNN, but over
    time, they can evolve into a compact, educational library of building blocks for
    transformer training.
  id: totrans-3858
  prefs: []
  type: TYPE_NORMAL
  zh: 开发内核库是一个长期努力。它需要分析、基准测试和迭代调优。最初，自定义内核可能比 cuDNN 慢，但随着时间的推移，它们可以演变成一个紧凑的、用于变压器训练的构建块的教育库。
- en: Why It Matters
  id: totrans-3859
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: Reducing dependence on cuDNN isn’t just about performance—it’s about control
    and portability. By having your own kernels, you gain the freedom to run *llm.c*
    on more platforms, test new research ideas, and understand exactly what’s happening
    inside the GPU. For a minimal, educational project like this one, that’s a natural
    next step.
  id: totrans-3860
  prefs: []
  type: TYPE_NORMAL
  zh: 减少对 cuDNN 的依赖不仅关乎性能，还关乎控制和可移植性。通过拥有自己的内核，你将获得在更多平台上运行 *llm.c*、测试新的研究想法以及了解 GPU
    内部确切发生情况的自由。对于这样一个最小化、教育性的项目来说，这是自然的下一步。
- en: Try It Yourself
  id: totrans-3861
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 尝试自己动手做
- en: Write a simple CUDA kernel for vector addition, like the one above, and call
    it from a C program. Compare its performance to `cublasSaxpy`.
  id: totrans-3862
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个简单的 CUDA 内核，用于向量加法，就像上面的例子一样，并在 C 程序中调用它。将其性能与 `cublasSaxpy` 进行比较。
- en: Replace one small piece of *llm.c* (e.g., softmax) with a custom kernel. Check
    if the outputs match cuDNN.
  id: totrans-3863
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 *llm.c* 中的一个小的部分（例如，softmax）替换为自定义内核。检查输出是否与 cuDNN 匹配。
- en: Benchmark a custom kernel against cuDNN on a small input size. Does cuDNN still
    dominate?
  id: totrans-3864
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在小输入大小上对自定义内核与 cuDNN 进行基准测试。cuDNN 是否仍然占主导地位？
- en: Try porting your kernel to HIP (for AMD) or Metal (for Apple GPUs).
  id: totrans-3865
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试将你的内核移植到 HIP（用于 AMD）或 Metal（用于 Apple GPU）。
- en: The Takeaway
  id: totrans-3866
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 总结
- en: Building a kernel library is about moving from consumer of black-box libraries
    to creator of transparent, flexible tools. It’s a lot of work, but it transforms
    *llm.c* into not just a project for using LLMs, but also a platform for learning
    the deep internals of GPU programming. By reducing cuDNN dependence, you open
    the door to true end-to-end control of model training.
  id: totrans-3867
  prefs: []
  type: TYPE_NORMAL
  zh: 构建内核库是将消费者从黑盒库转变为透明、灵活工具的过渡。这是一项大量工作，但它将 *llm.c* 从仅仅是一个使用 LLMs 的项目转变为一个学习 GPU
    编程深层次内部结构的平台。通过减少对 cuDNN 的依赖，你打开了真正端到端控制模型训练的大门。
- en: 100\. Community, GitHub Discussions, and Suggested Learning Path
  id: totrans-3868
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 100. 社区、GitHub 讨论区和建议的学习路径
- en: Large language models are complicated, but one of the goals of *llm.c* is to
    make them accessible. The code is clean, minimal, and approachable—but learning
    doesn’t stop at reading code. The broader community around *llm.c* plays a huge
    role in helping people understand, experiment, and grow. This section highlights
    where to connect with others, how to contribute, and how to build your own learning
    journey.
  id: totrans-3869
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型很复杂，但 *llm.c* 的一个目标就是让它们易于访问。代码干净、简洁、易于接近——但学习并不仅仅止于阅读代码。围绕 *llm.c* 的更广泛社区在帮助人们理解、实验和成长方面发挥着巨大作用。本节突出了如何与他人建立联系、如何贡献以及如何构建自己的学习之旅。
- en: GitHub as the Hub
  id: totrans-3870
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: GitHub 作为中心
- en: 'The central place for *llm.c* discussions is the [GitHub repository](https://github.com/karpathy/llm.c).
    There, you’ll find:'
  id: totrans-3871
  prefs: []
  type: TYPE_NORMAL
  zh: '*llm.c* 讨论的中心地点是 [GitHub 仓库](https://github.com/karpathy/llm.c)。在那里，你会发现：'
- en: 'Issues: where users ask questions, report bugs, or propose improvements.'
  id: totrans-3872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题：用户提问、报告错误或提出改进的地方。
- en: 'Discussions: an open forum for sharing results, asking “how do I…?” questions,
    and comparing training logs.'
  id: totrans-3873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论区：一个公开的论坛，用于分享结果、提出“我该如何……？”问题以及比较训练日志。
- en: 'Pull Requests: contributions ranging from bug fixes to new features, often
    with valuable code reviews.'
  id: totrans-3874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提交拉取请求：从错误修复到新功能，通常伴随着有价值的代码审查。
- en: Even just browsing issues and discussions can be an educational experience.
    Many questions you might have—about CUDA errors, dataset preparation, or optimizer
    quirks—have already been asked and answered.
  id: totrans-3875
  prefs: []
  type: TYPE_NORMAL
  zh: 即使只是浏览问题和讨论，也可能是一种教育体验。你可能提出的许多问题——关于 CUDA 错误、数据集准备或优化器的怪癖——已经有人提出并得到了解答。
- en: The Value of Community
  id: totrans-3876
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 社区的价值
- en: 'Working alone on language models can feel overwhelming. By engaging with the
    community, you:'
  id: totrans-3877
  prefs: []
  type: TYPE_NORMAL
  zh: 单独从事语言模型工作可能会感到压力重重。通过参与社区，你：
- en: See how others run experiments with different datasets and hardware.
  id: totrans-3878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 看看其他人如何使用不同的数据集和硬件进行实验。
- en: Learn troubleshooting strategies for common problems (e.g., out-of-memory errors).
  id: totrans-3879
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习常见问题的故障排除策略（例如，内存不足错误）。
- en: Get inspiration for extensions—like custom kernels, new optimizers, or non-GPT
    architectures.
  id: totrans-3880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展的灵感——如自定义内核、新的优化器或非 GPT 架构。
- en: Find collaborators for experiments that go beyond what one person can do.
  id: totrans-3881
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 寻找合作伙伴进行超出个人能力范围的实验。
- en: Suggested Learning Path
  id: totrans-3882
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 建议的学习路径
- en: 'Because *llm.c* is minimal, it works well as a self-study tool. Here’s a suggested
    path to build up your knowledge:'
  id: totrans-3883
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 *llm.c* 是最小化的，它非常适合作为自学工具。以下是一个建议的学习路径，以构建你的知识：
- en: Start Small
  id: totrans-3884
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从小做起
- en: Train GPT-2 124M on Tiny Shakespeare using CPU-only mode.
  id: totrans-3885
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用仅 CPU 模式在 Tiny Shakespeare 上训练 GPT-2 124M。
- en: Inspect training logs and watch how loss decreases.
  id: totrans-3886
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查训练日志并观察损失如何减少。
- en: Generate text and see how quickly the model memorizes.
  id: totrans-3887
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成文本并观察模型如何快速记忆。
- en: Step Into CUDA
  id: totrans-3888
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进入CUDA
- en: Switch to `train_gpt2.cu` and train with GPU acceleration.
  id: totrans-3889
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 切换到`train_gpt2.cu`并使用GPU加速进行训练。
- en: Try mixed precision (`FP16`/`BF16`) and observe memory savings.
  id: totrans-3890
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试混合精度（`FP16`/`BF16`）并观察内存节省。
- en: Scale Up
  id: totrans-3891
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 扩展规模
- en: Attempt GPT-2 355M or 774M on your hardware (or cloud GPUs).
  id: totrans-3892
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试在你的硬件（或云GPU）上运行GPT-2 355M或774M。
- en: Learn how to use gradient accumulation and checkpointing.
  id: totrans-3893
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何使用梯度累积和检查点。
- en: Experiment
  id: totrans-3894
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实验
- en: 'Modify the training loop: try new schedulers, tweak optimizer hyperparameters.'
  id: totrans-3895
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修改训练循环：尝试新的调度器，调整优化器超参数。
- en: Add your own datasets (e.g., your personal text corpus).
  id: totrans-3896
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加你自己的数据集（例如，你个人的文本语料库）。
- en: Explore Internals
  id: totrans-3897
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 探索内部结构
- en: Step through forward and backward passes in `train_gpt2.c`.
  id: totrans-3898
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`train_gpt2.c`中逐步进行正向和反向传播。
- en: Write small experiments to isolate key concepts (e.g., LayerNorm).
  id: totrans-3899
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写小实验来隔离关键概念（例如，LayerNorm）。
- en: Join the Discussion
  id: totrans-3900
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加入讨论
- en: Share your results on GitHub Discussions.
  id: totrans-3901
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在GitHub Discussions上分享你的结果。
- en: Contribute improvements, even small ones—like documentation fixes.
  id: totrans-3902
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贡献改进，即使是小的改进——比如文档修复。
- en: Why It Matters
  id: totrans-3903
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: The journey of learning LLM internals isn’t just about reading code—it’s about
    active practice, asking questions, and comparing experiences with others. Community
    provides the feedback loop that accelerates learning and keeps motivation alive.
  id: totrans-3904
  prefs: []
  type: TYPE_NORMAL
  zh: 学习LLM内部结构的旅程不仅仅是阅读代码——它关于主动实践、提问和与他人比较经验。社区提供了加速学习和保持动力的反馈循环。
- en: Try It Yourself
  id: totrans-3905
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 亲自尝试
- en: Clone the *llm.c* repository and explore open issues. Can you answer one for
    someone else?
  id: totrans-3906
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 克隆`*llm.c*`仓库并探索开放的问题。你能为别人解答一个问题吗？
- en: Run a training experiment and share your loss curve in GitHub Discussions.
  id: totrans-3907
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行一个训练实验并在GitHub Discussions上分享你的损失曲线。
- en: Contribute a small improvement (like a new dataset script) as a pull request.
  id: totrans-3908
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过拉取请求贡献一个小改进（比如一个新的数据集脚本）。
- en: Create your own “learning log” to track experiments, much like a public notebook.
  id: totrans-3909
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建自己的“学习日志”来跟踪实验，就像一个公开的笔记本。
- en: The Takeaway
  id: totrans-3910
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 摘要
- en: '*llm.c* isn’t just a codebase—it’s an invitation to join a learning community.
    By engaging with GitHub, trying experiments, and sharing your results, you move
    from passive reader to active participant. That’s where the deepest understanding
    comes from: learning together, not alone.'
  id: totrans-3911
  prefs: []
  type: TYPE_NORMAL
  zh: '`*llm.c*`不仅仅是一个代码库——它是一个邀请你加入学习社区的邀请。通过参与GitHub、尝试实验和分享你的结果，你从被动读者转变为积极参与者。这就是最深层次的理解来源：一起学习，而不是独自一人。'
