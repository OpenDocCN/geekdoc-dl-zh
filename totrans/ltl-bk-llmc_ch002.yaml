- en: The Book
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Chapter 1\. Orientation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 1\. What *llm.c* Is
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Imagine you wanted to peek inside a modern AI model-not by reading thousands
    of lines of optimized C++ or CUDA hidden inside a giant framework, but by opening
    a small, neat folder and seeing the entire training pipeline laid out in front
    of you. That is what *llm.c* gives you.
  prefs: []
  type: TYPE_NORMAL
- en: 'At its heart, *llm.c* is a reference implementation of how to train and run
    a GPT-2 style language model, written in pure C (and CUDA). The key word is *reference*:
    the code is meant to be minimal, readable, and educational. You don’t need to
    wade through abstraction layers or device-specific macros. Instead, you get a
    version that looks almost like pseudocode, but still compiles and runs on your
    computer.'
  prefs: []
  type: TYPE_NORMAL
- en: Why This Project Exists
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Deep learning frameworks like PyTorch and TensorFlow are amazing for getting
    models to work quickly, but they hide most of the actual mechanics. Under the
    hood, there’s a lot happening: tensors are allocated in memory, gradients are
    computed through backpropagation, optimizer states are updated, and schedulers
    adjust the learning rate. Most of us never see those details, because the framework
    handles them for us.'
  prefs: []
  type: TYPE_NORMAL
- en: '*llm.c* flips this around. It says: *what if we removed the black box and showed
    you exactly how a GPT-2 model is trained, line by line?* It’s not about speed
    or production deployment. It’s about clarity, education, and demystifying how
    large language models work.'
  prefs: []
  type: TYPE_NORMAL
- en: Key Characteristics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Minimalism: The CPU version (`train_gpt2.c`) avoids complicated optimizations
    so that beginners can follow the logic. Even the CUDA version tries to stay simple,
    with only necessary calls to cuBLAS/cuDNN.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Self-contained: No external frameworks. The code defines its own tokenizer,
    dataloader, optimizer, and scheduler. Everything you need is in the repository.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Parallels to PyTorch: Each function in the C/CUDA implementation has a counterpart
    in PyTorch. The repo even ships with Python test files to prove that the outputs
    match within tolerance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step-by-step scalability: You can start with a tiny model on CPU and, once
    you understand the basics, switch to GPU, multi-GPU, or even multi-node training.
    The structure remains the same, just faster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What You Can Do With It
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Train GPT-2 from scratch: Start with a small dataset (like Tiny Shakespeare)
    and see the model learn patterns in language.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Experiment with configurations: Change number of layers, sequence length, or
    hidden size, then watch how memory and training time scale.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Learn GPU training internals: Move from CPU to CUDA, and later to multi-GPU
    with MPI/NCCL, to see how real distributed training works under the hood.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Profile performance: The repo includes profiling tools so you can measure FLOPs,
    memory bandwidth, and kernel execution times.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Reproduce big models: With enough hardware, you can actually retrain GPT-2
    124M or larger versions, using the exact same setup described in the README.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why You Should Care
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If you’ve ever used a large language model and thought *“this feels like magic”*,
    *llm.c* is your chance to peel back the curtain. You’ll see:'
  prefs: []
  type: TYPE_NORMAL
- en: How text is converted into numbers (tokenization).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How those numbers are fed into matrices and transformed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How attention actually looks in code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How gradients are computed and weights updated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How scaling from one GPU to many is managed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s not magic anymore-it’s math plus code you can read.
  prefs: []
  type: TYPE_NORMAL
- en: The Big Picture
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Think of *llm.c* as a teaching lab for modern AI:'
  prefs: []
  type: TYPE_NORMAL
- en: On day one, you can run a small demo on CPU and see loss values dropping as
    the model learns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On day ten, you might be profiling CUDA kernels or debugging NCCL synchronization
    across GPUs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At every step, the code remains approachable because it avoids unnecessary complexity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This philosophy makes *llm.c* unique: it’s not trying to be the fastest or
    the most feature-rich. It’s trying to be the clearest window into how large language
    models actually work when stripped down to their essentials.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Takeaway: *llm.c* is a minimal, transparent re-implementation of GPT-2
    training and inference, designed to teach you how language models operate under
    the hood. It’s not about production performance-it’s about giving you a guided
    tour through the machinery of modern AI, one C file at a time.'
  prefs: []
  type: TYPE_NORMAL
- en: Would you like me to now expand Section 2 (“Repository Tour”) in the same friendly
    beginner tone, showing what’s inside each folder and file?
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Repository Tour
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When you first open the *llm.c* repository, it may look like a random collection
    of `.c`, `.cu`, `.py`, and shell scripts. But there’s a clear structure hiding
    underneath. Each part has a purpose, and once you know where things live, navigating
    the repo feels natural. Let’s take a guided walk through the main pieces.
  prefs: []
  type: TYPE_NORMAL
- en: 'Root Directory: The Entry Points'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'At the top level, you’ll find the core training programs. These are the files
    you actually compile and run:'
  prefs: []
  type: TYPE_NORMAL
- en: '`train_gpt2.c` - The CPU reference implementation. This is the simplest, most
    readable version of GPT-2 training. It avoids special optimizations so you can
    follow the math and logic step by step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_gpt2.cu` - The CUDA implementation. Faster, uses GPU kernels, cuBLAS,
    and optional cuDNN FlashAttention. This is the version you’d use for serious training
    runs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_gpt2_fp32.cu` - A legacy CUDA path, using plain FP32 precision instead
    of mixed precision. It’s slower but useful as a debugging baseline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_gpt2.py` - The PyTorch reference. This is the oracle: a tiny script
    in Python/PyTorch that trains the same GPT-2 so you can compare outputs and verify
    correctness.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Other important root-level files:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Makefile` - Defines how to build different versions. Targets like `make train_gpt2`
    or `make train_gpt2cu` are your entry points.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`README.md` - The main guide for running experiments, installing dependencies,
    and reproducing models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`llmc/` Directory: Utilities and Building Blocks'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This folder holds reusable C utilities that the main training files include:'
  prefs: []
  type: TYPE_NORMAL
- en: '`utils.h` - Safety wrappers (`fopenCheck`, `mallocCheck`) and helper functions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer.h` - Implements GPT-2’s tokenizer in C: encoding text into token
    IDs and decoding back to text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dataloader.h` - Defines how training batches are loaded and served, handling
    dataset splits and iteration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rand.h` - Random number utilities, mirroring PyTorch’s `manual_seed` and normal
    distributions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`schedulers.h` - Learning rate scheduling, like cosine decay with warmup.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sampler.h` - Implements softmax sampling for text generation and helper RNG.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logger.h` - Minimal logging functionality for tracking progress.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Think of `llmc/` as the library that keeps the main files clean and readable.
    Instead of cluttering `train_gpt2.c` with helpers, everything is modularized here.
  prefs: []
  type: TYPE_NORMAL
- en: '`dev/` Directory: Scripts and Extras'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This folder is full of supporting tools that make experiments easier:'
  prefs: []
  type: TYPE_NORMAL
- en: '`dev/download_starter_pack.sh` - Fetches the GPT-2 124M weights, tokenizer,
    and datasets. This is the quickest way to get started.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dev/data/` - Contains scripts for preparing datasets like Tiny Shakespeare
    or OpenWebText in the binary format that *llm.c* expects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dev/cuda/` - A place for experimenting with standalone CUDA kernels. This
    is where you’d go if you want to tinker with custom GPU code beyond the main trainer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`doc/` Directory: Learning Resources'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Documentation that digs deeper into specific topics. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '`doc/layernorm/layernorm.md` - A tutorial-style explanation of Layer Normalization,
    complete with math and code. It helps you understand one of GPT-2’s core components
    before diving into the C implementation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This folder is a learning aid. Whenever a concept feels too dense, check here
    for a more gentle walkthrough.
  prefs: []
  type: TYPE_NORMAL
- en: Test Files
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Testing is taken seriously in *llm.c*, because the goal is to prove that the
    C/CUDA implementation is correct compared to PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '`test_gpt2.c` - Runs forward passes and training steps on CPU and compares
    outputs to PyTorch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`test_gpt2cu.cu` - Same idea but for CUDA, including both FP32 and mixed-precision
    runs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These files keep everything honest: you can always verify that your build produces
    the same results as the canonical PyTorch model.'
  prefs: []
  type: TYPE_NORMAL
- en: Profiling Tools
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For performance deep dives:'
  prefs: []
  type: TYPE_NORMAL
- en: '`profile_gpt2.cu` - A CUDA profiling harness that benchmarks kernels and measures
    throughput.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`profile_gpt2cu.py` - Python-side profiler for analyzing GPU utilization, memory
    bandwidth, and FLOPs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you’re curious about where time is being spent in training, these files show
    you how to measure it.
  prefs: []
  type: TYPE_NORMAL
- en: Datasets and Artifacts
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When you run `download_starter_pack.sh`, you’ll get:'
  prefs: []
  type: TYPE_NORMAL
- en: '`gpt2_tokenizer.bin` - GPT-2’s byte-pair encoding tokenizer, serialized in
    binary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset `.bin` files - Training and validation sets, tokenized and ready for
    the dataloader.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These files are not in the repo by default but are downloaded or generated locally.
  prefs: []
  type: TYPE_NORMAL
- en: Putting It Together
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The repository is structured like a teaching lab:'
  prefs: []
  type: TYPE_NORMAL
- en: Root files are the main experiments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`llmc/` is the library of building blocks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dev/` provides extra tools and scripts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`doc/` explains tricky concepts in tutorial form.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tests and profilers make sure everything matches PyTorch and runs efficiently.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once you see the pattern, the repo feels less intimidating. Every file has a
    role in telling the story of how a GPT-2 model is built from scratch in C and
    CUDA.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Makefile Targets & Flags
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Every C or CUDA program needs a build system, and in *llm.c* that role is handled
    by a simple but powerful Makefile. If you’ve never used `make` before, think of
    it as a recipe book: you type `make <target>` in your terminal, and it follows
    the instructions for compiling the code into an executable. In *llm.c*, this file
    is your control center for choosing which trainer to build, whether to enable
    GPUs, and which optional features to turn on.'
  prefs: []
  type: TYPE_NORMAL
- en: Why a Makefile?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Instead of memorizing long `gcc` or `nvcc` compile commands with dozens of
    flags, the Makefile captures those instructions once and gives them a short name.
    For example, building the CPU trainer is as easy as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Behind the scenes, this calls `gcc`, sets optimization flags, includes the right
    headers, and links everything together. The same applies to CUDA builds with `nvcc`.
  prefs: []
  type: TYPE_NORMAL
- en: Core Targets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here are the most important build targets you’ll find:'
  prefs: []
  type: TYPE_NORMAL
- en: '`train_gpt2` - Builds the CPU-only reference trainer. Uses `gcc` (or `clang`)
    and links against OpenMP for parallel loops.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_gpt2cu` - Builds the CUDA trainer with mixed precision and optional
    cuDNN FlashAttention. Uses `nvcc`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_gpt2_fp32` - Builds the legacy CUDA trainer that stays in pure FP32
    (slower but simpler).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`test_gpt2` - Compiles the CPU test program to compare results against PyTorch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`test_gpt2cu` - Compiles the CUDA test program to check GPU parity with PyTorch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`profile_gpt2.cu` - Compiles the CUDA profiler harness, used to benchmark kernels
    and FLOPs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each of these produces a binary you can run directly, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Key Flags You Can Toggle
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The Makefile also exposes several switches that let you customize the build.
    You set them when running `make`, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the most important flags:'
  prefs: []
  type: TYPE_NORMAL
- en: '`USE_CUDNN` - Enables cuDNN FlashAttention if your system has cuDNN installed.
    This can give big speedups for attention, but it’s optional. By default, it’s
    off.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OMP=1` - Tells the CPU trainer to compile with OpenMP enabled. This allows
    multithreaded execution, making CPU runs much faster. Usually on by default if
    OpenMP is detected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DEBUG=1` - Compiles with debugging symbols (`-g`) instead of maximum optimization.
    Useful when stepping through code in an IDE or using a debugger.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PROFILE=1` - Adds profiling hooks, helping you analyze execution time and
    performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimization Choices
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The default build uses `-O3` optimization, which makes the code run fast but
    sometimes harder to debug. If you’re just learning and want clarity, you can switch
    to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This creates a binary that runs slower but lets you step through line by line
    in a debugger. For performance benchmarking, stick with the optimized default.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-GPU and MPI Support
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When building the CUDA trainer, the Makefile can also link against MPI and NCCL
    if they’re installed. That’s what enables multi-GPU and multi-node training. You
    usually don’t need to change anything-the Makefile automatically detects these
    libraries and includes them if available.
  prefs: []
  type: TYPE_NORMAL
- en: Putting It All Together
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Think of the Makefile as a switchboard for the whole project:'
  prefs: []
  type: TYPE_NORMAL
- en: Want to run the simple CPU demo? → `make train_gpt2`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Want to train faster on GPU? → `make train_gpt2cu`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Want to debug kernels? → `make train_gpt2cu DEBUG=1`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Want to test parity with PyTorch? → `make test_gpt2` or `make test_gpt2cu`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With just a few keystrokes, you control whether you’re running a beginner-friendly
    CPU demo, a high-performance GPU build, or a debugging session.
  prefs: []
  type: TYPE_NORMAL
- en: 'The takeaway: The Makefile is your control center. It abstracts away complicated
    compiler commands and gives you a clean menu of options: CPU vs GPU, FP32 vs mixed
    precision, debug vs optimized, and single vs multi-GPU. Mastering it is the first
    step to feeling comfortable experimenting inside *llm.c*.'
  prefs: []
  type: TYPE_NORMAL
- en: '4\. Quickstart: CPU Reference Path (`train_gpt2.c`)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The simplest way to begin exploring *llm.c* is with the CPU-only reference implementation.
    This file, `train_gpt2.c`, is deliberately designed to be minimal, readable, and
    approachable. It doesn’t hide complexity behind libraries or macros. Instead,
    it shows you exactly how a GPT-2 model is trained, step by step, using plain C
    and a sprinkle of OpenMP for speed.
  prefs: []
  type: TYPE_NORMAL
- en: Why Start with CPU?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Clarity first: GPUs add layers of complexity (CUDA kernels, memory transfers,
    cuBLAS). On CPU, you can focus on the core algorithm without distraction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Portability: Any machine with a C compiler can run it-no special hardware required.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Debuggability: Errors are easier to trace, and you can single-step through
    the code in an IDE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The CPU version is slower, but that’s a feature here-it forces you to really
    see what’s happening under the hood.
  prefs: []
  type: TYPE_NORMAL
- en: Building the CPU Trainer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'From the root of the repository, you just type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This compiles `train_gpt2.c` into an executable named `train_gpt2`. If your
    system has OpenMP, the Makefile will detect it and add the right flags.
  prefs: []
  type: TYPE_NORMAL
- en: Running Your First Training Run
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Before running, download the starter pack (tokenizer, dataset, configs):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now launch training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'You’ll see output like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Each line tells you:'
  prefs: []
  type: TYPE_NORMAL
- en: Model size and config (sequence length, vocabulary size, layers, heads, channels).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset stats (how many batches for training and validation).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Activation memory size (a measure of how big the intermediate states are).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training progress (step number, train loss, validation loss, time per step).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inside the Training Loop
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Although you don’t need to dive into the code yet, here’s the high-level flow
    in `train_gpt2.c`:'
  prefs: []
  type: TYPE_NORMAL
- en: Load tokenizer and dataset → turns text into tokens.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize model parameters → embeddings, attention weights, MLPs, norms.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each batch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Forward pass → compute logits and loss.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Backward pass → compute gradients.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Update parameters → optimizer step.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Log progress → print losses, occasionally run validation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This mirrors exactly what happens in PyTorch, just spelled out in C.
  prefs: []
  type: TYPE_NORMAL
- en: Performance Notes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'On CPU, don’t expect speed. Training GPT-2 124M can take days or weeks. But
    that’s not the point. The CPU reference path is like a glass box: everything is
    visible, no shortcuts. You’ll use this to learn the mechanics and to verify that
    your GPU runs match the same results.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to speed things up slightly, you can:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Increase OpenMP threads:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Use a smaller dataset (Tiny Shakespeare) to see faster progress.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduce model size by changing config values (fewer layers, smaller channels).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When to Move On
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Once you’re comfortable with how training looks on CPU-loss values going down,
    checkpoints being written, logs appearing-you’ll be ready to graduate to the GPU
    version (`train_gpt2.cu`). That’s where performance and scaling come in, but the
    CPU run gives you the conceptual foundation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The takeaway: Running `train_gpt2.c` is your first hands-on encounter with
    GPT-2 training in *llm.c*. It’s slow, transparent, and designed for learning.
    You’ll see every piece of the model at work, one step at a time, before diving
    into the complexity of CUDA.'
  prefs: []
  type: TYPE_NORMAL
- en: '5\. Quickstart: 1-GPU Legacy Path (`train_gpt2_fp32.cu`)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once you’ve seen the CPU trainer in action, the natural next step is to try
    training on a GPU. The file `train_gpt2_fp32.cu` is the simplest GPU entry point.
    It predates the more advanced mixed-precision trainer (`train_gpt2.cu`), and it
    runs everything in full 32-bit floating point (FP32) precision. That makes it
    easier to follow and debug, even though it’s slower than modern approaches. Think
    of it as the “training wheels” for GPU training in *llm.c*.
  prefs: []
  type: TYPE_NORMAL
- en: Why This Path Exists
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Modern GPU training almost always uses mixed precision (FP16/BF16 for speed
    and memory savings, FP32 for stability). But mixed precision introduces extra
    complexity: scaling losses, maintaining master weights, checking for overflows.
    For beginners, all that can be distracting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The FP32 path avoids those complications:'
  prefs: []
  type: TYPE_NORMAL
- en: Every tensor (activations, weights, gradients) is stored as 32-bit floats.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No special handling of loss scaling is needed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Debugging mismatches with PyTorch is straightforward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The trade-off is performance-this version runs significantly slower and uses
    more memory.
  prefs: []
  type: TYPE_NORMAL
- en: Building the FP32 CUDA Trainer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'From the root of the repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This invokes `nvcc` (the NVIDIA CUDA compiler) and links against cuBLAS for
    matrix multiplications. The output is an executable named `train_gpt2_fp32`.
  prefs: []
  type: TYPE_NORMAL
- en: Running It
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Just like the CPU version, make sure you’ve downloaded the starter pack first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Then launch training on your GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: If CUDA is installed correctly, the program will detect your GPU and start training.
    You’ll see logs that look similar to the CPU trainer’s, but with much shorter
    step times. For example, a training step that took ~2 seconds on CPU might take
    ~50 milliseconds on GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Under the Hood
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Although the training loop looks the same on the surface, a lot changes under
    the hood when running on GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: Tensors are allocated in GPU memory (not system RAM).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Matrix multiplications (the core of attention and MLP layers) are executed by
    cuBLAS, NVIDIA’s high-performance linear algebra library.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kernels for elementwise operations (like adding residuals, applying softmax,
    or normalizing) are written in CUDA or use built-in primitives.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gradients and optimizer states are updated entirely on the device, with minimal
    CPU↔︎GPU transfers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This makes training dramatically faster, but the structure of the code is still
    recognizable compared to the CPU version.
  prefs: []
  type: TYPE_NORMAL
- en: When to Use FP32 vs Mixed Precision
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Use FP32 (this path) when:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You’re learning how GPU training works step by step.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You want a clean comparison with the CPU trainer.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You’re debugging correctness issues without worrying about loss scaling.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Use Mixed Precision (`train_gpt2.cu`) when:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You want real performance (2–4× faster training).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You’re training larger models (774M, 1.6B parameters) where memory efficiency
    matters.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You’re aiming to reproduce published GPT-2 runs on modern GPUs.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Common Pitfalls
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: CUDA not installed → If `nvcc` isn’t found, the Makefile will fail. You’ll need
    the CUDA Toolkit installed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Driver mismatch → Your NVIDIA driver must match the CUDA version.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Out of memory errors → FP32 uses more GPU memory, so you may need to lower batch
    size if you’re on a smaller GPU.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why This Step Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The FP32 trainer is like a bridge:'
  prefs: []
  type: TYPE_NORMAL
- en: On one side is the CPU reference path, slow but crystal-clear.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the other is the mixed-precision CUDA path, fast but more complex.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By walking across this bridge, you learn how GPU acceleration works without
    being overwhelmed by optimizations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The takeaway: `train_gpt2_fp32.cu` is your first taste of real GPU training
    in *llm.c*. It skips advanced tricks and shows you a clean, one-GPU, full-precision
    implementation. It’s not the fastest, but it’s the friendliest way to understand
    how training moves from CPU to GPU.'
  prefs: []
  type: TYPE_NORMAL
- en: '6\. Quickstart: Modern CUDA Path (`train_gpt2.cu`)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is the high-performance trainer most people use day to day. It runs on
    a single NVIDIA GPU (and also forms the basis for multi-GPU), uses mixed precision
    (FP16/BF16 where safe, FP32 where needed), and can optionally enable cuDNN FlashAttention
    for fast attention. Compared to the FP32 legacy path, it’s significantly faster
    and uses less memory, while keeping the training loop easy to follow.
  prefs: []
  type: TYPE_NORMAL
- en: What “mixed precision” means (in plain words)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Weights & activations: stored/processed in FP16 or BF16 for speed and lower
    memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Master weights: a FP32 copy of parameters kept for stable updates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Loss scaling: multiply the loss before backward to avoid underflow; unscale
    the grads before the optimizer step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Autocast-like behavior: the code picks safe dtypes for each op (GEMMs in tensor
    cores, reductions in FP32, etc.).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You get 2–4× speedups on many GPUs, and the same final accuracy when configured
    properly.
  prefs: []
  type: TYPE_NORMAL
- en: Build the modern CUDA trainer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Common variants:'
  prefs: []
  type: TYPE_NORMAL
- en: 'With cuDNN FlashAttention (if available):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With debug symbols (slower, but easier to step through):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This produces an executable named `train_gpt2cu`.
  prefs: []
  type: TYPE_NORMAL
- en: One-time data & artifacts
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If you haven’t already:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This fetches the tokenizer and a small dataset so you can run immediately.
  prefs: []
  type: TYPE_NORMAL
- en: Run your first GPU training session
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: You should see a config header (model dims, vocab, sequence length) followed
    by step-by-step loss prints. Step times will be much shorter than CPU and noticeably
    faster than FP32, especially on tensor-core GPUs (Turing and newer).
  prefs: []
  type: TYPE_NORMAL
- en: 'Speed tips right away:'
  prefs: []
  type: TYPE_NORMAL
- en: Use a larger global batch if memory allows-it improves GPU utilization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Set environment threads for any CPU preprocessing:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: What’s different under the hood vs. FP32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Tensor cores: GEMMs run in FP16/BF16 paths via cuBLAS/cuBLASLt for big throughput.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scaled loss & unscale pass: Forward computes the loss, multiplies it by a scale
    factor; backward divides gradients by the same factor before updates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Master FP32 copy: Optimizer (AdamW) updates this copy, then casts back to low
    precision for the next forward.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fused/fast attention (optional): With `USE_CUDNN=1`, attention may route through
    cuDNN FlashAttention backends.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You still recognize the same loop: load batch → forward → loss → backward →
    AdamW step → log.'
  prefs: []
  type: TYPE_NORMAL
- en: Choosing FP16 vs. BF16
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'FP16: best speed, needs loss scaling; widely supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BF16: more numerically forgiving (often needs little/no scaling), requires
    hardware support (Ampere+); slightly larger memory than FP16 but often simpler.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The trainer picks what your GPU supports or what the code defaults to; you can
    expose a flag later if you want to force one.
  prefs: []
  type: TYPE_NORMAL
- en: Common command patterns
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Small GPU (less VRAM):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Faster warmup with cosine schedule:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Periodic eval to sanity-check:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: (Flag names above mirror typical patterns; adjust to match the binary’s printed
    help.)
  prefs: []
  type: TYPE_NORMAL
- en: Validating correctness (highly recommended)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Run the CUDA test binary to compare against the PyTorch reference on small
    batches:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Check that logits/loss match within a small tolerance. If mismatches happen,
    recompile without optimizations or disable cuDNN fast paths (`USE_CUDNN=0`) to
    isolate the issue.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enabling FlashAttention (when available)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Good signs: faster attention time and lower step latency. If you hit build/runtime
    errors, ensure your CUDA, cuDNN, and driver versions are compatible; fall back
    to `USE_CUDNN=0` while you sort it out.'
  prefs: []
  type: TYPE_NORMAL
- en: Memory & performance tuning checklist
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Batching: Increase `micro_batch_size` until you reach ~90% GPU utilization
    without OOM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sequence length: Longer sequences increase compute quadratically in attention;
    reduce `--seq_len` if memory is tight.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Grad accumulation: Keep global batch size large by accumulating over multiple
    micro-batches.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pinned host memory & async copies: Already used where sensible; keep CPU↔︎GPU
    transfers minimal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Profiler: Once it runs, profile hotspots to confirm GEMMs dominate (as expected)
    and attention isn’t a bottleneck unless FlashAttention is off.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Troubleshooting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`cudaErrorNoKernelImageForDevice`: Toolkit too new/old for your GPU; rebuild
    with proper `-arch=` or update drivers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CUBLAS_STATUS_ALLOC_FAILED` / OOM: Lower batch size, sequence length, or switch
    to BF16 if supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Diverging loss with FP16: Increase loss scale (if configurable) or try BF16;
    confirm master-weight updates are in FP32.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'cuDNN errors: Rebuild without `USE_CUDNN` to verify the base path works, then
    revisit versions/paths.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The takeaway: `train_gpt2.cu` is the practical, fast trainer: mixed precision,
    optional FlashAttention, and ready to scale. You keep the same readable training
    loop while tapping your GPU’s tensor cores for large speedups and much better
    memory efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Starter Artifacts & Data Prep (`dev/download_starter_pack.sh`, `dev/data/`)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before you can actually train or test a model in *llm.c*, you need a few essential
    artifacts: the tokenizer, a dataset, and a config. These files aren’t stored in
    the repo directly (they’re too large and often under different licenses), so the
    project provides scripts to fetch or generate them. This is where the `dev/` folder
    comes into play.'
  prefs: []
  type: TYPE_NORMAL
- en: The Starter Pack Script
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The easiest way to get going is with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This script pulls down a ready-made bundle containing:'
  prefs: []
  type: TYPE_NORMAL
- en: '`gpt2_tokenizer.bin` - The GPT-2 byte-pair encoding (BPE) tokenizer in binary
    format.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train.bin` / `val.bin` - Pre-tokenized training and validation datasets, often
    based on OpenWebText or Tiny Shakespeare for demos.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model configs - A JSON or header file that sets hyperparameters like layers,
    hidden size, and number of heads for GPT-2 124M.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Think of this as your “starter kit”: it contains just enough to run a demo
    and see training loss decreasing without setting up a full-scale dataset pipeline
    yourself.'
  prefs: []
  type: TYPE_NORMAL
- en: The Tokenizer File (`gpt2_tokenizer.bin`)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This is a binary representation of GPT-2’s tokenizer vocabulary. It maps raw
    text (like `"Hello world"`) into integer token IDs, which are the actual inputs
    to the model.
  prefs: []
  type: TYPE_NORMAL
- en: Why binary? It’s faster to load in C than parsing a text-based vocabulary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Size? ~500 KB, representing ~50,000 tokens.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Role in training? Used in both the dataloader (to prepare inputs) and the sampler
    (to decode outputs).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Without this file, the model can’t understand text at all-it would just be manipulating
    meaningless numbers.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset Files (`train.bin`, `val.bin`)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Each dataset file is a binary blob containing:'
  prefs: []
  type: TYPE_NORMAL
- en: A header (about 1 KB) describing sequence length, vocab size, and other metadata.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A stream of token IDs (`uint16`), representing the text corpus already tokenized.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This design means the C dataloader can simply `fread()` chunks of tokens into
    memory, without needing to tokenize text on the fly. It’s fast and memory-efficient,
    perfect for a lean project like *llm.c*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The script usually fetches two versions:'
  prefs: []
  type: TYPE_NORMAL
- en: Training set (`train.bin`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validation set (`val.bin`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That way, the training loop can occasionally switch to validation mode and report
    a validation loss, helping you track overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: The `dev/data/` Folder
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If you want to generate your own datasets, this is where you’ll find the tools:'
  prefs: []
  type: TYPE_NORMAL
- en: Scripts for Tiny Shakespeare, OpenWebText, or other corpora.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilities to tokenize text using the GPT-2 tokenizer and write out the `.bin`
    format.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Small Python snippets to check dataset statistics (like number of tokens or
    average sequence length).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, if you wanted to try fine-tuning GPT-2 on your own text files,
    you’d:'
  prefs: []
  type: TYPE_NORMAL
- en: Run a preprocessing script in `dev/data/` to tokenize and save your corpus.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Point `train_gpt2.c` or `train_gpt2.cu` to your new `train.bin` and `val.bin`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kick off training as usual.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why Preprocessing Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Tokenization and dataset preparation can be surprisingly heavy in Python, especially
    for large corpora. By precomputing everything into compact `.bin` files, *llm.c*
    keeps the runtime training loop as simple as possible-just reading arrays of integers
    and feeding them into the model.
  prefs: []
  type: TYPE_NORMAL
- en: This separation of concerns (preprocessing vs. training) is what makes the training
    code clean and focused.
  prefs: []
  type: TYPE_NORMAL
- en: Quick Sanity Check
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'After running `download_starter_pack.sh`, you should see these files in your
    working directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: If any are missing, re-run the script. Without them, the trainer will exit with
    a file-not-found error.
  prefs: []
  type: TYPE_NORMAL
- en: 'The takeaway: The starter pack is your ticket to running *llm.c* right away.
    It gives you a tokenizer and datasets in exactly the format the C code expects.
    Later, when you’re ready to train on your own text or scale up, the `dev/data/`
    folder shows you how to prepare custom datasets the same way.'
  prefs: []
  type: TYPE_NORMAL
- en: 8\. Debugging Tips & IDE Stepping (`-g`)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Even though *llm.c* is designed to be small and readable, training a transformer
    model is still a big program with lots of moving parts. When something goes wrong-whether
    it’s a segmentation fault, `NaN` losses, or unexpected results-you’ll want to
    be able to debug effectively. That’s where debug builds and IDE stepping come
    in.
  prefs: []
  type: TYPE_NORMAL
- en: Why Debug Mode Exists
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'By default, the Makefile compiles with heavy optimization (`-O3`). That makes
    the code run fast, but it also makes debugging harder:'
  prefs: []
  type: TYPE_NORMAL
- en: Variables may be optimized away.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Functions might get inlined so you can’t step through them clearly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The debugger may jump around unpredictably.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding the `-g` flag (enabled with `DEBUG=1`) tells the compiler to include
    extra information in the binary so you can see exactly what the code is doing
    at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Debug Binary
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To build with debug info:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces a slower executable, but one that works seamlessly with tools
    like:'
  prefs: []
  type: TYPE_NORMAL
- en: gdb - the classic GNU debugger.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: lldb - default on macOS.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VS Code / CLion / Xcode - IDEs with integrated debuggers and GUI interfaces.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using gdb on Linux/macOS
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Start your program under gdb:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Inside gdb:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the program: `run`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Set a breakpoint at `main`: `break main`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step through line by line: `step` or `next`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Inspect variables: `print loss`, `print i`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Quit: `quit`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the fastest way to see exactly where a crash happens.
  prefs: []
  type: TYPE_NORMAL
- en: Using an IDE
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If command-line debugging feels intimidating, you can use an IDE like VS Code
    or CLion:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the project folder.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configure the debugger (choose `gdb` or `lldb` backend).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add breakpoints by clicking next to line numbers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run the debug build (`train_gpt2` with `DEBUG=1`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Step through forward pass, backward pass, or optimizer updates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This way, you can visually watch variables update with each step.
  prefs: []
  type: TYPE_NORMAL
- en: Debugging CUDA Code
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'CUDA debugging is a bit trickier, but still possible:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cuda-gdb` - NVIDIA’s GPU debugger, works like gdb but supports stepping into
    kernels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nsight Systems / Nsight Compute - graphical profilers/debuggers that let you
    trace kernel launches, memory transfers, and GPU utilization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your CUDA code crashes with cryptic messages like `illegal memory access`,
    `cuda-gdb` can help pinpoint the kernel and even the exact line.
  prefs: []
  type: TYPE_NORMAL
- en: Debugging Common Issues in llm.c
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: File not found → Make sure `gpt2_tokenizer.bin`, `train.bin`, and `val.bin`
    are downloaded.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Segfault at malloc/fread → Check file paths and dataset sizes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Loss becomes NaN →
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On CPU: check for division by zero in normalization.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On GPU: check loss scaling (mixed precision) or try FP32 path for comparison.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Mismatch with PyTorch tests → Run `test_gpt2` or `test_gpt2cu` and compare outputs;
    this usually isolates whether the bug is in forward pass, backward pass, or optimizer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Logging & Sanity Checks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When debugging, it helps to add extra logging. The repo already has a lightweight
    logger, but you can also sprinkle `printf`s (on CPU) or `cudaDeviceSynchronize();
    printf(...)` (on GPU) to track values. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Sometimes the quickest fix is just to print and see what’s going on.
  prefs: []
  type: TYPE_NORMAL
- en: Best Practices for Beginners
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Start with the CPU build when learning-it’s easier to debug than CUDA.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Always keep a small dataset (like Tiny Shakespeare) for fast iteration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compare against the PyTorch reference for the same batch to catch subtle errors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use `DEBUG=1` whenever you hit strange behavior-you’ll trade speed for clarity,
    which is usually worth it when learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The takeaway: Debug builds (`-g`) turn *llm.c* from a black box into a step-through
    learning tool. With gdb, lldb, or an IDE, you can pause at any line, inspect variables,
    and understand exactly how GPT-2 training works inside C or CUDA. It’s slower,
    but it’s the clearest way to learn and fix issues.'
  prefs: []
  type: TYPE_NORMAL
- en: 9\. Project Constraints & Readability Contract
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The *llm.c* project isn’t trying to be the fastest or most feature-rich GPT-2
    trainer. Instead, it has a very deliberate set of constraints-rules the author
    imposes on the codebase to keep it approachable and educational. You can think
    of these as the “contract” between the code and the reader: certain things are
    kept simple on purpose, even if they cost some performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Minimalism over Optimizations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'No processor-specific intrinsics: You won’t see AVX, NEON, or other hardware-tuned
    assembly calls in the CPU path.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'No fancy template metaprogramming: Unlike in C++ frameworks, here you get plain
    C structs and functions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'No exotic libraries: Aside from cuBLAS/cuDNN for GPU acceleration, most functionality
    is implemented directly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This means the code runs almost anywhere, and you don’t need to understand deep
    compiler tricks to follow what’s going on.
  prefs: []
  type: TYPE_NORMAL
- en: Transparency over Abstraction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Every operation is visible in the source. For example, instead of calling a
    framework function like `nn.CrossEntropyLoss`, you’ll find an explicit forward
    and backward pass coded in C.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data loading, tokenization, optimizer steps, and schedulers are all implemented
    as separate, small modules in `llmc/`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You don’t need to guess what’s happening-if you’re curious, you can open the
    corresponding `.h` file and see the exact code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The guiding idea: if something is central to training GPT-2, you should be
    able to read and understand it.'
  prefs: []
  type: TYPE_NORMAL
- en: Performance Where It Matters (but No More)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: OpenMP pragmas are allowed in CPU builds, because they give large speedups with
    minimal extra code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: cuBLAS/cuDNN are used for GPU matmuls and attention, because re-implementing
    them would be a distraction and would make the project impossibly large.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But the project avoids unnecessary complexity-no kernel fusion, no elaborate
    caching layers, no half-implemented “framework” abstractions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This balance ensures you can still run experiments at a reasonable speed, but
    the code never sacrifices readability.
  prefs: []
  type: TYPE_NORMAL
- en: Educational First
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The code is written to teach, not to win benchmarks. That means:'
  prefs: []
  type: TYPE_NORMAL
- en: Variable names are descriptive, not cryptic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comments explain not just *what* happens, but also *why*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Files are kept small and focused, rather than sprawling across dozens of layers
    of abstraction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There’s a matching PyTorch reference implementation so you can always check
    your understanding against a familiar baseline.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limitations You Should Expect
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Training is slower than PyTorch/XLA/JAX or DeepSpeed-tuned runs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-GPU scaling is functional but not heavily optimized.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Only GPT-2 architectures are covered-don’t expect GPT-3 or transformer variants.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Features like dataset streaming, checkpoint sharding, or advanced distributed
    tricks are intentionally left out.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are not bugs-they’re conscious trade-offs to keep the codebase small,
    sharp, and didactic.
  prefs: []
  type: TYPE_NORMAL
- en: Why This Matters for You
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If you’re learning how transformers work, this contract is a gift:'
  prefs: []
  type: TYPE_NORMAL
- en: You won’t get lost in performance hacks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You won’t fight through an abstraction jungle.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You’ll always know that what you’re reading is close to the “pure” algorithmic
    idea.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the flip side, if you’re aiming for production-grade speed, you’ll need to
    layer more on top. But that’s outside the mission of *llm.c*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The takeaway: *llm.c* is bound by a readability contract: clarity over raw
    speed, transparency over abstraction, minimalism over complexity. These constraints
    keep the project small enough to fit in your head, while still powerful enough
    to reproduce GPT-2 training. It’s a teaching lab, not a racing car-and that’s
    exactly why it’s valuable.'
  prefs: []
  type: TYPE_NORMAL
- en: 10\. Community, Discussions, and Learning Path
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The last piece of the quickstart isn’t about code at all-it’s about the people
    and resources around the project. *llm.c* has grown into more than just a single
    repository; it has become a meeting point for learners, tinkerers, and researchers
    who want to strip large language models down to their essentials. Understanding
    this community layer is just as important as understanding the code itself.
  prefs: []
  type: TYPE_NORMAL
- en: Discussions and Issues on GitHub
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The project’s Discussions tab is full of valuable context:'
  prefs: []
  type: TYPE_NORMAL
- en: Developers asking about build errors on different platforms (Linux, macOS, Windows).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explorations of how to extend *llm.c* to train larger GPT-2 models (355M, 774M,
    1.6B).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reports on multi-GPU and MPI runs, including troubleshooting NCCL hangs and
    performance bottlenecks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Debates on mixed precision vs FP32 vs BF16 stability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading these threads is like looking over the shoulders of hundreds of other
    learners. You’ll see not only the official answers but also the thought process
    of people solving problems in real time.
  prefs: []
  type: TYPE_NORMAL
- en: Roadmap and Contributions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The README and issues sometimes hint at where the project might grow:'
  prefs: []
  type: TYPE_NORMAL
- en: Making the CUDA kernels more modular in `dev/cuda/`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simplifying multi-GPU startup for clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding small tutorial-style docs (like the LayerNorm walkthrough).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The project is open to contributions, but it follows the same minimalist philosophy.
    If you’re thinking of contributing, remember: the goal is clarity first, performance
    second.'
  prefs: []
  type: TYPE_NORMAL
- en: External Learning Resources
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'While *llm.c* is self-contained, it pairs nicely with outside material:'
  prefs: []
  type: TYPE_NORMAL
- en: The PyTorch reference implementation in `train_gpt2.py` is your canonical oracle
    for correctness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The GPT-2 paper gives the architecture background.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CUDA and cuBLAS/cuDNN docs explain the GPU APIs that the project calls into.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Community blog posts often walk through specific sections of the code in plain
    English, making it easier to digest.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By combining the code, the paper, and these resources, you can triangulate a
    much deeper understanding.
  prefs: []
  type: TYPE_NORMAL
- en: A Suggested Learning Path
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If you’re coming to *llm.c* as a beginner, here’s a natural progression:'
  prefs: []
  type: TYPE_NORMAL
- en: Run the CPU trainer (`train_gpt2.c`) on Tiny Shakespeare. Watch the loss decrease.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Step through the code with `DEBUG=1`, confirming that you understand forward,
    backward, and optimizer steps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Move to the FP32 CUDA trainer to see how the same loop runs on GPU.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Switch to the modern CUDA trainer (`train_gpt2.cu`) and learn how mixed precision
    works.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Experiment with dataset scripts in `dev/data/`-try your own text corpus.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read the LayerNorm doc in `doc/` to deepen your theory-practice connection.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explore multi-GPU runs with MPI/NCCL if you have access to multiple GPUs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Follow GitHub Discussions for real-world debugging and scaling stories.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why This Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Code alone is not enough. The community context, the discussions, and the learning
    path make *llm.c* a living project. By engaging with them, you avoid the feeling
    of learning in isolation. You’ll see others wrestling with the same challenges,
    and you’ll have a clearer sense of what to try next.
  prefs: []
  type: TYPE_NORMAL
- en: 'The takeaway: Beyond the files and scripts, *llm.c* is a community-driven learning
    environment. GitHub issues, discussions, reference docs, and external tutorials
    all form part of the “extended classroom.” If the code is the lab bench, the community
    is the set of lab partners who help you figure things out along the way.'
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 2\. Data, Tokenization, and Loaders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 11\. GPT-2 Tokenizer Artifacts (`gpt2_tokenizer.bin`)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A language model like GPT-2 doesn’t directly understand English, Vietnamese,
    or any other natural language. Instead, it understands numbers. These numbers
    are called tokens. A tokenizer is the tool that translates between human text
    and tokens. In *llm.c*, the GPT-2 tokenizer is stored in a small file called `gpt2_tokenizer.bin`.
    This file is the key that lets the model read input text and produce output text
    that we can understand.
  prefs: []
  type: TYPE_NORMAL
- en: What This File Contains
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The file `gpt2_tokenizer.bin` is a binary version of GPT-2’s tokenizer. It
    includes:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Component | Purpose |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Byte vocabulary (0–255) | Makes sure every possible character can be represented.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Merge rules (BPE) | Combines frequent sequences like “ing” or ” the” into
    single tokens for efficiency. |'
  prefs: []
  type: TYPE_TB
- en: '| Vocabulary size (~50,257) | Defines how many distinct tokens GPT-2 can work
    with. |'
  prefs: []
  type: TYPE_TB
- en: '| Mapping IDs ↔︎ text | Lets the program turn model outputs (numbers) back
    into human-readable strings. |'
  prefs: []
  type: TYPE_TB
- en: Instead of being written as JSON or text, the tokenizer is stored in binary
    form. This allows *llm.c* to load it very quickly using a simple file read, which
    keeps the code clean and fast.
  prefs: []
  type: TYPE_NORMAL
- en: Where It Comes From
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You don’t need to build this file by hand. The repository provides a script
    to download it, along with small training and validation datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: After running the script, you should see `gpt2_tokenizer.bin`, `train.bin`,
    and `val.bin` in your working directory. If the tokenizer is missing, the program
    cannot run because it won’t know how to interpret text.
  prefs: []
  type: TYPE_NORMAL
- en: How the Code Uses It
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: During training, the tokenizer is not active because the datasets (`train.bin`
    and `val.bin`) are already pre-tokenized into integers. This keeps the training
    loop fast and simple.
  prefs: []
  type: TYPE_NORMAL
- en: During sampling or evaluation, the tokenizer becomes important again. After
    the model predicts a sequence of token IDs, the tokenizer translates those numbers
    back into text that you can read on your screen.
  prefs: []
  type: TYPE_NORMAL
- en: 'The C API for the tokenizer, defined in `llmc/tokenizer.h`, provides just three
    main functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This is all you need: initialize the tokenizer from the file, decode tokens
    into text, and free memory when done.'
  prefs: []
  type: TYPE_NORMAL
- en: Example Workflow in Practice
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Initialize the tokenizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Decode a sequence of tokens back to text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Clean up memory when you no longer need it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This small cycle is enough to turn model outputs into readable sentences.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Without the tokenizer, the model cannot communicate. The tokenizer is like a
    shared dictionary between humans and the neural network. If you give the model
    text, the tokenizer converts it into numbers the model understands. When the model
    responds, the tokenizer converts its numbers back into text. If the tokenizer
    does not match the dataset, the model’s predictions will come out as gibberish.
    Keeping the tokenizer and dataset in sync is essential for correct training and
    evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here are a few small exercises you can do to understand the tokenizer better:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check that the file exists: After running the starter pack script, verify that
    `gpt2_tokenizer.bin` is in your directory. Try running the trainer without it
    and observe the error message.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Inspect vocab size: Run the trainer and look for the line that prints `vocab_size:
    50257`. Compare this with `padded_vocab_size: 50304`. Why do you think padding
    helps GPUs?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Decode a sequence manually: Write a short C program that loads the tokenizer
    and decodes a fixed list of token IDs (for example `[464, 3290, 318]`). Observe
    what text you get.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Mismatch experiment: If you build your own dataset with a different tokenizer
    (say, a custom vocabulary), try decoding it with `gpt2_tokenizer.bin`. Notice
    how the output becomes meaningless, showing why consistency matters.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Dataset + tokenizer link: Open `train.bin` in a hex viewer. You’ll see it’s
    just numbers. Use the tokenizer to decode the first few hundred tokens and see
    real text emerge.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`gpt2_tokenizer.bin` is a tiny but vital file. It is the bridge that allows
    the model and humans to speak the same language. Training is efficient because
    all data is pre-tokenized, and when you want to see what the model has written,
    the tokenizer turns raw numbers back into words. Without it, the entire system
    would be silent.'
  prefs: []
  type: TYPE_NORMAL
- en: 12\. Binary Dataset Format (`train.bin` and `val.bin`)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Just like the tokenizer turns text into numbers, the datasets in *llm.c* are
    stored as numbers too. Instead of reading plain text files like `.txt`, the training
    and validation data are kept in simple binary files: `train.bin` and `val.bin`.
    These files are the fuel for the training loop.'
  prefs: []
  type: TYPE_NORMAL
- en: What These Files Look Like
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'At first glance, `train.bin` and `val.bin` look like unreadable blobs if you
    open them in a text editor. That’s because they are not meant to be human-readable.
    They contain:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Part | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| A tiny header (about 1 KB) | Stores metadata such as sequence length and
    vocab size. |'
  prefs: []
  type: TYPE_TB
- en: '| A stream of token IDs (`uint16`) | Every tokenized word piece from the dataset,
    saved as 16-bit integers. |'
  prefs: []
  type: TYPE_TB
- en: Each integer represents one token from the tokenizer’s vocabulary. Since GPT-2
    has a vocabulary of about 50,000 tokens, 16-bit integers (`uint16_t`) are enough
    to store them all.
  prefs: []
  type: TYPE_NORMAL
- en: Why Binary Format?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Efficiency: Instead of re-tokenizing text every time, the data is pre-tokenized
    once and stored as numbers. The trainer just reads them directly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Speed: Reading integers from a file is faster than parsing and processing raw
    text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Simplicity: The training loop only has to deal with arrays of integers-no string
    handling, no parsing, no surprises.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This choice makes the training code in *llm.c* much cleaner and faster.
  prefs: []
  type: TYPE_NORMAL
- en: How the Dataloader Uses Them
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When training starts, the dataloader reads chunks of numbers from `train.bin`.
    Each chunk corresponds to one batch of size B × T:'
  prefs: []
  type: TYPE_NORMAL
- en: B = batch size (number of examples in a batch).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: T = sequence length (number of tokens per example).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, if `B = 8` and `T = 1024`, the dataloader will read `8 × 1024 =
    8192` token IDs from the file, reshape them into sequences, and feed them to the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: The validation file (`val.bin`) works the same way but is only used occasionally
    during training to measure validation loss. This helps detect overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Workflow in Code
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Inside the repo, you’ll see functions like these in `llmc/dataloader.h`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s what happens step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize with the binary file and batch/sequence sizes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next batch reads the next B × T tokens into an input array and a target array.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reset allows re-reading from the beginning when you start a new epoch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Free cleans up resources when training ends.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The target array is simply the same sequence shifted by one token-because language
    modeling predicts the *next* token.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The dataset format is what makes *llm.c* practical. Without it, the code would
    need to handle messy text, encodings, and tokenization during every training step.
    By storing clean arrays of token IDs, the training loop becomes very short and
    easy to follow. It’s a design decision that keeps the project minimal yet faithful
    to real training pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Check file size: Run `ls -lh train.bin` and notice how large it is compared
    to a plain `.txt` file. Why is it smaller or larger?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Peek inside: Use a hex viewer (`xxd train.bin | head`) to see raw numbers.
    They won’t look like text, but they are the tokens the model trains on.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Count tokens: Write a short Python or C script to count how many token IDs
    are stored in `train.bin`. This gives you a sense of dataset size.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Mini-dataset: Try generating your own dataset from a small `.txt` file using
    the scripts in `dev/data/`. See how the `.bin` file is created.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Validation experiment: During training, reduce the validation set to only a
    few batches and observe how the validation loss stabilizes or fluctuates compared
    to training loss.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`train.bin` and `val.bin` may look like gibberish, but they are carefully prepared
    binary files containing token IDs. They make training faster, simpler, and more
    reproducible. The dataloader in *llm.c* reads these numbers in neat chunks and
    serves them directly to the model, letting you focus on learning how transformers
    work instead of wrestling with raw text parsing.'
  prefs: []
  type: TYPE_NORMAL
- en: 13\. Dataset Scripts in `dev/data/`
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The repository doesn’t just give you ready-made binary datasets like `train.bin`
    and `val.bin`. It also provides scripts inside the `dev/data/` folder that show
    you how to create your own. These scripts are important because they demonstrate
    how raw text gets transformed into the binary format that the dataloader in *llm.c*
    expects.
  prefs: []
  type: TYPE_NORMAL
- en: What’s Inside `dev/data/`
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This folder contains small Python scripts that:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Script | Purpose |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `prepare_shakespeare.py` | Turns the Tiny Shakespeare dataset into `train.bin`
    and `val.bin`. |'
  prefs: []
  type: TYPE_TB
- en: '| `prepare_openwebtext.py` | Prepares a large-scale dataset similar to the
    one GPT-2 was trained on. |'
  prefs: []
  type: TYPE_TB
- en: '| Other helpers | Tokenize raw `.txt` files, split them into train/val, and
    save to binary. |'
  prefs: []
  type: TYPE_TB
- en: 'Each script follows the same basic recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: Read raw text from a source file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply the GPT-2 tokenizer to turn text into token IDs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the tokens into training and validation portions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write the IDs into binary files that *llm.c* can read directly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why Preprocessing Happens Outside C
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In C, handling text files with Unicode, punctuation, and different encodings
    is messy. Instead, preprocessing is done once in Python, where tokenizers are
    easier to use. The results are saved in a simple binary format (`uint16` IDs).
    From then on, C only has to deal with arrays of integers-clean and efficient.
  prefs: []
  type: TYPE_NORMAL
- en: 'This design keeps the training loop minimal: no text parsing, no string handling,
    just numbers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Tiny Shakespeare'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'One of the simplest datasets is Tiny Shakespeare, about 1 MB of text from Shakespeare’s
    plays. The script `prepare_shakespeare.py` will:'
  prefs: []
  type: TYPE_NORMAL
- en: Read `input.txt` (the raw text).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the GPT-2 tokenizer (`gpt2_tokenizer.bin`) to turn every word and symbol
    into token IDs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Split 90% of the data into `train.bin` and 10% into `val.bin`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After running the script, you’ll have small binary files that let you train
    GPT-2 from scratch in minutes on CPU or GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: OpenWebText'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The script `prepare_openwebtext.py` shows how to tokenize a much larger dataset,
    closer to what GPT-2 was originally trained on. This is heavier and requires more
    disk space, but it’s useful if you want to try scaling up training to bigger models.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'These scripts are more than convenience tools-they are examples of how to adapt
    llm.c to your own data. If you have a collection of emails, poems, or programming
    code, you can:'
  prefs: []
  type: TYPE_NORMAL
- en: Put them into a single `.txt` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modify one of the scripts in `dev/data/`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate new `train.bin` and `val.bin` files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train GPT-2 on your own text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By separating dataset creation from training, *llm.c* keeps the C code small
    and makes experimentation flexible.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Run the Shakespeare script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Then check that `train.bin` and `val.bin` were created.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Open the binary files with a hex viewer and confirm that they contain only numbers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modify the script to tokenize a different text file (for example, your own writing).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compare dataset sizes: Tiny Shakespeare is tiny (MBs), OpenWebText is huge
    (GBs). Observe how training speed changes depending on dataset size.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Re-run training with your custom dataset and watch how the model starts generating
    text in your style.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `dev/data/` scripts are the bridge between raw human text and the binary
    datasets used in training. They let you prepare small demo datasets or scale up
    to larger corpora. By experimenting with these scripts, you learn how to bring
    your own data into *llm.c* and train a GPT-style model on anything you like.
  prefs: []
  type: TYPE_NORMAL
- en: 14\. DataLoader Design (Batching, Strides, Epochs)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that the datasets are prepared as `.bin` files, we need a way to feed them
    into the model during training. This is the job of the DataLoader in *llm.c*.
    You’ll find its interface in `llmc/dataloader.h`, and its purpose is very simple:
    take a big stream of token IDs from `train.bin` or `val.bin`, cut it into manageable
    chunks, and serve those chunks to the training loop as batches.'
  prefs: []
  type: TYPE_NORMAL
- en: The Core Idea
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Training a language model requires two arrays for every batch:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Inputs: a sequence of token IDs, like `[The, cat, sat, on]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Targets: the same sequence shifted by one, like `[cat, sat, on, the]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model learns to predict each next token in the sequence. The DataLoader
    automates slicing these arrays out of the giant dataset file.
  prefs: []
  type: TYPE_NORMAL
- en: The Interface
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the code you’ll see function declarations like these:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s what each does:'
  prefs: []
  type: TYPE_NORMAL
- en: '`dataloader_init`: opens the dataset file, remembers batch size `B` and sequence
    length `T`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dataloader_next_batch`: returns the next chunk of `B × T` tokens (inputs)
    and their shifted version (targets).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dataloader_reset`: rewinds to the start of the file when an epoch ends.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dataloader_free`: closes the file and releases memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This design keeps the training loop clean: just call `next_batch` and you get
    the data ready for forward/backward passes.'
  prefs: []
  type: TYPE_NORMAL
- en: B × T Explained
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The two most important parameters are:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Symbol | Meaning | Example |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| B | Batch size (how many sequences per step) | 16 |'
  prefs: []
  type: TYPE_TB
- en: '| T | Sequence length (how many tokens per sequence) | 1024 |'
  prefs: []
  type: TYPE_TB
- en: So one batch contains `B × T` tokens. For example, with `B = 16` and `T = 1024`,
    each batch holds 16,384 tokens. The DataLoader simply reads that many numbers
    from the binary file and arranges them in memory.
  prefs: []
  type: TYPE_NORMAL
- en: Strides Through the Dataset
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As you call `dataloader_next_batch`, the loader moves forward through the dataset
    by `B × T` tokens each time. When it reaches the end of the dataset file, it either:'
  prefs: []
  type: TYPE_NORMAL
- en: Resets back to the beginning (`dataloader_reset`), or
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Switches from training to validation, depending on the training loop’s needs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This stride-based reading is efficient: no random access, just sequential reads
    from a file.'
  prefs: []
  type: TYPE_NORMAL
- en: Epochs and Shuffling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In deep learning, an epoch means one full pass through the dataset. The DataLoader
    in *llm.c* is simple: it goes linearly from start to finish. It doesn’t shuffle
    data like PyTorch’s `DataLoader`. Why? Because language data is already very diverse,
    and the project values minimal code over extra features. If you want shuffling,
    you can preprocess the dataset differently before creating `.bin` files.'
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The DataLoader is the quiet workhorse of training. It ensures that every step
    sees a fresh batch of token sequences, always with matching inputs and targets.
    By separating dataset reading from the training loop, the code stays clean and
    focused. This design also makes it easy to swap datasets-once you generate a `.bin`
    file, the loader doesn’t care where it came from.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Print the first batch: Modify the code to print the first 20 input tokens and
    their targets. See how each input token aligns with the next target token.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Experiment with B and T: Set `B = 2` and `T = 8` and observe how the loader
    slices the dataset into tiny chunks. Then try larger values and see how memory
    usage changes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Check epoch length: Write a small loop to count how many batches you get before
    `dataloader_reset` is called. Does this match the total tokens divided by `B ×
    T`?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Validation check: Observe how often the training loop switches to `val.bin`.
    How does validation loss compare to training loss over time?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Custom stride: Modify the code so the DataLoader skips some tokens between
    batches. What effect does this have on training?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The DataLoader in *llm.c* is intentionally simple. It streams token IDs in fixed-sized
    batches, moves forward stride by stride, and resets when done. This straightforward
    design avoids complexity and keeps the focus on the model itself, while still
    teaching you the essential mechanics of batching and sequence handling in language
    model training.
  prefs: []
  type: TYPE_NORMAL
- en: 15\. EvalLoader and Validation Workflow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Training a model isn’t just about watching the training loss go down. To know
    whether the model is actually learning patterns that generalize-and not just memorizing
    the training data-you need to run validation. In *llm.c*, validation is handled
    by a component called the EvalLoader, which works just like the DataLoader but
    reads from the validation dataset (`val.bin`) instead of the training dataset
    (`train.bin`).
  prefs: []
  type: TYPE_NORMAL
- en: Why We Need Validation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Imagine teaching a student only by drilling them with the same math problems
    over and over. They might get really good at those problems, but fail completely
    when given new ones. Validation is like giving the student a pop quiz with unseen
    questions. If they do well, you know they’ve actually learned the concepts.
  prefs: []
  type: TYPE_NORMAL
- en: 'For language models, validation helps detect overfitting: when the training
    loss keeps improving but the validation loss stays flat or even gets worse.'
  prefs: []
  type: TYPE_NORMAL
- en: How EvalLoader Works
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'EvalLoader lives in the same code file as the DataLoader (`llmc/dataloader.h`),
    but it points to a different dataset file. Its workflow is nearly identical:'
  prefs: []
  type: TYPE_NORMAL
- en: Open `val.bin` and prepare for reading.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Serve up batches of size `B × T` (batch size × sequence length).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Provide inputs and targets the same way as the training DataLoader.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reset after one full pass through the file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The training loop typically calls the EvalLoader at intervals-for example, every
    few hundred steps-so you get a snapshot of validation loss during training.
  prefs: []
  type: TYPE_NORMAL
- en: What Happens During Validation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When validation is triggered:'
  prefs: []
  type: TYPE_NORMAL
- en: The current model parameters are frozen (no gradient updates).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A few batches are read from `val.bin`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model runs forward passes only, computing the loss on each batch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The losses are averaged and reported as validation loss.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This doesn’t take long because it usually samples just a subset of the validation
    dataset, not the entire file.
  prefs: []
  type: TYPE_NORMAL
- en: Training Loop with Validation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In pseudocode, the loop looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'This is simplified, but it shows the idea: the validation loop is nested inside
    the training loop, running occasionally instead of every step.'
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Validation is the reality check of training. Without it, you could train forever
    and celebrate low training losses, only to discover that your model produces nonsense
    on new text. By tracking validation loss, you can:'
  prefs: []
  type: TYPE_NORMAL
- en: Detect overfitting early.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adjust hyperparameters (like learning rate or batch size).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Know when training has plateaued and it’s time to stop.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In professional setups, validation curves are often plotted live, but in *llm.c*,
    the minimalist approach is to just print numbers to the console.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Watch val loss: Run training and note how validation loss compares to training
    loss. Do they both decrease together?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Overfitting demo: Train on a very tiny dataset (like 10 KB of text). Notice
    how training loss plummets but validation loss stalls or rises.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Change eval interval: Reduce `eval_interval` so validation runs every step.
    How much slower does training feel?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Change eval batches: Set `eval_batches` to 1 vs 100\. What difference does
    this make in the stability of the reported validation loss?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Validation as stopping rule: Stop training when validation loss stops improving
    for many intervals. How does this affect final performance?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The EvalLoader is a twin of the DataLoader, but for validation. It feeds the
    model data it has never seen during training, and the resulting validation loss
    tells you whether your model is learning useful patterns or just memorizing. It’s
    the simplest safeguard against wasted compute, and it’s an essential part of every
    training loop-even in the stripped-down world of *llm.c*.
  prefs: []
  type: TYPE_NORMAL
- en: 16\. Sequence Length and Memory Budgeting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When training GPT-2 in *llm.c*, one of the most important decisions you make
    is choosing the sequence length (often called T). This value determines how many
    tokens the model processes in a single forward pass. It might sound like just
    another parameter, but sequence length has a huge impact on what the model can
    learn, how much memory it uses, and how fast training runs.
  prefs: []
  type: TYPE_NORMAL
- en: What Sequence Length Means
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Sequence length is simply the number of tokens per training example. If `T =
    1024`, the model reads 1,024 tokens in a row (like words or subwords) and tries
    to predict the next token at each position.
  prefs: []
  type: TYPE_NORMAL
- en: 'Think of it like this: if you give the model a paragraph of text, sequence
    length is how much of that paragraph it sees at once. Shorter lengths give the
    model less context, while longer lengths allow it to capture bigger patterns,
    like whole paragraphs or even multiple pages.'
  prefs: []
  type: TYPE_NORMAL
- en: Where It Appears in the Code
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the logs, you’ll often see lines like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: This number is defined in the GPT-2 configuration and passed into the DataLoader.
    The DataLoader slices chunks of exactly `T` tokens from `train.bin` and `val.bin`.
    The model itself has fixed positional embeddings of size `T`, so it cannot process
    sequences longer than this maximum.
  prefs: []
  type: TYPE_NORMAL
- en: Memory Costs of Longer Sequences
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Transformers are powerful but expensive. The attention mechanism compares every
    token to every other token in the sequence. This means memory and compute scale
    with the square of sequence length:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Sequence Length (T) | Relative Attention Cost |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 256 | 1× |'
  prefs: []
  type: TYPE_TB
- en: '| 512 | 4× |'
  prefs: []
  type: TYPE_TB
- en: '| 1024 | 16× |'
  prefs: []
  type: TYPE_TB
- en: '| 2048 | 64× |'
  prefs: []
  type: TYPE_TB
- en: So doubling `T` doesn’t just double the cost-it multiplies it by four. That’s
    why training at long context lengths requires a lot of GPU memory.
  prefs: []
  type: TYPE_NORMAL
- en: Trade-offs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Shorter sequences: Faster, less memory, but limited context. Good for quick
    experiments or tiny datasets like Tiny Shakespeare.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Longer sequences: More memory, slower, but the model can understand larger
    spans of text. Required for large-scale GPT-2 training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can think of sequence length as a dial: turning it up increases the model’s
    ability to “remember,” but it also makes training much heavier.'
  prefs: []
  type: TYPE_NORMAL
- en: Practical Choices in *llm.c*
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Tiny Shakespeare example: often trained with `T = 64` or `128` for speed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GPT-2 small (124M): typically uses `T = 1024`, the same as the original paper.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your GPU has limited memory, you might need to shrink `T` and/or batch size
    `B`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Choosing sequence length is about balancing learning power against hardware
    limits. A too-small sequence length can prevent the model from capturing long-term
    dependencies. A too-large one can make training impossible on your hardware. Every
    run of *llm.c* is a negotiation between what you’d like the model to see and what
    your system can handle.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Short vs long: Train Tiny Shakespeare with `T = 64` and then `T = 256`. Compare
    both the speed and the coherence of generated text.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Memory test: Increase `T` step by step until you hit an out-of-memory (OOM)
    error. Note the maximum your GPU can handle.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Batch trade-off: Try reducing batch size `B` while increasing `T`. Can you
    keep GPU memory stable while giving the model more context?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Validation impact: Run with different `T` values and watch how validation loss
    behaves. Does longer context always help?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Inspect embeddings: Print out the shape of the positional embeddings. Notice
    how they are always tied to `T`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Sequence length (`T`) controls how much context the model sees. It directly
    determines the size of the positional embeddings, the structure of batches, and
    the memory required for attention. In *llm.c*, adjusting `T` is one of the fastest
    ways to explore the trade-offs between speed, memory, and model capability.
  prefs: []
  type: TYPE_NORMAL
- en: 17\. Reproducibility and Seeding Across Runs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When training machine learning models, it’s common to notice that two runs-using
    the same code and the same dataset-don’t produce exactly the same results. This
    happens because many parts of training involve randomness. In *llm.c*, reproducibility
    is controlled by random seeds. A seed is a starting point for a random number
    generator. If you always start from the same seed, the sequence of “random” numbers
    will be identical, and so will the training run.
  prefs: []
  type: TYPE_NORMAL
- en: Where Randomness Appears
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Even in a small project like *llm.c*, randomness shows up in several places:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Component | Random Role |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Weight initialization | The model’s parameters (like attention matrices)
    are set randomly at the start. |'
  prefs: []
  type: TYPE_TB
- en: '| Optimizer states | Some optimizers use random noise (though AdamW is mostly
    deterministic). |'
  prefs: []
  type: TYPE_TB
- en: '| Sampling outputs | When generating text, randomness decides which token to
    pick if probabilities are close. |'
  prefs: []
  type: TYPE_TB
- en: '| Parallelism | On GPU, threads may execute in slightly different orders, sometimes
    introducing small nondeterminism. |'
  prefs: []
  type: TYPE_TB
- en: Without a fixed seed, every training run can drift apart, even if all settings
    look the same.
  prefs: []
  type: TYPE_NORMAL
- en: How *llm.c* Handles Seeds
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The repository provides a small random utilities module: `llmc/rand.h`. Inside
    you’ll find functions such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '`manual_seed` sets the seed for the internal random number generator, ensuring
    reproducibility.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`normal_` is used for initializing weights with Gaussian noise, similar to
    PyTorch’s `torch.nn.init.normal_`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you call `manual_seed(1337);`, the model weights will be initialized the
    same way every time.
  prefs: []
  type: TYPE_NORMAL
- en: Why Seeds Don’t Guarantee Perfect Reproducibility
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Even with a fixed seed, you may still see small differences:'
  prefs: []
  type: TYPE_NORMAL
- en: GPU kernels sometimes use parallel algorithms that are not bitwise deterministic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Floating-point math can produce slightly different rounding on different hardware.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-GPU runs (via NCCL/MPI) may introduce nondeterministic reduce operations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These differences are usually tiny-validation loss might vary by 0.001-but they
    exist. For most educational purposes, *llm.c* seeds are enough to make experiments
    repeatable.
  prefs: []
  type: TYPE_NORMAL
- en: Typical Defaults
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In many examples, you’ll see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: This “magic number” 1337 is just a convention. You can change it to any integer.
    Using the same seed across runs guarantees the same starting weights, which helps
    when comparing hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Reproducibility is crucial in machine learning because it lets you:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Debug effectively: If a bug appears, you want it to appear consistently.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Compare settings: You can test learning rates or batch sizes fairly by keeping
    everything else the same.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Share results: Other people can run your exact setup and see the same outcomes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Without seeds, it becomes hard to tell whether a difference came from your hyperparameter
    change or just random luck.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Run twice with same seed: Train GPT-2 with `manual_seed(1337)` set. Do you
    get identical training loss curves?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Change the seed: Try `manual_seed(42)` and compare the loss curve. How similar
    are they? Do they converge to about the same final validation loss?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Remove seeding: Comment out the seed line and run again. Notice how runs diverge.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Sampling experiment: With a fixed seed, generate text multiple times. Then
    change the seed and generate again. See how outputs change.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Multi-GPU test: If you have more than one GPU, run the same seed across devices.
    Do results stay exactly the same or only approximately?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Reproducibility in *llm.c* comes from setting seeds for random number generators.
    While floating-point quirks mean you can’t always get perfect bit-for-bit matches,
    seeds let you control the biggest source of randomness: weight initialization
    and sampling. With seeding, you can debug, compare, and share results confidently.'
  prefs: []
  type: TYPE_NORMAL
- en: 18\. Error Surfaces from Bad Data (Bounds, Asserts)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When training a model in *llm.c*, everything depends on the quality and correctness
    of the data you feed in. If the dataset or batches contain mistakes, the training
    process can go off track quickly-sometimes by crashing outright, other times by
    producing strange loss values like `NaN`. To guard against this, the code uses
    bounds checks and asserts that catch problems early.
  prefs: []
  type: TYPE_NORMAL
- en: What Can Go Wrong with Data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are several common data issues:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Problem | What Happens |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Token ID out of range | The model expects IDs between 0 and `vocab_size-1`.
    A wrong ID can cause array indexing errors. |'
  prefs: []
  type: TYPE_TB
- en: '| Empty or short dataset | The DataLoader may run out of tokens before filling
    a batch. |'
  prefs: []
  type: TYPE_TB
- en: '| Mismatched tokenizer | If you build a dataset with a different tokenizer,
    IDs may not correspond to the GPT-2 tokenizer in `gpt2_tokenizer.bin`. This produces
    nonsense outputs. |'
  prefs: []
  type: TYPE_TB
- en: '| Corrupt `.bin` files | If files are incomplete or written incorrectly, the
    DataLoader might read garbage values. |'
  prefs: []
  type: TYPE_TB
- en: These errors show up as segfaults, invalid memory access, or exploding losses
    during training.
  prefs: []
  type: TYPE_NORMAL
- en: How *llm.c* Defends Against Bad Data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The repository makes heavy use of asserts-simple checks that stop the program
    immediately if something unexpected happens. For example, in `llmc/utils.h`, functions
    like `freadCheck` and `mallocCheck` ensure that file reads and memory allocations
    succeed. If not, they print an error message and abort instead of silently failing.
  prefs: []
  type: TYPE_NORMAL
- en: Inside the DataLoader, token IDs are often validated to make sure they fall
    inside the expected vocabulary range. If you try to access an invalid index in
    the embedding table, the program will crash quickly, which is better than continuing
    with corrupted values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Vocab Range Check'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: During training, every input token is used to look up a row in the embedding
    matrix. If a token ID is too large, you’d access memory outside the matrix. This
    is why checking `0 <= id < vocab_size` is essential. In C, asserts provide this
    safety net.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: This kind of check may look simple, but it saves hours of debugging mysterious
    crashes.
  prefs: []
  type: TYPE_NORMAL
- en: Error Surfaces in Loss
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Even if your program doesn’t crash, bad data can create “error surfaces” in
    the loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: 'NaNs: Appear when invalid values propagate through softmax, layernorm, or division
    operations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Flat loss: If the dataset is empty or repetitive, the model never improves.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mismatch behavior: Training loss decreases but validation loss stays high if
    training and validation sets use inconsistent tokenization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are signs that something is wrong with the dataset or preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: C is a low-level language with very little safety by default. One out-of-range
    index can corrupt memory and cause unpredictable bugs. By aggressively checking
    assumptions (file sizes, vocab bounds, token IDs), *llm.c* turns hard-to-find
    errors into immediate, clear failures. For learners, this makes it much easier
    to understand what went wrong.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Corrupt a dataset: Open `train.bin` and delete a few bytes. Run training and
    see what error appears. Notice how quickly asserts catch it.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Force a bad ID: Modify the DataLoader to add `+100000` to a token. Does the
    model crash with an assertion?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Skip asserts: Temporarily disable checks and rerun. Compare how much harder
    it is to figure out what went wrong.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Validation mismatch: Tokenize a file with a different tokenizer and save it
    as `val.bin`. Watch how the validation loss behaves compared to training loss.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Print debug info: Add logging to display the first 20 tokens of each batch.
    Can you spot bad data before it crashes?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Bad data can silently sabotage training, but *llm.c* uses asserts and bounds
    checks to make errors loud and clear. This design choice helps learners focus
    on the real logic of transformers instead of chasing hidden bugs caused by corrupted
    or mismatched datasets. In machine learning, good data hygiene and strict validation
    are as important as the model itself.
  prefs: []
  type: TYPE_NORMAL
- en: 19\. Tokenization Edge Cases (UNKs, EOS, BOS)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Tokenization looks simple at first: take text, split it into tokens, and assign
    each token an ID. But in practice, there are always tricky situations. *llm.c*
    inherits the quirks of the GPT-2 tokenizer, which is byte-level BPE (Byte Pair
    Encoding). This design mostly avoids “unknown” tokens, but it still has details
    you need to understand when preparing datasets or interpreting outputs.'
  prefs: []
  type: TYPE_NORMAL
- en: No True “UNK” in GPT-2
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Some tokenizers, like those used in earlier NLP systems, include a special
    `UNK` (unknown) token for words that aren’t in the vocabulary. GPT-2 avoids this
    problem by working at the byte level:'
  prefs: []
  type: TYPE_NORMAL
- en: Every possible byte (0–255) is in the base vocabulary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the tokenizer doesn’t know how to split a character or word, it just falls
    back to raw bytes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That means you will never see an `UNK` token in *llm.c*. Any input text is always
    representable. This is one of the main reasons GPT-2’s tokenizer is so robust.
  prefs: []
  type: TYPE_NORMAL
- en: 'Special Tokens: EOS and BOS'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Even though GPT-2 doesn’t use `UNK`, it does use other special tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Token | ID | Purpose |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| EOS (End of Sequence) | 50256 | Marks the end of a text segment. Used during
    training and sampling. |'
  prefs: []
  type: TYPE_TB
- en: '| BOS (Beginning of Sequence) | Not explicit in GPT-2 | GPT-2 doesn’t use a
    fixed BOS token. Instead, the model assumes generation starts at position 0. |'
  prefs: []
  type: TYPE_TB
- en: In *llm.c*, you’ll often see `EOS` at the end of training sequences or when
    sampling text. If you generate text and see strange endings, it’s usually because
    the model predicted `EOS`.
  prefs: []
  type: TYPE_NORMAL
- en: Whitespace Quirks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The tokenizer also handles whitespace in a slightly unusual way. For example,
    the word “hello” and the word ” hello” (with a leading space) map to different
    tokens. This is why generated text sometimes starts with a space-it’s part of
    the token definition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '`"hello"` → token ID 31373'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`" hello"` → token ID 15496'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is normal behavior for GPT-2\. It helps the model capture spacing and punctuation
    consistently.
  prefs: []
  type: TYPE_NORMAL
- en: Unicode and Rare Characters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Because it’s byte-level, GPT-2 can encode emojis, accented characters, or even
    binary junk data. But the BPE merges are optimized for English, so rare characters
    often get split into multiple byte tokens. That means sequences with lots of rare
    symbols (like Chinese or emojis) will use more tokens than plain English text.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Edge cases in tokenization affect both dataset preparation and model outputs.
    If you see weird spacing or early `EOS` tokens, it’s not a bug-it’s just how the
    tokenizer works. Understanding these quirks helps you debug outputs and prepare
    datasets without surprises.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'EOS inspection: Open `val.bin` with a hex viewer and look for token ID `50256`.
    These mark the ends of text segments.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Whitespace check: Use the tokenizer to encode `"hello"` and `" hello"`. Compare
    the token IDs.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Emoji test: Encode a string with emojis (e.g., `"🙂🙂🙂"`) and see how many tokens
    it becomes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Rare character dataset: Create a small `.txt` file with accented characters
    and tokenize it. How many bytes does each character consume?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Sampling experiment: Generate text until you see the EOS token appear. Notice
    how the model “knows” to stop.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Tokenization in GPT-2 is robust, but it has quirks. There are no unknown tokens
    thanks to byte-level encoding, but whitespace and special tokens like `EOS` play
    important roles. By experimenting with these edge cases, you’ll develop an intuition
    for how raw text is mapped into the numbers that drive training and generation
    in *llm.c*.
  prefs: []
  type: TYPE_NORMAL
- en: 20\. Data Hygiene and Logging
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When training with *llm.c*, having clean data is just as important as having
    the right model code. If the dataset contains errors, duplicates, or formatting
    issues, the model may waste capacity memorizing noise instead of learning useful
    patterns. This is where data hygiene comes in-making sure your training and validation
    sets are prepared properly. Alongside this, logging ensures you can monitor what’s
    happening during training and catch problems early.
  prefs: []
  type: TYPE_NORMAL
- en: What Data Hygiene Means
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Data hygiene is about making sure your dataset is both valid and useful. For
    language models, this includes:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Check | Why It Matters |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Correct tokenization | Must match the tokenizer (`gpt2_tokenizer.bin`), otherwise
    IDs won’t line up. |'
  prefs: []
  type: TYPE_TB
- en: '| No corrupt files | Binary `.bin` files must be complete; partial writes cause
    crashes. |'
  prefs: []
  type: TYPE_TB
- en: '| Balanced splits | Training and validation sets should come from the same
    distribution. |'
  prefs: []
  type: TYPE_TB
- en: '| Reasonable size | Too small → overfitting. Too large → slow or infeasible.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Deduplication | Repeated passages (e.g., web scrapes) make models memorize
    instead of generalize. |'
  prefs: []
  type: TYPE_TB
- en: The scripts in `dev/data/` handle basic hygiene by tokenizing consistently and
    splitting into train/val sets. But if you bring your own dataset, you are responsible
    for cleaning it first.
  prefs: []
  type: TYPE_NORMAL
- en: Logging During Training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Once training starts, logging becomes your window into what’s happening. *llm.c*
    uses a minimal logging system (`llmc/logger.h`) to print progress to the console.
    Typical logs include:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'These numbers let you track:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Training loss: Is the model fitting the data?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Validation loss: Is it generalizing, or overfitting?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step timing: How long each batch takes, useful for profiling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even in such a small project, this logging loop gives you most of what you need
    to debug runs.
  prefs: []
  type: TYPE_NORMAL
- en: Why Hygiene and Logging Go Together
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Bad data often reveals itself in the logs. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: If validation loss is much higher than training loss, your validation set may
    be mismatched.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If loss suddenly becomes `NaN`, your dataset might contain corrupt tokens.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If loss plateaus at a high value, you may have too little data or poor preprocessing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By keeping your data clean and watching logs closely, you can detect these issues
    early instead of wasting hours of compute.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Dirty dataset test: Take a `.txt` file, add random symbols or binary junk,
    and prepare a `.bin` dataset. What happens to training loss?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Duplicate passages: Copy the same paragraph 100 times into a training file.
    Does validation loss improve, or does the model just memorize?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Log frequency: Modify the code to log every step instead of every N steps.
    How noisy are the results?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Custom logger: Extend the logger to also print gradient norms or learning rate
    values. Does this help you understand training dynamics better?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compare splits: Build two datasets with different train/val splits. Which one
    gives more stable validation losses?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Data hygiene ensures the model learns from clean, consistent input, while logging
    ensures you can see whether learning is actually happening. Together, they form
    the foundation of reliable experiments in *llm.c*. If you clean your data carefully
    and pay attention to the logs, you’ll catch most problems before they become serious.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 3\. Model Definition and Weights
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '21\. GPT-2 Config: Vocab, Layers, Heads, Channels'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Every GPT-2 model, no matter how large or small, is defined by a handful of
    configuration numbers. These numbers decide how big the model is, how much memory
    it needs, and how powerful it can become. In *llm.c*, these settings are stored
    in a simple config struct and printed at the start of training. They describe
    the “blueprint” of the transformer.
  prefs: []
  type: TYPE_NORMAL
- en: The Core Parameters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here are the most important values you’ll see in the logs:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Parameter | Meaning | Example (GPT-2 Small) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `vocab_size` | Number of distinct tokens (from tokenizer). | 50,257 |'
  prefs: []
  type: TYPE_TB
- en: '| `padded_vocab_size` | Vocab size rounded up to nearest multiple (for GPU
    efficiency). | 50,304 |'
  prefs: []
  type: TYPE_TB
- en: '| `max_seq_len` | Longest sequence of tokens the model can handle. | 1,024
    |'
  prefs: []
  type: TYPE_TB
- en: '| `num_layers` | Number of transformer blocks stacked on top of each other.
    | 12 |'
  prefs: []
  type: TYPE_TB
- en: '| `num_heads` | Number of attention heads per block. | 12 |'
  prefs: []
  type: TYPE_TB
- en: '| `channels` | Width of hidden states (embedding dimension). | 768 |'
  prefs: []
  type: TYPE_TB
- en: '| `num_parameters` | Total trainable weights in the model. | ~124M |'
  prefs: []
  type: TYPE_TB
- en: Together, these values define both the structure and the capacity of the model.
  prefs: []
  type: TYPE_NORMAL
- en: What They Control
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Vocabulary size connects the model to the tokenizer. Every input token ID must
    be less than `vocab_size`. The padded version makes GPU matrix multiplications
    easier.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Max sequence length fixes the size of the positional embeddings. If you set
    this to 1024, the model can’t read beyond 1024 tokens in one pass.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Layers control model depth. Each layer contains an attention block and an MLP.
    More layers = more representational power.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Heads divide attention into parallel “subspaces.” With 12 heads, the model can
    track different types of relationships in the text at the same time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Channels set the dimensionality of embeddings and hidden vectors. Larger channels
    mean more expressive representations but also more computation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parameters are the sum of it all. This number tells you how heavy the model
    is to train and how much memory it will consume.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configs Across GPT-2 Sizes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The original GPT-2 models come in several sizes:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Layers | Heads | Channels | Parameters |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-2 Small | 12 | 12 | 768 | 124M |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-2 Medium | 24 | 16 | 1024 | 355M |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-2 Large | 36 | 20 | 1280 | 774M |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-2 XL | 48 | 25 | 1600 | 1.6B |'
  prefs: []
  type: TYPE_TB
- en: '*llm.c* can scale between these by just changing a few numbers in the config
    struct.'
  prefs: []
  type: TYPE_NORMAL
- en: Where Config Appears in the Code
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In `train_gpt2.c` and `train_gpt2.cu`, you’ll see something like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Later, the model is initialized using this struct, and the log prints all the
    derived information (like `num_parameters`).
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The config is the contract between your dataset and your model.
  prefs: []
  type: TYPE_NORMAL
- en: If `vocab_size` doesn’t match your tokenizer, you’ll get crashes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `max_seq_len` is too small, you’ll lose context.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `num_layers` or `channels` are too large for your GPU, you’ll run out of
    memory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By tweaking the config, you decide whether you want a tiny model for learning
    or a massive one closer to GPT-2 XL.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Print config: Run the trainer and note the printed values. Compare them with
    the GPT-2 sizes in the table.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Shrink the model: Change `num_layers = 4`, `num_heads = 4`, and `channels =
    256`. Train on Tiny Shakespeare and see how fast it runs.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Increase sequence length: Try setting `max_seq_len = 2048`. Does your GPU still
    handle it, or do you get out-of-memory errors?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Parameter count check: Compute how many parameters your custom config has.
    Compare it to the reported `num_parameters`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Tokenizer mismatch test: Intentionally set `vocab_size = 30000` and watch what
    error appears when loading the tokenizer.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The GPT-2 config struct in *llm.c* is small but powerful. It defines everything
    about the model’s architecture: vocabulary, sequence length, depth, width, and
    total parameters. By adjusting just a few integers, you can scale from a toy model
    that runs on CPU to a billion-parameter giant (if your hardware allows it). Understanding
    these numbers is the first step to understanding how transformer capacity is controlled.'
  prefs: []
  type: TYPE_NORMAL
- en: 22\. Parameter Tensors and Memory Layout
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once the GPT-2 configuration is set, the next big step is to allocate the parameters
    of the model. These are the trainable numbers-weights and biases-that define how
    the model processes input tokens. In *llm.c*, parameters are stored in flat arrays
    of floats rather than in deeply nested objects like in PyTorch. This choice makes
    the code easier to read and keeps memory access predictable.
  prefs: []
  type: TYPE_NORMAL
- en: What Are Parameters?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Every part of the transformer has its own trainable weights:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Embedding tables: one for tokens and one for positions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Attention layers: query, key, value, and output projections.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MLP layers: two linear layers plus their biases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LayerNorms: scale (`gamma`) and shift (`beta`) values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Final projection: maps hidden states back to vocab size for logits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Together, these add up to hundreds of millions of numbers, even for GPT-2 Small.
  prefs: []
  type: TYPE_NORMAL
- en: Flat Memory Design in *llm.c*
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Instead of allocating each parameter separately, *llm.c* stores all parameters
    in one contiguous block of memory. Each layer is given a slice of this big array.
  prefs: []
  type: TYPE_NORMAL
- en: 'This has two benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Simplicity: You only need one malloc (or cudaMalloc) for all parameters.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Performance: Contiguous memory access is faster on both CPU and GPU.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To keep track of where each layer’s weights live inside the block, the code
    uses offsets.
  prefs: []
  type: TYPE_NORMAL
- en: Example in Code
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In `train_gpt2.c`, parameters are packed into a single array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Later, helper functions compute pointers into this array for each sub-module.
    For example, the token embedding weights are just the first slice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Then the program moves forward, assigning chunks to positional embeddings, attention
    weights, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Shapes of the Tensors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Even though parameters are stored in 1D memory, they conceptually form 2D or
    3D tensors. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Parameter | Shape | Purpose |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Token embeddings | `[vocab_size, channels]` | Maps token IDs to vectors.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Positional embeddings | `[max_seq_len, channels]` | Adds position info. |'
  prefs: []
  type: TYPE_TB
- en: '| Attention weights (Q, K, V, O) | `[channels, channels]` | Project hidden
    states. |'
  prefs: []
  type: TYPE_TB
- en: '| MLP layers | `[channels, 4×channels]` and `[4×channels, channels]` | Expand
    and contract hidden states. |'
  prefs: []
  type: TYPE_TB
- en: '| LayerNorm scale/shift | `[channels]` | Normalize and rescale features. |'
  prefs: []
  type: TYPE_TB
- en: 'When you look at the code, remember: these shapes are “virtual.” They’re just
    views into slices of the big 1D array.'
  prefs: []
  type: TYPE_NORMAL
- en: Why This Layout Works Well
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'PyTorch or TensorFlow manage parameter tensors with lots of abstractions. *llm.c*
    strips this away: you see the raw memory, the exact number of parameters, and
    the order they’re laid out in. This makes it clear how large the model really
    is and why it uses so much RAM or VRAM.'
  prefs: []
  type: TYPE_NORMAL
- en: It also means you can easily save and load checkpoints by writing or reading
    the flat array directly to disk. No need for complicated serialization formats.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Understanding parameter layout helps you:'
  prefs: []
  type: TYPE_NORMAL
- en: See how the model’s size explodes as you increase layers, heads, or channels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Debug memory issues by checking how big each slice is.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Realize how much of training is just linear algebra on big arrays of floats.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This perspective is powerful because it demystifies deep learning: at its core,
    GPT-2 is just multiplying slices of one giant float array again and again.'
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Print parameter count: Add a line in the code to print `config.num_parameters`.
    Compare it with the table for GPT-2 Small/Medium/Large.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Inspect a slice: Print the first 10 numbers of the embedding table. They’ll
    look random (from initialization).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Change precision: Modify the code to allocate `half` (FP16) instead of `float`.
    How much memory do you save?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Checkpoint peek: Save a checkpoint, then open it in a hex viewer. It’s just
    raw floats-proof that parameters are stored flat.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Parameter scaling: Double the number of layers and see how `num_parameters`
    changes. Can you predict the increase?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In *llm.c*, parameters are not hidden inside classes or objects. They live in
    one flat block of memory, sliced up by convention into embeddings, attention matrices,
    MLP weights, and norms. This design makes the relationship between model architecture
    and memory crystal clear-and reminds you that even a billion-parameter transformer
    is “just” a giant array of numbers.
  prefs: []
  type: TYPE_NORMAL
- en: '23\. Embedding Tables: Token + Positional'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before a transformer can reason about text, it first needs to turn tokens into
    vectors. In *llm.c*, this job is handled by the embedding tables: one for tokens,
    one for positions. These tables are the very first layer of GPT-2, and they transform
    plain integer IDs into continuous values that the neural network can process.'
  prefs: []
  type: TYPE_NORMAL
- en: Token Embedding Table
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When you feed in a batch of token IDs, the model looks up their corresponding
    vectors in the token embedding table.
  prefs: []
  type: TYPE_NORMAL
- en: 'Shape: `[vocab_size, channels]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vocab_size ≈ 50,257` (for GPT-2)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`channels = hidden size` (768 for GPT-2 Small)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Each row corresponds to one token in the vocabulary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each row is a dense vector of size `channels`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So if your input batch has size `(B, T)`, looking up embeddings gives you a
    tensor of shape `(B, T, channels)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the code, this is implemented as an array slice from the flat parameter
    block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: At runtime, token IDs index directly into this table.
  prefs: []
  type: TYPE_NORMAL
- en: Positional Embedding Table
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Transformers don’t inherently know about word order. That’s what positional
    embeddings are for.
  prefs: []
  type: TYPE_NORMAL
- en: 'Shape: `[max_seq_len, channels]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_seq_len = 1024` in GPT-2 Small'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Same channel dimension as token embeddings
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Each position (0, 1, 2, …, 1023) has its own vector.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During training, when the model sees token `i` at position `j`, it takes the
    token embedding vector and adds the positional embedding vector for `j`. This
    gives the model both word identity and word position.
  prefs: []
  type: TYPE_NORMAL
- en: In *llm.c*, positional embeddings immediately follow the token embeddings in
    the flat parameter array.
  prefs: []
  type: TYPE_NORMAL
- en: Adding Them Together
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The embedding layer’s forward pass is simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: This results in a `(B, T, channels)` tensor that becomes the input to the first
    transformer block.
  prefs: []
  type: TYPE_NORMAL
- en: Why This Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Embeddings are the bridge between discrete tokens and continuous math. Without
    them, the model couldn’t use linear algebra to learn patterns. By adding positional
    embeddings, GPT-2 knows the difference between:'
  prefs: []
  type: TYPE_NORMAL
- en: “dog bites man” → `dog` comes first, `man` comes last
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “man bites dog” → same tokens, but swapped positions change the meaning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This small step is essential: order and identity must both be captured before
    attention can begin.'
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Inspect shapes: Print the sizes of the token and positional embedding tables
    during initialization. Confirm they match `[vocab_size, channels]` and `[max_seq_len,
    channels]`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Look at first rows: Print the first 5 vectors of the token embedding table.
    They should look like small random floats from initialization.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Change max_seq_len: Double `max_seq_len` in the config. How does this change
    the size of the positional table? Does training still work?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Overwrite embeddings: Try setting the token embedding table to all zeros. What
    happens to training loss?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Sampling experiment: After training a few steps, decode outputs without adding
    positional embeddings. Do the results become nonsensical or repetitive?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The embedding tables are the foundation of GPT-2\. Token embeddings give meaning
    to symbols, while positional embeddings give structure to sequences. In *llm.c*,
    they are just two slices of the flat parameter array, added together at the very
    start of the forward pass-but without them, the transformer would be blind to
    both words and order.
  prefs: []
  type: TYPE_NORMAL
- en: '24\. Attention Stack: QKV Projections and Geometry'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After embeddings, the real magic of transformers begins: the attention mechanism.
    In GPT-2, every transformer block contains an attention stack. This is where the
    model learns how each token relates to others in the sequence-whether it’s paying
    attention to the previous word, the beginning of a sentence, or even punctuation
    marks far away.'
  prefs: []
  type: TYPE_NORMAL
- en: What Attention Does
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Attention lets the model answer the question:'
  prefs: []
  type: TYPE_NORMAL
- en: “Given the current word, which other words in the context should I care about,
    and how much?”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Instead of treating words independently, the model uses attention to build connections
    across the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: The Q, K, V Projections
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Each attention block starts with three linear projections:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Shape | Purpose |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Q (Query) | `[channels, channels]` | Represents what each token is *asking*
    about. |'
  prefs: []
  type: TYPE_TB
- en: '| K (Key) | `[channels, channels]` | Represents how each token can be *recognized*.
    |'
  prefs: []
  type: TYPE_TB
- en: '| V (Value) | `[channels, channels]` | Represents the actual *information*
    to pass along. |'
  prefs: []
  type: TYPE_TB
- en: 'Here’s the flow:'
  prefs: []
  type: TYPE_NORMAL
- en: Each input vector (from embeddings or previous block) is multiplied by these
    three matrices to produce Q, K, and V vectors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Attention scores are computed by comparing Qs with Ks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These scores are used to weight the Vs, mixing information from other tokens
    into the current one.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Geometry of Attention
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Q and K define a similarity score: how well does this token match another one?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: V carries the actual features (like meaning, grammar cues).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The result is a weighted sum: tokens borrow information from others based on
    attention scores.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: The division by `sqrt(d_k)` normalizes scores so they don’t blow up as dimensions
    grow.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Head Attention
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'GPT-2 doesn’t use just one attention projection-it uses many in parallel, called
    heads. Each head learns to focus on different types of relationships:'
  prefs: []
  type: TYPE_NORMAL
- en: One head might track subject–verb agreement.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another might watch punctuation and quotes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another might connect pronouns to their referents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For GPT-2 Small:'
  prefs: []
  type: TYPE_NORMAL
- en: 12 heads per layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each head works on a reduced dimension (`channels / num_heads`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outputs are concatenated and projected back to `channels`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This setup is what gives transformers their flexibility.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation in *llm.c*
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the parameter array, each transformer block has slices for Q, K, V, and
    output projection (O). During forward pass:'
  prefs: []
  type: TYPE_NORMAL
- en: Multiply input by Q, K, V matrices.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reshape into heads.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute attention scores (masked to prevent looking forward).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply softmax.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Multiply by V to get weighted values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Concatenate heads and apply the O projection.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All of this is done with plain matrix multiplications and softmax calls-no magic
    beyond linear algebra.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Attention is the beating heart of GPT-2\. It’s how the model captures dependencies
    across text, from short-term grammar to long-range coherence. Without QKV, embeddings
    would stay isolated, and the model could never build context-aware representations.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Print shapes: Log the shapes of Q, K, V matrices in one layer. Confirm they
    match `[channels, channels]`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Visualize scores: After a forward pass, print the attention weights for one
    head. Do they concentrate on recent tokens or spread across the sequence?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Reduce heads: Change `num_heads` from 12 to 4\. What happens to validation
    loss?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Break symmetry: Initialize all Q, K, V matrices with zeros. Does training loss
    decrease at all?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Mask experiment: Disable the causal mask (allow looking ahead). Does the model
    “cheat” by predicting future tokens perfectly?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The attention stack is where tokens stop being isolated and start talking to
    each other. Q, K, and V projections turn context into weighted relationships,
    and multi-head attention lets the model juggle many types of dependencies at once.
    In *llm.c*, this is implemented with straightforward linear algebra, making the
    most powerful idea in modern NLP visible and accessible.
  prefs: []
  type: TYPE_NORMAL
- en: '25\. MLP Block: Linear Layers + Activation'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After attention mixes information across tokens, GPT-2 applies a second transformation
    inside each block: the MLP (Multi-Layer Perceptron). This part doesn’t look at
    other tokens-it processes each position independently. But it’s just as important
    because it gives the model extra capacity to transform and refine the hidden features
    before passing them to the next layer.'
  prefs: []
  type: TYPE_NORMAL
- en: What the MLP Looks Like
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Every transformer block contains an MLP with two linear layers and a nonlinear
    activation in between:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: This structure expands the feature dimension and then compresses it back down,
    which lets the network learn richer representations.
  prefs: []
  type: TYPE_NORMAL
- en: Shapes of the Layers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If the hidden size (channels) is `d_model`, the MLP works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Step | Shape | Purpose |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Input | `[B, T, d_model]` | Output of attention for each token. |'
  prefs: []
  type: TYPE_TB
- en: '| Linear1 | `[d_model, 4 × d_model]` | Expands features 4× wider. |'
  prefs: []
  type: TYPE_TB
- en: '| GELU | elementwise | Introduces nonlinearity. |'
  prefs: []
  type: TYPE_TB
- en: '| Linear2 | `[4 × d_model, d_model]` | Projects back to original size. |'
  prefs: []
  type: TYPE_TB
- en: '| Output | `[B, T, d_model]` | Same shape as input, ready for residual add.
    |'
  prefs: []
  type: TYPE_TB
- en: For GPT-2 Small (`d_model = 768`), Linear1 expands to 3072 channels, then Linear2
    reduces back to 768.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activation: GELU'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The activation function in GPT-2 is GELU (Gaussian Error Linear Unit). It’s
    smoother than ReLU, giving the model a more nuanced way of handling values around
    zero. In code, GELU looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'This formula may look complicated, but the idea is simple: it smoothly squashes
    negative values toward zero and keeps positive values flowing through.'
  prefs: []
  type: TYPE_NORMAL
- en: Why Expand and Shrink?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The expansion to `4 × d_model` may seem wasteful, but it’s deliberate:'
  prefs: []
  type: TYPE_NORMAL
- en: Expanding gives the model more capacity to represent patterns at each token.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shrinking keeps the overall parameter count manageable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Together, they act like a bottleneck layer that forces the model to transform
    information more effectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This “expand → activate → shrink” design is one of the main reasons transformers
    scale so well.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation in *llm.c*
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Just like attention, the MLP parameters live in the flat array of floats. Each
    block stores two weight matrices and two bias vectors. During forward pass:'
  prefs: []
  type: TYPE_NORMAL
- en: Multiply input by `Linear1` weights, add bias.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply GELU elementwise.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Multiply by `Linear2` weights, add bias.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pass result through residual connection.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Because each position is processed independently, the MLP is easy to parallelize
    across tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The MLP is the nonlinear refiner of transformer blocks. Attention spreads information,
    but MLPs transform it in-place, giving the model more expressive power. Without
    the MLP, the network would be mostly linear, limiting its ability to capture complex
    patterns in text.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Print shapes: Log the dimensions of Linear1 and Linear2 weights in one block.
    Do they match `[768, 3072]` and `[3072, 768]` for GPT-2 Small?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Swap activation: Replace GELU with ReLU in the code. Does training still work?
    How does validation loss compare?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Reduce expansion: Change expansion from 4× to 2× (`[768, 1536]`). What effect
    does this have on parameter count and performance?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Zero out MLP: Set MLP weights to zero. Does the model still learn anything,
    or does performance collapse?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compare speed: Measure training step time with and without the MLP enabled.
    How much slower is it?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The MLP block in GPT-2 is a simple two-layer network with GELU activation, applied
    independently to each token. It expands, activates, and compresses features, giving
    the model nonlinear power to reshape hidden states. In *llm.c*, it’s implemented
    with basic matrix multiplications and a smooth GELU function, proving that even
    small building blocks can have a big impact on the model’s ability to learn language.
  prefs: []
  type: TYPE_NORMAL
- en: '26\. LayerNorm: Theory and Implementation (`doc/layernorm`)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep neural networks often suffer from unstable training if activations drift
    too high or too low. To stabilize this, GPT-2 uses Layer Normalization (LayerNorm)
    inside every transformer block. In *llm.c*, LayerNorm is implemented directly
    in C, and there’s even a detailed explanation in the repo’s `doc/layernorm` file
    to help learners understand how it works.
  prefs: []
  type: TYPE_NORMAL
- en: The Idea of Normalization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When you pass vectors through many layers, their values can become unbalanced-some
    features dominate while others shrink. Normalization fixes this by:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Centering: subtracting the mean of the vector.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scaling: dividing by the standard deviation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This makes every feature vector have mean 0 and variance 1, improving stability.
  prefs: []
  type: TYPE_NORMAL
- en: Why “Layer” Norm?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are different kinds of normalization (BatchNorm, InstanceNorm, etc.).
    LayerNorm is special because:'
  prefs: []
  type: TYPE_NORMAL
- en: It normalizes across the features of a single token (the “layer”), not across
    the batch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This makes it independent of batch size, which is important for NLP where batch
    sizes can vary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So if a hidden vector has 768 channels, LayerNorm computes the mean and variance
    over those 768 numbers for each token.
  prefs: []
  type: TYPE_NORMAL
- en: Trainable Parameters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'LayerNorm isn’t just normalization-it also has two trainable vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: 'γ (gamma): scales each feature after normalization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'β (beta): shifts each feature after normalization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These allow the network to “undo” normalization when necessary, giving it flexibility.
  prefs: []
  type: TYPE_NORMAL
- en: Formula
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For each input vector `x` of size `d`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Where `eps` is a tiny constant (like `1e-5`) to avoid dividing by zero.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation in *llm.c*
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the code, LayerNorm is implemented as a simple function that loops over features,
    computes mean and variance, and applies the formula above. It’s not hidden inside
    a framework-it’s right there in C, so you can step through it line by line.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the forward pass looks like this (simplified):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: This is the kind of clear, low-level implementation that makes *llm.c* educational.
  prefs: []
  type: TYPE_NORMAL
- en: Where It Fits in GPT-2
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Each transformer block contains two LayerNorms:'
  prefs: []
  type: TYPE_NORMAL
- en: One before attention.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One before the MLP.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GPT-2 uses Pre-LN architecture: inputs are normalized before each sublayer.
    This makes training more stable and gradients flow better.'
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: LayerNorm may look like a small detail, but without it, GPT-2 would fail to
    train reliably. It smooths out the flow of activations so attention and MLP layers
    can do their job. In practice, this is one of the critical “glue” components that
    makes deep transformers trainable at scale.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Print statistics: After applying LayerNorm, print the mean and variance of
    the output. Do they stay close to 0 and 1?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Remove γ and β: Force gamma to 1 and beta to 0\. Does the model still train?
    Compare losses.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Disable normalization: Comment out LayerNorm and train. How unstable does training
    become?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compare positions: Try switching to Post-LN (apply normalization after attention/MLP).
    Does this change convergence speed?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Vary epsilon: Change `1e-5` to `1e-2` or `1e-8`. How sensitive is training?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: LayerNorm is the quiet stabilizer of GPT-2\. It makes sure each token’s features
    stay balanced, while γ and β keep flexibility. In *llm.c*, it’s implemented directly
    with clear C code, letting you see exactly how normalization is calculated. It’s
    a small but indispensable piece of the transformer puzzle.
  prefs: []
  type: TYPE_NORMAL
- en: '27\. Residual Connections: Keeping the Signal Flowing'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Transformers like GPT-2 don’t just stack layers on top of each other blindly.
    They use residual connections-a trick that allows the input of a layer to be added
    back to its output. This simple addition helps signals flow through the network
    without vanishing or exploding, and it makes training deep models possible.
  prefs: []
  type: TYPE_NORMAL
- en: The Basic Idea
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Imagine you have a function `F(x)` representing some transformation (like attention
    or an MLP). Instead of just computing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'the transformer does:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: This means the layer learns only the *difference* it needs to add to the input,
    instead of replacing it entirely.
  prefs: []
  type: TYPE_NORMAL
- en: Why This Helps
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Residuals solve two big problems in deep networks:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Gradient flow: During backpropagation, gradients can get smaller and smaller
    as they pass through many layers. Adding the input back ensures gradients always
    have a path straight through.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Information preservation: Even if `F(x)` distorts the signal, the original
    `x` is still there. This prevents the model from “forgetting” important information.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Faster training: The network doesn’t have to re-learn identity mappings-it
    can just pass them through the skip connection.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implementation in *llm.c*
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Residuals in *llm.c* are implemented as a straightforward elementwise addition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`inp1` is the output of the layer (like attention).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inp2` is the original input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`out` is the combined result.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is done for every token position and feature channel.
  prefs: []
  type: TYPE_NORMAL
- en: Where Residuals Are Used
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In GPT-2, every transformer block has two residuals:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Attention residual: Adds the input of the attention layer to its output.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'MLP residual: Adds the input of the MLP to its output.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So the data flowing through the network always carries both the new transformation
    and the original signal.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Without residual connections, stacking 12–48 transformer blocks would be nearly
    impossible to train. Gradients would vanish, and the model would either stop learning
    or take forever to converge. Residuals let deep transformers scale smoothly.
  prefs: []
  type: TYPE_NORMAL
- en: 'They also add an intuitive interpretation: each block is like a “refinement
    step” rather than a full rewrite of the representation.'
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Remove residuals: Comment out the addition in the code. Does training collapse?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scale residuals: Multiply the input by 0.5 before adding. Does this slow convergence?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Check loss curves: Compare training with and without residuals for the first
    500 steps.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Inspect outputs: Print the norms of `inp1`, `inp2`, and `out`. Are the scales
    balanced?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Deeper models: Increase the number of layers from 12 to 24\. Does the importance
    of residuals become more obvious?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Residual connections are the “lifeline” of deep transformers. By simply adding
    inputs back into outputs, they make it possible to train very deep networks without
    losing gradients or information. In *llm.c*, the implementation is as simple as
    looping over arrays and adding them-but the effect is profound: residuals are
    what let GPT-2 go deep and still work.'
  prefs: []
  type: TYPE_NORMAL
- en: '28\. Attention Masking: Enforcing Causality'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the defining traits of GPT-2 is that it’s a causal language model. That
    means it predicts the *next* token given all the tokens before it, but never cheats
    by looking ahead. To enforce this, GPT-2 applies an attention mask inside every
    attention layer.
  prefs: []
  type: TYPE_NORMAL
- en: Why a Mask Is Needed
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Without a mask, attention is free to connect any token to any other, including
    future ones. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: “The cat sat on the”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Target: “mat”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the model could peek at “mat” while computing attention, the task would be
    trivial-it could just copy the next word. That would break the training objective.
  prefs: []
  type: TYPE_NORMAL
- en: The mask forces the model to only use tokens at or before the current position
    when making predictions.
  prefs: []
  type: TYPE_NORMAL
- en: How the Mask Works
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When computing attention scores (`Q × K^T / sqrt(d_k)`), the result is a matrix
    of size `[T, T]` where each row corresponds to one token attending to all others.
  prefs: []
  type: TYPE_NORMAL
- en: 'The mask modifies this matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Allowed positions (past and present): keep scores as is.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Disallowed positions (future): set scores to `-inf`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After applying softmax, those `-inf` entries become zero probability, effectively
    blocking attention to the future.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation in *llm.c*
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The causal mask is applied during the attention forward pass. The code uses
    a loop to zero out invalid positions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Here `T` is the sequence length. This ensures that token `t` can only attend
    to itself and earlier tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the Mask
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Think of the mask as a triangular matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | 0 | 1 | 2 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | ✓ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | ✓ | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | ✓ | ✓ | ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | ✓ | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: Each row shows which past tokens a given position can look at. Future positions
    remain blank.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The mask is what makes GPT-2 a predictive model instead of a bidirectional encoder
    like BERT. Without it, the model could “cheat” and the training objective would
    no longer match how it’s used at inference time (generating text step by step).
  prefs: []
  type: TYPE_NORMAL
- en: This small detail-just filling part of a matrix with `-inf`-is critical to making
    autoregressive text generation possible.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Disable the mask: Comment out the masking code. Watch validation loss drop
    unrealistically, then notice that text generation produces garbage.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Reverse the mask: Block the past and allow the future. Does the model still
    train? What does it predict?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Partial mask: Only allow attention to the previous 5 tokens (a sliding window).
    How does this affect learning long-range structure?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Print scores: Before and after masking, log a row of attention scores. Notice
    how future positions become huge negatives.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Visualize: Write a small script to plot the attention mask as a matrix. It
    should look strictly lower-triangular.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Attention masking is a simple but essential trick. By filling future positions
    with `-inf` before softmax, GPT-2 ensures that each token can only attend to its
    past. In *llm.c*, this is implemented with just a couple of loops-but it’s what
    turns a generic transformer into a true causal language model.
  prefs: []
  type: TYPE_NORMAL
- en: '29\. Output Head: From Hidden States to Vocabulary'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After tokens pass through embeddings, attention, MLPs, LayerNorm, and residuals,
    we end up with hidden states for every position in the sequence. But GPT-2’s final
    job is not to output vectors-it must predict the next token from the vocabulary.
    This is handled by the output head, the last stage of the model.
  prefs: []
  type: TYPE_NORMAL
- en: What the Output Head Does
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The output head maps hidden states of shape `(B, T, channels)` into logits of
    shape `(B, T, vocab_size)`. Each logit represents the model’s “raw score” for
    how likely a particular token is at the next step.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipeline looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Logits: real numbers, one per token in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Softmax: converts logits into probabilities that sum to 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Predicted token: the token with the highest probability (or sampled from the
    distribution).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tied Weights with Embeddings
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In GPT-2, the token embedding table and the output head share weights. This
    means the same matrix is used both for:'
  prefs: []
  type: TYPE_NORMAL
- en: Mapping tokens to vectors at the start (embedding lookup).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mapping vectors back to tokens at the end (output head).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mathematically, this improves efficiency and helps align input and output representations.
  prefs: []
  type: TYPE_NORMAL
- en: In *llm.c*, this is done by simply pointing both embedding and output head to
    the same parameter slice.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: When the model projects hidden states back to vocab space, it does a matrix
    multiply with this shared matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Shapes in Action
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For GPT-2 Small:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hidden states: `[B, T, 768]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Output projection (embedding transpose): `[768, 50257]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Logits: `[B, T, 50257]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That’s more than 50k scores per position, one for each token in the vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: Why Weight Tying Helps
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Memory efficiency: You don’t need a separate giant matrix for the output head.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Better learning: The same vectors that represent tokens going in also represent
    them going out, which reinforces consistency.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Simpler code: Just reuse the same parameter slice.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This trick is why GPT-2 can scale vocab sizes without blowing up parameter counts
    too much.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The output head is where everything comes together. For each position, the model
    collapses its hidden representation into a distribution over possible next tokens.
    This is how GPT-2 generates text one step at a time. Without this step, you’d
    only have abstract hidden states-useful internally, but not something you can
    read.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Print logits: After a forward pass, print the logits for the last token. Do
    they look like random floats at initialization?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Check probability sum: Apply softmax to logits and verify the probabilities
    sum to 1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Untie weights: Make the output head its own matrix instead of reusing embeddings.
    Does training still work? How does the parameter count change?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Top-k sampling: Modify sampling to keep only the top 5 logits before softmax.
    What kind of text does this produce?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Greedy vs random: Compare greedy decoding (argmax) vs random sampling from
    probabilities. Which one gives more interesting outputs?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The output head is the final bridge between hidden vectors and actual words.
    By reusing the token embedding matrix, GPT-2 projects hidden states back into
    vocabulary space and produces logits for every possible token. In *llm.c*, this
    step is just another matrix multiplication-but it’s the one that turns internal
    math into real text predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '30\. Loss Function: Cross-Entropy over Vocabulary'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Training GPT-2 means teaching it to predict the next token in a sequence. To
    measure how well it’s doing, we need a loss function that compares the model’s
    predicted probabilities with the true token IDs. In *llm.c*, this is done with
    the cross-entropy loss-a standard choice for classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: From Logits to Probabilities
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'After the output head, we have logits of shape `(B, T, vocab_size)`. These
    are raw scores. To turn them into probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Softmax ensures two things:'
  prefs: []
  type: TYPE_NORMAL
- en: All values are between 0 and 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They sum to 1 across the vocabulary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So for each position, you get a probability distribution over all possible next
    tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-Entropy Definition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Cross-entropy compares the predicted distribution `p` with the true distribution
    `q`. For language modeling:'
  prefs: []
  type: TYPE_NORMAL
- en: '`q` is a one-hot vector (all zeros, except 1 at the true token index).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`p` is the probability vector from softmax.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The formula for one token:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: For a batch, you average across all tokens in all sequences.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation in *llm.c*
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In C, this boils down to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: This snippet shows how *llm.c* explicitly computes softmax and cross-entropy
    in loops. No black boxes-just raw math.
  prefs: []
  type: TYPE_NORMAL
- en: Intuition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If the model assigns high probability to the correct token → loss is small.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the model assigns low probability to the correct token → loss is large.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Minimizing loss means pushing probability mass toward the right answers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why Cross-Entropy Works for Language
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Language modeling is essentially a huge multi-class classification problem:
    at each step, which word comes next? Cross-entropy is perfect here because it
    directly penalizes wrong predictions proportional to how confident the model was.'
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The loss function is the only signal the model gets about how well it’s doing.
    Everything else-parameter updates, weight tuning, learning dynamics-flows from
    this single number. A well-implemented cross-entropy ensures training is stable
    and meaningful.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Check values: Print the loss after the first few steps. It should be close
    to `log(vocab_size)` (≈10.8 for 50k vocab) before training.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Overfit tiny batch: Train on just one sequence. Does the loss go near 0 after
    enough steps?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Change target: Replace the true token with a random one. Does the loss increase
    immediately?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compare vocab sizes: Train with a smaller vocabulary (e.g., 100 tokens). Does
    initial loss drop to `log(100) ≈ 4.6`?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Inspect probabilities: For one token, print the top 5 predicted probabilities.
    Does the true token climb to the top as training progresses?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The cross-entropy loss is the compass guiding GPT-2 during training. It turns
    raw logits into probabilities and measures how well the model predicts the correct
    next token. In *llm.c*, it’s implemented with explicit loops and math, letting
    you see exactly how probabilities and losses are computed. Without this step,
    the model would have no way to learn from its mistakes.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 4\. CPU Inference (Forward Only)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 31\. Forward Pass Walkthrough
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When we talk about the *forward pass* in GPT-2, we mean the process of turning
    an input sentence (like “The cat sat on the”) into predictions for the next word.
    In simple terms, it’s how the model “thinks” before giving an answer. In `train_gpt2.c`,
    this happens inside the function `gpt2_forward`. Let’s walk through it slowly,
    step by step, so you can see how numbers flow through the model and transform
    along the way.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. From Words to Numbers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Computers don’t understand words like *cat* or *sat*. They only understand
    numbers. Before the forward pass starts, text is already tokenized into IDs (integers).
    For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Each number is a token ID. The model doesn’t yet know what “464” means in plain
    English-it just knows it’s a number that points into a table.
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Embedding: Giving Words Meaning'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The first real step in the forward pass is embedding lookup. Imagine we have
    a huge dictionary, but instead of definitions in English, each word ID points
    to a long vector of numbers (say, 768 numbers for GPT-2 small).
  prefs: []
  type: TYPE_NORMAL
- en: 'Word embeddings (`wte`): Each token ID becomes a vector that captures the meaning
    of the word.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Position embeddings (`wpe`): Each token also gets a vector for its position:
    first word, second word, third word, etc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model adds these two vectors together. This way, it knows not just what
    the word is, but also where it is in the sentence.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Token | Word | Word Embedding (shortened) | Position Embedding (shortened)
    | Combined Vector |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 464 | “The” | [0.2, -0.5, 0.1, …] | [0.0, 0.1, -0.3, …] | [0.2, -0.4, -0.2,
    …] |'
  prefs: []
  type: TYPE_TB
- en: '| 3290 | “cat” | [0.9, -0.2, 0.4, …] | [0.1, -0.1, -0.2, …] | [1.0, -0.3, 0.2,
    …] |'
  prefs: []
  type: TYPE_TB
- en: Now every token is a vector with both meaning and position built in.
  prefs: []
  type: TYPE_NORMAL
- en: '3\. Transformer Layers: The Thinking Steps'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'GPT-2 has multiple identical layers stacked on top of each other. Each layer
    has two big parts: attention and MLP (feed-forward network).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Attention (looking around):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each word asks: “Which other words should I pay attention to right now?”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For “sat,” attention might focus heavily on “cat,” because those words are related.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code computes *queries*, *keys*, and *values* for every word, then does
    dot-products, softmax, and weighted sums to mix information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MLP (processing deeply):'
  prefs: []
  type: TYPE_NORMAL
- en: After attention, each token passes through a mini neural network (two matrix
    multiplications with a nonlinear GELU function in between).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This helps each word refine its understanding, even if it doesn’t directly attend
    to another word.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Both blocks have residual connections: the input is added back to the output,
    like keeping the original notes while adding new insights. This prevents information
    loss.'
  prefs: []
  type: TYPE_NORMAL
- en: '4\. Normalization: Keeping Numbers Stable'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: At many points, the model normalizes vectors so they don’t explode in size or
    shrink too small. This is called LayerNorm. It ensures training is stable, like
    making sure your cooking pot doesn’t boil over or dry out.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. The Final Prediction Layer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'After all layers, the model produces a final vector for each position. Then:'
  prefs: []
  type: TYPE_NORMAL
- en: It multiplies those vectors by the embedding table again (but transposed).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This gives logits: raw scores for each word in the vocabulary (about 50k options).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: for the last token “on the,” the logits might be:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Word | Logit | Probability (after softmax) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| “mat” | 7.2 | 0.85 |'
  prefs: []
  type: TYPE_TB
- en: '| “dog” | 5.1 | 0.10 |'
  prefs: []
  type: TYPE_TB
- en: '| “car” | 3.0 | 0.05 |'
  prefs: []
  type: TYPE_TB
- en: The highest probability is “mat.”
  prefs: []
  type: TYPE_NORMAL
- en: '6\. Softmax: Turning Scores into Probabilities'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The logits are big numbers, but they don’t mean much until we apply softmax.
    Softmax makes them into probabilities that sum to 1\. This way, we can interpret
    them as chances: “There’s an 85% chance the next word is *mat*.”'
  prefs: []
  type: TYPE_NORMAL
- en: '7\. Cross-Entropy Loss: Measuring Mistakes'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If we’re training, we also give the model the correct next word. The model checks
    how much probability it gave to that word. If it gave it high probability, the
    loss is low. If it gave it low probability, the loss is high.
  prefs: []
  type: TYPE_NORMAL
- en: 'Correct: “mat” (probability 0.85 → loss ≈ 0.16, small).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wrong: “car” (probability 0.05 → loss ≈ 3.0, large).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This loss is averaged across all tokens, and it’s the signal that tells the
    backward pass how to update the model.
  prefs: []
  type: TYPE_NORMAL
- en: 8\. Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The forward pass is the part of GPT-2 that generates predictions. Without it,
    the model can’t “think” or make sense of input. It’s like the brain processing
    sensory input before deciding what to do. In `train_gpt2.c`, the forward pass
    is written with plain C loops, which makes the math crystal clear instead of hidden
    inside deep learning libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 9\. Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Print embeddings: Modify the code to print the vector for the first token.
    See how it’s just numbers, but those numbers are the “meaning” of the word.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Inspect probabilities: After the forward pass, print the softmax probabilities
    for one position. They should sum to 1.0.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Change sequence length: Increase `T` from 64 to 128\. Notice how validation
    slows down, because attention compares all tokens with all others (`T²` scaling).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Baseline loss: Before training, measure the loss. It should be around `log(vocab_size)`
    (≈10.8 for GPT-2 small). That’s the loss of random guessing.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Mask experiment: Temporarily remove the causal mask in attention. The model
    will “cheat” by looking ahead, and loss will drop unrealistically.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The forward pass is like the thought process of GPT-2\. Input words become vectors,
    vectors mix through attention and MLPs, everything gets normalized, and finally
    the model produces probabilities for the next word. It’s a carefully choreographed
    dance of math operations, all coded in plain C loops in `train_gpt2.c`. Once you
    understand this flow, you can follow exactly how GPT-2 turns raw tokens into intelligent
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 32\. Token and Positional Embedding Lookup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before GPT-2 can do anything intelligent with text, it needs to turn raw numbers
    (token IDs) into vectors that capture meaning and context. This is the role of
    embeddings. In `train_gpt2.c`, this step is handled by the function `encoder_forward`.
    Let’s take a closer look at how it works and why it matters.
  prefs: []
  type: TYPE_NORMAL
- en: Tokens Are Just Numbers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Suppose you type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'After tokenization, this sentence might look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: These are just IDs. The model doesn’t inherently know that `3290` means “cat.”
    It only knows it needs to use these numbers to fetch vectors from a table.
  prefs: []
  type: TYPE_NORMAL
- en: The Embedding Tables
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The model has two important tables stored in memory:'
  prefs: []
  type: TYPE_NORMAL
- en: Word Token Embeddings (`wte`)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Size: `(V, C)` where `V` is vocab size (~50,000 for GPT-2 small) and `C` is
    channels (768).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Each row corresponds to a token ID.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: row 3290 might be `[0.12, -0.45, 0.88, …]`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Positional Embeddings (`wpe`)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Size: `(maxT, C)` where `maxT` is the maximum sequence length (e.g. 1024).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each row corresponds to a position index: 0 for the first token, 1 for the
    second, etc.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: position 2 might be `[0.07, 0.31, -0.22, …]`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Both tables are filled with trainable values. At the start, they’re random.
    As training progresses, the optimizer updates them so they encode useful patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Adding Them Together
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For each token at position `t`:'
  prefs: []
  type: TYPE_NORMAL
- en: Look up its word vector from `wte`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Look up its position vector from `wpe`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add them elementwise.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This gives a final vector of size `C` that represents what the token is and
    where it is.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example with simplified numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Token ID | Word | Word Embedding | Position | Combined |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 464 | The | [0.1, -0.2, 0.3] | [0.2, 0.0, -0.1] | [0.3, -0.2, 0.2] |'
  prefs: []
  type: TYPE_TB
- en: '| 3290 | cat | [0.4, 0.5, -0.3] | [0.0, 0.1, 0.2] | [0.4, 0.6, -0.1] |'
  prefs: []
  type: TYPE_TB
- en: Now the vector doesn’t just mean “cat,” it means “cat at position 1.”
  prefs: []
  type: TYPE_NORMAL
- en: Why Position Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Without positions, the model would treat:'
  prefs: []
  type: TYPE_NORMAL
- en: “The cat sat”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Sat cat the”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: as identical, because they use the same tokens. But word order is essential
    in language. By adding positional embeddings, GPT-2 knows the difference between
    “dog bites man” and “man bites dog.”
  prefs: []
  type: TYPE_NORMAL
- en: Inside the Code
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The embedding lookup is written explicitly with loops in C:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'What’s happening here:'
  prefs: []
  type: TYPE_NORMAL
- en: Loop over batches (`b`) and sequence positions (`t`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find the token ID `ix`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fetch its embedding `wte_ix`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fetch its position embedding `wpe_t`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add them element by element.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The result, `out_bt`, is the vector for this token at this position.
  prefs: []
  type: TYPE_NORMAL
- en: Analogy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Think of it like name tags at a conference:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The word embedding is your name: “Alice.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The position embedding is your table number: “Table 7.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Together, they tell the conference staff who you are and where you are seated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Without the table number, they might know who you are but not where to find
    you. Without your name, they just know there’s someone at Table 7 but not who.
    Both are needed for proper context.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Embeddings are the foundation of the whole model. If this step is wrong, everything
    else collapses. They transform meaningless IDs into rich vectors that carry semantic
    and positional information. This is the entry point where language starts becoming
    something a neural network can reason about.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Print a token embedding: Modify the code to print out `wte_ix` for a specific
    token ID like “cat.” You’ll see a vector of floats, the learned representation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Print a position embedding: Do the same for `wpe_t` at position 0, 1, 2… Notice
    how positions have unique but consistent patterns.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Check the sum: Verify that `out_bt[i] = wte_ix[i] + wpe_t[i]`. This is literally
    how word and position are fused.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Shuffle words: Try feeding “cat sat” vs. “sat cat.” The embeddings will differ
    because the position vectors change, even though the words are the same.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Observe growth during training: After some training steps, dump the embeddings
    again. You’ll notice they stop being random and start showing structure.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The embedding lookup is the very first step of the forward pass. It takes raw
    numbers and makes them meaningful by combining token identity and position. This
    prepares the input for the deeper transformer layers. Even though the C code looks
    simple-a few nested loops-it’s doing the crucial work of giving words a mathematical
    shape the model can understand.
  prefs: []
  type: TYPE_NORMAL
- en: '33\. Attention: Matmuls, Masking, and Softmax on CPU'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The attention mechanism is the heart of GPT-2\. It’s where each word in the
    input sequence decides which other words to look at when forming its representation.
    In `train_gpt2.c`, this happens inside the `attention_forward` function, which
    implements multi-head self-attention using plain C loops and matrix multiplications.
    Let’s break it down carefully, step by step, so even an absolute beginner can
    follow the flow.
  prefs: []
  type: TYPE_NORMAL
- en: The Big Idea of Attention
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Imagine you’re reading:'
  prefs: []
  type: TYPE_NORMAL
- en: “The cat sat on the mat.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: When the model is trying to understand the word *sat*, it doesn’t just look
    at *sat* by itself. It wants to consider other words like *cat* (the subject)
    and *mat* (likely the object). Attention gives each token a way to “consult” earlier
    tokens and decide how important they are.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is done mathematically by projecting each token into three roles: Query
    (Q), Key (K), and Value (V).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Query (Q): “What am I looking for?”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Key (K): “What do I offer?”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Value (V): “What information do I carry?”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 1: Creating Q, K, and V'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For every input vector of size `C` (e.g., 768), the code performs three separate
    linear projections (matrix multiplications). These produce Q, K, and V vectors
    of smaller size, divided among attention heads.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`acts.ln1` is the normalized input from the previous step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`params.wq`, `params.wk`, `params.wv` are the weight matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output shapes are `(B, T, C)`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So each token now has three new representations: Q, K, and V.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Computing Attention Scores'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For each token at position `t`, we want to know how much it should pay attention
    to every earlier token (including itself). This is done with a dot product between
    its Query and all Keys.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '`t` = current token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`u` = another token at or before `t`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sqrt(dk)` is a scaling factor (dk = size of each head) to keep values stable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the code, these dot products are done explicitly in loops.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Applying the Causal Mask'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'GPT-2 is an autoregressive model, meaning it only predicts the future from
    the past, not the other way around. To enforce this, the attention matrix is masked:'
  prefs: []
  type: TYPE_NORMAL
- en: Token at position `t` can only look at positions `≤ t`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anything beyond `t` is set to a very negative value (`-1e9`), which becomes
    effectively zero after softmax.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This ensures, for example, that when predicting the 3rd word, the model doesn’t
    cheat by looking at the 4th.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: Turning Scores into Probabilities'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The scores are raw numbers that can be large and unstable. To convert them
    into meaningful weights, the code applies softmax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: This makes all weights positive and ensures they sum to 1\. Now each token has
    a probability distribution over earlier tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example for the word *sat*:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Attended Token | Raw Score | After Softmax |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| The | 1.2 | 0.10 |'
  prefs: []
  type: TYPE_TB
- en: '| cat | 3.4 | 0.80 |'
  prefs: []
  type: TYPE_TB
- en: '| sat (itself) | 0.7 | 0.10 |'
  prefs: []
  type: TYPE_TB
- en: '| on | masked | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: Clearly, *sat* focuses most strongly on *cat*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 5: Weighted Sum of Values'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Once the attention weights are computed, the model uses them to take a weighted
    sum of the Value vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: This produces a new representation for each token that blends in information
    from others.
  prefs: []
  type: TYPE_NORMAL
- en: For *sat*, its new vector will be mostly influenced by *cat*, but also a little
    by *The* and itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 6: Multi-Head Attention'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In practice, attention is split into multiple heads (12 for GPT-2 small). Each
    head works on smaller chunks of the vector (C/heads).
  prefs: []
  type: TYPE_NORMAL
- en: Head 1 might focus on subject–verb relationships.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Head 2 might track distances (like “how far back was this token?”).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Head 3 might specialize in punctuation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After all heads compute their outputs, the results are concatenated and projected
    back into size `C` with another matrix multiply.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 7: Residual Connection'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Finally, the output of the attention block is added back to the original input
    (residual connection). This keeps the original signal flowing, even if the attention
    introduces distortions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: This ensures information isn’t lost and gradients flow smoothly during training.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Attention is the mechanism that lets GPT-2 capture relationships between words.
    Without it, the model would treat each token independently, losing context. By
    explicitly computing “who should I look at?” for every token, GPT-2 learns patterns
    like subject–verb agreement, long-distance dependencies, and even stylistic nuances.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Inspect the attention mask: Print out the scores before and after masking.
    Notice how future tokens are set to huge negative values.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Visualize weights: Run attention on a short sentence and plot the weights.
    You’ll see which words attend to which.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Change sequence length: Try increasing `T` and observe how computation grows
    quadratically (`T²`). Attention is expensive!'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Experiment with heads: Force the model to use only 1 head instead of 12\. See
    how this limits the diversity of patterns it can capture.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Check sum of weights: For one token, verify that all attention weights add
    up to 1.0 after softmax.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Attention is what makes transformers powerful. It allows each word to dynamically
    decide which other words matter for understanding its role in a sentence. In `train_gpt2.c`,
    this process is spelled out with explicit loops and matrix multiplications, so
    you can follow every step of the math. Understanding this section gives you the
    key to why GPT-2-and all modern LLMs-work so well.
  prefs: []
  type: TYPE_NORMAL
- en: '34\. MLP: GEMMs and Activation Functions'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After the attention block lets tokens “talk to each other,” GPT-2 applies a
    second kind of transformation called the MLP block (multi-layer perceptron). Unlike
    attention, which mixes information between tokens, the MLP processes each token
    independently, enriching its internal representation. Even though it looks simpler
    than attention, the MLP is essential for capturing complex relationships in language.
  prefs: []
  type: TYPE_NORMAL
- en: What the MLP Does
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Every token’s vector (size `C`, e.g., 768 for GPT-2 small) goes through:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Linear expansion: project from size `C` to size `4C` (3072 in GPT-2 small).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Nonlinear activation: apply the GELU function, which adds flexibility.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Linear projection back: reduce size from `4C` back to `C`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Residual connection: add the input vector back to the output, keeping the original
    signal intact.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This allows the model to not only share information between tokens (via attention)
    but also refine how each token represents itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Expanding with a Matrix Multiply'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The first step is to expand each token’s vector from 768 to 3072 dimensions.
    This is done with a general matrix multiply (GEMM):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '`acts.ln2`: the normalized input from the previous residual.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`params.wfc`: the weight matrix of size `(C, 4C)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`params.bfc`: bias vector of size `(4C)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`acts.mlp_in`: the result, shape `(B, T, 4C)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Think of it like stretching a rubber band-suddenly, the token has much more
    room to express richer features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: GELU Activation'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: After expansion, each number passes through GELU (Gaussian Error Linear Unit).
  prefs: []
  type: TYPE_NORMAL
- en: 'The formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'This looks complicated, but the key idea is:'
  prefs: []
  type: TYPE_NORMAL
- en: For small negative numbers, output ≈ 0 (ignore weak signals).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For large positive numbers, output ≈ x (pass strong signals).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For numbers in between, it smoothly blends.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unlike ReLU, which just chops off negatives, GELU lets small signals through
    in a probabilistic way. This makes it better for language, where even small hints
    matter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Analogy: Imagine you’re grading homework. If an answer is completely wrong,
    you give 0 points (ReLU style). If it’s perfect, you give full credit. But if
    it’s partially right, you give partial credit. GELU behaves like that-soft, nuanced
    grading.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Projecting Back Down'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Once the token vector has been expanded and passed through GELU, it’s projected
    back to the original size `C`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '`params.wproj`: the projection weights, size `(4C, C)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`params.bproj`: bias, size `(C)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`acts.mlp_out`: result, shape `(B, T, C)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now each token has gone through a non-linear “thinking step,” mixing and reshaping
    features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: Residual Connection'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Just like with attention, the MLP output is added back to the input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: This means the token keeps its old representation while adding the new refinements.
    If the MLP makes a mistake early in training, the residual ensures the token doesn’t
    lose all its meaning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inside the Code: Simplicity'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Even though MLPs in deep learning libraries like PyTorch are one-liners (`nn.Linear`
    + `nn.GELU` + `nn.Linear`), here in C you see every step spelled out:'
  prefs: []
  type: TYPE_NORMAL
- en: First GEMM expands to 4C.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loop applies GELU element by element.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Second GEMM projects back to C.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Residual adds input and output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s like watching a magician reveal the trick instead of just seeing the final
    illusion.
  prefs: []
  type: TYPE_NORMAL
- en: Why the Expansion Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You might ask: why expand to 4C and then shrink back? Why not just keep the
    size the same?'
  prefs: []
  type: TYPE_NORMAL
- en: The expansion allows the model to capture more complicated combinations of features.
    By spreading information out, applying a nonlinear transformation, and then compressing
    it again, the model can discover patterns that wouldn’t fit in the smaller space.
  prefs: []
  type: TYPE_NORMAL
- en: Think of it like brainstorming on a huge whiteboard. You spread out all your
    ideas (4C), reorganize them, and then condense the best ones into a neat summary
    (C).
  prefs: []
  type: TYPE_NORMAL
- en: Example Walkthrough
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s say we’re processing the token “cat” in the sentence *The cat sat*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input vector (size 768): `[0.12, -0.08, 0.33, …]`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After first matrix multiply: expanded to `[1.2, -0.9, 0.5, …]` (size 3072).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After GELU: `[1.1, -0.0, 0.4, …]` (smooth nonlinearity).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After projection: back to `[0.15, -0.02, 0.27, …]` (size 768).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add back original input: `[0.27, -0.10, 0.60, …]`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now “cat” has been enriched with new internal features that help the model predict
    what comes next.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The MLP is the part of GPT-2 that lets each token refine itself. Attention gives
    context from neighbors, but the MLP deepens the representation of the token itself.
    Without it, the model would lack the ability to detect fine-grained patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Print intermediate sizes: Add debug prints to see how token vectors grow to
    4C and shrink back to C.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Swap activation: Replace GELU with ReLU in the code and train. Compare losses-you’ll
    notice GPT-2 prefers GELU.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Disable residual: Temporarily remove the residual add. Watch how the model
    struggles to learn, because it can’t preserve information.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Visualize values: Track how many values are near 0 before and after GELU. You’ll
    see GELU softly zeroes out weak signals.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Smaller expansion: Try changing 4C to 2C in the code. You’ll save memory but
    lose accuracy, since the MLP has less expressive power.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The MLP block is a token’s personal deep thinker. It stretches the representation
    wide, filters it through GELU, compresses it again, and then adds it back to the
    original. While attention handles the conversation between words, the MLP ensures
    each word processes and refines its own role. Together, they create the layered
    reasoning ability that makes GPT-2 so powerful.
  prefs: []
  type: TYPE_NORMAL
- en: 35\. LayerNorm on CPU (Step-by-Step)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the most important but often overlooked ingredients in GPT-2 is Layer
    Normalization, or LayerNorm for short. While attention and MLPs are the big stars,
    LayerNorm is like the stage crew keeping everything running smoothly behind the
    scenes. It ensures the numbers flowing through the network stay stable and balanced,
    preventing explosions or collapses that could make training impossible. In `train_gpt2.c`,
    LayerNorm is implemented with explicit loops so you can see every calculation.
    Let’s walk through it carefully.
  prefs: []
  type: TYPE_NORMAL
- en: Why Do We Need Normalization?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Imagine a classroom where every student talks at different volumes. Some whisper,
    some shout. If you try to listen to all of them at once, the loud voices drown
    out the quiet ones.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks face a similar problem. The outputs of layers can have wildly
    different scales. If one dimension of a vector is much larger than the others,
    it dominates. Training becomes unstable, and gradients may vanish or explode.
  prefs: []
  type: TYPE_NORMAL
- en: 'LayerNorm fixes this by ensuring that, for each token at each layer, the vector
    has:'
  prefs: []
  type: TYPE_NORMAL
- en: Mean = 0 (centered around zero)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variance = 1 (consistent spread of values)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After that, trainable parameters scale and shift the result so the model can
    still learn flexible transformations.
  prefs: []
  type: TYPE_NORMAL
- en: The Math Behind LayerNorm
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For a given token vector `x` of size `C` (e.g., 768):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute mean:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute variance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Normalize:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: where `ε` is a tiny constant (like 1e-5) to avoid division by zero.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Scale and shift with trainable weights `g` (gamma) and `b` (beta):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: So the final output has controlled statistics, but still enough flexibility
    for the model to adjust.
  prefs: []
  type: TYPE_NORMAL
- en: The Code in `train_gpt2.c`
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here’s a simplified version from the repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice the structure:'
  prefs: []
  type: TYPE_NORMAL
- en: Outer loops over batch `B` and sequence length `T`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inner loops compute mean, variance, and then apply normalization per token vector
    of length `C`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weight` and `bias` are the learnable gamma and beta.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is exactly what LayerNorm means: normalize *each layer’s inputs per token*.'
  prefs: []
  type: TYPE_NORMAL
- en: Example Walkthrough
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Suppose we have a single token vector (C=4) = `[2.0, -1.0, 3.0, 0.0]`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mean: `(2 - 1 + 3 + 0)/4 = 1.0`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Variance: `((2-1)² + (-1-1)² + (3-1)² + (0-1)²)/4 = (1 + 4 + 4 + 1)/4 = 2.5`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Normalize: subtract mean and divide by sqrt(2.5):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Scale and shift (say weight=[1,1,1,1], bias=[0,0,0,0]):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now the vector has mean 0, variance 1, and is ready for the next layer.
  prefs: []
  type: TYPE_NORMAL
- en: Analogy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Think of LayerNorm like a baking recipe. If one ingredient is way too strong
    (like adding five times too much salt), the whole dish is ruined. LayerNorm tastes
    the mixture, balances all the flavors, and then lets you adjust the seasoning
    with learnable gamma (scale) and beta (shift).
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Without LayerNorm, the model would quickly become unstable:'
  prefs: []
  type: TYPE_NORMAL
- en: Some tokens would dominate, while others fade.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradients could explode, making loss jump wildly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training would be inconsistent between batches.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With LayerNorm, each layer works with clean, normalized inputs. This allows
    deeper stacks of attention and MLP blocks to learn reliably.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Print statistics: Add debug code to check the mean and variance before and
    after LayerNorm. Before: mean ≠ 0, variance ≠ 1\. After: mean ≈ 0, variance ≈
    1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Remove LayerNorm: Comment out LayerNorm in the code. Watch training collapse-loss
    will not decrease properly.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Change epsilon: Try making `ε = 1e-1` or `ε = 1e-12`. See how too large or
    too small values can break stability.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Observe gamma and beta: Initialize gamma=1, beta=0\. During training, watch
    how these parameters drift, fine-tuning normalization.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Experiment with batch norm: Replace LayerNorm with BatchNorm (not typical for
    transformers). You’ll see it doesn’t work well, because transformers process variable-length
    sequences where per-batch statistics vary too much.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'LayerNorm is the quiet but critical stabilizer in GPT-2\. It ensures every
    token vector is balanced, centered, and scaled before moving into attention or
    MLP. In `train_gpt2.c`, you see exactly how it works: compute mean, compute variance,
    normalize, then scale and shift. Even though it’s just a few lines of C code,
    it’s one of the main reasons deep transformers can stack dozens of layers without
    breaking.'
  prefs: []
  type: TYPE_NORMAL
- en: 36\. Residual Adds and Signal Flow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once embeddings, attention, and MLP blocks are computed, there’s still one
    piece left to keep the whole network stable and effective: residual connections.
    In `train_gpt2.c`, these appear in functions like `residual_forward`, where outputs
    of a layer are added back to their inputs. This simple-looking step is one of
    the key reasons GPT-2 and other deep transformer models can stack many layers
    without collapsing.'
  prefs: []
  type: TYPE_NORMAL
- en: The Core Idea
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A residual connection says:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: Instead of replacing the old representation with the new one, the model adds
    them together. That way, the original signal always survives, even if the new
    transformation is noisy or imperfect.
  prefs: []
  type: TYPE_NORMAL
- en: Think of it like taking lecture notes. Each time the teacher explains more,
    you don’t throw away your old notes. You add new details next to them. That way,
    you preserve everything learned so far, while layering new insights on top.
  prefs: []
  type: TYPE_NORMAL
- en: Why Residuals Are Crucial
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Preventing information loss: If you only applied transformations, some features
    might vanish forever. Adding the input back ensures no information is lost.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Helping gradients flow: During backpropagation, gradients must travel backward
    through many layers. Without shortcuts, they can vanish or explode. Residuals
    create direct paths for gradients, making learning stable.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Improving training speed: With residuals, deeper networks converge faster because
    the model can “skip” bad transformations while still using the identity mapping.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Code in `train_gpt2.c`
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here’s the implementation of residual addition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'It’s deceptively simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '`inp1` is the original input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inp2` is the new transformation (from attention or MLP).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`out` stores the sum.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`N` is the total number of floats (`B * T * C`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even though it’s just a single line inside a loop, this is what makes stacking
    12+ transformer blocks possible.
  prefs: []
  type: TYPE_NORMAL
- en: Example Walkthrough
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Suppose we’re processing the token “cat.”
  prefs: []
  type: TYPE_NORMAL
- en: 'Input vector (simplified, size=3): `[0.5, -0.3, 0.7]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After attention block: `[0.2, 0.1, -0.4]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Residual addition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: Now the representation of “cat” contains both the original signal and the contextual
    information from attention.
  prefs: []
  type: TYPE_NORMAL
- en: 'Later, after the MLP:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input again: `[0.7, -0.2, 0.3]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MLP output: `[0.1, -0.5, 0.4]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Residual: `[0.8, -0.7, 0.7]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Step by step, the vector grows richer without losing its foundation.
  prefs: []
  type: TYPE_NORMAL
- en: Analogy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Residual connections are like building layers in Photoshop. Each new layer adds
    adjustments, but you always keep the original photo underneath. If a new adjustment
    is bad, you can still see the original. This makes the final composition stronger
    and safer to experiment with.
  prefs: []
  type: TYPE_NORMAL
- en: Residuals Across the Model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In GPT-2’s forward pass, residuals appear in two main places inside each transformer
    block:'
  prefs: []
  type: TYPE_NORMAL
- en: 'After attention:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After MLP:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Together with LayerNorm before each block, these form the backbone of the transformer
    architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Without residual connections, GPT-2 would struggle to train past a few layers.
    Deeper stacks would lose track of the original signal, gradients would vanish,
    and performance would stall. Residuals are the glue that holds the whole architecture
    together, enabling models with billions of parameters to train effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Remove residuals: Temporarily comment out the `residual_forward` calls. Training
    will quickly fail-the loss won’t decrease properly.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Print before/after: Inspect a token’s vector before and after residual add.
    Notice how the numbers change smoothly rather than being overwritten.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Experiment with scaling: Try replacing `out[i] = inp1[i] + inp2[i];` with `out[i]
    = inp1[i] + 0.1 * inp2[i];`. This reduces the impact of the new transformation-sometimes
    used in advanced architectures.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compare to skip-less RNNs: Research how older recurrent networks without residuals
    had trouble scaling deep. You’ll see why residuals are a game changer.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Chain of signals: Track how a single token’s vector evolves across all 12 layers.
    You’ll notice it keeps its core identity while absorbing new context.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Residual connections may look like a simple addition, but they’re the key to
    deep learning’s success in transformers. They preserve information, stabilize
    training, and allow GPT-2 to stack many layers without falling apart. In `train_gpt2.c`,
    this idea is laid bare: a few lines of C code implementing one of the most powerful
    tricks in modern neural networks.'
  prefs: []
  type: TYPE_NORMAL
- en: 37\. Cross-Entropy Loss on CPU
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After embeddings, attention, MLPs, LayerNorm, and residuals have done their
    job, the model produces logits-raw scores for every word in the vocabulary at
    every position in the sequence. But logits alone don’t tell us if the model is
    “good” or “bad” at predicting the right word. To measure performance and guide
    learning, GPT-2 uses the cross-entropy loss function. In `train_gpt2.c`, this
    is implemented in the function `crossentropy_forward`.
  prefs: []
  type: TYPE_NORMAL
- en: What Are Logits?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'At the final stage of the forward pass, each token position has a vector of
    length `V` (vocabulary size, ~50k). For example, the model might produce these
    logits for a tiny vocabulary of 3 words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: Logits are just numbers-bigger means “more likely,” smaller means “less likely”-but
    they aren’t probabilities yet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Softmax – Turning Scores Into Probabilities'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To compare predictions with the true target, we first convert logits into probabilities.
    The tool for this is the softmax function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'Softmax has two important effects:'
  prefs: []
  type: TYPE_NORMAL
- en: It makes all values positive.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It normalizes them so they sum to 1, forming a probability distribution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Logits: `[5.2, 1.1, -2.7]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Subtract max (5.2) for stability → `[0.0, -4.1, -7.9]`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exponentiate → `[1.0, 0.017, 0.0004]`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalize → `[0.98, 0.017, 0.0004]`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now the model is saying:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Word 0: 98% chance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Word 1: 1.7% chance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Word 2: 0.04% chance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 2: Cross-Entropy – Measuring Mistakes'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Cross-entropy compares the predicted probability for the correct word against
    the ideal case (probability = 1).
  prefs: []
  type: TYPE_NORMAL
- en: 'Formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: If the model assigns high probability to the correct word, loss is small.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the model assigns low probability, loss is large.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: Correct word = Word 0, probability = 0.98 → loss = -log(0.98) ≈ 0.02 (excellent).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Correct word = Word 1, probability = 0.017 → loss = -log(0.017) ≈ 4.1 (bad).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 3: Averaging Over the Batch'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In practice, we don’t train on just one word, but a batch of sequences. The
    code loops over every token in every batch, collects their losses, and averages
    them.
  prefs: []
  type: TYPE_NORMAL
- en: 'From `train_gpt2.c`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'What’s happening here:'
  prefs: []
  type: TYPE_NORMAL
- en: For each token, find the max logit (`logit_max`) to improve numerical stability.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute softmax denominator (`sum`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate log probability of the correct token.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Accumulate losses across all tokens.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Divide by total tokens (`B*T`) to get the average.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Numerical Stability Tricks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Without subtracting `logit_max`, `exp(logits)` can overflow. For example, `exp(1000)`
    is infinite. By subtracting the max, the largest logit becomes 0, so its exponential
    is 1, and all others are ≤ 1\. This keeps numbers manageable while preserving
    the probability ratios.
  prefs: []
  type: TYPE_NORMAL
- en: Example With a Sentence
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Sentence: *The cat sat on the mat.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose the model predicts probabilities for the last token:'
  prefs: []
  type: TYPE_NORMAL
- en: '“mat”: 0.85'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '“dog”: 0.10'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '“car”: 0.05'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Correct word = “mat.”
  prefs: []
  type: TYPE_NORMAL
- en: Loss = `-log(0.85) ≈ 0.16`.
  prefs: []
  type: TYPE_NORMAL
- en: If instead the model guessed “dog” with 0.10, loss = `-log(0.10) ≈ 2.3`. Higher
    penalty for being wrong.
  prefs: []
  type: TYPE_NORMAL
- en: Analogy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Cross-entropy is like grading multiple-choice exams. If the student picks the
    right answer confidently (high probability), they lose almost no points. If they’re
    hesitant or wrong, they lose more points. Over many questions (tokens), you calculate
    their average score-the training loss.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Cross-entropy loss is the guiding signal for the entire training process. It
    tells the optimizer:'
  prefs: []
  type: TYPE_NORMAL
- en: “Increase probability for the right words.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Decrease probability for the wrong words.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Without it, GPT-2 would have no way of knowing whether its predictions are improving.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Check baseline loss: Before training, print the loss. It should be close to
    `log(vocab_size)` (~10.8 for GPT-2 small), which corresponds to random guessing.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Inspect softmax sums: For one token, sum all probabilities. It should equal
    ~1.0.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Force the wrong answer: Temporarily change the target to an incorrect word.
    Watch how the loss shoots up.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Observe loss during training: Print loss every step. It should steadily decrease
    as the model learns.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compare with accuracy: Track how often the model’s top prediction matches the
    target. Loss and accuracy will move together, but loss is smoother and more informative.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Cross-entropy loss turns raw model scores into a clear training signal. It penalizes
    wrong predictions, rewards confident correct ones, and ensures the optimizer knows
    exactly how to adjust weights. In `train_gpt2.c`, you see this implemented explicitly,
    without any library shortcuts-just loops, exponentials, and logs. Understanding
    this section is key to understanding how GPT-2 learns from its mistakes.
  prefs: []
  type: TYPE_NORMAL
- en: '38\. Putting It All Together: The `gpt2_forward` Function'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Up to this point, we’ve explored the forward pass piece by piece - embeddings,
    attention, feed-forward layers, layer normalization, residual connections, and
    finally the loss. But a model doesn’t live as disconnected pieces; they all come
    together in a single function that drives inference: `gpt2_forward`. This function
    is where the code actually executes the story we’ve been telling. Let’s walk through
    it carefully so you can see how every building block plugs into the whole picture.'
  prefs: []
  type: TYPE_NORMAL
- en: The role of `gpt2_forward`
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Think of `gpt2_forward` as the director of the play. The actors (embeddings,
    attention, MLP, layernorm, etc.) already know their roles. The director calls
    them on stage in the right order and makes sure they hand the script off smoothly
    to the next actor. In our case:'
  prefs: []
  type: TYPE_NORMAL
- en: Tokens come in as integers (word IDs).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They’re turned into embeddings (token + position).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each transformer block processes the sequence through attention, MLP, layernorm,
    and residuals.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The final hidden states are mapped back into vocabulary space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If labels are provided, a loss is computed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code skeleton
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here’s a simplified excerpt of the real function from `train_gpt2.c` (slightly
    shortened for readability):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: Don’t worry if this looks intimidating - we’ll decode each part in plain language.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Embedding lookup'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Before the model can reason about words, it has to map token IDs into continuous
    vectors. That’s where embedding tables come in:'
  prefs: []
  type: TYPE_NORMAL
- en: '`token_embedding` converts each integer token ID into a dense vector of size
    `C` (the channel dimension).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`position_embedding` does the same for positions (0, 1, 2, …, T-1).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These two are added together, giving each token both a meaning (word identity)
    and a place in the sentence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 2: Transformer blocks'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Each block is like a mini-pipeline that processes the sequence and passes it
    forward. Inside the loop:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Attention: compares tokens with each other, weighted by learned Q/K/V projections.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'MLP: expands each token vector, applies a nonlinear GELU activation, then projects
    back down.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'LayerNorm: normalizes values for stable training and inference.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Residual: adds the input of the block back to its output to keep information
    flowing.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This loop runs `n_layer` times - for GPT-2 124M, that’s 12 blocks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Final normalization and logits'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: After the last block, the sequence of token representations goes through a final
    layer normalization. Then, a large matrix multiplication (`lm_head`) projects
    each token’s hidden state into the size of the vocabulary (≈50,000 for GPT-2).
    The result is a tensor of shape `(B, T, vocab_size)` containing the raw prediction
    scores for each next token.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: Optional loss computation'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If you pass `labels` (the correct next tokens) into `gpt2_forward`, the function
    calls `crossentropy_forward`. This compares the predicted scores with the true
    tokens and outputs a single number: the loss. The loss tells you “how wrong” the
    model was, which is critical during training. But if you’re only doing inference,
    you don’t need this step.'
  prefs: []
  type: TYPE_NORMAL
- en: How the pieces connect
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here’s a table that maps our earlier sections to the parts of `gpt2_forward`:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Code Step | Concept | Section Covered Earlier |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Embeddings | token + positional vectors | 32 |'
  prefs: []
  type: TYPE_TB
- en: '| Attention | QKV projections, masking, softmax | 33 |'
  prefs: []
  type: TYPE_TB
- en: '| MLP | feed-forward expansion and compression | 34 |'
  prefs: []
  type: TYPE_TB
- en: '| LayerNorm | normalization for stability | 35 |'
  prefs: []
  type: TYPE_TB
- en: '| Residual | skip connections for signal flow | 36 |'
  prefs: []
  type: TYPE_TB
- en: '| CrossEntropy | comparing predictions with labels | 37 |'
  prefs: []
  type: TYPE_TB
- en: So `gpt2_forward` is really just a neat orchestration of everything you’ve already
    learned.
  prefs: []
  type: TYPE_NORMAL
- en: Why it matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Understanding `gpt2_forward` gives you the complete mental picture of inference.
    It shows how embeddings, attention, MLP, normalization, and residuals work together
    in code to turn a batch of tokens into predictions. Without this integration step,
    the model would just be a collection of disconnected parts.
  prefs: []
  type: TYPE_NORMAL
- en: Try it yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Print shapes: Add `printf` statements inside `gpt2_forward` to print tensor
    shapes after embeddings, after each block, and after logits. This helps you see
    the data flow.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use a single block: Change the loop to run only 1 transformer block instead
    of all 12\. Watch how the outputs degrade - the model loses depth of reasoning.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Disable position embeddings: Comment out the line that adds `position_embedding`.
    Try running inference. You’ll notice the model becomes worse at handling word
    order.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Loss vs no loss: Call `gpt2_forward` with and without labels. Compare the difference
    - with labels you get a scalar loss, without labels you just get logits.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Smaller vocab: Try using a toy tokenizer with a small vocabulary and rerun
    the projection step. You’ll see the logits shrink to `(B, T, tiny_vocab_size)`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`gpt2_forward` is where GPT-2 inference really happens. It ties together every
    concept - embeddings, attention, feed-forward layers, normalization, residuals,
    and the final projection into vocabulary space. Once you understand this function,
    you don’t just know the pieces of GPT-2, you know how they actually work together
    to produce predictions. It’s the “main stage” of inference, and mastering it means
    you can confidently say you understand how a transformer runs forward on CPU.'
  prefs: []
  type: TYPE_NORMAL
- en: 39\. OpenMP Pragmas for Parallel Loops
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CPU training in `train_gpt2.c` is intentionally “plain C,” but it still squeezes
    out a lot of speed by adding a few OpenMP pragmas (`#pragma omp …`) in the hottest
    loops. OpenMP lets the compiler split a loop’s iterations across multiple CPU
    cores-no threads to create by hand, no locks to manage. If you compile without
    OpenMP support, these pragmas are simply ignored and the code still runs (just
    slower).
  prefs: []
  type: TYPE_NORMAL
- en: Below we’ll (1) show exactly where OpenMP is used, (2) explain why those loops
    are good candidates, and (3) offer practical tips to get solid speedups on your
    machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenMP in this file: where it appears and why'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '| Location / Function | Pragma used | What’s parallelized | Why it’s a great
    fit |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `matmul_forward_naive` | `#pragma omp parallel for collapse(2)` | Outer loops
    over `b` (batch) and `t` (time) | Each `(b,t)` row computes an independent output
    vector; no write conflicts. Large, regular work = easy scaling. |'
  prefs: []
  type: TYPE_TB
- en: '| `matmul_forward` (tiled) | `#pragma omp parallel for` | Collapsed `B*T` loop
    in tiles of `LOOP_UNROLL` | Heaviest compute in the model; tiling + per-thread
    tiles keep caches warm. |'
  prefs: []
  type: TYPE_TB
- en: '| `matmul_backward` (part 1) | `#pragma omp parallel for collapse(2)` | Backprop
    into `inp` over `(b,t)` | Each `(b,t)` reads weights and `dout`, writes a private
    slice of `dinp` → no overlap. |'
  prefs: []
  type: TYPE_TB
- en: '| `matmul_backward` (part 2) | `#pragma omp parallel for` | Backprop into `weight`/`bias`
    over `o` (output channel) | Each thread owns one output channel’s gradient row,
    avoiding atomics. |'
  prefs: []
  type: TYPE_TB
- en: '| `softmax_forward` | `#pragma omp parallel for collapse(2)` | Over `(b,t)`
    positions | Each softmax is independent; perfect “embarrassingly parallel” loop.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `attention_forward` | `#pragma omp parallel for collapse(3)` | Over `(b,
    t, h)` = batch, time, head | Per `(b,t,h)` head’s work is independent; big 3-D
    grid parallelizes extremely well. |'
  prefs: []
  type: TYPE_TB
- en: 'A few key patterns to notice:'
  prefs: []
  type: TYPE_NORMAL
- en: Collapse clauses (`collapse(2)` / `collapse(3)`) fuse nested loops into one
    big iteration space so the scheduler can distribute more, smaller chunks-great
    for load-balancing when `B`, `T`, or `NH` are modest.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallelizing along independent dimensions avoids race conditions. For example,
    in `matmul_backward` the pass that writes `dinp[b,t,:]` is parallelized over `(b,t)`
    so no two threads update the same memory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Own-your-row strategy: when accumulating `dweight`, the loop goes over `o`
    (output channels) so each thread writes its own gradient row `dweight[o,:]`. No
    atomics needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Quick refresher: what OpenMP is doing'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A typical pattern looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: When compiled with OpenMP, the compiler creates a team of threads and divides
    the iteration space (`B*T` in this example) among them. Each thread executes its
    assigned iterations; when the loop finishes, threads sync at an implicit barrier.
  prefs: []
  type: TYPE_NORMAL
- en: Because each `(b,t)` (or `(b,t,h)`) writes to a disjoint slice of the output
    arrays, there’s no need for locks or atomics. This is why these loops scale cleanly
    across cores.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling OpenMP, safely
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The source guards the OpenMP header with:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'so you can define `OMP` in your build and add your compiler switch. Example
    (GCC/Clang):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If you forget `-fopenmp` (or your platform’s equivalent), the pragmas are ignored
    and the program runs single-threaded.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can control threads at runtime:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: A good rule of thumb is to start with the number of physical cores on your CPU.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Why these loops benefit the most
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Matrix multiplies dominate runtime. `matmul_forward`/`matmul_backward` consume
    the bulk of CPU time. Parallelizing them yields the largest end-to-end speedups.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Softmax is independent per position. Each `(b,t)` softmax computes a max, then
    exponentials and a sum-no cross-talk between positions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Attention splits across batch/time/head. The triple loop over `(b,t,h)` has
    lots of work per iteration (Q·K, softmax, weighted sum), making thread overhead
    negligible compared to useful compute.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Minimal synchronization and no atomics. By choosing iteration spaces that own
    exclusive output slices, we avoid costly synchronization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Practical tips for better scaling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Set `OMP_NUM_THREADS` to your CPU. Too many threads can hurt (oversubscription).
    Start with physical cores, then experiment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pin threads (optional, advanced). Some OpenMP runtimes support `OMP_PROC_BIND=close`
    to improve cache locality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mind memory bandwidth. On wide CPUs, GEMMs may become bandwidth-bound. Bigger
    `B`/`T` improves arithmetic intensity; tiny batches underutilize cores.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Warm caches with tiling. The “tiled” `matmul_forward` keeps small accumulators
    in registers and reuses loaded weights across `LOOP_UNROLL` inner iterations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoid hidden sharing. If you add new parallel loops, ensure each thread writes
    to unique memory regions. If you must accumulate to the same place, restructure
    (like “own-your-row”) or use per-thread scratch buffers then reduce.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Micro-walkthrough: why `collapse` helps'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Consider `softmax_forward`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: If `B=4`, `T=64`, that’s 256 independent softmaxes. With `collapse(2)`, OpenMP
    sees a single loop of 256 iterations to distribute evenly; without `collapse`,
    it might chunk by `b` first (only 4 big chunks), which can load-imbalance.
  prefs: []
  type: TYPE_NORMAL
- en: Common pitfalls (and how this code avoids them)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Race conditions: Two threads writing the same `out[i]`. *Avoided by design:*
    each parallel loop writes distinct slices (e.g., per `(b,t)` or per `o`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'False sharing: Threads write adjacent memory locations on the same cache line.
    It’s minimized by the large, contiguous slices per thread (entire rows/tiles),
    but if you extend the code with fine-grained parallelism, keep this in mind.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tiny loops: Overhead can exceed work. The file parallelizes only large, hot
    loops (GEMMs, attention, softmax), not small scalar ops.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try it yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Change thread count: Run with `OMP_NUM_THREADS=1,2,4,8,…` and log step time.
    Plot speedup vs. threads.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Toggle a pragma: Comment out `#pragma omp` in `matmul_forward` only. Measure
    the slowdown; you’ll see where most time goes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Experiment with `collapse`: Remove `collapse(2)` in `softmax_forward`. On small
    `B`, you’ll likely see worse scaling.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Per-layer profiling: Print elapsed time around `matmul_forward`, `attention_forward`,
    and `softmax_forward` to see which benefits most on your CPU.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Schedule policy (advanced): Try `#pragma omp parallel for schedule(static)`
    vs. `dynamic` on a heavy loop to see if it changes load balance (defaults are
    usually fine here).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A handful of well-placed OpenMP pragmas deliver big wins on CPU by parallelizing
    the most expensive loops (GEMMs, attention, softmax) across cores-without complicating
    the code. The design ensures each thread works on independent slices, so there’s
    no locking, no atomics, and very little overhead. If you compile with OpenMP enabled,
    you get fast, multi-core training; if not, you still have a clean, readable reference
    implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 40\. CPU Memory Footprint and Performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When you train GPT-2 on your CPU using `train_gpt2.c`, two big questions usually
    pop up almost immediately: *how much memory is this going to take?* and *how fast
    will it run?* Let’s walk through both of these in a beginner-friendly way, so
    you understand not just what happens in the code, but why it behaves the way it
    does.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Memory: where does it all go?'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Imagine training GPT-2 is like cooking a big meal in a small kitchen. You need
    space for ingredients, bowls for mixing, and counter space for preparing. Memory
    on your CPU is that kitchen. GPT-2 needs several “bowls” to hold different parts
    of the computation:'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters (the weights of the model). These are the “fixed recipe” - the actual
    numbers the network learns. They come from the checkpoint file you load at the
    start. For GPT-2 124M, this is about 124 million floating-point numbers. Each
    one takes 4 bytes, so just the weights are around 500 MB.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Optimizer state (AdamW). Training doesn’t just adjust weights blindly; it keeps
    track of two extra moving averages for each weight, called *m* and *v*. That means
    for every single parameter, you store three numbers: the weight, m, and v. So
    memory for optimizer state is often double the size of the weights themselves.
    For GPT-2 124M, that’s about 1 GB more.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gradients. Every time we run a backward pass, we store how much each weight
    should change. That’s another buffer roughly the same size as the weights - another
    500 MB.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Activations (intermediate results). This is the sneaky one. Every forward pass
    produces temporary tensors like embeddings, attention maps, and feed-forward outputs.
    Their size depends on batch size (B) and sequence length (T). If B=4 and T=64,
    activations are a few hundred MB. If B=32 and T=1024, they can balloon to many
    gigabytes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here’s a rough mental budget for GPT-2 124M with a small setup (B=4, T=64):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters: ~500 MB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Optimizer state: ~1 GB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gradients: ~500 MB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Activations: ~200–300 MB Total: ~2–2.5 GB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even for the “tiny” GPT-2, you already need a couple gigabytes of RAM to train.
    On a laptop, this can quickly push you to the limit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Performance: where does time go?'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now let’s talk speed. When you run `train_gpt2.c` on CPU, you’ll see lines
    like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'That “took X ms” tells you how long one step took. Why is it slow? Three main
    reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Matrix multiplications (matmuls). These are the heart of neural networks. Every
    attention head and every MLP layer does them. On CPU, most of your step time is
    spent here. That’s why the code uses OpenMP pragmas (`#pragma omp`) to parallelize
    loops across cores.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Attention softmax. Attention compares every token in a sequence with every other
    token. If your sequence length is 1024, that’s over a million comparisons per
    head per layer. On CPU, this quadratic growth is painful.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Memory bandwidth. CPUs can only move numbers from RAM to cores so fast. Even
    if you had infinite FLOPs, you’d still be slowed down by how quickly you can fetch
    and store these huge tensors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A simple experiment
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You can see these effects yourself:'
  prefs: []
  type: TYPE_NORMAL
- en: Change batch size (B). Run with B=1, then with B=8\. Notice how memory usage
    and step time scale up.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Change sequence length (T). Try T=16, then T=256\. You’ll see attention costs
    grow dramatically.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Change threads. Set `OMP_NUM_THREADS=1` versus `OMP_NUM_THREADS=8`. With more
    threads, you’ll often see speedups, but only up to the number of physical cores
    your CPU has.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why this matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For beginners, CPU runs are perfect for learning:'
  prefs: []
  type: TYPE_NORMAL
- en: You can debug with small batches and short sequences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can step into functions with a debugger and watch tensors being created.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You don’t need a GPU just to understand how training works.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But when it comes to *serious* training - larger GPT-2 models or even long sequences
    - CPU is simply too slow. What takes seconds on GPU may take minutes on CPU. That’s
    why in practice, people use CPUs for learning and testing, and GPUs for large-scale
    training.
  prefs: []
  type: TYPE_NORMAL
- en: The takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Training GPT-2 on CPU is like practicing piano on a small keyboard. It’s slower,
    limited, and you can’t play the biggest pieces, but it’s great for learning the
    fundamentals. Memory usage comes from weights, optimizer state, gradients, and
    activations, and performance is dominated by matmuls and attention. Once you understand
    where the resources go, you can adjust batch size, sequence length, and threads
    to find the sweet spot for your machine.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 5\. Training Loop (CPU Path)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 41\. Backward Pass Walkthrough
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Up until now, we’ve spent all our time looking at the forward pass. That’s the
    part of the model that takes tokens, pushes them through embeddings, attention,
    feed-forward layers, and finally produces logits or a loss. For inference, forward
    pass alone is enough. But if you want to train a model, forward is only half the
    story.
  prefs: []
  type: TYPE_NORMAL
- en: Training means adjusting the weights of the model so that its predictions become
    better over time. To do this, we need a way to figure out how wrong each weight
    was and in what direction it should move to reduce the loss. That’s the job of
    the backward pass.
  prefs: []
  type: TYPE_NORMAL
- en: 'The backward pass is also called backpropagation. It’s the algorithm that moves
    information in reverse through the network: from the loss, back through the final
    logits, through every transformer block, down to the embeddings. Along the way,
    it calculates gradients - small numbers that tell us how much each weight contributed
    to the error.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The big idea: chain rule in action'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'At the heart of backpropagation is something very familiar from calculus: the
    chain rule. If the output of the network depends on many functions stacked together
    (embedding → attention → MLP → … → loss), then the derivative of the loss with
    respect to an early parameter is a product of partial derivatives through the
    entire chain.'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of writing long formulas, the code in `train_gpt2.c` simply calls each
    layer’s backward function in reverse order. The gradient flows backward, step
    by step, and each layer computes its own contribution using local rules.
  prefs: []
  type: TYPE_NORMAL
- en: 'Think of it like a relay race, but run backwards: the loss hands a “blame baton”
    to the output head, which hands it back to the last transformer block, and so
    on, until it reaches the very first embedding table.'
  prefs: []
  type: TYPE_NORMAL
- en: Walking through `gpt2_backward`
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here’s a simplified sketch of how the backward function looks in the code (names
    shortened for readability):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: Let’s unpack this line by line.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Starting from the loss'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The journey begins with the loss function. In training, the most common loss
    is cross-entropy. Its backward function compares the predicted probabilities with
    the true labels and produces a gradient for the logits.
  prefs: []
  type: TYPE_NORMAL
- en: If the model predicted “cat” with high confidence and the true label was “dog,”
    the gradient will push the logits away from “cat” and toward “dog.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This gradient is the starting signal that propagates backward through the entire
    network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 2: Back through the output head'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'After the loss, the next stop is the final linear projection (`lm_head`). This
    is just a big matrix multiply that turns hidden states into vocabulary logits.
    Its backward function computes two things:'
  prefs: []
  type: TYPE_NORMAL
- en: The gradient with respect to the weights of `lm_head`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The gradient with respect to the hidden states that fed into it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This hidden-state gradient is then passed back to the last transformer block.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Transformer blocks in reverse'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Here comes the heavy lifting. Each block has multiple components, and their
    backward functions are called in the exact opposite order of the forward pass.
  prefs: []
  type: TYPE_NORMAL
- en: 'Residual backward: the skip connection splits the gradient into two paths -
    one flowing back into the transformed output, one flowing back into the original
    input.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'LayerNorm backward: computes gradients with respect to its scale (`gamma`)
    and shift (`beta`), and also passes gradients back to the normalized input.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'MLP backward: applies the chain rule to the two linear layers and the GELU
    activation. The code reuses temporary values from the forward pass (like activations)
    to make this efficient.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Attention backward: this is the trickiest. It computes gradients for Q, K,
    and V projections, as well as for the softmaxed attention weights. It has to apply
    the causal mask again to ensure no illegal gradient flows.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This loop continues until all transformer blocks have been processed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: Back to embeddings'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Finally, the gradient reaches the embedding tables. This is where the model
    first looked up vectors for tokens and positions. Now it calculates how much each
    embedding contributed to the error. These gradients are added into the embedding
    matrices, telling the optimizer how to update them.
  prefs: []
  type: TYPE_NORMAL
- en: Why this matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The backward pass is what makes learning possible. Without it, the model would
    forever output the same predictions, never improving. By flowing “blame” backwards,
    each parameter learns how to nudge itself so that the next forward pass is a little
    bit better.
  prefs: []
  type: TYPE_NORMAL
- en: 'Even though the code looks like a lot of function calls, the principle is simple:
    start from the loss, step backward through each layer, apply the chain rule locally,
    and collect gradients.'
  prefs: []
  type: TYPE_NORMAL
- en: Try it yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Print gradient norms: Add a `printf` to see the average gradient magnitude
    at each layer. Notice how they change - sometimes exploding, sometimes vanishing.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Freeze a layer: Comment out `mlp_backward` for one block and see how the model
    fails to update properly.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Inspect embeddings: After training a few steps, dump a few rows of the token
    embedding matrix. You’ll see the numbers changing because of gradient updates.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Tiny dataset experiment: Train on a very small dataset (like a 10-word corpus)
    and watch how the backward pass quickly pushes embeddings to memorize it.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Check symmetry: Compare the order of calls in `gpt2_forward` with `gpt2_backward`.
    They’re exact opposites - forward builds, backward unbuilds.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Backpropagation is the learning engine of neural networks. In `llm.c`, the backward
    pass is written out explicitly, showing how gradients flow from the loss, through
    the output head, back through every transformer block, and finally into embeddings.
    Once you understand this flow, you can see how training stitches forward and backward
    together to slowly shape a random model into a working language model.
  prefs: []
  type: TYPE_NORMAL
- en: 42\. Skeleton of Training Loop
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The backward pass gave us gradients, but gradients by themselves don’t train
    a model. Training requires a loop: a cycle that repeatedly runs forward, backward,
    and update steps over and over until the model improves. This cycle is called
    the training loop, and it is the heartbeat of every deep learning program. In
    `train_gpt2.c`, the loop is written explicitly in C, which means you can see every
    piece instead of it being hidden away in a framework.'
  prefs: []
  type: TYPE_NORMAL
- en: The basic rhythm
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Every training step follows the same rhythm:'
  prefs: []
  type: TYPE_NORMAL
- en: Get a batch of data (input tokens and their labels).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the forward pass to compute predictions and loss.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the backward pass to compute gradients.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update weights using an optimizer like AdamW.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Log progress and, occasionally, validate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This rhythm repeats thousands or millions of times. With each repetition, the
    weights shift slightly, nudging the model toward lower loss and better predictions.
  prefs: []
  type: TYPE_NORMAL
- en: How the loop looks in code
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here’s a simplified sketch from `train_gpt2.c` (with some details omitted for
    clarity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'This loop captures the full training lifecycle: data, forward, backward, update,
    and monitoring.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Batching data'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The dataloader feeds the loop with small chunks of tokens. Instead of sending
    the whole dataset at once, it breaks it down into batches of size `B` (number
    of sequences per batch) and length `T` (number of tokens per sequence).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: if `B=4` and `T=128`, each batch is 512 tokens long.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each sequence has a matching set of labels, which are simply the same tokens
    shifted one position ahead (so the model always predicts the *next* word).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This batching keeps memory use manageable and helps the model see many small
    samples instead of a few giant ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Forward pass'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The forward pass computes predictions for all tokens in the batch and calculates
    the loss. This is the “evaluation” step - how well did the model do on this batch?
    The result is stored in `model.loss`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Zeroing gradients'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Before calculating new gradients, the old ones must be cleared out. If you
    skip this step, gradients from previous batches would accumulate and corrupt the
    update. In frameworks like PyTorch you’d call `optimizer.zero_grad()`. Here it’s
    a plain C function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: It walks through all parameters and resets their gradient buffers to zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: Backward pass'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now the backward function is called. It pushes gradients back through the network,
    computing how each weight influenced the error. At this point, every parameter
    has an associated gradient stored in memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 5: Optimizer update'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'With gradients ready, the optimizer (AdamW in this code) updates each parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: This step is what actually changes the model. Without it, the model would never
    learn - the forward and backward passes would just repeat the same results forever.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 6: Logging and validation'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Every few steps, the loop prints out useful numbers: current step, loss, time
    taken, and sometimes throughput (tokens per second). This feedback is important
    to check whether training is actually working.'
  prefs: []
  type: TYPE_NORMAL
- en: Every few hundred or thousand steps, the loop also runs a validation pass on
    held-out data. This tells you whether the model is just memorizing training data
    or genuinely learning patterns that generalize.
  prefs: []
  type: TYPE_NORMAL
- en: Why the training loop matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The training loop is deceptively simple, but it is the engine room of machine
    learning. Every improvement in model performance happens because this loop runs
    many times. By writing it explicitly in C, `llm.c` exposes details that high-level
    frameworks usually hide: zeroing gradients, passing pointers to arrays, calling
    backward and optimizer functions directly.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This makes it a perfect learning tool. You can see clearly:'
  prefs: []
  type: TYPE_NORMAL
- en: Where the data comes in,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where predictions are made,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where gradients are calculated,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And where learning actually happens.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try it yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Print the loss curve: Add a `printf` inside the loop and write the loss to
    a file. Plot it - you should see it decrease over time.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Change batch size: Set `B=1` vs. `B=8`. Notice how the loop becomes noisier
    with smaller batches but smoother with larger ones.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Skip backward: Comment out `gpt2_backward` and optimizer update. Run the loop.
    You’ll see the loss never decreases - a clear demonstration that forward alone
    doesn’t train.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Experiment with steps: Try `max_steps=10` vs. `max_steps=1000`. Short runs
    show no improvement; longer runs start to reduce the loss.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Slow it down: Insert a `sleep(1);` inside the loop. This makes the rhythm visible
    step by step, so you can literally watch the model “breathe” as it trains.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The skeleton of the training loop is the core cycle of learning. It feeds data
    into the model, computes predictions, finds errors, sends them backward, updates
    weights, and logs progress. Everything else - optimizers, schedulers, distributed
    training, mixed precision - is just an enhancement of this basic loop. If you
    understand how this loop works in `llm.c`, you understand the beating heart of
    deep learning training.
  prefs: []
  type: TYPE_NORMAL
- en: 43\. AdamW Implementation in C
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Training a neural network is about adjusting millions of parameters so that
    the model gradually becomes better at predicting text. The function `gpt2_update`
    in `train_gpt2.c` is responsible for this adjustment. It implements the AdamW
    optimizer, one of the most widely used algorithms in deep learning. Let’s walk
    through both the theory and the actual implementation.
  prefs: []
  type: TYPE_NORMAL
- en: From Gradient Descent to AdamW
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The most basic optimizer is gradient descent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: 'This approach works, but it has weaknesses. The step size (learning rate) must
    be tuned carefully: too small and training is slow, too large and training diverges.
    Moreover, all parameters use the same step size, even though some may need gentler
    updates.'
  prefs: []
  type: TYPE_NORMAL
- en: AdamW improves this by keeping track of moving averages of gradients and scaling
    updates adaptively. It also introduces weight decay, which prevents parameters
    from growing too large and helps regularize the model.
  prefs: []
  type: TYPE_NORMAL
- en: How AdamW Works
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'AdamW combines several techniques into a single update rule. First, it uses
    momentum: instead of relying only on the current gradient, it averages recent
    gradients. This smooths noisy updates. Second, it maintains a running estimate
    of the squared gradient, which scales down steps in directions where gradients
    are consistently large. These are sometimes called the first and second moments.'
  prefs: []
  type: TYPE_NORMAL
- en: Since both running averages start at zero, the algorithm applies bias correction
    during the first few steps. Without this, the early updates would be too small.
    Finally, AdamW applies weight decay directly in the update, shrinking parameter
    values slightly each step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Putting it together, each parameter update looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: Here `m` is momentum, `v` is variance, `lr` is learning rate, `ε` is a small
    constant for stability, and `λ` is the weight decay factor.
  prefs: []
  type: TYPE_NORMAL
- en: The Implementation in `train_gpt2.c`
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: The first `if` block allocates memory for the moving averages `m` and `v` the
    first time the optimizer runs. Then, for each parameter, the code computes the
    new averages, applies bias correction, and finally updates the parameter with
    the AdamW formula.
  prefs: []
  type: TYPE_NORMAL
- en: Example Walkthrough
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Suppose we have a parameter `w = 0.5` with gradient `g = 0.2` on the first
    training step. Using β1 = 0.9 and β2 = 0.999:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Momentum:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Variance:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Bias correction:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Final update (lr = 0.001, weight_decay = 0.01):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: So the parameter becomes `w = 0.498995`.
  prefs: []
  type: TYPE_NORMAL
- en: Intuition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Think of a ball rolling down a slope. The gradient is the slope itself. Momentum
    makes the ball keep rolling even if the slope flattens briefly. The variance term
    makes the ball slow down on rocky ground where the slope changes rapidly. Bias
    correction ensures the ball doesn’t move too timidly at the start. Weight decay
    adds friction so the ball doesn’t roll out of control.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Optimizers are the difference between a model that trains smoothly and one that
    diverges or gets stuck. AdamW became popular because it combines stability with
    efficiency. It automatically adapts to each parameter’s scale, reduces the need
    for manual learning rate tuning, and includes weight decay in a principled way.
    For GPT-style models with hundreds of millions of parameters, these qualities
    make training feasible.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Change the learning rate from `0.001` to `0.01` in the code and see how quickly
    the model diverges.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set `weight_decay = 0` and compare validation loss after a few epochs. The model
    might overfit more quickly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print out the first 10 values of `m_memory` and `v_memory` during training to
    watch how they evolve over steps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Replace AdamW with plain SGD (just `param -= lr * grad`) and compare training
    speed and stability.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Experiment with β1 = 0 (no momentum) or β2 = 0 (no variance smoothing) and see
    how noisy updates become.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: AdamW provides a balance of speed, stability, and generalization. In practice,
    it allows models like GPT-2 to train much more reliably than with vanilla gradient
    descent. The C implementation in `llm.c` demonstrates that beneath the math, it’s
    just a simple loop applying a few arithmetic operations for each parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 44\. Gradient Accumulation and Micro-Batching
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Modern language models are enormous, and so are the batches of text we would
    like to feed them during training. But real hardware has limits: a single GPU
    or CPU may not have enough memory to process a large batch in one go. To solve
    this, training code often uses gradient accumulation and micro-batching. Both
    ideas allow us to simulate training with larger batches without requiring more
    memory than our hardware can provide.'
  prefs: []
  type: TYPE_NORMAL
- en: What Problem Are We Solving?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When you process a batch of data, you run forward and backward passes to calculate
    gradients. If your batch size is very large, you get smoother gradients (less
    noisy), which often helps the model converge better. But large batches may not
    fit in memory.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine trying to train with a batch of 1024 sequences on a GPU that can only
    handle 128 sequences at once. Without tricks, you would be forced to use the smaller
    batch size and give up the benefits of larger batches. Gradient accumulation fixes
    this problem by letting you split the big batch into smaller micro-batches, process
    them one at a time, and accumulate the results as if you had processed the big
    batch all at once.
  prefs: []
  type: TYPE_NORMAL
- en: How It Works in Practice
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let’s say we want an effective batch size of 1024, but our hardware only supports
    128\. We split the big batch into 8 micro-batches of 128 each:'
  prefs: []
  type: TYPE_NORMAL
- en: Run forward + backward on micro-batch 1, store the gradients.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run forward + backward on micro-batch 2, add its gradients to the stored ones.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat until all 8 micro-batches are processed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once gradients for all 8 are accumulated, perform the optimizer update.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The important part is step 4: we only update the parameters once per effective
    batch, not after each micro-batch. This preserves the effect of training with
    a large batch.'
  prefs: []
  type: TYPE_NORMAL
- en: Pseudocode Example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here’s how this might look in simplified pseudocode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: Notice how the optimizer only runs once per outer loop iteration, even though
    gradients were accumulated across multiple micro-batches.
  prefs: []
  type: TYPE_NORMAL
- en: Why Gradient Accumulation Helps
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Memory efficiency: You can train with larger effective batch sizes without
    needing more hardware.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Training stability: Larger batches reduce the variance of gradients, making
    training less noisy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Flexibility: You can scale effective batch size up or down depending on your
    needs without changing hardware.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Micro-Batching vs. Accumulation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Micro-batching refers to the act of splitting a batch into smaller parts. Gradient
    accumulation is what you do after micro-batching: sum up the gradients across
    those parts. Together, they allow you to simulate training with any batch size
    you want, within memory constraints.'
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The quality of training often depends on batch size. If you can’t fit a large
    batch directly, gradient accumulation ensures you still reap the benefits. It’s
    one of those “engineering hacks” that makes training state-of-the-art models possible
    on limited resources.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Run training with batch size = 16 and no accumulation. Watch how noisy the loss
    curve looks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now set micro-batch size = 4 and accumulation_steps = 4\. This simulates batch
    size = 16, but in smaller chunks. Compare the loss curve.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Increase accumulation_steps to simulate batch size = 32\. Observe if training
    becomes smoother.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Experiment with turning accumulation off and on while keeping the same effective
    batch size. Notice how optimizer updates per epoch differ.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print out how many times the optimizer is called. With accumulation, it should
    be fewer than the number of micro-batches.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Gradient accumulation and micro-batching are techniques that let you train
    with large effective batch sizes while staying within the limits of your hardware.
    They preserve the benefits of large batches-stability and smoother gradients-without
    demanding extra memory. In `llm.c`, the simplicity of the training loop means
    you can clearly see where accumulation fits: gradients are summed across micro-batches,
    and only then does the optimizer step in. This is a small adjustment in code but
    a huge enabler in practice.'
  prefs: []
  type: TYPE_NORMAL
- en: 45\. Logging and Progress Reporting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Every training loop needs a way to show what’s happening under the hood. Without
    logs, you wouldn’t know if the model is improving, if the code is running efficiently,
    or if something has silently gone wrong. In `train_gpt2.c`, logging is intentionally
    minimal but highly informative: each training step prints the step number, the
    current training loss, and how long that step took to run.'
  prefs: []
  type: TYPE_NORMAL
- en: The Real Code for Logging
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here’s the relevant snippet from `train_gpt2.c`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: 'This small block accomplishes two things:'
  prefs: []
  type: TYPE_NORMAL
- en: It measures how long the training step took using `clock_gettime`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It reports the step number, the loss, and the elapsed time in milliseconds.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The output looks like this when training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: Understanding What’s Reported
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Step number (`step`) Tells you where you are in training. Since deep learning
    often runs for thousands of steps, this acts like a progress bar.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training loss (`model.mean_loss`) Shows how well the model is fitting the training
    batch. A lower value generally means better predictions. Watching this number
    decrease over time is the main signal that learning is happening.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Step duration (`time_elapsed_s * 1000`) Measures performance. If one step takes
    2000 ms, then 5000 steps would take about 3 hours. Monitoring this helps you estimate
    total training time and spot performance regressions (e.g., if a new change suddenly
    doubles the step time).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Logs are your window into the training process. If the loss goes down smoothly,
    training is healthy. If it suddenly spikes or stays flat, something is wrong-maybe
    the learning rate is too high, or the model has run out of capacity. Timing information
    also matters: you need to know whether the code is running efficiently or wasting
    cycles.'
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Change the learning rate from `1e-4` to `1e-2` and watch how the loss behaves.
    If it jumps or becomes unstable, you’ll see it directly in the logs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add validation logging by running the model on a held-out dataset every 100
    steps and printing `val_loss`. Compare it to `train_loss`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Record the log output to a file with:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Then plot `train_loss` over steps in Python or Excel to visualize the curve.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Add throughput reporting: divide the batch size times sequence length (`B*T`)
    by the step time to print tokens per second. This gives a clearer sense of efficiency.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try disabling `clock_gettime` and only print loss. Notice how much harder it
    becomes to judge performance without timing information.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Even the simplest logs can tell you a lot. With just a single line-step, loss,
    and duration-you know how fast training is, whether it’s converging, and how long
    it will take. In larger frameworks, this kind of information is often hidden behind
    dashboards and monitoring tools, but the core idea is the same: training is only
    useful if you can see and interpret its progress.'
  prefs: []
  type: TYPE_NORMAL
- en: 46\. Validation Runs in the Training Loop
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When you train a model, it is not enough to look only at how well it does on
    the training data. The real test is whether the model has learned patterns that
    apply to new, unseen data. This is where validation comes in. Validation is like
    a quiz the model takes from time to time during training. It does not count toward
    learning-it is just a check to see how much the model has really understood.
  prefs: []
  type: TYPE_NORMAL
- en: In `train_gpt2.c`, validation is built right into the training loop. Every so
    often, instead of updating weights, the program pauses and runs the model on a
    set of tokens it has never trained on. It then prints out the average validation
    loss. This number tells you if the model is actually generalizing, not just memorizing.
  prefs: []
  type: TYPE_NORMAL
- en: How the validation code looks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here is the actual block of code that handles validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: At first glance, this might look like just a few lines of C code. But behind
    it are several important ideas about how machine learning models are tested while
    they learn. Let’s go through this step by step.
  prefs: []
  type: TYPE_NORMAL
- en: Step-by-step explanation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The first line checks whether it is time to run validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: This means that validation happens every 10 steps. The `%` operator is “modulo,”
    which returns the remainder of a division. If the step number is divisible by
    10 (like 0, 10, 20, 30), then the block runs. By spacing it out this way, validation
    does not slow training too much but still gives you regular updates.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, the code sets up a place to store the running total of the validation
    loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: 'Then it resets the validation dataloader:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: This makes sure the validation dataset starts from the beginning each time.
    That way, the results are consistent-you’re always checking the model on the same
    set of text, rather than starting from a random place.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now comes the loop over validation batches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s what’s happening inside:'
  prefs: []
  type: TYPE_NORMAL
- en: '`dataloader_next_batch` fetches the next chunk of tokens and labels from the
    validation set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gpt2_forward` runs the model forward on those tokens, predicting the next
    word for each one, and computes the loss against the true labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The loss from that batch is added to `val_loss`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Notice that there is no call to `gpt2_zero_grad`, no `gpt2_backward`, and no
    `gpt2_update`. That is because validation does not train the model. It only measures
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the program averages the loss across the number of batches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: 'And prints the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: This gives you a single number that summarizes how well the model is performing
    on unseen data at this point in training.
  prefs: []
  type: TYPE_NORMAL
- en: How to read validation loss
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Imagine you are training and see logs like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: The training loss is printed every step, while the validation loss appears every
    10 steps. If both numbers are going down, that is a sign the model is genuinely
    learning. If training loss drops but validation loss stays the same or starts
    going up, the model is probably memorizing the training set-this is called overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Why validation is important
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Without validation, you could be tricked into thinking the model is improving
    just because the training loss is going down. But that might only mean it has
    memorized the training data. Validation checks prevent this by showing you whether
    the model can handle data it has not seen before. It is like a student practicing
    with old exam papers (training) versus being tested with new problems (validation).
  prefs: []
  type: TYPE_NORMAL
- en: Small details that matter
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The code averages validation loss over `val_num_batches`, which is set earlier
    to 5\. That means it only checks 5 batches, not the entire validation dataset.
    This is a shortcut-it makes validation much faster, at the cost of some accuracy
    in the measurement. But for training feedback, this is usually enough.
  prefs: []
  type: TYPE_NORMAL
- en: The batch size `B` and sequence length `T` for validation are the same as training.
    This keeps the loss comparable between training and validation.
  prefs: []
  type: TYPE_NORMAL
- en: Try it yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You can experiment with the validation process to understand it better. Here
    are some ideas:'
  prefs: []
  type: TYPE_NORMAL
- en: Change the frequency from every 10 steps to every 5 or even every step. You’ll
    see more validation updates, but training will slow down.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Increase `val_num_batches` to 20\. The validation loss will become less noisy,
    but each check will take longer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Comment out the validation block and train again. Notice how you lose a sense
    of whether the model is really generalizing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save validation loss values to a file and plot them. Compare the curve against
    the training loss curve. You’ll see how they move together or diverge.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try using a very small validation dataset. Watch how the loss jumps around more
    compared to a larger, more stable dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Validation runs are short forward-only tests that give you confidence the model
    is learning patterns that apply to new text. They are easy to implement-a few
    lines of code in `train_gpt2.c`-but they are one of the most important tools for
    monitoring training. By checking validation loss regularly, you make sure your
    model is not just memorizing but actually becoming better at language modeling.
  prefs: []
  type: TYPE_NORMAL
- en: 47\. Checkpointing Parameters and Optimizer State
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Training a model can take hours, days, or even weeks. If you stop the program
    halfway-whether by accident (a crash, a power cut) or on purpose (pausing to save
    compute)-you don’t want to start over from scratch. Checkpointing solves this
    problem by saving the model’s parameters and optimizer state to disk so you can
    resume training later.
  prefs: []
  type: TYPE_NORMAL
- en: What a checkpoint contains
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A checkpoint is like a “save game” for machine learning. At a minimum, it needs:'
  prefs: []
  type: TYPE_NORMAL
- en: Model parameters – the actual weights of the neural network, stored as floating-point
    numbers in memory. These define what the model has learned so far.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Optimizer state – for AdamW, this includes the running averages of gradients
    (`m_memory`) and squared gradients (`v_memory`). Without these, the optimizer
    would lose its “memory” of past updates, which could destabilize resumed training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Step counter – the number of steps completed so far. This matters for bias correction
    in AdamW and for scheduling the learning rate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Together, these three things capture the full training state.
  prefs: []
  type: TYPE_NORMAL
- en: Saving a checkpoint
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Although `train_gpt2.c` is kept minimal and does not include full checkpointing
    code, the idea is straightforward. You allocate a file, write all parameters,
    optimizer buffers, and metadata, then close the file. In pseudocode, it looks
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: This is a binary dump of the model and optimizer. Later, you can load the file
    back with `fread` calls into the same memory locations.
  prefs: []
  type: TYPE_NORMAL
- en: Loading a checkpoint
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Loading is the reverse:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: Once loaded, training can continue exactly where it left off.
  prefs: []
  type: TYPE_NORMAL
- en: Why optimizer state matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It might seem enough to save only the model’s parameters. But AdamW depends
    on moving averages of past gradients. If you throw those away and restart with
    only the parameters, the optimizer will behave differently. Learning may suddenly
    become unstable, or the effective learning rate may feel wrong. That’s why saving
    both the parameters and optimizer state gives the most faithful restart.
  prefs: []
  type: TYPE_NORMAL
- en: Why checkpointing is essential
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Training is rarely smooth. Servers reboot, experiments are interrupted, bugs
    are found. Without checkpoints, any interruption means wasted compute and lost
    progress. With checkpoints, you can pause and resume at will. They also let you
    archive important moments in training-for example, saving the model when validation
    loss is lowest, not just at the end.
  prefs: []
  type: TYPE_NORMAL
- en: Try it yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Write a small function that saves the model’s parameters after every 100 steps.
    Then kill the program midway and reload from the saved file. Confirm that resumed
    training picks up where it left off.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try saving only parameters but not optimizer state. Resume training and compare
    loss curves. You’ll see that the run diverges from the original.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save checkpoints at multiple steps and later reload them to compare model generations
    (does the model produce more fluent text after 10 steps, 100 steps, 1000 steps?).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Intentionally corrupt part of a checkpoint file (flip a few bytes) and try reloading.
    This helps you understand why consistency checks or checksums are often added
    in real systems.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Store checkpoints in a versioned way (e.g., `checkpoint_step100.bin`, `checkpoint_step200.bin`)
    so you can roll back if a later training phase degrades performance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Checkpointing is what makes long-running training practical. By saving parameters,
    optimizer state, and the step counter, you preserve not just what the model knows
    but how it is learning. In real projects, checkpoints are the bridge between experiments
    and production: they let you stop, resume, compare, and deploy models without
    ever starting from scratch. Even though `llm.c` does not fully implement it, the
    concept is simple and invaluable.'
  prefs: []
  type: TYPE_NORMAL
- en: 48\. Reproducibility and Small Divergences
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When training deep learning models, two runs that look identical on the surface
    can still behave differently. One run might converge quickly, another might take
    longer, and sometimes losses diverge even though you used the same dataset and
    code. This happens because of the way randomness and numerical precision interact
    during training. Reproducibility is about controlling these factors so that results
    are consistent and meaningful.
  prefs: []
  type: TYPE_NORMAL
- en: Sources of randomness
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are several places where randomness sneaks into training:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Data order: if batches are shuffled differently, the model sees tokens in a
    new sequence. Early steps can influence the trajectory of training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weight initialization: initial parameters are usually set randomly. Different
    seeds lead to slightly different starting points.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dropout and sampling: while `train_gpt2.c` is minimal and doesn’t include dropout
    layers, many neural networks do. Dropout randomly disables activations during
    training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Floating-point arithmetic: on CPUs and GPUs, the order of summations or parallel
    reductions can cause tiny rounding differences. Over many steps, these small changes
    accumulate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How llm.c handles reproducibility
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The repository includes functions like `manual_seed` and `random_f32` in `llmc/rand.h`.
    These are simple random number generators that can be seeded with a fixed value.
    For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: If you call this before training, the random number generator starts from the
    same state every run. That means weight initialization and sampling will be reproducible.
  prefs: []
  type: TYPE_NORMAL
- en: The dataloaders also have reproducibility options. When you initialize a `DataLoader`,
    you can decide whether it shuffles batches or not. Keeping this consistent ensures
    the model sees the same data order each run.
  prefs: []
  type: TYPE_NORMAL
- en: Why small divergences happen anyway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Even with fixed seeds, you might notice that two runs are not perfectly identical.
    On CPUs, differences often come from OpenMP parallel loops-threads may sum numbers
    in a different order, producing slightly different results. On GPUs, parallelism
    and library implementations (like cuBLAS or cuDNN) can do the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'These differences are usually very small, but deep learning systems are chaotic:
    tiny changes in the early steps can grow into visible differences later. This
    doesn’t mean the code is wrong-it just means floating-point math has limits.'
  prefs: []
  type: TYPE_NORMAL
- en: Why reproducibility matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Reproducibility isn’t just about peace of mind. It has real uses:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Debugging: if a bug appears, you want to reproduce the exact same run to diagnose
    it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Comparisons: when testing new optimizers, schedulers, or architectures, you
    want fair comparisons on identical conditions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Science: reproducible results are essential for research papers and benchmarks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At the same time, absolute bit-for-bit reproducibility is often unrealistic
    in large parallel systems. Instead, the goal is practical reproducibility: ensuring
    that runs are *similar enough* to reach the same conclusions.'
  prefs: []
  type: TYPE_NORMAL
- en: Example experiment
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Suppose you seed training with `manual_seed(1337)` and use the same dataset.
    You might get a loss curve like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: The numbers are not identical, but they are close. The important part is that
    the model’s learning trajectory is stable and results are comparable.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you remove the seed and allow full randomness, you might get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: Both are valid, but harder to compare.
  prefs: []
  type: TYPE_NORMAL
- en: Try it yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Run training twice without setting a seed. Compare how training loss and validation
    loss differ at step 500.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a fixed seed with `manual_seed(42)` before building the model. Run training
    twice and compare again. You should see closer numbers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enable OpenMP with multiple threads and then run with a single thread. Notice
    how results differ slightly due to floating-point summation order.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save two checkpoints from runs with different seeds. Use the model to generate
    text and compare outputs. You’ll see different wording, but both grammatically
    plausible.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Increase the dataset size and check if differences between runs shrink. With
    more data, randomness matters less.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Reproducibility in training is about controlling randomness where possible and
    accepting small divergences where not. In `llm.c`, reproducibility is made clear
    through simple seeding functions and deterministic dataloader options. Perfect
    bit-level reproducibility isn’t the point-the goal is to ensure results are stable,
    comparable, and scientifically sound, even if tiny numerical differences creep
    in.
  prefs: []
  type: TYPE_NORMAL
- en: 49\. Command-Line Flags and Defaults
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When you run a training program, you often want to change certain settings without
    editing the source code. For example, you might want to try a different batch
    size, adjust the learning rate, or train for more steps. Command-line flags make
    this possible. In `train_gpt2.c`, defaults are set inside the program, but it
    can also be compiled to accept arguments, giving you flexibility while keeping
    the code minimal.
  prefs: []
  type: TYPE_NORMAL
- en: Why flags exist
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Deep learning experiments are highly sensitive to hyperparameters-values like
    learning rate, batch size, sequence length, or number of steps. If every change
    required modifying source code, recompiling, and rerunning, experimentation would
    be slow and error-prone. Flags allow you to configure these parameters quickly
    at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: 'In many large frameworks (like PyTorch or TensorFlow), command-line arguments
    are parsed with helper libraries. In `llm.c`, the philosophy is simplicity: flags
    are either defined in code as constants, or you can extend `main` with standard
    C argument parsing to override defaults.'
  prefs: []
  type: TYPE_NORMAL
- en: Defaults in `train_gpt2.c`
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Looking at the code, here are the main defaults hardcoded in the `main` function:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Batch size (`B`):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE127]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Sequence length (`T`):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Validation batches:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Training steps:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: By default, only 40 steps are run in this example.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Optimizer hyperparameters (inside `gpt2_update` call):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE131]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here the learning rate is `1e-4`, beta values for AdamW are `0.9` and `0.999`,
    epsilon is `1e-8`, and weight decay is `0.0`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: These defaults are chosen to make the reference training loop run quickly and
    predictably, especially on small datasets like Tiny Shakespeare or Tiny Stories.
  prefs: []
  type: TYPE_NORMAL
- en: How to add flags
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If you want flexibility, you can extend `main` with argument parsing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you can run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE133]'
  prefs: []
  type: TYPE_PRE
- en: This sets batch size to 8, sequence length to 128, and steps to 100, without
    changing source code.
  prefs: []
  type: TYPE_NORMAL
- en: Why it matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Command-line flags make experimentation far more efficient. You can try multiple
    configurations in one day without recompiling or editing the file repeatedly.
    This is especially useful when running jobs on clusters where you want scripts
    that launch many experiments automatically with different parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Defaults are equally important: they give you a safe, predictable starting
    point. Beginners can run the code without thinking about flags, while advanced
    users can override values as needed.'
  prefs: []
  type: TYPE_NORMAL
- en: Try it yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Keep the default batch size of 4 and sequence length of 64\. Run training and
    note the time per step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change batch size to 8 by editing the code. Observe how training speed changes
    and how memory usage increases.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modify the loop to train for 200 steps instead of 40\. Watch how loss decreases
    further.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add argument parsing to accept learning rate as a flag. Experiment with `1e-3`
    vs. `1e-5` and see how quickly training diverges or stalls.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a shell script that runs training multiple times with different values
    for `B` and `T`. Compare results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Command-line flags and defaults balance simplicity with flexibility. Defaults
    make the code runnable out of the box, while flags let you scale experiments without
    constantly editing the source. In `train_gpt2.c`, this design keeps the training
    loop minimal but still adaptable, encouraging both clarity and experimentation.
  prefs: []
  type: TYPE_NORMAL
- en: 50\. Example CPU Training Logs and Outputs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One of the best ways to understand what a training loop is doing is by reading
    its logs. Logs are the program’s way of telling you how training is progressing:
    what the loss is, how fast it’s running, and whether validation checks are improving.
    In `train_gpt2.c`, logging is deliberately minimal so you can easily see the essentials
    without being overwhelmed.'
  prefs: []
  type: TYPE_NORMAL
- en: What the logs look like
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here’s a snippet of output from running the CPU training loop on Tiny Shakespeare:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs: []
  type: TYPE_PRE
- en: 'Each part of this output has meaning:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Dataset sizes: how many training and validation batches are available.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Model config: confirmation that the GPT-2 model was loaded correctly (sequence
    length, vocab size, number of layers, etc.).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Validation loss: an average measure of how well the model is doing on unseen
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Training step logs: for each step, you see the training loss and how long the
    step took in milliseconds.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding loss values
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Loss is the number that tells us how far the model’s predictions are from the
    correct answers. Lower is better.
  prefs: []
  type: TYPE_NORMAL
- en: A loss around 5.3 means the model is essentially guessing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As training progresses, you want to see this number slowly decrease.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the number gets stuck, or goes up, it can indicate problems with the learning
    rate, dataset, or implementation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Think of it like a report card: at the beginning, the model is failing every
    test, but as it practices (trains), the grades (loss values) improve.'
  prefs: []
  type: TYPE_NORMAL
- en: Speed measurements
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The “took … ms” part shows how long each step took. On CPU, this is usually
    slow, sometimes a couple of seconds per step. On GPU, the same step might only
    take tens of milliseconds.
  prefs: []
  type: TYPE_NORMAL
- en: 'Timing logs are useful because they help you:'
  prefs: []
  type: TYPE_NORMAL
- en: Estimate how long full training will take.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compare performance between machines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spot problems if training suddenly slows down.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Occasional validation checks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Every few steps, the code switches to validation data and prints a `val loss`.
    This is crucial: training loss always goes down if the model memorizes the training
    set, but validation loss tells you if it is *actually learning patterns* that
    generalize.'
  prefs: []
  type: TYPE_NORMAL
- en: If training loss goes down but validation loss stays high, that’s a sign of
    overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Generated samples
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'At certain steps, the code also prints generated text like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE135]'
  prefs: []
  type: TYPE_PRE
- en: Even though the text might look strange at first, it’s a powerful sign that
    the model is learning. At the beginning, output is pure gibberish, but as training
    continues, you start to see recognizable words and patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Why it matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Logs are your window into the training process. Without them, training would
    be a black box-you’d wait hours and have no idea if it was working. By watching
    loss curves, step times, and sample outputs, you can make informed adjustments
    and gain confidence that the model is on the right track.
  prefs: []
  type: TYPE_NORMAL
- en: Try it yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Run the training loop as-is and save the console output. Mark how loss changes
    between step 0 and step 40.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Increase the number of steps to 200 and compare how the losses evolve.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change the batch size from 4 to 8 and note both the training speed and the loss
    behavior.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Edit the code to print validation loss every step instead of every 10 steps.
    Does the trend look smoother?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save the generated samples at steps 20 and 40\. Compare how the quality changes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Training logs are like a diary of the model’s progress. They show you how quickly
    the model is learning, how well it generalizes, and how fast the computation runs.
    By reading and interpreting logs carefully, you can guide experiments, detect
    problems early, and appreciate the progress that’s happening inside the model.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 6\. Testing and Profiling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 51\. Debug State Structs and Their Role
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When building and training a model as complex as GPT-2, you need ways to peek
    inside and check whether the values being passed around make sense. This is where
    debug state structs come in. In *llm.c*, the code is written in plain C, without
    the rich debugging utilities of frameworks like PyTorch. That means the developers
    had to create their own mechanism to store, inspect, and compare intermediate
    values.
  prefs: []
  type: TYPE_NORMAL
- en: 'A struct in C is just a container that groups related variables together. For
    debugging, you can think of a struct as a little notebook where the program writes
    down numbers as it computes them. These numbers might include:'
  prefs: []
  type: TYPE_NORMAL
- en: The raw embeddings for tokens.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The attention scores before and after softmax.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The outputs of each MLP block.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The predicted probabilities for the next token.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By saving these into a structured format, the program can later compare them
    to the outputs of a trusted reference implementation (usually PyTorch).
  prefs: []
  type: TYPE_NORMAL
- en: How it works in practice
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Inside *llm.c*, there are places where arrays of floats—like hidden states or
    logits—are copied into a debug state struct. Once stored, these values can be
    printed, dumped to a file, or checked against “golden” results from PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you’re testing a tiny batch of input tokens. The forward pass runs as
    usual, but at specific checkpoints (say, right after attention or after the final
    linear projection), the program writes those arrays into a struct. Later, when
    running a PyTorch model with the same inputs and weights, the two outputs can
    be compared element by element.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is essential for catching subtle errors:'
  prefs: []
  type: TYPE_NORMAL
- en: A misplaced transpose in matrix multiplication.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forgetting to apply a mask in attention.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A floating-point precision mismatch in softmax.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Without the struct, you’d only know the model loss looks “off.” With the struct,
    you know *exactly* which step went wrong.
  prefs: []
  type: TYPE_NORMAL
- en: Why it matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Debug structs bridge the gap between C and Python ecosystems. PyTorch has a
    decade of battle-tested layers, so it’s the gold standard for correctness. By
    saving intermediate activations in C and comparing them against PyTorch, developers
    ensure that every layer behaves identically. This builds confidence that the *llm.c*
    codebase isn’t just “roughly correct,” but precisely reproduces GPT-2’s math.
  prefs: []
  type: TYPE_NORMAL
- en: For anyone modifying the code—for example, writing a new activation function
    or experimenting with quantization—the debug structs act like a safety net. You
    can quickly see if your change accidentally altered the outputs in a way that
    breaks parity with the original model.
  prefs: []
  type: TYPE_NORMAL
- en: Try it yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Trace a forward pass: Run the CPU version with a tiny batch and enable debug
    dumps. Look at the embeddings, attention scores, and final logits.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Cross-check with PyTorch: Run the same input through Hugging Face GPT-2\. Print
    the same tensors. Compare a few entries by hand—do they match closely?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Introduce a bug: Change the scaling factor in attention (e.g., remove the `1/√d`
    term). Run again and see how quickly the mismatch shows up in the debug struct.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Extend the struct: Add a new field for an intermediate step you care about,
    like LayerNorm outputs. Print it during debugging to see how normalization changes
    the activations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Debug state structs are the microscope of *llm.c*. They allow you to pause the
    flow of numbers, record them, and compare them against a known-good model. Without
    them, development would feel like working blindfolded. With them, you can track
    down errors precisely, ensure parity with PyTorch, and confidently extend the
    system knowing you have a reliable safety net.
  prefs: []
  type: TYPE_NORMAL
- en: '52\. `test_gpt2.c`: CPU vs PyTorch'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Testing is one of the most important parts of the *llm.c* project. The file
    `test_gpt2.c` exists specifically to check whether the C implementation of GPT-2
    produces the same outputs as PyTorch, which is the trusted reference. Without
    this file, you would only know if the final training loss looked reasonable. With
    it, you can verify that every part of the forward pass matches.
  prefs: []
  type: TYPE_NORMAL
- en: 'At its core, `test_gpt2.c` runs a very controlled experiment: it loads a GPT-2
    model checkpoint (exported from PyTorch), prepares a small batch of input tokens,
    executes a forward pass in C, and compares the outputs against the corresponding
    tensors from PyTorch. If everything matches within a tight numerical tolerance,
    you know the C code is correct. If not, you have a clear signal that something
    is wrong.'
  prefs: []
  type: TYPE_NORMAL
- en: How the test works
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Load the checkpoint The test begins by reading a binary checkpoint file, such
    as `gpt2_124M.bin`, which contains the weights of the GPT-2 model. These weights
    were originally trained in PyTorch and then exported into a binary format that
    *llm.c* can understand.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prepare the inputs The test uses a known sequence of token IDs—sometimes from
    a dataset like Tiny Shakespeare, sometimes just a few hand-picked tokens. This
    ensures the same inputs can be run through both PyTorch and C implementations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the C forward pass The function `gpt2_forward` is executed on the CPU. All
    embeddings, attention layers, MLPs, and final logits are computed exactly as they
    would be during real inference or training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare with PyTorch For each major tensor (e.g., hidden states, attention outputs,
    final logits), the values are compared against saved outputs from PyTorch. The
    comparison usually allows for very small differences, since floating-point math
    can vary slightly between libraries. A tolerance like `1e-5` is common.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Report mismatches If any element deviates beyond the allowed tolerance, the
    test reports the difference. Developers can then investigate where the divergence
    started, often by adding more debug dumps of intermediate states.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why this test is crucial
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: C code is low-level and unforgiving. A single indexing mistake, a wrong stride,
    or a missing scaling factor in attention can make outputs diverge wildly. Since
    GPT-2 has millions of parameters, such errors are almost impossible to spot by
    hand. By tying the implementation back to PyTorch, `test_gpt2.c` provides a ground
    truth check.
  prefs: []
  type: TYPE_NORMAL
- en: This also ensures scientific reproducibility. If someone else downloads *llm.c*
    and runs `test_gpt2.c`, they should see the same level of agreement with PyTorch.
    That way, they can trust that training runs, losses, and model outputs are not
    artifacts of a broken implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Example in action
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Imagine you’ve just modified the attention code to optimize the matrix multiplication.
    You recompile and run `test_gpt2.c`. If you see an error like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: 'you know the two match within tolerance—everything is fine. But if you see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs: []
  type: TYPE_PRE
- en: that’s a red flag. It means the optimization introduced a bug. Without the test,
    you might not notice until much later when training fails to converge.
  prefs: []
  type: TYPE_NORMAL
- en: Why it matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`test_gpt2.c` is the guarantee that the C implementation is not just “close
    enough,” but *faithful*. It ensures that improvements, optimizations, or even
    rewrites don’t silently corrupt the model’s behavior. It’s a direct bridge between
    the experimental world of C internals and the well-established baseline of PyTorch.'
  prefs: []
  type: TYPE_NORMAL
- en: Try it yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Run `make test_gpt2` in the repository. Observe whether the outputs match PyTorch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deliberately change one line of code in `gpt2_forward`—for example, remove the
    attention scaling factor. Run the test again and see how quickly it fails.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add your own print statements to show which tensors are being compared. Watch
    how the numbers line up almost exactly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try running with different checkpoints (e.g., 124M vs 355M) to see if parity
    holds across scales.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`test_gpt2.c` is not just another file in the repository—it’s the truth meter.
    It reassures you that the complicated layers of GPT-2 have been implemented correctly
    in C and remain consistent with PyTorch. This confidence is what allows further
    work—whether training, profiling, or extending the model—to proceed on a solid
    foundation.'
  prefs: []
  type: TYPE_NORMAL
- en: 53\. Matching Outputs Within Tolerances
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once you have a test like `test_gpt2.c` set up, the next challenge is figuring
    out how close the outputs need to be for the test to pass. Computers don’t always
    produce bit-for-bit identical results when doing floating-point math. The order
    of operations, the precision of instructions, and even the type of hardware (CPU
    vs GPU) can cause tiny differences.
  prefs: []
  type: TYPE_NORMAL
- en: If you demanded exact matches, most tests would fail even when the implementation
    is correct. That’s why *llm.c* uses tolerance-based comparison. Instead of asking
    “are these numbers exactly equal?”, the code asks “are these numbers *close enough*?”
  prefs: []
  type: TYPE_NORMAL
- en: Absolute vs relative tolerance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are two common ways to define “close enough”:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Absolute tolerance: check that the difference between two numbers is smaller
    than a threshold. For example,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE138]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This works well for values near zero.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Relative tolerance: check that the difference is small relative to the size
    of the numbers. For example,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE139]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Even though the absolute difference is 0.1, that’s tiny compared to the scale
    of 1000.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In practice, the code often combines both. It passes if the difference is smaller
    than either the absolute tolerance or the relative tolerance.
  prefs: []
  type: TYPE_NORMAL
- en: Why tolerances are necessary
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Imagine you run the forward pass on CPU in C and in PyTorch. PyTorch might
    use fused kernels or higher-precision accumulations. If you compare the final
    logits, you may see values like:'
  prefs: []
  type: TYPE_NORMAL
- en: 'PyTorch: `-3.4521234`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'C: `-3.4521255`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The difference is just `0.0000021`. For practical purposes, they’re the same.
    Without tolerance, this tiny difference would fail the test. With tolerance, you
    can safely say both implementations agree.
  prefs: []
  type: TYPE_NORMAL
- en: Example from debugging
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Suppose you compare the probabilities after softmax. You might get:'
  prefs: []
  type: TYPE_NORMAL
- en: 'PyTorch: `0.3333333, 0.3333333, 0.3333333`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'C: `0.3333334, 0.3333333, 0.3333333`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, the first value differs in the last decimal place. The tolerance rule
    says that’s fine, since the absolute error is smaller than `1e-7`.
  prefs: []
  type: TYPE_NORMAL
- en: 'But if you saw something like:'
  prefs: []
  type: TYPE_NORMAL
- en: 'PyTorch: `0.3333333, 0.3333333, 0.3333333`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'C: `0.5000000, 0.2500000, 0.2500000`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the mismatch is huge—no tolerance rule would allow it. That’s a clear bug.
  prefs: []
  type: TYPE_NORMAL
- en: Why it matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Matching within tolerance isn’t just a technical detail; it’s about trust. It
    lets you say with confidence that the implementation is mathematically faithful
    to the reference. You don’t waste time chasing harmless decimal noise, but you
    also don’t miss real mistakes.
  prefs: []
  type: TYPE_NORMAL
- en: This approach is also what makes cross-platform development possible. The same
    *llm.c* code can be run on Linux, macOS, or even inside different compilers, and
    as long as results fall within tolerance, you know the model behavior is preserved.
  prefs: []
  type: TYPE_NORMAL
- en: Try it yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Run `test_gpt2.c` and look at the output logs. Notice how many decimal places
    match.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change the tolerance threshold in the code from `1e-5` to something stricter
    like `1e-8`. See if the test starts failing due to harmless floating-point noise.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a deliberate bug—for example, skip dividing by `√d` in the attention code—and
    rerun. The mismatches will be far larger than the tolerance, proving the bug is
    real.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare CPU results with PyTorch, then recompile with different compiler flags
    (like `-O0` vs `-O3`) and check if results still fall within tolerance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Tolerance-based testing is what allows *llm.c* to be both rigorous and realistic.
    It ensures that differences are only flagged when they matter, while ignoring
    the harmless quirks of floating-point math. This makes the test suite a reliable
    tool for catching true errors without overwhelming you with false alarms.
  prefs: []
  type: TYPE_NORMAL
- en: 54\. Profiling with `profile_gpt2.c`
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After verifying that the outputs match PyTorch, the next big question is: how
    fast is the code running? Correctness is essential, but performance is what makes
    a minimal C implementation like *llm.c* worthwhile. That’s where `profile_gpt2.c`
    comes in. It is a small program that runs controlled forward passes through GPT-2
    and measures the time they take, helping you understand where the bottlenecks
    are.'
  prefs: []
  type: TYPE_NORMAL
- en: What profiling means
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Profiling is the act of measuring the performance of a program, not just its
    correctness. Instead of asking “does this number match PyTorch?”, profiling asks:'
  prefs: []
  type: TYPE_NORMAL
- en: How many milliseconds does one forward pass take?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which part of the model consumes the most time?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does using OpenMP threads actually speed things up?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does batch size affect runtime?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By answering these, you can make informed decisions about optimization.
  prefs: []
  type: TYPE_NORMAL
- en: How `profile_gpt2.c` works
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The profiling program is structured like a simplified inference loop.
  prefs: []
  type: TYPE_NORMAL
- en: Model setup It loads a GPT-2 checkpoint (e.g., `gpt2_124M.bin`) into memory
    and allocates space for activations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dummy input Instead of using real text, it creates random token IDs. That way,
    the cost measured comes purely from the computation, not from data loading or
    tokenization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Timing with clock functions Before and after each forward pass, it records timestamps
    with `clock_gettime(CLOCK_MONOTONIC, &start)` and `&end`. The difference gives
    the runtime in seconds, which is usually converted into milliseconds.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Looping for stability A single run can be noisy due to background processes
    on your computer. To smooth things out, `profile_gpt2.c` runs the forward pass
    multiple times and averages the results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reporting results Finally, it prints the average time per forward pass. Sometimes
    it also estimates FLOPs (floating-point operations per second), giving you a rough
    idea of efficiency compared to the theoretical peak of your CPU.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What you can learn from profiling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Running `profile_gpt2.c` on a CPU gives insights like:'
  prefs: []
  type: TYPE_NORMAL
- en: The attention blocks dominate runtime, because they involve large matrix multiplications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increasing batch size makes the runtime longer, but not proportionally—sometimes
    bigger batches use hardware more efficiently.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenMP can speed things up when there are multiple CPU cores available, but
    scaling may flatten out after a certain number of threads.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This helps decide where to spend effort. For example, if LayerNorm takes 2%
    of the runtime but attention takes 70%, you know optimization should focus on
    the attention code.
  prefs: []
  type: TYPE_NORMAL
- en: Why it matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Profiling isn’t just about numbers. It’s about guiding your development choices.
    Without profiling, you might spend weeks hand-optimizing LayerNorm, only to discover
    it barely affects overall runtime. With profiling, you see immediately where the
    slowdowns are and can focus on the real bottlenecks.
  prefs: []
  type: TYPE_NORMAL
- en: It also provides baseline performance numbers. If you change something in the
    implementation, re-running `profile_gpt2.c` will tell you if it sped things up
    or slowed them down. This feedback loop is essential for optimization work.
  prefs: []
  type: TYPE_NORMAL
- en: Try it yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Compile and run `profile_gpt2.c` with a small model checkpoint. Note the reported
    runtime per forward pass.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change the batch size `B` and sequence length `T`, then re-run. Watch how runtime
    scales with larger inputs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set `OMP_NUM_THREADS=1` to disable threading and compare it against `OMP_NUM_THREADS=4`
    or higher. How much faster is it with multiple cores?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modify the code to time individual layers (embeddings, attention, MLP). This
    gives even more precise insight into which parts dominate computation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`profile_gpt2.c` turns performance from a guess into hard data. It tells you
    exactly how long a forward pass takes, how much threading helps, and where the
    real bottlenecks are. With it, you can track progress as you optimize the code,
    ensuring that your changes make the model not only correct but also efficient.'
  prefs: []
  type: TYPE_NORMAL
- en: 55\. Measuring FLOPs and CPU Performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When talking about performance in deep learning, it’s not enough to say “this
    run took 200 milliseconds.” To really understand efficiency, we need a measure
    that’s independent of hardware and input size. That’s where FLOPs come in: floating-point
    operations. A FLOP is a single numerical calculation involving real numbers—like
    an addition, multiplication, or division.'
  prefs: []
  type: TYPE_NORMAL
- en: By counting how many FLOPs a model requires and comparing it to how many the
    computer can perform per second, you can evaluate how close your implementation
    is to the theoretical maximum speed of your CPU.
  prefs: []
  type: TYPE_NORMAL
- en: What FLOPs mean in practice
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Every layer of GPT-2—embeddings, attention, feed-forward—can be broken down
    into a sequence of multiplications and additions. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: A matrix multiplication between two matrices of size `M × K` and `K × N` requires
    `2 × M × K × N` FLOPs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A softmax across a vector of size `d` requires about `3d` FLOPs (exponentials,
    sums, and divisions).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An MLP block with hidden size `h` and intermediate size `4h` requires multiple
    matrix multiplications, adding up to billions of FLOPs per training step.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you sum these across all layers, even a small GPT-2 model like 124M parameters
    involves several gigaFLOPs (billions of operations) for one forward pass.
  prefs: []
  type: TYPE_NORMAL
- en: How `profile_gpt2.c` estimates FLOPs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The profiling code doesn’t literally count every multiplication. Instead, it
    uses formulas derived from matrix dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: The configuration struct (`model.config`) gives the number of layers, heads,
    embedding size, and sequence length.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each block (attention + MLP), the code applies standard FLOPs formulas for
    matrix multiplication and softmax.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These counts are added up to estimate the total FLOPs for a forward pass.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dividing the total FLOPs by the measured runtime (in seconds) gives FLOPs/second,
    also known as throughput.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For example, if a forward pass takes 0.1 seconds and involves 20 billion FLOPs,
    then throughput is about 200 GFLOPs/s.
  prefs: []
  type: TYPE_NORMAL
- en: Why this is useful
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Compare hardware: You can test the same model on a laptop CPU and a server
    CPU, then compare FLOPs/s to see how much faster the server is.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compare implementations: If you modify attention to use a different algorithm,
    the FLOPs count won’t change, but if runtime decreases, throughput increases—showing
    the optimization worked.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Know your limits: CPUs often achieve only a fraction of their theoretical peak
    FLOPs due to memory bottlenecks. Profiling shows how close you’re getting in practice.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let’s say:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Forward pass FLOPs: 15 billion'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Runtime: 0.2 seconds'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Throughput: 75 GFLOPs/s'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your CPU’s datasheet says the peak is 200 GFLOPs/s, then you’re at about
    37% efficiency. That gap might be due to memory latency, cache misses, or lack
    of vectorization.
  prefs: []
  type: TYPE_NORMAL
- en: Try it yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Run `profile_gpt2.c` and note the reported FLOPs and runtime.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change the sequence length `T` and observe how FLOPs scale linearly with it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Increase the number of layers in the model configuration—watch FLOPs rise accordingly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare your measured FLOPs/s with the theoretical maximum listed for your CPU.
    How close are you?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: FLOPs turn performance from “this feels fast” into hard numbers you can compare
    across runs, machines, and implementations. By knowing both the operation count
    and the achieved throughput, you gain a clear picture of how efficient the *llm.c*
    code really is and where further optimizations might pay off.
  prefs: []
  type: TYPE_NORMAL
- en: 56\. Capturing Memory Usage on CPU
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While FLOPs tell us how much raw computation a model needs, performance isn’t
    just about speed—it’s also about memory usage. Modern language models are enormous,
    and on CPUs with limited RAM, memory can become the true bottleneck. That’s why
    *llm.c* also emphasizes monitoring how much memory is used during inference and
    training.
  prefs: []
  type: TYPE_NORMAL
- en: What consumes memory in GPT-2
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are several key components that take up space:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters (weights): GPT-2 124M has about 124 million parameters. Each is
    stored as a 32-bit float (4 bytes). That alone is roughly 500 MB.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Gradients: During training, gradients for each parameter are stored. That doubles
    the memory usage.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Optimizer states: AdamW requires two additional memory slots per parameter
    (`m` and `v`), which doubles it again. With parameters, gradients, and optimizer
    states combined, training can require 4× the parameter size in memory.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Activations: These are the intermediate outputs of each layer (attention scores,
    MLP results, normalized states). For backpropagation, activations from the forward
    pass must be kept until gradients are computed. Depending on batch size and sequence
    length, activations can rival parameter memory.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Measuring memory in practice
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'On CPU, memory usage can be inspected in several ways:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operating system tools: `top`, `htop`, or Activity Monitor show total memory
    used by the program.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Manual accounting in code: *llm.c* knows how many parameters, gradients, and
    optimizer states exist. By multiplying their counts by 4 bytes, it can estimate
    usage precisely.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Instrumentation during profiling: you can add checkpoints that print memory
    usage at different stages of the forward or backward pass.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters only: ~500 MB.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Parameters + gradients: ~1 GB.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Parameters + gradients + optimizer: ~2 GB.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Adding activations: 2.5–3 GB, depending on batch size and sequence length.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why memory matters on CPU
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: On a CPU, you don’t just care about “can it fit in RAM?” You also care about
    cache efficiency. Modern CPUs have multiple levels of cache (L1, L2, L3), which
    are much faster than main RAM. If activations or weights don’t fit well in cache,
    performance can suffer even if you technically have enough RAM.
  prefs: []
  type: TYPE_NORMAL
- en: Memory footprint also limits experiment flexibility. For example, increasing
    sequence length from 64 to 1024 multiplies activation storage by 16\. A run that
    fits at `T=64` may crash or swap at `T=1024`.
  prefs: []
  type: TYPE_NORMAL
- en: Example scenario
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Suppose you run GPT-2 124M with:'
  prefs: []
  type: TYPE_NORMAL
- en: Batch size `B=4`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequence length `T=64`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This might use ~2.5 GB of memory for training. If you raise `T` to 512, suddenly
    activations balloon, and total usage may exceed 10 GB. On a laptop with 8 GB RAM,
    this simply won’t work.
  prefs: []
  type: TYPE_NORMAL
- en: By monitoring memory carefully, you can avoid mysterious crashes and plan runs
    realistically.
  prefs: []
  type: TYPE_NORMAL
- en: Try it yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Run training with a small batch (`B=2`, `T=64`) and check memory usage with
    `htop`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Increase `T` step by step (128, 256, 512) and record the growth. Watch how activations
    dominate beyond a certain length.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Calculate parameter memory manually: `num_params × 4 bytes`. Compare it to
    what the OS reports. The difference comes from activations and optimizer states.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modify the code to print memory allocations explicitly when arrays are created.
    This gives an internal log of usage at each step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Memory is the silent partner of FLOPs: you need both to train and run models
    efficiently. Profiling without tracking memory is incomplete—you might have a
    model that’s fast but impossible to run on your machine. By capturing and understanding
    memory usage, you gain the ability to scale responsibly, balance batch size and
    sequence length, and keep your experiments stable.'
  prefs: []
  type: TYPE_NORMAL
- en: 57\. Reproducing Known Loss Curves (CPU-only)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once the model is correct and its performance is measured, the next important
    step is to check whether it learns in the way we expect. In deep learning, we
    usually monitor this with a loss curve—a graph that shows how the training loss
    decreases over time as the model sees more data.
  prefs: []
  type: TYPE_NORMAL
- en: For GPT-2 and other language models, the standard loss function is cross-entropy,
    which measures how well the predicted probability distribution over tokens matches
    the actual next token in the dataset. If the implementation is right, the loss
    should fall in a predictable way when trained on text like Tiny Shakespeare or
    Tiny Stories.
  prefs: []
  type: TYPE_NORMAL
- en: What a “known” loss curve looks like
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The community has already run countless GPT-2 experiments in PyTorch, so we
    know roughly what the curve should look like:'
  prefs: []
  type: TYPE_NORMAL
- en: At the very beginning, the loss is high (around 5–6) because the model is basically
    guessing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After a few hundred steps, the loss starts dropping steadily.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For Tiny Shakespeare, loss often goes down toward ~2.0 with a small GPT-2 model
    (124M parameters).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The exact numbers can vary, but the shape—a downward trend with small fluctuations—is
    consistent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the C implementation produces a similar curve, that’s a strong sign the forward
    pass, backward pass, and optimizer are all working correctly.
  prefs: []
  type: TYPE_NORMAL
- en: How reproduction is tested in practice
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Train with a small dataset The test usually uses Tiny Shakespeare or Tiny Stories
    since they are small enough to run quickly on CPU.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Log the loss per step Each training step prints something like:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE140]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Plot the curve Save the loss values and make a simple plot with step on the
    x-axis and loss on the y-axis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare against PyTorch If you train the same model in PyTorch with the same
    hyperparameters, the loss curve should look almost identical. Small differences
    are normal due to random seeds or floating-point math.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why this is important
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Reproducing a known loss curve is more than just a sanity check. It tells you:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The math is right: gradients, optimizer updates, and scheduler logic are functioning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The data pipeline is correct: tokens are being fed in properly and batches
    are consistent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nothing is silently broken: without this, a bug might go unnoticed until much
    later in training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is especially important on CPU, because training is slower and you may
    only run a few hundred steps. If the curve starts to dip in the expected way,
    you know you’re on the right track.
  prefs: []
  type: TYPE_NORMAL
- en: Example scenario
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Suppose you train GPT-2 124M with `B=4` and `T=64` on Tiny Shakespeare. The
    loss starts around 4.9, and by step 200 it falls to around 3.2\. If PyTorch shows
    a similar trajectory, then your implementation is validated.
  prefs: []
  type: TYPE_NORMAL
- en: But if your loss stays flat, say at 5.0 for hundreds of steps, that’s a red
    flag. It could mean gradients are not flowing, the optimizer isn’t updating weights,
    or your data loader is feeding the same batch repeatedly.
  prefs: []
  type: TYPE_NORMAL
- en: Try it yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Train for 200 steps on Tiny Shakespeare with the C code and save the printed
    losses.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the same setup in PyTorch. Plot both curves together and compare.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Intentionally break something—for example, comment out the optimizer update
    step—and observe how the loss no longer decreases.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Experiment with learning rates. Too high may cause the curve to bounce up and
    down, while too low will make it drop very slowly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Reproducing known loss curves is the ultimate integration test. It proves that
    the entire pipeline—data, model, training loop, optimizer—works together in harmony.
    When your loss curve matches the reference, you can trust that your C implementation
    of GPT-2 is not only correct in theory but also effective in practice.
  prefs: []
  type: TYPE_NORMAL
- en: 58\. Debugging Numerical Stability (NaNs, Infs)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Even if the model produces correct outputs most of the time, there’s a hidden
    danger in deep learning: numerical instability. This happens when floating-point
    numbers inside the computation blow up to infinity (`Inf`) or collapse into “not
    a number” (`NaN`). When this occurs, training usually grinds to a halt—loss becomes
    undefined, gradients explode, and parameters no longer update meaningfully.'
  prefs: []
  type: TYPE_NORMAL
- en: Why NaNs and Infs happen
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Neural networks involve many multiplications, exponentials, and divisions. On
    paper, all of these are fine. But computers store numbers with limited precision
    (32-bit floats in this case). When values get too large or too small, they can
    no longer be represented correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Common sources include:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Softmax overflow: computing `exp(x)` on large positive numbers leads to `Inf`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Division by very small numbers: for example, dividing by `sqrt(v_hat) + eps`
    in AdamW can produce instability if `eps` is too small.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Exploding gradients: during backpropagation, errors compound across many layers,
    producing extremely large values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Improper initialization or learning rates: weights that are too large or step
    sizes that are too aggressive can push activations outside a stable range.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to detect instability in *llm.c*
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Because the code is written in C without automatic checks, NaNs and Infs can
    spread silently unless you look for them. Some useful strategies include:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assertions: insert `assert(!isnan(value) && !isinf(value));` inside loops to
    catch bad values immediately.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Debug prints: log sample values from activations or gradients each step to
    see if they drift toward extremely large numbers.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Check the loss: if the loss suddenly becomes `nan` or `inf`, that’s a strong
    signal something went wrong upstream.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Small runs: testing on tiny sequences and batches makes it easier to inspect
    values directly.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How to fix instability
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Several practical techniques help keep numbers stable:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Add an epsilon: in divisions or square roots, add a small constant (like `1e-8`)
    to prevent division by zero.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rescale before softmax: subtract the maximum value in the vector before computing
    exponentials. This keeps values in a safe range.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gradient clipping: cap gradients so they cannot exceed a certain norm. This
    stops runaway updates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Adjust learning rate: if training diverges, lowering the learning rate often
    restores stability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Check data: corrupted inputs or unexpected tokens can inject extreme values
    into the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example scenario
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Suppose you’re training GPT-2 on Tiny Shakespeare. The first few steps look
    fine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE141]'
  prefs: []
  type: TYPE_PRE
- en: This sudden jump to `nan` suggests instability. Checking the gradients reveals
    extremely large values in the attention weights. The fix might be lowering the
    learning rate from `1e-4` to `5e-5` or enabling gradient clipping.
  prefs: []
  type: TYPE_NORMAL
- en: Try it yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Train with a very high learning rate (`1e-2`) and watch how quickly NaNs appear.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a debug check inside `gpt2_forward` that prints when any activation exceeds
    `1e6`. Run a few steps and observe if values explode.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modify the softmax code to omit subtracting the max. Compare stability before
    and after.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a gradient clipping routine and measure whether it prevents loss from diverging.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Numerical stability is the difference between a model that trains smoothly and
    one that collapses after a few steps. By anticipating where NaNs and Infs can
    arise, adding checks, and applying stabilizing tricks, you make *llm.c* robust.
    This ensures that experiments are reliable and that debugging focuses on real
    algorithmic issues rather than avoidable numerical traps.
  prefs: []
  type: TYPE_NORMAL
- en: 59\. From Unit Test to Full Training Readiness
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Unit tests are the first line of defense: they check whether small, isolated
    parts of the code—like embeddings, attention, or softmax—produce the correct outputs.
    But passing unit tests isn’t the same as being ready for full training. The transition
    from “this layer works” to “the whole system learns correctly over thousands of
    steps” involves additional challenges.'
  prefs: []
  type: TYPE_NORMAL
- en: The gap between unit tests and training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Unit tests check correctness: for example, verifying that `gpt2_forward` produces
    the same logits as PyTorch on a single batch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Training readiness checks robustness: making sure the model can run repeatedly
    for thousands of steps without crashing, diverging, or leaking memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Think of it like testing a car. Unit tests are like checking the brakes, headlights,
    and steering individually. Training readiness is taking the car on a 500-mile
    road trip and making sure nothing overheats, rattles loose, or fails under stress.
  prefs: []
  type: TYPE_NORMAL
- en: What needs to be validated for training readiness
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Loss curve behavior Run the training loop for several hundred steps. The training
    loss should steadily decrease, matching known curves from PyTorch. If it stagnates
    or spikes, something is wrong in gradients or the optimizer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Validation runs Regularly measure validation loss during training. If it decreases
    at first and then stabilizes, that shows the model is generalizing. If it decreases
    too quickly and then shoots up, that suggests overfitting.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Memory stability Training uses more memory than inference because of gradients
    and optimizer states. A memory leak—forgetting to free arrays or reallocating
    without release—will cause the program to crash after many steps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Optimizer state updates Check that AdamW accumulates `m` and `v` correctly over
    many iterations. If bias correction is missing, loss curves will diverge from
    expected baselines.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reproducibility With the same random seed, two runs should produce nearly identical
    loss curves. Small differences are normal, but major deviations suggest nondeterministic
    bugs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why it matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Without this step, you might believe your implementation is complete after unit
    tests, only to discover that training silently fails at step 500\. Training readiness
    ensures the system is not only mathematically correct in small pieces but also
    practically usable for long-running experiments.
  prefs: []
  type: TYPE_NORMAL
- en: This is also where confidence in deploying the code comes from. Passing training
    readiness means others can clone the repository, run the scripts, and expect stable
    training without mysterious crashes.
  prefs: []
  type: TYPE_NORMAL
- en: Example scenario
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You run `test_gpt2.c` and all outputs match PyTorch within tolerance—great.
    Then you launch training for 5,000 steps. After step 600, the loss becomes `nan`.
    Investigation reveals that `gpt2_update` wasn’t applying bias correction properly,
    so the optimizer went unstable. That’s a bug you’d never catch with a one-batch
    unit test, but training readiness exposes it.
  prefs: []
  type: TYPE_NORMAL
- en: Try it yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Run training for 1,000 steps on Tiny Shakespeare and log the loss every 10 steps.
    Check that it decreases smoothly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add validation runs every 100 steps. Watch for the classic gap between train
    loss (lower) and validation loss (slightly higher).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use `htop` or similar tools to monitor memory usage during training. Confirm
    that it stays steady rather than creeping upward.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the same training twice with the same seed. Compare the two loss curves—are
    they nearly identical?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Unit tests prove the pieces are correct. Training readiness proves the whole
    system works under real conditions. Both are necessary. Together, they give you
    the confidence that *llm.c* isn’t just a collection of working parts, but a functioning
    engine capable of training GPT-2 models end to end.
  prefs: []
  type: TYPE_NORMAL
- en: 60\. Limitations of CPU Testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Testing GPT-2 on CPU is invaluable for verifying correctness, but it comes with
    clear limits. Understanding these limitations helps you interpret the results
    properly and know when it’s time to move on to GPU-based experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Speed constraints
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The most obvious limitation is speed. CPUs are optimized for general-purpose
    tasks, not the massive parallelism that neural networks demand. A single forward
    and backward pass on GPT-2 124M can take seconds or even minutes on CPU, while
    a GPU might handle it in milliseconds. This makes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Full-scale training impractical: training GPT-2 124M to convergence could take
    weeks or months on CPU.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Experiment cycles slower: testing new optimizations or debugging is slowed
    because each run takes longer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For this reason, CPU testing is best suited for small-scale sanity checks, not
    full training runs.
  prefs: []
  type: TYPE_NORMAL
- en: Memory overhead
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: CPU memory is typically more abundant than GPU VRAM, but slower. The bottleneck
    often isn’t “do we have enough RAM?” but “how quickly can we move data in and
    out of memory?” As sequence length `T` grows, the activations balloon, and cache
    efficiency drops. This makes even medium-sized runs sluggish.
  prefs: []
  type: TYPE_NORMAL
- en: Limited realism
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Although CPU runs confirm that the math is correct, they don’t always reflect
    the realities of GPU execution. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: CUDA kernels have different numerical characteristics (fused operations, different
    rounding).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPU memory layouts can expose bugs that CPU arrays hide.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallel execution may create timing or synchronization issues that never appear
    on CPU.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So while CPU parity with PyTorch is necessary, it isn’t sufficient. You must
    repeat testing once CUDA code is introduced.
  prefs: []
  type: TYPE_NORMAL
- en: Loss of scale insights
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A CPU test can prove correctness for a few batches, but it doesn’t tell you
    how the code scales under heavy load. On GPU, you learn about kernel efficiency,
    memory throughput, and distributed training. CPU tests simply don’t expose those
    concerns.
  prefs: []
  type: TYPE_NORMAL
- en: Why it matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'CPU testing is the foundation: it proves the algorithm is implemented correctly,
    step by step, without relying on specialized hardware. But if you stop there,
    you’ll miss the bigger picture of performance and scalability. CPU results should
    be treated as a green light to proceed, not the final word on readiness.'
  prefs: []
  type: TYPE_NORMAL
- en: Example scenario
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Suppose you run 500 training steps on Tiny Shakespeare. The loss curve drops
    exactly as expected—success. But training on CPU is so slow that finishing an
    epoch takes several hours. This validates correctness, but makes it obvious that
    GPUs are required for meaningful experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Try it yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Train GPT-2 124M on CPU for 100 steps and record the time per step. Extrapolate
    how long it would take to run 100k steps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Increase sequence length from 64 to 512 and observe how memory access times
    affect throughput.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare your CPU loss curve with a GPU run from PyTorch. Notice they align in
    shape but differ dramatically in speed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use profiling tools (`perf`, `valgrind`, or `gprof`) to see which CPU functions
    dominate runtime.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: CPU testing is the safe laboratory where you validate correctness, catch numerical
    errors, and reproduce known loss curves. But its limitations—slow speed, reduced
    realism, and lack of scaling insights—mean it’s only a first step. Once CPU testing
    passes, the journey continues with GPU testing, profiling, and multi-device scaling.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 7\. CUDA Training (`train_gpt2.cu`)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 61\. CUDA Architecture Overview (streams, kernels)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When the CPU version of *llm.c* runs, it executes instructions one after another
    on your processor cores. This is fine for small models or debugging, but deep
    learning workloads—especially Transformers like GPT-2—demand an enormous number
    of floating-point operations. To handle that, *llm.c* also includes CUDA versions
    of the training loop that shift computation to NVIDIA GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: At a high level, CUDA is NVIDIA’s programming model that lets developers write
    code to run directly on the GPU. Unlike CPUs, which might have a few cores optimized
    for general-purpose tasks, GPUs contain thousands of simpler cores designed to
    process large batches of data in parallel. CUDA provides the tools to organize
    work so that those cores can stay busy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kernels: Small Programs That Run on the GPU'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In CUDA, a *kernel* is a function that runs on the GPU. When you launch a kernel,
    you don’t call it once like a normal C function—you launch thousands of copies
    at the same time. Each copy handles a different piece of the data. For example,
    if you want to multiply two vectors of a million elements, you can launch a million
    GPU threads, each multiplying one pair of numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *llm.c*, kernels are used for operations that can be expressed in terms
    of lots of small, independent tasks. Examples include:'
  prefs: []
  type: TYPE_NORMAL
- en: Applying the GeLU activation function elementwise to a big tensor.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding residual connections across every dimension.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalizing values in LayerNorm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For bigger, structured operations like matrix multiplications (GEMMs), the CUDA
    code often relies on specialized libraries such as cuBLAS or cuBLASLt, which are
    highly tuned for NVIDIA GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Streams: Overlapping Work'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A GPU has the ability to handle multiple tasks at once. CUDA introduces the
    idea of *streams*, which are sequences of operations that run in order relative
    to each other, but can overlap with operations in other streams. This means:'
  prefs: []
  type: TYPE_NORMAL
- en: While one kernel is executing, another can start transferring data between CPU
    and GPU.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computation and communication can overlap, reducing idle time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the training loop of *llm.c*, streams let you schedule batches of work so
    that data preparation and model computation can proceed side by side. This is
    crucial for keeping the GPU saturated with useful work instead of waiting on the
    CPU.
  prefs: []
  type: TYPE_NORMAL
- en: The Memory Hierarchy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'CUDA programming is also shaped by the GPU memory hierarchy:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Registers: Fastest, private to each thread.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shared Memory: Small chunks of memory shared among threads in a block; much
    faster than global memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Global Memory: Large, but slower. This is where tensors like weights, activations,
    and gradients usually live.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Host Memory (CPU RAM): Separate from GPU memory; transferring between them
    can be slow and should be minimized.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, in the attention kernel, partial results might be stored in registers
    or shared memory while processing a block of the sequence, before writing the
    final result back to global memory.
  prefs: []
  type: TYPE_NORMAL
- en: How This Fits Into *llm.c*
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In `train_gpt2.cu`, most of the heavy lifting is done by calls into cuBLAS/cuBLASLt
    and cuDNN for matrix multiplications and attention. But understanding the CUDA
    model—kernels, streams, and memory—helps explain:'
  prefs: []
  type: TYPE_NORMAL
- en: Why we batch operations the way we do.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why minimizing data transfers between CPU and GPU is so important.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How GPU kernels map naturally to the kinds of tensor operations GPT-2 requires.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Without CUDA, training GPT-2 would be painfully slow, even on a powerful CPU.
    CUDA gives access to thousands of cores working in parallel, but it also requires
    careful programming to avoid bottlenecks. Knowing about kernels, streams, and
    memory hierarchy is the foundation for understanding later sections where we dive
    into matrix multiplication, attention, and optimization strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Write a simple CUDA kernel that adds two arrays elementwise. Compare its performance
    to a CPU loop.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modify the kernel to use shared memory and see if it improves performance for
    larger arrays.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create two CUDA streams: one for computing a kernel, and another for copying
    data. Measure whether the operations overlap in time.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use `nvprof` or `nsys` to profile a CUDA program and observe how kernels and
    memory transfers appear on the timeline.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Think about how you would split a big matrix multiplication across thousands
    of threads—each thread computing one row, one column, or one element? What are
    the tradeoffs?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: CUDA is not just about writing code for GPUs—it’s about rethinking computation
    as thousands of small tasks that can run side by side. Kernels handle the per-thread
    work, streams let you schedule and overlap operations, and the memory hierarchy
    dictates how to organize data for maximum speed. All of these ideas come together
    in *llm.c*’s CUDA implementation, making training feasible for models like GPT-2.
  prefs: []
  type: TYPE_NORMAL
- en: 62\. Matrix Multiplication via cuBLAS/cuBLASLt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Matrix multiplication—often called GEMM (General Matrix-Matrix Multiply)—is
    the beating heart of deep learning. In GPT-2, most of the computation comes from
    multiplying large matrices: projecting embeddings into query, key, and value vectors,
    applying attention weights, and processing the MLP feed-forward layers. On the
    CPU, we saw this done with nested loops and mild optimizations. On the GPU, however,
    we need far more efficient approaches. That’s where cuBLAS and cuBLASLt come in.'
  prefs: []
  type: TYPE_NORMAL
- en: Why Matrix Multiplication Is So Central
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Almost every step of a Transformer involves multiplying two big matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: Embedding lookup can be seen as a matrix multiply between one-hot token vectors
    and the embedding table.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The attention mechanism computes dot products between queries and keys, followed
    by a weighted sum of values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The MLP applies two fully connected layers, each of which is essentially a GEMM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you profile GPT-2, you’ll find that GEMM operations dominate the runtime.
    That’s why NVIDIA’s libraries devote enormous effort to making these multiplications
    as fast as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'cuBLAS: The Classic Workhorse'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'cuBLAS is NVIDIA’s GPU-accelerated version of the BLAS (Basic Linear Algebra
    Subprograms) library. It provides highly optimized implementations of GEMM and
    related routines. Under the hood, cuBLAS:'
  prefs: []
  type: TYPE_NORMAL
- en: Splits large matrices into tiles that fit into the GPU’s shared memory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schedules thousands of threads to compute different tiles in parallel.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uses fused multiply-add (FMA) instructions for high throughput.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adapts to different GPU architectures to exploit Tensor Cores where available.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A typical call looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE142]'
  prefs: []
  type: TYPE_PRE
- en: Here `A` and `B` are the input matrices, and `C` is the result. The function
    handles all the low-level scheduling.
  prefs: []
  type: TYPE_NORMAL
- en: 'cuBLASLt: The Flexible Successor'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'While cuBLAS is powerful, it’s somewhat rigid. cuBLASLt (Lightweight cuBLAS)
    is a newer API that adds:'
  prefs: []
  type: TYPE_NORMAL
- en: Better support for mixed precision (e.g., FP16 or BF16 inputs with FP32 accumulation).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More control over algorithm selection, so developers can tune for performance
    or memory usage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Features like epilogues, which let you fuse additional operations (e.g., bias
    addition, activation functions) directly into the GEMM, reducing memory transfers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In practice, cuBLASLt often outperforms cuBLAS because it can exploit Tensor
    Cores more aggressively and fuse multiple steps into a single kernel call.
  prefs: []
  type: TYPE_NORMAL
- en: Precision and Tensor Cores
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: On modern NVIDIA GPUs (Volta, Turing, Ampere, Hopper), Tensor Cores accelerate
    matrix multiplications dramatically when using FP16, BF16, or TF32\. These special
    hardware units can perform matrix-multiply-and-accumulate on small blocks of numbers
    in a single instruction.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: On CPUs, multiplying two 16×16 matrices is done with many scalar multiplications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On GPUs with Tensor Cores, the entire block can be computed in one fused operation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In GPT-2 training, using FP16 with cuBLASLt enables much higher throughput,
    while keeping master weights in FP32 to preserve numerical stability.
  prefs: []
  type: TYPE_NORMAL
- en: Practical Example in *llm.c*
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In `train_gpt2.cu`, most of the calls to perform linear layers—such as projecting
    input activations into query, key, and value matrices—are implemented with cuBLAS/cuBLASLt.
    For instance:'
  prefs: []
  type: TYPE_NORMAL
- en: Inputs `(B, T, C)` are multiplied by a weight matrix `(C, 3C)` to produce `(B,
    T, 3C)`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Later, the output of attention `(B, T, C)` is multiplied by another projection
    matrix `(C, C)`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead of writing custom kernels for each case, the code defers to cuBLAS/cuBLASLt,
    ensuring maximum performance across GPU architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Matrix multiplications are so frequent and heavy that their performance directly
    determines how fast you can train GPT-2\. By leaning on cuBLAS/cuBLASLt, *llm.c*
    avoids reinventing the wheel and gets near-peak GPU efficiency. This makes the
    code clean, maintainable, and scalable to larger models.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Write a small CUDA program that multiplies two matrices using naive kernels,
    and compare its performance to cuBLAS.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Experiment with FP32 versus FP16 inputs and observe the speedup when Tensor
    Cores are enabled.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enable cuBLASLt’s epilogues to fuse bias addition into GEMM, and measure memory
    savings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Profile GPT-2 training with `nvprof` or `nsys` to see how much time is spent
    in GEMM calls.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try scaling up matrix sizes to simulate bigger models and note how performance
    grows relative to CPU implementations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Matrix multiplication is the computational engine of GPT-2, and on GPUs it’s
    powered by cuBLAS and cuBLASLt. These libraries harness the GPU’s architecture—tiling,
    Tensor Cores, mixed precision—to squeeze out maximum efficiency. Understanding
    how they work gives insight into why the GPU version of *llm.c* runs so much faster
    than the CPU version, and sets the stage for attention kernels and other CUDA-accelerated
    components.
  prefs: []
  type: TYPE_NORMAL
- en: '63\. Attention Kernels: cuDNN FlashAttention'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The attention mechanism is at the core of every Transformer. It allows the
    model to weigh different parts of the input sequence when producing an output.
    For GPT-2, this means that when generating the next token, the model doesn’t just
    look at the last word—it considers the entire sequence of words before it, adjusting
    how much each past token contributes. But attention is expensive. Naively, it
    scales quadratically with sequence length: for a sequence of 1024 tokens, you
    need to compute a 1024×1024 attention matrix. That’s more than a million entries,
    and each must be computed, normalized, and multiplied back into the value vectors.'
  prefs: []
  type: TYPE_NORMAL
- en: 'On CPUs, we saw how this is implemented step by step: queries, keys, and values
    are projected with matrix multiplications, dot products between queries and keys
    are computed, softmax is applied, and the result is multiplied by values. On GPUs,
    we want to do the same thing, but much faster. That’s where cuDNN’s FlashAttention
    comes into play.'
  prefs: []
  type: TYPE_NORMAL
- en: What Is FlashAttention?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: FlashAttention is an algorithm that rethinks how attention is computed. Instead
    of materializing the full attention matrix in memory, it computes softmax and
    the weighted sum in a streaming fashion. This reduces memory usage and improves
    cache efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 'Normally, attention involves these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Compute scores = Q × Kᵀ (queries times keys transpose).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply softmax over scores to get attention weights.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Multiply weights × V (values) to get the output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The problem: step 1 produces a huge score matrix of size `(sequence_length
    × sequence_length)`. Storing and processing this full matrix becomes the bottleneck.'
  prefs: []
  type: TYPE_NORMAL
- en: FlashAttention avoids storing the full matrix by computing attention block by
    block. It processes tiles of queries and keys, applies the softmax incrementally,
    and accumulates results directly into the output. This drastically cuts memory
    bandwidth requirements, which is critical for GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: cuDNN FlashAttention in Practice
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In *llm.c*’s CUDA training code, when `USE_CUDNN` is enabled, the code can
    take advantage of cuDNN’s implementation of FlashAttention. This means:'
  prefs: []
  type: TYPE_NORMAL
- en: The library handles the tiling and streaming automatically.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can leverage Tensor Cores for mixed-precision computation (FP16/BF16 inputs
    with FP32 accumulation).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It reduces memory use, which allows training longer sequences without running
    out of GPU memory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From a developer’s point of view, enabling cuDNN FlashAttention usually involves
    passing specific descriptors and flags to cuDNN routines rather than writing custom
    kernels. Instead of manually managing loops and softmax stability tricks, you
    hand over the responsibility to cuDNN, which has a heavily optimized kernel.
  prefs: []
  type: TYPE_NORMAL
- en: Why This Is a Game-Changer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The quadratic cost of attention has long been a bottleneck in scaling Transformers.
    With FlashAttention, the bottleneck shifts. The computation is still O(N²), but
    because memory is handled so much more efficiently, the GPU spends less time waiting
    on memory loads and more time doing actual math. This means:'
  prefs: []
  type: TYPE_NORMAL
- en: Training can be faster even at the same sequence length.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can push to larger sequence lengths (e.g., 2K or 4K tokens) without running
    out of GPU memory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Energy efficiency improves because you avoid redundant reads/writes to global
    memory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: Why Memory Access Matters'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s imagine a toy example with 4 tokens. A naive implementation might build
    a 4×4 attention matrix, compute softmax, and multiply by values. That’s fine for
    4 tokens, but with 1024 tokens, you’d be juggling matrices of a million entries.
    Even if each entry is just 2 bytes (FP16), that’s megabytes of temporary storage
    per step. On a real GPU, constantly moving that in and out of global memory slows
    everything down.
  prefs: []
  type: TYPE_NORMAL
- en: 'FlashAttention says: instead of storing the whole million entries, compute
    them in chunks, normalize them on the fly, and immediately use them to update
    the output. This way, only small temporary blocks live in memory, and global memory
    pressure drops dramatically.'
  prefs: []
  type: TYPE_NORMAL
- en: How It Shows Up in GPT-2 Training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When GPT-2 processes a batch of sequences, each block of the model applies attention.
    In the CUDA version of *llm.c*, these attention calls can be routed through cuDNN
    FlashAttention. Practically, this means that the inner loop of training—the part
    that would otherwise grind on those giant attention matrices—becomes leaner and
    faster.
  prefs: []
  type: TYPE_NORMAL
- en: This matters even more as models grow. For GPT-2 124M (12 layers, 12 heads,
    1024 sequence length), attention is already expensive. For GPT-2 1.5B or LLaMA-style
    models with longer contexts, FlashAttention can be the difference between feasible
    training and “out of memory” errors.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Attention is the defining operation of Transformers, but it’s also their Achilles
    heel. FlashAttention addresses the biggest inefficiency—memory bandwidth—without
    changing the model’s outputs. By using cuDNN’s optimized kernels, *llm.c* ensures
    it runs close to hardware peak performance while still producing correct results.
    For anyone learning about deep learning systems, this is a perfect example of
    how algorithmic innovations (streaming softmax) and hardware-level optimizations
    (Tensor Cores, tiling) combine to make state-of-the-art training practical.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Run GPT-2 training in *llm.c* with `USE_CUDNN=0` and then with `USE_CUDNN=1`.
    Compare training speed and GPU memory usage.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a naive CUDA kernel that builds the full attention matrix, then benchmark
    it against cuDNN FlashAttention.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Vary sequence lengths (128, 512, 1024, 2048) and see how performance diverges
    between naive and FlashAttention implementations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Examine how mixed precision interacts with FlashAttention—try FP16 versus BF16.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explore the FlashAttention paper and compare its algorithmic explanation with
    what you see in practice using *llm.c*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Attention is expensive, but it doesn’t have to be crippling. FlashAttention
    shows that clever algorithm design plus hardware-aware implementation can shrink
    the memory bottleneck dramatically. By leaning on cuDNN’s implementation, *llm.c*
    can train GPT-2 models more efficiently, and learners get a real-world view of
    how deep learning libraries squeeze performance out of GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: '64\. Mixed Precision: FP16/BF16 with Master FP32 Weights'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Training large models like GPT-2 involves multiplying and adding enormous amounts
    of numbers—billions of operations in every training step. GPUs can do this very
    quickly, but the type of numbers you use matters a lot. Traditionally, training
    is done in 32-bit floating point (FP32), which gives good precision but is heavy
    on memory and compute. Modern GPUs offer special hardware—Tensor Cores—that run
    much faster when using reduced precision, such as FP16 (half-precision floating
    point) or BF16 (bfloat16). This technique is called mixed precision training.
  prefs: []
  type: TYPE_NORMAL
- en: Why Mixed Precision Helps
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Using FP16 or BF16 has two main benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Speed: GPUs can perform more FP16/BF16 operations per clock cycle than FP32\.
    For example, NVIDIA Tensor Cores are specifically designed to accelerate half-precision
    math, often delivering 2× or more throughput.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Memory: FP16/BF16 values take half the storage of FP32\. That means you can
    fit larger batches or longer sequences into the same GPU memory, which is critical
    for scaling models.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'But reduced precision comes with a tradeoff: it’s easier for numbers to underflow
    (become zero) or overflow (become infinity), which can destabilize training.'
  prefs: []
  type: TYPE_NORMAL
- en: Master Weights in FP32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The trick used in *llm.c* (and also in PyTorch and TensorFlow) is to keep a
    master copy of weights in FP32\. Here’s the process:'
  prefs: []
  type: TYPE_NORMAL
- en: During the forward pass, weights are cast to FP16/BF16 so the GPU can run the
    math on Tensor Cores.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The gradients are computed in reduced precision as well.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When it’s time to update parameters, the optimizer applies updates to the FP32
    master copy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The updated master weights are cast back to FP16/BF16 for the next forward pass.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This way, you get the speed and memory savings of mixed precision without fully
    losing the stability of FP32.
  prefs: []
  type: TYPE_NORMAL
- en: FP16 vs. BF16
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Both FP16 and BF16 use 16 bits, but they split the bits differently:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Format | Exponent Bits | Mantissa Bits | Range | Precision |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| FP16 | 5 | 10 | Smaller | Higher precision for small numbers |'
  prefs: []
  type: TYPE_TB
- en: '| BF16 | 8 | 7 | Wider | Rougher precision, better range |'
  prefs: []
  type: TYPE_TB
- en: FP16 has better precision near zero but a narrower range, so it’s more prone
    to overflow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BF16 has the same exponent size as FP32, giving it a much wider range but less
    precision.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modern NVIDIA GPUs (Ampere, Hopper) support both, but BF16 is often preferred
    for stability in very large models.
  prefs: []
  type: TYPE_NORMAL
- en: Example in Practice
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Imagine training GPT-2 with a sequence length of 1024 and batch size of 32\.
    With FP32, the activations might take ~12 GB of GPU memory. Switching to FP16
    halves that to ~6 GB, leaving room for larger models or more sequences.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *llm.c*, enabling mixed precision means the forward pass can look something
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Cast embeddings, weights, and activations to FP16/BF16.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run matrix multiplications on Tensor Cores (very fast).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute gradients in reduced precision.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert gradients back to FP32 for stable updates.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This flow is invisible to the high-level code but handled internally in CUDA/cuBLAS/cuDNN
    calls.
  prefs: []
  type: TYPE_NORMAL
- en: Common Challenges
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Mixed precision introduces new wrinkles:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Loss scaling: small gradients may underflow to zero in FP16\. The solution
    is to multiply the loss by a large factor during backpropagation, then divide
    gradients back later. This preserves information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Debugging: NaNs and Infs become more common when switching to FP16\. Careful
    monitoring is required to catch these early.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Performance tuning: Not all operations benefit equally from FP16\. For example,
    reductions (like summing a large array) may lose too much precision unless done
    in FP32.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Mixed precision is one of the key reasons modern Transformers can be trained
    efficiently on today’s hardware. Without it, many models would require double
    the GPU memory and much more time to train. By combining FP16/BF16 for speed and
    memory efficiency with FP32 master weights for stability, *llm.c* mirrors the
    strategy used in production frameworks. This shows how even a minimalist codebase
    can teach the cutting-edge tricks that power real-world large-scale training.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Train GPT-2 in *llm.c* with FP32 only, then repeat with FP16\. Compare memory
    usage (`nvidia-smi`) and runtime per step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Experiment with FP16 vs. BF16 if your GPU supports both. Observe whether one
    is more stable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Intentionally remove the FP32 master weights (update parameters in FP16 only)
    and see how quickly training diverges.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot validation loss curves with FP32, FP16, and BF16 runs to see if the model
    quality differs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try scaling the batch size up with FP16 and note how much bigger a model you
    can fit into the same GPU.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Mixed precision combines the best of both worlds: the speed and memory efficiency
    of FP16/BF16 with the stability of FP32\. This technique has become a standard
    in deep learning, and *llm.c* demonstrates it in a clear, accessible way. It’s
    not just a neat optimization—it’s what makes training large language models on
    modern GPUs feasible at all.'
  prefs: []
  type: TYPE_NORMAL
- en: 65\. Loss Scaling in Mixed Precision Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When training in mixed precision (FP16 or BF16), one of the biggest challenges
    is numerical underflow. Gradients can become so small that they round down to
    zero when represented in 16-bit format. If that happens too often, the optimizer
    stops receiving meaningful updates, and training can stagnate or collapse. To
    address this, frameworks introduce a technique called loss scaling.
  prefs: []
  type: TYPE_NORMAL
- en: The Core Idea
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Loss scaling works by multiplying the loss value by a constant factor (called
    the scale factor) before starting backpropagation. Since gradients are proportional
    to the loss, this also multiplies all gradients by the same factor, making them
    larger and less likely to underflow when stored in FP16.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of backpropagation, the gradients are divided by the same scale factor,
    restoring their correct values before the optimizer step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically:'
  prefs: []
  type: TYPE_NORMAL
- en: '`scaled_loss = loss × scale`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute gradients of `scaled_loss` → produces `scaled_gradients`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`true_gradients = scaled_gradients ÷ scale`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The optimizer then uses `true_gradients` to update the weights.
  prefs: []
  type: TYPE_NORMAL
- en: Static vs. Dynamic Scaling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are two common approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Static scaling: Use a fixed scale factor throughout training. For example,
    always multiply the loss by `1024`. This is simple but risky; if the scale is
    too high, gradients may overflow to infinity. If it’s too low, underflow still
    happens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dynamic scaling: Adjust the scale factor on the fly. If overflows (NaNs or
    Infs) are detected, the scale factor is reduced. If training proceeds smoothly,
    the scale factor is gradually increased. This balances stability and efficiency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In practice, dynamic scaling is the standard. Libraries like PyTorch’s `GradScaler`
    automatically handle this logic, so users don’t have to tweak values manually.
  prefs: []
  type: TYPE_NORMAL
- en: How It Appears in *llm.c*
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The minimal design of *llm.c* doesn’t yet include automatic loss scaling, but
    the idea fits neatly into its training loop. Before calling `gpt2_backward`, you
    would scale the loss. After gradients are computed, you would unscale them before
    `gpt2_update`. Conceptually:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE143]'
  prefs: []
  type: TYPE_PRE
- en: This is not yet in the repository, but it’s how one could extend *llm.c* to
    support stable FP16 training.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Without loss scaling, mixed precision can fail silently. Training might appear
    to run, but the gradients may be effectively zero for many parameters. This wastes
    GPU time and produces poor results. With loss scaling, FP16/BF16 training becomes
    both fast and reliable, combining the hardware speedups with numerical stability.
  prefs: []
  type: TYPE_NORMAL
- en: Example Scenario
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Suppose you are training GPT-2 with FP16 and notice that the validation loss
    barely decreases after several hundred steps. One possible reason is gradient
    underflow. By enabling loss scaling with a scale factor of 512 or 1024, you might
    suddenly see the loss curve behave normally again, matching the FP32 baseline.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Train with FP16 but without loss scaling. Monitor whether the loss decreases
    meaningfully.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a static scale factor (like 512) and rerun. Observe improvements in stability.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Implement a simple dynamic scaler: start with 128, double it if no NaNs appear
    for 100 steps, halve it if NaNs are detected.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare training curves (FP32 vs. FP16 with and without scaling) to see the
    effect.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Experiment with very large scale factors to trigger overflow intentionally,
    then watch how dynamic scaling recovers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Loss scaling is the hidden ingredient that makes mixed precision training practical.
    By rescaling the loss and gradients, we protect tiny numbers from disappearing
    in FP16 while still enjoying the massive performance and memory benefits. Even
    in a minimal codebase like *llm.c*, understanding loss scaling bridges the gap
    between a model that trains poorly and one that matches FP32 performance at half
    the cost.
  prefs: []
  type: TYPE_NORMAL
- en: 66\. Activation Checkpointing and Memory Tradeoffs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Training deep networks like GPT-2 involves storing a large number of activations—the
    intermediate outputs produced at every layer during the forward pass. These activations
    are needed later in the backward pass to compute gradients. The problem is that
    they take up a huge amount of GPU memory. For a 12-layer GPT-2 with long sequences
    and large batch sizes, activations can consume more memory than the model weights
    themselves.
  prefs: []
  type: TYPE_NORMAL
- en: Why Activations Matter
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s say you have a batch size of 8, sequence length of 1024, hidden size of
    768, and 12 layers. Each layer produces an activation tensor of shape `(batch_size,
    sequence_length, hidden_size)`, or `8 × 1024 × 768`. That’s about 6.3 million
    numbers per layer. Multiply by 12 layers, and you have ~75 million numbers. At
    FP16, that’s around 150 MB per forward pass just for storing activations, and
    this grows with larger models.
  prefs: []
  type: TYPE_NORMAL
- en: If you scale up to GPT-2 Medium or GPT-2 XL, this number balloons quickly into
    gigabytes, which may not fit in GPU memory.
  prefs: []
  type: TYPE_NORMAL
- en: The Idea of Checkpointing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Activation checkpointing offers a tradeoff: instead of storing all activations,
    you only keep a small subset (the checkpoints) and recompute the rest during the
    backward pass.'
  prefs: []
  type: TYPE_NORMAL
- en: 'During forward pass: save only checkpoints (for example, the activations at
    the end of each transformer block).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'During backward pass: when gradients for a layer are needed, recompute the
    missing activations by running part of the forward pass again.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This saves memory at the cost of extra computation.
  prefs: []
  type: TYPE_NORMAL
- en: How It Works in GPT-2
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A GPT-2 block has multiple steps: embedding lookup, attention, MLP, layer norm,
    residual connections. Normally, you’d store every output tensor. With checkpointing,
    you might only store the input to each block and discard the intermediate results.
    When backpropagation reaches that block, you rerun the forward pass locally to
    regenerate those results, then compute gradients.'
  prefs: []
  type: TYPE_NORMAL
- en: This reduces memory usage almost linearly with the number of discarded activations,
    at the cost of roughly 30–40% more compute.
  prefs: []
  type: TYPE_NORMAL
- en: In *llm.c* Context
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*llm.c* doesn’t yet include activation checkpointing in its minimal implementation,
    but it’s a natural extension. In CUDA, this might be implemented by wrapping blocks
    of code with a “checkpoint” function that decides whether to save or discard activations.
    In PyTorch, the equivalent is `torch.utils.checkpoint`.'
  prefs: []
  type: TYPE_NORMAL
- en: If you train with longer sequences (e.g., 2048 tokens instead of 1024), checkpointing
    could mean the difference between fitting in memory or running into out-of-memory
    (OOM) errors.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Modern GPUs have enormous compute capacity, but memory remains the bottleneck.
    Checkpointing shifts the tradeoff: you spend a bit more compute (re-running some
    forward passes) in exchange for freeing up gigabytes of memory. This lets you:'
  prefs: []
  type: TYPE_NORMAL
- en: Train larger models on the same hardware.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use longer sequence lengths for better context handling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increase batch size for more stable gradients.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In practice, this technique is used in nearly every large-scale Transformer
    training run today.
  prefs: []
  type: TYPE_NORMAL
- en: Example Analogy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Think of it like studying for an exam. You could take detailed notes on every
    page of the textbook (storing all activations), but your notebook would get huge.
    Alternatively, you could only mark the chapter headings (checkpoints) and re-read
    sections when you need them during review (recomputation). It takes more time,
    but saves notebook space.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Run training on a GPU with long sequences until you hit an out-of-memory error.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement a simple checkpointing scheme where you only store activations every
    other layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Measure how much memory usage decreases (using `nvidia-smi`) and how much runtime
    increases per step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Experiment with different checkpointing frequencies—every layer, every 2 layers,
    every 4 layers—and find the balance between memory savings and compute overhead.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare validation loss curves to confirm that checkpointing does not affect
    training quality (only runtime).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Activation checkpointing is a clever strategy to bend the memory limits of GPUs.
    By discarding and recomputing activations on demand, you can fit models or sequence
    lengths that would otherwise be impossible. The tradeoff is extra computation,
    but with today’s hardware, compute is usually cheaper than memory. This technique
    is one of the quiet enablers behind scaling Transformers to billions of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '67\. GPU Memory Planning: Parameters, Gradients, States'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When training a GPT-2 model on GPU, one of the most important practical challenges
    is managing memory. Every tensor—the model’s parameters, gradients, optimizer
    state, and activations—must fit into limited GPU RAM. Unlike CPUs, where you can
    often rely on swap space or large RAM pools, GPU memory is tight and unforgiving.
    If you exceed the limit, the program crashes with an out-of-memory error.
  prefs: []
  type: TYPE_NORMAL
- en: Breaking Down What Takes Memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Parameters (Weights) These are the trainable values of the model: embeddings,
    attention projections, MLP weights, and so on. For GPT-2 124M, there are about
    124 million parameters.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In FP32, that’s roughly 500 MB.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In FP16/BF16, it’s about 250 MB. Larger GPT-2 models (355M, 774M, 1.5B) scale
    this up proportionally.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradients For each parameter, the backward pass produces a gradient tensor of
    the same size. If parameters take 500 MB, gradients also take ~500 MB in FP32\.
    Mixed precision can halve this.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Optimizer States Optimizers like AdamW don’t just store gradients—they also
    track moving averages (`m` and `v`). Each adds another full-sized tensor. With
    AdamW, you often end up with 3× the parameter size: weights + m + v.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Activations During the forward pass, every layer’s intermediate outputs must
    be stored for the backward pass. This is often the largest single consumer of
    memory. For a 12-layer GPT-2 with sequence length 1024 and batch size 8, activations
    can easily exceed several GB. Checkpointing (as discussed earlier) helps reduce
    this.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A Simple Calculation Example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For GPT-2 124M in FP32:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters: ~500 MB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gradients: ~500 MB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'AdamW states: ~1 GB (two copies)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Activations: 2–4 GB (depends on batch size and sequence length)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Total: ~3–6 GB, which fits on most modern GPUs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For GPT-2 774M in FP32:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters: ~3 GB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gradients: ~3 GB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'AdamW states: ~6 GB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Activations: 8–12 GB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Total: ~20+ GB—too large for many GPUs unless you use tricks like FP16 and
    checkpointing.'
  prefs: []
  type: TYPE_NORMAL
- en: Strategies for Memory Efficiency
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Mixed Precision Using FP16/BF16 cuts parameter, gradient, and optimizer sizes
    in half. Instead of 20 GB, you may get by with ~10 GB.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Activation Checkpointing Store fewer activations and recompute them during backpropagation.
    This often saves multiple GB.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gradient Accumulation Instead of training with a huge batch at once, split it
    into smaller micro-batches and accumulate gradients across them. This reduces
    activation memory requirements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Parameter Sharding (Advanced) In multi-GPU setups, parameters and optimizer
    states can be split across devices (e.g., ZeRO optimizer in DeepSpeed). While
    not in *llm.c*, it’s a common technique at scale.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In *llm.c*
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `GPT2` struct organizes memory into fields like `params_memory`, `grads_memory`,
    `m_memory`, and `v_memory`. These are allocated as flat arrays, making it easy
    to calculate their size. This minimal design highlights the reality: for every
    parameter, there’s at least one matching gradient and potentially two optimizer
    state values.'
  prefs: []
  type: TYPE_NORMAL
- en: This structure mirrors how full frameworks like PyTorch allocate memory, but
    *llm.c* exposes it transparently so you can see exactly what’s taking up space.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When training models, memory is usually the first limit you hit, not compute.
    Even if your GPU is powerful enough to handle the math, if you run out of memory,
    you can’t proceed. Understanding how parameters, gradients, optimizer states,
    and activations interact helps you design training runs that actually fit.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Calculate memory usage for GPT-2 124M, 355M, and 774M using FP32 vs FP16\. Compare
    your numbers to your GPU’s memory size.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run *llm.c* with increasing batch sizes until you hit an out-of-memory error.
    Record the exact point where it breaks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enable mixed precision and checkpointing to see how much further you can push
    sequence length or batch size.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a script to print the sizes of `params_memory`, `grads_memory`, and optimizer
    states in *llm.c*. Compare this to `nvidia-smi` output during training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Experiment with reducing optimizer states (e.g., try SGD instead of AdamW) to
    see the memory difference.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Training Transformers is not just about writing the forward and backward passes—it’s
    about planning memory carefully. Parameters, gradients, optimizer states, and
    activations all compete for limited GPU RAM. By understanding these categories
    and using techniques like mixed precision and checkpointing, you can fit bigger
    models or longer contexts on the same hardware. This balance between memory and
    compute is at the heart of scaling modern deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: 68\. Kernel Launch Configurations and Occupancy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When writing CUDA code for training models like GPT-2, one of the most important
    yet subtle factors in performance is how kernels are launched. A kernel is just
    a function that runs on the GPU, but the way you configure it—the number of threads,
    blocks, and how work is divided—can make the difference between a GPU that runs
    at 10% efficiency and one that’s near peak utilization.
  prefs: []
  type: TYPE_NORMAL
- en: Threads, Blocks, and Grids
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'CUDA organizes computation hierarchically:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thread: the smallest unit of execution. Each thread runs the kernel code once.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Block: a collection of threads that share fast, on-chip memory (called shared
    memory).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Grid: a collection of blocks. Together, the grid represents the entire kernel
    launch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When you launch a kernel, you decide:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE144]'
  prefs: []
  type: TYPE_PRE
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE145]'
  prefs: []
  type: TYPE_PRE
- en: Here, `N` might be the total number of elements to process. The division ensures
    that all elements get covered.
  prefs: []
  type: TYPE_NORMAL
- en: What Is Occupancy?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Occupancy refers to how many threads are active on a GPU relative to the maximum
    possible. Higher occupancy usually means the GPU is better utilized, but it’s
    not the only factor—memory access patterns and instruction throughput also matter.
  prefs: []
  type: TYPE_NORMAL
- en: Each GPU has a fixed number of Streaming Multiprocessors (SMs), and each SM
    can support only a certain number of threads and blocks at once. If your kernel
    launch doesn’t provide enough threads, the GPU will be underutilized. If you launch
    too many, they may compete for shared memory and registers, leading to inefficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: A Simple Vector Add'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Suppose you want to add two arrays of size `N = 1e6`.
  prefs: []
  type: TYPE_NORMAL
- en: If you use `threads_per_block = 32`, you’ll have many tiny blocks. This wastes
    parallelism because modern GPUs are designed to run hundreds of threads per SM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you use `threads_per_block = 1024`, you may hit the hardware limit but run
    very large blocks that restrict scheduling flexibility.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A good balance might be 256 or 512 threads per block, which lets the GPU overlap
    computation and memory access effectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Transformer Training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For GPT-2 in *llm.c*, most heavy lifting is done by:'
  prefs: []
  type: TYPE_NORMAL
- en: Matrix multiplications (handled by cuBLAS/cuBLASLt). These libraries pick kernel
    launch parameters automatically.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attention and normalization kernels (custom or cuDNN). When written by hand,
    launch configuration becomes crucial.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, in a softmax kernel over a sequence of 1024 tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: You might launch one block per sequence row, with 1024 threads per block (one
    per token).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alternatively, you might launch multiple blocks per row, each handling a tile
    of tokens, if shared memory limits require it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing wisely can double or triple performance.
  prefs: []
  type: TYPE_NORMAL
- en: Balancing Factors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When configuring kernels, you balance:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Occupancy: Are enough threads active to use the GPU fully?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Memory Coalescing: Do threads access memory in aligned, sequential chunks (which
    is fast) or scattered patterns (which is slow)?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Shared Memory and Registers: Each block has limited resources. If your kernel
    uses too much shared memory, fewer blocks can fit per SM, reducing occupancy.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Arithmetic Intensity: If the kernel does a lot of math per memory load, occupancy
    matters less; if it’s memory-bound, occupancy matters more.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In GPU training, kernel launch decisions directly control how efficiently hardware
    is used. Two kernels implementing the same math can differ by 5–10× in runtime
    purely because of launch configuration. cuBLAS and cuDNN automate much of this
    for matrix-heavy ops, but understanding it is crucial when writing custom kernels.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Write a simple CUDA kernel for vector addition with different `threads_per_block`
    values (32, 128, 256, 512, 1024). Measure runtime and see which is fastest.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use `nvprof` or `nsys` to inspect occupancy of kernels during GPT-2 training.
    Note which kernels run at <50% occupancy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Experiment with a softmax kernel: launch one block per row vs. multiple blocks
    per row. Compare performance and memory use.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explore how shared memory allocation per block affects occupancy by artificially
    increasing shared memory usage.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare cuBLAS GEMM (matrix multiply) performance to a naive CUDA implementation
    and observe how kernel configuration explains the speed difference.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Kernel launch configuration is the hidden lever of GPU performance. By adjusting
    how threads and blocks are assigned, you control how much of the GPU is kept busy,
    how well memory bandwidth is used, and how smoothly computations flow. For models
    like GPT-2, libraries handle most kernels, but knowing what’s happening under
    the hood is key to writing or debugging efficient CUDA code.
  prefs: []
  type: TYPE_NORMAL
- en: 69\. CUDA Error Handling and Debugging
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When writing or running CUDA code, one of the most frustrating parts is that
    errors often don’t show up immediately. Unlike CPU code, where an invalid pointer
    or division by zero can crash right away, CUDA launches kernels asynchronously.
    This means the host code (running on the CPU) queues up GPU work and moves on,
    while the GPU processes it in the background. If something goes wrong inside the
    kernel, the error might not be visible until later—sometimes only after you try
    to synchronize.
  prefs: []
  type: TYPE_NORMAL
- en: Common Error Sources
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Out-of-bounds memory access A kernel thread tries to read or write past the
    end of an array. This can silently produce incorrect results or crash the program.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Invalid memory alignment Some CUDA operations require pointers to be aligned.
    Misaligned access can degrade performance or trigger errors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Illegal instruction or unsupported hardware feature Using Tensor Cores on an
    older GPU, or using instructions not supported by your GPU’s compute capability,
    can fail.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Out of memory (OOM) Allocating more GPU memory than available causes runtime
    errors. Unlike CPU memory, GPUs cannot “swap” to disk.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Race conditions Threads within a block or across blocks accessing the same memory
    location without synchronization can corrupt results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How CUDA Reports Errors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Every CUDA runtime API call returns an error code. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE146]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, after launching a kernel, you should check:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE147]'
  prefs: []
  type: TYPE_PRE
- en: This pattern ensures that if something fails, you see it quickly instead of
    later.
  prefs: []
  type: TYPE_NORMAL
- en: Debugging Tools
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: cuda-gdb A GPU-aware debugger. Lets you step through CUDA kernels much like
    gdb on CPU code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: cuda-memcheck Detects out-of-bounds accesses, race conditions, and misaligned
    memory operations. Essential when kernels produce “mysterious” wrong outputs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Nsight Systems / Nsight Compute Profiling tools that show kernel timelines,
    occupancy, memory throughput, and errors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sanity checks in code Often, simply inserting assertions (`assert(i < N)`) or
    zero-initializing memory can catch problems earlier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Debugging in *llm.c*
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In *llm.c*, most of the CUDA-heavy lifting is handled by cuBLAS and cuDNN. But
    when experimenting with custom kernels (e.g., softmax, masking, or layernorm),
    debugging becomes crucial. A small indexing mistake could make training diverge
    or crash with `nan` losses. By adding `cudaGetLastError()` checks after every
    kernel launch, you can catch issues right where they happen.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: A Softmax Bug'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Imagine a kernel computing softmax across 1024 tokens per row. If one thread
    index accidentally runs past 1024, it may read garbage memory. Without error checking,
    you might just see “loss is NaN” 100 steps later. With `cuda-memcheck`, you’d
    immediately see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE148]'
  prefs: []
  type: TYPE_PRE
- en: Now you know exactly where to fix the bug.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Training large models is expensive. A single bug in a CUDA kernel can waste
    hours of GPU time, produce invalid gradients, or silently corrupt weights. Robust
    error handling and debugging practices save not only frustration but also significant
    cost.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Write a CUDA kernel with an intentional bug (e.g., forget to check array bounds).
    Run it with and without `cuda-memcheck` to see the difference.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add `cudaGetLastError()` after every kernel in a simple project and watch how
    it pinpoints issues earlier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Experiment with Nsight Systems: run GPT-2 training and inspect kernel launches,
    checking for errors or unexpected stalls.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train with bad initialization (e.g., NaNs in inputs) and see how error checking
    reports failures.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Introduce a race condition by having two threads update the same memory without
    `__syncthreads()`. Debug using `cuda-memcheck`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: CUDA’s asynchronous nature makes error handling less straightforward than CPU
    programming. But with the right tools—error codes, synchronization, cuda-memcheck,
    and debuggers—you can systematically catch and fix problems. In *llm.c*, this
    discipline ensures that CUDA kernels not only run fast but also run correctly,
    which is just as important when training large-scale models.
  prefs: []
  type: TYPE_NORMAL
- en: '70\. `dev/cuda/`: From Simple Kernels to High Performance'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Inside the *llm.c* repository, there is a folder called `dev/cuda/`. At first
    glance it may look like a side experiment, but it’s actually one of the most instructive
    parts of the project. The main training files (`train_gpt2.cu`, `train_gpt2_fp32.cu`)
    rely heavily on cuBLAS and cuDNN—optimized libraries that already deliver near-peak
    performance. But if you want to understand how CUDA really works under the hood,
    you have to look at how kernels are written from scratch, and that’s exactly what
    this folder shows.
  prefs: []
  type: TYPE_NORMAL
- en: Why This Folder Exists
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The goal of `dev/cuda/` is not to replace cuBLAS or cuDNN. Instead, it acts
    as a sandbox for:'
  prefs: []
  type: TYPE_NORMAL
- en: Building intuition about how GPU kernels are structured.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experimenting with small-scale implementations of operations like vector addition,
    matrix multiplication, or normalization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing naive CUDA implementations to highly optimized library calls.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Teaching developers how memory layout, thread synchronization, and shared memory
    affect performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It’s a bridge: start simple with “hello world” style kernels, then step closer
    to the performance tricks used by NVIDIA’s professional libraries.'
  prefs: []
  type: TYPE_NORMAL
- en: A Journey from Naive to Optimized
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Simple Elementwise Kernels The first step is usually a kernel where each thread
    processes one element. For example, adding two vectors `C[i] = A[i] + B[i]`. This
    teaches indexing, memory coalescing, and the idea of grids and blocks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reduction Kernels Next, you move to slightly harder tasks like summing an array.
    Now you need thread cooperation and synchronization (`__syncthreads()`), plus
    shared memory usage.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Matrix Multiplication (GEMM) A naive kernel might have each thread compute
    one output element by looping over the input dimension. It works, but is slow
    because it reloads data from global memory repeatedly. The optimized version uses
    tiling: load a tile of the matrix into shared memory, let threads reuse it many
    times, and then move to the next tile. This can speed up performance by 10× or
    more.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Advanced Optimizations Later examples may add warp-level primitives, vectorized
    loads, and Tensor Core usage. These bring performance closer to cuBLAS.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Educational Value
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Seeing these steps side by side makes the performance story very tangible:'
  prefs: []
  type: TYPE_NORMAL
- en: A naive GEMM kernel might achieve 1% of cuBLAS speed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A tiled shared-memory GEMM can jump to 30–40%.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With careful warp scheduling, it can reach 60–70%.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: cuBLAS goes further with hand-tuned assembly and Tensor Cores, pushing 90–95%.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This teaches that optimization is not magic—it’s a sequence of logical improvements,
    each shaving off inefficiencies.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters for GPT-2 Training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Even if you never plan to reimplement matrix multiplication yourself, understanding
    what happens in `dev/cuda/` helps explain why the main training loop in `train_gpt2.cu`
    is so fast. You see why cuBLAS/cuDNN kernels are black boxes of efficiency: because
    writing your own at that level is extremely hard.'
  prefs: []
  type: TYPE_NORMAL
- en: But this also means you’re better prepared to write custom kernels when you
    need them. For example, maybe you want to test a new activation function or a
    different attention mechanism. By borrowing patterns from the experimental kernels,
    you can build your own, test them, and compare to baselines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Vector Add Kernel'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here’s a simple kernel you might find in this folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE149]'
  prefs: []
  type: TYPE_PRE
- en: It’s trivial compared to GPT-2’s attention, but this is where everyone starts.
    From here you scale up to 2D indexing for matrices, then to tiled shared-memory
    patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Run a kernel from `dev/cuda/` that does naive matrix multiplication. Compare
    its runtime to cuBLAS for the same dimensions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modify the naive GEMM to use tiling with shared memory. Measure how performance
    improves.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Inspect PTX (the intermediate assembly) generated by NVCC for a simple kernel.
    Observe how memory loads are translated.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add timing code around kernels to see how much performance scales with different
    block sizes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement a new custom kernel (e.g., ReLU activation) and compare its speed
    to applying ReLU via cuDNN.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `dev/cuda/` folder is not about production training. It’s about learning
    and experimenting. It starts with the simplest CUDA kernels and builds up to performance-conscious
    designs. This progression mirrors how professional libraries achieve their speed.
    By studying and experimenting here, you gain a deeper appreciation of what it
    takes to make GPUs run at full tilt—and you gain the skills to write your own
    kernels when the libraries don’t provide what you need.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 8\. Multi-GPU and Multi-node training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 71\. Data Parallelism in *llm.c*
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When you want to train a large model, a single GPU often isn’t enough. Either
    the model doesn’t fit in memory, or the training takes too long. One of the simplest
    and most widely used ways to scale training across multiple GPUs is data parallelism.
    The idea is conceptually simple: instead of giving all the training data to one
    GPU, you split it into smaller batches, send each GPU a piece, let them process
    it independently, and then combine their results.'
  prefs: []
  type: TYPE_NORMAL
- en: The Core Idea
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Imagine you have a batch of 128 sequences and 4 GPUs. In data parallelism:'
  prefs: []
  type: TYPE_NORMAL
- en: GPU 0 sees sequences 0–31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPU 1 sees sequences 32–63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPU 2 sees sequences 64–95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPU 3 sees sequences 96–127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each GPU runs the forward pass, computes the loss, and calculates gradients
    for its slice. At the end of the step, the gradients are averaged across GPUs,
    ensuring that all models stay synchronized. Every GPU holds a full copy of the
    model parameters, so they are always consistent after gradient averaging.
  prefs: []
  type: TYPE_NORMAL
- en: In *llm.c*
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The *llm.c* repository keeps things minimal, so there isn’t a full-fledged
    DeepSpeed or PyTorch DDP implementation. But the same principle applies:'
  prefs: []
  type: TYPE_NORMAL
- en: Each GPU gets a copy of the GPT-2 model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The batch is split across devices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After the backward pass, gradients from all GPUs must be synchronized.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This synchronization is usually done using NCCL all-reduce (covered in the next
    section), but the design remains data parallel at heart.
  prefs: []
  type: TYPE_NORMAL
- en: Why Data Parallelism Works
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The forward and backward passes are embarrassingly parallel across different
    data samples. A token in sequence A doesn’t need to know about a token in sequence
    B when computing gradients. As long as all GPUs agree on parameter updates after
    each step, splitting the batch is perfectly valid.
  prefs: []
  type: TYPE_NORMAL
- en: Example Walkthrough
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s say we’re training GPT-2 on TinyStories with batch size `B = 32` and sequence
    length `T = 64`.
  prefs: []
  type: TYPE_NORMAL
- en: On a single GPU, the forward pass computes embeddings, attention, MLP, and loss
    for all 32 sequences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With 2 GPUs, we set `B = 16` on each. Each GPU processes 16 sequences in parallel.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After backpropagation, both GPUs hold gradients for their half of the batch.
    Before applying the optimizer, the gradients are averaged so that the weight update
    is equivalent to training with the full batch of 32.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From the model’s perspective, it’s as if nothing changed—it just sees gradients
    from the whole batch.
  prefs: []
  type: TYPE_NORMAL
- en: Memory and Speed Benefits
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Memory: Each GPU stores only the activations for its local batch. This reduces
    per-GPU memory use, making it possible to train with larger global batches.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Speed: Training steps finish faster because multiple GPUs share the work. For
    example, doubling the number of GPUs often cuts training time per step nearly
    in half, though communication overhead prevents perfect scaling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Communication Overhead Synchronizing gradients across GPUs can become expensive,
    especially with large models or when running across multiple nodes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I/O Bottlenecks Feeding data to multiple GPUs fast enough requires efficient
    dataloaders and prefetching.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Optimizer State Replication With AdamW, each GPU also needs to store optimizer
    states (m and v). This means memory scales with the number of GPUs instead of
    shrinking.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Data parallelism is the workhorse of deep learning scaling. It’s conceptually
    easy to understand, straightforward to implement, and works well even for large
    models. In practice, nearly all large-scale GPT training begins with data parallelism,
    often enhanced by techniques like gradient accumulation or mixed precision.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Train GPT-2 in *llm.c* on a single GPU, then split the batch across two GPUs
    using `CUDA_VISIBLE_DEVICES`. Compare throughput and loss curves.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Experiment with increasing global batch size while keeping per-GPU batch size
    fixed. Notice how validation loss behaves.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Simulate gradient averaging by writing a simple script that averages arrays
    from two processes. Connect this idea back to how NCCL all-reduce works.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Measure the difference in memory usage per GPU when training with 1 vs 2 GPUs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run a small experiment with different numbers of GPUs (1, 2, 4) and plot how
    training time per step changes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Data parallelism splits the workload across GPUs by dividing the batch. Each
    GPU trains a full model replica on part of the data, then synchronizes gradients
    so that updates are consistent. It’s simple but powerful, forming the foundation
    of scaling strategies in *llm.c* and in most deep learning frameworks. Without
    it, training GPT-2 and larger models on modern datasets would be impractical.
  prefs: []
  type: TYPE_NORMAL
- en: 72\. MPI Process Model and GPU Affinity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When you scale training beyond a single GPU, you need a way to manage multiple
    processes and devices. In the *llm.c* codebase, the minimalist approach relies
    on MPI (Message Passing Interface), a library that has been around for decades
    in high-performance computing. MPI provides a simple abstraction: you launch multiple
    processes, each assigned a rank (an ID number), and they can communicate with
    each other by sending and receiving messages.'
  prefs: []
  type: TYPE_NORMAL
- en: In distributed deep learning, MPI typically works alongside NCCL (NVIDIA Collective
    Communications Library). MPI handles process management—spawning workers, assigning
    GPUs, setting up environment variables—while NCCL handles the actual gradient
    synchronization.
  prefs: []
  type: TYPE_NORMAL
- en: MPI Processes and Ranks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Suppose you want to train on 4 GPUs. MPI will start 4 processes. Each process:'
  prefs: []
  type: TYPE_NORMAL
- en: Loads the same GPT-2 model code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Initializes CUDA on one GPU.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reads a shard of the training data or the same dataset, depending on setup.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each process gets a rank:'
  prefs: []
  type: TYPE_NORMAL
- en: Rank 0 → GPU 0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rank 1 → GPU 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rank 2 → GPU 2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rank 3 → GPU 3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ranks are important because they determine roles. For example, rank 0 often
    acts as the “master,” printing logs or handling checkpoints, while the others
    focus purely on computation.
  prefs: []
  type: TYPE_NORMAL
- en: GPU Affinity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If you don’t explicitly map processes to GPUs, they can all try to use the same
    device. That leads to oversubscription—multiple processes fighting for one GPU
    while the others sit idle. To prevent this, you set GPU affinity.
  prefs: []
  type: TYPE_NORMAL
- en: 'The environment variable `CUDA_VISIBLE_DEVICES` is the simplest way to do this.
    For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE150]'
  prefs: []
  type: TYPE_PRE
- en: MPI automatically assigns process 0 to GPU 0, process 1 to GPU 1, and so on.
    Inside the code, you can confirm this by calling `cudaSetDevice(rank)`.
  prefs: []
  type: TYPE_NORMAL
- en: On multi-node clusters, GPU affinity also needs to consider network topology.
    You want each process close to its GPU and ideally aligned with the node’s network
    card for faster NCCL communication.
  prefs: []
  type: TYPE_NORMAL
- en: Synchronization and Communication
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: After each forward and backward pass, each MPI process has its local gradients.
    These must be averaged across processes to keep the model weights consistent.
    MPI itself offers collective operations like `MPI_Allreduce`, but in practice,
    *llm.c* uses NCCL for GPU-to-GPU communication because it is faster and topology-aware.
    MPI sets up the group, NCCL does the heavy lifting.
  prefs: []
  type: TYPE_NORMAL
- en: Example Workflow
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Launch: `mpirun -np 4 ./train_gpt2` starts 4 processes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Initialization: Each process determines its rank and sets its GPU with `cudaSetDevice(rank)`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Training loop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Forward pass on each process’s GPU.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Backward pass to compute gradients.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradients averaged with NCCL All-Reduce.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Update: Every process updates its copy of the weights.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Sync: At the next step, all model replicas are identical.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Debugging GPU Affinity Issues
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If you accidentally misconfigure GPU affinity, symptoms include:'
  prefs: []
  type: TYPE_NORMAL
- en: Two processes trying to use the same GPU → out-of-memory errors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPUs left idle because no process is assigned.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Slowdowns because processes are spread inefficiently across sockets or PCIe
    lanes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A quick way to debug is to print the rank and the GPU ID at startup:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE151]'
  prefs: []
  type: TYPE_PRE
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: MPI and GPU affinity might feel like low-level plumbing, but they are critical
    for scaling. If you don’t get this right, training may run at a fraction of expected
    speed or crash outright. For small setups (2–4 GPUs), it may feel like overkill,
    but for larger clusters with 8, 16, or 64 GPUs, careful mapping is the difference
    between success and wasted compute time.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Train GPT-2 with `mpirun -np 2` and verify each process prints a different GPU
    ID.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Intentionally misconfigure `CUDA_VISIBLE_DEVICES` so both processes map to GPU
    0, then observe the OOM error.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On a multi-GPU machine, experiment with running processes pinned to different
    GPUs. Measure training throughput.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use `nvidia-smi topo -m` to view the PCIe topology of your GPUs. Try to align
    MPI ranks with nearby GPUs for better performance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print the time spent in all-reduce with different mappings to see how GPU affinity
    affects communication overhead.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: MPI is the backbone for managing multiple processes in distributed training,
    and GPU affinity ensures each process has exclusive access to the right device.
    Together, they lay the groundwork for efficient multi-GPU training in *llm.c*.
    Get these details right, and scaling is smooth; get them wrong, and you run into
    memory crashes, idle GPUs, or bottlenecked communication.
  prefs: []
  type: TYPE_NORMAL
- en: 73\. NCCL All-Reduce for Gradient Sync
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once each GPU finishes its forward and backward pass, it has a set of gradients
    that reflect only its portion of the training data. To keep the model parameters
    consistent across all GPUs, these gradients must be synchronized. The standard
    way to do this in modern deep learning systems is with All-Reduce, and NVIDIA’s
    NCCL (NVIDIA Collective Communications Library, pronounced “Nickel”) provides
    the optimized implementation.
  prefs: []
  type: TYPE_NORMAL
- en: What All-Reduce Does
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: All-Reduce is a collective communication operation. Every process (GPU) starts
    with its own local buffer of values—here, the gradients—and the operation combines
    them (using a reduction, usually summation) and distributes the result back to
    all processes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, if GPU 0 has `g0`, GPU 1 has `g1`, GPU 2 has `g2`, and GPU
    3 has `g3`, then after All-Reduce, each GPU has the same result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE152]'
  prefs: []
  type: TYPE_PRE
- en: The division by 4 is optional—it depends whether you want a sum or an average—but
    averaging is common for gradient updates.
  prefs: []
  type: TYPE_NORMAL
- en: This ensures that every GPU applies the same weight update and stays synchronized
    with the others.
  prefs: []
  type: TYPE_NORMAL
- en: Why NCCL?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'While MPI provides an All-Reduce primitive, NCCL is specifically optimized
    for GPUs. It knows about PCIe, NVLink, NVSwitch, and Infiniband topologies and
    arranges communication to maximize bandwidth and minimize latency. Some of its
    key strategies include:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ring All-Reduce: GPUs are arranged in a ring. Each GPU sends its data to the
    next while receiving from the previous, accumulating partial sums as the data
    flows. This scales well with many GPUs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tree All-Reduce: Organizes communication as a tree, reducing depth (latency)
    at the cost of bandwidth.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hybrid schemes: NCCL dynamically chooses strategies depending on GPU count
    and topology.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By exploiting topology awareness, NCCL can saturate available communication
    channels.
  prefs: []
  type: TYPE_NORMAL
- en: Example in *llm.c* Training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the CPU-only training loop, gradients are updated directly without communication.
    In the multi-GPU CUDA path, after backpropagation (`gpt2_backward`), each GPU
    has its local gradients in memory. At this point:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE153]'
  prefs: []
  type: TYPE_PRE
- en: After this call, `model.grads_memory` on every GPU contains the summed gradients
    across all GPUs. Dividing by the number of GPUs turns it into the average.
  prefs: []
  type: TYPE_NORMAL
- en: Why Gradient Sync Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Without gradient synchronization, each GPU would drift apart, updating weights
    independently. This would be equivalent to training multiple smaller models rather
    than one unified model. Synchronization makes sure all replicas behave like a
    single large-batch training job.
  prefs: []
  type: TYPE_NORMAL
- en: Memory and Performance Considerations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Bandwidth-bound: Gradient synchronization often dominates runtime as model
    size grows. For GPT-2 774M, gradients alone can be several GB per step.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Overlapping Communication with Compute: Advanced systems overlap gradient exchange
    with backward computation. While later layers are computing gradients, earlier
    layers are already being synchronized.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Precision: Gradients can be synchronized in FP16/BF16 to cut communication
    bandwidth in half. This is called gradient compression.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Analogy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Think of four chefs cooking the same dish in separate kitchens. Each chef tastes
    their own version and suggests adjustments (gradients). If they don’t talk, their
    recipes diverge. With All-Reduce, the chefs share notes, average their adjustments,
    and apply the same changes—so all four kitchens end up cooking the same dish.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Run training on 2 GPUs with and without gradient synchronization (by commenting
    out All-Reduce). Watch how quickly the models diverge in loss.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use NCCL’s `NCCL_DEBUG=INFO` environment variable to print communication patterns.
    Observe the chosen ring/tree strategies.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Experiment with FP32 vs FP16 gradient synchronization and measure bandwidth
    savings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Profile training with `nsys` or `nvprof` to see how much time is spent in All-Reduce.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scale from 2 GPUs to 4 or 8 and measure how synchronization overhead grows.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: NCCL All-Reduce is the backbone of multi-GPU training in *llm.c*. It ensures
    that gradients computed on separate GPUs are combined into a single, consistent
    update. By leveraging topology-aware algorithms like ring and tree reductions,
    NCCL keeps synchronization efficient even as models and GPU counts scale up. Without
    it, distributed training would produce inconsistent, drifting models rather than
    a unified one.
  prefs: []
  type: TYPE_NORMAL
- en: 74\. Building and Running Multi-GPU Trainers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Getting multiple GPUs to cooperate isn’t automatic—you need to set up the environment,
    initialize communication, and make sure each process knows which GPU to use. In
    *llm.c*, the design is intentionally minimalist, but it still has to integrate
    with MPI (Message Passing Interface) and NCCL to allow training across several
    GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: MPI Launch'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Multi-GPU training starts with MPI. You don’t run the program once; you launch
    it with `mpirun` or `mpiexec`, which spawns one process per GPU. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE154]'
  prefs: []
  type: TYPE_PRE
- en: Here, `-np 4` starts four processes. Each process will attach itself to one
    GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'MPI provides:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Rank: a unique ID for each process (0, 1, 2, 3).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'World size: the total number of processes (here, 4).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each process knows who it is and how many peers it has.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: GPU Assignment'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Once MPI assigns ranks, each process must select a GPU. This is often done
    with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE155]'
  prefs: []
  type: TYPE_PRE
- en: So process 0 gets GPU 0, process 1 gets GPU 1, and so on. Without this step,
    processes might all pile onto the same GPU, leading to chaos.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: NCCL Communicator'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Next, the code creates an NCCL communicator. Think of it as a “conference call”
    between all GPUs. NCCL sets up the communication paths (rings, trees) across devices.
    A typical setup looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE156]'
  prefs: []
  type: TYPE_PRE
- en: 'Here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`world_size` is the number of GPUs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nccl_id` is a shared identifier obtained via MPI (all processes must use the
    same one).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rank` is the local ID.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now the GPUs can talk to each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: Training Loop Integration'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Once communication is established, the training loop doesn’t look dramatically
    different. Each GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: Loads its own batch of data (so the dataset is divided across GPUs).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Runs the forward pass.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Runs the backward pass.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calls NCCL All-Reduce to sync gradients.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Updates parameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The only new ingredient is step 4\. Without it, each GPU would wander off with
    its own gradients.
  prefs: []
  type: TYPE_NORMAL
- en: Example Command
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Suppose you have 2 GPUs on your machine. You can train with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE157]'
  prefs: []
  type: TYPE_PRE
- en: Each GPU trains on 8 sequences of 128 tokens. Combined, it’s like training with
    batch size 16, but split across GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Common Pitfalls
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Forgetting to set device by rank: all processes fight for GPU 0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mismatched NCCL IDs: communicator fails to initialize.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MPI vs NCCL versions: some builds are picky, and you may need to recompile
    with matching CUDA/NCCL.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Networking issues: on multi-node setups, firewalls or missing InfiniBand drivers
    can block communication.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Building a multi-GPU trainer is the gateway to scaling. A single GPU may take
    weeks to train a large model, but spreading the work across 4, 8, or 16 GPUs cuts
    the time dramatically. The simplicity of *llm.c* shows that distributed training
    doesn’t require a massive framework—just careful use of MPI and NCCL.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Launch training with 1 GPU and then 2 GPUs, keeping the global batch size the
    same. Compare training speed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Launch with 2 GPUs but forget All-Reduce. Notice how validation loss behaves
    differently on each GPU.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use `NCCL_DEBUG=INFO` to see how NCCL sets up communication.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try deliberately mismatching ranks and devices—observe the crash to understand
    why assignment matters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Measure GPU utilization with `nvidia-smi` during training to confirm both GPUs
    are working.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Multi-GPU trainers in *llm.c* are built around three pillars: MPI to manage
    processes, NCCL to synchronize gradients, and CUDA to run the math. Once these
    are in place, the training loop remains familiar, but the computation spreads
    across GPUs seamlessly. This design keeps the code minimal while still unlocking
    significant scaling power.'
  prefs: []
  type: TYPE_NORMAL
- en: 75\. Multi-Node Bootstrapping with MPI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'So far, running across multiple GPUs on a single machine is relatively straightforward:
    every process talks through shared memory or high-speed interconnects like NVLink.
    Things become more interesting when training has to scale across multiple machines
    (often called “nodes”)—for example, when you want to run on 2 servers, each with
    4 GPUs, to make a total of 8.'
  prefs: []
  type: TYPE_NORMAL
- en: The MPI World
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'MPI was designed for this. When you run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE158]'
  prefs: []
  type: TYPE_PRE
- en: '`-np 8` says you want 8 processes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-hostfile myhosts` lists the machines (and how many processes to run on each).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MPI then launches processes across nodes and assigns each one a rank. From the
    program’s perspective, it doesn’t matter if two ranks are on the same machine
    or different machines—they all see a global communicator of size 8.
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up NCCL Across Nodes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'NCCL doesn’t know how to find other machines by itself. It relies on MPI to
    exchange a unique NCCL ID. The typical flow is:'
  prefs: []
  type: TYPE_NORMAL
- en: Rank 0 creates a new NCCL ID.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rank 0 broadcasts the ID to all other ranks using MPI.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each process calls `ncclCommInitRank` with the shared ID, total world size,
    and its own rank.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This ensures all GPUs, even across different machines, join the same “conference
    call.”
  prefs: []
  type: TYPE_NORMAL
- en: Networking Considerations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When scaling across nodes, networking becomes critical:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ethernet vs InfiniBand: Standard Ethernet works but can be slow. High-performance
    clusters use InfiniBand for much higher bandwidth and lower latency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Firewall rules: NCCL needs open ports to connect nodes. Firewalls or strict
    security settings can block communication.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Environment variables: Variables like `NCCL_SOCKET_IFNAME` (to pick the right
    network interface) often need to be set. For example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE159]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Example Hostfile
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A simple `myhosts` file could look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE160]'
  prefs: []
  type: TYPE_PRE
- en: This says node1 and node2 each have 4 GPUs. MPI will launch 4 processes on each,
    totaling 8.
  prefs: []
  type: TYPE_NORMAL
- en: Synchronization Across Nodes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Because communication now spans machines, synchronization overhead becomes
    more visible. Gradient All-Reduce has to move data not only between GPUs in one
    server but also across the network. Efficient scaling depends on:'
  prefs: []
  type: TYPE_NORMAL
- en: Large enough batch sizes (so compute time outweighs communication).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overlapping communication with computation (advanced optimization).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fast interconnects between machines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Training large models rarely happens on a single machine. Multi-node training
    is how researchers and companies scale models to billions of parameters. By showing
    how to bootstrap MPI and NCCL across nodes, *llm.c* demonstrates the foundation
    of distributed AI training systems, but in a minimal and transparent way.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Prepare two machines with CUDA and NCCL installed, connected via the same network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a hostfile listing both machines, then launch with `mpirun`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set `NCCL_DEBUG=INFO` to watch how NCCL connects across nodes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare throughput between single-node and two-node runs with the same number
    of GPUs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Experiment with environment variables like `NCCL_SOCKET_IFNAME` or `NCCL_IB_DISABLE=1`
    to see how network choices affect speed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Bootstrapping multi-node training is about extending the same principles as
    single-node multi-GPU training, but with networking in the mix. MPI handles process
    management, NCCL sets up communication, and CUDA runs the math. With just a few
    lines of setup, *llm.c* can stretch from one GPU on your laptop to dozens of GPUs
    spread across multiple servers.
  prefs: []
  type: TYPE_NORMAL
- en: 76\. SLURM and PMIx Caveats
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: On many research clusters or supercomputers, you don’t launch jobs manually
    with `mpirun` and a hostfile. Instead, you interact with a job scheduler, most
    commonly SLURM. SLURM takes care of allocating resources, starting processes across
    nodes, and enforcing quotas. While this saves you from manually managing hostfiles,
    it introduces its own set of details that you need to understand.
  prefs: []
  type: TYPE_NORMAL
- en: SLURM Basics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In SLURM, you typically request GPUs and nodes using a script or command like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE161]'
  prefs: []
  type: TYPE_PRE
- en: '`-N 2` asks for 2 nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-G 8` requests 8 GPUs (across those nodes).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--time=01:00:00` sets a one-hour time limit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once the job starts, SLURM sets environment variables such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '`SLURM_NTASKS`: total number of tasks (processes).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SLURM_PROCID`: the rank of the current process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SLURM_NODEID`: which node this process is running on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MPI implementations (OpenMPI, MPICH) and NCCL can use these to bootstrap communication
    automatically.
  prefs: []
  type: TYPE_NORMAL
- en: PMIx Integration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Modern SLURM often works with PMIx (Process Management Interface for Exascale).
    PMIx allows MPI and other runtimes to query process information directly from
    SLURM without relying on older launchers. In practice, this means:'
  prefs: []
  type: TYPE_NORMAL
- en: You might not use `mpirun` at all. Instead, SLURM provides `srun`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE162]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here `-n 8` launches 8 tasks across your allocated nodes. SLURM/PMIx handles
    the rank assignments.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Common Pitfalls
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: MPI version mismatch If your cluster has multiple MPI libraries installed, you
    may accidentally compile with one and run with another. Always confirm that the
    `mpicc` and `mpirun` you’re using match the library your job is linking against.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Environment variable propagation NCCL relies on environment variables like `NCCL_DEBUG`,
    `NCCL_SOCKET_IFNAME`, and `NCCL_IB_HCA`. Sometimes SLURM doesn’t forward these
    to all nodes unless you configure it to. Using `--export=ALL` or adding exports
    in your job script can fix this.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: GPU visibility SLURM manages GPU allocation via `CUDA_VISIBLE_DEVICES`. Each
    process only “sees” the GPUs it was assigned. If your code assumes a global view
    of all GPUs, it may break. In *llm.c*, the mapping between rank and GPU ID needs
    to respect this.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Network fabric mismatches On big clusters, you may have multiple network fabrics
    (Ethernet, InfiniBand). If NCCL picks the wrong one, performance plummets. Explicitly
    setting `NCCL_SOCKET_IFNAME` or `NCCL_IB_DISABLE` can solve this.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Learning to run across nodes with SLURM is essential if you want to scale training
    beyond a single server. While local `mpirun` commands work for development, almost
    all serious training runs—whether academic or industrial—happen under SLURM or
    a similar workload manager. Understanding the quirks of SLURM and PMIx ensures
    that your code scales smoothly without mysterious hangs or slowdowns.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Write a small SLURM job script that requests 2 GPUs for 10 minutes and runs
    a dummy *llm.c* training loop.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use `srun` to launch the program and print out the `SLURM_PROCID` and `SLURM_NODEID`
    for each process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set `NCCL_DEBUG=INFO` in your job script and observe how NCCL initializes communication.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Experiment with `srun --ntasks-per-node` to control how many processes land
    on each node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Intentionally misconfigure `CUDA_VISIBLE_DEVICES` to see how it affects rank-to-GPU
    mapping.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: SLURM and PMIx streamline distributed training on large clusters, but they add
    another layer of complexity. The principles remain the same—MPI ranks, NCCL communicators,
    and CUDA kernels—but the scheduler decides how processes are placed and how environments
    are set up. With a bit of practice, these tools allow *llm.c* to move from simple
    multi-GPU experiments to scalable cluster-wide training runs.
  prefs: []
  type: TYPE_NORMAL
- en: 77\. Debugging Multi-GPU Hangs and Stalls
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When training on multiple GPUs, one of the most frustrating experiences is a
    job that simply hangs — no errors, no crashes, just frozen processes. In distributed
    deep learning, hangs are almost always related to synchronization mismatches.
    Every GPU worker is supposed to “meet up” at communication points (like gradient
    all-reduce), and if even one process gets lost, the whole group stalls.
  prefs: []
  type: TYPE_NORMAL
- en: Common Causes of Hangs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Mismatched Collective Calls If one rank calls `ncclAllReduce` while another
    rank skips it or calls `ncclBroadcast`, the system deadlocks. All GPUs wait forever
    because they’re not speaking the same “language” at that step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Uneven Batch Sizes If the training data isn’t perfectly divisible across GPUs,
    one process might run out of data earlier than others. The code tries to sync
    gradients, but some ranks never reach that point.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: CUDA Errors Silently Ignored A kernel launch failure on one GPU can prevent
    it from reaching synchronization. If error checks are missing, you won’t see the
    failure until the program is stuck.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Networking Issues NCCL depends on reliable network connections. If one node
    has a bad InfiniBand card, firewall rule, or misconfigured interface, communication
    halts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Debugging Strategies
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Enable NCCL Debugging Set:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE163]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This produces logs showing when each rank enters and leaves collective operations.
    By comparing ranks, you can see who got stuck.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Check CUDA Errors Always wrap CUDA calls with error checks, or run with:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE164]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This detects invalid memory access or kernel failures that might lead to stalls.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Simplify the Setup Start with 2 GPUs on a single node. If it works, increase
    to 4 GPUs, then expand to multiple nodes. This isolates whether the bug is in
    GPU logic or network communication.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Timeouts and Watchdogs NCCL provides environment variables like `NCCL_TIMEOUT`
    that can help detect when a collective is stalled. Although it won’t fix the hang,
    it prevents wasting hours waiting for nothing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In multi-GPU training, hangs are not rare — they are part of the debugging journey.
    Understanding that hangs usually mean “one rank is out of sync” helps you approach
    the problem methodically. By checking logs, validating batch sizes, and carefully
    testing collective calls, you avoid endless frustration and wasted GPU hours.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Run a 2-GPU training job and intentionally misconfigure the code so only one
    rank calls `gpt2_backward`. Observe how the system hangs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enable `NCCL_DEBUG=INFO` and compare logs between the two ranks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modify the dataloader so that one GPU gets fewer batches than the other. Watch
    how training stalls at the first gradient sync.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Experiment with `cuda-memcheck` to catch silent CUDA errors in a simple kernel.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Practice scaling up from 1 node to 2 nodes to see where hangs are more likely
    to appear.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Hangs in distributed training almost always trace back to mismatched synchronization,
    unbalanced workloads, or hidden errors. By using NCCL’s debug tools, adding error
    checks, and testing systematically, you can turn mysterious freezes into solvable
    problems. Multi-GPU training isn’t just about raw speed — it’s about learning
    to keep many moving parts in lockstep.
  prefs: []
  type: TYPE_NORMAL
- en: '78\. Scaling Stories: GPT-2 124M → 774M → 1.6B'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One of the most exciting parts of *llm.c* is that it doesn’t just stop at toy
    models. The same code that trains a small GPT-2 model on Tiny Shakespeare can
    be scaled up to much larger models, like GPT-2 774M and even 1.6B. But scaling
    isn’t just about making the numbers bigger — it changes almost everything about
    how you train: memory requirements, communication costs, optimizer stability,
    and even your workflow.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Starting Small: GPT-2 124M'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The 124M parameter model is the “hello world” of GPT-2 training. It fits comfortably
    on a single modern GPU, and you can even run a trimmed-down version on CPU. At
    this size:'
  prefs: []
  type: TYPE_NORMAL
- en: Batch sizes can stay small (e.g., 4–8).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory requirements are modest — a few gigabytes of VRAM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training speed is relatively fast, so you can iterate quickly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Purpose: sanity checks, debugging kernels, verifying correctness.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Think of 124M as the training wheels stage: you’re learning to balance, not
    yet racing.'
  prefs: []
  type: TYPE_NORMAL
- en: Moving to GPT-2 774M
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'At ~774M parameters, the picture changes:'
  prefs: []
  type: TYPE_NORMAL
- en: A single GPU can still *fit* the model, but training speed slows dramatically.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient synchronization across multiple GPUs becomes essential to get reasonable
    throughput.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Communication costs start to matter: an all-reduce of hundreds of megabytes
    per step stresses both PCIe and network bandwidth.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stability becomes more sensitive: learning rates and warmup schedules need
    more careful tuning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, training is less about “does the code run?” and more about “does the system
    scale?” This size is often used in academic replications of GPT-2 because it’s
    large enough to be interesting but not impossibly expensive.
  prefs: []
  type: TYPE_NORMAL
- en: 'GPT-2 1.6B: Scaling to the Edge'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'At 1.6B parameters, the model is too large for a single GPU to train efficiently.
    You need:'
  prefs: []
  type: TYPE_NORMAL
- en: Multi-GPU setups with NCCL all-reduce to share gradient updates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-node training on clusters when even 8 GPUs aren’t enough.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Careful optimizer tuning — without proper settings for AdamW and schedulers,
    the model may diverge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory tricks like mixed precision (FP16/BF16) and gradient checkpointing to
    fit activations in memory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training GPT-2 1.6B is a significant engineering challenge, but it proves that
    *llm.c* is not just a toy project — it’s a minimal yet real implementation that
    can push to billion-parameter scale.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling Lessons
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As you climb from 124M to 774M to 1.6B, several lessons emerge:'
  prefs: []
  type: TYPE_NORMAL
- en: Debug small, scale big — always test on 124M before attempting larger models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Communication dominates — at 774M and beyond, time spent moving gradients often
    exceeds compute time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hyperparameters evolve — a learning rate that works for 124M may explode the
    loss at 1.6B.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Infrastructure matters — GPUs, interconnects, and schedulers become as important
    as code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Scaling stories show that deep learning isn’t just about writing a clever algorithm
    — it’s about making that algorithm work under increasingly heavy loads. Each jump
    in size uncovers new bottlenecks and new engineering challenges. By following
    this path, you gain intuition for how large models are really trained in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Train GPT-2 124M on Tiny Shakespeare until the loss stabilizes. Record how long
    each step takes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Attempt the same experiment on OpenWebText with 124M — watch how dataset size
    now becomes the limiting factor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scale up to GPT-2 355M or 774M if you have access to multiple GPUs. Measure
    how much time is spent in NCCL all-reduce compared to compute.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you have cluster access, try running 774M with `srun` or `mpirun` across
    nodes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Study published training logs for GPT-2 1.6B and compare them to your own —
    how does scaling change the shape of the loss curve?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Scaling isn’t just “bigger models need bigger GPUs.” Each increase in model
    size reshapes the training process, introducing new bottlenecks and requiring
    new techniques. *llm.c* is valuable precisely because it makes these transitions
    transparent: you can start with a tiny model and gradually experience the real
    engineering hurdles of training state-of-the-art language models.'
  prefs: []
  type: TYPE_NORMAL
- en: 79\. NCCL Tuning and Overlap Opportunities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once your training runs extend beyond a single GPU, communication overhead
    becomes a central challenge. Every training step requires gradients to be exchanged
    among GPUs so that the optimizer updates stay synchronized. This is where NCCL
    (NVIDIA Collective Communications Library) comes in. NCCL provides efficient implementations
    of collective operations like all-reduce, all-gather, and broadcast. But simply
    using NCCL isn’t enough: how you tune it, and how you overlap communication with
    computation, can make the difference between sluggish training and near-linear
    scaling.'
  prefs: []
  type: TYPE_NORMAL
- en: How NCCL Works in Training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In *llm.c*, when multiple GPUs train together, each GPU computes its local gradients
    during backpropagation. At the end of the backward pass, NCCL’s all-reduce combines
    gradients across GPUs so that every GPU ends up with the same values. Only then
    can the optimizer step forward.
  prefs: []
  type: TYPE_NORMAL
- en: Without NCCL, you’d have to write custom point-to-point code with `cudaMemcpyPeer`
    or MPI, which would be both slower and harder to maintain. NCCL ensures the communication
    pattern is efficient for the underlying hardware — PCIe, NVLink, or InfiniBand.
  prefs: []
  type: TYPE_NORMAL
- en: Key NCCL Tuning Parameters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: NCCL_DEBUG Setting `NCCL_DEBUG=INFO` helps you understand what NCCL is doing.
    For performance tuning, logs are essential.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: NCCL_SOCKET_IFNAME On multi-node clusters, this decides which network interface
    NCCL binds to. Using the wrong interface (like Ethernet instead of InfiniBand)
    can slow training by orders of magnitude.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'NCCL_ALGO Determines how collectives are executed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Ring*: good for large message sizes, stable performance.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Tree*: faster for small messages, less latency. Some training runs benefit
    from experimenting with both.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: NCCL_IB_DISABLE Useful if you want to force NCCL to avoid InfiniBand and stick
    with TCP/IP, usually for debugging network issues.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Overlapping Communication with Computation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The backward pass doesn’t need to wait until all gradients are computed before
    starting to communicate. In fact, gradients for earlier layers can start their
    all-reduce while later layers are still computing gradients. This is called communication-computation
    overlap.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Without overlap: compute gradients for all layers → run NCCL all-reduce for
    all gradients → update parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With overlap: while gradients for higher layers are still being computed, start
    the all-reduce for earlier layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This reduces idle time and often leads to substantial throughput gains. Some
    frameworks (like PyTorch’s DistributedDataParallel) implement this automatically.
    In a low-level system like *llm.c*, this would require careful kernel launch ordering
    and stream management.
  prefs: []
  type: TYPE_NORMAL
- en: Practical Example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Imagine you’re training GPT-2 774M across 8 GPUs. Each backward pass produces
    ~3 GB of gradients. If you wait until all gradients are ready before syncing,
    the all-reduce might take 200 ms. If your compute step also takes 200 ms, then
    half of your training time is spent idle. With overlap, you can hide much of that
    communication inside compute time, potentially cutting step time almost in half.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As model sizes increase, communication costs can rival or exceed computation.
    Without tuning, GPUs spend more time waiting for data to arrive than actually
    training the model. By understanding NCCL and applying overlap techniques, you
    unlock the ability to scale efficiently to dozens or even hundreds of GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Run a multi-GPU training job with `NCCL_DEBUG=INFO` enabled and watch the communication
    patterns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change `NCCL_ALGO` between `Ring` and `Tree` and measure the effect on step
    times.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Experiment with setting `CUDA_LAUNCH_BLOCKING=1` to remove overlap, then remove
    it again to see how communication and computation interleave.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you have a cluster, try forcing NCCL to use Ethernet instead of InfiniBand
    and compare bandwidth.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Profile a multi-GPU run using `nvprof` or Nsight Systems and check whether NCCL
    collectives overlap with kernel execution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Efficient distributed training is not only about having more GPUs — it’s about
    keeping them busy. NCCL provides the communication backbone, but how you configure
    and overlap its operations determines whether you get close to linear scaling
    or waste resources. Mastering these details transforms multi-GPU training from
    “just working” into truly efficient large-scale computation.
  prefs: []
  type: TYPE_NORMAL
- en: 80\. Common Multi-GPU Errors and Fixes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When running *llm.c* across multiple GPUs, errors can range from confusing hangs
    to cryptic NCCL messages. These problems are normal in distributed training, but
    they can eat up hours unless you recognize the patterns. The good news is that
    most errors fall into a handful of common categories, and once you learn the typical
    causes, they’re easier to diagnose and fix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Error Type 1: Process Hangs with No Output'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Symptom: Training starts but then freezes. No error message, no crash, just
    silence. Cause: Usually, one or more ranks are out of sync. This could mean:'
  prefs: []
  type: TYPE_NORMAL
- en: Different batch sizes on each rank (one rank has fewer tokens left).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A mismatch in collective calls — for example, one GPU calls `all_reduce` while
    another skips it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A CUDA error in one process that prevents it from reaching synchronization.
    Fix:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check that dataloaders feed the same number of steps to every rank.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add error checking to CUDA calls.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enable `NCCL_DEBUG=INFO` to trace which rank got stuck.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Error Type 2: `NCCL WARN Net: no interface found`'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Symptom: NCCL reports it can’t find a network interface, or training is extremely
    slow. Cause: NCCL can’t discover the correct interface to use for inter-node communication.
    By default, it may try Ethernet instead of InfiniBand. Fix:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Set `NCCL_SOCKET_IFNAME` to the right interface, e.g.:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE165]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Confirm with your sysadmin which network interfaces are available for high-performance
    GPU communication.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Error Type 3: `CUDA_ERROR_OUT_OF_MEMORY`'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Symptom: Processes crash when allocating model weights or during the backward
    pass. Cause:'
  prefs: []
  type: TYPE_NORMAL
- en: Model too large for the available GPU memory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batch size too high.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Memory fragmentation from repeated allocations. Fix:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduce batch size `B` or sequence length `T`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try mixed precision (FP16/BF16) if supported.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Restart processes to clear memory fragmentation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Error Type 4: `unhandled system error, NCCL version mismatch`'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Symptom: One process logs NCCL version `2.17` and another logs `2.14`. Training
    fails. Cause: Different NCCL libraries are being used across nodes. This happens
    when software environments aren’t identical. Fix:'
  prefs: []
  type: TYPE_NORMAL
- en: Use the same container or module environment on all nodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Confirm NCCL versions with `ldd` or `conda list`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Error Type 5: Validation Loss Diverges on Multi-GPU but Not on Single GPU'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Symptom: Loss values explode only when running across multiple GPUs. Cause:
    Gradient synchronization may be broken — for example, only a subset of parameters
    are being all-reduced. Another possibility is using a different effective learning
    rate because of batch size scaling. Fix:'
  prefs: []
  type: TYPE_NORMAL
- en: Confirm that all parameters participate in gradient synchronization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scale the learning rate properly: if you double the global batch size by using
    more GPUs, you may need to adjust the learning rate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Multi-GPU training is powerful but unforgiving: even tiny mismatches in environment,
    data, or synchronization can cause errors. Instead of treating these as random
    mysteries, it’s helpful to recognize the patterns. Each error message or hang
    has a likely cause, and learning to map symptoms to fixes will make distributed
    training much smoother.'
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Intentionally reduce the dataset size so one rank runs out of data early — observe
    the hang.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Launch a multi-node run without setting `NCCL_SOCKET_IFNAME` and watch how performance
    collapses.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Increase the batch size until you trigger `CUDA_ERROR_OUT_OF_MEMORY`, then reduce
    it step by step to see the limit.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Experiment with mismatched NCCL versions across nodes if you have access to
    multiple environments, then fix it by standardizing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run a small model with different batch sizes per rank and study how the validation
    loss diverges.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Most multi-GPU errors aren’t mysterious once you understand what’s happening
    under the hood. They usually boil down to synchronization mismatches, network
    misconfigurations, memory limits, or environment inconsistencies. With the right
    debugging tools and a systematic mindset, you can fix these problems quickly and
    keep your training runs moving forward.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 9\. Extending the codebase
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 81\. The `dev/cuda` Library for Custom Kernels
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'So far, most of the CUDA logic in *llm.c* has relied on NVIDIA’s optimized
    libraries like cuBLAS and cuDNN. These libraries are extremely powerful and efficient,
    but sometimes you want more control: maybe you’re experimenting with a new attention
    mechanism, or maybe you want to fuse multiple operations into a single kernel
    to reduce memory traffic. That’s where the `dev/cuda` directory comes in. It’s
    the playground for custom kernels.'
  prefs: []
  type: TYPE_NORMAL
- en: What Lives in `dev/cuda`
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If you look at the repository structure, you’ll notice a folder named `dev/cuda`.
    This is not part of the minimal training path — you can train GPT-2 models without
    ever touching it. Instead, it contains experimental kernels that showcase how
    to move from simple CUDA examples toward more advanced, production-level implementations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inside, you’ll typically find:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hello World kernels: basic examples like elementwise addition to get familiar
    with CUDA.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fused operations: simple prototypes for combining steps like bias addition
    + activation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Benchmark code: small programs that measure kernel performance compared to
    cuBLAS/cuDNN.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These files are not polished production code. They’re meant to be read, modified,
    and played with — like lab notebooks for CUDA development.
  prefs: []
  type: TYPE_NORMAL
- en: Why Custom Kernels Matter
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Libraries like cuBLAS are designed to cover a wide range of use cases, but
    they don’t always hit the sweet spot for your specific workload. Writing custom
    kernels allows you to:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fuse operations: Instead of launching separate kernels for bias addition, activation,
    and dropout, you can do all three in one kernel, saving time on memory reads/writes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Experiment with new algorithms: If you invent a new type of attention or normalization,
    you can’t rely on cuDNN — you need to implement it yourself.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Learn how GPUs actually work: Reading and writing custom kernels teaches you
    about thread blocks, memory hierarchy, and warp scheduling, all of which deepen
    your understanding of GPU programming.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: A Simple Elementwise Kernel'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here’s a very small kernel from a toy example you might find in `dev/cuda`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE166]'
  prefs: []
  type: TYPE_PRE
- en: 'This kernel adds two arrays `a` and `b` elementwise. While trivial compared
    to attention mechanisms, it illustrates the GPU execution model:'
  prefs: []
  type: TYPE_NORMAL
- en: Each thread handles one index `i`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Threads are grouped into blocks, and blocks form a grid.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Memory access is explicit: you control exactly how `out` is written.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling this up to real workloads means adding more complexity — shared memory,
    warp shuffles, half-precision math — but the principles remain the same.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `dev/cuda` directory is not just for fun experiments. It’s a bridge between
    “using GPU libraries” and “designing GPU algorithms.” By learning to write kernels
    here, you gain the freedom to customize and optimize beyond what standard libraries
    provide. That skill becomes essential if you want to innovate in model architectures
    or squeeze the last bit of performance out of your hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Open one of the example `.cu` files in `dev/cuda` and compile it with `nvcc`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modify the elementwise kernel so it performs `out[i] = a[i] * b[i]` instead
    of addition.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Benchmark your kernel against the equivalent cuBLAS call (e.g., `cublasSaxpy`)
    and compare performance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a fused kernel that does bias addition followed by ReLU activation in
    one pass.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use `nvprof` or Nsight Systems to measure how many kernel launches occur in
    a forward pass and imagine how custom fusion might reduce them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `dev/cuda` library is your sandbox for learning and experimenting with CUDA.
    It’s not required for running GPT-2, but it’s where you build the skills to go
    beyond libraries and design your own GPU operations. Whether you’re optimizing
    for speed or testing new research ideas, this directory is where theory meets
    practice in GPU programming.
  prefs: []
  type: TYPE_NORMAL
- en: 82\. Adding New Dataset Pipelines (`dev/data/*`)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Training a language model is not just about having a clever model — it’s equally
    about the data you feed it. In *llm.c*, datasets are handled in a very lightweight
    way compared to frameworks like PyTorch. Instead of complicated abstractions,
    the project keeps things simple: tokenize your text once, save it as a `.bin`
    file, and then stream batches of tokens into the model.'
  prefs: []
  type: TYPE_NORMAL
- en: The `dev/data/` directory is where this happens. It contains scripts and utilities
    for preparing different datasets, from tiny toy corpora like Tiny Shakespeare
    to larger collections like TinyStories or OpenWebText subsets. Understanding how
    this directory works is the key to plugging in your own datasets.
  prefs: []
  type: TYPE_NORMAL
- en: How Data Pipelines Work in *llm.c*
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'At a high level, the pipeline follows three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Download or provide raw text data. For example, `tiny_shakespeare.txt` is just
    a plain text file with plays concatenated.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tokenize the data once using the GPT-2 tokenizer. The tokenizer converts text
    into integers according to `gpt2_tokenizer.bin`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write the tokens to a binary file (`.bin`). This is a flat array of integers
    stored as 32-bit values, which makes it fast to memory-map and stream during training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the `.bin` files exist, `dataloader_init` can open them, divide them into
    training and validation splits, and generate batches of shape `(B, T)` for the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: What’s Inside `dev/data/`
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The folder contains small scripts like:'
  prefs: []
  type: TYPE_NORMAL
- en: '`download_starter_pack.sh` — downloads Tiny Shakespeare and TinyStories.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tokenization scripts — often small Python snippets that run the GPT-2 tokenizer
    over raw text.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prebuilt `.bin` files — these are used in the quickstart so you don’t need to
    regenerate them yourself.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The design choice here is minimalism: instead of a heavy dataset framework,
    you get plain files and short scripts. You can read and understand everything
    in a few minutes.'
  prefs: []
  type: TYPE_NORMAL
- en: Adding Your Own Dataset
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Suppose you want to train on your company’s support chat logs or a new dataset
    you’ve found. The process looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare raw text in a simple format (one text file is fine).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the tokenizer from *llm.c* on it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE167]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This produces a binary token file.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Drop the file into `dev/data/`. You might name it `my_corpus_train.bin` and
    `my_corpus_val.bin`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Point the dataloader at it in your training code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE168]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That’s it — you now have a new dataset pipeline integrated with the same training
    loop.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Many frameworks hide data preprocessing behind layers of abstractions. *llm.c*
    takes the opposite approach: it makes the process transparent. You see exactly
    how text becomes tokens, how those tokens become batches, and how the model consumes
    them. This transparency makes it easier to debug, extend, and customize. Adding
    a new dataset is no longer a mystery — it’s just a matter of writing one file
    and updating a path.'
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Explore the `dev/data/` directory and read through the provided scripts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tokenize a new small dataset of your choice (a novel, a set of Wikipedia pages,
    or your own text).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train a 124M model on your new dataset and observe the loss curve.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare validation loss between Tiny Shakespeare and your dataset — how does
    the model behave differently?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try increasing sequence length `T` to see how batching interacts with longer
    documents.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `dev/data` folder is where you connect language models to the real world.
    It shows how raw text becomes training-ready binary files with almost no overhead.
    By learning to add your own pipelines, you gain the ability to train *llm.c* on
    any dataset — from classic literature to domain-specific corpora — while keeping
    the workflow fast and understandable.
  prefs: []
  type: TYPE_NORMAL
- en: 83\. Adding a New Optimizer to the Codebase
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'So far, *llm.c* has focused on AdamW, which is the workhorse optimizer for
    training transformer models. But deep learning is a fast-moving field: new optimizers
    appear, older ones sometimes resurface, and certain workloads benefit from alternatives.
    The simplicity of *llm.c* makes it a great environment to learn how to implement
    and experiment with optimizers.'
  prefs: []
  type: TYPE_NORMAL
- en: Where Optimizers Live in *llm.c*
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the CPU training path, the optimizer logic is implemented directly in C in
    the function `gpt2_update`. This function iterates through every parameter, applies
    AdamW’s moment updates, applies bias correction, and then modifies the parameter
    values in place.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because the parameters, gradients, and optimizer states are all stored as contiguous
    arrays in memory (`params_memory`, `grads_memory`, `m_memory`, `v_memory`), adding
    a new optimizer usually means:'
  prefs: []
  type: TYPE_NORMAL
- en: Allocating any new state arrays you need.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Defining the update rule in the training loop.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adding function calls for your new optimizer, similar to `gpt2_update`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Example: Implementing SGD with Momentum'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Stochastic Gradient Descent (SGD) with momentum is much simpler than AdamW.
    The update rule looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE169]'
  prefs: []
  type: TYPE_PRE
- en: Here, `m_memory` stores the velocity (the exponentially decayed average of past
    gradients). There’s no second moment estimate like in AdamW, so it’s leaner in
    both code and memory usage.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing Optimizers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Adding new optimizers lets you experiment and compare behaviors:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Optimizer | Strengths | Weaknesses | Memory Needs |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SGD | Simple, stable, fewer hyperparameters | Slow convergence on large models
    | Low |'
  prefs: []
  type: TYPE_TB
- en: '| SGD + Momentum | Faster convergence, smooths updates | Still less adaptive
    than Adam | Low |'
  prefs: []
  type: TYPE_TB
- en: '| Adam | Adapts learning rates per parameter | Can overfit small datasets |
    Medium |'
  prefs: []
  type: TYPE_TB
- en: '| AdamW | Same as Adam + correct weight decay | More complex | Medium |'
  prefs: []
  type: TYPE_TB
- en: '| Adagrad/RMSProp | Good for sparse features | Not always stable for transformers
    | Medium |'
  prefs: []
  type: TYPE_TB
- en: In *llm.c*, each optimizer is just a loop over parameters with a few math operations.
    That makes it the perfect playground to see how different optimizers actually
    behave in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Optimizers control how your model learns. While architectures like GPT-2 get
    a lot of attention, small changes in optimization can make the difference between
    a model that converges smoothly and one that diverges. By adding your own optimizers,
    you get a clearer understanding of this often “black box” part of deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Implement the SGD with momentum function shown above and swap it into the training
    loop instead of AdamW.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run training on Tiny Shakespeare and compare how many steps it takes for the
    loss to reach 2.0.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modify the code to implement RMSProp (similar to Adam, but without momentum
    on the first moment).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Benchmark memory usage: notice how AdamW allocates both `m_memory` and `v_memory`,
    while SGD only uses one.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try running AdamW with a very small dataset versus SGD — does one overfit faster?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Optimizers are just math on arrays. By writing and testing new ones inside *llm.c*,
    you’ll demystify how learning actually happens at the parameter level. This makes
    it easier to appreciate why AdamW became the default for transformers, and also
    gives you the tools to explore alternatives in a clean, transparent environment.
  prefs: []
  type: TYPE_NORMAL
- en: 84\. Adding a New Scheduler (cosine, step, etc.)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Training isn’t only about choosing an optimizer; the way you adjust the learning
    rate over time is just as important. A scheduler tells the optimizer *how fast
    to learn* at each step. Without a scheduler, you’d use a fixed learning rate,
    which often works poorly for large models. In *llm.c*, schedulers are kept intentionally
    simple so you can see exactly how they influence training.
  prefs: []
  type: TYPE_NORMAL
- en: Where Schedulers Fit
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If you look at the training loop, every step calls the optimizer like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE170]'
  prefs: []
  type: TYPE_PRE
- en: The `lr` here doesn’t have to be constant. Instead, a scheduler function can
    compute it based on the step number. In *llm.c*, this logic lives in `schedulers.h`
    and related helpers.
  prefs: []
  type: TYPE_NORMAL
- en: Common Schedulers You Can Add
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Step Decay Reduce the learning rate by a fixed factor every N steps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE171]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Cosine Decay Smoothly decrease the learning rate following a cosine curve.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE172]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Linear Warmup + Cosine Decay Start with a gradual increase (warmup) to avoid
    instability, then switch to cosine decay. This is the most common choice for transformers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Example: Cosine with Warmup'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here’s how you might implement cosine with warmup in *llm.c*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE173]'
  prefs: []
  type: TYPE_PRE
- en: 'This means:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Steps 0–`warmup_steps`: linearly scale from 0 → `base_lr`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After warmup: smoothly decay the learning rate using cosine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Schedulers help stabilize training. At the beginning, gradients can be very
    noisy, so warming up slowly prevents divergence. At the end, lowering the learning
    rate helps the model converge instead of bouncing around minima. Without schedulers,
    you’d often need more tuning to get the same results.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Train with a constant learning rate on Tiny Shakespeare and record the loss
    curve.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Switch to a step decay scheduler and see if convergence improves.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement cosine decay with warmup and compare against constant LR — which reaches
    a lower validation loss?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Experiment with different warmup lengths (e.g., 10 steps vs 100 steps) and watch
    how training stability changes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try running the same experiment on TinyStories and see if dataset size affects
    which scheduler works best.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Schedulers are small pieces of code with big impact. They don’t change the model
    or the optimizer, but they control the *pace of learning*. By adding new schedulers
    to *llm.c*, you get a hands-on way to see why modern training recipes almost always
    combine warmup with a smooth decay schedule.
  prefs: []
  type: TYPE_NORMAL
- en: 85\. Alternative Attention Mechanisms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Transformers became famous because of the self-attention mechanism, but “attention”
    is not a single fixed formula. Researchers have explored many alternatives that
    trade off memory use, speed, and accuracy. In *llm.c*, the default is scaled dot-product
    attention, but nothing prevents you from experimenting with new approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Default: Scaled Dot-Product Attention'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The standard attention formula looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><mtext mathvariant="normal">Attention</mtext><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>Q</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi>V</mi><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>=</mo><mtext mathvariant="normal">softmax</mtext><mrow><mo
    stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo
    stretchy="true" form="postfix">)</mo></mrow><mi>V</mi></mrow> <annotation encoding="application/x-tex">\text{Attention}(Q,
    K, V) = \text{softmax}\!\left(\frac{QK^T}{\sqrt{d_k}}\right) V</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: Q = queries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K = keys
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: V = values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <semantics><msub><mi>d</mi><mi>k</mi></msub><annotation encoding="application/x-tex">d_k</annotation></semantics>
    = key dimension (used for scaling)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In *llm.c*, this is implemented using matrix multiplications and masking to
    enforce causality. It’s correct and faithful to GPT-2, but has quadratic cost
    in sequence length <semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics>.
  prefs: []
  type: TYPE_NORMAL
- en: Variants You Could Add
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Sparse Attention Instead of attending to every token, restrict attention to
    a local window or a set of important positions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Good for long sequences.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Saves compute and memory.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: “sliding window” attention where each token only looks back 128 steps.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Linformer / Low-Rank Attention Approximate <semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation
    encoding="application/x-tex">QK^T</annotation></semantics> using low-rank projections.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reduces memory from <semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>T</mi><mn>2</mn></msup><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(T^2)</annotation></semantics>
    to <semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>T</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(T)</annotation></semantics>.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Works well when redundancy exists in sequences.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Performer (Linear Attention) Replace the softmax with kernel approximations
    so attention becomes linear in sequence length.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Trades exactness for scalability.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Allows much longer sequences on the same hardware.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: ALiBi (Attention with Linear Biases) Adds simple position-dependent biases instead
    of full positional embeddings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extremely efficient.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Helps extrapolate to longer sequences than seen in training.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How to Experiment in *llm.c*
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The attention implementation lives inside the `attention_forward` and `attention_backward`
    routines in `train_gpt2.c` (and their CUDA equivalents). To try an alternative:'
  prefs: []
  type: TYPE_NORMAL
- en: Replace the part where <semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation
    encoding="application/x-tex">QK^T</annotation></semantics> is computed with your
    chosen method.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Keep the interface the same: given inputs Q, K, V, return outputs shaped `(B,
    T, C)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run unit tests (`test_gpt2.c`) against the baseline to make sure the outputs
    stay reasonable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Attention is often the bottleneck in transformers. Quadratic time and memory
    usage limit how long your sequences can be. By experimenting with alternatives,
    you not only improve efficiency but also learn how new research ideas are implemented
    in practice. Many of today’s “efficient transformers” came from simple tweaks
    to this block.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Modify attention so each token only attends to the last 16 tokens (a toy form
    of sparse attention). Train on Tiny Shakespeare and compare speed vs. accuracy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement ALiBi by adding linear position-dependent bias terms and see if your
    model generalizes better to longer text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Benchmark the GPU memory footprint of standard attention vs. your custom version
    using Nsight Systems or `nvidia-smi`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try removing the scaling factor <semantics><mrow><mn>1</mn><mi>/</mi><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mrow><annotation
    encoding="application/x-tex">1/\sqrt{d_k}</annotation></semantics> — does training
    become unstable?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Replace softmax with a simple ReLU and see how the model behaves (hint: it
    usually diverges, but it teaches why softmax is important).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The attention block is where much of the magic happens in transformers, but
    it’s also the biggest bottleneck. By experimenting with alternatives in *llm.c*,
    you’ll gain a deeper understanding of why the standard formula works, what its
    weaknesses are, and how new ideas from research can be tested directly in code.
  prefs: []
  type: TYPE_NORMAL
- en: 86\. Profiling and Testing New Kernels
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When you start adding custom CUDA kernels or experimenting with new attention
    mechanisms, the next big question is: how do you know if they’re correct and efficient?
    That’s where profiling and testing come in. *llm.c* keeps this process minimal
    but transparent so you can see exactly how to validate both correctness and performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Correctness First: Testing Against a Reference'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Any new kernel you write should be compared against a known-good implementation.
    In *llm.c*, PyTorch usually serves as the “oracle.” For example:'
  prefs: []
  type: TYPE_NORMAL
- en: Generate random input tensors in both *llm.c* and PyTorch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run your custom kernel in *llm.c*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the equivalent operation in PyTorch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare the outputs within a small tolerance (e.g., differences less than `1e-5`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This ensures your kernel doesn’t silently compute the wrong thing. Without this
    step, you might train for hours before realizing your model is learning nonsense.
  prefs: []
  type: TYPE_NORMAL
- en: 'Performance: Profiling the Kernels'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Once correctness is established, the next step is performance. NVIDIA provides
    several tools:'
  prefs: []
  type: TYPE_NORMAL
- en: 'nvprof (older): still widely used, easy to launch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nsight Systems / Nsight Compute (modern): more detailed, lets you see kernel
    timings, memory transfers, occupancy, and more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In practice:'
  prefs: []
  type: TYPE_NORMAL
- en: Run your training loop with profiling enabled.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify which kernels take the most time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check if your custom kernel is faster than the baseline (e.g., cuBLAS or cuDNN).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common Metrics to Watch
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Kernel time (how long each launch takes).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Occupancy (how many CUDA cores are active relative to maximum).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory throughput (are you saturating memory bandwidth?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Launch count (do you call your kernel too many times instead of fusing operations?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even a correct kernel can be slower than a library implementation if it doesn’t
    use the GPU efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Example Workflow
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Suppose you write a fused bias + ReLU kernel. You can test it like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Generate a random tensor in C and in PyTorch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply your fused kernel vs. PyTorch’s separate `+` and `ReLU` ops.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compare results for correctness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Profile both approaches: is your kernel faster? Did it reduce kernel launch
    overhead?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Custom kernels are fun to write, but without testing and profiling they’re just
    guesswork. Many research ideas look promising in theory but fall apart in practice
    because they run slower or break correctness. By learning to systematically test
    and profile, you can separate ideas that are genuinely useful from those that
    are just experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Write a simple fused kernel for bias + ReLU. Compare it against PyTorch’s `x
    + bias` followed by `relu(x)`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use `nvprof` to check how many kernel launches happen in each version.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run Nsight Systems and look at the timeline: do you see your fused kernel overlapping
    better with other GPU activity?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try scaling sequence length `T` to very large values — does your kernel still
    perform well?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Record memory usage before and after your kernel runs. Is there a difference
    compared to the unfused version?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Profiling and testing turn kernel hacking from random tinkering into real engineering.
    With a reference for correctness and tools for performance measurement, you can
    iterate confidently, knowing when your changes are truly improvements. This is
    how *llm.c* bridges the gap between a learning project and real GPU systems work.
  prefs: []
  type: TYPE_NORMAL
- en: 87\. Using PyTorch Reference as Oracle
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the guiding principles of *llm.c* is to stay small, readable, and minimal
    — but that doesn’t mean you’re left without a safety net. When implementing something
    as mathematically dense as a transformer, how do you know your C or CUDA code
    is doing the right thing? The answer is to use PyTorch as a reference implementation,
    often called an “oracle.”
  prefs: []
  type: TYPE_NORMAL
- en: What Does “Oracle” Mean Here?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'An oracle is simply a trusted system you compare against. PyTorch is trusted
    because:'
  prefs: []
  type: TYPE_NORMAL
- en: It’s widely used in production and research.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Its operators (matrix multiply, attention, layernorm, etc.) have been thoroughly
    tested.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It gives you both CPU and GPU implementations with stable numerical behavior.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your *llm.c* forward or backward pass matches PyTorch’s within a small error
    tolerance, you can be confident that your implementation is correct.
  prefs: []
  type: TYPE_NORMAL
- en: How the Comparison Works
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The workflow usually looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Set up the same model in both PyTorch and *llm.c* with identical weights.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feed the same inputs to both models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare outputs — logits, losses, or gradients.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Allow for tiny differences due to floating point arithmetic, usually within
    `1e-5` to `1e-6`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For example, `test_gpt2.c` in the repository runs a forward pass in C and compares
    the logits to those produced by a PyTorch GPT-2 checkpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Suppose you’re testing the embedding lookup. In PyTorch you might write:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE174]'
  prefs: []
  type: TYPE_PRE
- en: In *llm.c*, you’d run the same two tokens through the embedding lookup and compare
    the resulting vectors element by element. If they match, your embedding implementation
    is correct.
  prefs: []
  type: TYPE_NORMAL
- en: Why PyTorch is the Perfect Oracle
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Transparency: it’s easy to extract weights from PyTorch checkpoints (`.bin`
    files).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Flexibility: you can test individual layers, not just the whole model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Debuggability: if something goes wrong, you can isolate which layer diverges
    first.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This last point is crucial — instead of training for days only to find your
    loss curve diverges, you can catch mismatches immediately at the layer level.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Deep learning models are fragile. Even a tiny mistake in normalization, masking,
    or gradient flow can ruin training. By anchoring your work to PyTorch, you avoid
    “trusting your gut” and instead rely on a battle-tested baseline. This practice
    isn’t unique to *llm.c* — many professional frameworks (Megatron-LM, DeepSpeed)
    also validate against PyTorch during development.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Extract GPT-2 weights from Hugging Face Transformers and load them into *llm.c*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run a forward pass in both PyTorch and *llm.c* with the same input tokens. Compare
    outputs numerically.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Focus on a single block: check whether the attention output matches PyTorch’s.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modify a kernel (e.g., change softmax to ReLU) and watch how quickly the outputs
    diverge from PyTorch’s.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use PyTorch to verify gradients by calling `.backward()` and comparing with
    `gpt2_backward` in *llm.c*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: PyTorch is your map and compass when navigating the dense jungle of transformer
    internals. By treating it as an oracle, you can move confidently from one layer
    to the next, knowing that your small, hand-written code matches the behavior of
    a full-featured deep learning framework. This practice turns *llm.c* into more
    than a toy project — it becomes a faithful, verifiable reimplementation of GPT-2.
  prefs: []
  type: TYPE_NORMAL
- en: '88\. Exploring Beyond GPT-2: LLaMA Example'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While *llm.c* focuses on GPT-2 for clarity, the same framework can extend to
    newer, larger, and more modern models such as LLaMA. LLaMA, released by Meta,
    uses many of the same building blocks as GPT-2 — embeddings, attention layers,
    MLPs, normalization, and residual streams — but with tweaks that improve efficiency
    and scaling. Looking at LLaMA through the lens of *llm.c* helps you see how language
    model designs evolve while still sharing the same DNA.
  prefs: []
  type: TYPE_NORMAL
- en: What Stays the Same
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Token embeddings: both GPT-2 and LLaMA use lookup tables to turn token IDs
    into dense vectors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Transformer blocks: the fundamental loop of attention → MLP → residuals is
    unchanged.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Autoregressive training: predict the next token given all previous ones, using
    causal masking.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This means much of the code in *llm.c* — dataloaders, embeddings, forward loops
    — would work with LLaMA almost unchanged.
  prefs: []
  type: TYPE_NORMAL
- en: What’s Different
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Normalization
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: GPT-2 uses LayerNorm before each block output.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: LLaMA uses RMSNorm, which normalizes using only the root mean square of activations
    (no mean subtraction).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This reduces compute slightly and improves stability.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Positional Encoding
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: GPT-2 has learned positional embeddings.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: LLaMA uses Rotary Position Embeddings (RoPE), which rotate queries and keys
    in attention space to encode positions.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: RoPE scales better to longer contexts.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Vocabulary
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: GPT-2’s vocab size is 50,257.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: LLaMA uses a different tokenizer (SentencePiece/BPE) with a larger vocabulary,
    closer to 32k for LLaMA-2.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Model Scale
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: GPT-2 tops out at 1.6B parameters.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: LLaMA-2 and LLaMA-3 scale from 7B up to 70B+. This makes distributed training
    mandatory, with mixed precision and checkpointing as standard.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Adapting *llm.c* to LLaMA
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If you wanted to modify *llm.c* to approximate LLaMA, the main tasks would
    be:'
  prefs: []
  type: TYPE_NORMAL
- en: Replace LayerNorm with an RMSNorm implementation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add RoPE into the attention mechanism. This means modifying the step where Q
    and K vectors are built, applying a rotation based on token positions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Swap out the GPT-2 tokenizer with a SentencePiece tokenizer trained on the desired
    vocabulary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The rest of the pipeline — optimizer, schedulers, dataloaders, multi-GPU support
    — would remain valid.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: By studying LLaMA in the context of GPT-2, you see that modern LLMs aren’t completely
    alien. They’re evolutionary improvements on the same transformer backbone. Recognizing
    these small architectural changes (RMSNorm, RoPE, scaling) helps demystify why
    newer models outperform older ones, and it shows you exactly what you’d need to
    tweak in *llm.c* to explore beyond GPT-2.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Implement RMSNorm in C by adapting the LayerNorm code in *llm.c*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a simplified version of RoPE to the attention kernel and run it on Tiny
    Shakespeare.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Swap the GPT-2 tokenizer for a SentencePiece model and train a small LLaMA-like
    model on your own dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare training stability between LayerNorm and RMSNorm — does the loss curve
    look different?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Study the memory use of GPT-2 vs. a small LLaMA-style variant and see how scaling
    behaves.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Exploring LLaMA through *llm.c* shows how flexible the codebase really is. With
    only a few targeted changes — normalization, positional encoding, tokenizer —
    you can shift from replicating GPT-2 to experimenting with the building blocks
    of modern LLMs. This makes *llm.c* not just a study tool for one model, but a
    foundation for understanding the entire lineage of transformers.
  prefs: []
  type: TYPE_NORMAL
- en: '89\. Porting Playbook: C → Go/Rust/Metal'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The *llm.c* codebase is written in plain C for maximum readability and minimal
    dependencies. But in practice, many developers want to experiment with other languages
    or platforms — for example, writing a Go or Rust version for better tooling, or
    targeting Apple’s Metal API for GPU acceleration on Macs. Porting is not just
    a copy-paste exercise; it requires careful thinking about how low-level memory,
    math operations, and parallelism map across ecosystems.
  prefs: []
  type: TYPE_NORMAL
- en: Why Port at All?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Go: strong concurrency model (goroutines, channels), good for building training
    services or distributed experiments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rust: memory safety and performance without garbage collection, ideal for writing
    reliable numerical kernels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Metal (Apple): GPU acceleration on macOS/iOS, a must if you want to train or
    run models efficiently on Apple Silicon.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each ecosystem has strengths that make *llm.c*’s concepts more approachable
    or more production-ready.
  prefs: []
  type: TYPE_NORMAL
- en: Mapping the Core Components
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let’s look at how the key pieces of *llm.c* translate:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Component | C (llm.c) | Go Equivalent | Rust Equivalent | Metal Equivalent
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Memory allocation | `malloc`, `calloc` | `make`, slices, `unsafe.Pointer`
    | `Vec<T>`, `Box`, `unsafe` if needed | Buffers allocated on GPU |'
  prefs: []
  type: TYPE_TB
- en: '| Math kernels | manual loops, OpenMP | loops or cgo bindings to BLAS | loops
    with iterators, Rayon for CPU | Metal compute shaders |'
  prefs: []
  type: TYPE_TB
- en: '| Tokenizer | `fread` binary file | standard file I/O, `encoding/json` | `serde`,
    binary read | Preprocessing on CPU, feed to GPU |'
  prefs: []
  type: TYPE_TB
- en: '| Training loop | for-loops, structs | goroutines for dataloader + trainer
    | async tasks, channels | CPU driver, GPU kernels |'
  prefs: []
  type: TYPE_TB
- en: '| Parallelism | `#pragma omp` | goroutines + sync primitives | Rayon or explicit
    threads | Warp/thread groups in Metal |'
  prefs: []
  type: TYPE_TB
- en: 'Example: LayerNorm in Rust'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here’s a small Rust sketch of how a forward pass for LayerNorm might look:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE175]'
  prefs: []
  type: TYPE_PRE
- en: This looks strikingly similar to the C code, but benefits from Rust’s type safety
    and the absence of manual memory management bugs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Attention Kernel in Metal'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Metal would handle attention differently — you’d write a compute shader in
    `.metal` language:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE176]'
  prefs: []
  type: TYPE_PRE
- en: This isn’t a line-for-line port, but it shows how the concept — multiply queries
    with keys, apply softmax, weight values — remains the same while the implementation
    moves into GPU-land.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges You’ll Face
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Numerical libraries: C often leans on BLAS/LAPACK or just hand-written loops.
    In Go and Rust, you’ll either bind to these libraries or reimplement them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Performance portability: getting code that runs fast both on CPU and GPU isn’t
    trivial. What works in C with OpenMP won’t directly translate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tokenizer compatibility: making sure tokenization matches byte-for-byte is
    essential. One mismatch can ruin training reproducibility.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Porting *llm.c* forces you to understand what each piece of the code is doing
    — you can’t just rely on `torch.nn.LayerNorm` or `torch.nn.MultiheadAttention`.
    This makes it an excellent exercise for truly learning transformers, while also
    giving you practical implementations in different environments.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Reimplement one kernel (like LayerNorm or matmul) in Go or Rust. Test it against
    the C version.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a minimal Metal kernel that adds two vectors, then extend it to matrix
    multiplication.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build a Rust tokenizer reader that loads `gpt2_tokenizer.bin` and decodes IDs
    back into text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compare training speed: C with OpenMP vs. Rust with Rayon vs. Go with goroutines.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try porting just the forward pass first — inference is easier than training
    because you don’t need backprop.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The *llm.c* design is portable by nature — it doesn’t hide behind opaque frameworks.
    Porting it to Go, Rust, or Metal is not just about performance or language preference.
    It’s about proving to yourself that the transformer algorithm is universal, and
    you can implement it anywhere once you truly understand it.
  prefs: []
  type: TYPE_NORMAL
- en: 90\. Keeping the Repo Minimal and Clean
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the defining features of *llm.c* is its minimalism. The repository avoids
    the sprawling complexity of large frameworks and instead sticks to a small, readable
    core. This design choice isn’t an accident—it’s a philosophy. By keeping the codebase
    small and clean, contributors can focus on understanding the fundamentals of transformers
    rather than navigating thousands of lines of boilerplate.
  prefs: []
  type: TYPE_NORMAL
- en: The Philosophy of Minimalism
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Readability over performance: While production frameworks like PyTorch or TensorFlow
    optimize aggressively, *llm.c* intentionally trades some performance for clarity.
    A loop in plain C is easier to study than a chain of optimized CUDA calls hidden
    behind macros.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Portability: A smaller codebase can be ported more easily to new environments
    (Go, Rust, Metal) without pulling in dozens of dependencies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Learning-first design: Every line of code has a clear purpose. There are no
    abstractions “just in case.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This philosophy turns *llm.c* into both a working training framework and an
    educational resource.
  prefs: []
  type: TYPE_NORMAL
- en: How Minimalism Is Enforced
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Flat structure: The repository avoids deep directory hierarchies. Most files
    live directly under `llmc/` or `dev/`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'No external libraries unless critical: You’ll see OpenMP, cuBLAS, or cuDNN
    for performance, but not sprawling dependency chains.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'One feature, one file: Tokenization, dataloading, schedulers, and samplers
    each get their own small C file. This prevents “god files” where too much is lumped
    together.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Consistent naming: Functions and structs use clear, descriptive names (e.g.,
    `dataloader_next_batch`, `tokenizer_decode`) so readers don’t get lost.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Practical Examples
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If you open `train_gpt2.c`, you’ll find that it builds the model, initializes
    dataloaders, and runs the training loop. It doesn’t try to handle every possible
    model configuration, dataset format, or distributed scenario. Those belong in
    specialized files or external tools.
  prefs: []
  type: TYPE_NORMAL
- en: 'Likewise, `llmc/utils.h` only defines the absolute essentials: safe file I/O
    wrappers (`fopenCheck`, `freadCheck`) and memory allocators. It’s not bloated
    with generic helpers unrelated to training GPT-2.'
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A minimal repo lowers the barrier to entry. Beginners can trace execution from
    `main()` all the way to the optimizer update without detours. Researchers can
    fork the code and modify it without worrying about breaking dozens of interconnected
    modules. Even advanced developers benefit because the simplicity forces clarity
    in reasoning about algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Pick any file in the repo and count how many lines it has. Most are under a
    few hundred. Compare this with a similar file in PyTorch or TensorFlow.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try adding a new feature—say, a different activation function. Notice how easy
    it is to slot it in because the structure is clean.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explore what happens if you make the repo “heavier.” Add too many helpers, abstractions,
    or configs. Does it make the code harder to read?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Practice explaining the training loop to a friend. If the repo is simple, you
    should be able to walk them through without glossing over details.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Keeping *llm.c* minimal is not just about saving lines of code. It’s about preserving
    clarity, ensuring reproducibility, and making the repository a place where anyone—from
    curious learners to experienced engineers—can open a file and *understand what’s
    happening*. The simplicity is the point, and that’s what makes *llm.c* a rare
    and valuable resource in a world of bloated ML frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 10\. Reproduction, community and roadmap
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 91\. Reproducing GPT-2 124M on Single Node
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first major milestone for anyone exploring *llm.c* is to reproduce the training
    of GPT-2 124M, the smallest version of the GPT-2 family. This model has around
    124 million parameters, which is large enough to be interesting, but still small
    enough to train on a single modern GPU—or even slowly on CPU for demonstration
    purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Why Start with 124M?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'GPT-2 comes in multiple sizes: 124M, 355M, 774M, and 1.6B parameters. Training
    the largest requires clusters of GPUs and serious compute budgets. The 124M version,
    however, fits comfortably on consumer-grade GPUs like an NVIDIA 3090, and can
    even be run on a laptop CPU if you’re patient. It’s the “hello world” of transformer
    reproduction: small, approachable, and still real.'
  prefs: []
  type: TYPE_NORMAL
- en: What the Training Setup Looks Like
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Training GPT-2 124M involves a few key steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Model Configuration The config for 124M is baked into *llm.c* with 12 layers,
    12 heads, hidden dimension of 768, and max sequence length of 1024.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE177]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Dataset You can train on small datasets like Tiny Shakespeare or Tiny Stories
    for quick runs. For more realistic reproduction, you need something closer to
    OpenWebText. The data is tokenized with the GPT-2 tokenizer (`gpt2_tokenizer.bin`)
    and stored in `.bin` files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Batch Size and Sequence Length A common setting is `B = 8, T = 1024`, meaning
    8 sequences, each of length 1024 tokens. Adjust these based on available memory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Optimizer AdamW with a learning rate around `3e-4` is the default. Warmup and
    cosine decay scheduling can be enabled to match published GPT-2 training curves.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What to Expect in Practice
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: On CPU, training a single step may take several seconds. On a single GPU with
    CUDA, each step may take under 100 milliseconds. With 124M parameters, training
    from scratch on a dataset the size of OpenWebText still takes days, but you can
    reproduce key dynamics (loss curve, sample generations) on smaller datasets in
    hours.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, here’s the type of log you might see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE178]'
  prefs: []
  type: TYPE_PRE
- en: Even within a few hundred steps, the model begins to generate text resembling
    English rather than pure noise.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Reproducing GPT-2 124M is a confidence check. If your setup is correct, your
    loss curves should match those in the original OpenAI paper or the PyTorch reference
    implementation. This validates that *llm.c* is a faithful reproduction, not just
    a toy. It also teaches you how much compute and data go into even the smallest
    GPT-2 model, building intuition about scaling laws.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Train GPT-2 124M for 1000 steps on Tiny Shakespeare. Watch how the generated
    text improves.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change the batch size `B` from 8 to 4\. What happens to the speed and the stability
    of training?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run training on CPU vs. GPU. Compare how long each step takes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Track both training and validation loss. Notice how they diverge when the model
    begins to overfit.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The GPT-2 124M run is more than just a demo—it’s your gateway into real LLM
    training. You see how data, model size, optimizer, and hardware come together.
    Once you’ve mastered this reproduction, you’re ready to push toward larger models
    and more complex setups. It’s the foundation on which everything else in *llm.c*
    builds.
  prefs: []
  type: TYPE_NORMAL
- en: 92\. Reproducing GPT-2 355M (Constraints and Tricks)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once GPT-2 124M has been successfully reproduced, the next logical step is
    scaling up to GPT-2 355M. This version is roughly three times larger, with about
    355 million parameters. It introduces new challenges that don’t appear at the
    smaller scale: memory pressure, training stability, and compute cost.'
  prefs: []
  type: TYPE_NORMAL
- en: Model Configuration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The 355M model still uses 1024 tokens as the maximum sequence length and the
    same GPT-2 tokenizer. The difference is in the depth and width of the network:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Layers: 24 transformer blocks instead of 12'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hidden dimension (channels): 1024 instead of 768'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Heads: 16 instead of 12'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The total parameter count rises from ~124M to ~355M. That means not just three
    times more math per step, but also more memory needed for parameters, gradients,
    and optimizer state.
  prefs: []
  type: TYPE_NORMAL
- en: The Compute Challenge
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: With 124M, a single GPU with 8–12 GB of VRAM is enough. For 355M, you need at
    least 16 GB to run comfortably with sequence length 1024 and batch size of 8\.
    On smaller GPUs, you’ll quickly hit “CUDA out of memory” errors.
  prefs: []
  type: TYPE_NORMAL
- en: One trick is to reduce batch size (B) or sequence length (T). For example, instead
    of training with `(B=8, T=1024)`, you might use `(B=4, T=512)`. This halves the
    memory footprint but still lets you test scaling dynamics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another approach is to use gradient accumulation: simulate a larger batch size
    by running multiple small steps and accumulating gradients before updating.'
  prefs: []
  type: TYPE_NORMAL
- en: Training Stability
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Larger models are more sensitive to hyperparameters. The AdamW optimizer still
    works, but the learning rate schedule becomes more important. Many practitioners
    use:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Learning rate: ~3e-4 peak'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Warmup steps: a few thousand'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cosine decay: to taper the learning rate gradually'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you skip warmup, the larger model may diverge early (loss exploding instead
    of decreasing).
  prefs: []
  type: TYPE_NORMAL
- en: Tricks for Feasibility
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Mixed Precision Training (FP16 or BF16): Cuts memory use nearly in half. Supported
    in CUDA paths of *llm.c*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Activation Checkpointing: Save memory by recomputing activations during backpropagation.
    Slower, but lets you fit bigger models.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Smaller Dataset Runs: Train on Tiny Shakespeare or Tiny Stories to sanity-check
    the setup, then scale to OpenWebText-like data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Example Logs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Running GPT-2 355M for a short demo might look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE179]'
  prefs: []
  type: TYPE_PRE
- en: The loss drops more slowly than with 124M because the model has more capacity
    to learn, but also needs more data to generalize.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '355M is the first step into “medium-sized” LLMs. You start to feel the bottlenecks
    that dominate larger models: VRAM limits, training speed, and hyperparameter tuning.
    Solving these prepares you for the 774M and 1.6B experiments, where such problems
    become even more pronounced.'
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Train GPT-2 355M with batch size 4 and sequence length 512\. Record how long
    each step takes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Experiment with warmup steps: run once with warmup=0, and once with warmup=2000\.
    Compare stability.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enable mixed precision if you have a CUDA-capable GPU. Measure memory usage
    before and after.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try training on Tiny Shakespeare vs. Tiny Stories. Does the model overfit faster
    on the smaller dataset?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Reproducing GPT-2 355M is all about learning how to stretch limited resources.
    You’ll discover memory-saving tricks, the importance of learning rate schedules,
    and the role of data scale. It’s a practical exercise in resource management—just
    like the real challenges faced when training today’s billion-parameter models.
  prefs: []
  type: TYPE_NORMAL
- en: 93\. Reproducing GPT-2 774M (Scaling Up)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The 774M parameter version of GPT-2 is often called “GPT-2 Medium.” This is
    the point where training transitions from a personal experiment to a small-scale
    research project. It’s about six times larger than the 124M baseline, and roughly
    twice the size of 355M. Running it requires careful planning of hardware, memory,
    and software tricks.
  prefs: []
  type: TYPE_NORMAL
- en: Model Configuration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For 774M, the architecture expands again:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Layers (transformer blocks): 36'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hidden size (channels): 1280'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Attention heads: 20'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Maximum sequence length: 1024 (unchanged)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This jump in size increases both the parameter storage and the number of activations
    that must be kept during training. Optimizer states (AdamW’s *m* and *v* vectors)
    alone consume several gigabytes.
  prefs: []
  type: TYPE_NORMAL
- en: Hardware Requirements
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Running GPT-2 774M from scratch generally requires GPUs with 24 GB VRAM or more
    (e.g., NVIDIA RTX 3090/4090, A100, or H100). With smaller cards, you’ll almost
    certainly hit memory errors unless you aggressively reduce batch size and use
    techniques like activation checkpointing.
  prefs: []
  type: TYPE_NORMAL
- en: On CPUs, training is technically possible but far too slow to be practical—steps
    that take milliseconds on GPUs might take many seconds or even minutes.
  prefs: []
  type: TYPE_NORMAL
- en: Practical Constraints
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Batch Size: In practice, you may need to lower `B` to 2 or even 1 with sequence
    length 1024.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Gradient Accumulation: A must-have to simulate larger batch sizes and stabilize
    training.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Mixed Precision: FP16 or BF16 reduces memory by about half, without hurting
    convergence much.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Checkpointing: Recomputes intermediate results instead of storing them, trading
    time for memory.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training Dynamics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The loss curve for GPT-2 774M drops more steadily and requires much more data
    to reach its potential. If you only train on Tiny Shakespeare or Tiny Stories,
    it will quickly overfit: the model is too large for such a small dataset. For
    meaningful reproduction, you need a dataset similar in scale to OpenWebText.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Training logs for a sanity check run might look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE180]'
  prefs: []
  type: TYPE_PRE
- en: Notice that losses start higher (due to the larger random initialization space)
    but decrease predictably once training gets underway.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The 774M model is the sweet spot where scaling laws become obvious. Compared
    to 124M and 355M, it generalizes better, generates more fluent text, and demonstrates
    the benefits of parameter growth. But it also shows why infrastructure matters:
    without careful management, it’s nearly impossible to train this model on consumer-grade
    hardware.'
  prefs: []
  type: TYPE_NORMAL
- en: This is also where distributed training (covered in Chapter 8) becomes relevant,
    because one GPU is often not enough for efficient scaling.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Train GPT-2 774M with `(B=1, T=1024)` and gradient accumulation over 8 steps.
    Watch how it simulates a batch size of 8.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare training with FP32 vs. mixed precision. Measure both memory use and
    speed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run a short fine-tuning experiment on Tiny Stories. Observe how quickly the
    model memorizes the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the training and validation loss curves. Does the larger model overfit
    faster or slower than 355M?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Reproducing GPT-2 774M is about scaling into the territory of real research
    workloads. You face serious memory constraints, dataset requirements, and compute
    costs. But if you succeed, you’ll see firsthand why the machine learning community
    kept pushing toward billion-parameter models: larger networks unlock noticeably
    stronger capabilities, even with the same architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: 94\. Reproducing GPT-2 1.6B on 8×H100 (24h Run)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The largest GPT-2 model, with 1.6 billion parameters, represents the upper bound
    of the original GPT-2 family. Training this model from scratch is not something
    you can casually attempt on a single workstation. It demands cluster-scale resources,
    distributed training software, and careful tuning to keep everything stable. In
    this section, we’ll walk through what makes the 1.6B model special, what infrastructure
    is required, and how a full reproduction might look.
  prefs: []
  type: TYPE_NORMAL
- en: Model Configuration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The jump from 774M to 1.6B doubles the parameter count and makes the network
    both deeper and wider:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Layers (transformer blocks): 48'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hidden size (channels): 1600'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Attention heads: 25'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sequence length: still 1024'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With these dimensions, every forward and backward pass requires massive amounts
    of memory and compute. Just storing the parameters in FP32 takes around 6.4 GB.
    Once you add gradients, optimizer states (AdamW’s *m* and *v*), and activations,
    the memory footprint easily exceeds 100 GB.
  prefs: []
  type: TYPE_NORMAL
- en: Hardware Setup
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To reproduce this model realistically, you need access to high-end accelerators
    such as NVIDIA A100s or H100s. A common baseline is 8 GPUs with 80 GB each. With
    this setup, it is possible to train GPT-2 1.6B in under 24 hours, assuming efficient
    utilization.
  prefs: []
  type: TYPE_NORMAL
- en: Without multi-GPU, training is impractical. Even if you could somehow fit the
    model on one GPU, the runtime would be weeks or months.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed Training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The main strategy is data parallelism: each GPU processes a different mini-batch
    of data, and gradients are averaged across all devices with NCCL all-reduce. The
    code paths in *llm.c* support this via MPI integration, so you can scale from
    single-GPU to multi-node setups.'
  prefs: []
  type: TYPE_NORMAL
- en: The training loop looks nearly identical to smaller models, but behind the scenes,
    every parameter update is coordinated across devices.
  prefs: []
  type: TYPE_NORMAL
- en: Training Dynamics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The loss curve for 1.6B smooths out compared to smaller models. With enough
    data, the model continues to improve where 774M starts to plateau. This was one
    of the key insights from the original GPT-2 paper: scaling laws hold, and performance
    improves predictably with size, data, and compute.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Logs from a distributed run might look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE181]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice the speed: each step still takes only a few hundred milliseconds despite
    the massive size, thanks to parallelism across multiple H100s.'
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Reproducing GPT-2 1.6B is less about training a useful model today and more
    about understanding the scaling challenges of large language models. This exercise
    demonstrates how compute, memory, and distributed infrastructure become the limiting
    factors as models grow. It also shows why modern research labs design entire pipelines
    around multi-GPU and multi-node scaling.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Simulate a multi-GPU run with fewer resources by reducing the model size but
    using the same parallel training setup. For example, train GPT-2 124M across 2
    GPUs to practice the workflow.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Experiment with gradient accumulation to mimic large global batch sizes even
    on smaller clusters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try enabling and disabling mixed precision. Watch how memory use drops dramatically
    with FP16/BF16.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot validation loss curves for 124M, 355M, 774M, and 1.6B side by side. Notice
    how the larger models sustain improvements longer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The GPT-2 1.6B reproduction is the capstone project of *llm.c*. It forces you
    to combine everything you’ve learned: data pipelines, optimizers, schedulers,
    distributed training, and system-level debugging. While few people will actually
    train 1.6B themselves, understanding what it takes provides a window into the
    engineering behind state-of-the-art LLMs and prepares you to engage with even
    larger modern models.'
  prefs: []
  type: TYPE_NORMAL
- en: 95\. CPU-only Fine-Tune Demo (Tiny Shakespeare)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Not everyone has access to powerful GPUs or large compute clusters. One of the
    strengths of *llm.c* is that it provides a clean, minimal CPU-only path that lets
    you run real experiments—even if they are small and slow. A practical way to explore
    this is by fine-tuning GPT-2 on a small dataset like Tiny Shakespeare, which has
    only about 1 MB of text.
  prefs: []
  type: TYPE_NORMAL
- en: Why Tiny Shakespeare?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Tiny Shakespeare is a classic toy dataset in machine learning. It’s small enough
    to fit in memory, yet it contains a rich variety of words, characters, and structures.
    Fine-tuning GPT-2 on this dataset allows the model to mimic Shakespearean style
    in just a few thousand steps. It’s not about building a state-of-the-art model—it’s
    about seeing the training process work end-to-end on modest hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Setup
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The fine-tuning process uses the same `train_gpt2.c` CPU path, but with fewer
    steps, smaller batch sizes, and lower sequence lengths to keep things fast. A
    typical setup looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE182]'
  prefs: []
  type: TYPE_PRE
- en: 'The dataset is tokenized once using `gpt2_tokenizer.bin` and stored in binary
    `.bin` files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE183]'
  prefs: []
  type: TYPE_PRE
- en: These files are only a few megabytes, making them perfect for quick experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Training Dynamics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When you fine-tune on Tiny Shakespeare, the training logs may look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE184]'
  prefs: []
  type: TYPE_PRE
- en: Within a few hundred steps, the loss drops rapidly. By the time you’ve run 1000
    steps, the model starts producing text that looks recognizably Shakespearean—complete
    with archaic words, unusual punctuation, and rhythmic patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Example Output
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here’s a short sample from a fine-tuned run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE185]'
  prefs: []
  type: TYPE_PRE
- en: It isn’t perfect, but it captures the “feel” of Shakespeare, which is remarkable
    given the tiny dataset and limited compute.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The CPU-only Tiny Shakespeare demo proves that LLMs are not just for massive
    data centers. With a minimal setup, you can watch the model learn, generate text,
    and overfit to a dataset. This hands-on practice builds intuition about what training
    does, how loss curves behave, and why scaling up matters.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Change the sequence length from 64 to 128\. How does training speed and loss
    change?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reduce the number of training steps to 200\. Do you still see Shakespeare-like
    text in generation?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Increase the batch size to 8\. Does the loss curve become smoother or noisier?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fine-tune again but initialize from scratch (random weights). Compare the results
    to fine-tuning from pretrained GPT-2 124M.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Fine-tuning GPT-2 on Tiny Shakespeare with CPU-only training is a simple yet
    powerful demonstration. You don’t need GPUs to understand the mechanics of transformers.
    Even a modest laptop can teach you how training works, why overfitting happens,
    and how LLMs adapt to new domains. It’s a reminder that the best way to learn
    machine learning is by rolling up your sleeves and running experiments—even small
    ones.
  prefs: []
  type: TYPE_NORMAL
- en: 96\. Cost and Time Estimation for Runs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the most eye-opening parts of working with large language models is realizing
    how expensive and time-consuming training can be. While *llm.c* makes it possible
    to run models of all sizes with simple, minimal C code, the hardware requirements
    grow quickly as you scale from GPT-2 124M to GPT-2 1.6B. Understanding cost and
    time estimation helps set realistic expectations, whether you’re running on a
    laptop CPU, a single GPU, or a rented cluster of accelerators.
  prefs: []
  type: TYPE_NORMAL
- en: Key Factors Affecting Training Time
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Several components determine how long training takes and how much it costs:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model size (parameters): Larger models mean more multiplications per forward/backward
    pass, and more memory for parameters, gradients, and optimizer states.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Batch size and sequence length: Increasing either multiplies the amount of
    work per step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dataset size: Bigger datasets require more steps to complete one epoch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hardware speed: CPUs are far slower than GPUs; high-end GPUs like H100s can
    be 100× faster than CPUs for this workload.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Parallelism: Multi-GPU or multi-node setups let you divide the work, reducing
    time per step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rough Time Estimates
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here’s a simplified view of how long it might take to train GPT-2 models depending
    on hardware and setup. These are very approximate, assuming full training on a
    dataset like OpenWebText:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Parameters | Hardware | Time per Step | Total Training Time |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-2 124M | ~124M | Laptop CPU | 1–2 s | Months |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-2 124M | ~124M | Single RTX 3090 | ~100 ms | ~2–4 weeks |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-2 355M | ~355M | Single RTX 3090 | ~300 ms | ~6–8 weeks |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-2 774M | ~774M | 2× A100 40GB | ~200 ms | ~4–6 weeks |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-2 1.6B | ~1.6B | 8× H100 80GB | ~300 ms | ~24 hours |'
  prefs: []
  type: TYPE_TB
- en: These numbers show why scaling matters. The larger models don’t just need more
    compute per step—they also need more data to reach their potential. That means
    the total cost balloons unless you have a cluster of top-tier GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Cost in Cloud Environments
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If you run these experiments on cloud providers like AWS, GCP, or Azure, costs
    can add up quickly. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: An NVIDIA A100 40GB instance costs around $2–3 per hour (spot pricing can be
    cheaper).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training GPT-2 124M for a week might cost $500–1,000.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training GPT-2 1.6B for 24 hours on 8× H100s might cost $5,000–10,000, depending
    on the provider.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is why many researchers test code paths on small datasets and small models
    first, then only scale up when absolutely necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Estimating cost and time prevents frustration and wasted money. It teaches you
    to prototype at small scale (CPU or 124M runs), validate your setup, and only
    then scale up to medium (355M, 774M) or large (1.6B) models. It also gives you
    a realistic appreciation for the engineering and budget that went into OpenAI’s
    original GPT-2 training runs.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Time how long a single training step takes on your hardware for GPT-2 124M.
    Multiply by 1000 to estimate training time for 1000 steps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reduce the batch size by half. Does time per step decrease linearly, or not?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run a short fine-tune (e.g., 200 steps) and measure the electricity cost if
    you’re on a home machine.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use a cloud GPU for one hour. Compare the cost and speed to your local CPU.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Training large language models is a balancing act between ambition, hardware,
    and budget. *llm.c* gives you the tools to explore everything from toy demos to
    billion-parameter reproductions, but you’ll quickly see why the field has shifted
    toward big labs and shared infrastructure. With careful planning, though, you
    can still learn a tremendous amount by running smaller experiments and scaling
    them up thoughtfully.
  prefs: []
  type: TYPE_NORMAL
- en: 97\. Hyperparameter Sweeps (`sweep.sh`)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Getting a model like GPT-2 to train well isn’t just about writing the code
    or having enough compute—it’s also about finding the right hyperparameters. These
    include the learning rate, batch size, weight decay, dropout rate, and scheduler
    configuration. A setting that works well for one dataset or model size might completely
    fail for another. That’s where hyperparameter sweeps come in: systematically trying
    different configurations to see which ones give the best results.'
  prefs: []
  type: TYPE_NORMAL
- en: The Role of `sweep.sh`
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In *llm.c*, there’s a simple shell script called `sweep.sh` designed to automate
    hyperparameter testing. It’s not a complex experiment management system like Ray
    Tune or Optuna; instead, it’s lightweight, transparent, and easy to adapt. The
    script usually looks like a loop over different learning rates or batch sizes,
    running the training executable with each setting, and logging the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'A very simplified version might look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE186]'
  prefs: []
  type: TYPE_PRE
- en: This way, you can launch multiple experiments with a single command and later
    compare validation losses to decide which hyperparameters are best.
  prefs: []
  type: TYPE_NORMAL
- en: Why Sweeps Are Important
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Training a transformer is highly sensitive to hyperparameters. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: If the learning rate is too high, loss might explode.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If it’s too low, training will be painfully slow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Too much weight decay can hurt performance, while too little can cause overfitting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of warmup steps can make the difference between stable convergence
    and failure in the first few hundred iterations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead of guessing, sweeps let you see patterns. For instance, you might discover
    that `3e-4` is optimal for GPT-2 124M, but GPT-2 355M prefers `2e-4`.
  prefs: []
  type: TYPE_NORMAL
- en: Example of a Sweep in Practice
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Suppose you want to test three learning rates and two batch sizes. You can
    write a nested loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE187]'
  prefs: []
  type: TYPE_PRE
- en: Afterward, you could open the logs and compare validation loss at the end of
    each run. This gives you data-driven evidence about what works best.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Hyperparameter sweeps are a cornerstone of practical machine learning. Even
    though *llm.c* is a minimalist project, the ability to quickly test and compare
    runs is essential. It transforms training from guesswork into an empirical process.
    You don’t just hope your model will converge—you verify it across multiple settings
    and pick the winner.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Run a sweep over learning rates `[1e-3, 3e-4, 1e-4]` for GPT-2 124M on Tiny
    Shakespeare. Which one converges fastest?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try sweeping over sequence length (`T=32, 64, 128`). How does it affect speed
    and loss?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare runs with and without weight decay. Which generalizes better to the
    validation set?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extend the sweep to test different schedulers (cosine vs. step decay).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Hyperparameter sweeps are the “experimentation muscle” of training LLMs. They
    teach you that no single setting works everywhere and that systematic testing
    is far more effective than intuition alone. With a simple script like `sweep.sh`,
    you can explore dozens of setups in a reproducible way and build confidence that
    your model is training as well as it can.
  prefs: []
  type: TYPE_NORMAL
- en: 98\. Validating Evaluation and Loss Curves
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Training logs—loss values printed after each step—are just numbers. To really
    understand whether your model is learning, you need to validate those numbers,
    plot them, and compare them across runs. This process of analyzing evaluation
    and loss curves is one of the most important skills in machine learning. It’s
    how you know whether your model is converging, overfitting, or failing entirely.
  prefs: []
  type: TYPE_NORMAL
- en: Training Loss vs. Validation Loss
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are two kinds of loss curves to pay attention to:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Training loss: computed on the batches the model actually sees during training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Validation loss: computed on a held-out dataset (like `*_val.bin` in *llm.c*).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Training loss almost always goes down steadily. Validation loss is the real
    test: if it decreases alongside training loss, the model is learning useful patterns.
    If it stalls or increases while training loss keeps dropping, the model is overfitting.'
  prefs: []
  type: TYPE_NORMAL
- en: Plotting Loss Curves
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Even though *llm.c* is a pure C project, you can redirect its training logs
    to a file and then use tools like Python + matplotlib to visualize. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE188]'
  prefs: []
  type: TYPE_PRE
- en: 'Then parse `logs/run1.txt` in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE189]'
  prefs: []
  type: TYPE_PRE
- en: 'Adding validation loss to the same plot makes it even more useful: you’ll see
    both curves and their relationship over time.'
  prefs: []
  type: TYPE_NORMAL
- en: What a Healthy Curve Looks Like
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Early phase: Both training and validation loss drop quickly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Middle phase: Training loss continues downward, validation loss drops more
    slowly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Late phase: Training loss may keep decreasing, but validation loss stabilizes
    or begins to rise. That’s the point of overfitting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, a good curve might look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE190]'
  prefs: []
  type: TYPE_PRE
- en: Training loss keeps going down, but validation loss plateaus around step 500\.
    That’s a signal to stop training or adjust hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Loss curves are your window into model behavior. They reveal whether your model
    is underfitting (loss too high), overfitting (gap between train and val too large),
    or training stably (both losses decreasing together). Without them, you’re flying
    blind—just staring at numbers without context.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Train GPT-2 124M for 500 steps on Tiny Shakespeare and plot both training and
    validation loss.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reduce the dataset size by half. Watch how validation loss worsens earlier due
    to overfitting.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run two experiments with different learning rates. Plot both curves and compare
    stability.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extend plotting to multiple runs (e.g., GPT-2 124M vs. 355M) to see scaling
    effects.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Validating evaluation and loss curves turns raw logs into insight. It helps
    you decide when to stop training, how to tune hyperparameters, and whether scaling
    up is worth it. In *llm.c*, even though the project is minimal, capturing and
    plotting these curves is the single most effective way to understand what’s happening
    inside your model.
  prefs: []
  type: TYPE_NORMAL
- en: '99\. Future Work: Kernel Library, Less cuDNN Dependence'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The CUDA path in *llm.c* already uses cuBLAS and cuDNN, NVIDIA’s high-performance
    math libraries, to handle the heavy lifting of matrix multiplications and attention
    operations. These libraries are battle-tested and extremely fast, but they also
    act as a “black box”: you call into them, they do the work, and you get results
    without seeing what’s inside. While this is convenient, it limits flexibility
    and makes it harder to experiment with novel optimizations.'
  prefs: []
  type: TYPE_NORMAL
- en: That’s why one of the most exciting areas of future work is building a lightweight
    custom kernel library for *llm.c*. This would mean replacing parts of cuDNN with
    hand-written CUDA kernels for operations like attention, normalization, and activation
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: Why Reduce cuDNN Dependence?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Transparency: With custom kernels, you see exactly how operations are implemented,
    which is great for learning and debugging.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Flexibility: You can experiment with new ideas (e.g., alternative attention
    mechanisms, sparsity tricks) without waiting for cuDNN support.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Portability: cuDNN is NVIDIA-specific. A custom kernel library could make it
    easier to port *llm.c* to other backends, like AMD GPUs (HIP) or even Metal for
    Apple silicon.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Performance Tuning: For small to medium models, hand-tuned kernels can sometimes
    outperform generic library calls because they’re tailored to the workload.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What a Kernel Library Might Include
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A first version of a kernel library for *llm.c* might implement:'
  prefs: []
  type: TYPE_NORMAL
- en: Matrix multiplication (the workhorse of transformers) using tiling and shared
    memory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Softmax kernels with numerical stability built in.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LayerNorm kernels for both forward and backward passes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attention kernels that integrate matmul + masking + softmax in one fused operation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Activation functions like GELU or ReLU in fused forms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, a very simplified CUDA kernel for vector addition might look like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE191]'
  prefs: []
  type: TYPE_PRE
- en: 'While trivial, this illustrates the idea: instead of calling a library, you
    write the math yourself and control how threads are launched and memory is accessed.'
  prefs: []
  type: TYPE_NORMAL
- en: The Path Forward
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Developing a kernel library is a long-term effort. It requires profiling, benchmarking,
    and iterative tuning. At first, custom kernels may be slower than cuDNN, but over
    time, they can evolve into a compact, educational library of building blocks for
    transformer training.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Reducing dependence on cuDNN isn’t just about performance—it’s about control
    and portability. By having your own kernels, you gain the freedom to run *llm.c*
    on more platforms, test new research ideas, and understand exactly what’s happening
    inside the GPU. For a minimal, educational project like this one, that’s a natural
    next step.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Write a simple CUDA kernel for vector addition, like the one above, and call
    it from a C program. Compare its performance to `cublasSaxpy`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Replace one small piece of *llm.c* (e.g., softmax) with a custom kernel. Check
    if the outputs match cuDNN.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Benchmark a custom kernel against cuDNN on a small input size. Does cuDNN still
    dominate?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try porting your kernel to HIP (for AMD) or Metal (for Apple GPUs).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Building a kernel library is about moving from consumer of black-box libraries
    to creator of transparent, flexible tools. It’s a lot of work, but it transforms
    *llm.c* into not just a project for using LLMs, but also a platform for learning
    the deep internals of GPU programming. By reducing cuDNN dependence, you open
    the door to true end-to-end control of model training.
  prefs: []
  type: TYPE_NORMAL
- en: 100\. Community, GitHub Discussions, and Suggested Learning Path
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Large language models are complicated, but one of the goals of *llm.c* is to
    make them accessible. The code is clean, minimal, and approachable—but learning
    doesn’t stop at reading code. The broader community around *llm.c* plays a huge
    role in helping people understand, experiment, and grow. This section highlights
    where to connect with others, how to contribute, and how to build your own learning
    journey.
  prefs: []
  type: TYPE_NORMAL
- en: GitHub as the Hub
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The central place for *llm.c* discussions is the [GitHub repository](https://github.com/karpathy/llm.c).
    There, you’ll find:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Issues: where users ask questions, report bugs, or propose improvements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Discussions: an open forum for sharing results, asking “how do I…?” questions,
    and comparing training logs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pull Requests: contributions ranging from bug fixes to new features, often
    with valuable code reviews.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even just browsing issues and discussions can be an educational experience.
    Many questions you might have—about CUDA errors, dataset preparation, or optimizer
    quirks—have already been asked and answered.
  prefs: []
  type: TYPE_NORMAL
- en: The Value of Community
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Working alone on language models can feel overwhelming. By engaging with the
    community, you:'
  prefs: []
  type: TYPE_NORMAL
- en: See how others run experiments with different datasets and hardware.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn troubleshooting strategies for common problems (e.g., out-of-memory errors).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get inspiration for extensions—like custom kernels, new optimizers, or non-GPT
    architectures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find collaborators for experiments that go beyond what one person can do.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suggested Learning Path
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Because *llm.c* is minimal, it works well as a self-study tool. Here’s a suggested
    path to build up your knowledge:'
  prefs: []
  type: TYPE_NORMAL
- en: Start Small
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train GPT-2 124M on Tiny Shakespeare using CPU-only mode.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Inspect training logs and watch how loss decreases.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate text and see how quickly the model memorizes.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Step Into CUDA
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Switch to `train_gpt2.cu` and train with GPU acceleration.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Try mixed precision (`FP16`/`BF16`) and observe memory savings.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Scale Up
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Attempt GPT-2 355M or 774M on your hardware (or cloud GPUs).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn how to use gradient accumulation and checkpointing.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Experiment
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Modify the training loop: try new schedulers, tweak optimizer hyperparameters.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Add your own datasets (e.g., your personal text corpus).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore Internals
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Step through forward and backward passes in `train_gpt2.c`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Write small experiments to isolate key concepts (e.g., LayerNorm).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Join the Discussion
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Share your results on GitHub Discussions.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Contribute improvements, even small ones—like documentation fixes.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The journey of learning LLM internals isn’t just about reading code—it’s about
    active practice, asking questions, and comparing experiences with others. Community
    provides the feedback loop that accelerates learning and keeps motivation alive.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Clone the *llm.c* repository and explore open issues. Can you answer one for
    someone else?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run a training experiment and share your loss curve in GitHub Discussions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Contribute a small improvement (like a new dataset script) as a pull request.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create your own “learning log” to track experiments, much like a public notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Takeaway
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*llm.c* isn’t just a codebase—it’s an invitation to join a learning community.
    By engaging with GitHub, trying experiments, and sharing your results, you move
    from passive reader to active participant. That’s where the deepest understanding
    comes from: learning together, not alone.'
  prefs: []
  type: TYPE_NORMAL
