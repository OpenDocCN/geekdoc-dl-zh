- en: 9  Loss functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/loss_functions.html](https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/loss_functions.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The concept of a loss function is essential to machine learning. At any iteration,
    the current loss value indicates how far the estimate is from the target. It is
    then used to update the parameters in a direction that will decrease the loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our applied example, we already have made use of a loss function: mean squared
    error, computed manually as'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '*As you might expect, here is another area where this kind of manual effort
    is not needed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this final conceptual chapter before we re-factor our running examples,
    we want to talk about two things: First, how to make use of `torch`’s built-in
    loss functions. And second, what function to choose.'
  prefs: []
  type: TYPE_NORMAL
- en: 9.1 `torch` loss functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In `torch`, loss functions start with `nn_` or `nnf_`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using `nnf_`, you directly *call a function*. Correspondingly, its arguments
    (estimate and target) both are tensors. For example, here is `nnf_mse_loss()`,
    the built-in analog to what we coded manually:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE2]'
  prefs: []
  type: TYPE_NORMAL
- en: 'With `nn_`, in contrast, you create an object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '*This object can then be called on tensors to yield the desired loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE5]'
  prefs: []
  type: TYPE_NORMAL
- en: Whether to choose object or function is mainly a matter of preference and context.
    In larger models, you may end up combining several loss functions, and then, creating
    loss objects can result in more modular, and more maintainable code. In this book,
    I’ll mainly use the first way, unless there are compelling reasons to do otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: On to the second question.***  ***## 9.2 What loss function should I choose?
  prefs: []
  type: TYPE_NORMAL
- en: 'In deep learning, or machine learning overall, most applications aim to do
    one (or both) of two things: predict a numerical value, or estimate a probability.
    The regression task of our running example does the former; real-world applications
    might forecast temperatures, infer employee churn, or predict sales. In the second
    group, the prototypical task is *classification*. To categorize, say, an image
    according to its most salient content, we really compute the respective probabilities.
    Then, when the probability for “dog” is 0.7, while that for “cat” is 0.3, we say
    it’s a dog.'
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.1 Maximum likelihood
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In both classification and regression, the mostly used loss functions are built
    on the *maximum likelihood* principle. Maximum likelihood means: We want to choose
    model parameters in a way that the *data*, the things we have observed or could
    have observed, are maximally likely. This principle is not “just” fundamental,
    it is also intuitively appealing. Imagine a simple example.'
  prefs: []
  type: TYPE_NORMAL
- en: Say we have the values 7.1, 22.14, and 11.3, and we know that the underlying
    process follows a normal distribution. Then it is much more likely that these
    data have been generated by a distribution with mean 14 and standard deviation
    7 than by one with mean 20 and standard deviation 1.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.2 Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In regression (that implicitly assumes the target distribution to be normal[¹](#fn1)),
    to maximize likelihood, we just keep using mean squared error – the loss we’ve
    been computing all along. Maximum likelihood estimators have all kinds of desirable
    statistical properties. However, in concrete applications, there may be reasons
    to use different ones.
  prefs: []
  type: TYPE_NORMAL
- en: For example, say a dataset has outliers where, for some reason, prediction and
    target are found to be deviating substantially. Mean squared error will allocate
    high importance to these outliers. In such cases, possible alternatives are mean
    absolute error (`nnf_l1_loss()`) and smooth L1 loss (`nn_smooth_l1_loss()`). The
    latter is a mixture type that, by default, computes the absolute (L1) error, but
    switches to squared (L2) error whenever the absolute errors get very small.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.3 Classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In classification, we are comparing two *distributions*. The estimate is a probability
    by design, and the target can be viewed as one, too. In that light, maximum likelihood
    estimation is equivalent to minimizing the Kullback-Leibler divergence (KL divergence).
  prefs: []
  type: TYPE_NORMAL
- en: 'KL divergence is a measure of how two distributions differ. It depends on two
    things: the likelihood of the data, as determined by some data-generating process,
    and the likelihood of the data under the model. In the machine learning scenario,
    however, we are concerned only with the latter. In that case, the criterion to
    be minimized reduces to the *cross-entropy* between the two distributions. And
    cross-entropy loss is exactly what is commonly used in classification tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In `torch`, there are several variants of loss functions that calculate cross-entropy.
    With this topic, it’s nice to have a quick reference around; so here is a quick
    lookup table ([tbl. 9.1](#tbl-loss-funcs-features) abbreviates the – rather long-ish
    – function names; see [tbl. 9.2](#tbl-loss-abbrevs) for the mapping):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 9.1: Loss functions, by type of data they work on (binary vs. multi-class)
    and expected input (raw scores, probabilities, or log probabilities).'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Data** |  | **Input** |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | binary | multi-class | raw scores | probabilities | log probs |'
  prefs: []
  type: TYPE_TB
- en: '| *BCeL* | Y |  | Y |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| *Ce* |  | Y | Y |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| *BCe* | Y |  |  | Y |  |'
  prefs: []
  type: TYPE_TB
- en: '| *Nll* |  | Y |  |  | Y |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9.2: Abbreviations used to refer to `torch` loss functions.'
  prefs: []
  type: TYPE_NORMAL
- en: '| *BCeL* | `nnf_binary_cross_entropy_with_logits()` |'
  prefs: []
  type: TYPE_TB
- en: '| *Ce* | `nnf_cross_entropy()` |'
  prefs: []
  type: TYPE_TB
- en: '| *BCe* | `nnf_binary_cross_entropy()` |'
  prefs: []
  type: TYPE_TB
- en: '| *Nll* | `nnf_nll_loss()` |'
  prefs: []
  type: TYPE_TB
- en: To pick the function applicable to your use case, there are two things to consider.
  prefs: []
  type: TYPE_NORMAL
- en: First, are there just two possible classes (“dog vs. cat”, “person present /
    person absent”, etc.), or are there several?
  prefs: []
  type: TYPE_NORMAL
- en: And second, what is the type of the estimated values? Are they raw scores (in
    theory, any value between plus and minus infinity)? Are they probabilities (values
    between 0 and 1)? Or (finally) are they log probabilities, that is, probabilities
    to which a logarithm has been applied? (In the final case, all values should be
    either negative or equal to zero.)
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.3.1 Binary data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Starting with binary data, our example classification vector is a sequence of
    zeros and ones. When thinking in terms of probabilities, it is most intuitive
    to imagine the ones standing for presence, the zeros for absence of one of the
    classes in question – cat or no cat, say.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '*The raw scores could be anything. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '*To turn these into probabilities, all we need to do is pass them to `nnf_sigmoid()`.
    `nnf_sigmoid()` squishes its argument to values between zero and one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE9]'
  prefs: []
  type: TYPE_NORMAL
- en: From the above table, we see that given `unnormalized_estimate` and `probability_estimate`,
    we can use both as inputs to a loss function – but we have to choose the appropriate
    one. Provided we do that, the output has to be the same in both cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see (raw scores first):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE11]'
  prefs: []
  type: TYPE_NORMAL
- en: 'And now, probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE13]'
  prefs: []
  type: TYPE_NORMAL
- en: That worked as expected. What does this mean in practice? It means that when
    we build a model for binary classification, and the final layer computes an un-normalized
    score, we don’t need to attach a sigmoid layer to obtain probabilities. We can
    just call `nnf_binary_cross_entropy_with_logits()` when training the network.
    In fact, doing so is the preferred way, also due to reasons of numerical stability.*****  ***####
    9.2.3.2 Multi-class data
  prefs: []
  type: TYPE_NORMAL
- en: 'Moving on to multi-class data, the most intuitive framing now really is in
    terms of (several) *classes*, not presence or absence of a single class. Think
    of classes as class indices (maybe indexing into some look-up table). Being indices,
    technically, classes start at 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '*In the multi-class scenario, raw scores are a two-dimensional tensor. Each
    row contains the scores for one observation, and each column corresponds to one
    of the classes. Here’s how the raw estimates could look:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '*As per the above table, given this estimate, we should be calling `nnf_cross_entropy()`
    (and we will, when below we compare results).'
  prefs: []
  type: TYPE_NORMAL
- en: So that’s the first option, and it works exactly as with binary data. For the
    second, there is an additional step.
  prefs: []
  type: TYPE_NORMAL
- en: First, we again turn raw scores into probabilities, using `nnf_softmax()`. For
    most practical purposes, `nnf_softmax()` can be seen as the multi-class equivalent
    of `nnf_sigmoid()`. Strictly though, their effects are not the same. In a nutshell,
    `nnf_sigmoid()` treats low-score and high-score values equivalently, while `nnf_softmax()`
    exacerbates the distances between the top score and the remaining ones (“winner
    takes all”).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE17]'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second step, the one that was not required in the binary case, consists
    in transforming the probabilities to log probabilities. In our example, this could
    be accomplished by calling `torch_log()` on the `probability_estimate` we just
    computed. Alternatively, both steps together are taken care of by `nnf_log_softmax()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE19]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have estimates in both possible forms, we can again compare results
    from applicable loss functions. First, `nnf_cross_entropy()` on the raw scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE21]'
  prefs: []
  type: TYPE_NORMAL
- en: 'And second, `nnf_nll_loss()` on the log probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE23]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Application-wise, what was said for the binary case applies here as well: In
    a multi-class classification network, there is no need to have a softmax layer
    at the end.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we end this chapter, let’s address a question that might have come to
    mind. Is not binary classification a sub-type of the multi-class setup? Should
    we not, in that case, arrive at the same result, whatever the method chosen?******  ***####
    9.2.3.3 Check: Binary data, multi-class method'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see. We re-use the binary-classification scenario employed above. Here
    it is again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE25]'
  prefs: []
  type: TYPE_NORMAL
- en: 'We hope to get the same value doing things the multi-class way. We already
    have the probabilities (namely, `probability_estimate`); we just need to put them
    into the “observation by class” format expected by `nnf_nll_loss()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '*Now, we still want to apply the logarithm. And there is one other thing to
    be taken care of: In the binary setup, classes were coded as probabilities (either
    0 or 1); now, we’re dealing with indices. This means we add 1 to the `target`
    tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '*Finally, we can call `nnf_nll_loss()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE29]'
  prefs: []
  type: TYPE_NORMAL
- en: There we go. The results are indeed the same.**********  **** * *
  prefs: []
  type: TYPE_NORMAL
- en: For cases where that assumption seems unlikely, distribution-adequate loss functions
    are provided (e.g., Poisson negative log likelihood, available as `nnf_poisson_nll_loss()`
    .[↩︎](#fnref1)*******
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
