["```py\n`# Import the required modules from the Kubeflow Pipelines SDK from  kfp  import dsl, compiler  # Suppress FutureWarning messages from the Kubeflow Pipelines SDK import  warnings warnings.filterwarnings(\"ignore\", category=FutureWarning, module='kfp.*')` \n```", "```py\n`from  kfp  import dsl  # Define a simple component that adds two numbers @dsl.component def  add_numbers(num1: int, num2: int) -> int:     return num1 + num2` \n```", "```py\n`import  warnings warnings.filterwarnings(\"ignore\", category=DeprecationWarning)` \n```", "```py\n`from  kfp  import dsl  # Component that generates a fixed number @dsl.component def  generate_number() -> int:     return 42  # Component that doubles the input number @dsl.component def  double_number(input_number: int) -> int:     return input_number * 2  # Define a pipeline that connects two components @dsl.pipeline(     name=\"Number doubling pipeline\",     description=\"A pipeline that generates a number and doubles it.\" ) def  number_doubling_pipeline():     # Step 1: Generate a number     generated_number_task = generate_number()      # Step 2: Double the generated number     double_number_task = double_number(input_number=generated_number_task.output)` \n```", "```py\n`from  kfp  import compiler  # Assume the pipeline definition is named number_doubling_pipeline pipeline_func = number_doubling_pipeline  # Compile the pipeline compiler.Compiler().compile(     pipeline_func=pipeline_func,     package_path='number_doubling_pipeline.yaml' )` \n```", "```py\n`# This is a hypothetical function that cannot be executed as‑is. It is intended to illustrate the concept. def  handle_pipeline_task():     # Hypothetical call to a component function named my_component     # In a real scenario, this should occur inside a pipeline function     task = my_component(param1=\"value\")      # Access the component’s output     # This line is illustrative and typically used to pass outputs between components in a pipeline     output = task.output      print(\"Accessing the component output:\", output)  # Note: In real usage, my_component would be defined as a Kubeflow Pipeline component, # and task manipulations should occur within the context of a pipeline function.` \n```", "```py\n`from  kfp  import dsl  # Incorrect pipeline definition @dsl.pipeline(     name='Incorrect Pipeline',     description='An example that attempts to return a PipelineTask object directly.' ) def  incorrect_pipeline_example():     @dsl.component     def  generate_number() -> int:         return 42      generated_number_task = generate_number()     # Incorrect attempt to return a PipelineTask object directly     return generated_number_task  # This will cause an error  # Correct pipeline definition @dsl.pipeline(     name='Correct Pipeline',     description='A corrected example that does not attempt to return a PipelineTask object.' ) def  correct_pipeline_example():     @dsl.component     def  generate_number() -> int:         return 42      generated_number_task = generate_number()     # Correct approach: do not attempt to return a PipelineTask directly from a pipeline function.     # A pipeline function should not return anything.  # Explanation: a pipeline function orchestrates steps and data flow, but does not return data directly. # Attempting to return a PipelineTask from a pipeline function is incorrect, because the pipeline definition # should describe component structure and dependencies, not process data directly. # The corrected version removes the return statement, which matches the expected behavior of pipeline functions.` \n```", "```py\n`import  json  # Simulated data preparation for model training def  preprocess_data(input_file_path, output_file_path):     # Read data from a JSON file     with open(input_file_path, 'r') as infile:         data = json.load(infile)      # Perform a simple transformation: filter data     # For illustration, assume we only need items meeting a certain condition     # Example: filter items where the value of \"useful\" is True     filtered_data = [item for item in data if item.get(\"useful\", False)]      # Save the transformed data to another JSON file     with open(output_file_path, 'w') as outfile:         json.dump(filtered_data, outfile, indent=4)  # Example usage preprocess_data('input_data.json', 'processed_data.json')  # Note: This script assumes the file 'input_data.json' exists in the current directory # and will save processed data to 'processed_data.json'. # In a real scenario, paths and transformation logic should be adjusted to your requirements.` \n```", "```py\n`from  datetime  import datetime  def  generate_model_name(base_model_name: str) -> str:     # Generate a timestamp in the format \"YYYYMMDD-HHMMSS\"     timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")     # Append the timestamp to the base model name to create a unique name     model_name = f\"{base_model_name}-{timestamp}\"     return model_name  # Example usage base_model_name = \"my_model\" model_name = generate_model_name(base_model_name) print(\"Generated model name:\", model_name)  # This function generates a unique model name by appending the current date and time to the base model name. # This practice helps with model versioning, making it easier to track and manage different model versions in ML operations.` \n```", "```py\n``# Assume the necessary imports and configuration for interacting with the execution environment are present  def  submit_pipeline_execution(compiled_pipeline_path: str, pipeline_arguments: dict):     # Placeholder for the API/SDK method to submit a pipeline for execution     # In a real scenario, this would use the Kubeflow Pipelines SDK or a cloud provider SDK     # For example, using the Kubeflow Pipelines SDK or a cloud service like Google Cloud AI Platform Pipelines      # Assume a function `submit_pipeline_job` exists and can be used to submit     # This function would be part of the SDK or the environment’s API     submit_pipeline_job(compiled_pipeline_path, pipeline_arguments)  # Example pipeline arguments pipeline_arguments = {     \"recipient_name\": \"Alice\" }  # Path to the compiled Kubeflow pipeline YAML file compiled_pipeline_path = \"path_to_compiled_pipeline.yaml\"  # Submit the pipeline for execution submit_pipeline_execution(compiled_pipeline_path, pipeline_arguments)  # Note: This example assumes a `submit_pipeline_job` function exists, which will be specific # to the environment’s API or SDK. In a real implementation, replace this placeholder # with actual code that interacts with the Kubeflow Pipelines API or a managed service API, such as Google Cloud AI Platform.`` \n```"]