<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><h2 class="h2" id="ch01"><span class="ash">1</span></h2>
<h2 class="h2a">Overview of Large Language Models</h2>
<p>Ever since an advanced artificial intelligence (AI) deep learning model called the Transformer was introduced by a team at Google Brain in 2017, it has become the standard for tackling various natural language processing (NLP) tasks in academia and industry. It is likely that you have interacted with a Transformer model today without even realizing it, as Google uses BERT to enhance its search engine by better understanding users’ search queries. The GPT family of models from OpenAI have also received attention for their ability to generate human-like text and images.</p>
<p>These Transformers now power applications such as GitHub’s Copilot (developed by OpenAI in collaboration with Microsoft), which can convert comments and snippets of code into fully functioning source code that can even call upon other LLMs (like in <a href="ch01.html#list1_1">Listing 1.1</a>) to perform NLP tasks.</p>
<p class="ex-caption" id="list1_1"><strong>Listing 1.1</strong> Using the Copilot LLM to get an output from Facebook’s BART LLM</p>
<div class="pre-box">
<pre><code>from transformers import pipeline

def classify_text(email):
    """
    Use Facebook's BART model to classify an email into "spam" or "not spam"  

    Args:
        email (str): The email to classify
    Returns:
        str: The classification of the email
    """
    <strong># COPILOT START. EVERYTHING BEFORE THIS COMMENT WAS INPUT TO BART</strong>
    classifier = pipeline(
        'zero-shot-classification', model='facebook/bart-large-mnli')
    labels = ['spam', 'not spam']
    hypothesis_template = 'This email is {}.'

    results = classifier(
        email, labels, hypothesis_template=hypothesis_template)

    return results['labels'][0]
    <strong># COPILOT END</strong></code></pre>
</div>
<p>In this listing, I use Copilot to take in only a Python function definition and some comments I wrote and wrote all of the code to make the function do what I wrote. No cherry-picking here, just a fully working python function that I can call like this:</p>
<pre class="pre"><code>classify_text('hi I am spam')  # spam</code></pre>
<p>It appears we are surrounded by LLMs, but just what are they doing under the hood? Let’s find out!</p>
<h3 class="h3" id="ch01lev1sec1">What Are Large Language Models (LLMs)?</h3>
<p><strong>Large language models</strong> (LLMs) are AI models that are usually (but not necessarily) derived from the Transformer architecture and are designed to <em>understand</em> and <em>generate</em> human language, code, and much more. These models are trained on vast amounts of text data, allowing them to capture the complexities and nuances of human language. LLMs can perform a wide range of language tasks, from simple text classification to text generation, with high accuracy, fluency, and style.</p>
<p>In the healthcare industry, LLMs are being used for electronic medical record (EMR) processing, clinical trial matching, and drug discovery. In finance, LLMs are being utilized for fraud detection, sentiment analysis of financial news, and even trading strategies. LLMs are also used for customer service automation via chatbots and virtual assistants. With their versatility and highly performant natures, Transformer-based LLMs are becoming an increasingly valuable asset in a variety of industries and applications.</p>
<div class="note">
<p class="note-title"><strong>Note</strong></p>
<p class="note-para">I will use the term <strong>understand</strong> a fair amount in this text. I am usually referring to “Natural Language Understanding” (NLU) which is a research branch of NLP that focuses on developing algorithms and models that can accurately interpret human language. As we will see, NLU models excel at tasks such as classification, sentiment analysis, and named entity recognition. However, it is important to note that while these models can perform complex language tasks, they do not possess true understanding in the way humans do.</p>
</div>
<p>The success of LLMs and Transformers is due to the combination of several ideas. Most of these ideas had been around for years but were also being actively researched around the same time. Mechanisms such as attention, transfer learning, and scaling up neural networks which provide the scaffolding for Transformers were seeing breakthroughs right around the same time. <a href="ch01.html#ch01fig01">Figure 1.1</a> outlines some of the biggest advancements in NLP in the last few decades, all leading up to the invention of the Transformer.</p>
<div class="group">
<div class="image" id="ch01fig01"><img src="graphics/01fig01.jpg" alt="Images" width="728" height="375"/></div>
<p class="fig-caption"><strong>Figure 1.1</strong> <em>A brief history of Modern NLP highlights using deep learning to tackle language modeling, advancements in large scale semantic token embeddings (Word2vec), sequence to sequence models with attention (something we will see in more depth later in this chapter), and finally the Transformer in 2017.</em></p>
</div>
<p>The Transformer architecture itself is quite impressive. It can be highly parallelized and scaled in ways that previous state of the art NLP models could not be, allowing it to scale to much larger data sets and training times than previous NLP models. The Transformer uses a special kind of attention calculation called <strong>self-attention</strong> to allow each word in a sequence to “attend to” (look to for context) all other words in the sequence, enabling it to capture long-range dependencies and contextual relationships between words. Of course, no architecture is perfect. Transformers are still limited to an input context window which represents the maximum length of text it can process at any given moment.</p>
<p>Since the advent of the Transformer in 2017, the ecosystem around using and deploying Transformers has only exploded. The aptly named “Transformers” library and its supporting packages have made it accessible for practitioners to use, train, and share models, greatly accelerating its adoption and being used by thousands of organizations and counting. Popular LLM repositories like Hugging Face have popped up, providing access to powerful open-source models to the masses. In short, using and productionizing a Transformer has never been easier.</p>
<p>That’s where this book comes in.</p>
<p>My goal is to guide you on how to use, train, and optimize all kinds of LLMs for practical applications while giving you just enough insight into the inner workings of the model to know how to make optimal decisions about model choice, data format, fine-tuning parameters, and so much more.</p>
<p>My aim is to make using Transformers accessible for software developers, data scientists, analysts, and hobbyists alike. To do that, we should start on a level playing field and learn a bit more about LLMs.</p>
<h4 class="h4" id="ch01lev2sec1">Definition of LLMs</h4>
<p>To back up only slightly, we should talk first about the specific NLP task that LLMs and Transformers are being used to solve and provides the foundation layer for their ability to solve a multitude of tasks. <strong>Language modeling</strong> is a subfield of NLP that involves the creation of statistical/deep learning models for predicting the likelihood of a sequence of tokens in a specified <strong>vocabulary</strong> (a limited and known set of tokens). There are generally two kinds of language modeling tasks out there: autoencoding tasks and autoregressive tasks <a href="ch01.html#ch01fig02">Figure 1.2</a>)</p>
<div class="note">
<p class="note-title"><strong>Note</strong></p>
<p class="note-para">The term <strong>token</strong> refers to the smallest unit of semantic meaning created by breaking down a sentence or piece of text into smaller units and are the basic inputs for an LLM. Tokens can be words but also can be “sub-words” as we will see in more depth throughout this book. Some readers may be familiar with the term “n-gram” which refers to a sequence of n consecutive tokens.</p>
</div>
<div class="group">
<div class="image" id="ch01fig02"><img src="graphics/01fig02.jpg" alt="Images" width="734" height="418"/></div>
<p class="fig-caption"><strong>Figure 1.2</strong> <em>Both the autoencoding and autoregressive language modeling task involves filling in a missing token but only the autoencoding task allows for context to be seen on both sides of the missing token.</em></p>
</div>
<p><strong>Autoregressive</strong> language models are trained to predict the next token in a sentence, based only on the previous tokens in the phrase. These models correspond to the decoder part of the transformer model, and a mask is applied to the full sentence so that the attention heads can only see the tokens that came before. Autoregressive models are ideal for text generation and a good example of this type of model is GPT.</p>
<p><strong>Autoencoding</strong> language models are trained to reconstruct the original sentence from a corrupted version of the input. These models correspond to the encoder part of the transformer model and have access to the full input without any mask. Autoencoding models create a bidirectional representation of the whole sentence. They can be fine-tuned for a variety of tasks such as text generation, but their main application is sentence classification or token classification. A typical example of this type of model is BERT.</p>
<p>To summarize, Large Language Models (LLMs) are language models that are either autoregressive, autoencoding, or a combination of the two. Modern LLMs are usually based on the Transformer architecture which is what we will use but they can be based on another architecture. The defining feature of LLMs is their large size and large training datasets which enables them to perform complex language tasks, such as text generation and classification, with high accuracy and with little to no fine-tuning.</p>
<p><a href="ch01.html#ch01tab01">Table 1.1</a> shows the disk size, memory usage, number of parameters, and approximate size of the pre-training data for several popular large language models (LLMs). Note that these sizes are approximate and may vary depending on the specific implementation and hardware used.</p>
<div class="group">
<p class="tab-caption"><strong>Table 1.1 Comparison of Popular Large Language Models (LLMs)</strong></p>
<div class="imaget" id="ch01tab01"><img src="graphics/01tab01.jpg" alt="Images" width="751" height="471"/></div>
</div>
<p>But size is everything. Let’s look at some of the key characteristics of LLMs and then dive into how LLMs learn to read and write.</p>
<h4 class="h4" id="ch01lev2sec2">Key Characteristics of LLMs</h4>
<p>The original Transformer architecture, as devised in 2017, was a <strong>sequence-to-sequence model</strong>, which means it had two main components:</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> An <strong>encoder</strong> which is tasked with taking in raw text, splitting them up into its core components (more on this later), converting them into vectors (similar to the Word2vec process), and using attention to <em>understand</em> the context of the text</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> A <strong>decoder</strong> which excels at <em>generating</em> text by using a modified type of attention to predict the next best token</p>
<p>As shown in <a href="ch01.html#ch01fig03">Figure 1.3</a>, The Transformer has many other sub-components that we won’t get into that promotes faster training, generalizability, and better performance. Today’s LLMs are for the most part variants of the original Transformer. Models like BERT and GPT dissect the Transformer into only an encoder and decoder (respectively) in order to build models that excel in understanding and generating (also respectively).</p>
<div class="group">
<div class="image" id="ch01fig03"><img src="graphics/01fig03.jpg" alt="Images" width="667" height="428"/></div>
<p class="fig-caption"><strong>Figure 1.3</strong> <em>The original Transformer has two main components: an encoder which is great at understanding text, and a decoder which is great at generating text. Putting them together makes the entire model a “sequence to sequence” model.</em></p>
</div>
<p>In general, LLMs can be categorized into three main buckets:</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> <strong>Autoregressive models</strong>, such as GPT, which predict the next token in a sentence based on the previous tokens. They are effective at generating coherent free-text following a given context</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> <strong>Autoencoding models</strong>, such as BERT, which build a bidirectional representation of a sentence by masking some of the input tokens and trying to predict them from the remaining ones. They are adept at capturing contextual relationships between tokens quickly and at scale which make them great candidates for text classification tasks for example.</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> <strong>Combinations</strong> of autoregressive and autoencoding, like T5, which can use the encoder and decoder to be more versatile and flexible in generating text. It has been shown that these combination models can generate more diverse and creative text in different contexts compared to pure decoder-based autoregressive models due to their ability to capture additional context using the encoder.</p>
<div class="group">
<div class="image" id="ch01fig04"><img src="graphics/01fig04.jpg" alt="Images" width="739" height="541"/></div>
<p class="fig-caption"><strong>Figure 1.4</strong> <em>A breakdown of the key characteristics of LLMs based on how they are derived from the original Transformer architecture.</em></p>
</div>
<p><a href="ch01.html#ch01fig04">Figure 1.4</a> shows the breakdown of the key characteristics of LLMs based on these three buckets.</p>
<h5 class="h5" id="ch01lev3sec1">More Context Please</h5>
<p>No matter how the LLM is constructed and what parts of the Transformer it is using, they all care about context (<a href="ch01.html#ch01fig05">Figure 1.5</a>). The goal is to understand each token as it relates to the other tokens in the input text. Beginning with the popularity of Word2vec around 2013, NLP practitioners and researchers were always curious about the best ways of combining semantic meaning (basically word definitions) and context (with the surrounding tokens) to create the most meaningful token embeddings possible. The Transformer relies on the attention calculation to make this combination a reality.</p>
<div class="group">
<div class="image" id="ch01fig05"><img src="graphics/01fig05.jpg" alt="Images" width="712" height="251"/></div>
<p class="fig-caption"><strong>Figure 1.5</strong> <em>LLMs are great at understanding context. The word “Python” can have different meanings depending on the context. We could be talking about a snake, or a pretty cool coding language.</em></p>
</div>
<p>Choosing what kind of Transformer derivation you want isn’t enough. Just choosing the encoder doesn’t mean your Transformer is magically good at understanding text. Let’s take a look at how these LLMs actually learn to read and write.</p>
<h4 class="h4" id="ch01lev2sec3">How LLMs Work</h4>
<p>How an LLM is pre-trained and fine-tuned makes all the difference between an alright performing model and something state of the art and highly accurate. We’ll need to take a quick look into how LLMs are pre-trained to understand what they are good at, what they are bad at, and whether or not we would need to update them with our own custom data.</p>
<h5 class="h5" id="ch01lev3sec2">Pre-training</h5>
<p>Every LLM on the market has been <strong>pre-trained</strong> on a large corpus of text data and on specific language modeling related tasks. During pre-training, the LLM tries to learn and understand general language and relationships between words. Every LLM is trained on different corpora and on different tasks.</p>
<p>BERT, for example, was originally pre-trained on two publicly available text corpora (<a href="ch01.html#ch01fig06">Figure 1.6</a>):</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> <strong>English Wikipedia</strong> - a collection of articles from the English version of Wikipedia, a free online encyclopedia. It contains a range of topics and writing styles, making it a diverse and representative sample of English language text</p>
<p class="bullet-sub">• At the time 2.5 billion words.</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> <strong>The BookCorpus</strong> - a large collection of fiction and non-fiction books. It was created by scraping book text from the web and includes a range of genres, from romance and mystery to science fiction and history. The books in the corpus were selected to have a minimum length of 2000 words and to be written in English by authors with verified identities</p>
<p class="bullet-sub">• 800M words.</p>
<p>and on two specific language modeling specific tasks (<a href="ch01.html#ch01fig07">Figure 1.7</a>):</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> The Masked Language Modeling (MLM) task (AKA the autoencoding task)—this helps BERT recognize token interactions within a single sentence.</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> The Next Sentence Prediction Task—this helps BERT understand how tokens interact with each other between sentences.</p>
<div class="group">
<div class="image" id="ch01fig06"><img src="graphics/01fig06.jpg" alt="Images" width="744" height="501"/></div>
<p class="fig-caption"><strong>Figure 1.6</strong> <em>BERT was originally pre-trained on English Wikipedia and the BookCorpus. More modern LLMs are trained on datasets thousands of times larger.</em></p>
</div>
<div class="group">
<div class="image" id="ch01fig07"><img src="graphics/01fig07.jpg" alt="Images" width="699" height="239"/></div>
<p class="fig-caption"><strong>Figure 1.7</strong> <em>BERT was pre-trained on two tasks: the autoencoding language modeling task (referred to as the “masked language modeling” task) to teach it individual word embeddings and the “next sentence prediction” task to help it learn to embed entire sequences of text.</em></p>
</div>
<p>Pre-training on these corpora allowed BERT (mainly via the self-attention mechanism) to learn a rich set of language features and contextual relationships. The use of large, diverse corpora like these has become a common practice in NLP research, as it has been shown to improve the performance of models on downstream tasks.</p>
<div class="note">
<p class="note-title"><strong>Note</strong></p>
<p class="note-para">The pre-training process for an LLM can evolve over time as researchers find better ways of training LLMs and phase out methods that don’t help as much. For example within a year of the original Google BERT release that used the Next Sentence Prediction (NSP) pre-training task, a BERT variant called RoBERTa (yes, most of these LLM names will be fun) by Facebook AI was shown to not require the NSP task to match and even beat the original BERT model’s performance in several areas.</p>
</div>
<p>Depending on which LLM you decide to use, it will likely be pre-trained differently from the rest. This is what sets LLMs apart from each other. Some LLMs are trained on proprietary data sources including OpenAI’s GPT family of models in order to give their parent companies an edge over their competitors.</p>
<p>We will not revisit the idea of pre-training often in this book because it’s not exactly the “quick” part of a “quick start guide” but it can be worth knowing how these models were pre-trained because it’s because of this pre-training that we can apply something called transfer learning to let us achieve the state-of-the-art results we want, which is a big deal!</p>
<h5 class="h5" id="ch01lev3sec3">Transfer Learning</h5>
<p>Transfer learning is a technique used in machine learning to leverage the knowledge gained from one task to improve performance on another related task. Transfer learning for LLMs involves taking an LLM that has been pre-trained on one corpus of text data and then fine-tuning it for a specific “downstream” task, such as text classification or text generation, by updating the model’s parameters with task-specific data.</p>
<p>The idea behind transfer learning is that the pre-trained model has already learned a lot of information about the language and relationships between words, and this information can be used as a starting point to improve performance on a new task. Transfer learning allows LLMs to be fine-tuned for specific tasks with much smaller amounts of task-specific data than it would require if the model were trained from scratch. This greatly reduces the amount of time and resources required to train LLMs. <a href="ch01.html#ch01fig08">Figure 1.8</a> provides a visual representation of this relationship.</p>
<div class="group">
<div class="image" id="ch01fig08"><img src="graphics/01fig08.jpg" alt="Images" width="696" height="283"/></div>
<p class="fig-caption"><strong>Figure 1.8</strong> <em>The general transfer learning loop involves pre-training a model on a generic dataset on some generic self-supervised task and then fine-tuning the model on a task-specific dataset.</em></p>
</div>
<h5 class="h5" id="ch01lev3sec4">Fine-tuning</h5>
<p>Once a LLM has been pre-trained, it can be fine-tuned for specific tasks. Fine-tuning involves training the LLM on a smaller, task-specific dataset to adjust its parameters for the specific task at hand. This allows the LLM to leverage its pre-trained knowledge of the language to improve its accuracy for the specific task. Fine-tuning has been shown to drastically improve performance on domain-specific and task-specific tasks and lets LLMs adapt quickly to a wide variety of NLP applications.</p>
<p><a href="ch01.html#ch01fig09">Figure 1.9</a> shows the basic fine-tuning loop that we will use for our models in later chapters. Whether they are open-sourced or closed-sourced the loop is more or less the same:</p>
<p class="numbera">1. We define the model we want to fine-tune as well as any fine-tuning parameters (e.g., learning rate)</p>
<p class="numbera">2. We will aggregate some training data (the format and other characteristics depend on the model we are updating)</p>
<p class="numbera">3. We compute losses (a measure of error) and gradients (information about how to change the model to minimize error)</p>
<p class="numbera">4. We update the model through backpropagation – a mechanism to update model parameters to minimize errors</p>
<p>If some of that went over your head, not to worry: we will rely on pre-built tools from Hugging Face’s Transformers package (<a href="ch01.html#ch01fig09">Figure 1.9</a>) and OpenAI’s Fine-tuning API to abstract away a lot of this so we can really focus on our data and our models.</p>
<div class="note">
<p class="note-title"><strong>Note</strong></p>
<p class="note-para">You will not need a Hugging Face account or key to follow along and use any of this code apart from very specific advanced exercises where I will call it out.</p>
</div>
<div class="group">
<div class="image" id="ch01fig09"><img src="graphics/01fig09.jpg" alt="Images" width="736" height="313"/></div>
<p class="fig-caption"><strong>Figure 1.9</strong> <em>The Transformers package from Hugging Face provides a neat and clean interface for training and fine-tuning LLMs.</em></p>
</div>
<h5 class="h5" id="ch01lev3sec5">Attention</h5>
<p>The name of the original paper that introduced the Transformer was called “Attention is all you need”. <strong>Attention</strong> is a mechanism used in deep learning models (not just Transformers) that assigns different weights to different parts of the input, allowing the model to prioritize and emphasize the most important information while performing tasks like translation or summarization. Essentially, attention allows a model to “focus” on different parts of the input dynamically, leading to improved performance and more accurate results. Before the popularization of attention, most neural networks processed all inputs equally and the models relied on a fixed representation of the input to make predictions. Modern LLMs that rely on attention can dynamically focus on different parts of input sequences, allowing them to weigh the importance of each part in making predictions.</p>
<p>To recap, LLMs are pre-trained on large corpora and sometimes fine-tuned on smaller datasets for specific tasks. Recall that one of the factors behind the Transformer’s effectiveness as a language model is that it is highly parallelizable, allowing for faster training and efficient processing of text. What really sets the Transformer apart from other deep learning architectures is its ability to capture long-range dependencies and relationships between tokens using attention. In other words, attention is a crucial component of Transformer-based LLMs, and it enables them to effectively retain information between training loops and tasks (i.e. transfer learning), while being able to process lengthy swatches of text with ease.</p>
<p>Attention is attributed for being the most responsible for helping LLMs learn (or at least recognize) internal world models and human-identifiable rules. A Stanford study in 2019 showed that certain attention calculations in BERT corresponded to linguistic notions of syntax and grammar rules. For example, they noticed that BERT was able to notice direct objects of verbs, determiners of nouns, and objects of prepositions with remarkably high accuracy from only its pre-training. These relationships are presented visually in <a href="ch01.html#ch01fig010">Figure 1.10</a>.</p>
<p>There is research that explores what other kinds of “rules” LLMs are able to learn simply by pre-training and fine-tuning. One example is a series of experiments led by researchers at Harvard that explored an LLM’s ability to learn a set of rules to a synthetic task like the game of Othello (<a href="ch01.html#ch01fig011">Figure 1.11</a>). They found evidence that an LLM was able to understand the rules of the game simply by training on historical move data.</p>
<div class="group">
<div class="image" id="ch01fig010"><img src="graphics/01fig10.jpg" alt="Images" width="732" height="675"/></div>
<p class="fig-caption"><strong>Figure 1.10</strong> <em>Research has probed into LLMs to uncover that they seem to be recognizing grammatical rules even when they were never explicitly told these rules.</em></p>
</div>
<div class="group">
<div class="image" id="ch01fig011"><img src="graphics/01fig11.jpg" alt="Images" width="760" height="413"/></div>
<p class="fig-caption"><strong>Figure 1.11</strong> <em>LLMs may be able to learn all kinds of things about the world, whether it be the rules and strategy of a game or the rules of human language.</em></p>
</div>
<p>For any LLM to learn any kind of rule, however, it has to convert what we perceive as text into something machine readable. This is done through a process called embedding.</p>
<h5 class="h5" id="ch01lev3sec6">Embeddings</h5>
<p>Embeddings are the mathematical representations of words, phrases, or tokens in a large-dimensional space. In NLP, embeddings are used to represent the words, phrases, or tokens in a way that captures their semantic meaning and relationships with other words. There are several types of embeddings, including position embeddings, which encode the position of a token in a sentence, and token embeddings, which encode the semantic meaning of a token (<a href="ch01.html#ch01fig012">Figure 1.12</a>).</p>
<div class="group">
<div class="image" id="ch01fig012"><img src="graphics/01fig12.jpg" alt="Images" width="720" height="334"/></div>
<p class="fig-caption"><strong>Figure 1.12</strong> <em>An example of how BERT uses three layers of embedding for a given piece of text. Once the text is tokenized, each token is given an embedding and then the values are added up, so each token ends up with an initial embedding before any attention is calculated. We won’t focus too much on the individual layers of LLM embeddings in this text unless they serve a more practical purpose but it is good to know about some of these parts and how they look under the hood!</em></p>
</div>
<p>LLMs learn different embeddings for tokens based on their pre-training and can further update these embeddings during fine-tuning.</p>
<h5 class="h5" id="ch01lev3sec7">Tokenization</h5>
<p>Tokenization, as mentioned previously, involves breaking text down into the smallest unit of understanding - tokens. These tokens are the pieces of information that are embedded into semantic meaning and act as inputs to the attention calculations which leads to ... well, the LLM actually learning and working. Tokens make up an LLMs static vocabulary and don’t always represent entire words. Tokens can represent punctuation, individual characters, or even a sub-word if a word is not known to the LLM. Nearly all LLMs also have <em>special tokens</em> that have specific meaning to the model. For example, the BERT model has a few special tokens including the <strong>[CLS]</strong> token which BERT automatically injects as the first token of every input and is meant to represent an encoded semantic meaning for the entire input sequence.</p>
<p>Readers may be familiar with techniques like stop words removal, stemming, and truncation which are used in traditional NLP. These techniques are not used nor are they necessary for LLMs. LLMs are designed to handle the inherent complexity and variability of human language, including the usage of stop words like “the” and “an” and variations in word forms like tenses and misspellings. Altering the input text to an LLM using these techniques could potentially harm the performance of the model by reducing the contextual information and altering the original meaning of the text.</p>
<p>Tokenization can also involve several preprocessing steps like <strong>casing</strong>, which refers to the capitalization of the tokens. There are two types of casing: uncased and cased. In uncased tokenization, all the tokens are lowercased and usually accents from letters are stripped, while in cased tokenization, the capitalization of the tokens is preserved. The choice of casing can impact the performance of the model, as capitalization can provide important information about the meaning of a token. An example of this can be found in <a href="ch01.html#ch01fig013">Figure 1.13</a>.</p>
<div class="note">
<p class="note-title"><strong>Note</strong></p>
<p class="note-para">It is worth mentioning that even the concept of casing has some bias to it depending on the model. To uncase a text - lowercasing and stripping of accents - is a pretty Western style preprocessing step. I myself speak Turkish and know that the umlaut (e.g. the Ö in my last name) matters and can actually help the LLM understand the word being said. Any language model that has not been sufficiently trained on diverse corpora may have trouble parsing and utilizing these bits of context.</p>
</div>
<div class="group">
<div class="image" id="ch01fig013"><img src="graphics/01fig13.jpg" alt="Images" width="698" height="237"/></div>
<p class="fig-caption"><strong>Figure 1.13</strong> <em>The choice of uncased versus cased tokenization depends on the task. Simple tasks like text classification usually prefer uncased tokenization while tasks that derive meaning from case like Named Entity Recognition prefer a cased tokenization.</em></p>
</div>
<p><a href="ch01.html#ch01fig014">Figure 1.14</a> shows an example of tokenization, and in particular, an example of how LLMs tend to handle Out of Vocabulary (OOV) phrases. OOV phrases are simply phrases/words that the LLM doesn’t recognize as a token and has to split up into smaller sub-words. For example, my name (Sinan) is not a token in most LLMs (story of my life) so in BERT, the tokenization scheme will split my name up into two tokens (assuming uncased tokenization):</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> sin - the first part of my name</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> ##an - a special sub-word token that is different from the word “an” and is used only as a means to split up unknown words</p>
<div class="group">
<div class="image" id="ch01fig014"><img src="graphics/01fig14.jpg" alt="Images" width="707" height="214"/></div>
<p class="fig-caption"><strong>Figure 1.14</strong> <em>Any LLM has to deal with words they’ve never seen before. How an LLM tokenizes text can matter if we care about the token limit of an LLM.</em></p>
</div>
<p>Some LLMs limit the number of tokens we can input at any one time so how an LLM tokenizes text can matter if we are trying to be mindful about this limit.</p>
<p>So far, we have talked a lot about language modeling - predicting missing/next tokens in a phrase, but modern LLMs also can also borrow from other fields of AI to make their models more performant and more importantly more <strong>aligned</strong> - meaning that the AI is performing in accordance with a human’s expectation. Put another way, an aligned LLM has an objective that matches a human’s objective.</p>
<h5 class="h5" id="ch01lev3sec8">Beyond Language Modeling—Alignment + RLHF</h5>
<p><strong>Alignment</strong> in language models refers to how well the model can respond to input prompts that match the user’s expectations. Standard language models predict the next word based on the preceding context, but this can limit their usefulness for specific instructions or prompts. Researchers are coming up with scalable and performant ways of aligning language models to a user’s intent. One such broad method of aligning language models is through the incorporation of reinforcement learning (RL) into the training loop.</p>
<p><strong>RL with Human Feedback</strong> (RLHF) is a popular method of aligning pre-trained LLMs that uses human feedback to enhance their performance. It allows the LLM to learn from feedback on its own outputs from a relatively small, high-quality batch of human feedback, thereby overcoming some of the limitations of traditional supervised learning. RLHF has shown significant improvements in modern LLMs like ChatGPT. RLHF is one example of approaching alignment with RL, but there are other emerging approaches like RL with AI feedback (e.g. Constitutional AI).</p>
<p>Let’s take a look at some of the popular LLMs we’ll be using in this book.</p>
<h3 class="h3" id="ch01lev1sec2">Popular Modern LLMs</h3>
<p>BERT, T5, and GPT are three popular LLMs developed by Google, Google, and OpenAI respectively. These models differ in their architecture pretty greatly even though they all share the Transformer as a common ancestor. Other widely used variants of LLMs in the Transformer family include RoBERTa, BART (which we saw earlier performing some text classification), and ELECTRA.</p>
<h4 class="h4" id="ch01lev2sec4">BERT</h4>
<p>BERT (<a href="ch01.html#ch01fig015">Figure 1.15</a>) is an autoencoding model that uses attention to build a bidirectional representation of a sentence, making it ideal for sentence classification and token classification tasks.</p>
<div class="group">
<div class="image" id="ch01fig015"><img src="graphics/01fig15.jpg" alt="Images" width="726" height="226"/></div>
<p class="fig-caption"><strong>Figure 1.15</strong> <em>BERT was one of the first LLMs and continues to be popular for many NLP tasks that involve fast processing of large amounts of text.</em></p>
</div>
<p>BERT uses the encoder of the Transformer and ignores the decoder to become exceedingly good at processing/understanding massive amounts of text very quickly relative to other, slower LLMs that focus on generating text one token at a time. BERT-derived architectures, therefore, are best for working with and analyzing large corpora quickly when we don’t need to write free text.</p>
<p>BERT itself doesn’t classify text or summarize documents but it is often used as a pre-trained model for downstream NLP tasks. BERT has become a widely used and highly regarded LLM in the NLP community, paving the way for the development of even more advanced language models.</p>
<h4 class="h4" id="ch01lev2sec5">GPT-3 and ChatGPT</h4>
<p>GPT (<a href="ch01.html#ch01fig016">Figure 1.16</a>), on the other hand, is an autoregressive model that uses attention to predict the next token in a sequence based on the previous tokens. The GPT family of algorithms (including ChatGPT and GPT-3) is primarily used for text generation and has been known for its ability to generate natural sounding human-like text.</p>
<div class="group">
<div class="image" id="ch01fig016"><img src="graphics/01fig16.jpg" alt="Images" width="744" height="382"/></div>
<p class="fig-caption"><strong>Figure 1.16</strong> <em>The GPT family of models excels at generating free text aligned with a user’s intent.</em></p>
</div>
<p>GPT relies on the decoder portion of the Transformer and ignores the encoder to become exceptionally good at generating text one token at a time. GPT-based models are best for generating text given a rather large context window. They can also be used to process/understand text as we will see in an upcoming chapter. GPT-derived architectures are ideal for applications that require the ability to freely write text.</p>
<h4 class="h4" id="ch01lev2sec6">T5</h4>
<p>T5 is a pure encoder/decoder transformer model that was designed to perform several NLP tasks, from text classification to text summarization and generation, right off the shelf. It is one of the first popular models to be able to boast such a feat, in fact. Before T5, LLMs like BERT and GPT-2 generally had to be fine-tuned using labeled data before they could be relied on to perform such specific tasks.</p>
<p>T5 uses both the encoder and decoder of the Transformer to become highly versatile in both processing and generating text. T5-based models can perform a wide range of NLP tasks, from text classification to text generation, due to their ability to build representations of the input text using the encoder and generate text using the decoder (<a href="ch01.html#ch01fig017">Figure 1.17</a>). T5-derived architectures are ideal for applications that require both the ability to process and understand text and generate text freely.</p>
<div class="group">
<div class="image" id="ch01fig017"><img src="graphics/01fig17.jpg" alt="Images" width="719" height="345"/></div>
<p class="fig-caption"><strong>Figure 1.17</strong> <em>T5 was one of the first LLMs to show promise in solving multiple tasks at once without any fine-tuning.</em></p>
</div>
<p>T5’s ability to perform multiple tasks with no fine-tuning spurred the development of other versatile LLMs that can perform multiple tasks with efficiency and accuracy with little/no fine-tuning. GPT-3, released around the same time at T5, also boasted this ability.</p>
<p>These three LLMs are highly versatile and are used for various NLP tasks, such as text classification, text generation, machine translation, and sentiment analysis, among others. These three LLMs, along with flavors (variants) of them will be the main focus of this book and our applications.</p>
<h3 class="h3" id="ch01lev1sec3">Domain-Specific LLMs</h3>
<p>Domain-specific LLMs are LLMs that are trained specifically in a particular subject area, such as biology or finance. Unlike general-purpose LLMs, these models are designed to understand the specific language and concepts used within the domain they were trained on.</p>
<p>One example of a domain-specific LLM is BioGPT (<a href="ch01.html#ch01fig018">Figure 1.18</a>); a domain-specific LLM that is pre-trained on large-scale biomedical literature. The model was developed by the AI healthcare company, Owkin, in collaboration with Hugging Face. The model is trained on a dataset of over 2 million biomedical research articles, making it highly effective for a wide range of biomedical NLP tasks such as named entity recognition, relationship extraction, and question-answering.</p>
<div class="group">
<div class="image" id="ch01fig018"><img src="graphics/01fig18.jpg" alt="Images" width="722" height="361"/></div>
<p class="fig-caption"><strong>Figure 1.18</strong> <em>BioGPT is a domain-specific Transformer model pre-trained on large-scale biomedical literature. BioGPT’s success in the biomedical domain has inspired other domain-specific LLMs such as SciBERT and BlueBERT.</em></p>
</div>
<p>BioGPT, whose pre-training encoded biomedical knowledge and domain-specific jargon into the LLM, can be fine-tuned on smaller datasets, making it adaptable for specific biomedical tasks and reducing the need for large amounts of labeled data.</p>
<p>The advantage of using domain-specific LLMs lies in their training on a specific set of texts. This allows them to better understand the language and concepts used within their specific domain, leading to improved accuracy and fluency for NLP tasks that are contained within that domain. By comparison, general-purpose LLMs may struggle to handle the language and concepts used in a specific domain as effectively.</p>
<h3 class="h3" id="ch01lev1sec4">Applications of LLMs</h3>
<p>As we’ve already seen, applications of LLMs vary widely and researchers continue to find novel applications of LLMs to this day. We will use LLMs in this book in generally three ways:</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> Using a pre-trained LLM’s underlying ability to process and generate text with no further fine-tuning as part of a larger architecture.</p>
<p class="bullet-sub"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> For example, creating an information retrieval system using a pre-trained BERT/GPT.</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> Fine-tuning a pre-trained LLM to perform a very specific task using Transfer Learning.</p>
<p class="bullet-sub"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> For example, fine-tuning T5 to create summaries of documents in a specific domain/industry.</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> Asking a pre-trained LLM to solve a task it was pre-trained to solve or could reasonably intuit.</p>
<p class="bullet-sub"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> For example, prompting GPT3 to write a blog post.</p>
<p class="bullet-sub"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> For example, prompting T5 to perform language translation..</p>
<p>These methods use LLMs in different ways and while all options take advantage of an LLM’s pre-training, only option 2 requires any fine-tuning. Let’s take a look at some specific applications of LLMs.</p>
<h4 class="h4" id="ch01lev2sec7">Classical NLP Tasks</h4>
<p>A vast majority of applications of LLMs are delivering state of the art results in very common NLP tasks like classification and translation. It’s not that we weren’t solving these tasks before Transformers and LLMs, it’s just that now developers and practioners can solve them with comparatively less labeled data (due to the efficient pre-training of the Transformer on huge corpora) and with a higher degree of accuracy.</p>
<h5 class="h5" id="ch01lev3sec9">Text Classification</h5>
<p>The text classification task assigns a label to a given piece of text. This task is commonly used in sentiment analysis, where the goal is to classify a piece of text as positive, negative, or neutral, or in topic classification, where the goal is to classify a piece of text into one or more predefined categories. Models like BERT can be fine-tuned to perform classification with relatively little labeled data as seen in <a href="ch01.html#ch01fig019">Figure 1.19</a>.</p>
<div class="group">
<div class="image" id="ch01fig019"><img src="graphics/01fig19.jpg" alt="Images" width="682" height="387"/></div>
<p class="fig-caption"><strong>Figure 1.19</strong> <em>A peek at the architecture of using BERT to achieve fast and accurate text classification results. Classification layers usually act on that special [CLS] token that BERT uses to encode the semantic meaning of the entire input sequence.</em></p>
</div>
<p>Text classification remains one of the most globally recognizable and solvable NLP tasks because when it comes down to it, sometimes we just need to know whether this email is “spam” or not and get on with our days!</p>
<h5 class="h5" id="ch01lev3sec10">Translation Tasks</h5>
<p>A harder and yet still classic NLP task is machine translation where the goal is to automatically translate text from one language to another while preserving meaning and context. Traditionally, this task is quite difficult because it involves having sufficient examples and domain knowledge of both languages to accurately gauge how well the model is doing but modern LLMs seem to have an easier time with this task again due to their pre-training and efficient attention calculations.</p>
<h5 class="h5" id="ch01lev3sec11">Human Language &lt;&gt; Human Language</h5>
<p>One of the first applications of attention even before Transformers was for machine translation tasks where AI models were expected to translate from one human language to another. T5 was one of the first LLMs to tout the ability to perform multiple tasks off the shelf (<a href="ch01.html#ch01fig020">Figure 1.20</a>). One of these tasks was the ability to translate English into a few languages and back.</p>
<div class="group">
<div class="image" id="ch01fig020"><img src="graphics/01fig20.jpg" alt="Images" width="738" height="248"/></div>
<p class="fig-caption"><strong>Figure 1.20</strong> <em>T5 could perform many NLP tasks off the shelf, including grammar correction, summarization, and translation.</em></p>
</div>
<p>Since T5, language translation in LLMs has only gotten better and more diverse. Models like GPT-3 and the latest T5 models can translate between dozens of languages with relative ease. Of course this bumps up against one major known limitation of LLMs that they are mostly trained from an English-speaking/usually American point of view so most LLMs can handle English well and non-English languages, well, not as well.</p>
<h5 class="h5" id="ch01lev3sec12">SQL Generation</h5>
<p>If we consider SQL as a language, then converting English to SQL is really not that different from converting English to French (<a href="ch01.html#ch01fig021">Figure 1.21</a>). Modern LLMs can already do this at a basic level off the shelf, but more advanced SQL queries often require some fine-tuning.</p>
<div class="group">
<div class="image" id="ch01fig021"><img src="graphics/01fig21.jpg" alt="Images" width="677" height="311"/></div>
<p class="fig-caption"><strong>Figure 1.21</strong> <em>Using GPT-3 to generate functioning SQL code from an (albeit simple) Postgres schema</em></p>
</div>
<p>If we expand our thinking of what can be considered a “translation” then a lot of new opportunities lie ahead of us. For example, what if we wanted to “translate” between English and a series of wavelengths that a brain might interpret and execute as motor functions. I’m not a neuro-scientist or anything, but that seems like a fascinating area of research!</p>
<h4 class="h4" id="ch01lev2sec8">Free Text Generation</h4>
<p>What first caught the world’s eye in terms of modern LLMs like ChatGPT was their ability to freely write blogs, emails, and even academic papers. This notion of text generation is why many LLMs are affectionately referred to as “Generative AI”, although that term is a bit reductive and imprecise. I will not often use the term “Generative AI” as the specific word “generative” has its own meaning in machine learning as the analogous way of learning to a “discriminative” model. For more on that, check out my first book: The Principles of Data Science)</p>
<p>We could for example prompt (ask) ChatGPT to help plan out a blog post like in <a href="ch01.html#ch01fig022">Figure 1.22</a>. Even if you don’t agree with the results, this can help humans with the “tabula rasa” problem and give us something to at least edit and start from rather than staring at a blank page for too long.</p>
<div class="group">
<div class="image" id="ch01fig022"><img src="graphics/01fig22.jpg" alt="Images" width="622" height="619"/></div>
<p class="fig-caption"><strong>Figure 1.22</strong> <em>ChatGPT can help ideate, scaffold, and even write entire blog posts</em></p>
</div>
<div class="note">
<p class="note-title"><strong>Note</strong></p>
<p class="note-para">I would be remiss if I didn’t mention the controversy that LLMs like this can cause at the academic level. Just because an LLM can write entire blogs or even essays doesn’t mean we should let them. Just like how the internet caused some to believe that we’d never need books again, some argue that ChatGPT means that we’ll never need to write anything again. As long as institutions are aware of how to use this technology and proper regulations/rules are put in place, students and teachers alike can use ChatGPT and other text-generation-focused AIs safely and ethically.</p>
</div>
<p>We will be using ChatGPT to solve a few tasks in this book. We will rely on ChatGPT’s ability to contextualize information in its context window and freely write back (usually) accurate responses. We will mostly be interacting with ChatGPT through the Playground and the API provided by OpenAI as this model is not open source.</p>
<h4 class="h4" id="ch01lev2sec9">Information Retrieval / Neural Semantic Search</h4>
<p>LLMs encode information directly into their parameters via pre-training and fine-tuning but keeping them up to date with new information is tricky. We either have to further fine-tune the model on new data or run the pre-training steps again from scratch. To dynamically keep information fresh, we will architect our own information retrieval system with a vector database (don’t worry we will go into more details on all of this in the next chapter). <a href="ch01.html#ch01fig023">Figure 1.23</a> shows an outline of the architecture we will build.</p>
<div class="group">
<div class="image" id="ch01fig023"><img src="graphics/01fig23.jpg" alt="Images" width="720" height="230"/></div>
<p class="fig-caption"><strong>Figure 1.23</strong> <em>Our neural semantic search system will be able to take in new information dynamically and be able to retrieve relevant documents quickly and accurately given a user’s query using LLMs.</em></p>
</div>
<p>We will then add onto this system by building a ChatGPT-based chatbot to conversationally answer questions from our users.</p>
<h4 class="h4" id="ch01lev2sec10">Chatbots</h4>
<p>Everyone loves a good chatbot, right? Well, whether you love them or hate them, LLMs’ capacity for holding a conversation is evident through systems like ChatGPT and even GPT-3 (as seen in <a href="ch01.html#ch01fig024">Figure 1.24</a>). The way we architect chatbots using LLMs will be quite different from the traditional way of designing chatbots through intents, entities, and tree-based conversation flows. These concepts will be replaced by system prompts, context, and personas – all of which we will dive into in the coming chapters.</p>
<div class="group">
<div class="image" id="ch01fig024"><img src="graphics/01fig24.jpg" alt="Images" width="736" height="450"/></div>
<p class="fig-caption"><strong>Figure 1.24</strong> <em>ChatGPT isn’t the only LLM that can hold a conversation. We can use GPT-3 to construct a simple conversational chatbot. The text highlighted in green represents GPT-3’s output. Note that before the chat even begins, I inject context to GPT-3 that would not be shown to the end-user but GPT-3 needs to provide accurate responses.</em></p>
</div>
<p>We have our work cut out for us. I’m excited to be on this journey with you and I’m excited to get started!</p>
<h3 class="h3" id="ch01lev1sec5">Summary</h3>
<p>LLMs are advanced AI models that have revolutionized the field of NLP. LLMs are highly versatile and are used for a variety of NLP tasks, including text classification, text generation, and machine translation. They are pre-trained on large corpora of text data and can then be fine-tuned for specific tasks.</p>
<p>Using LLMs in this fashion has become a standard step in the development of NLP models. In our first case study, we will explore the process of launching an application with proprietary models like GPT-3 and ChatGPT. We will get a hands-on look at the practical aspects of using LLMs for real-world NLP tasks, from model selection and fine-tuning to deployment and maintenance.</p>
</div>
</div>
</body></html>