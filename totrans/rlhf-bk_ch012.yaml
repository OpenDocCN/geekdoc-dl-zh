- en: Direct Alignment Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Direct Alignment Algorithms (DAAs) allow one to update models to solve the same
    RLHF objective, shown again in eq.¬†[68](ch012.xhtml#eq:review_rlhf), without ever
    training an intermediate reward model or using reinforcement learning optimizers.
    It solves the same preference learning problem we‚Äôve been studying (with literally
    the same data!), in order to make language models more aligned, smarter, and easier
    to use. The lack of a reward model and online optimization makes DAAs far simpler
    to implement, reducing compute spent during training and making experimentation
    easier. This chapter details the complex mathematics that were done to derive
    these algorithms, and then shows that the sometimes tedious derivations result
    in simple implementations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most prominent DAA and one that catalyzed an entire academic movement of
    aligning language models is Direct Preference Optimization (DPO) [[20]](ch021.xhtml#ref-rafailov2024direct).
    At its core, DPO is using gradient ascent to solve the same constrained RLHF objective
    (see Chapter 4):'
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><munder><mi mathvariant="normal">max</mi><mi>œÄ</mi></munder><msub><mi>ùîº</mi><mrow><mi>x</mi><mo>‚àº</mo><mi>ùíü</mi></mrow></msub><msub><mi>ùîº</mi><mrow><mi>y</mi><mo>‚àº</mo><mi>œÄ</mi><mo
    stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow></msub><mrow><mo stretchy="true"
    form="prefix">[</mo><msub><mi>r</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow><mo>‚àí</mo><mi>Œ≤</mi><msub><mi>ùíü</mi><mtext
    mathvariant="normal">KL</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>œÄ</mi><mo
    stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">‚à•</mo><msub><mi>œÄ</mi><mtext
    mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo
    stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo
    stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="false" form="prefix">(</mo><mn>68</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\max_{\pi}
    \mathbb{E}_{x \sim \mathcal{D}}\mathbb{E}_{y \sim \pi(y|x)} \left[r_\theta(x,
    y)\right] - \beta \mathcal{D}_{\text{KL}}\left(\pi(y|x) \| \pi_{\text{ref}}(y|x)\right)\qquad{(68)}</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: Since its release in May of 2023, after a brief delay where the community figured
    out the right data and hyperparameters to use DPO with (specifically, surprisingly
    low learning rates), many popular models have used DPO or its variants, from Zephyr-<semantics><mi>Œ≤</mi><annotation
    encoding="application/x-tex">\beta</annotation></semantics> kickstarting it in
    October of 2023 [[21]](ch021.xhtml#ref-tunstall2023zephyr), Llama 3 Instruct [[24]](ch021.xhtml#ref-dubey2024llama),
    T√ºlu 2 [[22]](ch021.xhtml#ref-ivison2023camels) and 3 [[6]](ch021.xhtml#ref-lambert2024t),
    Nemotron 4 340B [[25]](ch021.xhtml#ref-adler2024nemotron), and others. Technically,
    Sequence Likelihood Calibration (SLiC-HF) was the first, modern direct alignment
    algorithm released [[210]](ch021.xhtml#ref-zhao2023slic), but it did not catch
    on due to a combination of factors (unwinding the adoption of research methods
    is always a tricky task).
  prefs: []
  type: TYPE_NORMAL
- en: The most impactful part of DPO and DAAs is lowering the barrier of entry to
    experimenting with language model post-training ‚Äì it uses less compute, is easier
    to implement from scratch, and is easier to get working on both toy and production
    examples.
  prefs: []
  type: TYPE_NORMAL
- en: '*Throughout this chapter, we use <semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics>
    to denote prompts and <semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics>
    to denote completions. This notation is common in the language model literature,
    where methods operate on full prompt-completion pairs rather than individual tokens.*'
  prefs: []
  type: TYPE_NORMAL
- en: Direct Preference Optimization (DPO)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here we explain intuitions for how DPO works and re-derive the core equations
    fully.
  prefs: []
  type: TYPE_NORMAL
- en: How DPO Works
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'DPO at a surface level is directly optimizing a policy to solve the RLHF objective.
    The loss function for this, which we will revisit below in the derivations, is
    a pairwise relationship of log-probabilities. The loss function derived from a
    Bradley-Terry reward model follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><msub><mi>‚Ñí</mi><mtext mathvariant="normal">DPO</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo>;</mo><msub><mi>œÄ</mi><mtext
    mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>‚àí</mi><msub><mi>ùîº</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><msub><mi>y</mi><mi>c</mi></msub><mo>,</mo><msub><mi>y</mi><mi>r</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo>‚àº</mo><mi>ùíü</mi></mrow></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><mi mathvariant="normal">log</mi><mi>œÉ</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>Œ≤</mi><mi mathvariant="normal">log</mi><mfrac><mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>‚à£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>‚à£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow></mfrac><mo>‚àí</mo><mi>Œ≤</mi><mi mathvariant="normal">log</mi><mfrac><mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>r</mi></msub><mo>‚à£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>r</mi></msub><mo>‚à£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mo
    stretchy="true" form="postfix">]</mo></mrow><mrow><mo stretchy="false" form="prefix">(</mo><mn>69</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\mathcal{L}_{\text{DPO}}(\pi_\theta;
    \pi_{\text{ref}}) = -\mathbb{E}_{(x, y_c, y_r) \sim \mathcal{D}}\left[ \log \sigma\left(
    \beta \log \frac{\pi_{\theta}(y_c \mid x)}{\pi_{\text{ref}}(y_c \mid x)} - \beta
    \log \frac{\pi_{\theta}(y_r \mid x)}{\pi_{\text{ref}}(y_r \mid x)} \right) \right]
    \qquad{(69)}</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: 'Throughout, <semantics><mi>Œ≤</mi><annotation encoding="application/x-tex">\beta</annotation></semantics>
    is a hyperparameter balancing the reward optimization to the KL distance between
    the final model and the initial reference (i.e.¬†balancing over-optimization, as
    a crucial hyperparameter to using DPO correction). This relies on the implicit
    reward for DPO training that replaces using an external reward model, which is
    a log-ratio of probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><mi>r</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mi>Œ≤</mi><mrow><mi mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><mfrac><mrow><msub><mi>œÄ</mi><mi>r</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>y</mi><mo>‚à£</mo><mi>x</mi><mo stretchy="false"
    form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>y</mi><mo>‚à£</mo><mi>x</mi><mo stretchy="false"
    form="postfix">)</mo></mrow></mfrac><mrow><mo stretchy="false" form="prefix">(</mo><mn>70</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">r(x,
    y) = \beta \log \frac{\pi_r(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\qquad{(70)}</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: where <semantics><mrow><msub><mi>œÄ</mi><mi>r</mi></msub><mo stretchy="false"
    form="prefix">(</mo><mi>y</mi><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">\pi_r(y \mid x)</annotation></semantics> is the exact,
    optimal reward policy that we are solving for. This comes from deriving the Bradley-Terry
    reward with respect to an optimal policy (shown in eq.¬†[84](ch012.xhtml#eq:dpo_opt_policy)),
    as shown in the Bradley-Terry model section of Chapter 7\. Essentially, the implicit
    reward model shows ‚Äúthe probability of human preference data in terms of the optimal
    policy rather than the reward model.‚Äù
  prefs: []
  type: TYPE_NORMAL
- en: Let us consider the loss shown in eq.¬†[69](ch012.xhtml#eq:dpo_core) that the
    optimizer must decrease. Here, the loss will be lower when the log-ratio of the
    chosen response is bigger than the log-ratio of the rejected response (normalized
    by the reference model). In practice, this is a sum of log-probabilities of the
    model across the sequence of tokens in the data presented. Hence, DPO is increasing
    the delta in probabilities between the chosen and rejected responses.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the reward in eq.¬†[70](ch012.xhtml#eq:dpo_reward), we can write the gradient
    of the loss to further interpret what is going on:'
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><msub><mi>‚Ñí</mi><mtext mathvariant="normal">DPO</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo>;</mo><msub><mi>œÄ</mi><mtext
    mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>‚àí</mi><mi>Œ≤</mi><msub><mi>ùîº</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><msub><mi>y</mi><mi>c</mi></msub><mo>,</mo><msub><mi>y</mi><mi>r</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo>‚àº</mo><mi>ùíü</mi></mrow></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><mi>œÉ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>r</mi><mi>Œ∏</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><msub><mi>y</mi><mi>r</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo>‚àí</mo><msub><mi>r</mi><mi>Œ∏</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><msub><mi>y</mi><mi>c</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mi mathvariant="normal">log</mi><mi>œÄ</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>‚à£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo>‚àí</mo><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mi
    mathvariant="normal">log</mi><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>r</mi></msub><mo>‚à£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow><mo
    stretchy="true" form="postfix">]</mo></mrow><mrow><mo stretchy="false" form="prefix">(</mo><mn>71</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\nabla_{\theta}\mathcal{L}_{\text{DPO}}(\pi_{\theta};
    \pi_{\text{ref}}) = -\beta \mathbb{E}_{(x, y_c, y_r)\sim \mathcal{D}}\left[ \sigma\left(r_{\theta}(x,
    y_r) - r_{\theta}(x, y_c)\right) \left(\nabla_{\theta}\log \pi(y_c \mid x) - \nabla_{\theta}\log
    \pi(y_r \mid x)\right) \right] \qquad{(71)}</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the gradient solves the above objective by doing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The first term within the sigmoid function, <semantics><mrow><mi>œÉ</mi><mo stretchy="false"
    form="prefix">(</mo><mi>‚ãÖ</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">\sigma(\cdot)</annotation></semantics>, creates a
    weight of the parameter update from 0 to 1 that is higher when the reward estimate
    is incorrect. When the rejected sample is preferred over the chosen, the weight
    update should be larger!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Second, the terms in the inner brackets <semantics><mrow><mo stretchy="false"
    form="prefix">[</mo><mi>‚ãÖ</mi><mo stretchy="false" form="postfix">]</mo></mrow><annotation
    encoding="application/x-tex">[\cdot]</annotation></semantics> increase the likelihood
    of the chosen response <semantics><msub><mi>y</mi><mi>c</mi></msub><annotation
    encoding="application/x-tex">y_c</annotation></semantics> and decrease the likelihood
    of the rejected <semantics><msub><mi>y</mi><mi>r</mi></msub><annotation encoding="application/x-tex">y_r</annotation></semantics>.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These terms are weighted by <semantics><mi>Œ≤</mi><annotation encoding="application/x-tex">\beta</annotation></semantics>,
    which controls how the update balances ordering the completions correctly relative
    to the KL distance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The core intuition is that DPO is fitting an implicit reward model whose corresponding
    optimal policy can be extracted in a closed form (thanks to gradient descent and
    our ML tools). The closed form of the equation means that it is straightforward
    to implement the exact gradient, rather than needing to reach it by proxy of training
    a reward model and sampling completions to score. What is often misunderstood
    is that DPO is learning a reward model at its core, hence the subtitle of the
    paper *Your Language Model is Secretly a Reward Model.* It is easy to confuse
    this with the DPO objective training a policy directly, hence studying the derivations
    below is good for a complete understanding.
  prefs: []
  type: TYPE_NORMAL
- en: With the implicit reward model learning, DPO is generating an optimal solution
    to the RLHF objective given the data in the dataset and the specific KL constraint
    in the objective <semantics><mi>Œ≤</mi><annotation encoding="application/x-tex">\beta</annotation></semantics>.
    Here, DPO solves for the exact policy given a specific KL distance because the
    generations are not online as in policy gradient algorithms ‚Äì a core difference
    from the RL methods for preference tuning. In many ways, this makes the <semantics><mi>Œ≤</mi><annotation
    encoding="application/x-tex">\beta</annotation></semantics> value easier to tune
    with DPO relative to online RL methods, but crucially and intuitively the optimal
    value depends on the model being trained and the data training it.
  prefs: []
  type: TYPE_NORMAL
- en: At each batch of preference data, composed of many pairs of completions <semantics><mrow><msub><mi>y</mi><mrow><mi>c</mi><mi>h</mi><mi>o</mi><mi>s</mi><mi>e</mi><mi>n</mi></mrow></msub><mo>‚âª</mo><msub><mi>y</mi><mrow><mi>r</mi><mi>e</mi><mi>j</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>e</mi><mi>d</mi></mrow></msub></mrow><annotation
    encoding="application/x-tex">y_{chosen} \succ y_{rejected}</annotation></semantics>,
    DPO takes gradient steps directly towards the optimal solution. It is far simpler
    than policy gradient methods.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18: When DPO first released it sparked a fierce debate in the research
    community about how to best do RLHF and preference learning. This meme is a great
    job capturing the sentiment, where the debate often felt forced and over the top,
    but many people both getting started and in top labs were getting immense benefit
    out of DPO. DPO simplicity meme, credit Tom Goldstein.](../media/file16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: When DPO first released it sparked a fierce debate in the research
    community about how to best do RLHF and preference learning. This meme is a great
    job capturing the sentiment, where the debate often felt forced and over the top,
    but many people both getting started and in top labs were getting immense benefit
    out of DPO. DPO simplicity meme, credit Tom Goldstein.'
  prefs: []
  type: TYPE_NORMAL
- en: DPO Derivation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The DPO derivation takes two primary parts. First, the authors show the form
    of the policy that optimally solved the RLHF objective used throughout this book.
    Next, they show how to arrive at that solution from pairwise preference data (i.e.¬†a
    Bradley Terry model).
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Deriving the Optimal RLHF Solution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To start, we should consider the RLHF optimization objective once again, here
    indicating we wish to maximize this quantity:'
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><munder><mi mathvariant="normal">max</mi><mi>œÄ</mi></munder><msub><mi>ùîº</mi><mrow><mi>x</mi><mo>‚àº</mo><mi>ùíü</mi></mrow></msub><msub><mi>ùîº</mi><mrow><mi>y</mi><mo>‚àº</mo><mi>œÄ</mi><mo
    stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow></msub><mrow><mo stretchy="true"
    form="prefix">[</mo><msub><mi>r</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow><mo>‚àí</mo><mi>Œ≤</mi><msub><mi>ùíü</mi><mtext
    mathvariant="normal">KL</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>œÄ</mi><mo
    stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">‚à•</mo><msub><mi>œÄ</mi><mtext
    mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo
    stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo
    stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="false" form="prefix">(</mo><mn>72</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\max_{\pi}
    \mathbb{E}_{x \sim \mathcal{D}}\mathbb{E}_{y \sim \pi(y|x)} \left[r_\theta(x,
    y)\right] - \beta \mathcal{D}_{\text{KL}}\left(\pi(y|x) \| \pi_{\text{ref}}(y|x)\right)\qquad{(72)}</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the dual expectation only applies to the sampling to compute the expected
    reward, as the KL term is still an analytical expression. First, let us expand
    the definition of KL-divergence. Recall that <semantics><mrow><msub><mi>ùíü</mi><mtext
    mathvariant="normal">KL</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>œÄ</mi><mo
    stretchy="false" form="postfix">‚à•</mo><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ùîº</mi><mrow><mi>y</mi><mo>‚àº</mo><mi>œÄ</mi></mrow></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><mi mathvariant="normal">log</mi><mfrac><mrow><mi>œÄ</mi><mo
    stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow></mfrac><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation
    encoding="application/x-tex">\mathcal{D}_{\text{KL}}(\pi \| \pi_{\text{ref}})
    = \mathbb{E}_{y \sim \pi}\left[\log \frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)}\right]</annotation></semantics>,
    where the <semantics><mrow><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo
    stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">\pi(y|x)</annotation></semantics> weighting in the
    sum becomes the sampling distribution. Since both terms now share the same expectation
    over <semantics><mrow><mi>y</mi><mo>‚àº</mo><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo
    stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">y \sim \pi(y|x)</annotation></semantics>, we can
    combine them:'
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><munder><mi mathvariant="normal">max</mi><mi>œÄ</mi></munder><msub><mi>ùîº</mi><mrow><mi>x</mi><mo>‚àº</mo><mi>ùíü</mi></mrow></msub><msub><mi>ùîº</mi><mrow><mi>y</mi><mo>‚àº</mo><mi>œÄ</mi><mo
    stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow></msub><mrow><mo stretchy="true"
    form="prefix">[</mo><mi>r</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo
    stretchy="false" form="postfix">)</mo><mo>‚àí</mo><mi>Œ≤</mi><mi mathvariant="normal">log</mi><mfrac><mrow><mi>œÄ</mi><mo
    stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow></mfrac><mo stretchy="true" form="postfix">]</mo></mrow><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>73</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">\max_{\pi} \mathbb{E}_{x \sim \mathcal{D}}\mathbb{E}_{y
    \sim \pi(y|x)}\left[r(x,y)-\beta\log\frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)}\right]
    \qquad{(73)}</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, pull the negative sign out of the difference in brackets. To do this,
    split it into two terms:'
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><mo>=</mo><munder><mi mathvariant="normal">max</mi><mi>œÄ</mi></munder><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>ùîº</mi><mrow><mi>x</mi><mo>‚àº</mo><mi>ùíü</mi></mrow></msub><msub><mi>ùîº</mi><mrow><mi>y</mi><mo>‚àº</mo><mi>œÄ</mi><mo
    stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow></msub><mo stretchy="false" form="prefix">[</mo><mi>r</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false"
    form="postfix">)</mo><mo stretchy="false" form="postfix">]</mo><mo>‚àí</mo><mi>Œ≤</mi><msub><mi>ùîº</mi><mrow><mi>x</mi><mo>‚àº</mo><mi>ùíü</mi></mrow></msub><msub><mi>ùîº</mi><mrow><mi>y</mi><mo>‚àº</mo><mi>œÄ</mi><mo
    stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow></msub><mrow><mo stretchy="true"
    form="prefix">[</mo><mi mathvariant="normal">log</mi><mfrac><mrow><mi>œÄ</mi><mo
    stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow></mfrac><mo stretchy="true" form="postfix">]</mo></mrow><mo
    stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="false" form="prefix">(</mo><mn>74</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">=
    \max_{\pi}\left(\mathbb{E}_{x \sim \mathcal{D}}\mathbb{E}_{y \sim \pi(y|x)}[r(x,y)]
    - \beta\,\mathbb{E}_{x \sim \mathcal{D}}\mathbb{E}_{y \sim \pi(y|x)}\left[\log\frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)}\right]\right)
    \qquad{(74)}</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: Then, remove the factor of <semantics><mrow><mi>‚àí</mi><mn>1</mn></mrow><annotation
    encoding="application/x-tex">-1</annotation></semantics> and <semantics><mi>Œ≤</mi><annotation
    encoding="application/x-tex">\beta</annotation></semantics>,
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><mo>=</mo><munder><mi mathvariant="normal">min</mi><mi>œÄ</mi></munder><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>‚àí</mi><msub><mi>ùîº</mi><mrow><mi>x</mi><mo>‚àº</mo><mi>ùíü</mi></mrow></msub><msub><mi>ùîº</mi><mrow><mi>y</mi><mo>‚àº</mo><mi>œÄ</mi><mo
    stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow></msub><mo stretchy="false" form="prefix">[</mo><mi>r</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false"
    form="postfix">)</mo><mo stretchy="false" form="postfix">]</mo><mo>+</mo><mi>Œ≤</mi><msub><mi>ùîº</mi><mrow><mi>x</mi><mo>‚àº</mo><mi>ùíü</mi></mrow></msub><msub><mi>ùîº</mi><mrow><mi>y</mi><mo>‚àº</mo><mi>œÄ</mi><mo
    stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow></msub><mrow><mo stretchy="true"
    form="prefix">[</mo><mi mathvariant="normal">log</mi><mfrac><mrow><mi>œÄ</mi><mo
    stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mrow><mi mathvariant="normal">r</mi><mi
    mathvariant="normal">e</mi><mi mathvariant="normal">f</mi></mrow></msub><mo stretchy="false"
    form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow></mfrac><mo stretchy="true" form="postfix">]</mo></mrow><mo
    stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="false" form="prefix">(</mo><mn>75</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">=
    \min_{\pi}\left(-\mathbb{E}_{x \sim \mathcal{D}}\mathbb{E}_{y \sim \pi(y|x)}[r(x,y)]
    + \beta\,\mathbb{E}_{x \sim \mathcal{D}}\mathbb{E}_{y \sim \pi(y|x)}\left[\log\frac{\pi(y|x)}{\pi_{\mathrm{ref}}(y|x)}\right]\right)
    \qquad{(75)}</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: 'Divide by <semantics><mi>Œ≤</mi><annotation encoding="application/x-tex">\beta</annotation></semantics>
    and recombine:'
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><mo>=</mo><munder><mi mathvariant="normal">min</mi><mi>œÄ</mi></munder><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>ùîº</mi><mrow><mi>x</mi><mo>‚àº</mo><mi>ùíü</mi></mrow></msub><msub><mi>ùîº</mi><mrow><mi>y</mi><mo>‚àº</mo><mi>œÄ</mi><mo
    stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow></msub><mrow><mo stretchy="true"
    form="prefix">[</mo><mi mathvariant="normal">log</mi><mfrac><mrow><mi>œÄ</mi><mo
    stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow></mfrac><mo>‚àí</mo><mfrac><mn>1</mn><mi>Œ≤</mi></mfrac><mi>r</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false"
    form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow><mo stretchy="true"
    form="postfix">)</mo></mrow><mrow><mo stretchy="false" form="prefix">(</mo><mn>76</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">=
    \min_{\pi}\left(\mathbb{E}_{x \sim \mathcal{D}}\mathbb{E}_{y \sim \pi(y|x)}\left[
    \log\frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)} - \frac{1}{\beta}r(x,y) \right]\right)
    \qquad{(76)}</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we must introduce a partition function, <semantics><mrow><mi>Z</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">Z(x)</annotation></semantics>:'
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><mi>Z</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><munder><mo>‚àë</mo><mi>y</mi></munder><msub><mi>œÄ</mi><mtext
    mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo
    stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mrow><mi
    mathvariant="normal">exp</mi><mo>‚Å°</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mn>1</mn><mi>Œ≤</mi></mfrac><mi>r</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false"
    form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>77</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">Z(x) = \sum_y \pi_{\text{ref}}(y|x)\exp\left(\frac{1}{\beta}r(x,y)\right)
    \qquad{(77)}</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: 'The partition function acts as a normalization factor over the reference policy,
    summing over all possible responses <semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics>
    to a prompt <semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics>.
    With this substituted in, we obtain our intermediate transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><munder><mi mathvariant="normal">min</mi><mi>œÄ</mi></munder><msub><mi>ùîº</mi><mrow><mi>x</mi><mo>‚àº</mo><mi>ùíü</mi></mrow></msub><msub><mi>ùîº</mi><mrow><mi>y</mi><mo>‚àº</mo><mi>œÄ</mi><mo
    stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow></msub><mrow><mo stretchy="true"
    form="prefix">[</mo><mi mathvariant="normal">log</mi><mfrac><mrow><mi>œÄ</mi><mo
    stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><mfrac><mn>1</mn><mrow><mi>Z</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><msub><mi>œÄ</mi><mtext
    mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo
    stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mrow><mi
    mathvariant="normal">exp</mi><mo>‚Å°</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mn>1</mn><mi>Œ≤</mi></mfrac><mi>r</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false"
    form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo>‚àí</mo><mi
    mathvariant="normal">log</mi><mi>Z</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>78</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">\min_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\mathbb{E}_{y\sim\pi(y|x)}\left[\log\frac{\pi(y|x)}{\frac{1}{Z(x)}\pi_{\text{ref}}(y|x)\exp\left(\frac{1}{\beta}r(x,y)\right)}
    - \log Z(x)\right] \qquad{(78)}</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: 'To see how this is obtained, consider the internal part of the optimization
    in brackets of eq.¬†[76](ch012.xhtml#eq:dpo_deriv_4):'
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><mrow><mi mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><mfrac><mrow><mi>œÄ</mi><mo
    stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow></mfrac><mo>‚àí</mo><mfrac><mn>1</mn><mi>Œ≤</mi></mfrac><mi>r</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false"
    form="postfix">)</mo><mrow><mo stretchy="false" form="prefix">(</mo><mn>79</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\log\frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)}
    - \frac{1}{\beta}r(x,y) \qquad{(79)}</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, add <semantics><mrow><mrow><mi mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><mi>Z</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>‚àí</mo><mrow><mi
    mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><mi>Z</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\log
    Z(x) - \log Z(x)</annotation></semantics> to both sides:'
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><mo>=</mo><mrow><mi mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><mfrac><mrow><mi>œÄ</mi><mo
    stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow></mfrac><mo>‚àí</mo><mfrac><mn>1</mn><mi>Œ≤</mi></mfrac><mi>r</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false"
    form="postfix">)</mo><mo>+</mo><mrow><mi mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><mi>Z</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>‚àí</mo><mrow><mi
    mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><mi>Z</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mrow><mo stretchy="false" form="prefix">(</mo><mn>80</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">=
    \log\frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)} - \frac{1}{\beta}r(x,y) + \log Z(x)
    - \log Z(x) \qquad{(80)}</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we group the terms:'
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi
    mathvariant="normal">log</mi><mfrac><mrow><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo
    stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext
    mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo
    stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo>+</mo><mi
    mathvariant="normal">log</mi><mi>Z</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow><mo>‚àí</mo><mrow><mi
    mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><mi>Z</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo>‚àí</mo><mfrac><mn>1</mn><mi>Œ≤</mi></mfrac><mi>r</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false"
    form="postfix">)</mo><mrow><mo stretchy="false" form="prefix">(</mo><mn>81</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">=
    \left( \log \frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)} + \log Z(x) \right) - \log
    Z(x) - \frac{1}{\beta}r(x,y) \qquad{(81)}</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: 'With <semantics><mrow><mrow><mi mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>+</mo><mrow><mi
    mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mrow><mi mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo>‚ãÖ</mo><mi>y</mi><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\log(x) +
    \log(y) = \log(x\cdot y)</annotation></semantics> (and moving <semantics><mi>Z</mi><annotation
    encoding="application/x-tex">Z</annotation></semantics> to the denominator), we
    get:'
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><mo>=</mo><mrow><mi mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><mfrac><mrow><mi>œÄ</mi><mo
    stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><mfrac><mn>1</mn><mrow><mi>Z</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><msub><mi>œÄ</mi><mtext
    mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo
    stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo>‚àí</mo><mrow><mi
    mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><mi>Z</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo>‚àí</mo><mfrac><mn>1</mn><mi>Œ≤</mi></mfrac><mi>r</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false"
    form="postfix">)</mo><mrow><mo stretchy="false" form="prefix">(</mo><mn>82</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">=
    \log \frac{\pi(y|x)}{\frac{1}{Z(x)}\pi_{\text{ref}}(y|x)}- \log Z(x) - \frac{1}{\beta}r(x,y)
    \qquad{(82)}</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we expand <semantics><mrow><mfrac><mn>1</mn><mi>Œ≤</mi></mfrac><mi>r</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\frac{1}{\beta}r(x,y)</annotation></semantics>
    to <semantics><mrow><mrow><mi mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><mrow><mi
    mathvariant="normal">exp</mi><mo>‚Å°</mo></mrow><mfrac><mn>1</mn><mi>Œ≤</mi></mfrac><mi>r</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\log \exp
    \frac{1}{\beta}r(x,y)</annotation></semantics> and do the same operation to get
    eq.¬†[78](ch012.xhtml#eq:dpo_deriv_5). With this optimization form, we need to
    actually solve for the optimal policy <semantics><msup><mi>œÄ</mi><mo>*</mo></msup><annotation
    encoding="application/x-tex">\pi^*</annotation></semantics>. To do so, let us
    consider the above optimization as a KL distance:'
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><munder><mi mathvariant="normal">min</mi><mi>œÄ</mi></munder><msub><mi>ùîº</mi><mrow><mi>x</mi><mo>‚àº</mo><mi>ùíü</mi></mrow></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><msub><mi>ùíü</mi><mtext mathvariant="normal">KL</mtext></msub><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo
    stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo
    stretchy="false" form="prefix">|</mo><mo stretchy="false" form="prefix">|</mo><mfrac><mn>1</mn><mrow><mi>Z</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><msub><mi>œÄ</mi><mtext
    mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo
    stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mi
    mathvariant="normal">exp</mi><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mn>1</mn><mi>Œ≤</mi></mfrac><mi>r</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false"
    form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>‚àí</mo><mi mathvariant="normal">log</mi><mi>Z</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo
    stretchy="true" form="postfix">]</mo></mrow><mrow><mo stretchy="false" form="prefix">(</mo><mn>83</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\min_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathcal{D}_{\text{KL}}
    \left(\pi(y|x)||\frac{1}{Z(x)}\pi_{\text{ref}}(y|x)\exp\left(\frac{1}{\beta}r(x,y)\right)
    \right) - \log Z(x)\right] \qquad{(83)}</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the partition function <semantics><mrow><mi>Z</mi><mo stretchy="false"
    form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">Z(x)</annotation></semantics> does not depend on
    the final answer, we can ignore it. This leaves us with just the KL distance between
    our policy we are learning and a form relating the partition, <semantics><mi>Œ≤</mi><annotation
    encoding="application/x-tex">\beta</annotation></semantics>, reward, and reference
    policy. The Gibb‚Äôs inequality tells this is minimized at a distance of 0, only
    when the two quantities are equal! Hence, we get an optimal policy:'
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><msup><mi>œÄ</mi><mo>*</mo></msup><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo
    stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>œÄ</mi><mo
    stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Z</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><msub><mi>œÄ</mi><mtext
    mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo
    stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mrow><mi
    mathvariant="normal">exp</mi><mo>‚Å°</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mn>1</mn><mi>Œ≤</mi></mfrac><mi>r</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false"
    form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>84</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">\pi^*(y|x) = \pi(y|x) = \frac{1}{Z(x)}\pi_{\text{ref}}(y|x)\exp\left(\frac{1}{\beta}r(x,y)\right)
    \qquad{(84)}</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Deriving DPO Objective for Bradley Terry Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To start, recall from Chapter 7 on Reward Modeling and Chapter 6 on Preference
    Data that a Bradley-Terry model of human preferences is formed as:'
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><msup><mi>p</mi><mo>*</mo></msup><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>‚âª</mo><msub><mi>y</mi><mn>2</mn></msub><mo>‚à£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mrow><mrow><mi mathvariant="normal">exp</mi><mo>‚Å°</mo></mrow><mrow><mo
    stretchy="true" form="prefix">(</mo><msup><mi>r</mi><mo>*</mo></msup><mo stretchy="false"
    form="prefix">(</mo><mi>x</mi><mo>,</mo><msub><mi>y</mi><mn>1</mn></msub><mo stretchy="false"
    form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mrow><mi
    mathvariant="normal">exp</mi><mo>‚Å°</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>r</mi><mo>*</mo></msup><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><msub><mi>y</mi><mn>1</mn></msub><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mi
    mathvariant="normal">exp</mi><mo>‚Å°</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>r</mi><mo>*</mo></msup><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><msub><mi>y</mi><mn>2</mn></msub><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>85</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">p^*(y_1 \succ y_2 \mid x) = \frac{\exp\left(r^*(x,y_1)\right)}{\exp\left(r^*(x,y_1)\right)
    + \exp\left(r^*(x, y_2)\right)} \qquad{(85)}</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: 'By manipulating eq.¬†[84](ch012.xhtml#eq:dpo_opt_policy), we can solve for the
    optimal reward. First, take the logarithm of both sides:'
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><mrow><mi mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><msup><mi>œÄ</mi><mo>*</mo></msup><mo
    stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mrow><mi mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><mrow><mo
    stretchy="true" form="prefix">(</mo><mfrac><mn>1</mn><mrow><mi>Z</mi><mo stretchy="false"
    form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><msub><mi>œÄ</mi><mtext
    mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo
    stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mi
    mathvariant="normal">exp</mi><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mn>1</mn><mi>Œ≤</mi></mfrac><msup><mi>r</mi><mo>*</mo></msup><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false"
    form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true"
    form="postfix">)</mo></mrow><mrow><mo stretchy="false" form="prefix">(</mo><mn>86</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\log
    \pi^*(y|x) = \log \left( \frac{1}{Z(x)}\pi_{\text{ref}}(y|x)\exp\left(\frac{1}{\beta}r^*(x,y)\right)
    \right)\qquad{(86)}</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: 'Expanding the right-hand side using <semantics><mrow><mrow><mi mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><mo
    stretchy="false" form="prefix">(</mo><mi>a</mi><mi>b</mi><mi>c</mi><mo stretchy="false"
    form="postfix">)</mo><mo>=</mo><mrow><mi mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><mi>a</mi><mo>+</mo><mrow><mi
    mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><mi>b</mi><mo>+</mo><mrow><mi mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><mi>c</mi></mrow><annotation
    encoding="application/x-tex">\log(abc) = \log a + \log b + \log c</annotation></semantics>:'
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><mrow><mi mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><msup><mi>œÄ</mi><mo>*</mo></msup><mo
    stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mi>‚àí</mi><mrow><mi mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><mi>Z</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>+</mo><mrow><mi
    mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo>+</mo><mfrac><mn>1</mn><mi>Œ≤</mi></mfrac><msup><mi>r</mi><mo>*</mo></msup><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false"
    form="postfix">)</mo><mrow><mo stretchy="false" form="prefix">(</mo><mn>87</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\log
    \pi^*(y|x) = -\log Z(x) + \log \pi_{\text{ref}}(y|x) + \frac{1}{\beta}r^*(x,y)\qquad{(87)}</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: 'Rearranging to solve for <semantics><mrow><msup><mi>r</mi><mo>*</mo></msup><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">r^*(x,y)</annotation></semantics>:'
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><mfrac><mn>1</mn><mi>Œ≤</mi></mfrac><msup><mi>r</mi><mo>*</mo></msup><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false"
    form="postfix">)</mo><mo>=</mo><mrow><mi mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><msup><mi>œÄ</mi><mo>*</mo></msup><mo
    stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo>‚àí</mo><mrow><mi mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><msub><mi>œÄ</mi><mtext
    mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo
    stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>+</mo><mrow><mi
    mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><mi>Z</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mrow><mo stretchy="false" form="prefix">(</mo><mn>88</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\frac{1}{\beta}r^*(x,y)
    = \log \pi^*(y|x) - \log \pi_{\text{ref}}(y|x) + \log Z(x)\qquad{(88)}</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiplying both sides by <semantics><mi>Œ≤</mi><annotation encoding="application/x-tex">\beta</annotation></semantics>:'
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><msup><mi>r</mi><mo>*</mo></msup><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mi>Œ≤</mi><mrow><mi mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><mfrac><mrow><msup><mi>œÄ</mi><mo>*</mo></msup><mo
    stretchy="false" form="prefix">(</mo><mi>y</mi><mo>‚à£</mo><mi>x</mi><mo stretchy="false"
    form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>y</mi><mo>‚à£</mo><mi>x</mi><mo stretchy="false"
    form="postfix">)</mo></mrow></mfrac><mo>+</mo><mi>Œ≤</mi><mrow><mi mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><mi>Z</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>89</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">r^*(x, y) = \beta \log \frac{\pi^*(y \mid x)}{\pi_{\text{ref}}(y
    \mid x)} + \beta \log Z(x)\qquad{(89)}</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: 'We then can substitute the reward into the Bradley-Terry equation shown in
    eq.¬†[85](ch012.xhtml#eq:bradley_terry_dpo) to obtain:'
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><msup><mi>p</mi><mo>*</mo></msup><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>‚âª</mo><msub><mi>y</mi><mn>2</mn></msub><mo>‚à£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mrow><mrow><mi mathvariant="normal">exp</mi><mo>‚Å°</mo></mrow><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>Œ≤</mi><mi mathvariant="normal">log</mi><mfrac><mrow><msup><mi>œÄ</mi><mo>*</mo></msup><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>‚à£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>‚à£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow></mfrac><mo>+</mo><mi>Œ≤</mi><mi mathvariant="normal">log</mi><mi>Z</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mrow><mi mathvariant="normal">exp</mi><mo>‚Å°</mo></mrow><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>Œ≤</mi><mi mathvariant="normal">log</mi><mfrac><mrow><msup><mi>œÄ</mi><mo>*</mo></msup><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>‚à£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>‚à£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow></mfrac><mo>+</mo><mi>Œ≤</mi><mi mathvariant="normal">log</mi><mi>Z</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mi mathvariant="normal">exp</mi><mo>‚Å°</mo></mrow><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>Œ≤</mi><mi mathvariant="normal">log</mi><mfrac><mrow><msup><mi>œÄ</mi><mo>*</mo></msup><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>2</mn></msub><mo>‚à£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>2</mn></msub><mo>‚à£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow></mfrac><mo>+</mo><mi>Œ≤</mi><mi mathvariant="normal">log</mi><mi>Z</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mrow><mo stretchy="false"
    form="prefix">(</mo><mn>90</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">p^*(y_1 \succ y_2 \mid x) = \frac{\exp\left(\beta
    \log \frac{\pi^*(y_1 \mid x)}{\pi_{\text{ref}}(y_1 \mid x)} + \beta \log Z(x)\right)}
    {\exp\left(\beta \log \frac{\pi^*(y_1 \mid x)}{\pi_{\text{ref}}(y_1 \mid x)} +
    \beta \log Z(x)\right) + \exp\left(\beta \log \frac{\pi^*(y_2 \mid x)}{\pi_{\text{ref}}(y_2
    \mid x)} + \beta \log Z(x)\right)} \qquad{(90)}</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: 'By decomposing the exponential expressions from <semantics><msup><mi>e</mi><mrow><mi>a</mi><mo>+</mo><mi>b</mi></mrow></msup><annotation
    encoding="application/x-tex">e^{a+b}</annotation></semantics> to <semantics><mrow><msup><mi>e</mi><mi>a</mi></msup><msup><mi>e</mi><mi>b</mi></msup></mrow><annotation
    encoding="application/x-tex">e^a e^b</annotation></semantics> and then cancelling
    out the terms <semantics><msup><mi>e</mi><mrow><mrow><mi mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><mo
    stretchy="false" form="prefix">(</mo><mi>Z</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">)</mo></mrow></msup><annotation
    encoding="application/x-tex">e^{\log(Z(x))}</annotation></semantics>, this simplifies
    to:'
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><msup><mi>p</mi><mo>*</mo></msup><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>‚âª</mo><msub><mi>y</mi><mn>2</mn></msub><mo>‚à£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mrow><mrow><mi mathvariant="normal">exp</mi><mo>‚Å°</mo></mrow><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>Œ≤</mi><mi mathvariant="normal">log</mi><mfrac><mrow><msup><mi>œÄ</mi><mo>*</mo></msup><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>‚à£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>‚à£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mrow><mi
    mathvariant="normal">exp</mi><mo>‚Å°</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ≤</mi><mi
    mathvariant="normal">log</mi><mfrac><mrow><msup><mi>œÄ</mi><mo>*</mo></msup><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>‚à£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>‚à£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mi
    mathvariant="normal">exp</mi><mo>‚Å°</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ≤</mi><mi
    mathvariant="normal">log</mi><mfrac><mrow><msup><mi>œÄ</mi><mo>*</mo></msup><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>2</mn></msub><mo>‚à£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>2</mn></msub><mo>‚à£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>91</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">p^*(y_1 \succ y_2 \mid x) = \frac{\exp\left(\beta
    \log \frac{\pi^*(y_1 \mid x)}{\pi_{\text{ref}}(y_1 \mid x)}\right)} {\exp\left(\beta
    \log \frac{\pi^*(y_1 \mid x)}{\pi_{\text{ref}}(y_1 \mid x)}\right) + \exp\left(\beta
    \log \frac{\pi^*(y_2 \mid x)}{\pi_{\text{ref}}(y_2 \mid x)}\right)} \qquad{(91)}</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, multiply the numerator and denominator by <semantics><mrow><mrow><mi
    mathvariant="normal">exp</mi><mo>‚Å°</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>‚àí</mi><mi>Œ≤</mi><mi
    mathvariant="normal">log</mi><mfrac><mrow><msup><mi>œÄ</mi><mo>*</mo></msup><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>‚à£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>‚à£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">\exp\left(-\beta \log \frac{\pi^*(y_1 \mid x)}{\pi_{\text{ref}}(y_1
    \mid x)}\right)</annotation></semantics> to obtain:'
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><msup><mi>p</mi><mo>*</mo></msup><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>‚âª</mo><msub><mi>y</mi><mn>2</mn></msub><mo>‚à£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mrow><mi
    mathvariant="normal">exp</mi><mo>‚Å°</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ≤</mi><mi
    mathvariant="normal">log</mi><mfrac><mrow><msup><mi>œÄ</mi><mo>*</mo></msup><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>2</mn></msub><mo>‚à£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>2</mn></msub><mo>‚à£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow></mfrac><mo>‚àí</mo><mi>Œ≤</mi><mi mathvariant="normal">log</mi><mfrac><mrow><msup><mi>œÄ</mi><mo>*</mo></msup><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>‚à£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>‚à£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>92</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">p^*(y_1 \succ y_2 \mid x) = \frac{1}{1 + \exp\left(\beta
    \log \frac{\pi^*(y_2 \mid x)}{\pi_{\text{ref}}(y_2 \mid x)} - \beta \log \frac{\pi^*(y_1
    \mid x)}{\pi_{\text{ref}}(y_1 \mid x)}\right)} \qquad{(92)}</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, with the definition of a sigmoid function as <semantics><mrow><mi>œÉ</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mi>‚àí</mi><mi>x</mi></mrow></msup></mrow></mfrac></mrow><annotation
    encoding="application/x-tex">\sigma(x) = \frac{1}{1+e^{-x}}</annotation></semantics>,
    we obtain:'
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><msup><mi>p</mi><mo>*</mo></msup><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>‚âª</mo><msub><mi>y</mi><mn>2</mn></msub><mo>‚à£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mi>œÉ</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>Œ≤</mi><mi mathvariant="normal">log</mi><mfrac><mrow><msup><mi>œÄ</mi><mo>*</mo></msup><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>‚à£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>‚à£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow></mfrac><mo>‚àí</mo><mi>Œ≤</mi><mi mathvariant="normal">log</mi><mfrac><mrow><msup><mi>œÄ</mi><mo>*</mo></msup><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>2</mn></msub><mo>‚à£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>2</mn></msub><mo>‚à£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>93</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">p^*(y_1 \succ y_2 \mid x) = \sigma\left(\beta \log
    \frac{\pi^*(y_1 \mid x)}{\pi_{\text{ref}}(y_1 \mid x)} - \beta \log \frac{\pi^*(y_2
    \mid x)}{\pi_{\text{ref}}(y_2 \mid x)}\right) \qquad{(93)}</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: This is the loss function for DPO, as shown in eq.¬†[69](ch012.xhtml#eq:dpo_core).
    The DPO paper has an additional derivation for the objective under a Plackett-Luce
    Model, which is far less used in practice [[20]](ch021.xhtml#ref-rafailov2024direct).
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Deriving the Bradley Terry DPO Gradient
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We used the DPO gradient shown in eq.¬†[71](ch012.xhtml#eq:dpo_gradient) to explain
    intuitions for how the model learns. To derive this, we must take the gradient
    of eq.¬†[93](ch012.xhtml#eq:dpo_loss_deriv3) with respect to the model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><msub><mi>‚Ñí</mi><mtext mathvariant="normal">DPO</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo>;</mo><msub><mi>œÄ</mi><mtext
    mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>‚àí</mi><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><msub><mi>ùîº</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><msub><mi>y</mi><mi>c</mi></msub><mo>,</mo><msub><mi>y</mi><mi>r</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo>‚àº</mo><mi>ùíü</mi></mrow></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><mi mathvariant="normal">log</mi><mi>œÉ</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>Œ≤</mi><mi mathvariant="normal">log</mi><mfrac><mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo stretchy="false"
    form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext
    mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo
    stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo>‚àí</mo><mi>Œ≤</mi><mi
    mathvariant="normal">log</mi><mfrac><mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>r</mi></msub><mo stretchy="false"
    form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext
    mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>r</mi></msub><mo
    stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo
    stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>94</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">\nabla_{\theta}\mathcal{L}_{\text{DPO}}(\pi_{\theta};
    \pi_{\text{ref}}) = -\nabla_{\theta}\mathbb{E}_{(x,y_c,y_r)\sim\mathcal{D}}\left[
    \log \sigma\left(\beta \log \frac{\pi_{\theta}(y_c|x)}{\pi_{\text{ref}}(y_c|x)}
    - \beta \log \frac{\pi_{\theta}(y_r|x)}{\pi_{\text{ref}}(y_r|x)}\right)\right]
    \qquad{(94)}</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: To start, this can be rewritten. We know that the derivative of a sigmoid function
    <semantics><mrow><mfrac><mi>d</mi><mrow><mi>d</mi><mi>x</mi></mrow></mfrac><mi>œÉ</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>œÉ</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo
    stretchy="false" form="prefix">(</mo><mn>1</mn><mo>‚àí</mo><mi>œÉ</mi><mo stretchy="false"
    form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\frac{d}{dx}
    \sigma(x) = \sigma(x)(1-\sigma(x))</annotation></semantics>, the derivative of
    logarithm <semantics><mrow><mfrac><mi>d</mi><mrow><mi>d</mi><mi>x</mi></mrow></mfrac><mrow><mi
    mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><mi>x</mi><mo>=</mo><mfrac><mn>1</mn><mi>x</mi></mfrac></mrow><annotation
    encoding="application/x-tex">\frac{d}{dx} \log x = \frac{1}{x}</annotation></semantics>,
    and properties of sigmoid <semantics><mrow><mi>œÉ</mi><mo stretchy="false" form="prefix">(</mo><mi>‚àí</mi><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mn>1</mn><mo>‚àí</mo><mi>œÉ</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">\sigma(-x)=1-\sigma(x)</annotation></semantics>,
    so we can reformat the above equation.
  prefs: []
  type: TYPE_NORMAL
- en: First, let <semantics><mrow><mi>u</mi><mo>=</mo><mi>Œ≤</mi><mrow><mi mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><mfrac><mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo stretchy="false"
    form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext
    mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo
    stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo>‚àí</mo><mi>Œ≤</mi><mrow><mi
    mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><mfrac><mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>r</mi></msub><mo stretchy="false"
    form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext
    mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>r</mi></msub><mo
    stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac></mrow><annotation
    encoding="application/x-tex">u=\beta \log \frac{\pi_{\theta}(y_c|x)}{\pi_{\text{ref}}(y_c|x)}
    - \beta \log \frac{\pi_{\theta}(y_r|x)}{\pi_{\text{ref}}(y_r|x)}</annotation></semantics>
    (the expression inside the sigmoid). Then, we have
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><msub><mi>‚Ñí</mi><mtext mathvariant="normal">DPO</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo>;</mo><msub><mi>œÄ</mi><mtext
    mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>‚àí</mi><msub><mi>ùîº</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><msub><mi>y</mi><mi>c</mi></msub><mo>,</mo><msub><mi>y</mi><mi>r</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo>‚àº</mo><mi>ùíü</mi></mrow></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><mfrac><mrow><msup><mi>œÉ</mi><mo>‚Ä≤</mo></msup><mo
    stretchy="false" form="prefix">(</mo><mi>u</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><mi>œÉ</mi><mo
    stretchy="false" form="prefix">(</mo><mi>u</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mi>u</mi><mo
    stretchy="true" form="postfix">]</mo></mrow><mrow><mo stretchy="false" form="prefix">(</mo><mn>95</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\nabla_{\theta}\mathcal{L}_{\text{DPO}}(\pi_{\theta};\pi_{\text{ref}})
    = -\mathbb{E}_{(x, y_c, y_r)\sim \mathcal{D}}\left[\frac{\sigma'(u)}{\sigma(u)}\nabla_{\theta}u\right]
    \qquad{(95)}</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: 'Expanding this and using the above expressions for sigmoid and logarithms results
    in the gradient introduced earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><mi>‚àí</mi><msub><mi>ùîº</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><msub><mi>y</mi><mi>c</mi></msub><mo>,</mo><msub><mi>y</mi><mi>r</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo>‚àº</mo><mi>ùíü</mi></mrow></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><mi>Œ≤</mi><mi>œÉ</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>Œ≤</mi><mi mathvariant="normal">log</mi><mfrac><mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>r</mi></msub><mo stretchy="false"
    form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext
    mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>r</mi></msub><mo
    stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo>‚àí</mo><mi>Œ≤</mi><mi
    mathvariant="normal">log</mi><mfrac><mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo stretchy="false"
    form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext
    mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo
    stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo
    stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mi
    mathvariant="normal">log</mi><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo
    stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>‚àí</mo><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mi
    mathvariant="normal">log</mi><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>r</mi></msub><mo
    stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo
    stretchy="true" form="postfix">]</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>96</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">-\mathbb{E}_{(x,y_c,y_r)\sim\mathcal{D}}\left[\beta\sigma\left(\beta\log\frac{\pi_{\theta}(y_r|x)}{\pi_{\text{ref}}(y_r|x)}
    - \beta\log\frac{\pi_{\theta}(y_c|x)}{\pi_{\text{ref}}(y_c|x)}\right)\left[\nabla_{\theta}\log\pi(y_c|x)-\nabla_{\theta}\log\pi(y_r|x)\right]\right]
    \qquad{(96)}</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: Numerical Concerns, Weaknesses, and Alternatives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many variants of the DPO algorithm have been proposed to address weaknesses
    of DPO. For example, without rollouts where a reward model can rate generations,
    DPO treats every pair of preference data with equal weight. In reality, as seen
    in Chapter 6 on Preference Data, there are many ways of capturing preference data
    with a richer label than binary. Multiple algorithms have been proposed to re-balance
    the optimization away from treating each pair equally.
  prefs: []
  type: TYPE_NORMAL
- en: '**REgression to RElative REward Based RL (REBEL)** adds signal from a reward
    model, as a margin between chosen and rejected responses, rather than solely the
    pairwise preference data to more accurately solve the RLHF problem [[166]](ch021.xhtml#ref-gao2024rebel).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Conservative DPO (cDPO) and Identity Preference Optimization (IPO)** address
    the overfitting by assuming noise in the preference data. cDPO assumes N percent
    of the data is incorrectly labelled [[20]](ch021.xhtml#ref-rafailov2024direct)
    and IPO changes the optimization to soften probability of preference rather than
    optimize directly from a label [[211]](ch021.xhtml#ref-azar2024general). Practically,
    IPO changes the preference probability to a nonlinear function, moving away from
    the Bradley-Terry assumption, with <semantics><mrow><mi mathvariant="normal">Œ®</mi><mo
    stretchy="false" form="prefix">(</mo><mi>q</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mrow><mi
    mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mi>q</mi><mrow><mn>1</mn><mo>‚àí</mo><mi>q</mi></mrow></mfrac><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Psi(q)
    = \log\left(\frac{q}{1-q}\right)</annotation></semantics>.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DPO with an offset (ODPO)** ‚Äúrequires the difference between the likelihood
    of the preferred and dispreferred response to be greater than an offset value‚Äù
    [[212]](ch021.xhtml#ref-amini2024direct) ‚Äì do not treat every data pair equally,
    but this can come at the cost of a more difficult labeling environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some variants to DPO attempt to either improve the learning signal by making
    small changes to the loss or make the application more efficient by reducing memory
    usage.
  prefs: []
  type: TYPE_NORMAL
- en: '**Odds Ratio Policy Optimization (ORPO)** directly updates the policy model
    with a pull towards the chosen response, similar to the instruction finetuning
    loss, with a small penalty on the chosen response [[213]](ch021.xhtml#ref-hong2024reference).
    This change of loss function removes the need for a reference model, simplifying
    the setup. The best way to view ORPO is DPO inspired, rather than a DPO derivative.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Simple Preference Optimization SimPO** makes a minor change to the DPO optimization,
    by averaging the log-probabilities rather than summing them (SimPO) or adding
    length normalization, to improve performance [[214]](ch021.xhtml#ref-meng2025simpo).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 19: Sketch of preference displacement in DPO.](../media/file17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19: Sketch of preference displacement in DPO.'
  prefs: []
  type: TYPE_NORMAL
- en: One of the core issues *apparent* in DPO is that the optimization drives only
    to increase the margin between the probability of the chosen and rejected responses.
    Numerically, the model reduces the probability of both the chosen and rejected
    responses, but the *rejected response is reduced by a greater extent* as shown
    in fig.¬†[19](#fig:dpo_issue). Intuitively, it is not clear how this generalizes,
    but work has posited that it increases the probability of unaddressed for behaviors
    ‚Äì i.e.¬†tokens that language model could generate, but are not in the distribution
    of the post-training datasets [[215]](ch021.xhtml#ref-razin2024unintentional)
    [[216]](ch021.xhtml#ref-ren2024learning). Simple methods‚Äîsuch as Cal-DPO [[217]](ch021.xhtml#ref-xiao2024cal),
    which adjusts the optimization process, and AlphaPO [[218]](ch021.xhtml#ref-gupta2025alphapo),
    which modifies the reward shape‚Äîmitigate this **preference displacement**. In
    practice, the exact impact of this is not well known, but points to a potential
    reason why online methods can outperform vanilla DPO.
  prefs: []
  type: TYPE_NORMAL
- en: The largest other reason that is posited for DPO-like methods to have a lower
    ceiling on performance than online (RL based) RLHF methods is that the training
    signal comes from completions from previous or other models. Online variants of
    DPO alleviate these limitations by generating new completions and incorporating
    a preference signal at training time. **Online DPO** [[219]](ch021.xhtml#ref-guo2024direct)
    samples generations from the current model, while **Discriminator-Guided DPO**
    (D2PO) [[220]](ch021.xhtml#ref-singhal2024d2po) uses reward model relabelling
    to create new preference data on the fly, and many more variants exist.
  prefs: []
  type: TYPE_NORMAL
- en: There is a long list of other DAA variants, such as Direct Nash Optimization
    (DNO) [[221]](ch021.xhtml#ref-rosset2024direct) or Binary Classifier Optimization
    (BCO) [[222]](ch021.xhtml#ref-jung2024binary), but the choice of algorithm is
    far less important than the initial model and the data used [[6]](ch021.xhtml#ref-lambert2024t)
    [[223]](ch021.xhtml#ref-zhao2024rainbowpo) [[224]](ch021.xhtml#ref-gorbatovski2025differences).
  prefs: []
  type: TYPE_NORMAL
- en: Implementation Considerations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'DAAs such as DPO are implemented very differently than policy gradient optimizers.
    The DPO loss, taken from the original implementation, largely can be summarized
    as follows [[20]](ch021.xhtml#ref-rafailov2024direct):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This can be used in standard language model training stacks as this information
    is already collated during the forward pass of a model (with the addition of a
    reference model).
  prefs: []
  type: TYPE_NORMAL
- en: In most ways, this is simpler and a quality of life improvement, but also they
    offer a different set of considerations.
  prefs: []
  type: TYPE_NORMAL
- en: '**KL distance is static**: In DPO and other algorithms, the KL distance is
    set explicitly by the <semantics><mi>Œ≤</mi><annotation encoding="application/x-tex">\beta</annotation></semantics>
    parameter that balances the distance penalty to the optimization. This is due
    to the fact that DPO takes gradient steps towards the *optimal* solution to the
    RLHF objective given the data ‚Äì it steps exactly to the solution set by the <semantics><mi>Œ≤</mi><annotation
    encoding="application/x-tex">\beta</annotation></semantics> term. On the other
    hand, RL based optimizers take steps based on the batch and recent data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Caching log-probabilities**: Simple implementations of DPO do the forward
    passes for the policy model and reference models at the same time for conveniences
    with respect to the loss function. Though, this doubles the memory used and results
    in increased GPU usage. To avoid this, one can compute the log-probabilities of
    the reference model over the training dataset first, then reference it when computing
    the loss and updating the parameters per batch, reducing the peak memory usage
    by 50%.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'DAAs vs.¬†RL: Online vs.¬†Offline Data'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Broadly, the argument boils down to one question: Do we need the inner workings
    of reinforcement learning, with value functions, policy gradients, and all, to
    align language models with RLHF? This, like most questions phrased this way, is
    overly simplistic. Of course, both methods are well-established, but it is important
    to illustrate where the fundamental differences and performance manifolds lie.'
  prefs: []
  type: TYPE_NORMAL
- en: Multiple reports have concluded that policy-gradient based and RL methods outperform
    DPO and its variants. The arguments take different forms, from training models
    with different algorithms but controlled data[[190]](ch021.xhtml#ref-ivison2024unpacking)
    [[225]](ch021.xhtml#ref-xu2024dpo) or studying the role of on-policy data within
    the RL optimization loop [[226]](ch021.xhtml#ref-tajwar2024preference). In all
    of these cases, DPO algorithms are a hair behind.
  prefs: []
  type: TYPE_NORMAL
- en: Even with this performance delta, DAA are still used extensively in leading
    models due to its simplicity. DAAs provide a controlled environment where iterations
    on training data and other configurations can be made rapidly, and given that
    data is often far more important than algorithms, using DPO can be fine.
  prefs: []
  type: TYPE_NORMAL
- en: With the emergence of reasoning models that are primarily trained with RL, further
    investment will return to using RL for preference-tuning, which in the long-term
    will improve the robustness of RL infrastructure and cement this margin between
    DAAs and RL for optimizing from human feedback.
  prefs: []
  type: TYPE_NORMAL
