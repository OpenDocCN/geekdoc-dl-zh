- en: '**JAX & Flax ebook**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../images/00001.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Download all notebooks JAX (What it is and how to use it in Python) What is
    XLA? Installing JAX **Setting up TPUs on Google Colab Data types in JAX Ways to
    create JAX arrays Generating random numbers with JAX Pure functions JAX NumPy
    operations JAX arrays are immutable Out-of-Bounds Indexing Data placement on devices
    in JAX How fast is JAX? Using jit() to speed up functions How JIT works Taking
    derivatives with grad() Auto-vectorization with vmap Parallelization with pmap
    Debugging NANs in JAX Double (64bit) precision What is a pytree? Handling state
    in JAX Loading datasets with JAX Building neural networks with JAX Final thoughts**
    Optimizers in JAX and Flax **Adaptive vs stochastic gradient descent (SGD) optimizers
    AdaBelief AdaGrad Adam – Adaptive moment estimation AdamW RAdam – Rectified Adam
    optimizer AdaFactor Fromage Lamb – Layerwise adaptive large batch optimization
    Lars – Layer-wise Adaptive Rate Scaling SM3 - Square-root of Minima of Sums of
    Maxima of Squared-gradients Method SGD– Stochastic Gradient Descent Noisy SGD
    Optimistic GD Differentially Private SGD RMSProp Yogi Final thoughts JAX loss
    functions What is a loss function? Creating custom loss functions in JAX Which
    loss functions are available in JAX? Sigmoid binary cross entropy Softmax cross
    entropy Cosine distance Cosine similarity Huber loss l2 loss log cosh Smooth labels
    Computing loss with JAX Metrics How to monitor JAX loss functions Why JAX loss
    nan happens Final thoughts Activation functions in JAX and Flax ReLU – Rectified
    linear unit PReLU– Parametric Rectified Linear Unit Sigmoid Log sigmoid Softmax
    Log softmax ELU – Exponential linear unit activation CELU – Continuously-differentiable
    exponential linear unit GELU– Gaussian error linear unit activation GLU – Gated
    linear unit activation Soft sign Softplus Swish–Sigmoid Linear Unit( SiLU) Custom
    activation functions in JAX and Flax Final thoughts How to load datasets in JAX
    with TensorFlow How to load text data in JAX Clean the text data Label encode
    the sentiment column Text preprocessing with TensorFlow How to load image data
    in JAX How to load CSV data in JAX Final thoughts Image classification with JAX
    & Flax Loading the dataset Define Convolution Neural Network with Flax Define
    loss Compute metrics Create training state Define training step Define evaluation
    step Training function Evaluate the model Train and evaluate the model Model performance
    Final thoughts Distributed training with JAX & Flax Perform standard imports Setup
    TPUs on Colab Download the dataset Load the dataset Define the model with Flax
    Create training state Apply the model Training function Train the model Model
    evaluation Final thoughts How to use TensorBoard in JAX & Flax How to use TensorBoard
    How to install TensorBoard Using TensorBoard with Jupyter notebooks and Google
    Colab How to launch TensorBoard Tensorboard dashboards How to use TensorBoard
    with Flax How to log images with TensorBoard in Flax How to log text with TensorBoard
    in Flax Track model training in JAX using TensorBoard How to profile JAX programs
    with TensorBoard Programmatic profiling Manual profiling with TensorBoard How
    to profile JAX program on a remote machine Share TensorBoard dashboards Final
    thoughts Handling state in JAX & Flax (BatchNorm and DropOut layers) Perform standard
    imports Download the dataset Loading datasets in JAX Data processing with PyTorch
    Define Flax model with BatchNorm and DropOut Create loss function Compute metrics
    Create custom Flax training state Training step Evaluation step Train Flax model
    Set up TensorBoard in Flax Train model Save Flax model Load Flax model Evaluate
    Flax model Visualize Flax model performance Final thoughts LSTM in JAX & Flax
    Dataset download Data processing with NLTK Text vectorization with Keras Create
    tf.data dataset Define LSTM model in Flax Compute metrics in Flax Create training
    state Define training step Evaluate the Flax model Create training function Train
    LSTM model in Flax Visualize LSTM model performance in Flax Save LSTM model Final
    thoughts Flax vs. TensorFlow Random number generation in TensorFlow and Flax Model
    definition in Flax and TensorFlow Activations in Flax and TensorFlow Optimizers
    in Flax and TensorFlow Metrics in Flax and TensorFlow Computing gradients in Flax
    and TensorFlow Loading datasets in Flax and TensorFlow Training model in Flax
    vs. TensorFlow Distributed training in Flax and TensorFlow Working with TPU accelerators
    Model evaluation Visualize model performance Final thoughts Train ResNet in Flax
    from scratch(Distributed ResNet training) Install Flax models Perform standard
    imports Download dataset Loading dataset in Flax Data transformation in Flax Instantiate
    Flax ResNet model Compute metrics Create Flax model training state Apply model
    function TensorBoard in Flax Train Flax ResNet model Evaluate model with TensorBoard
    Visualize Flax model performance Save Flax ResNet model Load Flax RestNet model
    Final thoughts Transfer learning with JAX & Flax Install JAX ResNet Download dataset
    Data loading in JAX Data processing ResNet model definition Create head network
    Combine ResNet backbone with head Load pre-trained ResNet 50 Get model and variables
    Zero gradients Define Flax optimizer Define Flax loss function Compute Flax metrics
    Create Flax training state Training step Evaluation step Train ResNet model in
    Flax Set up TensorBoard in Flax Train model Save Flax model Load saved Flax model
    Evaluate Flax ResNet model Visualize model performance Final thoughts Elegy(High-level
    API for deep learning in JAX & Flax) Data pre-processing Model definition in Elegy
    Elegy model summary Distributed training in Elegy Keras-like callbacks in Flax
    Train Elegy models Evaluate Elegy models Visualize Elegy model with TensorBoard
    Plot model performance with Matplotlib Making predictions with Elegy models Saving
    and loading Elegy models Final thoughts** Appendix Disclaimer Copyright Other
    things to learn
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Download all notebooks**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Link to download all notebooks. The password for the ZIP file is FDCPJx0D5A6SO#%Qsg
  prefs: []
  type: TYPE_NORMAL
- en: '**JAX (What it is and how to use it in Python)**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'JAX is a Python library offering high performance in machine learning with
    XLA and Just In Time (JIT) compilation. Its API is similar to NumPy''s with a
    few differences. JAX ships with functionalities that aim to improve and increase
    speed in machine learning research. These functionalities include:'
  prefs: []
  type: TYPE_NORMAL
- en: Automatic differentiation
  prefs: []
  type: TYPE_NORMAL
- en: Vectorization
  prefs: []
  type: TYPE_NORMAL
- en: JIT compilation
  prefs: []
  type: TYPE_NORMAL
- en: This article will cover these functionalities and other JAX concepts. Let's
    get started.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is XLA?**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: XLA (Accelerated Linear Algebra) is a linear algebra compiler for accelerating
    machine learning models. It leads to an increase in the speed of model execution
    and reduced memory usage. XLA programs can be generated by JAX, PyTorch, Julia,
    and NX.
  prefs: []
  type: TYPE_NORMAL
- en: '**Installing JAX**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: JAX can be installed from the Python Package Index using:pip install jaxJAX
    is pre-installed on Google Colab. See the link below for other installation options.
  prefs: []
  type: TYPE_NORMAL
- en: '**Setting up TPUs on Google Colab**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to set up JAX to use TPUs on Colab. That is done by executing the following
    code. Ensure that you have changed the runtime to TPU by going to Runtime-> Change
    Runtime Type. If no accelerator is available, JAX will use the CPU.
  prefs: []
  type: TYPE_NORMAL
- en: import jax.tools.colab_tpu jax.tools.colab_tpu.setup_tpu() jax.devices()
  prefs: []
  type: TYPE_NORMAL
- en: '**Data types in JAX**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The data types in NumPy are similar to those in JAX arrays. For instance, here
    is how you can create float and int data in JAX.
  prefs: []
  type: TYPE_NORMAL
- en: import jax.numpy as jnp x = jnp.float32(1.25844) x = jnp.int32(45.25844)
  prefs: []
  type: TYPE_NORMAL
- en: When you check the type of the data, you will see that it's
  prefs: []
  type: TYPE_NORMAL
- en: a DeviceArray.DeviceArray in JAX is the equivalent of numpy.ndarry in NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: jax.numpy  provides an interface similar to NumPy's. However, JAX also provides jax.lax a
    low-level API that is more powerful and stricter. For example, with [jax.numpy] you
    can add numbers that have mixed types but [jax.lax] will not allow this.
  prefs: []
  type: TYPE_NORMAL
- en: '**Ways to create JAX arrays**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can create JAX arrays like you would in NumPy. For example, can use:arange
  prefs: []
  type: TYPE_NORMAL
- en: linspacePython lists.ones.zeros.identity.
  prefs: []
  type: TYPE_NORMAL
- en: jnp.arange(10)
  prefs: []
  type: TYPE_NORMAL
- en: jnp.arange(0,10)
  prefs: []
  type: TYPE_NORMAL
- en: scores = [50,60,70,30,25,70] scores_array = jnp.array(scores) jnp.zeros(5)
  prefs: []
  type: TYPE_NORMAL
- en: jnp.ones(5)
  prefs: []
  type: TYPE_NORMAL
- en: jnp.eye(5)
  prefs: []
  type: TYPE_NORMAL
- en: jnp.identity(5)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00002.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '**Generating random numbers with JAX**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Random number generation is one main difference between JAX and NumPy. JAX is
    meant to be used with functional programs. JAX expects these functions to be pure.
    A **pure function **has no side effects and expects the output to only come from
    its inputs. JAX transformation functions expect pure functions.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, when working with JAX, all input should be passed through function
    parameters, while all output should come from the function results. Hence, something
    like Python's print function is not pure.
  prefs: []
  type: TYPE_NORMAL
- en: A pure function returns the same results when called with the same inputs. This
    is not possible with [np.random.random()] because it is stateful and returns different
    results when called several times.
  prefs: []
  type: TYPE_NORMAL
- en: print(np.random.random()) print(np.random.random()) print(np.random.random())
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00003.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: JAX implements random number generation using a random state. This random state
    is referred to as a **key🔑** . JAX generates pseudorandom numbers from the pseudorandom
    number generator (PRNGs)  state.
  prefs: []
  type: TYPE_NORMAL
- en: seed = 98
  prefs: []
  type: TYPE_NORMAL
- en: key = jax.random.PRNGKey(seed) jax.random.uniform(key)
  prefs: []
  type: TYPE_NORMAL
- en: You should, therefore, not reuse the same state. Instead, you should split the
    PRNG to obtain as many sub keys as you need.key, subkey = jax.random.split(key)
    Using the same key will always generate the same output. ![](../images/00004.gif)
  prefs: []
  type: TYPE_NORMAL
- en: '**Pure functions**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have mentioned that the output of a pure function should only come from the
    result of the function. Therefore, something like Python's [print] function introduces
    impurity. This can be demonstrated using this function.
  prefs: []
  type: TYPE_NORMAL
- en: 'def impure_print_side_effect(x):'
  prefs: []
  type: TYPE_NORMAL
- en: 'print("Executing function") # This is a side-effect return x'
  prefs: []
  type: TYPE_NORMAL
- en: 'The side-effects appear during the first runprint ("First call: ", jax.jit(impure_print_side_effect)(4.))'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Subsequent runs with parameters of same type and shape may no t show the side-effect
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is because JAX now invokes a cached compilation of the f unction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'print ("Second call: ", jax.jit(impure_print_side_effect)(5.))'
  prefs: []
  type: TYPE_NORMAL
- en: JAX re-runs the Python function when the type or shape of the argument changes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'print ("Third call, different type: ", jax.jit(impure_print_sid e_effect)(jnp.array([5.])))'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00005.gif)'
  prefs: []
  type: TYPE_IMG
- en: We can see the printed statement the first time the function is executed. However,
    we don't see that print statement in consecutive runs because it is cached. We
    only see the statement again after changing the data's shape, which forces JAX
    to recompile the function. More on jax.jit in a moment.
  prefs: []
  type: TYPE_NORMAL
- en: '**JAX NumPy operations**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Operations on JAX arrays are similar to operations with NumPy arrays. For example,
    you can [max], [argmax], and [sum] like in NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: matrix = matrix.reshape(4,4) jnp.max(matrix)
  prefs: []
  type: TYPE_NORMAL
- en: jnp.argmax(matrix)
  prefs: []
  type: TYPE_NORMAL
- en: jnp.min(matrix)
  prefs: []
  type: TYPE_NORMAL
- en: jnp.argmin(matrix)
  prefs: []
  type: TYPE_NORMAL
- en: jnp.sum(matrix)
  prefs: []
  type: TYPE_NORMAL
- en: jnp.sqrt(matrix)
  prefs: []
  type: TYPE_NORMAL
- en: matrix.transpose()
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00006.jpeg)However, JAX doesn''t allow operations on non-array
    input like NumPy. For example, passing Python lists or tuples will lead to an
    error.'
  prefs: []
  type: TYPE_NORMAL
- en: 'try:'
  prefs: []
  type: TYPE_NORMAL
- en: jnp.sum([1, 2, 3])
  prefs: []
  type: TYPE_NORMAL
- en: 'except TypeError as e:'
  prefs: []
  type: TYPE_NORMAL
- en: 'print(f"TypeError: {e}")'
  prefs: []
  type: TYPE_NORMAL
- en: 'TypeError: sum requires ndarray or scalar arguments, got <c'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: lass 'list'> at position 0.
  prefs: []
  type: TYPE_NORMAL
- en: '**JAX arrays are immutable**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unlike in NumPy, JAX arrays can not be modified in place. This is because JAX
    expects pure functions.
  prefs: []
  type: TYPE_NORMAL
- en: scores = [50,60,70,30,25]
  prefs: []
  type: TYPE_NORMAL
- en: scores_array = jnp.array(scores)
  prefs: []
  type: TYPE_NORMAL
- en: scores_array[0:3] = [20,40,90]
  prefs: []
  type: TYPE_NORMAL
- en: 'TypeError: ''<class ''jaxlib.xla_extension.DeviceArray''>'' objec t does not
    support item assignment.'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: JAX arrays are immutable. Instead of ``x[idx] = y``, use ``x
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: = x.at[idx].set(y)`` or another .at[]
  prefs: []
  type: TYPE_NORMAL
- en: 'method: https://jax.readthedocs.io/en/latest/_autosummary/ja x.numpy.ndarray.at.html'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Array updates in JAX are performed using [x.at[idx].set(y)]. This returns a
    new array while the old array stays unaltered.
  prefs: []
  type: TYPE_NORMAL
- en: 'try:'
  prefs: []
  type: TYPE_NORMAL
- en: jnp.sum([1, 2, 3])
  prefs: []
  type: TYPE_NORMAL
- en: 'except TypeError as e:'
  prefs: []
  type: TYPE_NORMAL
- en: 'print(f"TypeError: {e}")'
  prefs: []
  type: TYPE_NORMAL
- en: 'TypeError: sum requires ndarray or scalar arguments, got <c'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: lass 'list'> at position 0.
  prefs: []
  type: TYPE_NORMAL
- en: '**Out-of-Bounds Indexing**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: NumPy usually throws an error when you try to get an item in an array that is
    out of bounds. JAX doesn't throw any error but returns the last item in the array.
  prefs: []
  type: TYPE_NORMAL
- en: matrix = jnp.arange(1,17) matrix[20]
  prefs: []
  type: TYPE_NORMAL
- en: DeviceArray(16, dtype=int32)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: JAX is designed like this because throwing errors in accelerators can be challenging.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data placement on devices in JAX**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: JAX arrays are placed in the first device, [jax.devices()[0]]that is, GPU, TPU,
    or CPU. Data can be placed on a particular device
  prefs: []
  type: TYPE_NORMAL
- en: using jax.device_put().
  prefs: []
  type: TYPE_NORMAL
- en: from jax import device_put
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  prefs: []
  type: TYPE_NORMAL
- en: size = 5000
  prefs: []
  type: TYPE_NORMAL
- en: x = np.random.normal(size=(size, size)).astype(np.float32) x = device_put(x)
  prefs: []
  type: TYPE_NORMAL
- en: The data becomes committed to that device, and operations on it are also committed
    on the same device.
  prefs: []
  type: TYPE_NORMAL
- en: '**How fast is JAX?**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: JAX uses asynchronous dispatch, meaning that it does not wait for computation
    to complete to give control back to the Python program. Therefore, when you perform
    an execution, JAX will return a future. JAX forces Python to wait for the execution
    when you want to print the output or if you convert the result to a NumPy array.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, if you want to compute the time of execution of a program you'll
    have to convert the result to a NumPy array
  prefs: []
  type: TYPE_NORMAL
- en: using [block_until_ready()] to wait for the execution to complete.   Generally
    speaking, NumPy will outperform JAX on the CPU, but JAX will outperform NumPy on
    accelerators and when using jitted functions.
  prefs: []
  type: TYPE_NORMAL
- en: '**Using jit() to speed up functions**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[jit]  performs just-in-time compilation with XLA. [jax.jit] expects a pure
    function. Any side effects in the function will only be executed once. Let''s
    create a pure function and time its execution time without jit.'
  prefs: []
  type: TYPE_NORMAL
- en: 'def test_fn(sample_rate=3000,frequency=3):'
  prefs: []
  type: TYPE_NORMAL
- en: x = jnp.arange(sample_rate)
  prefs: []
  type: TYPE_NORMAL
- en: y = np.sin(2*jnp.pi*frequency * (frequency/sample_rate)) return jnp.dot(x,y)
  prefs: []
  type: TYPE_NORMAL
- en: '%timeit test_fn()'
  prefs: []
  type: TYPE_NORMAL
- en: 'best of 5: 76.1 µs per loop'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's now use jit and time the execution of the same function. In this case,
    we can see that using jit makes the execution almost 20 times faster.
  prefs: []
  type: TYPE_NORMAL
- en: test_fn_jit = jax.jit(test_fn)
  prefs: []
  type: TYPE_NORMAL
- en: '%timeit test_fn_jit().block_until_ready() # best of 5: 4.54 µs per loop'
  prefs: []
  type: TYPE_NORMAL
- en: In the above example, [test_fn_jit] is the jit-compiled version of the function.
    JAX then created code that is optimized for GPU or TPU. The optimized code is
    what will be used the next time this function is called.
  prefs: []
  type: TYPE_NORMAL
- en: '**How JIT works**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: JAX works by converting Python functions into an intermediate language called
    jaxpr(JAX Expression). The [jax.make_jaxpr] can be used to show the jaxpr representation
    of a Python function. If the function has any side effects, they are not recorded
    by jaxpr. We saw earlier that any side effects, for example, printing, will only
    be shown during the first call.
  prefs: []
  type: TYPE_NORMAL
- en: 'def sum_logistic(x):'
  prefs: []
  type: TYPE_NORMAL
- en: print("printed x:", x)
  prefs: []
  type: TYPE_NORMAL
- en: return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))
  prefs: []
  type: TYPE_NORMAL
- en: x_small = jnp.arange(6.)print(jax.make_jaxpr(sum_logistic)(x_small))
  prefs: []
  type: TYPE_NORMAL
- en: JAX creates the jaxpr through tracing. Each argument in the function is wrapped
    with a tracer object. The purpose of these tracers is to record all JAX operations
    performed on them when the function is called. JAX uses the tracer records to
    rebuild the function, which leads to jaxpr. Python side-effects don't show up
    in the jaxpr because the tracers do not record them.
  prefs: []
  type: TYPE_NORMAL
- en: JAX requires arrays shapes to be static and known at compile time. Decorating
    a function conditioned on a value with jit results in error. Therefore, not all
    code can be jit-compiled.
  prefs: []
  type: TYPE_NORMAL
- en: '@jax.jitdef f(boolean, x):return -x if boolean else x'
  prefs: []
  type: TYPE_NORMAL
- en: f(True, 1)
  prefs: []
  type: TYPE_NORMAL
- en: 'ConcretizationTypeError: Abstract tracer value encountered wh ere concrete
    value is expected: Traced<ShapedArray(bool[], weak _type=True)>with<DynamicJaxprTrace(level=0/1)>'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are a couple of solutions to this problem:'
  prefs: []
  type: TYPE_NORMAL
- en: Remove conditionals on the value. Use JAX control flow operators such as [jax.lax.cond].
  prefs: []
  type: TYPE_NORMAL
- en: Jit only a part of the function. Make parameters static.
  prefs: []
  type: TYPE_NORMAL
- en: We can implement the last option and make the boolean parameter static. This
    is done by specifying [static_argnums] or [static_argnames]. This forces JAX to
    recompile the function when the value of the static parameter changes. This is
    not a good strategy if the function will get many values for the static argument.
    You don't want to recompile the function too many times.
  prefs: []
  type: TYPE_NORMAL
- en: You can pass the static arguments using Python’s [functools.partial] .from functools
    import partial@partial(jax.jit, static_argnums=(0,)) def f(boolean, x):return
    -x if boolean else xf(True, 1)
  prefs: []
  type: TYPE_NORMAL
- en: '**Taking derivatives with grad()**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Computing derivatives in JAX is done using jax.grad.@jax.jitdef sum_logistic(x):return
    jnp.sum(1.0 / (1.0 + jnp.exp(-x)))
  prefs: []
  type: TYPE_NORMAL
- en: x_small = jnp.arange(6.)
  prefs: []
  type: TYPE_NORMAL
- en: derivative_fn = jax.grad(sum_logistic) print(derivative_fn(x_small))
  prefs: []
  type: TYPE_NORMAL
- en: The [grad] function has a [has_aux] argument that allows you to return auxiliary
    data. For example, when building machine learning models, you can use it to return
    loss and gradients.
  prefs: []
  type: TYPE_NORMAL
- en: '@jax.jitdef sum_logistic(x):return jnp.sum(1.0 / (1.0 + jnp.exp(-x))),(x +
    1)'
  prefs: []
  type: TYPE_NORMAL
- en: x_small = jnp.arange(6.)
  prefs: []
  type: TYPE_NORMAL
- en: derivative_fn = jax.grad(sum_logistic, has_aux=True) print(derivative_fn(x_small))
  prefs: []
  type: TYPE_NORMAL
- en: (DeviceArray([0.25 , 0.19661194, 0.10499357, 0.04517666, 0.01766271,
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 0.00664806], dtype=float32), DeviceArray([1., 2.,
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 3., 4., 5., 6.], dtype=float32))You can perform advanced automatic differentiation
    using **jax.vjp()**and **jax.jvp().** **## Auto-vectorization with vmap**
  prefs: []
  type: TYPE_NORMAL
- en: vmap(Vectorizing map) allows you write a function that can be applied to a single
    data and then vmap will map it to a batch of data. Without vmap the solution would
    be to loop through the batches while applying the function. Using jit with for
    loops is a little complicated and may be slower.
  prefs: []
  type: TYPE_NORMAL
- en: 'mat = jax.random.normal(key, (150, 100)) batched_x = jax.random.normal(key,
    (10, 100)) def apply_matrix(v):'
  prefs: []
  type: TYPE_NORMAL
- en: return jnp.dot(mat, v)
  prefs: []
  type: TYPE_NORMAL
- en: '@jax.jit'
  prefs: []
  type: TYPE_NORMAL
- en: 'def vmap_batched_apply_matrix(v_batched):'
  prefs: []
  type: TYPE_NORMAL
- en: return jax.vmap(apply_matrix)(v_batched)
  prefs: []
  type: TYPE_NORMAL
- en: print('Auto-vectorized with vmap')
  prefs: []
  type: TYPE_NORMAL
- en: '%timeit vmap_batched_apply_matrix(batched_x).block_until_ready ()'
  prefs: []
  type: TYPE_NORMAL
- en: '*In JAX, the [jax.vmap] transformation is designed to generate a vectorized
    implementation of a function automatically. It does this by tracing the function
    similarly to [jax.jit], and automatically adding batch axes at the beginning of
    each input. If the batch dimension is not the first, you may use the [in_axes] and [out_axes] arguments
    to specify the location of the batch dimension in inputs and outputs. These may
    be an integer if the batch axis is the same for all inputs and outputs, or lists,
    otherwise*. *Matteo Hessel, JAX author.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Parallelization with pmap**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The working of  jax.pmap is similar to jax.vmap. The difference is that jax.pmap is
    meant for parallel execution, that is, computation on multiple devices.  This
    is applicable when training a machine learning model on batches of data.
  prefs: []
  type: TYPE_NORMAL
- en: Computation on batches can occur in different devices then the results are aggregated.
    The [pmap]ed function returns a [ShardedDeviceArray] . This is because the arrays
    are split across all the devices. There is no need to decorate the function with
    jit because the function is jitcompiled by default when using [pmap].
  prefs: []
  type: TYPE_NORMAL
- en: x = np.arange(5)w = np.array([2., 3., 4.])
  prefs: []
  type: TYPE_NORMAL
- en: 'def convolve(x, w):'
  prefs: []
  type: TYPE_NORMAL
- en: output = []
  prefs: []
  type: TYPE_NORMAL
- en: 'for i in range(1, len(x)-1):'
  prefs: []
  type: TYPE_NORMAL
- en: output.append(jnp.dot(x[i-1:i+2], w)) return jnp.array(output)
  prefs: []
  type: TYPE_NORMAL
- en: convolve(x, w) n_devices = jax.local_device_count()
  prefs: []
  type: TYPE_NORMAL
- en: xs = np.arange(5 * n_devices).reshape(-1, 5)
  prefs: []
  type: TYPE_NORMAL
- en: ws = np.stack([w] * n_devices)
  prefs: []
  type: TYPE_NORMAL
- en: jax.pmap(convolve)(xs, ws)
  prefs: []
  type: TYPE_NORMAL
- en: ShardedDeviceArray([[ 11., 20., 29.],
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '.................'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[326., 335., 344.]], dtype=float32)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may need to aggregate data using one of the collective operators, for example,
    to compute the mean of the accuracy or mean of the logits. In that case, you'll
    need to specify an [axis_name]. This name is important to achieve communication
    between devices.
  prefs: []
  type: TYPE_NORMAL
- en: '**Debugging NANs in JAX**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By default, the occurrence of NANs in JAX program will not lead to an error.jnp.divide(0.0,0.0)#
    DeviceArray(nan, dtype=float32, weak_type=True)
  prefs: []
  type: TYPE_NORMAL
- en: You can turn on the NAN checker and your program will error out at the occurrence
    of NANs. You should only use the NAN checker for debugging because it leads to
    performance issues. Also, it doesn't work with [pmap] , use [vmap] instead.
  prefs: []
  type: TYPE_NORMAL
- en: from jax.config import config
  prefs: []
  type: TYPE_NORMAL
- en: config.update("jax_debug_nans", True)
  prefs: []
  type: TYPE_NORMAL
- en: jnp.divide(0.0,0.0)
  prefs: []
  type: TYPE_NORMAL
- en: 'FloatingPointError: invalid value (nan) encountered in div'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Double (64bit) precision**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: JAX enforces single-precision of numbers. For example, you will get a warning
    when you create a [float64] number. If you check the type of the number, you will
    notice that it's [float32].
  prefs: []
  type: TYPE_NORMAL
- en: x = jnp.float64(1.25844)
  prefs: []
  type: TYPE_NORMAL
- en: '/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_num py.py:1806: UserWarning:
    Explicitly requested dtype float64 req uested in array is not available, and will
    be truncated to dtyp e float32\. To enable more dtypes, set the jax_enable_x64
    config uration option or the JAX_ENABLE_X64 shell environment variabl e. See https://github.com/google/jax#current-gotchas
    for more. # lax_internal._check_user_dtype_supported(dtype, "array") # DeviceArray(1.25844,
    dtype=float32)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can use double-precision numbers by setting that in the configuration using [jax_enable_x64].
  prefs: []
  type: TYPE_NORMAL
- en: set this config at the begining of the program from jax.config import config
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: config.update("jax_enable_x64", True)
  prefs: []
  type: TYPE_NORMAL
- en: x = jnp.float64(1.25844)
  prefs: []
  type: TYPE_NORMAL
- en: x
  prefs: []
  type: TYPE_NORMAL
- en: DeviceArray(1.25844, dtype=float64)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**What is a pytree?**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A pytree is a container that holds Python objects. In JAX, it can hold arrays,
    tuples, lists, dictionaries, etc. A Pytree contains leaves. For example, model
    parameters in JAX are pytrees.
  prefs: []
  type: TYPE_NORMAL
- en: example_trees = [
  prefs: []
  type: TYPE_NORMAL
- en: '[1, ''a'', object()],'
  prefs: []
  type: TYPE_NORMAL
- en: (1, (2, 3), ()),
  prefs: []
  type: TYPE_NORMAL
- en: '[1, {''k1'': 2, ''k2'': (3, 4)}, 5], {''a'': 2, ''b'': (2, 3)},'
  prefs: []
  type: TYPE_NORMAL
- en: jnp.array([1, 2, 3]),
  prefs: []
  type: TYPE_NORMAL
- en: ']'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how many leaves they have:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'for pytree in example_trees:'
  prefs: []
  type: TYPE_NORMAL
- en: leaves = jax.tree_leaves(pytree)
  prefs: []
  type: TYPE_NORMAL
- en: 'print(f"{repr(pytree):<45} has {len(leaves)} leaves: {leave s}")'
  prefs: []
  type: TYPE_NORMAL
- en: '[1, ''a'', <object object at 0x7f280a01f6d0>] [1, ''a'', <object object at
    0x7f280a01f6d0>]'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: (1, (2, 3), ())
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1, 2, 3]'
  prefs: []
  type: TYPE_NORMAL
- en: '[1, {''k1'': 2, ''k2'': (3, 4)}, 5]'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1, 2, 3, 4, 5]'
  prefs: []
  type: TYPE_NORMAL
- en: '{''a'': 2, ''b'': (2, 3)}'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[2, 2, 3]'
  prefs: []
  type: TYPE_NORMAL
- en: 'DeviceArray([1, 2, 3], dtype=int64) [DeviceArray([1, 2, 3], dtype=int64)] has
    3 leaves:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'has 3 leaves:has 5 leaves:has 3 leaves: has 1 leaves:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Handling state in JAX**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Training machine learning models will often involve state in areas such as model
    parameters, optimizer state, and stateful Layer such as BatchNorm. However, jit-compiled
    functions must have no side effects. We, therefore, need a way to track and update
    model parameters, optimizer state, and stateful layers. The solution is to define
    the state explicitly.
  prefs: []
  type: TYPE_NORMAL
- en: '**Loading datasets with JAX**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: JAX doesn't ship with any data loading tools. However, JAX recommends using
    data loaders from PyTorch and TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: import tensorflow as tf
  prefs: []
  type: TYPE_NORMAL
- en: Ensure TF does not see GPU and grab all GPU memory. tf.config.set_visible_devices([],
    device_type='GPU')
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: import tensorflow_datasets as tfdsdata_dir = '/tmp/tfds'
  prefs: []
  type: TYPE_NORMAL
- en: Fetch full datasets for evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: tfds.load returns tf.Tensors (or tf.data.Datasets if batch_si ze != -1)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can convert them to NumPy arrays (or iterables of NumPy a rrays) with tfds.dataset_as_numpy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: mnist_data, info = tfds.load(name="mnist", batch_size=-1, data_ dir=data_dir,
    with_info=True)
  prefs: []
  type: TYPE_NORMAL
- en: mnist_data = tfds.as_numpy(mnist_data)
  prefs: []
  type: TYPE_NORMAL
- en: train_data, test_data = mnist_data['train'], mnist_data['test'] num_labels =
    info.features['label'].num_classes
  prefs: []
  type: TYPE_NORMAL
- en: h, w, c = info.features['image'].shape
  prefs: []
  type: TYPE_NORMAL
- en: num_pixels = h * w * c
  prefs: []
  type: TYPE_NORMAL
- en: Full train set
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: train_images, train_labels = train_data['image'], train_data['l abel']
  prefs: []
  type: TYPE_NORMAL
- en: train_images = jnp.reshape(train_images, (len(train_images), nu m_pixels))
  prefs: []
  type: TYPE_NORMAL
- en: train_labels = one_hot(train_labels, num_labels)
  prefs: []
  type: TYPE_NORMAL
- en: Full test set
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: test_images, test_labels = test_data['image'], test_data['labe l']
  prefs: []
  type: TYPE_NORMAL
- en: test_images = jnp.reshape(test_images, (len(test_images), num_p ixels))
  prefs: []
  type: TYPE_NORMAL
- en: test_labels = one_hot(test_labels, num_labels)
  prefs: []
  type: TYPE_NORMAL
- en: print('Train:', train_images.shape, train_labels.shape) print('Test:', test_images.shape,
    test_labels.shape)
  prefs: []
  type: TYPE_NORMAL
- en: 'Train: (60000, 784) (60000, 10) # Test: (10000, 784) (10000, 10)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Building neural networks with JAX**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can build a model from scratch using JAX. However, various neural network
    libraries are built on top of JAX to make building neural networks with JAX easier.
     The Image classification with JAX & Flax article shows how to load data with
    PyTorch and build a convolutional neural network with Jax and Flax.
  prefs: []
  type: TYPE_NORMAL
- en: '**Final thoughts**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this article, we have covered the basics of JAX. We have seen that JAX uses
    XLA and just-in-time compilation to improve the performance of Python functions.
    Specifically, we have covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up JAX to use TPUs on Google Colab.
  prefs: []
  type: TYPE_NORMAL
- en: Comparison between data types in JAX and NumPy. Creating arrays in JAX.
  prefs: []
  type: TYPE_NORMAL
- en: How to generate random numbers in JAX.
  prefs: []
  type: TYPE_NORMAL
- en: Operations on JAX arrays.
  prefs: []
  type: TYPE_NORMAL
- en: Gotchas in JAX, such as using pure functions and the immutability of JAX arrays.
  prefs: []
  type: TYPE_NORMAL
- en: Placing JAX arrays in GPUs or TPUs.
  prefs: []
  type: TYPE_NORMAL
- en: How to use JIT to speed up functions.
  prefs: []
  type: TYPE_NORMAL
- en: '...and so much more'
  prefs: []
  type: TYPE_NORMAL
- en: '**Optimizers in JAX and Flax**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Optimizers are applied when training neural networks to reduce the error between
    the true and predicted values. This optimization is done via gradient descent.
    Gradient descent adjusts errors in the network through a cost function. In JAX,
    optimizers are applied from the Optax library.
  prefs: []
  type: TYPE_NORMAL
- en: 'Optimizers can be classified into two broad categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Adaptive** such as Adam, Adagrad, AdaDelta, and RMSProp.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Accelerated stochastic gradient descent (SGD)** , for example, SGD with momentum,
    heavy-ball method (HB), and Nesterov accelerated gradient (NAG).'
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at common optimizer functions used in JAX and Flax.
  prefs: []
  type: TYPE_NORMAL
- en: '**Adaptive vs stochastic gradient descent (SGD) optimizers**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When performing optimization, adaptive optimizers start with large update steps
    but reduce the step size as they get close to the global minimum. This ensures
    that they don't miss the global minimum.
  prefs: []
  type: TYPE_NORMAL
- en: '**Adaptive** optimizers such as Adam are quite common because they converge
    faster, but they may have poor generalization. **SGD-based optimizers apply a
    global learning rate on all parameters, while adaptive optimizers calculate a
    learning rate for each parameter.**  **## AdaBelief** The authors of AdaBelief
    introduced the optimizer to:'
  prefs: []
  type: TYPE_NORMAL
- en: Converge fast as in adaptive methods.
  prefs: []
  type: TYPE_NORMAL
- en: Have good generalization like SGD.
  prefs: []
  type: TYPE_NORMAL
- en: Be stable during training.
  prefs: []
  type: TYPE_NORMAL
- en: AdaBelief works on the concept of " **belief**" in the current gradient direction.
    If it results in good performance, then that direction is trusted, and large updates
    are applied. Otherwise, it's distrusted and the step size is reduced.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at a Flax training state that applies the AdaBelief optimizer.from
    flax.training import train_state
  prefs: []
  type: TYPE_NORMAL
- en: 'def create_train_state(rng, learning_rate):'
  prefs: []
  type: TYPE_NORMAL
- en: '"""Creates initial `TrainState`."""'
  prefs: []
  type: TYPE_NORMAL
- en: cnn = CNN()
  prefs: []
  type: TYPE_NORMAL
- en: params = cnn.init(rng, jnp.ones([1, size_image, size_image,
  prefs: []
  type: TYPE_NORMAL
- en: 3]))['params']
  prefs: []
  type: TYPE_NORMAL
- en: tx = optax.adabelief(learning_rate)
  prefs: []
  type: TYPE_NORMAL
- en: return train_state.TrainState.create(
  prefs: []
  type: TYPE_NORMAL
- en: apply_fn=cnn.apply, params=params, tx=tx)Here's the performance of AdaBelief
    on various tasks as provided by its authors.![](../images/00007.jpeg)
  prefs: []
  type: TYPE_NORMAL
- en: '**AdaGrad**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AdaGrad works well in situations leading to sparse gradients. Adagrad is an
    algorithm for gradient-based optimization that anneals the learning rate for each
    parameter during training– Optax.
  prefs: []
  type: TYPE_NORMAL
- en: from flax.training import train_state
  prefs: []
  type: TYPE_NORMAL
- en: 'def create_train_state(rng, learning_rate):'
  prefs: []
  type: TYPE_NORMAL
- en: '"""Creates initial `TrainState`."""'
  prefs: []
  type: TYPE_NORMAL
- en: cnn = CNN()
  prefs: []
  type: TYPE_NORMAL
- en: params = cnn.init(rng, jnp.ones([1, size_image, size_image,
  prefs: []
  type: TYPE_NORMAL
- en: 3]))['params']
  prefs: []
  type: TYPE_NORMAL
- en: tx = optax.AdaGrad(learning_rate)
  prefs: []
  type: TYPE_NORMAL
- en: return train_state.TrainState.create(
  prefs: []
  type: TYPE_NORMAL
- en: apply_fn=cnn.apply, params=params, tx=tx)
  prefs: []
  type: TYPE_NORMAL
- en: '**Adam – Adaptive moment estimation**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Adam is a common optimizer in deep learning because it gives good results with
    default parameters, is computationally inexpensive, and uses little memory.
  prefs: []
  type: TYPE_NORMAL
- en: from flax.training import train_state
  prefs: []
  type: TYPE_NORMAL
- en: 'def create_train_state(rng):'
  prefs: []
  type: TYPE_NORMAL
- en: '"""Creates initial `TrainState`."""'
  prefs: []
  type: TYPE_NORMAL
- en: model = LSTMModel()
  prefs: []
  type: TYPE_NORMAL
- en: params = model.init(rng, jnp.array(X_train_padded[0]))['param
  prefs: []
  type: TYPE_NORMAL
- en: s']
  prefs: []
  type: TYPE_NORMAL
- en: tx = optax.adam(0.001,0.9,0.999,1e-07)
  prefs: []
  type: TYPE_NORMAL
- en: return train_state.TrainState.create(
  prefs: []
  type: TYPE_NORMAL
- en: apply_fn=model.apply, params=params, tx=tx)![](../images/00008.jpeg) **Training
    of multilayer neural networks on MNIST images. (a) Neural networks using dropout
    stochastic regularization. (b) Neural networks with deterministic cost function**  **##
    AdamW**
  prefs: []
  type: TYPE_NORMAL
- en: AdamW is Adam with weight decay regularization. Weight decay regularization
    penalizes the cost function making the weights smaller during backpropagarion.
    It results in small weights that lead to better generalization. In some cases,
    Adam with decoupled weight decay leads to better results compared Adam with L2
    regularization.
  prefs: []
  type: TYPE_NORMAL
- en: from flax.training import train_state
  prefs: []
  type: TYPE_NORMAL
- en: 'def create_train_state(rng):'
  prefs: []
  type: TYPE_NORMAL
- en: '"""Creates initial `TrainState`."""'
  prefs: []
  type: TYPE_NORMAL
- en: model = LSTMModel()
  prefs: []
  type: TYPE_NORMAL
- en: params = model.init(rng, jnp.array(X_train_padded[0]))['param
  prefs: []
  type: TYPE_NORMAL
- en: s']tx = optax.adamw(0.001,0.9,0.999,1e-07)return train_state.TrainState.create(apply_fn=model.apply,
    params=params, tx=tx)![](../images/00009.jpeg) **Learning curves (top row) and
    generalization results (bottom row) obtained by a 26 2x96d ResNet trained with
    Adam and AdamW on CIFAR-10**  **## RAdam – Rectified Adam optimizer** RAdam aims
    to solve large variances during the early training stages when applying an adaptive
    learning rate.from flax.training import train_state
  prefs: []
  type: TYPE_NORMAL
- en: 'def create_train_state(rng, learning_rate):'
  prefs: []
  type: TYPE_NORMAL
- en: '"""Creates initial `TrainState`."""'
  prefs: []
  type: TYPE_NORMAL
- en: cnn = CNN()
  prefs: []
  type: TYPE_NORMAL
- en: params = cnn.init(rng, jnp.ones([1, size_image, size_image,
  prefs: []
  type: TYPE_NORMAL
- en: 3]))['params']
  prefs: []
  type: TYPE_NORMAL
- en: tx = optax.radam(learning_rate)
  prefs: []
  type: TYPE_NORMAL
- en: return train_state.TrainState.create(
  prefs: []
  type: TYPE_NORMAL
- en: apply_fn=cnn.apply, params=params, tx=tx)
  prefs: []
  type: TYPE_NORMAL
- en: '**AdaFactor**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AdaFactor is used for training large neural networks because it is implemented
    to reduce memory utilization.from flax.training import train_state
  prefs: []
  type: TYPE_NORMAL
- en: 'def create_train_state(rng, learning_rate):'
  prefs: []
  type: TYPE_NORMAL
- en: '"""Creates initial `TrainState`."""'
  prefs: []
  type: TYPE_NORMAL
- en: cnn = CNN()
  prefs: []
  type: TYPE_NORMAL
- en: params = cnn.init(rng, jnp.ones([1, size_image, size_image,
  prefs: []
  type: TYPE_NORMAL
- en: 3]))['params']
  prefs: []
  type: TYPE_NORMAL
- en: tx = optax.adafactor(learning_rate)
  prefs: []
  type: TYPE_NORMAL
- en: return train_state.TrainState.create(
  prefs: []
  type: TYPE_NORMAL
- en: apply_fn=cnn.apply, params=params, tx=tx)
  prefs: []
  type: TYPE_NORMAL
- en: '**Fromage**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fromage introduces a distance function on deep neural networks called* **deep
    relative trust**. *It requires little to no learning rate tuning.from flax.training
    import train_state
  prefs: []
  type: TYPE_NORMAL
- en: 'def create_train_state(rng, learning_rate): """Creates initial `TrainState`."""
    cnn = CNN()'
  prefs: []
  type: TYPE_NORMAL
- en: params = cnn.init(rng, jnp.ones([1, size_image, size_image,
  prefs: []
  type: TYPE_NORMAL
- en: 3]))['params']
  prefs: []
  type: TYPE_NORMAL
- en: tx = optax.fromage(learning_rate)
  prefs: []
  type: TYPE_NORMAL
- en: return train_state.TrainState.create(
  prefs: []
  type: TYPE_NORMAL
- en: apply_fn=cnn.apply, params=params, tx=tx)
  prefs: []
  type: TYPE_NORMAL
- en: '**Lamb – Layerwise adaptive large batch optimization**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lamb aims to enable the training of deep neural networks by computing gradients
    using large mini-batches. It leads to good performance on attention-based models
    such as Transformers and ResNet-50.
  prefs: []
  type: TYPE_NORMAL
- en: from flax.training import train_state
  prefs: []
  type: TYPE_NORMAL
- en: 'def create_train_state(rng, learning_rate):'
  prefs: []
  type: TYPE_NORMAL
- en: '"""Creates initial `TrainState`."""'
  prefs: []
  type: TYPE_NORMAL
- en: cnn = CNN()
  prefs: []
  type: TYPE_NORMAL
- en: params = cnn.init(rng, jnp.ones([1, size_image, size_image,
  prefs: []
  type: TYPE_NORMAL
- en: 3]))['params']
  prefs: []
  type: TYPE_NORMAL
- en: tx = optax.lamb(learning_rate)
  prefs: []
  type: TYPE_NORMAL
- en: return train_state.TrainState.create(
  prefs: []
  type: TYPE_NORMAL
- en: apply_fn=cnn.apply, params=params, tx=tx)
  prefs: []
  type: TYPE_NORMAL
- en: '**Lars – Layer-wise Adaptive Rate Scaling**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lars is inspired by Lamb to scale SGD to large batch sizes. Lars has been used
    to train AlexNet with an 8K batch size and Resnet-50 with a 32K batch size without
    degrading the accuracy. from flax.training import train_state
  prefs: []
  type: TYPE_NORMAL
- en: 'def create_train_state(rng, learning_rate):'
  prefs: []
  type: TYPE_NORMAL
- en: '"""Creates initial `TrainState`."""'
  prefs: []
  type: TYPE_NORMAL
- en: cnn = CNN()
  prefs: []
  type: TYPE_NORMAL
- en: params = cnn.init(rng, jnp.ones([1, size_image, size_image,
  prefs: []
  type: TYPE_NORMAL
- en: 3]))['params']
  prefs: []
  type: TYPE_NORMAL
- en: tx = optax.lars(learning_rate)
  prefs: []
  type: TYPE_NORMAL
- en: return train_state.TrainState.create(
  prefs: []
  type: TYPE_NORMAL
- en: 'apply_fn=cnn.apply, params=params, tx=tx)![](../images/00010.jpeg)**LARS: Alexnet-BN
    with B=8K ** **## SM3 - Square-root of Minima of Sums of Maxima of Squaredgradients
    Method**'
  prefs: []
  type: TYPE_NORMAL
- en: SM3 was designed to reduce memory utilization when training very large models
    such as Transformer for machine translation, BERT for language modeling, and AmoebaNet-D
    for image classification
  prefs: []
  type: TYPE_NORMAL
- en: from flax.training import train_state
  prefs: []
  type: TYPE_NORMAL
- en: 'def create_train_state(rng, learning_rate):'
  prefs: []
  type: TYPE_NORMAL
- en: '"""Creates initial `TrainState`."""'
  prefs: []
  type: TYPE_NORMAL
- en: cnn = CNN()
  prefs: []
  type: TYPE_NORMAL
- en: params = cnn.init(rng, jnp.ones([1, size_image, size_image,
  prefs: []
  type: TYPE_NORMAL
- en: 3]))['params']
  prefs: []
  type: TYPE_NORMAL
- en: tx = optax.sm3(learning_rate)
  prefs: []
  type: TYPE_NORMAL
- en: return train_state.TrainState.create(
  prefs: []
  type: TYPE_NORMAL
- en: apply_fn=cnn.apply, params=params, tx=tx)![](../images/00011.jpeg)**Top-1 (left)
    and Top-5 (right) test accuracy of AmoebaNet-D on ImageNet ** **## SGD– Stochastic
    Gradient Descent**
  prefs: []
  type: TYPE_NORMAL
- en: SDG implements stochastic gradient descent with support for momentum and Nesterov
    acceleration. Momentum makes obtaining optimal model weights faster by accelerating
    gradient descent in a certain direction.
  prefs: []
  type: TYPE_NORMAL
- en: from flax.training import train_state
  prefs: []
  type: TYPE_NORMAL
- en: 'def create_train_state(rng, learning_rate):'
  prefs: []
  type: TYPE_NORMAL
- en: '"""Creates initial `TrainState`."""'
  prefs: []
  type: TYPE_NORMAL
- en: cnn = CNN()
  prefs: []
  type: TYPE_NORMAL
- en: params = cnn.init(rng, jnp.ones([1, size_image, size_image,
  prefs: []
  type: TYPE_NORMAL
- en: 3]))['params']
  prefs: []
  type: TYPE_NORMAL
- en: tx = optax.sgd(learning_rate)
  prefs: []
  type: TYPE_NORMAL
- en: return train_state.TrainState.create(
  prefs: []
  type: TYPE_NORMAL
- en: apply_fn=cnn.apply, params=params, tx=tx)
  prefs: []
  type: TYPE_NORMAL
- en: '**Noisy SGD**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Noisy SGD is SGD with added noise. Adding noise to gradients can prevent overfitting
    and improve training error and generalization in deep architectures.
  prefs: []
  type: TYPE_NORMAL
- en: from flax.training import train_state
  prefs: []
  type: TYPE_NORMAL
- en: 'def create_train_state(rng, learning_rate):'
  prefs: []
  type: TYPE_NORMAL
- en: '"""Creates initial `TrainState`."""'
  prefs: []
  type: TYPE_NORMAL
- en: cnn = CNN()
  prefs: []
  type: TYPE_NORMAL
- en: params = cnn.init(rng, jnp.ones([1, size_image, size_image,
  prefs: []
  type: TYPE_NORMAL
- en: 3]))['params']
  prefs: []
  type: TYPE_NORMAL
- en: tx = optax.noisy_sgd(learning_rate)
  prefs: []
  type: TYPE_NORMAL
- en: return train_state.TrainState.create(
  prefs: []
  type: TYPE_NORMAL
- en: 'apply_fn=cnn.apply, params=params, tx=tx)![](../images/00012.jpeg) **: Noise
    vs. No Noise in our experiment with tables containing 5 columns. The models trained
    with noise generalizes almost always better**  **## Optimistic GD** An Optimistic
    Gradient Descent optimizer.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Optimistic gradient descent is an approximation of extra-gradient methods
    which require multiple gradient calls to compute the next update. It has strong
    formal guarantees for last-iterate convergence in min-max games, for which standard
    gradient descent can oscillate or even diverge– Optax.*'
  prefs: []
  type: TYPE_NORMAL
- en: from flax.training import train_state
  prefs: []
  type: TYPE_NORMAL
- en: 'def create_train_state(rng, learning_rate):'
  prefs: []
  type: TYPE_NORMAL
- en: '"""Creates initial `TrainState`."""'
  prefs: []
  type: TYPE_NORMAL
- en: cnn = CNN()
  prefs: []
  type: TYPE_NORMAL
- en: params = cnn.init(rng, jnp.ones([1, size_image, size_image, tx = optax.optimistic_gradient_descent(learning_rate)
    return train_state.TrainState.create(
  prefs: []
  type: TYPE_NORMAL
- en: apply_fn=cnn.apply, params=params, tx=tx)
  prefs: []
  type: TYPE_NORMAL
- en: '**Differentially Private SGD**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Differentially Private SGD is used for training networks with sensitive data.
    It ensures that the models don't expose sensitive training data.
  prefs: []
  type: TYPE_NORMAL
- en: from flax.training import train_state
  prefs: []
  type: TYPE_NORMAL
- en: 'def create_train_state(rng, learning_rate):'
  prefs: []
  type: TYPE_NORMAL
- en: '"""Creates initial `TrainState`."""'
  prefs: []
  type: TYPE_NORMAL
- en: cnn = CNN()
  prefs: []
  type: TYPE_NORMAL
- en: params = cnn.init(rng, jnp.ones([1, size_image, size_image,
  prefs: []
  type: TYPE_NORMAL
- en: 3]))['params']
  prefs: []
  type: TYPE_NORMAL
- en: tx = optax.dpsgd(learning_rate)
  prefs: []
  type: TYPE_NORMAL
- en: return train_state.TrainState.create(
  prefs: []
  type: TYPE_NORMAL
- en: apply_fn=cnn.apply, params=params, tx=tx)
  prefs: []
  type: TYPE_NORMAL
- en: '**RMSProp**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RMSProp works by dividing the gradient of a running average of its recent magnitude–Hinton.from
    flax.training import train_state
  prefs: []
  type: TYPE_NORMAL
- en: 'def create_train_state(rng, learning_rate):'
  prefs: []
  type: TYPE_NORMAL
- en: '"""Creates initial `TrainState`."""'
  prefs: []
  type: TYPE_NORMAL
- en: cnn = CNN()
  prefs: []
  type: TYPE_NORMAL
- en: params = cnn.init(rng, jnp.ones([1, size_image, size_image, tx = optax.rmsprop(learning_rate)
  prefs: []
  type: TYPE_NORMAL
- en: return train_state.TrainState.create(
  prefs: []
  type: TYPE_NORMAL
- en: apply_fn=cnn.apply, params=params, tx=tx)
  prefs: []
  type: TYPE_NORMAL
- en: '**Yogi**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Yogi is a modified Adam optimizer for optimizing the stochastic nonconvex optimization
    problem.from flax.training import train_state
  prefs: []
  type: TYPE_NORMAL
- en: 'def create_train_state(rng, learning_rate):'
  prefs: []
  type: TYPE_NORMAL
- en: '"""Creates initial `TrainState`."""'
  prefs: []
  type: TYPE_NORMAL
- en: cnn = CNN()
  prefs: []
  type: TYPE_NORMAL
- en: params = cnn.init(rng, jnp.ones([1, size_image, size_image,
  prefs: []
  type: TYPE_NORMAL
- en: 3]))['params']
  prefs: []
  type: TYPE_NORMAL
- en: tx = optax.yogi(learning_rate)
  prefs: []
  type: TYPE_NORMAL
- en: return train_state.TrainState.create(
  prefs: []
  type: TYPE_NORMAL
- en: apply_fn=cnn.apply, params=params, tx=tx)![](../images/00013.jpeg) **Comparison
    of highly tuned RMSProp optimizer with YOGI for Inception-Resnet-v2 on Imagenet.
    First plot shows the mini-batch estimate of the loss during training, while the
    remaining two plots show top-1 and top-5 error rates on the held-out Imagenet
    validation set**  **## Final thoughts**
  prefs: []
  type: TYPE_NORMAL
- en: 'Choosing the right optimizer function determines how long training a network
    will take. It also determines how well the model performs. Choosing the appropriate
    optimizer functions is therefore paramount. This article discusses various optimizer
    functions that you can apply to your JAX and Flax networks. In particular, you
    walk away with nuggets about these optimizers:'
  prefs: []
  type: TYPE_NORMAL
- en: Adam optimizer in JAX.
  prefs: []
  type: TYPE_NORMAL
- en: RMSProp optimizer in Flax.
  prefs: []
  type: TYPE_NORMAL
- en: Stochastic Gradient Descent in JAX.
  prefs: []
  type: TYPE_NORMAL
- en: ..to mention a few.
  prefs: []
  type: TYPE_NORMAL
- en: '**JAX loss functions**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Loss functions are at the core of training machine learning. They can be used
    to identify how well the model is performing on a dataset. Poor performance leads
    to a very high loss, while a well-performing model will have a lower loss. Therefore,
    the choice of a loss function is an important one when building machine learning
    models.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we'll look at the loss functions available in JAX and how you
    can use them.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is a loss function?**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Machine learning models learn by evaluating predictions against true values
    and adjusting the weights. The objective is to obtain the weights that minimize
    the **loss function**, that is the **error**. The loss function is also referred
    to as the **cost function**. The choice of a loss function depends on the problem.
    The two most common problems are classification and regression problems. Each
    will require a different set of loss functions.
  prefs: []
  type: TYPE_NORMAL
- en: '**Creating custom loss functions in JAX**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When training networks with JAX, you'll need to obtain the logits at the training
    stage. These logits are used for computing the loss. You'll then need to evaluate
    the loss function and its gradient. The gradient is used to update the model parameters.
    At this point, you can compute the training metrics for the model.
  prefs: []
  type: TYPE_NORMAL
- en: 💡 ***What are logits?** Logits are unnormalized log probabilities.*
  prefs: []
  type: TYPE_NORMAL
- en: 'def compute_loss(params,images,labels):'
  prefs: []
  type: TYPE_NORMAL
- en: 'logits = CNN().apply({''params'': params}, images) loss = cross_entropy_loss(logits=logits,
    labels=labels) return loss, logits'
  prefs: []
  type: TYPE_NORMAL
- en: '@jax.jitdef train_step(state,images, labels): """Train for a single step."""
    (_, logits), grads = jax.value_and_grad(compute_loss, has_aux'
  prefs: []
  type: TYPE_NORMAL
- en: =True)(state.params,images,labels)
  prefs: []
  type: TYPE_NORMAL
- en: state = state.apply_gradients(grads=grads)
  prefs: []
  type: TYPE_NORMAL
- en: metrics = compute_metrics(logits=logits, labels=labels) return state, metrics
  prefs: []
  type: TYPE_NORMAL
- en: You can use JAX functions such as [log_sigmoid] and [log_softmax] to build custom
    loss functions. You can even write your loss functions from scratch without using
    these functions.
  prefs: []
  type: TYPE_NORMAL
- en: Here is an example of computing the sigmoid binary cross entropy loss.
  prefs: []
  type: TYPE_NORMAL
- en: import jax
  prefs: []
  type: TYPE_NORMAL
- en: 'def custom_sigmoid_binary_cross_entropy(logits, labels): log_p = jax.nn.log_sigmoid(logits)'
  prefs: []
  type: TYPE_NORMAL
- en: log_not_p = jax.nn.log_sigmoid(-logits)
  prefs: []
  type: TYPE_NORMAL
- en: return -labels * log_p - (1\. - labels) * log_not_p
  prefs: []
  type: TYPE_NORMAL
- en: 'custom_sigmoid_binary_cross_entropy(0.5,0.0) # DeviceArray(0.974077, dtype=float32,
    weak_type=True)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Which loss functions are available in JAX?**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Building custom loss functions for your networks can introduce errors in your
    program. Furthermore, you have to take the burden of maintaining these functions.
    However, if the loss function you want is unavailable, there is a strong case
    for creating a custom loss function. Be that as it may, there is no need to reinvent
    the wheel and rewrite the already implemented loss functions.
  prefs: []
  type: TYPE_NORMAL
- en: JAX doesn't ship with any loss functions. In JAX, we use optax for defining
    loss functions. It's important to ensure that you use JAXcompatible libraries
    to take advantage of functions such as [JIT], [vmap] and [pmap] that make your
    programs faster.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at some of the loss functions available in optax.
  prefs: []
  type: TYPE_NORMAL
- en: '**Sigmoid binary cross entropy**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The sigmoid binary cross entropy loss is computed
  prefs: []
  type: TYPE_NORMAL
- en: using optax.sigmoid_binary_cross_entropy. The function expects
  prefs: []
  type: TYPE_NORMAL
- en: logits and class labels. It is used in problems where the classes are not mutually
    exclusive. For example, the model can predict that the image contains two objects
    in an image classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: optax.sigmoid_binary_cross_entropy(0.5,0.0)# DeviceArray(0.974077, dtype=float32,
    weak_type=True)
  prefs: []
  type: TYPE_NORMAL
- en: '**Softmax cross entropy**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The softmax cross entropy function is used where the classes are mutually exclusive.
    For example, in the MNIST dataset, each digit has exactly one label. The function
    expects an array of logits and probability distributions. The probability distribution
    sum to 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'logits = jnp.array([0.50,0.60,0.70,0.30,0.25]) labels = jnp.array([0.20,0.30,0.10,0.20,0.2])
    optax.softmax_cross_entropy(logits,labels) # DeviceArray(1.6341426, dtype=float32)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cosine distance**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The cosine distance measures the cosine distance between targets and predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'predictions = jnp.array([0.50,0.60,0.70,0.30,0.25]) targets = jnp.array([0.20,0.30,0.10,0.20,0.2])
    optax.cosine_distance(predictions,targets,epsilon=0.7) # DeviceArray(0.4128204,
    dtype=float32)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cosine similarity**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The cosine similarity loss measures the cosine similarity between the true and
    predicted values. The cosine similarity is the cosine of the angle between two
    vectors. This is obtained by the dot product of the vectors divided by the product
    of their lengths.
  prefs: []
  type: TYPE_NORMAL
- en: The result is a number between -1 and 1\. 0 shows orthogonality, while numbers
    closer to -1 indicate similarity. Numbers close to  1 portray high dissimilarity.
  prefs: []
  type: TYPE_NORMAL
- en: 'predictions = jnp.array([0.50,0.60,0.70,0.30,0.25]) targets = jnp.array([0.20,0.30,0.10,0.20,0.2])
    optax.cosine_similarity(predictions,targets,epsilon=0.5) # DeviceArray(0.8220514,
    dtype=float32)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Huber loss**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Huber loss is used for regression problems. It is less sensitive to outliers
    compared to the squared error loss. A variant of the Huber loss that can be used
    in classification problems exists.
  prefs: []
  type: TYPE_NORMAL
- en: logits = jnp.array([0.50,0.60,0.70,0.30,0.25])
  prefs: []
  type: TYPE_NORMAL
- en: labels = jnp.array([0.20,0.30,0.10,0.20,0.2])
  prefs: []
  type: TYPE_NORMAL
- en: optax.huber_loss(logits,labels)
  prefs: []
  type: TYPE_NORMAL
- en: DeviceArray([0.045 , 0.045 , 0.17999998, 0.005 ,
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 0.00125 ], dtype=float32)
  prefs: []
  type: TYPE_NORMAL
- en: '**l2 loss**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: L2 loss function is the Least Square Errors. The L2 loss aims at minimizing
    the sum of the squared differences between the true and predicted values. The
    Mean Squared Error is the mean of all L2 loss values.
  prefs: []
  type: TYPE_NORMAL
- en: predictions = jnp.array([0.50,0.60,0.70,0.30,0.25]) targets = jnp.array([0.20,0.30,0.10,0.20,0.2])
  prefs: []
  type: TYPE_NORMAL
- en: optax.l2_loss(predictions,targets)
  prefs: []
  type: TYPE_NORMAL
- en: DeviceArray([0.045 , 0.045 , 0.17999998, 0.005 ,
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 0.00125 ], dtype=float32)
  prefs: []
  type: TYPE_NORMAL
- en: '**log cosh**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[log_cosh] is the logarithm of the hyperbolic cosine of the prediction error.'
  prefs: []
  type: TYPE_NORMAL
- en: 💡  *log(cosh(x)) is approximately equal to (x ** 2) / 2 for small x and to abs(x)
    - log(2) for large x. This means that 'logcosh' works mostly like the mean squared
    error, but will not be so strongly affected by the occasional wildly incorrect
    prediction. TensorFlow Docs*
  prefs: []
  type: TYPE_NORMAL
- en: predictions = jnp.array([0.50,0.60,0.70,0.30,0.25]) targets = jnp.array([0.20,0.30,0.10,0.20,0.2])
  prefs: []
  type: TYPE_NORMAL
- en: optax.log_cosh(predictions,targets)
  prefs: []
  type: TYPE_NORMAL
- en: DeviceArray([0.04434085, 0.04434085, 0.17013526, 0.00499171,
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 0.00124949], dtype=float32)
  prefs: []
  type: TYPE_NORMAL
- en: '**Smooth labels**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[optax.smooth_labels]  is used together with a cross-entropy loss to smooth
    labels. It returns a smoothed version of the one hot input labels. Label smoothing
    has been applied in image classification, language translation, and speech recognition
    to prevent models from becoming overconfident.'
  prefs: []
  type: TYPE_NORMAL
- en: labels = jnp.array([0.20,0.30,0.10,0.20,0.2])
  prefs: []
  type: TYPE_NORMAL
- en: optax.smooth_labels(labels,alpha=0.4)
  prefs: []
  type: TYPE_NORMAL
- en: DeviceArray([0.2 , 0.26, 0.14, 0.2 , 0.2 ], dtype=float32)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Computing loss with JAX Metrics**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: JAX Metrics is an open-source package for computing losses and metrics in JAX.
    It provides a Keras-like API for computing model loss and metrics.
  prefs: []
  type: TYPE_NORMAL
- en: For example, here is how you use the library to compute the crossentropy loss.
    Similar to Keras, the losses can be computed by either instantiating the [Loss] or [loss].
  prefs: []
  type: TYPE_NORMAL
- en: pip install jax_metrics import jax_metrics as jm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: crossentropy = jm.losses.Crossentropy() logits = jnp.array([0.50,0.60,0.70,0.30,0.25])
    y = jnp.array([0.50,0.60,0.70,0.30,0.25]) crossentropy(target=y, preds=logits)
  prefs: []
  type: TYPE_NORMAL
- en: DeviceArray(3.668735, dtype=float32)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'logits = jnp.array([0.50,0.60,0.70,0.30,0.25]) y = jnp.array([0.50,0.60,0.70,0.30,0.25])
    jm.losses.crossentropy(target=y, preds=logits) # DeviceArray(3.668735, dtype=float32)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is what the code would like in a JAX training step.import jax_metrics
    as jmmetric = jm.metrics.Accuracy()@jax.jitdef init_step(metric: jm.Metric) ->
    jm.Metric: return metric.init()'
  prefs: []
  type: TYPE_NORMAL
- en: 'def loss_fn(params, metric, x, y): ...'
  prefs: []
  type: TYPE_NORMAL
- en: metric = metric.update(target=y, preds=logits) ...
  prefs: []
  type: TYPE_NORMAL
- en: return loss, metric@jax.jitdef train_step(params, metric, x, y):grads, metric
    = jax.grad(loss_fn, has_aux=True)(
  prefs: []
  type: TYPE_NORMAL
- en: params, metric, x, y
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: '...'
  prefs: []
  type: TYPE_NORMAL
- en: return params, metric
  prefs: []
  type: TYPE_NORMAL
- en: '@jax.jitdef reset_step(metric: jm.Metric) -> jm.Metric: return metric.reset()The
    losses we have seen earlier can also be computed using JAX Metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: '! pip install jax_metrics'
  prefs: []
  type: TYPE_NORMAL
- en: import jax_metrics as jm
  prefs: []
  type: TYPE_NORMAL
- en: target = jnp.array([50,60,70,30,25]) preds = jnp.array([0.50,0.60,0.70,0.30,0.25])
    huber_loss = jm.losses.Huber()
  prefs: []
  type: TYPE_NORMAL
- en: 'huber_loss(target=target, preds=preds) # DeviceArray(46.030003, dtype=float32)'
  prefs: []
  type: TYPE_NORMAL
- en: target = jnp.array([50,60,70,30,25])
  prefs: []
  type: TYPE_NORMAL
- en: preds = jnp.array([0.50,0.60,0.70,0.30,0.25])
  prefs: []
  type: TYPE_NORMAL
- en: 'jm.losses.mean_absolute_error(target=target, preds=preds) # DeviceArray(46.530003,
    dtype=float32)'
  prefs: []
  type: TYPE_NORMAL
- en: rng = jax.random.PRNGKey(42)
  prefs: []
  type: TYPE_NORMAL
- en: target = jax.random.randint(rng, shape=(2, 3), minval=0, maxval =2)
  prefs: []
  type: TYPE_NORMAL
- en: preds = jax.random.uniform(rng, shape=(2, 3))
  prefs: []
  type: TYPE_NORMAL
- en: jm.losses.cosine_similarity(target, preds, axis=1)
  prefs: []
  type: TYPE_NORMAL
- en: DeviceArray([-0.8602638 , -0.33731455], dtype=float32) target = jnp.array([50,60,70,30,25])
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: preds = jnp.array([0.50,0.60,0.70,0.30,0.25])
  prefs: []
  type: TYPE_NORMAL
- en: jm.losses.mean_absolute_percentage_error(target=target, preds=p reds)
  prefs: []
  type: TYPE_NORMAL
- en: DeviceArray(98.99999, dtype=float32)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: target = jnp.array([50,60,70,30,25])
  prefs: []
  type: TYPE_NORMAL
- en: preds = jnp.array([0.50,0.60,0.70,0.30,0.25])
  prefs: []
  type: TYPE_NORMAL
- en: jm.losses.mean_squared_logarithmic_error(target=target, preds=p reds)
  prefs: []
  type: TYPE_NORMAL
- en: DeviceArray(11.7779, dtype=float32)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'target = jnp.array([0.50,0.60,0.70,0.30,0.25]) preds = jnp.array([0.50,0.60,0.70,0.30,0.25])
    jm.losses.mean_squared_error(target=target, preds=preds) # DeviceArray(0., dtype=float32)'
  prefs: []
  type: TYPE_NORMAL
- en: '**How to monitor JAX loss functions**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Monitoring the loss of your network is important because it indicates whether
    it's learning or not. A glance at the loss can tell you if there are any problems
    in the network, such as overfitting. One way to monitor the loss is to print the
    training and validation loss as the network is training.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00014.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: You can also plot the training and validation loss to represent the training
    visually.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00015.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '**Why JAX loss nan happens**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: JAX will not show errors when NANs occur in your program. This is by design
    because of the complexities involved in showing errors from accelerators. When
    debugging, you can turn on the NAN checker to show NAN errors. NANs should be
    fixed because the network stops learning when they occur.
  prefs: []
  type: TYPE_NORMAL
- en: from jax.config import config
  prefs: []
  type: TYPE_NORMAL
- en: config.update("jax_debug_nans", True)
  prefs: []
  type: TYPE_NORMAL
- en: jnp.divide(0.0,0.0)
  prefs: []
  type: TYPE_NORMAL
- en: 'FloatingPointError: invalid value (nan) encountered in div'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'However, what produces NANs in a network. There are various factors, not limited
    to:'
  prefs: []
  type: TYPE_NORMAL
- en: The dataset has not been scaled.
  prefs: []
  type: TYPE_NORMAL
- en: There are NANs in the training set. The occurrence of infinite values in the
    training data.
  prefs: []
  type: TYPE_NORMAL
- en: Wrong optimizer function. Exploding gradients leading to large updates to training
    weights. Using a very large learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: '**Final thoughts**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this article, we have seen that choosing the right loss function is critical
    to the learning of a network. We have also discussed various loss functions in
    JAX. More precisely, we have coved:'
  prefs: []
  type: TYPE_NORMAL
- en: What is a loss function?
  prefs: []
  type: TYPE_NORMAL
- en: How to create custom loss functions in JAX.
  prefs: []
  type: TYPE_NORMAL
- en: Loss functions available in JAX.
  prefs: []
  type: TYPE_NORMAL
- en: Computing loss with JAX Metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring loss in JAX.
  prefs: []
  type: TYPE_NORMAL
- en: How to avoid NANs in JAX.
  prefs: []
  type: TYPE_NORMAL
- en: '**Activation functions in JAX and Flax**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Activation functions are applied in neural networks to ensure that the network
    outputs the desired result. The activations functions cap the output within a
    specific range. For instance, when solving a binary classification problem, the
    outcome should be a number between 0 and 1\. This indicates the probability of
    an item belonging to either of the two classes.
  prefs: []
  type: TYPE_NORMAL
- en: However, in a regression problem, you want the numerical prediction of a quantity,
    for example, the price of an item. You should, therefore, choose an appropriate
    activation function for the problem being solved.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at common activation functions in JAX and Flax.
  prefs: []
  type: TYPE_NORMAL
- en: '**ReLU – Rectified linear unit**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The  **ReLU activation function** is primarily used in the hidden layers of
    neural networks to ensure non-linearity. The function caps all outputs to zero
    and above. Outputs below zero are returned as zero, while numbers above zero are
    returned as they are. This ensures that there are no negative numbers in the network.
  prefs: []
  type: TYPE_NORMAL
- en: On line 9 we apply the ReLu activation function after the convolution layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'import flaxfrom flax import linen as nnclass CNN(nn.Module):'
  prefs: []
  type: TYPE_NORMAL
- en: '@nn.compact'
  prefs: []
  type: TYPE_NORMAL
- en: 'def __call__(self, x):'
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.Conv(features=32, kernel_size=(3, 3))(x)
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.relu(x)
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.Conv(features=64, kernel_size=(3, 3))(x)
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.relu(x)
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))
  prefs: []
  type: TYPE_NORMAL
- en: x = x.reshape((x.shape[0], -1))
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.Dense(features=256)(x)
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.relu(x)
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.Dense(features=2)(x)
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.log_softmax(x)
  prefs: []
  type: TYPE_NORMAL
- en: return x
  prefs: []
  type: TYPE_NORMAL
- en: '**PReLU– Parametric Rectified Linear Unit**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Parametric Rectified Linear Unit**  is ReLU with extra parameters equal to
    the number of channels. It works by introducing *a– *a learnable parameter. PReLU
    allows for non-negative values.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00016.gif)x = nn.PReLU(x)'
  prefs: []
  type: TYPE_IMG
- en: '**Sigmoid**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The  **sigmoid activation function **caps output to a number between 0 and 1
    and is mainly used for binary classification tasks. Sigmoid is used where the
    classes are non-exclusive. For example, an image can have a car, a building, a
    tree, etc. Just because there is a car in the image doesn’t mean a tree can’t
    be in the picture. Use the sigmoid function when there is more than one correct
    answer.
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.sigmoid(x)
  prefs: []
  type: TYPE_NORMAL
- en: '**Log sigmoid**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Log sigmoid** computes the log of the sigmoid activation, and its output
    is within the range of −∞ to 0.![](../images/00017.gif)x = nn.log_sigmoid(x)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Softmax**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The  **softmax activation function** is a variant of the sigmoid function used
    in multi-class problems where labels are mutually exclusive. For example, a picture
    is either grayscale or color. Use the softmax activation when there is only one
    correct answer.
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.softmax(x)
  prefs: []
  type: TYPE_NORMAL
- en: '**Log softmax**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Log softmax **computes the logarithm of the softmax function, which rescales
    elements to the range −∞ to 0.![](../images/00018.gif)x = nn.log_softmax(x)'
  prefs: []
  type: TYPE_NORMAL
- en: '**ELU – Exponential linear unit activation**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**ELU activation**  function helps in solving the vanishing and exploding gradients
    problem. Unlike ReLu, ELU allows negative numbers pushing the mean unit activations
    closer to zero. ELUs may lead to faster training and better generalization in
    networks with more than five layers.'
  prefs: []
  type: TYPE_NORMAL
- en: For values above zero the number is returned as is but for numbers below zeros
    they are a number that is less that but close to zero.![](../images/00019.gif)x
    = nn.elu(x)
  prefs: []
  type: TYPE_NORMAL
- en: '**CELU – Continuouslydifferentiable exponential linear unit**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CELU is ELU that is continuously differentiable.![](../images/00020.gif)x =
    nn.celu(x)
  prefs: []
  type: TYPE_NORMAL
- en: '**GELU– Gaussian error linear unit activation**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**GELU** non-linearity weights inputs by their value rather than gates inputs
    by their sign as in ReLU– Source.![](../images/00021.gif)x = nn.gelu(x) ![](../images/00022.jpeg)'
  prefs: []
  type: TYPE_NORMAL
- en: '**GLU – Gated linear unit activation**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**GLU**  is computed as [*GLU* ( *a* , *b* )= *a* ⊗ *σ* ( *b* )]. It has been
    applied in Gated CNNs for natural language processing. In the formula, the b gate
    controls what information is passed to the next layer. GLU helps tackle the vanishing
    gradient problem.'
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.glu(x)
  prefs: []
  type: TYPE_NORMAL
- en: '**Soft sign**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The  **Soft sign** activation function caps values between -1 and 1\. It is
    similar to the hyperbolic tangent activation function– tanh. The difference is
    that tanh converges exponentially while  Soft sign converges polynomially.
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.soft_sign(x) ![](../images/00023.gif)
  prefs: []
  type: TYPE_NORMAL
- en: '**Softplus**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Softplus activation** returns values as zero and above. It is a smooth
    version of the ReLu.x = nn.soft_plus(x) ![](../images/00024.gif)![](../images/00025.jpeg)**The
    Softplus activation ** **## Swish–Sigmoid Linear Unit( SiLU)**
  prefs: []
  type: TYPE_NORMAL
- en: The SiLU activation function is computed as [x * sigmoid(beta * x)] where beta
    is the hyperparameter for Swish activation function. SiLU, is, therefore, computed
    by multiplying the sigmoid function with its input.
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.swish(x)![](../images/00026.gif)
  prefs: []
  type: TYPE_NORMAL
- en: '**Custom activation functions in JAX and Flax**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can also define custom activation functions in JAX. For example, here''s
    how you''d define the LeakyReLu activation function.from flax import linen as
    nnimport jax.numpy as jnpclass LeakyReLU(nn.Module):alpha : float = 0.1def __call__(self,
    x):return jnp.where(x > 0, x, self.alpha * x)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Final thoughts**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have learned about the various activation functions you can use in JAX and
    Flax. You have also seen that you can create new functions by creating a class
    that implements the [__call__] method.
  prefs: []
  type: TYPE_NORMAL
- en: '**How to load datasets in JAX with TensorFlow**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: JAX doesn't ship with data loading utilities. This keeps JAX focused on providing
    a fast tool for building and training machine learning models. Loading data in
    JAX is done using
  prefs: []
  type: TYPE_NORMAL
- en: either TensorFlow or PyTorch. This article will focus on how to load datasets
    in JAX using TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Let's dive in!
  prefs: []
  type: TYPE_NORMAL
- en: '**How to load text data in JAX**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's use the IMDB dataset from Kaggle to illustrate how to load text datasets
    with JAX. We'll use the Kaggle Python library to download the data. That requires
    your Kaggle username and key. Head over
  prefs: []
  type: TYPE_NORMAL
- en: https://www.kaggle.com/your_username/ account to obtain the API
  prefs: []
  type: TYPE_NORMAL
- en: key.
  prefs: []
  type: TYPE_NORMAL
- en: The library downloads the data as a zip file. We'll therefore extract it afterward.
  prefs: []
  type: TYPE_NORMAL
- en: import os
  prefs: []
  type: TYPE_NORMAL
- en: Obtain from https://www.kaggle.com/username/account os.environ["KAGGLE_USERNAME"]="YOUR_KAGGLE_USERNAME"
    os.environ["KAGGLE_KEY"]="YOUR_KAGGLE_KEY"
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: import kaggle
  prefs: []
  type: TYPE_NORMAL
- en: '!kaggle datasets download lakshmi25npathi/imdb-dataset-of-50k-m ovie-reviews'
  prefs: []
  type: TYPE_NORMAL
- en: import zipfile
  prefs: []
  type: TYPE_NORMAL
- en: with zipfile.ZipFile('imdb-dataset-of-50k-movie-reviews.zip',
  prefs: []
  type: TYPE_NORMAL
- en: '''r'') as zip_ref:zip_ref.extractall(''imdb-dataset-of-50k-movie-reviews'')Next,
    import the standard data science packages and view a sample of the data.'
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  prefs: []
  type: TYPE_NORMAL
- en: import pandas as pd
  prefs: []
  type: TYPE_NORMAL
- en: from numpy import array
  prefs: []
  type: TYPE_NORMAL
- en: import tensorflow as tf
  prefs: []
  type: TYPE_NORMAL
- en: from sklearn.model_selection import train_test_split
  prefs: []
  type: TYPE_NORMAL
- en: from sklearn.preprocessing import LabelEncoder
  prefs: []
  type: TYPE_NORMAL
- en: import matplotlib.pyplot as plt
  prefs: []
  type: TYPE_NORMAL
- en: df = pd.read_csv("imdb-dataset-of-50k-movie-reviews/IMDB Dataset.c df.head()
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00027.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '**Clean the text data**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's do some processing of the data before we proceed to load it using TensorFlow.
    Standard processing in text problems is to remove stop words. Stop words are common
    words such as a , the that don't help the model in identifying the polarity of
    a
  prefs: []
  type: TYPE_NORMAL
- en: sentence. NLTK provides the stops words. We can, therefore, write a function
    to remove them from the IMDB dataset.
  prefs: []
  type: TYPE_NORMAL
- en: import nltk
  prefs: []
  type: TYPE_NORMAL
- en: from nltk.corpus import stopwords
  prefs: []
  type: TYPE_NORMAL
- en: nltk.download('stopwords')
  prefs: []
  type: TYPE_NORMAL
- en: 'def remove_stop_words(review):'
  prefs: []
  type: TYPE_NORMAL
- en: review_minus_sw = []
  prefs: []
  type: TYPE_NORMAL
- en: stop_words = stopwords.words('english')
  prefs: []
  type: TYPE_NORMAL
- en: review = review.split()
  prefs: []
  type: TYPE_NORMAL
- en: cleaned_review = [review_minus_sw.append(word) for word in
  prefs: []
  type: TYPE_NORMAL
- en: review if word not in stop_words]
  prefs: []
  type: TYPE_NORMAL
- en: cleaned_review = ' '.join(review_minus_sw)
  prefs: []
  type: TYPE_NORMAL
- en: return cleaned_review
  prefs: []
  type: TYPE_NORMAL
- en: df['review'] = df['review'].apply(remove_stop_words) view raw
  prefs: []
  type: TYPE_NORMAL
- en: '**Label encode the sentiment column**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Convert the sentiment column to numerical representation using Scikit-learn's
    label encoder. This is important because neural networks expect numerical data.
  prefs: []
  type: TYPE_NORMAL
- en: labelencoder = LabelEncoder()
  prefs: []
  type: TYPE_NORMAL
- en: df = df.assign(sentiment = labelencoder.fit_transform(df["senti ment"]))
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00028.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '**Text preprocessing with TensorFlow**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have converted the sentiment column to a numerical
  prefs: []
  type: TYPE_NORMAL
- en: representation. However, the reviews are still in text form. We need to convert
    them to numbers as well.
  prefs: []
  type: TYPE_NORMAL
- en: We start by splitting the dataset into a training and testing set.
  prefs: []
  type: TYPE_NORMAL
- en: from sklearn.model_selection import train_test_split df = df.drop_duplicates()
  prefs: []
  type: TYPE_NORMAL
- en: docs = df['review']
  prefs: []
  type: TYPE_NORMAL
- en: labels = array(df['sentiment'])
  prefs: []
  type: TYPE_NORMAL
- en: X_train, X_test , y_train, y_test = train_test_split(docs, labe ls , test_size
    = 0.20, random_state=0)
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we use TensorFlow''s TextVectorization function to convert the text data
    to integer representations. The function expects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[standardize]  used to specify how the text data is processed. For example,
    the [lower_and_strip_punctuation] option will lowercase the data and remove punctuations.'
  prefs: []
  type: TYPE_NORMAL
- en: max_tokens dictates the maximum size of the vocabulary. [output_mode] determines
    the output of the vectorization layer. Setting [int] outputs integers.
  prefs: []
  type: TYPE_NORMAL
- en: '[output_sequence_length]  indicates the maximum length of the output sequence.
    This ensures that all sequences have the same length.'
  prefs: []
  type: TYPE_NORMAL
- en: import tensorflow as tf
  prefs: []
  type: TYPE_NORMAL
- en: 'max_features = 5000 # Maximum vocab size.'
  prefs: []
  type: TYPE_NORMAL
- en: batch_size = 32
  prefs: []
  type: TYPE_NORMAL
- en: 'max_len = 512 # Sequence length to pad the outputs to. vectorize_layer = tf.keras.layers.TextVectorization(standardize
    =''lower_and_strip_punctuation'',max_tokens=max_features,output_m ode=''int'',output_sequence_length=max_len)'
  prefs: []
  type: TYPE_NORMAL
- en: vectorize_layer.adapt(X_train,batch_size=None)
  prefs: []
  type: TYPE_NORMAL
- en: Next, apply this layer to the training and testing data.X_train_padded = vectorize_layer(X_train)
    X_test_padded = vectorize_layer(X_test)![](../images/00029.jpeg)
  prefs: []
  type: TYPE_NORMAL
- en: Convert the data to a TensorFlow dataset and create a function to fetch the
    data in batches. We also convert the data to NumPy arrays because JAX expects
    NumPy or JAX arrays.
  prefs: []
  type: TYPE_NORMAL
- en: import tensorflow_datasets as tfds
  prefs: []
  type: TYPE_NORMAL
- en: training_data = tf.data.Dataset.from_tensor_slices((X_train_pad ded, y_train))
  prefs: []
  type: TYPE_NORMAL
- en: validation_data = tf.data.Dataset.from_tensor_slices((X_test_pa dded, y_test))
  prefs: []
  type: TYPE_NORMAL
- en: training_data = training_data.batch(batch_size)
  prefs: []
  type: TYPE_NORMAL
- en: validation_data = validation_data.batch(batch_size)
  prefs: []
  type: TYPE_NORMAL
- en: 'def get_train_batches():'
  prefs: []
  type: TYPE_NORMAL
- en: ds = training_data.prefetch(1)
  prefs: []
  type: TYPE_NORMAL
- en: tfds.dataset_as_numpy converts the tf.data.Dataset into an
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: iterable of NumPy arraysreturn tfds.as_numpy(ds)The data is now in the right
    format and to be passed to a Flax network.
  prefs: []
  type: TYPE_NORMAL
- en: Let's quickly walk through the rest of the steps required to train neural networks
    in Flax using this data.
  prefs: []
  type: TYPE_NORMAL
- en: First, create a simple neural network in Flax.
  prefs: []
  type: TYPE_NORMAL
- en: pip install flax
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: import flax
  prefs: []
  type: TYPE_NORMAL
- en: from flax import linen as nn
  prefs: []
  type: TYPE_NORMAL
- en: 'class Model(nn.Module):'
  prefs: []
  type: TYPE_NORMAL
- en: '@nn.compact'
  prefs: []
  type: TYPE_NORMAL
- en: 'def __call__(self, x): x = nn.Dense(features=256)(x) x = nn.relu(x)'
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.Dense(features=2)(x) x = nn.log_softmax(x)
  prefs: []
  type: TYPE_NORMAL
- en: return x
  prefs: []
  type: TYPE_NORMAL
- en: Define a function to compute the loss.
  prefs: []
  type: TYPE_NORMAL
- en: import optax
  prefs: []
  type: TYPE_NORMAL
- en: import jax.numpy as jnp
  prefs: []
  type: TYPE_NORMAL
- en: 'def cross_entropy_loss(*, logits, labels):'
  prefs: []
  type: TYPE_NORMAL
- en: labels_onehot = jax.nn.one_hot(labels, num_classes=2) return optax.softmax_cross_entropy(logits=logits,
    labels=labe ls_onehot).mean()Next, define the function to compute the network
    metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'def compute_metrics(*, logits, labels):'
  prefs: []
  type: TYPE_NORMAL
- en: loss = cross_entropy_loss(logits=logits, labels=labels) accuracy = jnp.mean(jnp.argmax(logits,
    -1) == labels) metrics = {
  prefs: []
  type: TYPE_NORMAL
- en: '''loss'': loss,'
  prefs: []
  type: TYPE_NORMAL
- en: '''accuracy'': accuracy,'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: return metrics
  prefs: []
  type: TYPE_NORMAL
- en: The training state is used to track the network training. It tracks the optimizer
    and model parameters and can be modified to track other things such as dropout
    and batch normalization statistics.
  prefs: []
  type: TYPE_NORMAL
- en: 'from flax.training import train_state def create_train_state(rng, learning_rate,
    momentum): """Creates initial `TrainState`."""'
  prefs: []
  type: TYPE_NORMAL
- en: model = Model()
  prefs: []
  type: TYPE_NORMAL
- en: params = model.init(rng, X_train_padded[0])['params'] tx = optax.sgd(learning_rate,
    momentum)
  prefs: []
  type: TYPE_NORMAL
- en: return train_state.TrainState.create(
  prefs: []
  type: TYPE_NORMAL
- en: apply_fn=model.apply, params=params, tx=tx)In the training step, we [Apply] the
    model to obtain the loss. This is then used to compute the gradients that update
    the model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'def compute_loss(params,text,labels):'
  prefs: []
  type: TYPE_NORMAL
- en: 'logits = Model().apply({''params'': params}, text) loss = cross_entropy_loss(logits=logits,
    labels=labels) return loss, logits'
  prefs: []
  type: TYPE_NORMAL
- en: '@jax.jit'
  prefs: []
  type: TYPE_NORMAL
- en: 'def train_step(state,text, labels):'
  prefs: []
  type: TYPE_NORMAL
- en: '"""Train for a single step."""'
  prefs: []
  type: TYPE_NORMAL
- en: (_, logits), grads = jax.value_and_grad(compute_loss, has_aux =True)(state.params,text,labels)
  prefs: []
  type: TYPE_NORMAL
- en: state = state.apply_gradients(grads=grads)
  prefs: []
  type: TYPE_NORMAL
- en: metrics = compute_metrics(logits=logits, labels=labels)
  prefs: []
  type: TYPE_NORMAL
- en: return state, metrics
  prefs: []
  type: TYPE_NORMAL
- en: The evaluation step applies the model to the testing data to compute the test
    metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '@jax.jit'
  prefs: []
  type: TYPE_NORMAL
- en: 'def eval_step(state, text, labels):'
  prefs: []
  type: TYPE_NORMAL
- en: 'logits = Model().apply({''params'': state.params}, text)'
  prefs: []
  type: TYPE_NORMAL
- en: return compute_metrics(logits=logits, labels=labels)
  prefs: []
  type: TYPE_NORMAL
- en: The evaluation function runs the above evaluation step to obtain the evaluation
    metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'def evaluate_model(state, text, test_lbls):'
  prefs: []
  type: TYPE_NORMAL
- en: '"""Evaluate on the validation set."""'
  prefs: []
  type: TYPE_NORMAL
- en: metrics = eval_step(state, text, test_lbls) metrics = jax.device_get(metrics)
  prefs: []
  type: TYPE_NORMAL
- en: 'metrics = jax.tree_map(lambda x: x.item(), metrics) return metrics'
  prefs: []
  type: TYPE_NORMAL
- en: We use the get_train_batches function in thetrain_epoch method. We loop through
    the batches as we apply the [train_step] method. We obtain the train metrics and
    return them.
  prefs: []
  type: TYPE_NORMAL
- en: 'def train_one_epoch(state):'
  prefs: []
  type: TYPE_NORMAL
- en: '"""Train for 1 epoch on the training set.""" batch_metrics = []'
  prefs: []
  type: TYPE_NORMAL
- en: 'for text, labels in get_train_batches():'
  prefs: []
  type: TYPE_NORMAL
- en: 'state, metrics = train_step(state, text, labels) batch_metrics.append(metrics)batch_metrics_np
    = jax.device_get(batch_metrics) epoch_metrics_np = {k: np.mean([metrics[k] for
    metrics in batch_metrics_n p])for k in batch_metrics_np[0] }return state, epoch_metrics_np'
  prefs: []
  type: TYPE_NORMAL
- en: The final step is to train the network on the training set and evaluate it on
    the test set. A training state is required before training the model. This is
    because JAX expects pure functions.
  prefs: []
  type: TYPE_NORMAL
- en: rng = jax.random.PRNGKey(0)rng, init_rng = jax.random.split(rng)
  prefs: []
  type: TYPE_NORMAL
- en: learning_rate = 0.1 momentum = 0.9
  prefs: []
  type: TYPE_NORMAL
- en: seed = 0
  prefs: []
  type: TYPE_NORMAL
- en: 'state = create_train_state(init_rng, learning_rate, momentum) del init_rng
    # Must not be used anymore.'
  prefs: []
  type: TYPE_NORMAL
- en: num_epochs = 30
  prefs: []
  type: TYPE_NORMAL
- en: (text, test_labels) = next(iter(validation_data)) text = jnp.array(text)
  prefs: []
  type: TYPE_NORMAL
- en: test_labels = jnp.array(test_labels)
  prefs: []
  type: TYPE_NORMAL
- en: state = create_train_state(jax.random.PRNGKey(seed), learning_r ate, momentum)
  prefs: []
  type: TYPE_NORMAL
- en: training_loss = []
  prefs: []
  type: TYPE_NORMAL
- en: training_accuracy = []
  prefs: []
  type: TYPE_NORMAL
- en: testing_loss = []
  prefs: []
  type: TYPE_NORMAL
- en: testing_accuracy = []
  prefs: []
  type: TYPE_NORMAL
- en: 'for epoch in range(1, num_epochs + 1):'
  prefs: []
  type: TYPE_NORMAL
- en: train_state, train_metrics = train_one_epoch(state) training_loss.append(train_metrics['loss'])
  prefs: []
  type: TYPE_NORMAL
- en: 'training_accuracy.append(train_metrics[''accuracy'']) print(f"Train epoch:
    {epoch}, loss: {train_metrics[''los'
  prefs: []
  type: TYPE_NORMAL
- en: 's'']}, accuracy: {train_metrics[''accuracy''] * 100}")'
  prefs: []
  type: TYPE_NORMAL
- en: test_metrics = evaluate_model(train_state, text, test_label s)
  prefs: []
  type: TYPE_NORMAL
- en: testing_loss.append(test_metrics['loss'])
  prefs: []
  type: TYPE_NORMAL
- en: testing_accuracy.append(test_metrics['accuracy'])
  prefs: []
  type: TYPE_NORMAL
- en: 'print(f"Test epoch: {epoch}, loss: {test_metrics[''loss'']}, accuracy: {test_metrics[''accuracy'']
    * 100}")'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00030.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '**How to load image data in JAX**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's now see how we can load image data with TensorFlow. We'll use the popular cats
    and dogs images from Kaggle. We start by downloading the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'import wget # pip install wgetimport zipfile'
  prefs: []
  type: TYPE_NORMAL
- en: wget.download("https://ml.machinelearningnuggets.com/train.zi p")
  prefs: []
  type: TYPE_NORMAL
- en: 'with zipfile.ZipFile(''train.zip'', ''r'') as zip_ref:'
  prefs: []
  type: TYPE_NORMAL
- en: zip_ref.extractall('.')Next, create a Pandas DataFrame containing the labels
    and paths to the images.
  prefs: []
  type: TYPE_NORMAL
- en: import pandas as pd
  prefs: []
  type: TYPE_NORMAL
- en: base_dir = 'train'
  prefs: []
  type: TYPE_NORMAL
- en: filenames = os.listdir(base_dir) categories = []
  prefs: []
  type: TYPE_NORMAL
- en: 'for filename in filenames:'
  prefs: []
  type: TYPE_NORMAL
- en: category = filename.split('.')[0] if category == 'dog':categories.append("dog")
  prefs: []
  type: TYPE_NORMAL
- en: 'else:'
  prefs: []
  type: TYPE_NORMAL
- en: 'categories.append("cat") df = pd.DataFrame({''filename'': filenames,''category'':
    categorie s})'
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to define an ImageDataGenerator for scaling the images and
    performing simple augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: from tensorflow.keras.preprocessing.image import ImageDataGener ator
  prefs: []
  type: TYPE_NORMAL
- en: train_datagen = ImageDataGenerator(rescale=1./255,
  prefs: []
  type: TYPE_NORMAL
- en: shear_range=0.2, zoom_range=0.2,
  prefs: []
  type: TYPE_NORMAL
- en: horizontal_flip=True, width_shift_range=0.1, height_shift_range=0.1, validation_split=0.2
    )
  prefs: []
  type: TYPE_NORMAL
- en: validation_gen = ImageDataGenerator(rescale=1./255,validation_s plit=0.2)
  prefs: []
  type: TYPE_NORMAL
- en: Load the images using the [flow_from_dataframe]of these generators. This will
    match the image paths in the DataFrame to the images we downloaded.
  prefs: []
  type: TYPE_NORMAL
- en: image_size = (128, 128)
  prefs: []
  type: TYPE_NORMAL
- en: batch_size = 128
  prefs: []
  type: TYPE_NORMAL
- en: training_set = train_datagen.flow_from_dataframe(df,base_dir,
  prefs: []
  type: TYPE_NORMAL
- en: seed=101, target_size=ima ge_size,
  prefs: []
  type: TYPE_NORMAL
- en: batch_size=batc h_size,
  prefs: []
  type: TYPE_NORMAL
- en: x_col='filenam e',
  prefs: []
  type: TYPE_NORMAL
- en: y_col='categor y',
  prefs: []
  type: TYPE_NORMAL
- en: subset = "train ing",
  prefs: []
  type: TYPE_NORMAL
- en: class_mode='bin ary')
  prefs: []
  type: TYPE_NORMAL
- en: validation_set = validation_gen.flow_from_dataframe(df,base_di r,
  prefs: []
  type: TYPE_NORMAL
- en: target_size=image _size,
  prefs: []
  type: TYPE_NORMAL
- en: batch_size=batch_ size,
  prefs: []
  type: TYPE_NORMAL
- en: x_col='filename',
  prefs: []
  type: TYPE_NORMAL
- en: y_col='categor y',
  prefs: []
  type: TYPE_NORMAL
- en: subset = "validat ion",
  prefs: []
  type: TYPE_NORMAL
- en: class_mode='binar y')
  prefs: []
  type: TYPE_NORMAL
- en: Loop through the training set to confirm that a batch of images are being generated.
  prefs: []
  type: TYPE_NORMAL
- en: 'for train_images, train_labels in training_set: print(''Train:'', train_images.shape,
    train_labels.shape) break'
  prefs: []
  type: TYPE_NORMAL
- en: 'Train: (128, 128, 128, 3) (128,)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The next step is to define a network and pass the data. The steps are similar
    to what we did for the text data above
  prefs: []
  type: TYPE_NORMAL
- en: '**How to load CSV data in JAX**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can use Pandas to load CSV data as we did for the text data at the beginning
    of the article. Convert the data to NumPy or JAX arrays once preprocessing is
    done. Passing Torch tensors or TensorFlow tensors to JAX neural networks will
    result in an error.
  prefs: []
  type: TYPE_NORMAL
- en: '**Final thoughts**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article shows how you can use TensorFlow to load datasets in JAX and Flax
    applications. We have walked through an example of loading text data with TensorFlow.
    After that, we discussed loading image and CSV data in JAX.
  prefs: []
  type: TYPE_NORMAL
- en: '**Image classification with JAX & Flax**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Flax is a neural network library for JAX. JAX is a Python library that provides
    high-performance computing in machine learning research. JAX provides an API similar
    to NumPy making it easy to adopt. JAX also includes other functionalities for
    improving machine learning research. They include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Automatic differentiation** . JAX supports forward and reverse automatic
    differential of numerical functions with functions such as jacrev, grad, hessian and jacfwd.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Vectorization** . JAX supports automatic vectorization via the [vmap] function.
    It also makes it easy to parallelize large-scale data processing via the [pmap] function.'
  prefs: []
  type: TYPE_NORMAL
- en: '**JIT compilation** . JAX uses XLA for Just In Time (JIT) compilation and execution
    of code on GPUs and TPUs. In this article, let''s look at how you can use JAX and
    Flax to build a simple convolutional neural network.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Loading the dataset**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ll use the cats and dogs dataset from Kaggle. Let''s start by downloading
    and extracting it.import wget # pip install wget import zipfile'
  prefs: []
  type: TYPE_NORMAL
- en: wget.download("https://ml.machinelearningnuggets.com/train.zi p")
  prefs: []
  type: TYPE_NORMAL
- en: 'with zipfile.ZipFile(''train.zip'', ''r'') as zip_ref:'
  prefs: []
  type: TYPE_NORMAL
- en: zip_ref.extractall('.')
  prefs: []
  type: TYPE_NORMAL
- en: Flax doesn't ship with any data loading tools. You can use the data loaders
    from PyTorch
  prefs: []
  type: TYPE_NORMAL
- en: or TensorFlow. In this case, let's load the data using PyTorch. The first step
    is to define the dataset class.
  prefs: []
  type: TYPE_NORMAL
- en: from PIL import Image
  prefs: []
  type: TYPE_NORMAL
- en: import pandas as pd
  prefs: []
  type: TYPE_NORMAL
- en: from torch.utils.data import Dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'class CatsDogsDataset(Dataset):'
  prefs: []
  type: TYPE_NORMAL
- en: def __init__(self, root_dir, annotation_file, transform=Non
  prefs: []
  type: TYPE_NORMAL
- en: 'e):'
  prefs: []
  type: TYPE_NORMAL
- en: self.root_dir = root_dir
  prefs: []
  type: TYPE_NORMAL
- en: self.annotations = pd.read_csv(annotation_file) self.transform = transform
  prefs: []
  type: TYPE_NORMAL
- en: 'def __len__(self):return len(self.annotations) def __getitem__(self, index):img_id
    = self.annotations.iloc[index, 0]img = Image.open(os.path.join(self.root_dir,
    img_id)).c onvert("RGB")y_label = torch.tensor(float(self.annotations.iloc[inde
    x, 1]))if self.transform is not None: img = self.transform(img)return (img, y_label)Next,
    we create a Pandas DataFrame that will contain the categories.import osimport
    pandas as pd'
  prefs: []
  type: TYPE_NORMAL
- en: 'train_df = pd.DataFrame(columns=["img_path","label"]) train_df["img_path"]
    = os.listdir("train/") for idx, i in enumerate(os.listdir("train/")):'
  prefs: []
  type: TYPE_NORMAL
- en: 'if "cat" in i:'
  prefs: []
  type: TYPE_NORMAL
- en: train_df["label"][idx] = 0
  prefs: []
  type: TYPE_NORMAL
- en: 'if "dog" in i:'
  prefs: []
  type: TYPE_NORMAL
- en: train_df["label"][idx] = 1
  prefs: []
  type: TYPE_NORMAL
- en: train_df.to_csv (r'train_csv.csv', index = False, header=True)Define a function
    that will stack the data and return it as NumPy arrays.
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  prefs: []
  type: TYPE_NORMAL
- en: 'def custom_collate_fn(batch):'
  prefs: []
  type: TYPE_NORMAL
- en: transposed_data = list(zip(*batch))
  prefs: []
  type: TYPE_NORMAL
- en: labels = np.array(transposed_data[1])
  prefs: []
  type: TYPE_NORMAL
- en: imgs = np.stack(transposed_data[0])
  prefs: []
  type: TYPE_NORMAL
- en: return imgs, labels
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready to define the training and test data and use that with the
    PyTorch DataLoader. We also define a PyTorch transformation for resizing the images.
  prefs: []
  type: TYPE_NORMAL
- en: import torch
  prefs: []
  type: TYPE_NORMAL
- en: from torch.utils.data import DataLoader from torchvision import transforms import
    numpy as np
  prefs: []
  type: TYPE_NORMAL
- en: size_image = 64 batch_size = 32
  prefs: []
  type: TYPE_NORMAL
- en: transform = transforms.Compose([
  prefs: []
  type: TYPE_NORMAL
- en: transforms.Resize((size_image,size_image)),
  prefs: []
  type: TYPE_NORMAL
- en: np.array])
  prefs: []
  type: TYPE_NORMAL
- en: dataset = CatsDogsDataset("train","train_csv.csv",transform=tra nsform)
  prefs: []
  type: TYPE_NORMAL
- en: train_set, validation_set = torch.utils.data.random_split(datas et,[20000,5000])
  prefs: []
  type: TYPE_NORMAL
- en: train_loader = DataLoader(dataset=train_set, collate_fn=custom_ collate_fn,shuffle=True,
    batch_size=batch_size)
  prefs: []
  type: TYPE_NORMAL
- en: validation_loader = DataLoader(dataset=validation_set,collate_f n=custom_collate_fn,
    shuffle=False, batch_size=batch_size)
  prefs: []
  type: TYPE_NORMAL
- en: '**Define Convolution Neural Network with Flax**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Install Flax to create a simple neural network.pip install flax
  prefs: []
  type: TYPE_NORMAL
- en: Networks are created in Flax using the Linen API by
  prefs: []
  type: TYPE_NORMAL
- en: subclassing Module.  All Flax modules are Python dataclasses. This means that
    they have  [__.init__] by default. You should, therefore, override [setup()] instead
    to initialize the network. However, you can use the compact wrapper
  prefs: []
  type: TYPE_NORMAL
- en: to make the model definition more concise.
  prefs: []
  type: TYPE_NORMAL
- en: 'import flaxfrom flax import linen as nnclass CNN(nn.Module):'
  prefs: []
  type: TYPE_NORMAL
- en: '@nn.compact'
  prefs: []
  type: TYPE_NORMAL
- en: 'def __call__(self, x):'
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.Conv(features=32, kernel_size=(3, 3))(x)
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.relu(x)
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.Conv(features=64, kernel_size=(3, 3))(x)
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.relu(x)
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))
  prefs: []
  type: TYPE_NORMAL
- en: x = x.reshape((x.shape[0], -1))
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.Dense(features=256)(x)
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.relu(x)
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.Dense(features=2)(x)
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.log_softmax(x)
  prefs: []
  type: TYPE_NORMAL
- en: return x
  prefs: []
  type: TYPE_NORMAL
- en: '**Define loss**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The loss can be computed using the Optax package. We one-hot encode the integer
    labels before passing them to the softmax crossentropy function. num_classes is
    2 because we are dealing with two classes.
  prefs: []
  type: TYPE_NORMAL
- en: import optax
  prefs: []
  type: TYPE_NORMAL
- en: 'def cross_entropy_loss(*, logits, labels):'
  prefs: []
  type: TYPE_NORMAL
- en: labels_onehot = jax.nn.one_hot(labels, num_classes=2)
  prefs: []
  type: TYPE_NORMAL
- en: return optax.softmax_cross_entropy(logits=logits, labels=labe ls_onehot).mean()
  prefs: []
  type: TYPE_NORMAL
- en: '**Compute metrics**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, we define a function that will use the above loss function to compute
    and return the loss. We also compute the accuracy in the same function.
  prefs: []
  type: TYPE_NORMAL
- en: 'def compute_metrics(*, logits, labels):'
  prefs: []
  type: TYPE_NORMAL
- en: loss = cross_entropy_loss(logits=logits, labels=labels) accuracy = jnp.mean(jnp.argmax(logits,
    -1) == labels) metrics = {
  prefs: []
  type: TYPE_NORMAL
- en: '''loss'': loss,'
  prefs: []
  type: TYPE_NORMAL
- en: '''accuracy'': accuracy,'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: return metrics
  prefs: []
  type: TYPE_NORMAL
- en: '**Create training state**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A training state holds the model variables such as parameters and optimizer
    state. These variables are modified at each iteration using the optimizer. You
    can subclass [flax.training.train_state] to track more data. You might want to
    do that for tracking the state of dropout and batch statistics if you include
    those layers in your model. For this simple model, the default class will suffice.
  prefs: []
  type: TYPE_NORMAL
- en: from flax.training import train_state
  prefs: []
  type: TYPE_NORMAL
- en: 'def create_train_state(rng, learning_rate, momentum): """Creates initial `TrainState`."""'
  prefs: []
  type: TYPE_NORMAL
- en: cnn = CNN()
  prefs: []
  type: TYPE_NORMAL
- en: params = cnn.init(rng, jnp.ones([1, size_image, size_image,
  prefs: []
  type: TYPE_NORMAL
- en: 3]))['params']
  prefs: []
  type: TYPE_NORMAL
- en: tx = optax.sgd(learning_rate, momentum)
  prefs: []
  type: TYPE_NORMAL
- en: return train_state.TrainState.create(
  prefs: []
  type: TYPE_NORMAL
- en: apply_fn=cnn.apply, params=params, tx=tx)
  prefs: []
  type: TYPE_NORMAL
- en: '**Define training step**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this function, we evaluate the model with a set of input images using the Apply method.
    We use the obtained logits to compute the loss. We then use  value_and_grad  to
    evaluate the loss function and its gradient. The gradients are then used to update
    the model parameters. Finally, it uses the [compute_metrics] function defined
    above to calculate the loss and accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'def compute_loss(params,images,labels):'
  prefs: []
  type: TYPE_NORMAL
- en: 'logits = CNN().apply({''params'': params}, images) loss = cross_entropy_loss(logits=logits,
    labels=labels) return loss, logits'
  prefs: []
  type: TYPE_NORMAL
- en: '@jax.jit'
  prefs: []
  type: TYPE_NORMAL
- en: 'def train_step(state,images, labels):'
  prefs: []
  type: TYPE_NORMAL
- en: '"""Train for a single step."""'
  prefs: []
  type: TYPE_NORMAL
- en: (_, logits), grads = jax.value_and_grad(compute_loss, has_aux =True)(state.params,images,labels)
  prefs: []
  type: TYPE_NORMAL
- en: state = state.apply_gradients(grads=grads)
  prefs: []
  type: TYPE_NORMAL
- en: metrics = compute_metrics(logits=logits, labels=labels)
  prefs: []
  type: TYPE_NORMAL
- en: return state, metrics
  prefs: []
  type: TYPE_NORMAL
- en: The function is decorated with the @Jit decorator to trace the function and
    compile just-in-time for faster computation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Define evaluation step**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The evaluation function will use [Apply]  to evaluate the model on the test
    data.
  prefs: []
  type: TYPE_NORMAL
- en: '@jax.jit'
  prefs: []
  type: TYPE_NORMAL
- en: 'def eval_step(state, images, labels):'
  prefs: []
  type: TYPE_NORMAL
- en: 'logits = CNN().apply({''params'': state.params}, images)'
  prefs: []
  type: TYPE_NORMAL
- en: return compute_metrics(logits=logits, labels=labels)
  prefs: []
  type: TYPE_NORMAL
- en: '**Training function**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this function, we apply the training step we defined above.  We loop through
    each batch in the data loader and perform optimization for each batch. We use
    the [jax.device_get] to get the metrics and compute the mean.
  prefs: []
  type: TYPE_NORMAL
- en: 'def train_one_epoch(state, dataloader):'
  prefs: []
  type: TYPE_NORMAL
- en: '"""Train for 1 epoch on the training set.""" batch_metrics = []'
  prefs: []
  type: TYPE_NORMAL
- en: 'for cnt, (images, labels) in enumerate(dataloader):'
  prefs: []
  type: TYPE_NORMAL
- en: images = images / 255.0
  prefs: []
  type: TYPE_NORMAL
- en: state, metrics = train_step(state, images, labels) batch_metrics.append(metrics)
  prefs: []
  type: TYPE_NORMAL
- en: 'batch_metrics_np = jax.device_get(batch_metrics) epoch_metrics_np = {k: np.mean([metrics[k]
    for metrics in batch_metrics_n p])for k in batch_metrics_np[0] }return state,
    epoch_metrics_np'
  prefs: []
  type: TYPE_NORMAL
- en: '**Evaluate the model**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The evaluation function runs the evaluation step and returns the test metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'def evaluate_model(state, test_imgs, test_lbls): """Evaluate on the validation
    set."""'
  prefs: []
  type: TYPE_NORMAL
- en: metrics = eval_step(state, test_imgs, test_lbls) metrics = jax.device_get(metrics)
  prefs: []
  type: TYPE_NORMAL
- en: 'metrics = jax.tree_map(lambda x: x.item(), metrics) return metrics'
  prefs: []
  type: TYPE_NORMAL
- en: '**Train and evaluate the model**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We need to initialize the train state before training the model. The function
    to initialize the state requires a pseudo-random number (PRNG) key. Use the [PRNGKey] function
    to obtain a key and split it to get another key that you'll use for parameter
    initialization. Follow this link to learn more about JAX PRNG Design.
  prefs: []
  type: TYPE_NORMAL
- en: Pass this key to the [create_train_state] function together with the learning
    rate and momentum. You can now use the [train_one_epoch] function to train the
    model and the eval_modelfunction to evaluate the model.
  prefs: []
  type: TYPE_NORMAL
- en: import jaxrng = jax.random.PRNGKey(0)rng, init_rng = jax.random.split(rng)
  prefs: []
  type: TYPE_NORMAL
- en: learning_rate = 0.1 momentum = 0.9
  prefs: []
  type: TYPE_NORMAL
- en: seed = 0
  prefs: []
  type: TYPE_NORMAL
- en: 'state = create_train_state(init_rng, learning_rate, momentum) del init_rng
    # Must not be used anymore.'
  prefs: []
  type: TYPE_NORMAL
- en: num_epochs = 30
  prefs: []
  type: TYPE_NORMAL
- en: (test_images, test_labels) = next(iter(validation_loader)) test_images = test_images
    / 255.0
  prefs: []
  type: TYPE_NORMAL
- en: state = create_train_state(jax.random.PRNGKey(seed), learning_r ate, momentum)
  prefs: []
  type: TYPE_NORMAL
- en: training_loss = []
  prefs: []
  type: TYPE_NORMAL
- en: training_accuracy = []
  prefs: []
  type: TYPE_NORMAL
- en: testing_loss = []
  prefs: []
  type: TYPE_NORMAL
- en: testing_accuracy = []
  prefs: []
  type: TYPE_NORMAL
- en: 'for epoch in range(1, num_epochs + 1):'
  prefs: []
  type: TYPE_NORMAL
- en: train_state, train_metrics = train_one_epoch(state, train_l oader)
  prefs: []
  type: TYPE_NORMAL
- en: training_loss.append(train_metrics['loss'])
  prefs: []
  type: TYPE_NORMAL
- en: training_accuracy.append(train_metrics['accuracy'])
  prefs: []
  type: TYPE_NORMAL
- en: 'print(f"Train epoch: {epoch}, loss: {train_metrics[''los s'']}, accuracy: {train_metrics[''accuracy'']
    * 100}")'
  prefs: []
  type: TYPE_NORMAL
- en: test_metrics = evaluate_model(train_state, test_images, tes t_labels)
  prefs: []
  type: TYPE_NORMAL
- en: testing_loss.append(test_metrics['loss'])
  prefs: []
  type: TYPE_NORMAL
- en: testing_accuracy.append(test_metrics['accuracy'])
  prefs: []
  type: TYPE_NORMAL
- en: 'print(f"Test epoch: {epoch}, loss: {test_metrics[''loss'']}, accuracy: {test_metrics[''accuracy'']
    * 100}")'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model performance**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the training is happening, we print the training and validation metrics.
    You can also use the resulting metrics to plot training and validation charts.
  prefs: []
  type: TYPE_NORMAL
- en: import matplotlib.pyplot as plt
  prefs: []
  type: TYPE_NORMAL
- en: plt.plot(training_accuracy, label="Training") plt.plot(testing_accuracy, label="Test")
    plt.xlabel("Epoch")
  prefs: []
  type: TYPE_NORMAL
- en: plt.ylabel("Accuracy")
  prefs: []
  type: TYPE_NORMAL
- en: plt.legend()
  prefs: []
  type: TYPE_NORMAL
- en: plt.show()
  prefs: []
  type: TYPE_NORMAL
- en: plt.plot(training_loss, label="Training") plt.plot(testing_loss, label="Test")
    plt.xlabel("Epoch")
  prefs: []
  type: TYPE_NORMAL
- en: plt.ylabel("Accuracy")
  prefs: []
  type: TYPE_NORMAL
- en: plt.legend()
  prefs: []
  type: TYPE_NORMAL
- en: plt.show()
  prefs: []
  type: TYPE_NORMAL
- en: '**Final thoughts**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this article, we have seen how to set up a simple neural network with Flax
    and train it on the CPU.
  prefs: []
  type: TYPE_NORMAL
- en: '**Distributed training with JAX & Flax**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Training models on accelerators with JAX and Flax differs slightly from training
    with CPU. For instance, the data needs to be replicated in the different devices
    when using multiple accelerators. After that, we need to execute the training
    on multiple devices and aggregate the results. Flax supports TPU and GPU accelerators.
  prefs: []
  type: TYPE_NORMAL
- en: In the last article, we saw how to train models with the CPU. This article will
    focus on training models with Flax and JAX using GPUs and TPU.
  prefs: []
  type: TYPE_NORMAL
- en: '**Perform standard imports**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You'll need to install Flax for this illustration.pip install flaxLet's import
    all the packages we'll use in this project.
  prefs: []
  type: TYPE_NORMAL
- en: import wget
  prefs: []
  type: TYPE_NORMAL
- en: import zipfile
  prefs: []
  type: TYPE_NORMAL
- en: import torch
  prefs: []
  type: TYPE_NORMAL
- en: from torch.utils.data import DataLoader import os
  prefs: []
  type: TYPE_NORMAL
- en: from PIL import Image
  prefs: []
  type: TYPE_NORMAL
- en: from torchvision import transforms from torch.utils.data import Dataset import
    numpy as np
  prefs: []
  type: TYPE_NORMAL
- en: import pandas as pd
  prefs: []
  type: TYPE_NORMAL
- en: import matplotlib.pyplot as plt
  prefs: []
  type: TYPE_NORMAL
- en: import functools
  prefs: []
  type: TYPE_NORMAL
- en: import time
  prefs: []
  type: TYPE_NORMAL
- en: from tqdm.notebook import tqdm
  prefs: []
  type: TYPE_NORMAL
- en: ignore harmless warnings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: import warnings
  prefs: []
  type: TYPE_NORMAL
- en: warnings.filterwarnings("ignore") warnings.simplefilter('ignore') import jax
  prefs: []
  type: TYPE_NORMAL
- en: from jax import numpy as jnp
  prefs: []
  type: TYPE_NORMAL
- en: from flax import linen as nn
  prefs: []
  type: TYPE_NORMAL
- en: from flax.training import train_state import optax
  prefs: []
  type: TYPE_NORMAL
- en: import math
  prefs: []
  type: TYPE_NORMAL
- en: from flax import jax_utils
  prefs: []
  type: TYPE_NORMAL
- en: import jax.tools.colab_tpu
  prefs: []
  type: TYPE_NORMAL
- en: '**Setup TPUs on Colab**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Change the runtime on Colab to TPUs. Next, run the code below to set up JAX to
    use TPUs.jax.tools.colab_tpu.setup_tpu() jax.devices()![](../images/00031.gif)
  prefs: []
  type: TYPE_NORMAL
- en: '**Download the dataset**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ll use the cats and dogs dataset from Kaggle. Let''s start by downloading
    and extracting it.import wget # pip install wget import zipfile'
  prefs: []
  type: TYPE_NORMAL
- en: wget.download("https://ml.machinelearningnuggets.com/train.zi p")
  prefs: []
  type: TYPE_NORMAL
- en: 'with zipfile.ZipFile(''train.zip'', ''r'') as zip_ref:'
  prefs: []
  type: TYPE_NORMAL
- en: zip_ref.extractall('.')
  prefs: []
  type: TYPE_NORMAL
- en: '**Load the dataset**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We'll use existing data loaders to load the data since JAX and Flax don't ship
    with any data loaders. In this case, let's use PyTorch to load the dataset. The
    first step is to set up a dataset class.
  prefs: []
  type: TYPE_NORMAL
- en: 'class CatsDogsDataset(Dataset):'
  prefs: []
  type: TYPE_NORMAL
- en: def __init__(self, root_dir, annotation_file, transform=Non
  prefs: []
  type: TYPE_NORMAL
- en: 'e):'
  prefs: []
  type: TYPE_NORMAL
- en: self.root_dir = root_dir
  prefs: []
  type: TYPE_NORMAL
- en: self.annotations = pd.read_csv(annotation_file) self.transform = transform
  prefs: []
  type: TYPE_NORMAL
- en: def __len__(self):return len(self.annotations)
  prefs: []
  type: TYPE_NORMAL
- en: 'def __getitem__(self, index):'
  prefs: []
  type: TYPE_NORMAL
- en: img_id = self.annotations.iloc[index, 0]
  prefs: []
  type: TYPE_NORMAL
- en: img = Image.open(os.path.join(self.root_dir, img_id)).c
  prefs: []
  type: TYPE_NORMAL
- en: onvert("RGB")
  prefs: []
  type: TYPE_NORMAL
- en: y_label = torch.tensor(float(self.annotations.iloc[inde
  prefs: []
  type: TYPE_NORMAL
- en: x, 1]))
  prefs: []
  type: TYPE_NORMAL
- en: 'if self.transform is not None: img = self.transform(img)return (img, y_label)Next,
    we create a DataFrame containing the categories.'
  prefs: []
  type: TYPE_NORMAL
- en: 'train_df = pd.DataFrame(columns=["img_path","label"]) train_df["img_path"]
    = os.listdir("train/") for idx, i in enumerate(os.listdir("train/")):'
  prefs: []
  type: TYPE_NORMAL
- en: 'if "cat" in i:'
  prefs: []
  type: TYPE_NORMAL
- en: train_df["label"][idx] = 0
  prefs: []
  type: TYPE_NORMAL
- en: 'if "dog" in i:'
  prefs: []
  type: TYPE_NORMAL
- en: train_df["label"][idx] = 1
  prefs: []
  type: TYPE_NORMAL
- en: train_df.to_csv (r'train_csv.csv', index = False, header=True)
  prefs: []
  type: TYPE_NORMAL
- en: We then use the dataset class to create training and testing data. We also apply
    a custom function to return the data as NumPy arrays. Later, we'll use this train_loader
  prefs: []
  type: TYPE_NORMAL
- en: when training the model. We'll then evaluate it on a batch of the test data.
  prefs: []
  type: TYPE_NORMAL
- en: 'def custom_collate_fn(batch):'
  prefs: []
  type: TYPE_NORMAL
- en: transposed_data = list(zip(*batch)) labels = np.array(transposed_data[1]) imgs
    = np.stack(transposed_data[0]) return imgs, labels
  prefs: []
  type: TYPE_NORMAL
- en: size_image = 224 batch_size = 64
  prefs: []
  type: TYPE_NORMAL
- en: transform = transforms.Compose([
  prefs: []
  type: TYPE_NORMAL
- en: transforms.Resize((size_image,size_image)),
  prefs: []
  type: TYPE_NORMAL
- en: np.array])
  prefs: []
  type: TYPE_NORMAL
- en: dataset = CatsDogsDataset("train","train_csv.csv",transform=tra nsform)
  prefs: []
  type: TYPE_NORMAL
- en: train_set, validation_set = torch.utils.data.random_split(datas et,[20000,5000])
  prefs: []
  type: TYPE_NORMAL
- en: train_loader = DataLoader(dataset=train_set, collate_fn=custom_ collate_fn,shuffle=True,
    batch_size=batch_size)
  prefs: []
  type: TYPE_NORMAL
- en: validation_loader = DataLoader(dataset=validation_set,collate_f n=custom_collate_fn,
    shuffle=False, batch_size=batch_size)
  prefs: []
  type: TYPE_NORMAL
- en: '**Define the model with Flax**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Flax, models are defined using the Linen API. It provides the building blocks
    for defining convolution layers, dropout, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Networks are created by subclassing [Module].   Flax allows you to define your
    networks using [setup] or [nn.compact]. Both approaches behave the same way but [nn.compact]
  prefs: []
  type: TYPE_NORMAL
- en: is more concise.
  prefs: []
  type: TYPE_NORMAL
- en: '**Create training state**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We now need to create parallel versions of our functions. Parallelization in JAX is
    done using
  prefs: []
  type: TYPE_NORMAL
- en: the [pmap] function. [pmap] compiles a function with XLA and executes it on
    multiple devices.
  prefs: []
  type: TYPE_NORMAL
- en: '@functools.partial(jax.pmap, static_broadcasted_argnums=(1, 2)) def create_train_state(rng,
    learning_rate, momentum): """Creates initial `TrainState`."""'
  prefs: []
  type: TYPE_NORMAL
- en: cnn = CNN()
  prefs: []
  type: TYPE_NORMAL
- en: params = cnn.init(rng, jnp.ones([1, size_image, size_image, 3]))['params']
  prefs: []
  type: TYPE_NORMAL
- en: tx = optax.sgd(learning_rate, momentum)
  prefs: []
  type: TYPE_NORMAL
- en: return train_state.TrainState.create(
  prefs: []
  type: TYPE_NORMAL
- en: apply_fn=cnn.apply, params=params, tx=tx)
  prefs: []
  type: TYPE_NORMAL
- en: '**Apply the model**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next step is to define
  prefs: []
  type: TYPE_NORMAL
- en: parallel apply_model and update_modelfunctions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The [apply_model] function:'
  prefs: []
  type: TYPE_NORMAL
- en: Computes the loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'Computes predictions from all devices by calculating the average of the probabilities
    using [jax.lax.pmean()] .@functools.partial(jax.pmap, axis_name=''ensemble'')def
    apply_model(state, images, labels):def loss_fn(params):'
  prefs: []
  type: TYPE_NORMAL
- en: 'logits = CNN().apply({''params'': params}, images) one_hot = jax.nn.one_hot(labels,
    2)'
  prefs: []
  type: TYPE_NORMAL
- en: loss = optax.softmax_cross_entropy(logits=logits, labels=on
  prefs: []
  type: TYPE_NORMAL
- en: e_hot).mean()return loss, logits
  prefs: []
  type: TYPE_NORMAL
- en: grad_fn = jax.value_and_grad(loss_fn, has_aux=True) (loss, logits), grads =
    grad_fn(state.params)
  prefs: []
  type: TYPE_NORMAL
- en: probs = jax.lax.pmean(jax.nn.softmax(logits), axis_name='ense
  prefs: []
  type: TYPE_NORMAL
- en: mble')accuracy = jnp.mean(jnp.argmax(probs, -1) == labels) return grads,loss,
    accuracy@jax.pmapdef update_model(state, grads):return state.apply_gradients(grads=grads)
  prefs: []
  type: TYPE_NORMAL
- en: Notice the use of the [axis_name]. You can give this any name. You'll need to
    specify that when computing the mean of the probabilities and accuracies.
  prefs: []
  type: TYPE_NORMAL
- en: The [update_model] function updates the model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '**Training function**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The next step is to define the model training function. In the function, we:'
  prefs: []
  type: TYPE_NORMAL
- en: Replicate the training data at batch level
  prefs: []
  type: TYPE_NORMAL
- en: using jax_utils.replicat e.
  prefs: []
  type: TYPE_NORMAL
- en: '[apply_model] to the replicated data.'
  prefs: []
  type: TYPE_NORMAL
- en: Obtain the epoch loss and accuracy and unreplicate them
  prefs: []
  type: TYPE_NORMAL
- en: using jax_utils.unreplicate.
  prefs: []
  type: TYPE_NORMAL
- en: Compute the mean of the loss and accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '[apply_model] to the test data and obtain test metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: Print the training and evaluation metrics per epoch. Append the training and
    test metrics to lists for visualization later.
  prefs: []
  type: TYPE_NORMAL
- en: 'def train_one_epoch(state, dataloader,num_epochs): """Train for 1 epoch on
    the training set."""'
  prefs: []
  type: TYPE_NORMAL
- en: epoch_loss = []
  prefs: []
  type: TYPE_NORMAL
- en: epoch_accuracy = []
  prefs: []
  type: TYPE_NORMAL
- en: testing_accuracy = []
  prefs: []
  type: TYPE_NORMAL
- en: testing_loss = []
  prefs: []
  type: TYPE_NORMAL
- en: 'for epoch in range(num_epochs):'
  prefs: []
  type: TYPE_NORMAL
- en: 'for cnt, (images, labels) in tqdm(enumerate(dataloade r), total=(math.ceil(len(train_set)/batch_size))):
    images = images / 255.0'
  prefs: []
  type: TYPE_NORMAL
- en: images = jax_utils.replicate(images)
  prefs: []
  type: TYPE_NORMAL
- en: labels = jax_utils.replicate(labels)
  prefs: []
  type: TYPE_NORMAL
- en: grads, loss, accuracy = apply_model(state, images,
  prefs: []
  type: TYPE_NORMAL
- en: labels)
  prefs: []
  type: TYPE_NORMAL
- en: state = update_model(state, grads)
  prefs: []
  type: TYPE_NORMAL
- en: epoch_loss.append(jax_utils.unreplicate(loss)) epoch_accuracy.append(jax_utils.unreplicate(accuracy))
    train_loss = np.mean(epoch_loss)
  prefs: []
  type: TYPE_NORMAL
- en: train_accuracy = np.mean(epoch_accuracy)
  prefs: []
  type: TYPE_NORMAL
- en: _, test_loss, test_accuracy = jax_utils.unreplicate(app ly_model(state, test_images,
    test_labels))
  prefs: []
  type: TYPE_NORMAL
- en: testing_accuracy.append(test_accuracy)
  prefs: []
  type: TYPE_NORMAL
- en: testing_loss.append(test_loss)
  prefs: []
  type: TYPE_NORMAL
- en: 'print(f"Epoch: {epoch + 1}, train loss: {train_loss:.4 f}, train accuracy:
    {train_accuracy * 100:.4f}, test loss: {tes t_loss:.4f}, test accuracy: {test_accuracy*
    100:.4f}", flush=Tr ue)'
  prefs: []
  type: TYPE_NORMAL
- en: return state, epoch_loss, epoch_accuracy, testing_accuracy, testing_loss
  prefs: []
  type: TYPE_NORMAL
- en: '**Train the model**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When creating the training state, we generate pseudo-random numbers equivalent
    to the number of devices. We also replicate a small batch of the test data for
    testing. The next step is to run the training function and unpack the training
    and test metrics.
  prefs: []
  type: TYPE_NORMAL
- en: learning_rate = 0.1 momentum = 0.9
  prefs: []
  type: TYPE_NORMAL
- en: seed = 0
  prefs: []
  type: TYPE_NORMAL
- en: num_epochs = 30
  prefs: []
  type: TYPE_NORMAL
- en: rng = jax.random.PRNGKey(0)
  prefs: []
  type: TYPE_NORMAL
- en: rng, init_rng = jax.random.split(rng)
  prefs: []
  type: TYPE_NORMAL
- en: 'state = create_train_state(jax.random.split(init_rng, jax.device_count()),learning_rate,
    momentum) del init_rng # Must not be used anymore.'
  prefs: []
  type: TYPE_NORMAL
- en: (test_images, test_labels) = next(iter(validation_loader)) test_images = test_images
    / 255.0
  prefs: []
  type: TYPE_NORMAL
- en: test_images = jax_utils.replicate(test_images)
  prefs: []
  type: TYPE_NORMAL
- en: test_labels = jax_utils.replicate(test_labels)
  prefs: []
  type: TYPE_NORMAL
- en: start = time.time()
  prefs: []
  type: TYPE_NORMAL
- en: 'state, epoch_loss, epoch_accuracy, testing_accuracy, testing_loss = train_one_epoch(state,
    train_loader,num_epochs) print("Total time: ", time.time() - start, "seconds")'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00032.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '**Model evaluation**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The metrics obtained above can be used to plot the metrics.
  prefs: []
  type: TYPE_NORMAL
- en: plt.plot(epoch_accuracy, label="Training")
  prefs: []
  type: TYPE_NORMAL
- en: plt.plot(testing_accuracy, label="Test")
  prefs: []
  type: TYPE_NORMAL
- en: plt.xlabel("Epoch")
  prefs: []
  type: TYPE_NORMAL
- en: plt.ylabel("Accuracy")
  prefs: []
  type: TYPE_NORMAL
- en: plt.legend()
  prefs: []
  type: TYPE_NORMAL
- en: plt.show()
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00033.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '**Final thoughts**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article shows how you can use JAX and Flax to train machine learning models
    in parallel on multiple devices. You have seen that the process involves making
    a few functions parallel using JAX's pmap function. We have also covered how to
    replicate the training and test data on multiple devices.
  prefs: []
  type: TYPE_NORMAL
- en: '**How to use TensorBoard in JAX & Flax**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tracking machine learning experiments makes understanding and visualizing the
    model's performance easy. It also makes it possible to spot any problems in the
    network. For example, you can quickly spot overfitting by looking at the training
    and validation charts. You can plot these charts using your favorite charts package,
    such as Matplotlib. However, you can also use more advanced tools such as TensorBoard.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorBoard is an open-source library that provides tools for experiment tracking in
    machine learning. You can use TensorBoard for:'
  prefs: []
  type: TYPE_NORMAL
- en: Tracking and visualizing model evaluation metrics such as accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Logging images.
  prefs: []
  type: TYPE_NORMAL
- en: Visualize hyper-parameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Project embeddings such as word embedding in natural language processing problems.
  prefs: []
  type: TYPE_NORMAL
- en: Visualize histograms of the model's weights and biases.
  prefs: []
  type: TYPE_NORMAL
- en: Plot the architecture of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Profile the performance of the network.
  prefs: []
  type: TYPE_NORMAL
- en: You can use TensorBoard with popular machine learning libraries such as XGBoost, JAX, Flax,
    and PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: This article will focus on how to use TensorBoard when building networks with JAX and
    Flax.
  prefs: []
  type: TYPE_NORMAL
- en: '**How to use TensorBoard**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's start by exploring how to use TensorBoard.
  prefs: []
  type: TYPE_NORMAL
- en: '**How to install TensorBoard**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step is to install TensorBoard from the Python Index. pip install
    tensorboard
  prefs: []
  type: TYPE_NORMAL
- en: '**Using TensorBoard with Jupyter notebooks and Google Colab**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once TensorBoard is installed, you need to load it in your environment, usually
    Google Colab or your local notebook.%load_ext tensorboardNext, inform TensorBoard
    which folder will contain the log information.log_folder = "runs"
  prefs: []
  type: TYPE_NORMAL
- en: '**How to launch TensorBoard**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tensorboard is launched using the [tensorboard] magic command in notebook environments
    while specifying the [logdir].%tensorboard --logdir={log_folder}
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also launch TensorBoard on the command line using a similar pattern.
    Apart from viewing the terminal on the notebook environment, you can also view
    it on the browser by visiting: http://localhost:6006.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tensorboard dashboards**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TensorBoard has various dashboards for showing different types of information.
  prefs: []
  type: TYPE_NORMAL
- en: The  **Scalars** dashboard tracks numerical information such as training metrics
    per epoch. You can use it to track other scalar values such as model training
    speed and learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: The **Graphs** dashboard is used for showing visualizations. For example, you
    can use it to check the architecture of the network.
  prefs: []
  type: TYPE_NORMAL
- en: The  **Distributions and Histograms** dashboard show the distribution of tensors
    over time. Use it to check the weights and biases of the network.
  prefs: []
  type: TYPE_NORMAL
- en: The **Images** dashboard shows the images you have logged to TensorBoard.
  prefs: []
  type: TYPE_NORMAL
- en: The **HParams** dashboard visualizes hyperparameter optimization. It helps identify
    the best parameters for the network.
  prefs: []
  type: TYPE_NORMAL
- en: The  **Embedding Projector** is used to visualize low-level embeddings, for
    example, text embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: The **What-If Tool** dashboard helps in understanding the performance of a model.
    It also enables the measurement of a model's fairness on data subsets.
  prefs: []
  type: TYPE_NORMAL
- en: The  **TensorFlow Profiler** monitors the model training process. It also shows
    the events in the CPU and GPU during training. The TensorFlow profiler goes further
    to offer recommendations based on the data collected. You can also use it to debug
    performance issues in the input pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '**How to use TensorBoard with Flax**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With TensorBoard installed and some basics out of the way, let's look at how
    you can use it in Flax. Let's use
  prefs: []
  type: TYPE_NORMAL
- en: the [SummaryWriter] from PyTorch to write to the log folder.
  prefs: []
  type: TYPE_NORMAL
- en: '**How to log images with TensorBoard in Flax**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may want to log sample images when solving computer vision problems. You
    can also log predictions while training the model. For example, you can log prediction
    images containing bounding boxes for an object detection network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at how we can log an image to TensorBoard.from torch.utils.tensorboard
    import SummaryWriter import torchvision.transforms.functional as Fwriter = SummaryWriter(logdir)def
    show(imgs):if not isinstance(imgs, list):'
  prefs: []
  type: TYPE_NORMAL
- en: imgs = [imgs]
  prefs: []
  type: TYPE_NORMAL
- en: 'fig, axs = plt.subplots(ncols=len(imgs), squeeze=False) for i, img in enumerate(imgs):'
  prefs: []
  type: TYPE_NORMAL
- en: img = img.detach()
  prefs: []
  type: TYPE_NORMAL
- en: img = F.to_pil_image(img)
  prefs: []
  type: TYPE_NORMAL
- en: axs[0, i].imshow(np.asarray(img))
  prefs: []
  type: TYPE_NORMAL
- en: axs[0, i].set(xticklabels=[], yticklabels=[], xticks=
  prefs: []
  type: TYPE_NORMAL
- en: '[], yticks=[])writer.flush() # Ensure that everything is written to diskNext,
    create a grid with the images that will be logged.'
  prefs: []
  type: TYPE_NORMAL
- en: from torchvision.utils import make_grid from torchvision.io import read_image
    from pathlib import Path
  prefs: []
  type: TYPE_NORMAL
- en: cat = read_image(str(Path('train') / 'cat.1.jpg')) grid = make_grid(cat)
  prefs: []
  type: TYPE_NORMAL
- en: show(grid)
  prefs: []
  type: TYPE_NORMAL
- en: The [add_image] function is used to write images to TensorBoard.writer.add_image('sample_cat',
    grid)Now, load the TensorBoard extension and point it to the logs folder.%tensorboard
    --logdir={logdir}The logged images will be visible on the Images dashboard. ![](../images/00034.gif)**TensorBoard
    Image dashboard ** **## How to log text with TensorBoard in Flax** Writing text
    to TensorBoard is done using the [add_text] function.writer.add_text('Text', 'Write
    image to TensorBoard', 0) The logged data is available on the Text dashboard.![](../images/00035.jpeg)
  prefs: []
  type: TYPE_NORMAL
- en: '**Track model training in JAX using TensorBoard**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can log the evaluation metrics when training machine learning models with
    JAX. They obtained at the training stage. At this point, you can log the metrics
    to TensorBoard. In the example below, we log the training and evaluation metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'for epoch in range(1, num_epochs + 1):'
  prefs: []
  type: TYPE_NORMAL
- en: train_state, train_metrics = train_one_epoch(state, train_l
  prefs: []
  type: TYPE_NORMAL
- en: oader)
  prefs: []
  type: TYPE_NORMAL
- en: training_loss.append(train_metrics['loss'])
  prefs: []
  type: TYPE_NORMAL
- en: 'training_accuracy.append(train_metrics[''accuracy'']) print(f"Train epoch:
    {epoch}, loss: {train_metrics[''los'
  prefs: []
  type: TYPE_NORMAL
- en: 's'']}, accuracy: {train_metrics[''accuracy''] * 100}")'
  prefs: []
  type: TYPE_NORMAL
- en: test_metrics = evaluate_model(train_state, test_images, tes t_labels)
  prefs: []
  type: TYPE_NORMAL
- en: testing_loss.append(test_metrics['loss'])
  prefs: []
  type: TYPE_NORMAL
- en: testing_accuracy.append(test_metrics['accuracy'])
  prefs: []
  type: TYPE_NORMAL
- en: writer.add_scalar('Loss/train', train_metrics['loss'], epoc h)
  prefs: []
  type: TYPE_NORMAL
- en: writer.add_scalar('Loss/test', test_metrics['loss'], epoch)
  prefs: []
  type: TYPE_NORMAL
- en: 'writer.add_scalar(''Accuracy/train'', train_metrics[''accurac y''], epoch)writer.add_scalar(''Accuracy/test'',
    test_metrics[''accurac y''], epoch)print(f"Test epoch: {epoch}, loss: {test_metrics[''loss'']},
    accuracy: {test_metrics[''accuracy''] * 100}")These metrics will be available
    on the **Scalars** dashboard of TensorBoard.![](../images/00036.gif)'
  prefs: []
  type: TYPE_NORMAL
- en: '**How to profile JAX programs with TensorBoard**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To profileJAX programs, send data to the TensorBoard profiler. The first step
    is to install the profile plugin.pip install -U tensorboard-plugin-profile
  prefs: []
  type: TYPE_NORMAL
- en: '**Programmatic profiling**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use jax.profiler.start_trace() to start a trace
  prefs: []
  type: TYPE_NORMAL
- en: and jax.profiler.stop_trace() to stop a trace.
  prefs: []
  type: TYPE_NORMAL
- en: The [start_trace()] expects the path to the directory where the traces will
    be written.import jaxjax.profiler.start_trace("runs")
  prefs: []
  type: TYPE_NORMAL
- en: Run the operations to be profiled key = jax.random.PRNGKey(0)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: x = jax.random.normal(key, (5000, 5000)) y = x @ x
  prefs: []
  type: TYPE_NORMAL
- en: y.block_until_ready()
  prefs: []
  type: TYPE_NORMAL
- en: jax.profiler.stop_trace()
  prefs: []
  type: TYPE_NORMAL
- en: '**Manual profiling with TensorBoard**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The second option is to profile the JAX program manually. ![](../images/00037.jpeg)This
    is done in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize TensorBoard tensorboard --logdir /runsStart a JAX profiler server
    at the begining of the program and stop the server at the end of the program.
  prefs: []
  type: TYPE_NORMAL
- en: import jax.profiler
  prefs: []
  type: TYPE_NORMAL
- en: jax.profiler.start_server(9999)
  prefs: []
  type: TYPE_NORMAL
- en: train_one_epoch(state, train_loader,num_epochs)
  prefs: []
  type: TYPE_NORMAL
- en: jax.profiler.stop_server()
  prefs: []
  type: TYPE_NORMAL
- en: Open the Profile dashboard of TensorBoard. Click  **CAPTURE PROFILE** and enter
    the URL of the server that you started above, in this case localhost:9999\. Click
    CAPTURE to start profiling.
  prefs: []
  type: TYPE_NORMAL
- en: Select  **trace_viewer** under **Tools** on the profile dashboard. Use the navigation
    tools here to click specific events to see more information about them.
  prefs: []
  type: TYPE_NORMAL
- en: '**How to profile JAX program on a remote machine**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can profile a JAX program on a remote server by executing the above instructions
    on the remote server. This involves starting a TensorBoard server on the remote
    machine, and port forwarding it to your local machine. You will then access TensorBoard
    locally via the web UI.
  prefs: []
  type: TYPE_NORMAL
- en: ssh -L 6006:localhost:6006 <remote server address>
  prefs: []
  type: TYPE_NORMAL
- en: '**Share TensorBoard dashboards**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TensorBoard.dev is a hosted version of TensorBoard that makes it easy to share
    your experiments. Let's upload the above TensorBoard to TensorBoard.dev.
  prefs: []
  type: TYPE_NORMAL
- en: On Colab or Jupyter nmotebook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '!tensorboard dev upload --logdir ./runs \'
  prefs: []
  type: TYPE_NORMAL
- en: --name "Flax experiments" \
  prefs: []
  type: TYPE_NORMAL
- en: --description "Logging model metrics with JAX" \
  prefs: []
  type: TYPE_NORMAL
- en: --one_shot
  prefs: []
  type: TYPE_NORMAL
- en: When you run the above code, you will get a prompt to authorize the upload.
    You should be keen not to share sensitive data because  TensorBoard.dev experiments
    are public.
  prefs: []
  type: TYPE_NORMAL
- en: You can view the experiment on TensorBoard.dev.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00038.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '**Final thoughts**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this article, we have seen how you can use TensorBoard to log your experiments
    in Flax. More specifically, you have learned: What is TensorBoard?'
  prefs: []
  type: TYPE_NORMAL
- en: How to install and launch TensorBoard.
  prefs: []
  type: TYPE_NORMAL
- en: How to log images and text to TensorBoard.
  prefs: []
  type: TYPE_NORMAL
- en: How to log model metrics to TensorBoard.
  prefs: []
  type: TYPE_NORMAL
- en: How to profile JAX and Flax programs using TensorBoard How to upload the log
    to TensorBoard.dev.
  prefs: []
  type: TYPE_NORMAL
- en: '**Handling state in JAX & Flax (BatchNorm and DropOut layers)**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Jitting functions in Flax makes them faster but requires that the functions
    have no side effects. The fact that jitted functions can't have side effects introduces
    a challenge when dealing with stateful items such as model parameters and stateful
    layers such as batch normalization layers.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we'll create a network with the BatchNorm and DropOut layers.
    After that, we'll see how to deal with generating the random number for the DropOut
    layer and adding the batch statistics when training the network.
  prefs: []
  type: TYPE_NORMAL
- en: '**Perform standard imports**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We kick off by importing standard data science packages that we'll use in this
    article.
  prefs: []
  type: TYPE_NORMAL
- en: import torch
  prefs: []
  type: TYPE_NORMAL
- en: from torch.utils.data import DataLoader import os
  prefs: []
  type: TYPE_NORMAL
- en: from PIL import Image
  prefs: []
  type: TYPE_NORMAL
- en: from torchvision import transforms from torch.utils.data import Dataset import
    numpy as np
  prefs: []
  type: TYPE_NORMAL
- en: import pandas as pd
  prefs: []
  type: TYPE_NORMAL
- en: from typing import Any
  prefs: []
  type: TYPE_NORMAL
- en: import matplotlib.pyplot as plt
  prefs: []
  type: TYPE_NORMAL
- en: '%matplotlib inline'
  prefs: []
  type: TYPE_NORMAL
- en: ignore harmless warnings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: import warnings
  prefs: []
  type: TYPE_NORMAL
- en: warnings.filterwarnings("ignore") import jax
  prefs: []
  type: TYPE_NORMAL
- en: from jax import numpy as jnp
  prefs: []
  type: TYPE_NORMAL
- en: import flax
  prefs: []
  type: TYPE_NORMAL
- en: from flax import linen as nn
  prefs: []
  type: TYPE_NORMAL
- en: from flax.training import train_state import optax
  prefs: []
  type: TYPE_NORMAL
- en: '**Download the dataset**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's illustrate how to include BatchNorm and DropOut layers in a Flax network
    by designing a simple Convolutional Neural Network using the cat and dogs dataset from
    Kaggle.
  prefs: []
  type: TYPE_NORMAL
- en: Download and extract the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: import wget
  prefs: []
  type: TYPE_NORMAL
- en: wget.download("https://ml.machinelearningnuggets.com/train.zi p")
  prefs: []
  type: TYPE_NORMAL
- en: 'import zipfilewith zipfile.ZipFile(''train.zip'', ''r'') as zip_ref: zip_ref.extractall(''.'')'
  prefs: []
  type: TYPE_NORMAL
- en: '**Loading datasets in JAX**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since JAX doesn't ship with data loading tools, load the dataset using PyTorch.
    We start by creating a PyTorch Dataset class.
  prefs: []
  type: TYPE_NORMAL
- en: 'class CatsDogsDataset(Dataset):'
  prefs: []
  type: TYPE_NORMAL
- en: def __init__(self, root_dir, annotation_file, transform=Non
  prefs: []
  type: TYPE_NORMAL
- en: 'e):'
  prefs: []
  type: TYPE_NORMAL
- en: self.root_dir = root_dir
  prefs: []
  type: TYPE_NORMAL
- en: self.annotations = pd.read_csv(annotation_file) self.transform = transform
  prefs: []
  type: TYPE_NORMAL
- en: def __len__(self):return len(self.annotations)
  prefs: []
  type: TYPE_NORMAL
- en: 'def __getitem__(self, index):'
  prefs: []
  type: TYPE_NORMAL
- en: img_id = self.annotations.iloc[index, 0]
  prefs: []
  type: TYPE_NORMAL
- en: img = Image.open(os.path.join(self.root_dir, img_id)).c
  prefs: []
  type: TYPE_NORMAL
- en: onvert("RGB")
  prefs: []
  type: TYPE_NORMAL
- en: y_label = torch.tensor(float(self.annotations.iloc[inde
  prefs: []
  type: TYPE_NORMAL
- en: x, 1]))
  prefs: []
  type: TYPE_NORMAL
- en: 'if self.transform is not None: img = self.transform(img)return (img, y_label)Interested
    in learning more about loading datasets in JAX?👉 Check our How to load datasets
    in JAX with TensorFlow tutorial.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, create a Pandas DataFrame containing the image path and the labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'train_df = pd.DataFrame(columns=["img_path","label"]) train_df["img_path"]
    = os.listdir("train/") for idx, i in enumerate(os.listdir("train/")):'
  prefs: []
  type: TYPE_NORMAL
- en: 'if "cat" in i:'
  prefs: []
  type: TYPE_NORMAL
- en: train_df["label"][idx] = 0
  prefs: []
  type: TYPE_NORMAL
- en: 'if "dog" in i:'
  prefs: []
  type: TYPE_NORMAL
- en: train_df["label"][idx] = 1
  prefs: []
  type: TYPE_NORMAL
- en: train_df.to_csv (r'train_csv.csv', index = False, header=True)
  prefs: []
  type: TYPE_NORMAL
- en: '**Data processing with PyTorch**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, create a function to stack the dataset and return it as a NumPy array.
  prefs: []
  type: TYPE_NORMAL
- en: 'def custom_collate_fn(batch):'
  prefs: []
  type: TYPE_NORMAL
- en: transposed_data = list(zip(*batch)) labels = np.array(transposed_data[1]) imgs
    = np.stack(transposed_data[0]) return imgs, labels
  prefs: []
  type: TYPE_NORMAL
- en: We then use PyTorch to create training and testing data loaders.size_image =
    224 batch_size = 64
  prefs: []
  type: TYPE_NORMAL
- en: transform = transforms.Compose([
  prefs: []
  type: TYPE_NORMAL
- en: transforms.Resize((size_image,size_image)),
  prefs: []
  type: TYPE_NORMAL
- en: np.array])
  prefs: []
  type: TYPE_NORMAL
- en: dataset = CatsDogsDataset("train","train_csv.csv",transform=tra nsform)
  prefs: []
  type: TYPE_NORMAL
- en: train_set, validation_set = torch.utils.data.random_split(datas et,[20000,5000])
  prefs: []
  type: TYPE_NORMAL
- en: train_loader = DataLoader(dataset=train_set, collate_fn=custom_ collate_fn,shuffle=True,
    batch_size=batch_size)
  prefs: []
  type: TYPE_NORMAL
- en: validation_loader = DataLoader(dataset=validation_set,collate_f n=custom_collate_fn,
    shuffle=False, batch_size=batch_size)
  prefs: []
  type: TYPE_NORMAL
- en: '**Define Flax model with BatchNorm and DropOut**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Define the Flax network with the BatchNorm and DropOut layers. In the network,
    we introduce the [training] variable to control when the batch stats should be
    updated. We ensure that they aren't updated during testing.
  prefs: []
  type: TYPE_NORMAL
- en: In the BatchNorm layer we set use_running_average to False meaning
  prefs: []
  type: TYPE_NORMAL
- en: that the stats stored in [batch_stats] will not be used, but batch stats of
    the input will be computed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The [DropOut] layer takes the following rate:'
  prefs: []
  type: TYPE_NORMAL
- en: The rate drop out probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Whether it''s deterministic. If deterministic inputs are scaled and masked.
    Otherwise, they are not masked and returned as they are.class CNN(nn.Module):'
  prefs: []
  type: TYPE_NORMAL
- en: '@nn.compact'
  prefs: []
  type: TYPE_NORMAL
- en: 'def __call__(self, x, training):'
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.Conv(features=128, kernel_size=(3, 3))(x)
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.relu(x)
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.Conv(features=64, kernel_size=(3, 3))(x)
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.relu(x)
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.Conv(features=32, kernel_size=(3, 3))(x)
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.relu(x)
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))
  prefs: []
  type: TYPE_NORMAL
- en: x = x.reshape((x.shape[0], -1))
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.Dense(features=256)(x)
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.Dense(features=128)(x)
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.BatchNorm(use_running_average=not training)(x)
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.Dropout(0.2, deterministic=not training)(x)
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.relu(x)
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.Dense(features=2)(x)
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.log_softmax(x) return x
  prefs: []
  type: TYPE_NORMAL
- en: '**Create loss function**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The next step is to create the loss function. When applying the model, we:'
  prefs: []
  type: TYPE_NORMAL
- en: Pass the batch stats parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[training] as True. Set the [batch_stats] as mutable.'
  prefs: []
  type: TYPE_NORMAL
- en: Set the random number for the DropOut
  prefs: []
  type: TYPE_NORMAL
- en: 'def cross_entropy_loss(*, logits, labels):'
  prefs: []
  type: TYPE_NORMAL
- en: labels_onehot = jax.nn.one_hot(labels, num_classes=2) return optax.softmax_cross_entropy(logits=logits,
    labels=labe
  prefs: []
  type: TYPE_NORMAL
- en: ls_onehot).mean()
  prefs: []
  type: TYPE_NORMAL
- en: 'def compute_loss(params, batch_stats, images, labels): logits,batch_stats =
    CNN().apply({''params'': params,''batch_s tats'': batch_stats},images, training=True,rngs={''dropout'':
    jax. random.PRNGKey(0)}, mutable=[''batch_stats''])'
  prefs: []
  type: TYPE_NORMAL
- en: loss = cross_entropy_loss(logits=logits, labels=labels) return loss, (logits,
    batch_stats)
  prefs: []
  type: TYPE_NORMAL
- en: '**Compute metrics**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The compute metrics function calculates the loss and accuracy and returns them.
  prefs: []
  type: TYPE_NORMAL
- en: 'def compute_metrics(*, logits, labels):'
  prefs: []
  type: TYPE_NORMAL
- en: loss = cross_entropy_loss(logits=logits, labels=labels) accuracy = jnp.mean(jnp.argmax(logits,
    -1) == labels) metrics = {
  prefs: []
  type: TYPE_NORMAL
- en: '''loss'': loss,'
  prefs: []
  type: TYPE_NORMAL
- en: '''accuracy'': accuracy,'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: return metrics
  prefs: []
  type: TYPE_NORMAL
- en: '**Create custom Flax training state**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's create a custom Flax training state that will store the batch stats information.
    To do that, create a new training state class that subclasses Flax's TrainState.
  prefs: []
  type: TYPE_NORMAL
- en: initialize weights
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: model = CNN()
  prefs: []
  type: TYPE_NORMAL
- en: key = jax.random.PRNGKey(0)
  prefs: []
  type: TYPE_NORMAL
- en: variables = model.init(key, jnp.ones([1, size_image, size_imag e, 3]), training=False)
  prefs: []
  type: TYPE_NORMAL
- en: 'class TrainState(train_state.TrainState): batch_stats: flax.core.FrozenDictTo
    define a Flax training state, use [TrainState.create] and pass the: Apply function.'
  prefs: []
  type: TYPE_NORMAL
- en: Model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: The optimizer function. The batch stats.
  prefs: []
  type: TYPE_NORMAL
- en: state = TrainState.create(
  prefs: []
  type: TYPE_NORMAL
- en: apply_fn = model.apply,
  prefs: []
  type: TYPE_NORMAL
- en: params = variables['params'],
  prefs: []
  type: TYPE_NORMAL
- en: tx = optax.sgd(0.01),
  prefs: []
  type: TYPE_NORMAL
- en: batch_stats = variables['batch_stats'],
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: '**Training step**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the training step, we compute the gradients with respect to the loss and
    model parameters– the **model parameters **and **batch statistics**. We use the
    gradients to update the model parameters and return the new state and model metrics.
    The function is decorated
  prefs: []
  type: TYPE_NORMAL
- en: with @jax.jit to make the computation faster.
  prefs: []
  type: TYPE_NORMAL
- en: '@jax.jit'
  prefs: []
  type: TYPE_NORMAL
- en: 'def train_step(state,images, labels):'
  prefs: []
  type: TYPE_NORMAL
- en: '"""Train for a single step."""'
  prefs: []
  type: TYPE_NORMAL
- en: (batch_loss, (logits, batch_stats)), grads= jax.value_and_gra d(compute_loss,
    has_aux=True)(state.params,state.batch_stats, i mages,labels)
  prefs: []
  type: TYPE_NORMAL
- en: state = state.apply_gradients(grads=grads)
  prefs: []
  type: TYPE_NORMAL
- en: 'metrics = compute_metrics(logits=logits, labels=labels) return state, metricsNext,
    define a function that applies the training step for one epoch. The functions:'
  prefs: []
  type: TYPE_NORMAL
- en: Loops through the training data.
  prefs: []
  type: TYPE_NORMAL
- en: Passes each training batch the training step.
  prefs: []
  type: TYPE_NORMAL
- en: Obtains the batch metrics. Computes the mean to obtain the epoch metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Returns the new state and metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'def train_one_epoch(state, dataloader):'
  prefs: []
  type: TYPE_NORMAL
- en: '"""Train for 1 epoch on the training set.""" batch_metrics = []'
  prefs: []
  type: TYPE_NORMAL
- en: 'for cnt, (images, labels) in enumerate(dataloader):'
  prefs: []
  type: TYPE_NORMAL
- en: images = images / 255.0
  prefs: []
  type: TYPE_NORMAL
- en: state, metrics = train_step(state, images, labels) batch_metrics.append(metrics)
  prefs: []
  type: TYPE_NORMAL
- en: 'batch_metrics_np = jax.device_get(batch_metrics) epoch_metrics_np = {k: np.mean([metrics[k]
    for metrics in batch_metrics_n'
  prefs: []
  type: TYPE_NORMAL
- en: p])
  prefs: []
  type: TYPE_NORMAL
- en: for k in batch_metrics_np[0] }
  prefs: []
  type: TYPE_NORMAL
- en: return state, epoch_metrics_np
  prefs: []
  type: TYPE_NORMAL
- en: '**Evaluation step**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We pass the test images and labels to the model in the evaluation step and obtain
    the evaluation metrics. The function is also jitted to take advantage of JAX's
    fast computation. During the evaluation, set [training] to [False] so that the
    model parameters are not updated. In this step, we also pass the batch stats and
    the random number generator for the [DropOut] layer.
  prefs: []
  type: TYPE_NORMAL
- en: '@jax.jitdef eval_step(batch_stats, params, images, labels): logits = CNN().apply({''params'':
    params,''batch_stats'': batch _stats}, images, training=False,rngs={''dropout'':
    jax.random.PRN GKey(0)})return compute_metrics(logits=logits, labels=labels)The [evaluate_model] function
    applies the [eval_step] to the test data and returns the evaluation metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: 'def evaluate_model(state, test_imgs, test_lbls):'
  prefs: []
  type: TYPE_NORMAL
- en: '"""Evaluate on the validation set."""'
  prefs: []
  type: TYPE_NORMAL
- en: metrics = eval_step(state.batch_stats,state.params, test_im
  prefs: []
  type: TYPE_NORMAL
- en: gs, test_lbls)
  prefs: []
  type: TYPE_NORMAL
- en: metrics = jax.device_get(metrics)
  prefs: []
  type: TYPE_NORMAL
- en: 'metrics = jax.tree_map(lambda x: x.item(), metrics) return metrics'
  prefs: []
  type: TYPE_NORMAL
- en: '**Train Flax model**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To train the model, we define another function that implements  [train_one_epoch].
    Let's start by defining the model evaluation data.(test_images, test_labels) =
    next(iter(validation_loader)) test_images = test_images / 255.0
  prefs: []
  type: TYPE_NORMAL
- en: '**Set up TensorBoard in Flax**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can log the model metrics to TensorBoard by writing the scalars to TensorBoard.
  prefs: []
  type: TYPE_NORMAL
- en: from torch.utils.tensorboard import SummaryWriter import torchvision.transforms.functional
    as F logdir = "flax_logs"
  prefs: []
  type: TYPE_NORMAL
- en: writer = SummaryWriter(logdir)
  prefs: []
  type: TYPE_NORMAL
- en: '**Train model**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can also append the metrics to a list and visualize them with Matplotlib.
  prefs: []
  type: TYPE_NORMAL
- en: training_loss = [] training_accuracy = [] testing_loss = []
  prefs: []
  type: TYPE_NORMAL
- en: testing_accuracy = []
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, define the training function that will: Train the Flax model for the
    specified number of epochs.'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate the model on the test data.
  prefs: []
  type: TYPE_NORMAL
- en: Append the metrics to a list. Write model metrics to TensorBoard.
  prefs: []
  type: TYPE_NORMAL
- en: Print the metrics on every epoch. Return the trained model statedef train_model(epochs):for
    epoch in range(1, epochs + 1):train_state, train_metrics = train_one_epoch(state,
    tra
  prefs: []
  type: TYPE_NORMAL
- en: in_loader)
  prefs: []
  type: TYPE_NORMAL
- en: training_loss.append(train_metrics['loss']) training_accuracy.append(train_metrics['accuracy'])
    test_metrics = evaluate_model(train_state, test_images,
  prefs: []
  type: TYPE_NORMAL
- en: test_labels)
  prefs: []
  type: TYPE_NORMAL
- en: testing_loss.append(test_metrics['loss'])
  prefs: []
  type: TYPE_NORMAL
- en: testing_accuracy.append(test_metrics['accuracy'])
  prefs: []
  type: TYPE_NORMAL
- en: writer.add_scalar('Loss/train', train_metrics['loss'], epoch)
  prefs: []
  type: TYPE_NORMAL
- en: writer.add_scalar('Loss/test', test_metrics['loss'], ep och)
  prefs: []
  type: TYPE_NORMAL
- en: writer.add_scalar('Accuracy/train', train_metrics['accu racy'], epoch)
  prefs: []
  type: TYPE_NORMAL
- en: writer.add_scalar('Accuracy/test', test_metrics['accura cy'], epoch)
  prefs: []
  type: TYPE_NORMAL
- en: 'print(f"Epoch: {epoch}, training loss: {train_metrics [''loss'']}, training
    accuracy: {train_metrics[''accuracy''] * 10 0}, validation loss: {test_metrics[''loss'']},
    validation accurac y: {test_metrics[''accuracy''] * 100}")'
  prefs: []
  type: TYPE_NORMAL
- en: return train_stateRun the training function to train the model. trained_model_state
    = train_model(30)
  prefs: []
  type: TYPE_NORMAL
- en: '**Save Flax model**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The [save_checkpoint] saves a Flax model. It expects:'
  prefs: []
  type: TYPE_NORMAL
- en: The directory to save the model checkpoint.
  prefs: []
  type: TYPE_NORMAL
- en: The Flax trained model, in this case [trained_model_state]. The model's prefix.
  prefs: []
  type: TYPE_NORMAL
- en: Whether to overwrite existing models.
  prefs: []
  type: TYPE_NORMAL
- en: from flax.training import checkpoints
  prefs: []
  type: TYPE_NORMAL
- en: ckpt_dir = 'model_checkpoint/'
  prefs: []
  type: TYPE_NORMAL
- en: checkpoints.save_checkpoint(ckpt_dir=ckpt_dir,
  prefs: []
  type: TYPE_NORMAL
- en: target=trained_model_state, step=100,
  prefs: []
  type: TYPE_NORMAL
- en: prefix='flax_model', overwrite=True
  prefs: []
  type: TYPE_NORMAL
- en: )![](../images/00039.jpeg)
  prefs: []
  type: TYPE_NORMAL
- en: '**Load Flax model**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The [restore_checkpoint] method loads a saved Flax model from the saved location.
  prefs: []
  type: TYPE_NORMAL
- en: loaded_model = checkpoints.restore_checkpoint(
  prefs: []
  type: TYPE_NORMAL
- en: ckpt_dir=ckpt_dir, target=state, prefix='flax_mode
  prefs: []
  type: TYPE_NORMAL
- en: l')
  prefs: []
  type: TYPE_NORMAL
- en: '**Evaluate Flax model**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Run the [evalaute_model] function to check the performance of the model on test
    data.evaluate_model(trained_model_state,test_images, test_labels)
  prefs: []
  type: TYPE_NORMAL
- en: '**Visualize Flax model performance**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To visualize the performance of the Flax model, you can plot the metrics using Matplotlib or
    load TensorBoard and check the scalars tab.
  prefs: []
  type: TYPE_NORMAL
- en: '%load_ext tensorboard%tensorboard --logdir={logdir}'
  prefs: []
  type: TYPE_NORMAL
- en: '**Final thoughts**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this article, you have seen how to build networks in Flax containing BatchNorm
    and DropOut layers. You have also seen how to adjust the training process to cater
    to these new layers. Specifically, you have learned:'
  prefs: []
  type: TYPE_NORMAL
- en: How to define Flax models with BatchNorm and DropOut layers.
  prefs: []
  type: TYPE_NORMAL
- en: How to create a custom Flax training state.
  prefs: []
  type: TYPE_NORMAL
- en: Training and evaluating a Flax model with BatchNorm and DropOut layers.
  prefs: []
  type: TYPE_NORMAL
- en: How to save and load a Flax model.
  prefs: []
  type: TYPE_NORMAL
- en: How to evaluate the performance of a Flax model
  prefs: []
  type: TYPE_NORMAL
- en: '**LSTM in JAX & Flax**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LSTMs are a class of neural networks used to solve sequence problems such as
    time series and natural language processing. The LSTMs maintain some internal
    state that is useful in solving these problems. LSTMs apply for loops to iterate
    over each time step. We can use functions from JAX and Flax instead of writing
    these for loops from scratch. In this article, we will build a natural language
    processing model using LSTMs in Flax.
  prefs: []
  type: TYPE_NORMAL
- en: Let's get started.
  prefs: []
  type: TYPE_NORMAL
- en: '**Dataset download**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We'll use the movie review dataset from Kaggle. We download the dataset using
    Kaggle's Python package.
  prefs: []
  type: TYPE_NORMAL
- en: import os
  prefs: []
  type: TYPE_NORMAL
- en: '#Obtain from https://www.kaggle.com/username/account os.environ["KAGGLE_USERNAME"]="KAGGLE_USERNAME"'
  prefs: []
  type: TYPE_NORMAL
- en: os.environ["KAGGLE_KEY"]="KAGGLE_KEY"
  prefs: []
  type: TYPE_NORMAL
- en: import kaggle
  prefs: []
  type: TYPE_NORMAL
- en: kaggle datasets download lakshmi25npathi/imdb-dataset-of-50k-mo vie-reviews
  prefs: []
  type: TYPE_NORMAL
- en: Next, extract the dataset.import zipfilewith zipfile.ZipFile('imdb-dataset-of-50k-movie-reviews.zip',
    'r') as zip_ref:zip_ref.extractall('imdb-dataset-of-50k-movie-reviews')Load the
    dataset using Pandas and display a sample of the reviews.
  prefs: []
  type: TYPE_NORMAL
- en: df = pd.read_csv("imdb-dataset-of-50k-movie-reviews/IMDB Datase t.csv")
  prefs: []
  type: TYPE_NORMAL
- en: df.head()
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00040.gif)'
  prefs: []
  type: TYPE_IMG
- en: '**Data processing with NLTK**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The dataset contains unnecessary characters for predicting whether a movie review
    is negative or positive. For instance, punctuation marks and special characters.
    We, therefore, remove these from the reviews. We also need to convert the [sentiment] column
    into a numerical representation. This is achieved using  [LabelEncoder] from Scikitlearn.
    Let's import that together with other packages we'll use throughout this article.
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  prefs: []
  type: TYPE_NORMAL
- en: import pandas as pd
  prefs: []
  type: TYPE_NORMAL
- en: from numpy import array
  prefs: []
  type: TYPE_NORMAL
- en: import tensorflow as tf
  prefs: []
  type: TYPE_NORMAL
- en: from sklearn.model_selection import train_test_split from sklearn.preprocessing
    import LabelEncoder import matplotlib.pyplot as plt
  prefs: []
  type: TYPE_NORMAL
- en: The reviews also contain words that are not useful in the sentiment prediction.
    These are common words in English, such as the, at, and, etc. These words are
    known as **stopwords**. We remove them with the help of the nltk library. Let's
    start by defining a function to remove all the English stopwords.
  prefs: []
  type: TYPE_NORMAL
- en: pip install nltk
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: import nltk
  prefs: []
  type: TYPE_NORMAL
- en: from nltk.corpus import stopwords
  prefs: []
  type: TYPE_NORMAL
- en: nltk.download('stopwords')
  prefs: []
  type: TYPE_NORMAL
- en: 'def remove_stop_words(review):'
  prefs: []
  type: TYPE_NORMAL
- en: review_minus_sw = []
  prefs: []
  type: TYPE_NORMAL
- en: stop_words = stopwords.words('english')
  prefs: []
  type: TYPE_NORMAL
- en: review = review.split()
  prefs: []
  type: TYPE_NORMAL
- en: cleaned_review = [review_minus_sw.append(word) for word in
  prefs: []
  type: TYPE_NORMAL
- en: review if word not in stop_words]
  prefs: []
  type: TYPE_NORMAL
- en: cleaned_review = ' '.join(review_minus_sw)
  prefs: []
  type: TYPE_NORMAL
- en: return cleaned_review
  prefs: []
  type: TYPE_NORMAL
- en: Apply the function to the sentiment column.df['review'] = df['review'].apply(remove_stop_words)Let's
    also convert the sentiment column to numerical representation.
  prefs: []
  type: TYPE_NORMAL
- en: labelencoder = LabelEncoder()
  prefs: []
  type: TYPE_NORMAL
- en: df = df.assign(sentiment = labelencoder.fit_transform(df["senti ment"]))
  prefs: []
  type: TYPE_NORMAL
- en: Compare the reviews with the review with and without the stop words.![](../images/00041.jpeg)
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the third review, we notice that the
  prefs: []
  type: TYPE_NORMAL
- en: words [this], [was] and [a] have been dropped from the sentence. However, we
    can still see some special characters, such as [<br>] in the review. Let's resolve
    that next.
  prefs: []
  type: TYPE_NORMAL
- en: '**Text vectorization with Keras**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The review data is still in text form. However, we need to convert it to a numeric
    representation like the sentiment column. Before we do that, let's split the dataset
    into a training and testing set.
  prefs: []
  type: TYPE_NORMAL
- en: from sklearn.model_selection import train_test_split df = df.drop_duplicates()
  prefs: []
  type: TYPE_NORMAL
- en: docs = df['review']
  prefs: []
  type: TYPE_NORMAL
- en: labels = array(df['sentiment'])
  prefs: []
  type: TYPE_NORMAL
- en: X_train, X_test , y_train, y_test = train_test_split(docs, labe ls , test_size
    = 0.20, random_state=0)
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the Keras text vectorization layer to convert the reviews to integer
    form. This function lets us filter out all punctuation marks and convert the reviews
    to lowercase. We pass the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: standardize as lower_and_strip_punctuation to convert to
  prefs: []
  type: TYPE_NORMAL
- en: lowercase and remove punctuation marks.
  prefs: []
  type: TYPE_NORMAL
- en: '[output_mode] to [int] to get the result as integers. [tf_idf] would apply
    the TF-IDF algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: '[output_sequence_length]  as 50 to get sentences of that length. Change this
    number to see how it affects the model''s performance. I found 50 to five some
    good results. Sentences longer than the specified length will be truncated, while
    shorter ones will be padded with zeros.'
  prefs: []
  type: TYPE_NORMAL
- en: '[max_tokens]  as 10,000 to have a vocabulary size of that number. Tweak this
    number and check how the model''s performance changes.'
  prefs: []
  type: TYPE_NORMAL
- en: After defining the vectorization layer, we apply it to the training data. This
    is done by calling the [adapt] function. The function computes the vocabulary
    from the provided dataset. The vocabulary will be truncated to[max_tokens], if
    that is provided.
  prefs: []
  type: TYPE_NORMAL
- en: import tensorflow as tf
  prefs: []
  type: TYPE_NORMAL
- en: 'max_features = 10000 # Maximum vocab size.'
  prefs: []
  type: TYPE_NORMAL
- en: batch_size = 128
  prefs: []
  type: TYPE_NORMAL
- en: 'max_len = 50 # Sequence length to pad the outputs to. vectorize_layer = tf.keras.layers.TextVectorization(standardize
    =''lower_and_strip_punctuation'',max_tokens=max_features,output_m ode=''int'',output_sequence_length=max_len)'
  prefs: []
  type: TYPE_NORMAL
- en: vectorize_layer.adapt(X_train)
  prefs: []
  type: TYPE_NORMAL
- en: To view the generated vocabulary, call the get_vocabulary function.vectorize_layer.get_vocabulary()![](../images/00042.jpeg)Convert
    the training and test data to numerical form using the trained vectorization layer.X_train_padded
    = vectorize_layer(X_train)X_test_padded = vectorize_layer(X_test)![](../images/00043.gif)
  prefs: []
  type: TYPE_NORMAL
- en: '**Create tf.data dataset**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's generate and prefetch batches from the training and test set to make loading
    them to the LSTM model more efficient. We start by creating a tf.data.Dataset.
  prefs: []
  type: TYPE_NORMAL
- en: training_data = tf.data.Dataset.from_tensor_slices((X_train_pad ded, y_train))
  prefs: []
  type: TYPE_NORMAL
- en: validation_data = tf.data.Dataset.from_tensor_slices((X_test_pa dded, y_test))
  prefs: []
  type: TYPE_NORMAL
- en: training_data = training_data.batch(batch_size)
  prefs: []
  type: TYPE_NORMAL
- en: validation_data = validation_data.batch(batch_size)
  prefs: []
  type: TYPE_NORMAL
- en: Next, we prefetch one batch, shuffle the data and return it as a NumPy array.
  prefs: []
  type: TYPE_NORMAL
- en: pip install tensorflow_datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: import tensorflow_datasets as tfds
  prefs: []
  type: TYPE_NORMAL
- en: 'def get_train_batches():'
  prefs: []
  type: TYPE_NORMAL
- en: ds = training_data.prefetch(1)
  prefs: []
  type: TYPE_NORMAL
- en: 'ds = ds.shuffle(3, reshuffle_each_iteration=True) # tfds.dataset_as_numpy converts
    the tf.data.Dataset into an'
  prefs: []
  type: TYPE_NORMAL
- en: iterable of NumPy arraysreturn tfds.as_numpy(ds)
  prefs: []
  type: TYPE_NORMAL
- en: '**Define LSTM model in Flax**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are now ready to define the LSTM model in Flax. To design LSTMs in Flax,
    we use the LSTMCell or the OptimizedLSTMCell.
  prefs: []
  type: TYPE_NORMAL
- en: The OptimizedLSTMCell is the efficient LSTMCell.
  prefs: []
  type: TYPE_NORMAL
- en: 'The [LSTMCell.initialize_carry] function is used to initialize the hidden state
    of the LSTM cell. It expects:'
  prefs: []
  type: TYPE_NORMAL
- en: A random number.
  prefs: []
  type: TYPE_NORMAL
- en: The batch dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: The number of units.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use the setup method to define the LSTM model. The LSTM contains the
    following layers:'
  prefs: []
  type: TYPE_NORMAL
- en: An Embedding layer with the same number of features and length as defined in
    the vectorization layer.
  prefs: []
  type: TYPE_NORMAL
- en: LSTM layers that pass data in one direction as specified by the [reverse] argument.
  prefs: []
  type: TYPE_NORMAL
- en: A couple of Dense layers.
  prefs: []
  type: TYPE_NORMAL
- en: Final dense output layer.from flax import linen as nn
  prefs: []
  type: TYPE_NORMAL
- en: 'class LSTMModel(nn.Module):'
  prefs: []
  type: TYPE_NORMAL
- en: 'def setup(self):'
  prefs: []
  type: TYPE_NORMAL
- en: self.embedding = nn.Embed(max_features, max_len) lstm_layer = nn.scan(nn.OptimizedLSTMCell,
  prefs: []
  type: TYPE_NORMAL
- en: 'variable_broadcast="params", split_rngs={"params": False}, in_axes=1,'
  prefs: []
  type: TYPE_NORMAL
- en: out_axes=1,
  prefs: []
  type: TYPE_NORMAL
- en: length=max_len,
  prefs: []
  type: TYPE_NORMAL
- en: reverse=False)
  prefs: []
  type: TYPE_NORMAL
- en: self.lstm1 = lstm_layer()
  prefs: []
  type: TYPE_NORMAL
- en: self.dense1 = nn.Dense(256)
  prefs: []
  type: TYPE_NORMAL
- en: self.lstm2 = lstm_layer()
  prefs: []
  type: TYPE_NORMAL
- en: self.dense2 = nn.Dense(128)
  prefs: []
  type: TYPE_NORMAL
- en: self.lstm3 = lstm_layer()
  prefs: []
  type: TYPE_NORMAL
- en: self.dense3 = nn.Dense(64)
  prefs: []
  type: TYPE_NORMAL
- en: self.dense4 = nn.Dense(2)
  prefs: []
  type: TYPE_NORMAL
- en: '@nn.rematdef __call__(self, x_batch): x = self.embedding(x_batch)'
  prefs: []
  type: TYPE_NORMAL
- en: carry, hidden = nn.OptimizedLSTMCell.initialize_carry(j ax.random.PRNGKey(0),
    batch_dims=(len(x_batch),), size=128)
  prefs: []
  type: TYPE_NORMAL
- en: (carry, hidden), x = self.lstm1((carry, hidden), x)
  prefs: []
  type: TYPE_NORMAL
- en: x = self.dense1(x) x = nn.relu(x)
  prefs: []
  type: TYPE_NORMAL
- en: carry, hidden = nn.OptimizedLSTMCell.initialize_carry(j ax.random.PRNGKey(0),
    batch_dims=(len(x_batch),), size=64)
  prefs: []
  type: TYPE_NORMAL
- en: (carry, hidden), x = self.lstm2((carry, hidden), x) x = self.dense2(x) x = nn.relu(x)
  prefs: []
  type: TYPE_NORMAL
- en: carry, hidden = nn.OptimizedLSTMCell.initialize_carry(j ax.random.PRNGKey(0),
    batch_dims=(len(x_batch),), size=32)
  prefs: []
  type: TYPE_NORMAL
- en: (carry, hidden), x = self.lstm3((carry, hidden), x)
  prefs: []
  type: TYPE_NORMAL
- en: x = self.dense3(x)
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.relu(x)
  prefs: []
  type: TYPE_NORMAL
- en: x = self.dense4(x[:, -1]) return nn.log_softmax(x)
  prefs: []
  type: TYPE_NORMAL
- en: We apply the scan function to iterate over the data. It expects:scan the items
    to be looped over. They must be the same size and will be stacked along the scan
    axis.carry a carried value that is updated at each iteration. The value must be
    the same shape and [dtype] throughout the iteration.
  prefs: []
  type: TYPE_NORMAL
- en: '[broadcast] a value that is closed over by the loop [<axis:int>] axis along
    which to scan.'
  prefs: []
  type: TYPE_NORMAL
- en: '[split_rngs] to define if to split the random number generator at each step.'
  prefs: []
  type: TYPE_NORMAL
- en: The [nn.remat] call saves memory when using LSTMs to compute long sequences.
  prefs: []
  type: TYPE_NORMAL
- en: '**Compute metrics in Flax**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, we define a function to compute the loss and accuracy of the network.
  prefs: []
  type: TYPE_NORMAL
- en: import optax
  prefs: []
  type: TYPE_NORMAL
- en: import jax.numpy as jnp
  prefs: []
  type: TYPE_NORMAL
- en: 'def compute_metrics(logits, labels):'
  prefs: []
  type: TYPE_NORMAL
- en: loss = jnp.mean(optax.softmax_cross_entropy(logits, jax.nn.on e_hot(labels,
    num_classes=2)))
  prefs: []
  type: TYPE_NORMAL
- en: accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)
  prefs: []
  type: TYPE_NORMAL
- en: metrics = {
  prefs: []
  type: TYPE_NORMAL
- en: '''loss'': loss,'
  prefs: []
  type: TYPE_NORMAL
- en: '''accuracy'': accuracy'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: return metrics
  prefs: []
  type: TYPE_NORMAL
- en: '**Create training state**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The training state applies gradients and updates the parameters and optimizer
    state. Flax provides [train_state] for this purpose. We define a function that:'
  prefs: []
  type: TYPE_NORMAL
- en: Creates an instance of the [LSTMModel].
  prefs: []
  type: TYPE_NORMAL
- en: Initializes the model to obtain the [params] by passing a sample of the training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Returns the created state after applying the Adam optimizer.from flax.training
    import train_state
  prefs: []
  type: TYPE_NORMAL
- en: 'def create_train_state(rng):'
  prefs: []
  type: TYPE_NORMAL
- en: '"""Creates initial `TrainState`."""'
  prefs: []
  type: TYPE_NORMAL
- en: model = LSTMModel()
  prefs: []
  type: TYPE_NORMAL
- en: params = model.init(rng, jnp.array(X_train_padded[0]))['param
  prefs: []
  type: TYPE_NORMAL
- en: s']
  prefs: []
  type: TYPE_NORMAL
- en: tx = optax.adam(0.001,0.9,0.999,1e-07) return train_state.TrainState.create(
  prefs: []
  type: TYPE_NORMAL
- en: apply_fn=model.apply, params=params, tx=tx)
  prefs: []
  type: TYPE_NORMAL
- en: '**Define training step**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The training function does the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Compute the loss and logits from the model with the apply method.
  prefs: []
  type: TYPE_NORMAL
- en: Compute the gradients using [value_and_grad]. Use the gradients to update the
    model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Compute the metrics using the function defined earlier. Returns the state and
    metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Applying [@jax.jit] makes the function run faster.@jax.jitdef train_step(state,
    text, labels):def loss_fn(params):'
  prefs: []
  type: TYPE_NORMAL
- en: 'logits = LSTMModel().apply({''params'': params}, text) loss = jnp.mean(optax.softmax_cross_entropy(
    logits=logits,'
  prefs: []
  type: TYPE_NORMAL
- en: labels=jax.nn.one_hot(labels, num_classes=2))) return loss, logits
  prefs: []
  type: TYPE_NORMAL
- en: grad_fn = jax.value_and_grad(loss_fn, has_aux=True) (_, logits), grads = grad_fn(state.params)
  prefs: []
  type: TYPE_NORMAL
- en: state = state.apply_gradients(grads=grads)
  prefs: []
  type: TYPE_NORMAL
- en: metrics = compute_metrics(logits, labels)
  prefs: []
  type: TYPE_NORMAL
- en: return state, metrics
  prefs: []
  type: TYPE_NORMAL
- en: '**Evaluate the Flax model**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The [eval_step] evaluates the model's performance on the test set using Module.apply.
    It returns the loss and accuracy on the testing set.
  prefs: []
  type: TYPE_NORMAL
- en: The [evaluate_model] function applies the [eval_step] , obtains the metrics
    from the device and returns them as a [jax.tree_map].
  prefs: []
  type: TYPE_NORMAL
- en: '@jax.jit'
  prefs: []
  type: TYPE_NORMAL
- en: 'def eval_step(state, text, labels):'
  prefs: []
  type: TYPE_NORMAL
- en: 'logits = LSTMModel().apply({''params'': state.params}, text)'
  prefs: []
  type: TYPE_NORMAL
- en: return compute_metrics(logits=logits, labels=labels)
  prefs: []
  type: TYPE_NORMAL
- en: 'def evaluate_model(state, text, test_lbls): """Evaluate on the validation set."""'
  prefs: []
  type: TYPE_NORMAL
- en: metrics = eval_step(state, text, test_lbls) metrics = jax.device_get(metrics)
  prefs: []
  type: TYPE_NORMAL
- en: 'metrics = jax.tree_map(lambda x: x.item(), metrics) return metrics'
  prefs: []
  type: TYPE_NORMAL
- en: '**Create training function**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, define a function that trains the Flax LSTM model on one epoch. The function
    applies  [train_step] to each batch in the training data. After each batch, it
    appends the metrics to a list.
  prefs: []
  type: TYPE_NORMAL
- en: 'def train_one_epoch(state):'
  prefs: []
  type: TYPE_NORMAL
- en: '"""Train for 1 epoch on the training set.""" batch_metrics = []'
  prefs: []
  type: TYPE_NORMAL
- en: 'for text, labels in get_train_batches():'
  prefs: []
  type: TYPE_NORMAL
- en: 'state, metrics = train_step(state, text, labels) batch_metrics.append(metrics)batch_metrics_np
    = jax.device_get(batch_metrics) epoch_metrics_np = {k: np.mean([metrics[k] for
    metrics in batch_metrics_n p])for k in batch_metrics_np[0] }return state, epoch_metrics_np'
  prefs: []
  type: TYPE_NORMAL
- en: The function obtains the metrics from the device and computes the mean from
    all the trained batches. This gives the loss and accuracy for one epoch.
  prefs: []
  type: TYPE_NORMAL
- en: '**Train LSTM model in Flax**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To train the LSTM model, we run the [train_one_epoch] function for several iterations.
    Next, apply the [evaluate_model] to obtain the test metrics for each epoch. Before
    training starts, we create
  prefs: []
  type: TYPE_NORMAL
- en: a [create_train_state] to hold the training information. The function initializes
    the model parameters and the optimizer. This information is stored in the training
    state dataclass.
  prefs: []
  type: TYPE_NORMAL
- en: 'rng = jax.random.PRNGKey(0)rng, input_rng, init_rng = jax.random.split(rng,num=3)seed
    = 0state = create_train_state(init_rng) del init_rng # Must not be used anymore.'
  prefs: []
  type: TYPE_NORMAL
- en: num_epochs = 30
  prefs: []
  type: TYPE_NORMAL
- en: (text, test_labels) = next(iter(validation_data)) text = jnp.array(text)
  prefs: []
  type: TYPE_NORMAL
- en: test_labels = jnp.array(test_labels) training_loss = []
  prefs: []
  type: TYPE_NORMAL
- en: training_accuracy = []
  prefs: []
  type: TYPE_NORMAL
- en: testing_loss = []
  prefs: []
  type: TYPE_NORMAL
- en: testing_accuracy = []
  prefs: []
  type: TYPE_NORMAL
- en: 'def train_model():'
  prefs: []
  type: TYPE_NORMAL
- en: 'for epoch in range(1, num_epochs + 1):'
  prefs: []
  type: TYPE_NORMAL
- en: train_state, train_metrics = train_one_epoch(state) training_loss.append(train_metrics['loss'])
    training_accuracy.append(train_metrics['accuracy']) test_metrics = evaluate_model(train_state,
    text, test_l
  prefs: []
  type: TYPE_NORMAL
- en: abels)
  prefs: []
  type: TYPE_NORMAL
- en: testing_loss.append(test_metrics['loss'])
  prefs: []
  type: TYPE_NORMAL
- en: 'testing_accuracy.append(test_metrics[''accuracy'']) print(f"Epoch: {epoch},
    train loss: {train_metrics[''los'
  prefs: []
  type: TYPE_NORMAL
- en: 's'']}, train accuracy: {train_metrics[''accuracy''] * 100}, test l oss: {test_metrics[''loss'']},
    test accuracy: {test_metrics[''accu racy''] * 100}")'
  prefs: []
  type: TYPE_NORMAL
- en: return train_statetrained_model_state = train_model()After each epoch, we print
    the metrics and append them to a list.
  prefs: []
  type: TYPE_NORMAL
- en: '**Visualize LSTM model performance in Flax**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can then use Matplotlib to visualize the metrics appended to the list. The
    training is not quite smooth, but you can tweak the architecture of the network,
    the length of each review, and the vocabulary size to improve performance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Save LSTM model**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To save a Flax model checkpoint, use the [save_checkpoint] method. It expects:'
  prefs: []
  type: TYPE_NORMAL
- en: The directory to save the checkpoint files.
  prefs: []
  type: TYPE_NORMAL
- en: The Flax object to be saved, that is, [target].
  prefs: []
  type: TYPE_NORMAL
- en: The prefix of the checkpoint file name.
  prefs: []
  type: TYPE_NORMAL
- en: Whether to overwrite previous checkpoints
  prefs: []
  type: TYPE_NORMAL
- en: from flax.training import checkpoints
  prefs: []
  type: TYPE_NORMAL
- en: checkpoints.save_checkpoint(ckpt_dir='lstm_model_checkpoint/', target=trained_model_state,
  prefs: []
  type: TYPE_NORMAL
- en: step=100,
  prefs: []
  type: TYPE_NORMAL
- en: prefix='lstm_model',
  prefs: []
  type: TYPE_NORMAL
- en: overwrite=False
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: To restore the saved model, use [restore_checkpoint] method.
  prefs: []
  type: TYPE_NORMAL
- en: loaded_model = checkpoints.restore_checkpoint(
  prefs: []
  type: TYPE_NORMAL
- en: ckpt_dir='lstm_mod
  prefs: []
  type: TYPE_NORMAL
- en: el_checkpoint/',
  prefs: []
  type: TYPE_NORMAL
- en: target=state, prefix='lstm_mode
  prefs: []
  type: TYPE_NORMAL
- en: l'
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: loaded_model ![](../images/00044.gif)This model can be used to make predictions
    right away.![](../images/00045.gif)
  prefs: []
  type: TYPE_NORMAL
- en: '**Final thoughts**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You have learned to solve natural language processing problems with JAX and
    Flax in this article. In particular, the nuggets you have covered include:'
  prefs: []
  type: TYPE_NORMAL
- en: How to process text data with NLTK.
  prefs: []
  type: TYPE_NORMAL
- en: Text vectorization with Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Creating batches of text data with Keras and TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: How to create LSTM models in JAX and Flax. How to train and evaluate the LSTM
    model in Flax. Saving and restoring Flax LSTM models.
  prefs: []
  type: TYPE_NORMAL
- en: '**Flax vs. TensorFlow**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Flax is the neural network library for JAX. TensorFlow is a deep learning library
    with a large ecosystem of tools and resources. Flax and TensorFlow are similar
    but different in some ways. For instance, both Flax and TensorFlow can run on
    XLA.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at the differences between Flax and TensorFlow from my perspective
    as a user of both libraries.
  prefs: []
  type: TYPE_NORMAL
- en: '**Random number generation in TensorFlow and Flax**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In TensorFlow, you can set global or function level seeds. Generating random
    numbers in TensorFlow is quite straightforward.tf.random.set_seed(6853)
  prefs: []
  type: TYPE_NORMAL
- en: However, this is not the case in Flax. Flax is built on top of JAX. JAX expects
    pure functions, meaning functions without any side effects. To achieve this JAX
    introduces stateless pseudo-random number generators (PRNGs). For example, calling
    the random number generator from NumPy will result in a different number every
    time.
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  prefs: []
  type: TYPE_NORMAL
- en: print(np.random.random()) print(np.random.random()) print(np.random.random())
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00046.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In JAX and Flax, the result should be the same on every call. We, therefore,
    generate random numbers from a random state. The state should not be re-used.
    It can be split to obtain several pseudo-random numbers.
  prefs: []
  type: TYPE_NORMAL
- en: import jax
  prefs: []
  type: TYPE_NORMAL
- en: key = jax.random.PRNGKey(0)
  prefs: []
  type: TYPE_NORMAL
- en: key1, key2, key3 = jax.random.split(key, num=3)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00047.gif)'
  prefs: []
  type: TYPE_IMG
- en: '**Model definition in Flax and TensorFlow**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Model definition in TensorFlow is made easy by the Keras API. You can use Keras
    to define Sequential or Functional networks. Keras has many layers for designing
    various types of networks, such as CNNs, and LSTMS.
  prefs: []
  type: TYPE_NORMAL
- en: In Flax, networks are designed using the setup or compact way. The setup method
    is explicit, while the compact way is in-line. Setup is very similar to how networks
    are designed in PyTorch. For example, here is a network designed with the setup
    way.
  prefs: []
  type: TYPE_NORMAL
- en: 'class MLP(nn.Module):def setup(self):'
  prefs: []
  type: TYPE_NORMAL
- en: Submodule names are derived by the attributes you assign to. In this
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: case, "dense1" and "dense2". This follows the logic in Py Torch.
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: self.dense1 = nn.Dense(32)
  prefs: []
  type: TYPE_NORMAL
- en: self.dense2 = nn.Dense(32)
  prefs: []
  type: TYPE_NORMAL
- en: 'def __call__(self, x): x = self.dense1(x) x = nn.relu(x)'
  prefs: []
  type: TYPE_NORMAL
- en: x = self.dense2(x) return x
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the same network designed in a compact way. The compact way is more
    straightforward because there is less code duplicity.class MLP(nn.Module):'
  prefs: []
  type: TYPE_NORMAL
- en: '@nn.compact'
  prefs: []
  type: TYPE_NORMAL
- en: 'def __call__(self, x):'
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.Dense(32, name="dense1")(x)
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.relu(x)
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.Dense(32, name="dense2")(x)
  prefs: []
  type: TYPE_NORMAL
- en: return x
  prefs: []
  type: TYPE_NORMAL
- en: '**Activations in Flax and TensorFlow**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The [tf.keras.activations] module in TensorFlow provides most of the activations
    needed when designing networks.  In Flax, activation functions are available via
    the linen module.
  prefs: []
  type: TYPE_NORMAL
- en: '**Optimizers in Flax and TensorFlow**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The [tf.keras.optimizers] in TensorFlow has popular optimizer functions. However,
    Flax doesn't ship with any optimizer functions. Optimizers used in Flax are provided
    by another library known as Optax.
  prefs: []
  type: TYPE_NORMAL
- en: '**Metrics in Flax and TensorFlow**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In TensorFlow, metrics are available via
  prefs: []
  type: TYPE_NORMAL
- en: the [tf.keras.metrics] module. As of this writing, Flax has no metrics module.
     You'll need to define metric functions for your networks or use other third-party
    libraries.
  prefs: []
  type: TYPE_NORMAL
- en: import optax
  prefs: []
  type: TYPE_NORMAL
- en: import jax.numpy as jnp
  prefs: []
  type: TYPE_NORMAL
- en: 'def compute_metrics(logits, labels):'
  prefs: []
  type: TYPE_NORMAL
- en: loss = jnp.mean(optax.softmax_cross_entropy(logits, jax.nn.on e_hot(labels,
    num_classes=2)))
  prefs: []
  type: TYPE_NORMAL
- en: accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)
  prefs: []
  type: TYPE_NORMAL
- en: metrics = {
  prefs: []
  type: TYPE_NORMAL
- en: '''loss'': loss,'
  prefs: []
  type: TYPE_NORMAL
- en: '''accuracy'': accuracy'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: return metrics
  prefs: []
  type: TYPE_NORMAL
- en: '**Computing gradients in Flax and TensorFlow**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The [jax.grad] function is used to compute gradients in Flax. It offers the
    ability to return auxillary data. For example, you can return loss and gradients
    at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: '@jax.jitdef sum_logistic(x):return jnp.sum(1.0 / (1.0 + jnp.exp(-x))),(x +
    1)'
  prefs: []
  type: TYPE_NORMAL
- en: x_small = jnp.arange(6.)
  prefs: []
  type: TYPE_NORMAL
- en: derivative_fn = jax.grad(sum_logistic, has_aux=True) print(derivative_fn(x_small))
  prefs: []
  type: TYPE_NORMAL
- en: (DeviceArray([0.25 , 0.19661194, 0.10499357, 0.04517666, 0.01766271,
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 0.00664806], dtype=float32), DeviceArray([1., 2.,
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 3., 4., 5., 6.], dtype=float32))Advanced automatic differentiation can also
    be done
  prefs: []
  type: TYPE_NORMAL
- en: using jax.vjp() and jax.jvp().
  prefs: []
  type: TYPE_NORMAL
- en: In TensorFlow, gradients are computed using [tf.GradientTape].def grad(model,
    inputs, targets):with tf.GradientTape() as tape:loss_value = loss(model, inputs,
    targets, training=True) return loss_value, tape.gradient(loss_value, model.trainable_
    variables)
  prefs: []
  type: TYPE_NORMAL
- en: Unless you are creating custom training loops in TensorFlow, you will not define
    a gradient function. This is done automatically when you train the network.
  prefs: []
  type: TYPE_NORMAL
- en: '**Loading datasets in Flax and TensorFlow**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TensorFlow provides utilities for loading data. Flax doesn't ship with any data
    loaders. You have to use the data loaders from other libraries such as TensorFlow.
    As long as the data is in JAX NumPy or regular arrays and has the proper shape,
    it can be passed to Flax networks.
  prefs: []
  type: TYPE_NORMAL
- en: '**Training model in Flax vs. TensorFlow**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Training models in TensorFlow is done by compiling the network and calling the
    fit method. However, in Flax, we create a training state to hold the training
    information and then pass data to the network.
  prefs: []
  type: TYPE_NORMAL
- en: from flax.training import train_state
  prefs: []
  type: TYPE_NORMAL
- en: 'def create_train_state(rng):'
  prefs: []
  type: TYPE_NORMAL
- en: '"""Creates initial `TrainState`."""'
  prefs: []
  type: TYPE_NORMAL
- en: model = LSTMModel()
  prefs: []
  type: TYPE_NORMAL
- en: params = model.init(rng, jnp.array(X_train_padded[0]))['param
  prefs: []
  type: TYPE_NORMAL
- en: s']
  prefs: []
  type: TYPE_NORMAL
- en: tx = optax.adam(0.001,0.9,0.999,1e-07)
  prefs: []
  type: TYPE_NORMAL
- en: return train_state.TrainState.create(
  prefs: []
  type: TYPE_NORMAL
- en: apply_fn=model.apply, params=params, tx=tx)
  prefs: []
  type: TYPE_NORMAL
- en: After that, we define a training step that will compute the loss and gradients.
    It then uses these gradients to update the model parameters and returns the model
    metrics and the new state.
  prefs: []
  type: TYPE_NORMAL
- en: '@jax.jitdef train_step(state, text, labels):def loss_fn(params):'
  prefs: []
  type: TYPE_NORMAL
- en: 'logits = LSTMModel().apply({''params'': params}, text) loss = jnp.mean(optax.softmax_cross_entropy(
    logits=logits,'
  prefs: []
  type: TYPE_NORMAL
- en: labels=jax.nn.one_hot(labels, num_classes=2))) return loss, logits
  prefs: []
  type: TYPE_NORMAL
- en: grad_fn = jax.value_and_grad(loss_fn, has_aux=True) (_, logits), grads = grad_fn(state.params)
  prefs: []
  type: TYPE_NORMAL
- en: state = state.apply_gradients(grads=grads)
  prefs: []
  type: TYPE_NORMAL
- en: metrics = compute_metrics(logits, labels)
  prefs: []
  type: TYPE_NORMAL
- en: return state, metrics
  prefs: []
  type: TYPE_NORMAL
- en: Use Elegy to train networks like in Keras. Elegy is a high-level API for JAX
    neural network libraries.
  prefs: []
  type: TYPE_NORMAL
- en: '**Distributed training in Flax and TensorFlow**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Training networks in TensorFlow in a distributed manner is done by creating distributed
    strategy.mirrored_strategy = tf.distribute.MirroredStrategy()
  prefs: []
  type: TYPE_NORMAL
- en: 'with mirrored_strategy.scope():'
  prefs: []
  type: TYPE_NORMAL
- en: model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_s
  prefs: []
  type: TYPE_NORMAL
- en: hape=(1,))])
  prefs: []
  type: TYPE_NORMAL
- en: model.compile(loss='mse', optimizer='sgd')
  prefs: []
  type: TYPE_NORMAL
- en: To train networks in a distributed way in Flax, we define distributed versions
    of our Flax functions. This is done using the [pmap] function that executes a
    function on multiple devices. You'll then compute predictions from all devices
    and get the average
  prefs: []
  type: TYPE_NORMAL
- en: using [jax.lax.pmean()]. You also need to replicate the data on all the devices
    using [jax_utils.replicate] . To obtain metrics from the
  prefs: []
  type: TYPE_NORMAL
- en: device use jax_utils.unreplicate.
  prefs: []
  type: TYPE_NORMAL
- en: '**Working with TPU accelerators**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can use Flax and TensorFlow with TPU and GPU accelerators. To use Flax with
    TPUs on Colab, you'll need to set it up:jax.tools.colab_tpu.setup_tpu() jax.devices()![](../images/00048.gif)For
    TensorFlow, set up the TPU distributed strategy.cluster_resolver = tf.distribute.cluster_resolver.TPUClusterRes
    olver(tpu=tpu_address)
  prefs: []
  type: TYPE_NORMAL
- en: tf.config.experimental_connect_to_cluster(cluster_resolver) tf.tpu.experimental.initialize_tpu_system(cluster_resolver)
    tpu_strategy = tf.distribute.TPUStrategy(cluster_resolver)
  prefs: []
  type: TYPE_NORMAL
- en: '**Model evaluation**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TensorFlow provides the [evaluate] function for evaluating networks. Flax doesn't
    ship with such a function. You'll need to create a function that applies the model
    and returns the test metrics. Elegy provides Keras-like functions such as the evaluate method.
  prefs: []
  type: TYPE_NORMAL
- en: '@jax.jit'
  prefs: []
  type: TYPE_NORMAL
- en: 'def eval_step(state, text, labels):'
  prefs: []
  type: TYPE_NORMAL
- en: 'logits = LSTMModel().apply({''params'': state.params}, text)'
  prefs: []
  type: TYPE_NORMAL
- en: return compute_metrics(logits=logits, labels=labels)
  prefs: []
  type: TYPE_NORMAL
- en: 'def evaluate_model(state, text, test_lbls): """Evaluate on the validation set."""'
  prefs: []
  type: TYPE_NORMAL
- en: metrics = eval_step(state, text, test_lbls) metrics = jax.device_get(metrics)
  prefs: []
  type: TYPE_NORMAL
- en: 'metrics = jax.tree_map(lambda x: x.item(), metrics) return metrics'
  prefs: []
  type: TYPE_NORMAL
- en: '**Visualize model performance**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Model visualizing is similar in Flax and TensorFlow. Once you obtain the metrics,
    you can use a package such as Matplotlib to visualize the model's performance.
    You can also use TensorBoard in both Flax and TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: '**Final thoughts**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have seen the differences between the Flax and TensorFlow libraries. In
    particular, have seen the difference in model definition and training.
  prefs: []
  type: TYPE_NORMAL
- en: '**Train ResNet in Flax from scratch(Distributed ResNet training)**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Apart from designing custom CNN architectures, you can use architectures that
    have already been built. ResNet is one such popular architecture. In most cases,
    you'll achieve better performance by using such architectures. In this article,
    you will learn how to perform distributed training of a ResNet model in Flax.
  prefs: []
  type: TYPE_NORMAL
- en: '**Install Flax models**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The [flaxmodels] package provides pre-trained models for Jax and Flax, including:'
  prefs: []
  type: TYPE_NORMAL
- en: StyleGAN2
  prefs: []
  type: TYPE_NORMAL
- en: GPT2
  prefs: []
  type: TYPE_NORMAL
- en: VGG
  prefs: []
  type: TYPE_NORMAL
- en: ResNet
  prefs: []
  type: TYPE_NORMAL
- en: git clone https://github.com/matthias-wright/flaxmodels.git pip install -r flaxmodels/training/resnet/requirements.txt
  prefs: []
  type: TYPE_NORMAL
- en: In this project, we will train the model from scratch– meaning that we will
    not use the pre-trained weights. In a separate article, we have covered how to
    perform transfer learning with ResNet.
  prefs: []
  type: TYPE_NORMAL
- en: '**Perform standard imports**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With [flaxmodels] installed, let's import the standard libraries used in this
    article.
  prefs: []
  type: TYPE_NORMAL
- en: 'import wget # pip install wget'
  prefs: []
  type: TYPE_NORMAL
- en: import zipfile
  prefs: []
  type: TYPE_NORMAL
- en: import torch
  prefs: []
  type: TYPE_NORMAL
- en: from torch.utils.data import DataLoader import os
  prefs: []
  type: TYPE_NORMAL
- en: from PIL import Image
  prefs: []
  type: TYPE_NORMAL
- en: from torchvision import transforms from torch.utils.data import Dataset import
    numpy as np
  prefs: []
  type: TYPE_NORMAL
- en: import pandas as pd
  prefs: []
  type: TYPE_NORMAL
- en: import matplotlib.pyplot as plt
  prefs: []
  type: TYPE_NORMAL
- en: '%matplotlib inline'
  prefs: []
  type: TYPE_NORMAL
- en: ignore harmless warnings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: import warnings
  prefs: []
  type: TYPE_NORMAL
- en: warnings.filterwarnings("ignore") import jax
  prefs: []
  type: TYPE_NORMAL
- en: from jax import numpy as jnp
  prefs: []
  type: TYPE_NORMAL
- en: import flax
  prefs: []
  type: TYPE_NORMAL
- en: from flax import linen as nn
  prefs: []
  type: TYPE_NORMAL
- en: from flax.training import train_state import optax
  prefs: []
  type: TYPE_NORMAL
- en: import time
  prefs: []
  type: TYPE_NORMAL
- en: from tqdm.notebook import tqdm
  prefs: []
  type: TYPE_NORMAL
- en: import math
  prefs: []
  type: TYPE_NORMAL
- en: from flax import jax_utils
  prefs: []
  type: TYPE_NORMAL
- en: '**Download dataset**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will train the ResNet model to predict two classes from the cats and dogs dataset.
    Download and extract the cat and dog images.
  prefs: []
  type: TYPE_NORMAL
- en: wget.download("https://ml.machinelearningnuggets.com/train.zi p")
  prefs: []
  type: TYPE_NORMAL
- en: 'with zipfile.ZipFile(''train.zip'', ''r'') as zip_ref:'
  prefs: []
  type: TYPE_NORMAL
- en: zip_ref.extractall('.')
  prefs: []
  type: TYPE_NORMAL
- en: '**Loading dataset in Flax**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since JAX and Flax don't ship with any data loaders, we use data loading utilities
    from PyTorch or TensorFlow. When using PyTorch, we start by creating a dataset
    class.
  prefs: []
  type: TYPE_NORMAL
- en: 'class CatsDogsDataset(Dataset):'
  prefs: []
  type: TYPE_NORMAL
- en: def __init__(self, root_dir, annotation_file, transform=Non
  prefs: []
  type: TYPE_NORMAL
- en: 'e):'
  prefs: []
  type: TYPE_NORMAL
- en: self.root_dir = root_dir
  prefs: []
  type: TYPE_NORMAL
- en: self.annotations = pd.read_csv(annotation_file) self.transform = transform
  prefs: []
  type: TYPE_NORMAL
- en: def __len__(self):return len(self.annotations)
  prefs: []
  type: TYPE_NORMAL
- en: 'def __getitem__(self, index):'
  prefs: []
  type: TYPE_NORMAL
- en: img_id = self.annotations.iloc[index, 0]
  prefs: []
  type: TYPE_NORMAL
- en: img = Image.open(os.path.join(self.root_dir, img_id)).c
  prefs: []
  type: TYPE_NORMAL
- en: onvert("RGB")
  prefs: []
  type: TYPE_NORMAL
- en: y_label = torch.tensor(float(self.annotations.iloc[inde
  prefs: []
  type: TYPE_NORMAL
- en: x, 1]))
  prefs: []
  type: TYPE_NORMAL
- en: 'if self.transform is not None: img = self.transform(img)return (img, y_label)Next,
    create a Pandas DataFrame containing the image paths and labels.'
  prefs: []
  type: TYPE_NORMAL
- en: 'train_df = pd.DataFrame(columns=["img_path","label"]) train_df["img_path"]
    = os.listdir("train/") for idx, i in enumerate(os.listdir("train/")):'
  prefs: []
  type: TYPE_NORMAL
- en: 'if "cat" in i:'
  prefs: []
  type: TYPE_NORMAL
- en: train_df["label"][idx] = 0
  prefs: []
  type: TYPE_NORMAL
- en: 'if "dog" in i:'
  prefs: []
  type: TYPE_NORMAL
- en: train_df["label"][idx] = 1
  prefs: []
  type: TYPE_NORMAL
- en: train_df.to_csv (r'train_csv.csv', index = False, header=True)
  prefs: []
  type: TYPE_NORMAL
- en: '**Data transformation in Flax**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Define a function that will stack the data and return it as a NumPy array.
  prefs: []
  type: TYPE_NORMAL
- en: 'def custom_collate_fn(batch):'
  prefs: []
  type: TYPE_NORMAL
- en: transposed_data = list(zip(*batch)) labels = np.array(transposed_data[1]) imgs
    = np.stack(transposed_data[0]) return imgs, labels
  prefs: []
  type: TYPE_NORMAL
- en: Create a transformation for resizing the images. Next, apply the transformation
    to the dataset created earlier.size_image = 224
  prefs: []
  type: TYPE_NORMAL
- en: transform = transforms.Compose([
  prefs: []
  type: TYPE_NORMAL
- en: transforms.Resize((size_image,size_image)), np.array])
  prefs: []
  type: TYPE_NORMAL
- en: dataset = CatsDogsDataset("train","train_csv.csv",transform=tra nsform)
  prefs: []
  type: TYPE_NORMAL
- en: Split this dataset into a training and testing set and create data loaders for
    each set.batch_size = 32
  prefs: []
  type: TYPE_NORMAL
- en: train_set, validation_set = torch.utils.data.random_split(datas et,[20000,5000])
  prefs: []
  type: TYPE_NORMAL
- en: train_loader = DataLoader(dataset=train_set, collate_fn=custom_ collate_fn,shuffle=True,
    batch_size=batch_size)
  prefs: []
  type: TYPE_NORMAL
- en: validation_loader = DataLoader(dataset=validation_set,collate_f n=custom_collate_fn,
    shuffle=False, batch_size=batch_size)
  prefs: []
  type: TYPE_NORMAL
- en: '**Instantiate Flax ResNet model**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With the data in place, instantiate the Flax ResNet model using the [flaxmodels] package.
    The instantiation requires:'
  prefs: []
  type: TYPE_NORMAL
- en: The desired number of classes.
  prefs: []
  type: TYPE_NORMAL
- en: The type of output.
  prefs: []
  type: TYPE_NORMAL
- en: The data type.
  prefs: []
  type: TYPE_NORMAL
- en: Whether the model is pre-trained– in this case [False].import jax.numpy as jnp
    import flaxmodels as fm
  prefs: []
  type: TYPE_NORMAL
- en: num_classes = 2
  prefs: []
  type: TYPE_NORMAL
- en: dtype = jnp.float32 model = fm.ResNet50(output='log_softmax', pretrained=None,
    num_ classes=num_classes, dtype=dtype)
  prefs: []
  type: TYPE_NORMAL
- en: '**Compute metrics**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Define the metrics for evaluating the model during training. Let's start by
    creating the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: 'def cross_entropy_loss(*, logits, labels):'
  prefs: []
  type: TYPE_NORMAL
- en: labels_onehot = jax.nn.one_hot(labels, num_classes=num_classe
  prefs: []
  type: TYPE_NORMAL
- en: s)
  prefs: []
  type: TYPE_NORMAL
- en: return optax.softmax_cross_entropy(logits=logits, labels=labe
  prefs: []
  type: TYPE_NORMAL
- en: ls_onehot).mean()
  prefs: []
  type: TYPE_NORMAL
- en: Next, define a function that computes and returns the loss and accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'def compute_metrics(*, logits, labels):'
  prefs: []
  type: TYPE_NORMAL
- en: loss = cross_entropy_loss(logits=logits, labels=labels) accuracy = jnp.mean(jnp.argmax(logits,
    -1) == labels) metrics = {
  prefs: []
  type: TYPE_NORMAL
- en: '''loss'': loss,'
  prefs: []
  type: TYPE_NORMAL
- en: '''accuracy'': accuracy,'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: return metrics
  prefs: []
  type: TYPE_NORMAL
- en: '**Create Flax model training state**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Flax provides a training state for storing training information. The training
    state can be modified to add new information. In this case, we need to alter the
    training state to add the batch statistics since the ResNet model computes [batch_stats].
  prefs: []
  type: TYPE_NORMAL
- en: 'class TrainState(train_state.TrainState): batch_stats: flax.core.FrozenDict'
  prefs: []
  type: TYPE_NORMAL
- en: We need the model parameters and batch statistics to create the training state
    function. We can access these by initializing the model with the [train] as [False].
  prefs: []
  type: TYPE_NORMAL
- en: key = jax.random.PRNGKey(0)
  prefs: []
  type: TYPE_NORMAL
- en: variables = model.init(key, jnp.ones([1, size_image, size_imag e, 3]), train=False)
  prefs: []
  type: TYPE_NORMAL
- en: 'The  create method of TrainState expects the following parameters: The [apply_fn] –
    model apply function. The model parameters– [variables[''params'']].'
  prefs: []
  type: TYPE_NORMAL
- en: The optimizer, usually defined using Optax.
  prefs: []
  type: TYPE_NORMAL
- en: The batch statistics– variables['batch_stats'].
  prefs: []
  type: TYPE_NORMAL
- en: We apply [pmap] to this function to create a distributed version of the training
    state.  [pmap] compiles the function for execution on multiple devices such as
    multiple GPUs and TPUs.
  prefs: []
  type: TYPE_NORMAL
- en: import functools
  prefs: []
  type: TYPE_NORMAL
- en: '@functools.partial(jax.pmap)'
  prefs: []
  type: TYPE_NORMAL
- en: 'def create_train_state(rng):'
  prefs: []
  type: TYPE_NORMAL
- en: '"""Creates initial `TrainState`."""'
  prefs: []
  type: TYPE_NORMAL
- en: return TrainState.create(apply_fn = model.apply,params = vari ables['params'],tx
    = optax.adam(0.01,0.9),batch_stats = variabl es['batch_stats'])
  prefs: []
  type: TYPE_NORMAL
- en: '**Apply model function**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Next, define a parallel model training function. Pass an [axis_name] so you
    can use that to aggregate the metrics from all the devices. The function:'
  prefs: []
  type: TYPE_NORMAL
- en: Computes the loss.
  prefs: []
  type: TYPE_NORMAL
- en: Computes predictions from all devices by calculating the average of the probabilities
    using jax.lax.pmean() .
  prefs: []
  type: TYPE_NORMAL
- en: When applying the model, we also include the batch statistics and the random
    number for  [DropOut]. Since this is the training function, the [train] parameter
    is [True]. The [batch_stats] are also included when computing the gradients. The [update_model] function
    applies the computed gradients– updates the model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '@functools.partial(jax.pmap, axis_name=''ensemble'')def apply_model(state,
    images, labels):def loss_fn(params,batch_stats):'
  prefs: []
  type: TYPE_NORMAL
- en: 'logits,batch_stats = model.apply({''params'': params,''batch_s tats'': batch_stats},images,
    train=True,rngs={''dropout'': jax.ran dom.PRNGKey(0)}, mutable=[''batch_stats''])'
  prefs: []
  type: TYPE_NORMAL
- en: one_hot = jax.nn.one_hot(labels, num_classes)loss = optax.softmax_cross_entropy(logits=logits,
    labels=on e_hot).mean()return loss, (logits, batch_stats)(loss, (logits, batch_stats)),
    grads = jax.value_and_grad(los s_fn, has_aux=True)(state.params,state.batch_stats)
    probs = jax.lax.pmean(jax.nn.softmax(logits), axis_name='ense mble')accuracy =
    jnp.mean(jnp.argmax(probs, -1) == labels) return grads,loss, accuracy@jax.pmapdef
    update_model(state, grads):return state.apply_gradients(grads=grads)
  prefs: []
  type: TYPE_NORMAL
- en: '**TensorBoard in Flax**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next step is to train the ResNet model. However, you might be interested
    in tracking the training using TensorBoard. In that case, you have to configure
    TensorBoard. You can write the metrics to TensorBoard using the PyTorch SummaryWriter.
  prefs: []
  type: TYPE_NORMAL
- en: rm -rf ./flax_logs/
  prefs: []
  type: TYPE_NORMAL
- en: from torch.utils.tensorboard import SummaryWriter import torchvision.transforms.functional
    as F logdir = "flax_logs"
  prefs: []
  type: TYPE_NORMAL
- en: writer = SummaryWriter(logdir)
  prefs: []
  type: TYPE_NORMAL
- en: '**Train Flax ResNet model**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's train the ResNet model on the entire training set and evaluate it on a
    subset of the test set. You can also evaluate it on the whole test set. Replicate
    the test set to the available devices.
  prefs: []
  type: TYPE_NORMAL
- en: (test_images, test_labels) = next(iter(validation_loader)) test_images = test_images
    / 255.0
  prefs: []
  type: TYPE_NORMAL
- en: test_images = np.array(jax_utils.replicate(test_images)) test_labels = np.array(jax_utils.replicate(test_labels))
  prefs: []
  type: TYPE_NORMAL
- en: Create some lists to hold the training and evaluation metrics.
  prefs: []
  type: TYPE_NORMAL
- en: epoch_loss = []
  prefs: []
  type: TYPE_NORMAL
- en: epoch_accuracy = [] testing_accuracy = [] testing_loss = []
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, define the ResNet model training function. The function does the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Loops through the training dataset and scales it.
  prefs: []
  type: TYPE_NORMAL
- en: Replicates the data on the available devices.
  prefs: []
  type: TYPE_NORMAL
- en: Applies the model on the dataset and computes the metrics. Obtains the metrics
    from the devices
  prefs: []
  type: TYPE_NORMAL
- en: using jax_utils.unreplicate.
  prefs: []
  type: TYPE_NORMAL
- en: Appends the metrics to a list.
  prefs: []
  type: TYPE_NORMAL
- en: Computes the mean of the loss and accuracy to obtain the metrics for each epoch.
  prefs: []
  type: TYPE_NORMAL
- en: Applies the model to the test set and obtains the metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Append the test metrics to a list.
  prefs: []
  type: TYPE_NORMAL
- en: Writes the training and evaluation metrics to TensorBaord.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prints the training and evaluation metrics.def train_one_epoch(state, dataloader,num_epochs):
    """Train for 1 epoch on the training set."""for epoch in range(num_epochs):for
    cnt, (images, labels) in tqdm(enumerate(dataloade r), total=(math.ceil(len(train_set)/batch_size))):'
  prefs: []
  type: TYPE_NORMAL
- en: images = images / 255.0
  prefs: []
  type: TYPE_NORMAL
- en: images = jax_utils.replicate(images)
  prefs: []
  type: TYPE_NORMAL
- en: labels = jax_utils.replicate(labels)
  prefs: []
  type: TYPE_NORMAL
- en: grads, loss, accuracy = apply_model(state, images,
  prefs: []
  type: TYPE_NORMAL
- en: labels)state = update_model(state, grads)
  prefs: []
  type: TYPE_NORMAL
- en: epoch_loss.append(jax_utils.unreplicate(loss)) epoch_accuracy.append(jax_utils.unreplicate(accuracy))
    train_loss = np.mean(epoch_loss)
  prefs: []
  type: TYPE_NORMAL
- en: train_accuracy = np.mean(epoch_accuracy)
  prefs: []
  type: TYPE_NORMAL
- en: _, test_loss, test_accuracy = jax_utils.unreplicate(app ly_model(state, test_images,
    test_labels))
  prefs: []
  type: TYPE_NORMAL
- en: testing_accuracy.append(test_accuracy)
  prefs: []
  type: TYPE_NORMAL
- en: testing_loss.append(test_loss)
  prefs: []
  type: TYPE_NORMAL
- en: writer.add_scalar('Loss/train', np.array(train_loss), e poch)
  prefs: []
  type: TYPE_NORMAL
- en: writer.add_scalar('Loss/test', np.array(test_loss), epo ch)
  prefs: []
  type: TYPE_NORMAL
- en: writer.add_scalar('Accuracy/train', np.array(train_accu racy), epoch)
  prefs: []
  type: TYPE_NORMAL
- en: writer.add_scalar('Accuracy/test', np.array(test_accura cy), epoch)
  prefs: []
  type: TYPE_NORMAL
- en: 'print(f"Epoch: {epoch + 1}, train loss: {train_loss:.4 f}, train accuracy:
    {train_accuracy * 100:.4f}, test loss: {tes t_loss:.4f}, test accuracy: {test_accuracy*
    100:.4f}", flush=Tr ue)'
  prefs: []
  type: TYPE_NORMAL
- en: return state, epoch_loss, epoch_accuracy, testing_accuracy, testing_lossCreate
    a training state by generating random numbers equivalent to the number of devices.
  prefs: []
  type: TYPE_NORMAL
- en: seed = 0
  prefs: []
  type: TYPE_NORMAL
- en: rng = jax.random.PRNGKey(seed)
  prefs: []
  type: TYPE_NORMAL
- en: rng, init_rng = jax.random.split(rng)
  prefs: []
  type: TYPE_NORMAL
- en: state = create_train_state(jax.random.split(init_rng, jax.devic e_count()))
  prefs: []
  type: TYPE_NORMAL
- en: 'del init_rng # Must not be used anymore.'
  prefs: []
  type: TYPE_NORMAL
- en: Train the ResNet model by passing the training data and the number of epochs.
  prefs: []
  type: TYPE_NORMAL
- en: start = time.time()
  prefs: []
  type: TYPE_NORMAL
- en: num_epochs = 30
  prefs: []
  type: TYPE_NORMAL
- en: 'state, epoch_loss, epoch_accuracy, testing_accuracy, testing_lo ss = train_one_epoch(state,
    train_loader,num_epochs) print("Total time: ", time.time() - start, "seconds")'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00049.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '**Evaluate model with TensorBoard**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Run TensorBoard to see the logged scalars on TensorBoard.%load_ext tensorboard%tensorboard
    --logdir={logdir}![](../images/00050.jpeg)![](../images/00051.jpeg)
  prefs: []
  type: TYPE_NORMAL
- en: '**Visualize Flax model performance**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The metrics that were stored in a list can be plotted using Matplotlib.
  prefs: []
  type: TYPE_NORMAL
- en: plt.plot(epoch_accuracy, label="Training")
  prefs: []
  type: TYPE_NORMAL
- en: plt.plot(testing_accuracy, label="Test")
  prefs: []
  type: TYPE_NORMAL
- en: plt.xlabel("Epoch")
  prefs: []
  type: TYPE_NORMAL
- en: plt.ylabel("Accuracy")
  prefs: []
  type: TYPE_NORMAL
- en: plt.legend()
  prefs: []
  type: TYPE_NORMAL
- en: plt.show()
  prefs: []
  type: TYPE_NORMAL
- en: plt.plot(epoch_loss, label="Training")
  prefs: []
  type: TYPE_NORMAL
- en: plt.plot(testing_loss, label="Test")
  prefs: []
  type: TYPE_NORMAL
- en: plt.xlabel("Epoch")
  prefs: []
  type: TYPE_NORMAL
- en: plt.ylabel("Accuracy")
  prefs: []
  type: TYPE_NORMAL
- en: plt.legend()
  prefs: []
  type: TYPE_NORMAL
- en: plt.show()
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00052.jpeg)![](../images/00053.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '**Save Flax ResNet model**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To save the trained Flax ResNet model use
  prefs: []
  type: TYPE_NORMAL
- en: 'the save_checkpoint function. The function expects:'
  prefs: []
  type: TYPE_NORMAL
- en: The folder where the ResNet model will be saved.
  prefs: []
  type: TYPE_NORMAL
- en: The model to be saved– [target]. The step – training step number.
  prefs: []
  type: TYPE_NORMAL
- en: The model prefix. Whether to overwrite existing models.
  prefs: []
  type: TYPE_NORMAL
- en: '!pip install tensorstore'
  prefs: []
  type: TYPE_NORMAL
- en: from flax.training import checkpoints
  prefs: []
  type: TYPE_NORMAL
- en: ckpt_dir = 'model_checkpoint/'
  prefs: []
  type: TYPE_NORMAL
- en: checkpoints.save_checkpoint(ckpt_dir=ckpt_dir,
  prefs: []
  type: TYPE_NORMAL
- en: target=state, step=100,
  prefs: []
  type: TYPE_NORMAL
- en: prefix='flax_model', overwrite=True
  prefs: []
  type: TYPE_NORMAL
- en: )![](../images/00054.jpeg)
  prefs: []
  type: TYPE_NORMAL
- en: '**Load Flax RestNet model**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The saved ResNet Flax model can also be loaded to make predictions. Flax models
    are loaded using the [restore_checkpoint] function. The function expects:'
  prefs: []
  type: TYPE_NORMAL
- en: The target state.
  prefs: []
  type: TYPE_NORMAL
- en: The folder containing the saved model.
  prefs: []
  type: TYPE_NORMAL
- en: The model's prefix.
  prefs: []
  type: TYPE_NORMAL
- en: loaded_model = checkpoints.restore_checkpoint(
  prefs: []
  type: TYPE_NORMAL
- en: ckpt_dir=ckpt_dir, target=state, prefix='flax_mode
  prefs: []
  type: TYPE_NORMAL
- en: l') ![](../images/00055.gif)
  prefs: []
  type: TYPE_NORMAL
- en: '**Final thoughts**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this article, you have learned how to train a ResNet model from scratch
    in Flax. In particular, you have covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a ResNet model in Flax.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the training state for the ResNet Flax model.
  prefs: []
  type: TYPE_NORMAL
- en: Training the Flax ResNet model in a distributed manner. Track the performance
    of the Flax ResNet model with TensorBoard.
  prefs: []
  type: TYPE_NORMAL
- en: Saving and loading the Flax ResNet model.
  prefs: []
  type: TYPE_NORMAL
- en: '**Transfer learning with JAX & Flax**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Training large neural networks can take days or weeks. Once these networks are
    trained, you can take advantage of their weights and apply them to new tasks– **transfer
    learning**. As a result, you **finetune** a new network and get good results in
    a short period. Let's look at how you can fine-tune a pre-trained ResNet network
    in JAX and Flax.
  prefs: []
  type: TYPE_NORMAL
- en: '**Install JAX ResNet**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We'll use ResNet checkpoints provided by the jax-resnet library.pip install
    jax-resnetLet's import it together with other packages used in this article.
  prefs: []
  type: TYPE_NORMAL
- en: pip install flax
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: import numpy as np
  prefs: []
  type: TYPE_NORMAL
- en: import pandas as pd
  prefs: []
  type: TYPE_NORMAL
- en: from PIL import Image
  prefs: []
  type: TYPE_NORMAL
- en: import jax
  prefs: []
  type: TYPE_NORMAL
- en: import optax
  prefs: []
  type: TYPE_NORMAL
- en: import flax
  prefs: []
  type: TYPE_NORMAL
- en: import jax.numpy as jnp
  prefs: []
  type: TYPE_NORMAL
- en: from jax_resnet import pretrained_resnet, slice_variables, Sequ ential
  prefs: []
  type: TYPE_NORMAL
- en: from flax.training import train_state from flax import linen as nn
  prefs: []
  type: TYPE_NORMAL
- en: from flax.core import FrozenDict,frozen_dict from functools import partial
  prefs: []
  type: TYPE_NORMAL
- en: import os
  prefs: []
  type: TYPE_NORMAL
- en: import torch
  prefs: []
  type: TYPE_NORMAL
- en: from torch.utils.data import DataLoader from torchvision import transforms
  prefs: []
  type: TYPE_NORMAL
- en: from torch.utils.data import Dataset import matplotlib.pyplot as plt
  prefs: []
  type: TYPE_NORMAL
- en: '%matplotlib inline'
  prefs: []
  type: TYPE_NORMAL
- en: ignore harmless warnings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: import warnings
  prefs: []
  type: TYPE_NORMAL
- en: warnings.filterwarnings("ignore")
  prefs: []
  type: TYPE_NORMAL
- en: '**Download dataset**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will fine-tune the ResNet model to predict two classes from the cats and
    dogs dataset. Download and extract the cat and dog images.
  prefs: []
  type: TYPE_NORMAL
- en: pip install wget
  prefs: []
  type: TYPE_NORMAL
- en: import wget
  prefs: []
  type: TYPE_NORMAL
- en: wget.download("https://ml.machinelearningnuggets.com/train.zi p")
  prefs: []
  type: TYPE_NORMAL
- en: import zipfile
  prefs: []
  type: TYPE_NORMAL
- en: 'with zipfile.ZipFile(''train.zip'', ''r'') as zip_ref:'
  prefs: []
  type: TYPE_NORMAL
- en: zip_ref.extractall('.')
  prefs: []
  type: TYPE_NORMAL
- en: '**Data loading in JAX**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: JAX doesn't ship with data loading utilities. We use existing data loaders in TensorFlow and
    PyTorch to load the data. Let's use PyTorch to load the image data.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to create a PyTorch [Dataset] class.
  prefs: []
  type: TYPE_NORMAL
- en: 'class CatsDogsDataset(Dataset):'
  prefs: []
  type: TYPE_NORMAL
- en: def __init__(self, root_dir, annotation_file, transform=Non
  prefs: []
  type: TYPE_NORMAL
- en: 'e):'
  prefs: []
  type: TYPE_NORMAL
- en: self.root_dir = root_dir
  prefs: []
  type: TYPE_NORMAL
- en: self.annotations = pd.read_csv(annotation_file) self.transform = transform
  prefs: []
  type: TYPE_NORMAL
- en: def __len__(self):return len(self.annotations)
  prefs: []
  type: TYPE_NORMAL
- en: 'def __getitem__(self, index):'
  prefs: []
  type: TYPE_NORMAL
- en: img_id = self.annotations.iloc[index, 0]
  prefs: []
  type: TYPE_NORMAL
- en: img = Image.open(os.path.join(self.root_dir, img_id)).c
  prefs: []
  type: TYPE_NORMAL
- en: onvert("RGB")
  prefs: []
  type: TYPE_NORMAL
- en: y_label = torch.tensor(float(self.annotations.iloc[inde
  prefs: []
  type: TYPE_NORMAL
- en: x, 1]))
  prefs: []
  type: TYPE_NORMAL
- en: 'if self.transform is not None: img = self.transform(img) return (img, y_label)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data processing**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, create a Pandas DataFrame with the image paths and labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'train_df = pd.DataFrame(columns=["img_path","label"]) train_df["img_path"]
    = os.listdir("train/") for idx, i in enumerate(os.listdir("train/")):'
  prefs: []
  type: TYPE_NORMAL
- en: if "cat" in i:train_df["label"][idx] = 0if "dog" in i:train_df["label"][idx]
    = 1train_df.to_csv (r'train_csv.csv', index = False, header=True)![](../images/00056.jpeg)Define
    a function to stack the data and return the images and labels as a NumPy array.
  prefs: []
  type: TYPE_NORMAL
- en: 'def custom_collate_fn(batch):'
  prefs: []
  type: TYPE_NORMAL
- en: transposed_data = list(zip(*batch))
  prefs: []
  type: TYPE_NORMAL
- en: labels = np.array(transposed_data[1])
  prefs: []
  type: TYPE_NORMAL
- en: imgs = np.stack(transposed_data[0])
  prefs: []
  type: TYPE_NORMAL
- en: return imgs, labels
  prefs: []
  type: TYPE_NORMAL
- en: Let's also resize the images to ensure they are the same size. Define the size
    in a configuration dictionary. We'll use the other config variables later.
  prefs: []
  type: TYPE_NORMAL
- en: config = {
  prefs: []
  type: TYPE_NORMAL
- en: '''NUM_LABELS'': 2,'
  prefs: []
  type: TYPE_NORMAL
- en: '''BATCH_SIZE'': 32,'
  prefs: []
  type: TYPE_NORMAL
- en: '''N_EPOCHS'': 5,'
  prefs: []
  type: TYPE_NORMAL
- en: '''LR'': 0.001,'
  prefs: []
  type: TYPE_NORMAL
- en: '''IMAGE_SIZE'': 224,'
  prefs: []
  type: TYPE_NORMAL
- en: '''WEIGHT_DECAY'': 1e-5, ''FREEZE_BACKBONE'': True,'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: Resize the images using PyTorch transforms. Next, use the CatsDogsDataset class
    to define the training and testing data loaders.
  prefs: []
  type: TYPE_NORMAL
- en: transform = transforms.Compose([
  prefs: []
  type: TYPE_NORMAL
- en: transforms.Resize((config["IMAGE_SIZE"],config["IMAGE_SIZ
  prefs: []
  type: TYPE_NORMAL
- en: E"])),
  prefs: []
  type: TYPE_NORMAL
- en: np.array])
  prefs: []
  type: TYPE_NORMAL
- en: dataset = CatsDogsDataset("train","train_csv.csv",transform=tra
  prefs: []
  type: TYPE_NORMAL
- en: nsform)
  prefs: []
  type: TYPE_NORMAL
- en: train_set, validation_set = torch.utils.data.random_split(datas
  prefs: []
  type: TYPE_NORMAL
- en: et,[20000,5000])
  prefs: []
  type: TYPE_NORMAL
- en: train_loader = DataLoader(dataset=train_set, collate_fn=custom_
  prefs: []
  type: TYPE_NORMAL
- en: collate_fn,shuffle=True, batch_size=config["BATCH_SIZE"])
  prefs: []
  type: TYPE_NORMAL
- en: validation_loader = DataLoader(dataset=validation_set,collate_f
  prefs: []
  type: TYPE_NORMAL
- en: n=custom_collate_fn, shuffle=False, batch_size=config["BATCH_SI
  prefs: []
  type: TYPE_NORMAL
- en: ZE"])
  prefs: []
  type: TYPE_NORMAL
- en: '**ResNet model definition**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pre-trained ResNet models are trained on many classes. However, the dataset
    we have has two classes. We, therefore, use the ResNet as the backbone and define
    a custom classification layer.
  prefs: []
  type: TYPE_NORMAL
- en: '**Create head network**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Create a head network with output as per the problem, in this case, a binary
    image classification.
  prefs: []
  type: TYPE_NORMAL
- en: '"""'
  prefs: []
  type: TYPE_NORMAL
- en: reference - https://www.kaggle.com/code/alexlwh/happywhale-flax
  prefs: []
  type: TYPE_NORMAL
- en: -jax-tpu-gpu-resnet-baseline
  prefs: []
  type: TYPE_NORMAL
- en: '"""'
  prefs: []
  type: TYPE_NORMAL
- en: 'class Head(nn.Module):'
  prefs: []
  type: TYPE_NORMAL
- en: '''''''head model''''''batch_norm_cls: partial = partial(nn.BatchNorm, momentum=0.'
  prefs: []
  type: TYPE_NORMAL
- en: 9)
  prefs: []
  type: TYPE_NORMAL
- en: '@nn.compact'
  prefs: []
  type: TYPE_NORMAL
- en: 'def __call__(self, inputs, train: bool):'
  prefs: []
  type: TYPE_NORMAL
- en: output_n = inputs.shape[-1]x = self.batch_norm_cls(use_running_average=not train)
  prefs: []
  type: TYPE_NORMAL
- en: (inputs)
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.Dropout(rate=0.25)(x, deterministic=not train) x = nn.Dense(features=output_n)(x)
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.relu(x)
  prefs: []
  type: TYPE_NORMAL
- en: x = self.batch_norm_cls(use_running_average=not train)
  prefs: []
  type: TYPE_NORMAL
- en: (x)
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.Dropout(rate=0.5)(x, deterministic=not train) x = nn.Dense(features=config["NUM_LABELS"])(x)
    return x
  prefs: []
  type: TYPE_NORMAL
- en: '**Combine ResNet backbone with head**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Combine the pre-trained ResNet backbone with the custom head you created above.
  prefs: []
  type: TYPE_NORMAL
- en: 'class Model(nn.Module):'
  prefs: []
  type: TYPE_NORMAL
- en: '''''''Combines backbone and head model'''''' backbone: Sequential'
  prefs: []
  type: TYPE_NORMAL
- en: 'head: Head'
  prefs: []
  type: TYPE_NORMAL
- en: 'def __call__(self, inputs, train: bool): x = self.backbone(inputs)'
  prefs: []
  type: TYPE_NORMAL
- en: average pool layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: x = jnp.mean(x, axis=(1, 2)) x = self.head(x, train)
  prefs: []
  type: TYPE_NORMAL
- en: return x
  prefs: []
  type: TYPE_NORMAL
- en: '**Load pre-trained ResNet 50**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, create a function that loads the pre-trained ResNet model. Omit the last
    two layers of the network because we have defined a custom head. The function
    returns the ResNet model and its parameters. The model parameters are obtained
    using the [slice_variables] function.
  prefs: []
  type: TYPE_NORMAL
- en: 'def get_backbone_and_params(model_arch: str):'
  prefs: []
  type: TYPE_NORMAL
- en: ''''''''
  prefs: []
  type: TYPE_NORMAL
- en: Get backbone and params
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Loads pretrained model (resnet50)
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Get model and param structure except last 2 layers 3\. Extract the corresponding
    subset of the variables dict INPUT : model_arch'
  prefs: []
  type: TYPE_NORMAL
- en: RETURNS backbone , backbone_params
  prefs: []
  type: TYPE_NORMAL
- en: ''''''''
  prefs: []
  type: TYPE_NORMAL
- en: 'if model_arch == ''resnet50'':'
  prefs: []
  type: TYPE_NORMAL
- en: resnet_tmpl, params = pretrained_resnet(50) model = resnet_tmpl()else:raise
    NotImplementedError
  prefs: []
  type: TYPE_NORMAL
- en: get model & param structure for backbone start, end = 0, len(model.layers) -
    2 backbone = Sequential(model.layers[start:end]) backbone_params = slice_variables(params,
    start, end) return backbone, backbone_params
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Get model and variables**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use the above function to create the final model. Define a function that:'
  prefs: []
  type: TYPE_NORMAL
- en: Initializes the network's input.
  prefs: []
  type: TYPE_NORMAL
- en: Obtains the ResNet backbone and its parameters. Passes the input to the backbone
    and gets the output.
  prefs: []
  type: TYPE_NORMAL
- en: Initializes the network's head. Creates the final model using backbone and head.
  prefs: []
  type: TYPE_NORMAL
- en: Combines the parameters from backbone and head.
  prefs: []
  type: TYPE_NORMAL
- en: 'def get_model_and_variables(model_arch: str, head_init_key: in t):'
  prefs: []
  type: TYPE_NORMAL
- en: ''''''''
  prefs: []
  type: TYPE_NORMAL
- en: Get model and variables
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Initialise inputs(shape=(1,image_size,image_size,3))
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Get backbone and params
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Apply backbone model and get outputs
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Initialise head
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Create final model using backbone and head
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Combine params from backbone and head
  prefs: []
  type: TYPE_NORMAL
- en: INPUT model_arch, head_init_key RETURNS model, variables '''
  prefs: []
  type: TYPE_NORMAL
- en: '#backboneinputs = jnp.ones((1, config[''IMAGE_SIZE''],config[''IMAGE_SI ZE''],
    3), jnp.float32)backbone, backbone_params = get_backbone_and_params(model_a rch)
    key = jax.random.PRNGKey(head_init_key)backbone_output = backbone.apply(backbone_params,
    inputs, m utable=False)#headhead_inputs = jnp.ones((1, backbone_output.shape[-1]),
    jnp. float32)head = Head()head_params = head.init(key, head_inputs, train=False)'
  prefs: []
  type: TYPE_NORMAL
- en: '#final model'
  prefs: []
  type: TYPE_NORMAL
- en: model = Model(backbone, head)
  prefs: []
  type: TYPE_NORMAL
- en: variables = FrozenDict({
  prefs: []
  type: TYPE_NORMAL
- en: '''params'': {'
  prefs: []
  type: TYPE_NORMAL
- en: '''backbone'': backbone_params[''params''], ''head'': head_params[''params'']'
  prefs: []
  type: TYPE_NORMAL
- en: '},'
  prefs: []
  type: TYPE_NORMAL
- en: '''batch_stats'': {'
  prefs: []
  type: TYPE_NORMAL
- en: '''backbone'': backbone_params[''batch_stats''], ''head'': head_params[''batch_stats'']'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  prefs: []
  type: TYPE_NORMAL
- en: return model, variables
  prefs: []
  type: TYPE_NORMAL
- en: All names relating to the backbone network are prefixed with the name [backbone].
    You can use any name, but all backbone variable names should be the same. This
    is important when freezing layers, as we'll see later.
  prefs: []
  type: TYPE_NORMAL
- en: Next, use the function defined above to create the model.model, variables =
    get_model_and_variables('resnet50', 0) ![](../images/00057.jpeg)
  prefs: []
  type: TYPE_NORMAL
- en: '**Zero gradients**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since we are applying transfer learning, we need to ensure that the backbone
    is not updated. Otherwise, we'll be training the network from scratch. We want
    to take advantage of the pre-trained weights and use them as a **feature extractor** for
    the network. To achieve this, we freeze the parameters of all layers whose name
    starts
  prefs: []
  type: TYPE_NORMAL
- en: with [backbone]. As a result, these parameters will not be updated during training.
  prefs: []
  type: TYPE_NORMAL
- en: '"""'
  prefs: []
  type: TYPE_NORMAL
- en: reference - https://github.com/deepmind/optax/issues/159#issuec omment-896459491
  prefs: []
  type: TYPE_NORMAL
- en: '"""'
  prefs: []
  type: TYPE_NORMAL
- en: 'def zero_grads():'
  prefs: []
  type: TYPE_NORMAL
- en: ''''''''
  prefs: []
  type: TYPE_NORMAL
- en: Zero out the previous gradient computation
  prefs: []
  type: TYPE_NORMAL
- en: ''''''''
  prefs: []
  type: TYPE_NORMAL
- en: 'def init_fn(_):'
  prefs: []
  type: TYPE_NORMAL
- en: return ()
  prefs: []
  type: TYPE_NORMAL
- en: 'def update_fn(updates, state, params=None):'
  prefs: []
  type: TYPE_NORMAL
- en: return jax.tree_map(jnp.zeros_like, updates), () return optax.GradientTransformation(init_fn,
    update_fn)
  prefs: []
  type: TYPE_NORMAL
- en: '"""'
  prefs: []
  type: TYPE_NORMAL
- en: reference - https://colab.research.google.com/drive/1g_pt2Rc3bv 6H6qchvGHD-BpgF-Pt4vrC#scrollTo=TqDvTL_tIQCH&line=2&uniqifier=1
    """
  prefs: []
  type: TYPE_NORMAL
- en: 'def create_mask(params, label_fn):'
  prefs: []
  type: TYPE_NORMAL
- en: 'def _map(params, mask, label_fn):for k in params:if label_fn(k):'
  prefs: []
  type: TYPE_NORMAL
- en: mask[k] = 'zero'
  prefs: []
  type: TYPE_NORMAL
- en: 'else:'
  prefs: []
  type: TYPE_NORMAL
- en: 'if isinstance(params[k], FrozenDict): mask[k] = {}'
  prefs: []
  type: TYPE_NORMAL
- en: _map(params[k], mask[k], label_fn)
  prefs: []
  type: TYPE_NORMAL
- en: 'else:'
  prefs: []
  type: TYPE_NORMAL
- en: mask[k] = 'adam'
  prefs: []
  type: TYPE_NORMAL
- en: mask = {}
  prefs: []
  type: TYPE_NORMAL
- en: _map(params, mask, label_fn)
  prefs: []
  type: TYPE_NORMAL
- en: return frozen_dict.freeze(mask)
  prefs: []
  type: TYPE_NORMAL
- en: '**Define Flax optimizer**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Create an optimizer that will only be applied to the head and not backbone layers.
    This is done using the optax.multi_transform while passing the desired transformations.
  prefs: []
  type: TYPE_NORMAL
- en: adamw = optax.adamw(
  prefs: []
  type: TYPE_NORMAL
- en: learning_rate=config['LR'],
  prefs: []
  type: TYPE_NORMAL
- en: b1=0.9, b2=0.999,
  prefs: []
  type: TYPE_NORMAL
- en: eps=1e-6, weight_decay=1e-2
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: optimizer = optax.multi_transform(
  prefs: []
  type: TYPE_NORMAL
- en: '{''adam'': adamw, ''zero'': zero_grads()},'
  prefs: []
  type: TYPE_NORMAL
- en: 'create_mask(variables[''params''], lambda s: s.startswith(''ba ckbone''))'
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: '**Define Flax loss function**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, define the function to compute the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: 'def cross_entropy_loss(*, logits, labels):'
  prefs: []
  type: TYPE_NORMAL
- en: labels_onehot = jax.nn.one_hot(labels, num_classes=config["NU
  prefs: []
  type: TYPE_NORMAL
- en: M_LABELS"])
  prefs: []
  type: TYPE_NORMAL
- en: return optax.softmax_cross_entropy(logits=logits, labels=labe
  prefs: []
  type: TYPE_NORMAL
- en: ls_onehot).mean()
  prefs: []
  type: TYPE_NORMAL
- en: 'When computing the loss during training, set [train] to [True]. You also have
    to:'
  prefs: []
  type: TYPE_NORMAL
- en: Set the batch_statsDefine the random number for the [dropout] layers. Set the [batch_stats] as
    mutable.
  prefs: []
  type: TYPE_NORMAL
- en: 'def compute_loss(params, batch_stats, images, labels): logits,batch_stats =
    model.apply({''params'': params,''batch_s'
  prefs: []
  type: TYPE_NORMAL
- en: 'tats'': batch_stats},images, train=True,rngs={''dropout'': jax.ran'
  prefs: []
  type: TYPE_NORMAL
- en: dom.PRNGKey(0)}, mutable=['batch_stats'])
  prefs: []
  type: TYPE_NORMAL
- en: loss = cross_entropy_loss(logits=logits, labels=labels) return loss, (logits,
    batch_stats)
  prefs: []
  type: TYPE_NORMAL
- en: '**Compute Flax metrics**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using the loss function, define a function that will return the loss and accuracy
    during training.
  prefs: []
  type: TYPE_NORMAL
- en: 'def compute_metrics(*, logits, labels):'
  prefs: []
  type: TYPE_NORMAL
- en: loss = cross_entropy_loss(logits=logits, labels=labels) accuracy = jnp.mean(jnp.argmax(logits,
    -1) == labels) metrics = {
  prefs: []
  type: TYPE_NORMAL
- en: '''loss'': loss,'
  prefs: []
  type: TYPE_NORMAL
- en: '''accuracy'': accuracy,'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: return metrics
  prefs: []
  type: TYPE_NORMAL
- en: '**Create Flax training state**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Flax provides a training state for storing training information. In this case,
    we add the [batch_stats] information.class TrainState(train_state.TrainState):
    batch_stats: FrozenDict'
  prefs: []
  type: TYPE_NORMAL
- en: state = TrainState.create(
  prefs: []
  type: TYPE_NORMAL
- en: apply_fn = model.apply,
  prefs: []
  type: TYPE_NORMAL
- en: params = variables['params'],
  prefs: []
  type: TYPE_NORMAL
- en: tx = optimizer,
  prefs: []
  type: TYPE_NORMAL
- en: batch_stats = variables['batch_stats'],
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: '**Training step**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The training step receives the images and labels and computes the gradient with
    respect to the model parameters. It then returns the new state and the model metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '@jax.jit'
  prefs: []
  type: TYPE_NORMAL
- en: 'def train_step(state: TrainState,images, labels):'
  prefs: []
  type: TYPE_NORMAL
- en: '"""Train for a single step."""'
  prefs: []
  type: TYPE_NORMAL
- en: (batch_loss, (logits, batch_stats)), grads= jax.value_and_gra d(compute_loss,
    has_aux=True)(state.params,state.batch_stats, i mages,labels)
  prefs: []
  type: TYPE_NORMAL
- en: state = state.apply_gradients(grads=grads)
  prefs: []
  type: TYPE_NORMAL
- en: metrics = compute_metrics(logits=logits, labels=labels)
  prefs: []
  type: TYPE_NORMAL
- en: return state, metrics
  prefs: []
  type: TYPE_NORMAL
- en: To train the network for one epoch, loop through the training data while applying
    the training step.
  prefs: []
  type: TYPE_NORMAL
- en: 'def train_one_epoch(state, dataloader):'
  prefs: []
  type: TYPE_NORMAL
- en: '"""Train for 1 epoch on the training set.""" batch_metrics = []'
  prefs: []
  type: TYPE_NORMAL
- en: 'for cnt, (images, labels) in enumerate(dataloader):'
  prefs: []
  type: TYPE_NORMAL
- en: images = images / 255.0
  prefs: []
  type: TYPE_NORMAL
- en: state, metrics = train_step(state, images, labels) batch_metrics.append(metrics)
  prefs: []
  type: TYPE_NORMAL
- en: 'batch_metrics_np = jax.device_get(batch_metrics) epoch_metrics_np = {k: np.mean([metrics[k]
    for metrics in batch_metrics_n'
  prefs: []
  type: TYPE_NORMAL
- en: p])
  prefs: []
  type: TYPE_NORMAL
- en: for k in batch_metrics_np[0] }
  prefs: []
  type: TYPE_NORMAL
- en: return state, epoch_metrics_np
  prefs: []
  type: TYPE_NORMAL
- en: '**Evaluation step**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The model evaluation steps accept the test labels and images and applies them
    to the network. It then returns the model evaluation metrics. During evaluation,
    set the [train] parameter to [False]. You'll also define the [batch_stats] and
    the random number for the [dropout] layer.
  prefs: []
  type: TYPE_NORMAL
- en: '@jax.jitdef eval_step(batch_stats, params, images, labels): logits = model.apply({''params'':
    params,''batch_stats'': batch _stats}, images, train=False,rngs={''dropout'':
    jax.random.PRNGKe y(0)})return compute_metrics(logits=logits, labels=labels)'
  prefs: []
  type: TYPE_NORMAL
- en: 'def evaluate_model(state, test_imgs, test_lbls):'
  prefs: []
  type: TYPE_NORMAL
- en: '"""Evaluate on the validation set."""'
  prefs: []
  type: TYPE_NORMAL
- en: metrics = eval_step(state.batch_stats,state.params, test_im
  prefs: []
  type: TYPE_NORMAL
- en: gs, test_lbls)
  prefs: []
  type: TYPE_NORMAL
- en: metrics = jax.device_get(metrics)
  prefs: []
  type: TYPE_NORMAL
- en: 'metrics = jax.tree_map(lambda x: x.item(), metrics) return metrics'
  prefs: []
  type: TYPE_NORMAL
- en: '**Train ResNet model in Flax**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Train the ResNet model by applying the [train_one_epoch] function for the desired
    number of epochs. This is a few epochs since we are finetuning the network.
  prefs: []
  type: TYPE_NORMAL
- en: '**Set up TensorBoard in Flax**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To monitor model training via TensorBoard, you can write the training and validation
    metrics to TensorBoard.
  prefs: []
  type: TYPE_NORMAL
- en: from torch.utils.tensorboard import SummaryWriter import torchvision.transforms.functional
    as F logdir = "flax_logs"
  prefs: []
  type: TYPE_NORMAL
- en: writer = SummaryWriter(logdir)
  prefs: []
  type: TYPE_NORMAL
- en: '**Train model**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Define a function to train and evaluate the model while writing the metrics
    to TensorBoard.(test_images, test_labels) = next(iter(validation_loader)) test_images
    = test_images / 255.0
  prefs: []
  type: TYPE_NORMAL
- en: training_loss = [] training_accuracy = [] testing_loss = []
  prefs: []
  type: TYPE_NORMAL
- en: testing_accuracy = []
  prefs: []
  type: TYPE_NORMAL
- en: def train_model(epochs):for epoch in range(1, epochs + 1):train_state, train_metrics
    = train_one_epoch(state, tra
  prefs: []
  type: TYPE_NORMAL
- en: in_loader)
  prefs: []
  type: TYPE_NORMAL
- en: training_loss.append(train_metrics['loss']) training_accuracy.append(train_metrics['accuracy'])
  prefs: []
  type: TYPE_NORMAL
- en: test_metrics = evaluate_model(train_state, test_images, test_labels)
  prefs: []
  type: TYPE_NORMAL
- en: testing_loss.append(test_metrics['loss'])
  prefs: []
  type: TYPE_NORMAL
- en: testing_accuracy.append(test_metrics['accuracy'])
  prefs: []
  type: TYPE_NORMAL
- en: writer.add_scalar('Loss/train', train_metrics['loss'], epoch)
  prefs: []
  type: TYPE_NORMAL
- en: writer.add_scalar('Loss/test', test_metrics['loss'], ep och)
  prefs: []
  type: TYPE_NORMAL
- en: writer.add_scalar('Accuracy/train', train_metrics['accu racy'], epoch)
  prefs: []
  type: TYPE_NORMAL
- en: writer.add_scalar('Accuracy/test', test_metrics['accura cy'], epoch)
  prefs: []
  type: TYPE_NORMAL
- en: 'print(f"Epoch: {epoch}, training loss: {train_metrics [''loss'']}, training
    accuracy: {train_metrics[''accuracy''] * 10 0}, validation loss: {test_metrics[''loss'']},
    validation accurac y: {test_metrics[''accuracy''] * 100}")'
  prefs: []
  type: TYPE_NORMAL
- en: return train_stateRun the training function. trained_model_state = train_model(config["N_EPOCHS"])
  prefs: []
  type: TYPE_NORMAL
- en: '**Save Flax model**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use the [save_checkpoint] to save a trained Flax model.
  prefs: []
  type: TYPE_NORMAL
- en: from flax.training import checkpoints
  prefs: []
  type: TYPE_NORMAL
- en: ckpt_dir = 'model_checkpoint/'
  prefs: []
  type: TYPE_NORMAL
- en: checkpoints.save_checkpoint(ckpt_dir=ckpt_dir,
  prefs: []
  type: TYPE_NORMAL
- en: target=trained_model_state, step=100,
  prefs: []
  type: TYPE_NORMAL
- en: prefix='resnet_model', overwrite=True
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: '**Load saved Flax model**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A saved Flax model is loaded using the [restore_checkpoint] method.
  prefs: []
  type: TYPE_NORMAL
- en: loaded_model = checkpoints.restore_checkpoint(
  prefs: []
  type: TYPE_NORMAL
- en: ckpt_dir=ckpt_dir, target=state, prefix='resnet_mod
  prefs: []
  type: TYPE_NORMAL
- en: el') loaded_model
  prefs: []
  type: TYPE_NORMAL
- en: '**Evaluate Flax ResNet model**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To evaluate a Flax model, pass the test and training data to
  prefs: []
  type: TYPE_NORMAL
- en: the evalaute_model function.evaluate_model(loaded_model,test_images, test_labels)
    ![](../images/00058.gif)
  prefs: []
  type: TYPE_NORMAL
- en: '**Visualize model performance**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can check the network's performance via TensorBoard or plot the metrics
    using Matplotlib.
  prefs: []
  type: TYPE_NORMAL
- en: '**Final thoughts**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can apply transfer learning to take advantage of pre-trained models and
    get results with minimal effort. You have learned how to train a ResNet model
    in Flax. Specially, you have covered:'
  prefs: []
  type: TYPE_NORMAL
- en: How to define the ResNet model in Flax.
  prefs: []
  type: TYPE_NORMAL
- en: How to freeze the layers of the ResNet network.
  prefs: []
  type: TYPE_NORMAL
- en: Training a ResNet model on custom data in Flax.
  prefs: []
  type: TYPE_NORMAL
- en: Saving and loading a ResNet model in Flax.
  prefs: []
  type: TYPE_NORMAL
- en: '**Elegy(High-level API for deep learning in JAX & Flax)**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Training deep learning networks in Flax is done in a couple of steps. It involves
    creating the following functions:'
  prefs: []
  type: TYPE_NORMAL
- en: Model definition.
  prefs: []
  type: TYPE_NORMAL
- en: Compute metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Training state.
  prefs: []
  type: TYPE_NORMAL
- en: Training step.
  prefs: []
  type: TYPE_NORMAL
- en: Training and evaluation function.
  prefs: []
  type: TYPE_NORMAL
- en: Flax and JAX give more control in defining and training deep learning networks.
    However, this comes with more verbosity. Enter **Elegy**. Elegy is a high-level
    API for creating deep learning networks in JAX. Elegy's API is like the one in
    Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at how to use Elegy to define and train deep learning networks in
    Flax.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data pre-processing**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To make this illustration concrete, we'll use the movie review data from Kaggle to
    create an LSTM network in Flax.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to download and extract the data.
  prefs: []
  type: TYPE_NORMAL
- en: import os
  prefs: []
  type: TYPE_NORMAL
- en: import kaggle
  prefs: []
  type: TYPE_NORMAL
- en: Obtain from https://www.kaggle.com/username/account os.environ["KAGGLE_USERNAME"]="KAGGLE_USERNAME"
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: os.environ["KAGGLE_KEY"]="KAGGLE_KEY"
  prefs: []
  type: TYPE_NORMAL
- en: '!kaggle datasets download lakshmi25npathi/imdb-dataset-of-50k-m ovie-reviews'
  prefs: []
  type: TYPE_NORMAL
- en: import zipfile
  prefs: []
  type: TYPE_NORMAL
- en: with zipfile.ZipFile('imdb-dataset-of-50k-movie-reviews.zip',
  prefs: []
  type: TYPE_NORMAL
- en: '''r'') as zip_ref:zip_ref.extractall(''imdb-dataset-of-50k-movie-reviews'')Next,
    we define the following processing steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Split the data into a training and testing set. Remove stopwords from the data.
  prefs: []
  type: TYPE_NORMAL
- en: Clean the data by removing punctuations and other special characters.
  prefs: []
  type: TYPE_NORMAL
- en: Convert the data to a TensorFlow dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Conver the data to numerical representation using the Keras vectorization layer.
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  prefs: []
  type: TYPE_NORMAL
- en: import pandas as pd
  prefs: []
  type: TYPE_NORMAL
- en: from numpy import array
  prefs: []
  type: TYPE_NORMAL
- en: import tensorflow_datasets as tfds
  prefs: []
  type: TYPE_NORMAL
- en: import tensorflow as tf
  prefs: []
  type: TYPE_NORMAL
- en: from sklearn.model_selection import train_test_split from sklearn.preprocessing
    import LabelEncoder import matplotlib.pyplot as plt
  prefs: []
  type: TYPE_NORMAL
- en: from sklearn.model_selection import train_test_split import tensorflow as tf
  prefs: []
  type: TYPE_NORMAL
- en: df = pd.read_csv("imdb-dataset-of-50k-movie-reviews/IMDB Datase t.csv")
  prefs: []
  type: TYPE_NORMAL
- en: import nltk
  prefs: []
  type: TYPE_NORMAL
- en: from nltk.corpus import stopwords
  prefs: []
  type: TYPE_NORMAL
- en: nltk.download('stopwords')
  prefs: []
  type: TYPE_NORMAL
- en: 'def remove_stop_words(review):'
  prefs: []
  type: TYPE_NORMAL
- en: review_minus_sw = []
  prefs: []
  type: TYPE_NORMAL
- en: stop_words = stopwords.words('english')
  prefs: []
  type: TYPE_NORMAL
- en: review = review.split()
  prefs: []
  type: TYPE_NORMAL
- en: cleaned_review = [review_minus_sw.append(word) for word in
  prefs: []
  type: TYPE_NORMAL
- en: review if word not in stop_words]
  prefs: []
  type: TYPE_NORMAL
- en: cleaned_review = ' '.join(review_minus_sw)
  prefs: []
  type: TYPE_NORMAL
- en: return cleaned_review
  prefs: []
  type: TYPE_NORMAL
- en: df['review'] = df['review'].apply(remove_stop_words) labelencoder = LabelEncoder()
  prefs: []
  type: TYPE_NORMAL
- en: df = df.assign(sentiment = labelencoder.fit_transform(df["senti ment"]))
  prefs: []
  type: TYPE_NORMAL
- en: df = df.drop_duplicates()
  prefs: []
  type: TYPE_NORMAL
- en: docs = df['review']
  prefs: []
  type: TYPE_NORMAL
- en: labels = array(df['sentiment'])
  prefs: []
  type: TYPE_NORMAL
- en: X_train, X_test , y_train, y_test = train_test_split(docs, labe ls , test_size
    = 0.20, random_state=0)
  prefs: []
  type: TYPE_NORMAL
- en: 'max_features = 10000 # Maximum vocab size.'
  prefs: []
  type: TYPE_NORMAL
- en: batch_size = 128
  prefs: []
  type: TYPE_NORMAL
- en: 'max_len = 50 # Sequence length to pad the outputs to. vectorize_layer = tf.keras.layers.TextVectorization(standardize
    =''lower_and_strip_punctuation'',max_tokens=max_features,output_m ode=''int'',output_sequence_length=max_len)'
  prefs: []
  type: TYPE_NORMAL
- en: vectorize_layer.adapt(X_train)
  prefs: []
  type: TYPE_NORMAL
- en: X_train_padded = vectorize_layer(X_train)
  prefs: []
  type: TYPE_NORMAL
- en: X_test_padded = vectorize_layer(X_test)
  prefs: []
  type: TYPE_NORMAL
- en: training_data = tf.data.Dataset.from_tensor_slices((X_train_pad ded, y_train))
  prefs: []
  type: TYPE_NORMAL
- en: validation_data = tf.data.Dataset.from_tensor_slices((X_test_pa dded, y_test))
  prefs: []
  type: TYPE_NORMAL
- en: training_data = training_data.batch(batch_size)
  prefs: []
  type: TYPE_NORMAL
- en: 'validation_data = validation_data.batch(batch_size) def get_train_batches():'
  prefs: []
  type: TYPE_NORMAL
- en: ds = training_data.prefetch(1)
  prefs: []
  type: TYPE_NORMAL
- en: ds = ds.repeat(3)
  prefs: []
  type: TYPE_NORMAL
- en: 'ds = ds.shuffle(3, reshuffle_each_iteration=True) # tfds.dataset_as_numpy converts
    the tf.data.Dataset into an'
  prefs: []
  type: TYPE_NORMAL
- en: iterable of NumPy arraysreturn tfds.as_numpy(ds)
  prefs: []
  type: TYPE_NORMAL
- en: '**Model definition in Elegy**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Start by installing Elegy, Flax, and JAX.pip install -U elegy flax jax jaxlibNext,
    define the LSTM model.
  prefs: []
  type: TYPE_NORMAL
- en: import jax
  prefs: []
  type: TYPE_NORMAL
- en: import jax.numpy as jnp import elegy as eg
  prefs: []
  type: TYPE_NORMAL
- en: from flax import linen as nn
  prefs: []
  type: TYPE_NORMAL
- en: 'class LSTMModel(nn.Module):'
  prefs: []
  type: TYPE_NORMAL
- en: 'def setup(self):'
  prefs: []
  type: TYPE_NORMAL
- en: 'self.embedding = nn.Embed(max_features, max_len) lstm_layer = nn.scan(nn.OptimizedLSTMCell,
    variable_broadcast="params", split_rngs={"params": False}, in_axes=1,'
  prefs: []
  type: TYPE_NORMAL
- en: out_axes=1,
  prefs: []
  type: TYPE_NORMAL
- en: length=max_len,
  prefs: []
  type: TYPE_NORMAL
- en: reverse=False)
  prefs: []
  type: TYPE_NORMAL
- en: self.lstm1 = lstm_layer()
  prefs: []
  type: TYPE_NORMAL
- en: self.dense1 = nn.Dense(256)
  prefs: []
  type: TYPE_NORMAL
- en: self.lstm2 = lstm_layer()
  prefs: []
  type: TYPE_NORMAL
- en: self.dense2 = nn.Dense(128)
  prefs: []
  type: TYPE_NORMAL
- en: self.lstm3 = lstm_layer()
  prefs: []
  type: TYPE_NORMAL
- en: self.dense3 = nn.Dense(64)
  prefs: []
  type: TYPE_NORMAL
- en: self.dense4 = nn.Dense(2)
  prefs: []
  type: TYPE_NORMAL
- en: '@nn.rematdef __call__(self, x_batch): x = self.embedding(x_batch)'
  prefs: []
  type: TYPE_NORMAL
- en: carry, hidden = nn.OptimizedLSTMCell.initialize_carry(j ax.random.PRNGKey(0),
    batch_dims=(len(x_batch),), size=128)
  prefs: []
  type: TYPE_NORMAL
- en: (carry, hidden), x = self.lstm1((carry, hidden), x)
  prefs: []
  type: TYPE_NORMAL
- en: x = self.dense1(x) x = nn.relu(x)
  prefs: []
  type: TYPE_NORMAL
- en: carry, hidden = nn.OptimizedLSTMCell.initialize_carry(j ax.random.PRNGKey(0),
    batch_dims=(len(x_batch),), size=64)
  prefs: []
  type: TYPE_NORMAL
- en: (carry, hidden), x = self.lstm2((carry, hidden), x)
  prefs: []
  type: TYPE_NORMAL
- en: x = self.dense2(x) x = nn.relu(x)
  prefs: []
  type: TYPE_NORMAL
- en: carry, hidden = nn.OptimizedLSTMCell.initialize_carry(j ax.random.PRNGKey(0),
    batch_dims=(len(x_batch),), size=32)
  prefs: []
  type: TYPE_NORMAL
- en: (carry, hidden), x = self.lstm3((carry, hidden), x)
  prefs: []
  type: TYPE_NORMAL
- en: x = self.dense3(x)
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.relu(x)
  prefs: []
  type: TYPE_NORMAL
- en: x = self.dense4(x[:, -1]) return nn.log_softmax(x)
  prefs: []
  type: TYPE_NORMAL
- en: Let's now create an Elegy model using the above network. As you can see, the
    loss and metrics are defined like in Keras. The model compilation is done in the
    constructor, so you don't have to do this manually.
  prefs: []
  type: TYPE_NORMAL
- en: import optax
  prefs: []
  type: TYPE_NORMAL
- en: model = eg.Model(
  prefs: []
  type: TYPE_NORMAL
- en: module=LSTMModel(),
  prefs: []
  type: TYPE_NORMAL
- en: loss=[
  prefs: []
  type: TYPE_NORMAL
- en: eg.losses.Crossentropy(), eg.regularizers.L2(l=1e-4), ],
  prefs: []
  type: TYPE_NORMAL
- en: metrics=eg.metrics.Accuracy(),
  prefs: []
  type: TYPE_NORMAL
- en: optimizer=optax.adam(1e-3), )
  prefs: []
  type: TYPE_NORMAL
- en: '**Elegy model summary**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Like in Keras, we can print the model's summary. model.summary(jnp.array(X_train_padded[:64]))
    ![](../images/00059.jpeg)
  prefs: []
  type: TYPE_NORMAL
- en: '**Distributed training in Elegy**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To train models in a distributed manner in Flax, we define parallel versions
    of our model training functions.
  prefs: []
  type: TYPE_NORMAL
- en: However, with Elegy, we call the [distributed] method.model = model.distributed()
  prefs: []
  type: TYPE_NORMAL
- en: '**Keras-like callbacks in Flax**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Elegy supports callbacks similar to Keras callbacks. In this case, we train
    the model with the following callbacks:'
  prefs: []
  type: TYPE_NORMAL
- en: TensorBoard.
  prefs: []
  type: TYPE_NORMAL
- en: Model checkpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Early stopping.callbacks = [ eg.callbacks.TensorBoard("summaries"), eg.callbacks.ModelCheckpoint("models/high-level",
    save_best_only=True),eg.callbacks.EarlyStopping(monitor = 'val_loss',pa tience=10)]
  prefs: []
  type: TYPE_NORMAL
- en: '**Train Elegy models**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Elegy provides the [fit] method for training models. The method supports the
    following data sources:'
  prefs: []
  type: TYPE_NORMAL
- en: Tensorflow Dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Pytorch DataLoader
  prefs: []
  type: TYPE_NORMAL
- en: Elegy DataLoader, and
  prefs: []
  type: TYPE_NORMAL
- en: Python Generators.
  prefs: []
  type: TYPE_NORMAL
- en: history = model.fit(
  prefs: []
  type: TYPE_NORMAL
- en: training_data,
  prefs: []
  type: TYPE_NORMAL
- en: epochs=100,
  prefs: []
  type: TYPE_NORMAL
- en: validation_data=(validation_data), callbacks=callbacks,
  prefs: []
  type: TYPE_NORMAL
- en: )![](../images/00060.gif)
  prefs: []
  type: TYPE_NORMAL
- en: '**Evaluate Elegy models**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To evaluate Elegy models, use the [evaluate] function.model.evaluate(validation_data)![](../images/00061.jpeg)
  prefs: []
  type: TYPE_NORMAL
- en: '**Visualize Elegy model with TensorBoard**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since we applied the TensorBoard callback, we can view the performance of the
    model in TensorBoard.%load_ext tensorboard%tensorboard --logdir summaries ![](../images/00062.jpeg)
  prefs: []
  type: TYPE_NORMAL
- en: '**Plot model performance with Matplotlib**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can also plot the performance of the model using Matplotlib.import matplotlib.pyplot
    as plt
  prefs: []
  type: TYPE_NORMAL
- en: 'def plot_history(history):'
  prefs: []
  type: TYPE_NORMAL
- en: n_plots = len(history.history.keys()) // 2
  prefs: []
  type: TYPE_NORMAL
- en: plt.figure(figsize=(14, 24))
  prefs: []
  type: TYPE_NORMAL
- en: for i, key in enumerate(list(history.history.keys())[:n_plo
  prefs: []
  type: TYPE_NORMAL
- en: 'ts]):'
  prefs: []
  type: TYPE_NORMAL
- en: metric = history.history[key]
  prefs: []
  type: TYPE_NORMAL
- en: val_metric = history.history[f"val_{key}"]
  prefs: []
  type: TYPE_NORMAL
- en: plt.subplot(n_plots, 1, i + 1)
  prefs: []
  type: TYPE_NORMAL
- en: plt.plot(metric, label=f"Training {key}") plt.plot(val_metric, label=f"Validation
    {key}") plt.legend(loc="lower right")
  prefs: []
  type: TYPE_NORMAL
- en: plt.ylabel(key)
  prefs: []
  type: TYPE_NORMAL
- en: plt.title(f"Training and Validation {key}")
  prefs: []
  type: TYPE_NORMAL
- en: plt.show()plot_history(history) ![](../images/00063.jpeg)
  prefs: []
  type: TYPE_NORMAL
- en: '**Making predictions with Elegy models**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Like Keras, Elegy provides the [predict] method for making predictions.(text,
    test_labels) = next(iter(validation_data)) y_pred = model.predict(jnp.array(text))
    ![](../images/00064.jpeg)
  prefs: []
  type: TYPE_NORMAL
- en: '**Saving and loading Elegy models**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Elegy models can also be saved like Keras models and used to make predictions
    immediately.
  prefs: []
  type: TYPE_NORMAL
- en: You can use can use `save` but `ModelCheckpoint already seria lized the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: model.save("model")
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: current model referenceprint("current model id:", id(model))
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: load model from disk
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: model = eg.load("models/high-level")
  prefs: []
  type: TYPE_NORMAL
- en: new model reference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'print("new model id: ", id(model))'
  prefs: []
  type: TYPE_NORMAL
- en: check that it works!model.evaluate(validation_data) ![](../images/00065.jpeg)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Final thoughts**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This article has been a quick dive into Elegy– a JAX high-level API that you
    can use to build and train Flax networks. You have seen that Elegy is very similar
    to Keras and has a simple API for Flax. It also contains similar functions to
    Keras, like:'
  prefs: []
  type: TYPE_NORMAL
- en: Model training.
  prefs: []
  type: TYPE_NORMAL
- en: Making predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Creating callbacks.
  prefs: []
  type: TYPE_NORMAL
- en: Defining model loss and metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '**Appendix**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This book is provided in line with our terms and privacy policy.
  prefs: []
  type: TYPE_NORMAL
- en: '**Disclaimer**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The information in this eBook is not meant to be applied as is in a production
  prefs: []
  type: TYPE_NORMAL
- en: environment. By applying it to a production environment, you take full responsibility
  prefs: []
  type: TYPE_NORMAL
- en: for your actions.
  prefs: []
  type: TYPE_NORMAL
- en: The author has made every effort to ensure the accuracy of the information within
  prefs: []
  type: TYPE_NORMAL
- en: this book was correct at the time of publication. The author does not assume
    and
  prefs: []
  type: TYPE_NORMAL
- en: hereby disclaims any liability to any party for any loss, damage, or disruption
    caused
  prefs: []
  type: TYPE_NORMAL
- en: by errors or omissions, whether such errors or omissions result from accident,
  prefs: []
  type: TYPE_NORMAL
- en: negligence, or any other cause.
  prefs: []
  type: TYPE_NORMAL
- en: No part of this eBook may be reproduced or transmitted in any form or by any
  prefs: []
  type: TYPE_NORMAL
- en: means, electronic or mechanical, recording or by any information storage and
  prefs: []
  type: TYPE_NORMAL
- en: retrieval system, without written permission from the author.
  prefs: []
  type: TYPE_NORMAL
- en: '**Copyright**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: JAX and Flax book— Deep learning with Flax and JAX © Copyright Derrick Mwiti.
    All Rights Reserved.
  prefs: []
  type: TYPE_NORMAL
- en: '**Other things to learn**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Learn Python
  prefs: []
  type: TYPE_NORMAL
- en: Learn data science
  prefs: []
  type: TYPE_NORMAL
- en: Learn Streamlit
  prefs: []
  type: TYPE_NORMAL
