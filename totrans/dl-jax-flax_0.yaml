- en: '**JAX & Flax ebook**'
  id: totrans-0
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '![](`../images/00001.jpeg`)'
  id: totrans-1
  prefs: []
  stylish: true
  type: TYPE_IMG
- en: Download all notebooks JAX (What it is and how to use it in Python) What is
    XLA? Installing JAX **Setting up TPUs on Google Colab Data types in JAX Ways to
    create JAX arrays Generating random numbers with JAX Pure functions JAX NumPy
    operations JAX arrays are immutable Out-of-Bounds Indexing Data placement on devices
    in JAX How fast is JAX? Using jit() to speed up functions How JIT works Taking
    derivatives with grad() Auto-vectorization with vmap Parallelization with pmap
    Debugging NANs in JAX Double (64bit) precision What is a pytree? Handling state
    in JAX Loading datasets with JAX Building neural networks with JAX Final thoughts**
    Optimizers in JAX and Flax **Adaptive vs stochastic gradient descent (SGD) optimizers
    AdaBelief AdaGrad Adam – Adaptive moment estimation AdamW RAdam – Rectified Adam
    optimizer AdaFactor Fromage Lamb – Layerwise adaptive large batch optimization
    Lars – Layer-wise Adaptive Rate Scaling SM3 - Square-root of Minima of Sums of
    Maxima of Squared-gradients Method SGD– Stochastic Gradient Descent Noisy SGD
    Optimistic GD Differentially Private SGD RMSProp Yogi Final thoughts JAX loss
    functions What is a loss function? Creating custom loss functions in JAX Which
    loss functions are available in JAX? Sigmoid binary cross entropy Softmax cross
    entropy Cosine distance Cosine similarity Huber loss l2 loss log cosh Smooth labels
    Computing loss with JAX Metrics How to monitor JAX loss functions Why JAX loss
    nan happens Final thoughts Activation functions in JAX and Flax ReLU – Rectified
    linear unit PReLU– Parametric Rectified Linear Unit Sigmoid Log sigmoid Softmax
    Log softmax ELU – Exponential linear unit activation CELU – Continuously-differentiable
    exponential linear unit GELU– Gaussian error linear unit activation GLU – Gated
    linear unit activation Soft sign Softplus Swish–Sigmoid Linear Unit( SiLU) Custom
    activation functions in JAX and Flax Final thoughts How to load datasets in JAX
    with TensorFlow How to load text data in JAX Clean the text data Label encode
    the sentiment column Text preprocessing with TensorFlow How to load image data
    in JAX How to load CSV data in JAX Final thoughts Image classification with JAX
    & Flax Loading the dataset Define Convolution Neural Network with Flax Define
    loss Compute metrics Create training state Define training step Define evaluation
    step Training function Evaluate the model Train and evaluate the model Model performance
    Final thoughts Distributed training with JAX & Flax Perform standard imports Setup
    TPUs on Colab Download the dataset Load the dataset Define the model with Flax
    Create training state Apply the model Training function Train the model Model
    evaluation Final thoughts How to use TensorBoard in JAX & Flax How to use TensorBoard
    How to install TensorBoard Using TensorBoard with Jupyter notebooks and Google
    Colab How to launch TensorBoard Tensorboard dashboards How to use TensorBoard
    with Flax How to log images with TensorBoard in Flax How to log text with TensorBoard
    in Flax Track model training in JAX using TensorBoard How to profile JAX programs
    with TensorBoard Programmatic profiling Manual profiling with TensorBoard How
    to profile JAX program on a remote machine Share TensorBoard dashboards Final
    thoughts Handling state in JAX & Flax (BatchNorm and DropOut layers) Perform standard
    imports Download the dataset Loading datasets in JAX Data processing with PyTorch
    Define Flax model with BatchNorm and DropOut Create loss function Compute metrics
    Create custom Flax training state Training step Evaluation step Train Flax model
    Set up TensorBoard in Flax Train model Save Flax model Load Flax model Evaluate
    Flax model Visualize Flax model performance Final thoughts LSTM in JAX & Flax
    Dataset download Data processing with NLTK Text vectorization with Keras Create
    tf.data dataset Define LSTM model in Flax Compute metrics in Flax Create training
    state Define training step Evaluate the Flax model Create training function Train
    LSTM model in Flax Visualize LSTM model performance in Flax Save LSTM model Final
    thoughts Flax vs. TensorFlow Random number generation in TensorFlow and Flax Model
    definition in Flax and TensorFlow Activations in Flax and TensorFlow Optimizers
    in Flax and TensorFlow Metrics in Flax and TensorFlow Computing gradients in Flax
    and TensorFlow Loading datasets in Flax and TensorFlow Training model in Flax
    vs. TensorFlow Distributed training in Flax and TensorFlow Working with TPU accelerators
    Model evaluation Visualize model performance Final thoughts Train ResNet in Flax
    from scratch(Distributed ResNet training) Install Flax models Perform standard
    imports Download dataset Loading dataset in Flax Data transformation in Flax Instantiate
    Flax ResNet model Compute metrics Create Flax model training state Apply model
    function TensorBoard in Flax Train Flax ResNet model Evaluate model with TensorBoard
    Visualize Flax model performance Save Flax ResNet model Load Flax RestNet model
    Final thoughts Transfer learning with JAX & Flax Install JAX ResNet Download dataset
    Data loading in JAX Data processing ResNet model definition Create head network
    Combine ResNet backbone with head Load pre-trained ResNet 50 Get model and variables
    Zero gradients Define Flax optimizer Define Flax loss function Compute Flax metrics
    Create Flax training state Training step Evaluation step Train ResNet model in
    Flax Set up TensorBoard in Flax Train model Save Flax model Load saved Flax model
    Evaluate Flax ResNet model Visualize model performance Final thoughts Elegy(High-level
    API for deep learning in JAX & Flax) Data pre-processing Model definition in Elegy
    Elegy model summary Distributed training in Elegy Keras-like callbacks in Flax
    Train Elegy models Evaluate Elegy models Visualize Elegy model with TensorBoard
    Plot model performance with Matplotlib Making predictions with Elegy models Saving
    and loading Elegy models Final thoughts** Appendix Disclaimer Copyright Other
    things to learn
  id: totrans-2
  prefs:
  - PREF_H3
  stylish: true
  type: TYPE_NORMAL
- en: Download all notebooks
  id: totrans-3
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Link to download all notebooks. The password for the ZIP file is FDCPJx0D5A6SO#%Qsg
  id: totrans-4
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: JAX (What it is and how to use it in Python)
  id: totrans-5
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: 'JAX is a Python library offering high performance in machine learning with
    XLA and Just In Time (JIT) compilation. Its API is similar to NumPy''s with a
    few differences. JAX ships with functionalities that aim to improve and increase
    speed in machine learning research. These functionalities include:'
  id: totrans-6
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Automatic differentiation
  id: totrans-7
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Vectorization
  id: totrans-8
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: JIT compilation
  id: totrans-9
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: This article will cover these functionalities and other JAX concepts. Let's
    get started.
  id: totrans-10
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: What is XLA?
  id: totrans-11
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: XLA (Accelerated Linear Algebra) is a linear algebra compiler for accelerating
    machine learning models. It leads to an increase in the speed of model execution
    and reduced memory usage. XLA programs can be generated by JAX, PyTorch, Julia,
    and NX.
  id: totrans-12
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Installing JAX
  id: totrans-13
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: 'JAX can be installed from the Python Package Index using: `pip install jax`
    JAX is pre-installed on Google Colab. See the link below for other installation
    options.'
  id: totrans-14
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Setting up TPUs on Google Colab
  id: totrans-15
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: You need to set up JAX to use TPUs on Colab. That is done by executing the following
    code. Ensure that you have changed the runtime to TPU by going to Runtime-> Change
    Runtime Type. If no accelerator is available, JAX will use the CPU.
  id: totrans-16
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import jax.tools.colab_tpu jax.tools.colab_tpu.setup_tpu() jax.devices()`'
  id: totrans-17
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Data types in JAX
  id: totrans-18
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: The data types in NumPy are similar to those in JAX arrays. For instance, here
    is how you can create float and int data in JAX.
  id: totrans-19
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import jax.numpy as jnp x = jnp.float32(1.25844) x = jnp.int32(45.25844)`'
  id: totrans-20
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: When you check the type of the data, you will see that it's
  id: totrans-21
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: a `DeviceArray.DeviceArray` in JAX is the equivalent of `numpy.ndarray` in NumPy.
  id: totrans-22
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`jax.numpy` provides an interface similar to NumPy''s. However, JAX also provides
    `jax.lax` a low-level API that is more powerful and stricter. For example, with
    `[jax.numpy]` you can add numbers that have mixed types but `[jax.lax]` will not
    allow this.'
  id: totrans-23
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Ways to create JAX arrays
  id: totrans-24
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: You can create JAX arrays like you would in NumPy. For example, can use:arange
  id: totrans-25
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: linspacePython lists.ones.zeros.identity.
  id: totrans-26
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`jnp.arange(10)`'
  id: totrans-27
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`jnp.arange(0,10)`'
  id: totrans-28
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`scores = [50,60,70,30,25,70] scores_array = jnp.array(scores) jnp.zeros(5)`'
  id: totrans-29
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`jnp.ones(5)`'
  id: totrans-30
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`jnp.eye(5)`'
  id: totrans-31
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`jnp.identity(5)`'
  id: totrans-32
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`![](../images/00002.jpeg)`'
  id: totrans-33
  prefs: []
  stylish: true
  type: TYPE_IMG
- en: Generating random numbers with JAX
  id: totrans-34
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Random number generation is one main difference between JAX and NumPy. JAX is
    meant to be used with functional programs. JAX expects these functions to be pure.
    A **pure function** has no side effects and expects the output to only come from
    its inputs. JAX transformation functions expect pure functions.
  id: totrans-35
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Therefore, when working with JAX, all input should be passed through function
    parameters, while all output should come from the function results. Hence, something
    like Python's print function is not pure.
  id: totrans-36
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: A pure function returns the same results when called with the same inputs. This
    is not possible with [np.random.random()] because it is stateful and returns different
    results when called several times.
  id: totrans-37
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`print(np.random.random()) print(np.random.random()) print(np.random.random())`'
  id: totrans-38
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`![](../images/00003.jpeg)`'
  id: totrans-39
  prefs: []
  stylish: true
  type: TYPE_IMG
- en: JAX implements random number generation using a random state. This random state
    is referred to as a `key🔑`. JAX generates pseudorandom numbers from the pseudorandom
    number generator (PRNGs)  state.
  id: totrans-40
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`seed = 98`'
  id: totrans-41
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`key = jax.random.PRNGKey(seed) jax.random.uniform(key)`'
  id: totrans-42
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: You should, therefore, not reuse the same state. Instead, you should split the
    PRNG to obtain as many sub keys as you need.`key, subkey = jax.random.split(key)
    Using the same key will always generate the same output. ![](../images/00004.gif)`
  id: totrans-43
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Pure functions
  id: totrans-44
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: We have mentioned that the output of a pure function should only come from the
    result of the function. Therefore, something like Python's [print] function introduces
    impurity. This can be demonstrated using this function.
  id: totrans-45
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def impure_print_side_effect(x):`'
  id: totrans-46
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`print("Executing function")` # This is a side-effect return x'
  id: totrans-47
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'The side-effects appear during the first run`print ("First call: ", jax.jit(impure_print_side_effect)(4.))`'
  id: totrans-48
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: Subsequent runs with parameters of same type and shape may no t show the side-effect
  id: totrans-49
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: This is because JAX now invokes a cached compilation of the f unction
  id: totrans-50
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`print ("Second call: ", jax.jit(impure_print_side_effect)(5.))`'
  id: totrans-51
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: JAX re-runs the Python function when the type or shape of the argument changes
  id: totrans-52
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`print ("Third call, different type: ", jax.jit(impure_print_sid e_effect)(jnp.array([5.])))`'
  id: totrans-53
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`![](../images/00005.gif)`'
  id: totrans-54
  prefs: []
  stylish: true
  type: TYPE_IMG
- en: We can see the printed statement the first time the function is executed. However,
    we don't see that print statement in consecutive runs because it is cached. We
    only see the statement again after changing the data's shape, which forces JAX
    to recompile the function. More on jax.jit in a moment.
  id: totrans-55
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: JAX NumPy operations
  id: totrans-56
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Operations on JAX arrays are similar to operations with NumPy arrays. For example,
    you can [max], [argmax], and [sum] like in NumPy.
  id: totrans-57
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`matrix = matrix.reshape(4,4) jnp.max(matrix)`'
  id: totrans-58
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`jnp.argmax(matrix)`'
  id: totrans-59
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`jnp.min(matrix)`'
  id: totrans-60
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`jnp.argmin(matrix)`'
  id: totrans-61
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`jnp.sum(matrix)`'
  id: totrans-62
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`jnp.sqrt(matrix)`'
  id: totrans-63
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`matrix.transpose()`'
  id: totrans-64
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`![](../images/00006.jpeg)`However, JAX doesn''t allow operations on non-array
    input like NumPy. For example, passing Python lists or tuples will lead to an
    error.'
  id: totrans-65
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`try:`'
  id: totrans-66
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`jnp.sum([1, 2, 3])`'
  id: totrans-67
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`except TypeError as e:`'
  id: totrans-68
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`print(f"TypeError: {e}")`'
  id: totrans-69
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`TypeError: sum requires ndarray or scalar arguments, got <c`'
  id: totrans-70
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`lass ''list''> at position 0.`'
  id: totrans-71
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: JAX arrays are immutable
  id: totrans-72
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Unlike in NumPy, JAX arrays can not be modified in place. This is because JAX
    expects pure functions.
  id: totrans-73
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`scores = [50,60,70,30,25]`'
  id: totrans-74
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`scores_array = jnp.array(scores)`'
  id: totrans-75
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`scores_array[0:3] = [20,40,90]`'
  id: totrans-76
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`TypeError: ''<class ''jaxlib.xla_extension.DeviceArray''>'' objec t does not
    support item assignment.`'
  id: totrans-77
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: JAX arrays are immutable. Instead of ``x[idx] = y``, use ``x
  id: totrans-78
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`= x.at[idx].set(y)`` or another `.at[]`'
  id: totrans-79
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'method: `https://jax.readthedocs.io/en/latest/_autosummary/ja x.numpy.ndarray.at.html`'
  id: totrans-80
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: Array updates in JAX are performed using `[x.at[idx].set(y)]`. This returns
    a new array while the old array stays unaltered.
  id: totrans-81
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`try:`'
  id: totrans-82
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`jnp.sum([1, 2, 3])`'
  id: totrans-83
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`except TypeError as e:`'
  id: totrans-84
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`print(f"TypeError: {e}")`'
  id: totrans-85
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`TypeError: sum requires ndarray or scalar arguments, got <c`'
  id: totrans-86
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`lass ''list''> at position 0.`'
  id: totrans-87
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Out-of-Bounds Indexing**'
  id: totrans-88
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '`NumPy` usually throws an error when you try to get an item in an array that
    is out of bounds. JAX doesn''t throw any error but returns the last item in the
    array.'
  id: totrans-89
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`matrix = jnp.arange(1,17) matrix[20]`'
  id: totrans-90
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`DeviceArray(16, dtype=int32)`'
  id: totrans-91
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: JAX is designed like this because throwing errors in accelerators can be challenging.
  id: totrans-92
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Data placement on devices in JAX**'
  id: totrans-93
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: JAX arrays are placed in the first device, `[jax.devices()[0]]`that is, GPU,
    TPU, or CPU. Data can be placed on a particular device
  id: totrans-94
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: using `jax.device_put()`.
  id: totrans-95
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: from `jax` import `device_put`
  id: totrans-96
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: import `numpy` as `np`
  id: totrans-97
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`size = 5000`'
  id: totrans-98
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`x = np.random.normal(size=(size, size)).astype(np.float32) x = device_put(x)`'
  id: totrans-99
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The data becomes committed to that device, and operations on it are also committed
    on the same device.
  id: totrans-100
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**How fast is JAX?**'
  id: totrans-101
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: JAX uses `asynchronous` dispatch, meaning that it does not wait for computation
    to complete to give control back to the `Python program`. Therefore, when you
    perform an execution, JAX will return a future. JAX forces Python to wait for
    the execution when you want to print the output or if you convert the result to
    a `NumPy array`.
  id: totrans-102
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Therefore, if you want to compute the time of execution of a program you'll
    have to convert the result to a `NumPy` array
  id: totrans-103
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: using `[block_until_ready()]` to wait for the execution to complete. Generally
    speaking, `NumPy` will outperform JAX on the CPU, but JAX will outperform `NumPy`
    on accelerators and when using jitted functions.
  id: totrans-104
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Using jit() to speed up functions**'
  id: totrans-105
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '`[jit]` performs just-in-time compilation with XLA. `[jax.jit]` expects a pure
    function. Any side effects in the function will only be executed once. Let''s
    create a pure function and time its execution time without `jit`.'
  id: totrans-106
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def test_fn(sample_rate=3000,frequency=3):`'
  id: totrans-107
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`x = jnp.arange(sample_rate)`'
  id: totrans-108
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`y = np.sin(2*jnp.pi*frequency * (frequency/sample_rate)) return jnp.dot(x,y)`'
  id: totrans-109
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`%timeit test_fn()`'
  id: totrans-110
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`best of 5: 76.1 µs per loop`'
  id: totrans-111
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: Let's now use `jit` and time the execution of the same function. In this case,
    we can see that using `jit` makes the execution almost 20 times faster.
  id: totrans-112
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`test_fn_jit = jax.jit(test_fn)`'
  id: totrans-113
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`%timeit test_fn_jit().block_until_ready()` # best of 5: 4.54 µs per loop'
  id: totrans-114
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: In the above example, `[test_fn_jit]` is the jit-compiled version of the function.
    JAX then created code that is optimized for GPU or TPU. The optimized code is
    what will be used the next time this function is called.
  id: totrans-115
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**How JIT works**'
  id: totrans-116
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: JAX works by converting Python functions into an intermediate language called
    jaxpr (JAX Expression). The `[jax.make_jaxpr]` can be used to show the jaxpr representation
    of a Python function. If the function has any side effects, they are not recorded
    by jaxpr. We saw earlier that any side effects, for example, printing, will only
    be shown during the first call.
  id: totrans-117
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def sum_logistic(x):`'
  id: totrans-118
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`print("printed x:", x)`'
  id: totrans-119
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))`'
  id: totrans-120
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`x_small = jnp.arange(6.)print(jax.make_jaxpr(sum_logistic)(x_small))`'
  id: totrans-121
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: JAX creates the jaxpr through tracing. Each argument in the function is wrapped
    with a tracer object. The purpose of these tracers is to record all JAX operations
    performed on them when the function is called. JAX uses the tracer records to
    rebuild the function, which leads to jaxpr. Python side-effects don't show up
    in the jaxpr because the tracers do not record them.
  id: totrans-122
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: JAX requires arrays shapes to be static and known at compile time. Decorating
    a function conditioned on a value with jit results in error. Therefore, not all
    code can be jit-compiled.
  id: totrans-123
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`@jax.jitdef f(boolean, x):return -x if boolean else x`'
  id: totrans-124
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`f(True, 1)`'
  id: totrans-125
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`ConcretizationTypeError`: Abstract tracer value encountered where concrete
    value is expected: `Traced<ShapedArray(bool[], weak _type=True)>with<DynamicJaxprTrace(level=0/1)>`'
  id: totrans-126
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: 'There are a couple of solutions to this problem:'
  id: totrans-127
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Remove conditionals on the value. Use JAX control flow operators such as `[jax.lax.cond]`.
  id: totrans-128
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Jit only a part of the function. Make parameters static.
  id: totrans-129
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: We can implement the last option and make the boolean parameter static. This
    is done by specifying `[static_argnums]` or `[static_argnames]`. This forces JAX
    to recompile the function when the value of the static parameter changes. This
    is not a good strategy if the function will get many values for the static argument.
    You don't want to recompile the function too many times.
  id: totrans-130
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: You can pass the static arguments using Python’s `[functools.partial]`. `from
    functools import partial@partial(jax.jit, static_argnums=(0,)) def f(boolean,
    x):return -x if boolean else xf(True, 1)`
  id: totrans-131
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Taking derivatives with `grad()`**'
  id: totrans-132
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Computing derivatives in JAX is done using `jax.grad.` `@jax.jitdef sum_logistic(x):return
    jnp.sum(1.0 / (1.0 + jnp.exp(-x)))`
  id: totrans-133
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`x_small = jnp.arange(6.)`'
  id: totrans-134
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`derivative_fn = jax.grad(sum_logistic) print(derivative_fn(x_small))`'
  id: totrans-135
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The `[grad]` function has a `[has_aux]` argument that allows you to return auxiliary
    data. For example, when building machine learning models, you can use it to return
    loss and gradients.
  id: totrans-136
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`@jax.jitdef sum_logistic(x):return jnp.sum(1.0 / (1.0 + jnp.exp(-x))),(x +
    1)`'
  id: totrans-137
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`x_small = jnp.arange(6.)`'
  id: totrans-138
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`derivative_fn = jax.grad(sum_logistic, has_aux=True) print(derivative_fn(x_small))`'
  id: totrans-139
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`(DeviceArray([0.25 , 0.19661194, 0.10499357, 0.04517666, 0.01766271,`'
  id: totrans-140
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`0.00664806], dtype=float32), DeviceArray([1., 2.,`'
  id: totrans-141
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`3., 4., 5., 6.], dtype=float32))`You can perform advanced automatic differentiation
    using **`jax.vjp()`**and **`jax.jvp()`**. **## Auto-vectorization with vmap**'
  id: totrans-142
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: vmap(Vectorizing map) allows you write a function that can be applied to a single
    data and then vmap will map it to a batch of data. Without vmap the solution would
    be to loop through the batches while applying the function. Using jit with for
    loops is a little complicated and may be slower.
  id: totrans-143
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`mat = jax.random.normal(key, (150, 100)) batched_x = jax.random.normal(key,
    (10, 100)) def apply_matrix(v):`'
  id: totrans-144
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`return jnp.dot(mat, v)`'
  id: totrans-145
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`@jax.jit`'
  id: totrans-146
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def vmap_batched_apply_matrix(v_batched):`'
  id: totrans-147
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`return jax.vmap(apply_matrix)(v_batched)`'
  id: totrans-148
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`print(''Auto-vectorized with vmap'')`'
  id: totrans-149
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`%timeit vmap_batched_apply_matrix(batched_x).block_until_ready ()`'
  id: totrans-150
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: In JAX, the `[jax.vmap]` transformation is designed to generate a vectorized
    implementation of a function automatically. It does this by tracing the function
    similarly to `[jax.jit]`, and automatically adding batch axes at the beginning
    of each input. If the batch dimension is not the first, you may use the `[in_axes]`
    and `[out_axes]` arguments to specify the location of the batch dimension in inputs
    and outputs. These may be an integer if the batch axis is the same for all inputs
    and outputs, or lists, otherwise. `Matteo Hessel, JAX author.`
  id: totrans-151
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Parallelization with `[pmap]`
  id: totrans-152
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: The working of `jax.pmap` is similar to `jax.vmap`. The difference is that `jax.pmap`
    is meant for parallel execution, that is, computation on multiple devices. This
    is applicable when training a machine learning model on batches of data.
  id: totrans-153
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Computation on batches can occur in different devices then the results are aggregated.
    The `[pmap]`ed function returns a `[ShardedDeviceArray]`. This is because the
    arrays are split across all the devices. There is no need to decorate the function
    with jit because the function is jitcompiled by default when using `[pmap]`.
  id: totrans-154
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`x = np.arange(5)w = np.array([2., 3., 4.])`'
  id: totrans-155
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def convolve(x, w):`'
  id: totrans-156
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`output = []`'
  id: totrans-157
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`for i in range(1, len(x)-1):`'
  id: totrans-158
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`output.append(jnp.dot(x[i-1:i+2], w)) return jnp.array(output)`'
  id: totrans-159
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`convolve(x, w) n_devices = jax.local_device_count()`'
  id: totrans-160
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`xs = np.arange(5 * n_devices).reshape(-1, 5)`'
  id: totrans-161
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`ws = np.stack([w] * n_devices)`'
  id: totrans-162
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`jax.pmap(convolve)(xs, ws)`'
  id: totrans-163
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`ShardedDeviceArray([[ 11., 20., 29.],`'
  id: totrans-164
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`.................`'
  id: totrans-165
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`[326., 335., 344.]], dtype=float32)`'
  id: totrans-166
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: You may need to aggregate data using one of the `[collective operators]`, for
    example, to compute the mean of the accuracy or mean of the logits. In that case,
    you'll need to specify an `[axis_name]`. This name is important to achieve communication
    between devices.
  id: totrans-167
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Debugging NANs in JAX
  id: totrans-168
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: By default, the occurrence of NANs in JAX program will not lead to an error.
    `jnp.divide(0.0,0.0)# DeviceArray(nan, dtype=float32, weak_type=True)`
  id: totrans-169
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: You can turn on the NAN checker and your program will error out at the occurrence
    of NANs. You should only use the NAN checker for debugging because it leads to
    performance issues. Also, it doesn't work with `[pmap]`, use `[vmap]` instead.
  id: totrans-170
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from jax.config import config`'
  id: totrans-171
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`config.update("jax_debug_nans", True)`'
  id: totrans-172
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`jnp.divide(0.0,0.0)`'
  id: totrans-173
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`FloatingPointError: invalid value (nan) encountered in div`'
  id: totrans-174
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '**Double (64bit) precision**'
  id: totrans-175
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: JAX enforces single-precision of numbers. For example, you will get a warning
    when you create a `[float64]` number. If you check the type of the number, you
    will notice that it's `[float32]`.
  id: totrans-176
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`x = jnp.float64(1.25844)`'
  id: totrans-177
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_num py.py:1806`:
    UserWarning: Explicitly requested dtype `float64` requested in array is not available,
    and will be truncated to dtype `float32`. To enable more dtypes, set the `jax_enable_x64`
    configuration option or the `JAX_ENABLE_X64` shell environment variable. See [https://github.com/google/jax#current-gotchas](https://github.com/google/jax#current-gotchas)
    for more. `# lax_internal._check_user_dtype_supported(dtype, "array")` `# DeviceArray(1.25844,
    dtype=float32)`'
  id: totrans-178
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: You can use double-precision numbers by setting that in the configuration using
    `[jax_enable_x64]`.
  id: totrans-179
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`set this config at the begining of the program from jax.config import config`'
  id: totrans-180
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`config.update("jax_enable_x64", True)`'
  id: totrans-181
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`x = jnp.float64(1.25844)`'
  id: totrans-182
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`x`'
  id: totrans-183
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`DeviceArray(1.25844, dtype=float64)`'
  id: totrans-184
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '**What is a pytree?**'
  id: totrans-185
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: A pytree is a container that holds Python objects. In JAX, it can hold arrays,
    tuples, lists, dictionaries, etc. A Pytree contains leaves. For example, model
    parameters in JAX are pytrees.
  id: totrans-186
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`example_trees = [`'
  id: totrans-187
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`[1, ''a'', object()],`'
  id: totrans-188
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`(1, (2, 3), ())`'
  id: totrans-189
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`[1, {''k1'': 2, ''k2'': (3, 4)}, 5], {''a'': 2, ''b'': (2, 3)},`'
  id: totrans-190
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`jnp.array([1, 2, 3]),`'
  id: totrans-191
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`]`'
  id: totrans-192
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'Let''s see how many leaves they have:'
  id: totrans-193
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`for pytree in example_trees:`'
  id: totrans-194
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`leaves = jax.tree_leaves(pytree)`'
  id: totrans-195
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`print(f"{repr(pytree):<45} has {len(leaves)} leaves: {leave s}")`'
  id: totrans-196
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`[1, ''a'', <object object at 0x7f280a01f6d0>] [1, ''a'', <object object at
    0x7f280a01f6d0>]`'
  id: totrans-197
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`(1, (2, 3), ())`'
  id: totrans-198
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`[1, 2, 3]`'
  id: totrans-199
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`[1, {''k1'': 2, ''k2'': (3, 4)}, 5]`'
  id: totrans-200
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`[1, 2, 3, 4, 5]`'
  id: totrans-201
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`{''a'': 2, ''b'': (2, 3)}`'
  id: totrans-202
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`[2, 2, 3]`'
  id: totrans-203
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`DeviceArray([1, 2, 3], dtype=int64) [DeviceArray([1, 2, 3], dtype=int64)]
    has 3 leaves:`'
  id: totrans-204
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: 'has 3 leaves:has 5 leaves:has 3 leaves: has 1 leaves:'
  id: totrans-205
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Handling state in JAX**'
  id: totrans-206
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Training machine learning models will often involve state in areas such as model
    parameters, optimizer state, and stateful Layer such as BatchNorm. However, jit-compiled
    functions must have no side effects. We, therefore, need a way to track and update
    model parameters, optimizer state, and stateful layers. The solution is to define
    the state explicitly.
  id: totrans-207
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Loading datasets with JAX**'
  id: totrans-208
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: JAX doesn't ship with any data loading tools. However, JAX recommends using
    data loaders from `PyTorch` and `TensorFlow`.
  id: totrans-209
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: import `tensorflow` as `tf`
  id: totrans-210
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Ensure TF does not see GPU and grab all GPU memory. `tf.config.set_visible_devices([],
    device_type='GPU')`
  id: totrans-211
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: import `tensorflow_datasets` as `tfdsdata_dir = '/tmp/tfds'`
  id: totrans-212
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Fetch full datasets for evaluation
  id: totrans-213
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`tfds.load returns tf.Tensors (or tf.data.Datasets if batch_size != -1)`'
  id: totrans-214
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: You can convert them to NumPy arrays (or iterables of NumPy arrays) with `tfds.dataset_as_numpy`
  id: totrans-215
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`mnist_data, info = tfds.load(name="mnist", batch_size=-1, data_dir=data_dir,
    with_info=True)`'
  id: totrans-216
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`mnist_data = tfds.as_numpy(mnist_data)`'
  id: totrans-217
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`train_data`, `test_data` = `mnist_data[''train'']`, `mnist_data[''test'']`
    `num_labels = info.features[''label''].num_classes`'
  id: totrans-218
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`h, w, c = info.features[''image''].shape`'
  id: totrans-219
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`num_pixels = h * w * c`'
  id: totrans-220
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Full train set
  id: totrans-221
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`train_images`, `train_labels` = `train_data[''image'']`, `train_data[''l abel'']`'
  id: totrans-222
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`train_images = jnp.reshape(train_images, (len(train_images), nu m_pixels))`'
  id: totrans-223
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`train_labels = one_hot(train_labels, num_labels)`'
  id: totrans-224
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Full test set
  id: totrans-225
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`test_images`, `test_labels` = `test_data[''image'']`, `test_data[''labe l'']`'
  id: totrans-226
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`test_images = jnp.reshape(test_images, (len(test_images), num_p ixels))`'
  id: totrans-227
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`test_labels = one_hot(test_labels, num_labels)`'
  id: totrans-228
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`print(''Train:'', train_images.shape, train_labels.shape)` `print(''Test:'',
    test_images.shape, test_labels.shape)`'
  id: totrans-229
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'Train: `(60000, 784)` `(60000, 10)` # Test: `(10000, 784)` `(10000, 10)`'
  id: totrans-230
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: Building neural networks with JAX
  id: totrans-231
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: You can build a model from scratch using JAX. However, various neural network
    libraries are built on top of JAX to make building neural networks with JAX easier.
    The Image classification with JAX & Flax article shows how to load data with PyTorch
    and build a convolutional neural network with Jax and Flax.
  id: totrans-232
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Final thoughts
  id: totrans-233
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: 'In this article, we have covered the basics of JAX. We have seen that JAX uses
    XLA and just-in-time compilation to improve the performance of Python functions.
    Specifically, we have covered:'
  id: totrans-234
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Setting up JAX to use TPUs on Google Colab.
  id: totrans-235
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Comparison between data types in JAX and NumPy. Creating arrays in JAX.
  id: totrans-236
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: How to generate random numbers in JAX.
  id: totrans-237
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Operations on JAX arrays.
  id: totrans-238
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Gotchas in JAX, such as using pure functions and the immutability of JAX arrays.
  id: totrans-239
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Placing JAX arrays in GPUs or TPUs.
  id: totrans-240
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: How to use JIT to speed up functions.
  id: totrans-241
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '...and so much more'
  id: totrans-242
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Optimizers in JAX and Flax
  id: totrans-243
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Optimizers are applied when training neural networks to reduce the error between
    the true and predicted values. This optimization is done via gradient descent.
    Gradient descent adjusts errors in the network through a cost function. In JAX,
    optimizers are applied from the Optax library.
  id: totrans-244
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'Optimizers can be classified into two broad categories:'
  id: totrans-245
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Adaptive such as Adam, Adagrad, AdaDelta, and RMSProp.
  id: totrans-246
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Accelerated stochastic gradient descent (SGD), for example, SGD with momentum,
    heavy-ball method (HB), and Nesterov accelerated gradient (NAG).
  id: totrans-247
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Let's look at common optimizer functions used in JAX and Flax.
  id: totrans-248
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Adaptive vs stochastic gradient descent (SGD) optimizers
  id: totrans-249
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: When performing optimization, adaptive optimizers start with large update steps
    but reduce the step size as they get close to the global minimum. This ensures
    that they don't miss the global minimum.
  id: totrans-250
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'Adaptive optimizers such as Adam are quite common because they converge faster,
    but they may have poor generalization. `SGD-based optimizers apply a global learning
    rate on all parameters, while adaptive optimizers calculate a learning rate for
    each parameter.` `## AdaBelief` The authors of AdaBelief introduced the optimizer
    to:'
  id: totrans-251
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Converge fast as in adaptive methods.
  id: totrans-252
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Have good generalization like SGD.
  id: totrans-253
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Be stable during training.
  id: totrans-254
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`AdaBelief` works on the concept of " **belief**" in the current gradient direction.
    If it results in good performance, then that direction is trusted, and large updates
    are applied. Otherwise, it''s distrusted and the step size is reduced.'
  id: totrans-255
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Let's look at a `Flax training state` that applies the `AdaBelief` optimizer.from
    `flax.training` import `train_state`
  id: totrans-256
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'def `create_train_state`(rng, learning_rate):'
  id: totrans-257
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '"""Creates initial `TrainState`."""'
  id: totrans-258
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: cnn = `CNN()``
  id: totrans-259
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: params = `cnn.init`(rng, `jnp.ones([1, size_image, size_image,`
  id: totrans-260
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`3]))[''params'']`'
  id: totrans-261
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: tx = `optax.adabelief`(learning_rate)
  id: totrans-262
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: return `train_state.TrainState.create`(
  id: totrans-263
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: apply_fn=`cnn.apply`, params=params, tx=tx)Here's the performance of `AdaBelief`
    on various tasks as provided by its authors.![](../images/00007.jpeg)`
  id: totrans-264
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**`AdaGrad`**'
  id: totrans-265
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '`AdaGrad` works well in situations leading to sparse gradients. `Adagrad` is
    an algorithm for gradient-based optimization that anneals the learning rate for
    each parameter during training– `Optax`.'
  id: totrans-266
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: from `flax.training` import `train_state`
  id: totrans-267
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'def `create_train_state`(rng, learning_rate):'
  id: totrans-268
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '"""Creates initial `TrainState`."""'
  id: totrans-269
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: cnn = `CNN()``
  id: totrans-270
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: params = `cnn.init`(rng, `jnp.ones([1, size_image, size_image,`
  id: totrans-271
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`3]))[''params'']`'
  id: totrans-272
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: tx = `optax.AdaGrad`(learning_rate)
  id: totrans-273
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: return `train_state.TrainState.create`(
  id: totrans-274
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: apply_fn=`cnn.apply`, params=params, tx=tx)
  id: totrans-275
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**`Adam – Adaptive moment estimation`**'
  id: totrans-276
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '`Adam` is a common optimizer in deep learning because it gives good results
    with default parameters, is computationally inexpensive, and uses little memory.'
  id: totrans-277
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: from `flax.training` import `train_state`
  id: totrans-278
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'def `create_train_state`(rng):'
  id: totrans-279
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '"""Creates initial `TrainState`."""'
  id: totrans-280
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: model = `LSTMModel()``
  id: totrans-281
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: params = `model.init`(rng, `jnp.array(X_train_padded[0]))['param`
  id: totrans-282
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`s'']`'
  id: totrans-283
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: tx = `optax.adam`(0.001,0.9,0.999,1e-07)
  id: totrans-284
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: return `train_state.TrainState.create`(
  id: totrans-285
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: apply_fn=`model.apply`, params=params, tx=tx)![](../images/00008.jpeg) **`Training
    of multilayer neural networks on MNIST images. (a) Neural networks using dropout
    stochastic regularization. (b) Neural networks with deterministic cost function`**  **##
    `AdamW`**
  id: totrans-286
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`AdamW` is `Adam` with weight decay regularization. Weight decay regularization
    penalizes the cost function making the weights smaller during backpropagarion.
    It results in small weights that lead to better generalization. In some cases,
    `Adam` with decoupled weight decay leads to better results compared `Adam` with
    `L2 regularization`.'
  id: totrans-287
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: from `flax.training` import `train_state`
  id: totrans-288
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'def `create_train_state`(rng):'
  id: totrans-289
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '"""Creates initial `TrainState`."""'
  id: totrans-290
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: model = `LSTMModel()``
  id: totrans-291
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: params = `model.init`(rng, `jnp.array(X_train_padded[0]))['param`
  id: totrans-292
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`s'']tx = optax.adamw(0.001,0.9,0.999,1e-07)return train_state.TrainState.create(apply_fn=model.apply,
    params=params, tx=tx)![](../images/00009.jpeg) **`Learning curves (top row) and
    generalization results (bottom row) obtained by a 26 2x96d ResNet trained with
    Adam and AdamW on CIFAR-10`**  **## `RAdam – Rectified Adam optimizer`** `RAdam` aims
    to solve large variances during the early training stages when applying an adaptive
    learning rate.from `flax.training` import `train_state`'
  id: totrans-293
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def create_train_state(rng, learning_rate):`'
  id: totrans-294
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '"""Creates initial `TrainState`."""'
  id: totrans-295
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`cnn = CNN()`'
  id: totrans-296
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`'
  id: totrans-297
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`3]))[''params'']`'
  id: totrans-298
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`tx = optax.radam(learning_rate)`'
  id: totrans-299
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`return train_state.TrainState.create(`'
  id: totrans-300
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`apply_fn=cnn.apply, params=params, tx=tx)`'
  id: totrans-301
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**AdaFactor**'
  id: totrans-302
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '`AdaFactor is used for training large neural networks because it is implemented
    to reduce memory utilization.from flax.training import train_state`'
  id: totrans-303
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def create_train_state(rng, learning_rate):`'
  id: totrans-304
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '"""Creates initial `TrainState`."""'
  id: totrans-305
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`cnn = CNN()`'
  id: totrans-306
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`'
  id: totrans-307
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`3]))[''params'']`'
  id: totrans-308
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`tx = optax.adafactor(learning_rate)`'
  id: totrans-309
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`return train_state.TrainState.create(`'
  id: totrans-310
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`apply_fn=cnn.apply, params=params, tx=tx)`'
  id: totrans-311
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Fromage**'
  id: totrans-312
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '`Fromage introduces a distance function on deep neural networks called* **deep
    relative trust**. *It requires little to no learning rate tuning.from flax.training
    import train_state`'
  id: totrans-313
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def create_train_state(rng, learning_rate): """Creates initial `TrainState`."""
    cnn = CNN()`'
  id: totrans-314
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`'
  id: totrans-315
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`3]))[''params'']`'
  id: totrans-316
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`tx = optax.fromage(learning_rate)`'
  id: totrans-317
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`return train_state.TrainState.create(`'
  id: totrans-318
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`apply_fn=cnn.apply, params=params, tx=tx)`'
  id: totrans-319
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Lamb – Layerwise adaptive large batch optimization**'
  id: totrans-320
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '`Lamb aims to enable the training of deep neural networks by computing gradients
    using large mini-batches. It leads to good performance on attention-based models
    such as Transformers and ResNet-50.`'
  id: totrans-321
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from flax.training import train_state`'
  id: totrans-322
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def create_train_state(rng, learning_rate):`'
  id: totrans-323
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '"""Creates initial `TrainState`."""'
  id: totrans-324
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`cnn = CNN()`'
  id: totrans-325
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`'
  id: totrans-326
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`3]))[''params'']`'
  id: totrans-327
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`tx = optax.lamb(learning_rate)`'
  id: totrans-328
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`return train_state.TrainState.create(`'
  id: totrans-329
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`apply_fn=cnn.apply, params=params, tx=tx)`'
  id: totrans-330
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Lars – Layer-wise Adaptive Rate Scaling**'
  id: totrans-331
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '`Lars is inspired by Lamb to scale SGD to large batch sizes. Lars has been
    used to train AlexNet with an 8K batch size and Resnet-50 with a 32K batch size
    without degrading the accuracy. from flax.training import train_state`'
  id: totrans-332
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def create_train_state(rng, learning_rate):`'
  id: totrans-333
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '"""Creates initial `TrainState`."""'
  id: totrans-334
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`cnn = CNN()`'
  id: totrans-335
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`'
  id: totrans-336
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`3]))[''params'']`'
  id: totrans-337
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`tx = optax.lars(learning_rate)`'
  id: totrans-338
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`return train_state.TrainState.create(`'
  id: totrans-339
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`apply_fn=cnn.apply, params=params, tx=tx)![](../images/00010.jpeg)`**LARS:
    Alexnet-BN with B=8K ** **## SM3 - Square-root of Minima of Sums of Maxima of
    Squaredgradients Method**'
  id: totrans-340
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`SM3 was designed to reduce memory utilization when training very large models
    such as Transformer for machine translation, BERT for language modeling, and AmoebaNet-D
    for image classification`'
  id: totrans-341
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from flax.training import train_state`'
  id: totrans-342
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def create_train_state(rng, learning_rate):`'
  id: totrans-343
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '"""Creates initial `TrainState`."""'
  id: totrans-344
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`cnn = CNN()`'
  id: totrans-345
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`'
  id: totrans-346
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`3]))[''params'']`'
  id: totrans-347
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`tx = optax.sm3(learning_rate)`'
  id: totrans-348
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`return train_state.TrainState.create(`'
  id: totrans-349
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: apply_fn=cnn.apply, params=params, tx=tx)![](../images/00011.jpeg)**Top-1 (left)
    and Top-5 (right) test accuracy of AmoebaNet-D on ImageNet ** **## SGD– Stochastic
    Gradient Descent**
  id: totrans-350
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: SDG implements stochastic gradient descent with support for momentum and Nesterov
    acceleration. Momentum makes obtaining optimal model weights faster by accelerating
    gradient descent in a certain direction.
  id: totrans-351
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: from flax.training import train_state
  id: totrans-352
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'def create_train_state(rng, learning_rate):'
  id: totrans-353
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '"""Creates initial `TrainState`."""'
  id: totrans-354
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: cnn = CNN()
  id: totrans-355
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: params = cnn.init(rng, jnp.ones([1, size_image, size_image,
  id: totrans-356
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 3]))['params']
  id: totrans-357
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: tx = optax.sgd(learning_rate)
  id: totrans-358
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: return train_state.TrainState.create(
  id: totrans-359
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: apply_fn=cnn.apply, params=params, tx=tx)
  id: totrans-360
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Noisy SGD**'
  id: totrans-361
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Noisy SGD is SGD with added noise. Adding noise to gradients can prevent overfitting
    and improve training error and generalization in deep architectures.
  id: totrans-362
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: from flax.training import train_state
  id: totrans-363
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'def create_train_state(rng, learning_rate):'
  id: totrans-364
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '"""Creates initial `TrainState`."""'
  id: totrans-365
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: cnn = CNN()
  id: totrans-366
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: params = cnn.init(rng, jnp.ones([1, size_image, size_image,
  id: totrans-367
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 3]))['params']
  id: totrans-368
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: tx = optax.noisy_sgd(learning_rate)
  id: totrans-369
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: return train_state.TrainState.create(
  id: totrans-370
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'apply_fn=cnn.apply, params=params, tx=tx)![](../images/00012.jpeg) **: Noise
    vs. No Noise in our experiment with tables containing 5 columns. The models trained
    with noise generalizes almost always better**  **## Optimistic GD** An Optimistic
    Gradient Descent optimizer.'
  id: totrans-371
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '*Optimistic gradient descent is an approximation of extra-gradient methods
    which require multiple gradient calls to compute the next update. It has strong
    formal guarantees for last-iterate convergence in min-max games, for which standard
    gradient descent can oscillate or even diverge– Optax.*'
  id: totrans-372
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: from flax.training import train_state
  id: totrans-373
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'def create_train_state(rng, learning_rate):'
  id: totrans-374
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '"""Creates initial `TrainState`."""'
  id: totrans-375
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: cnn = CNN()
  id: totrans-376
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: params = cnn.init(rng, jnp.ones([1, size_image, size_image, tx = optax.optimistic_gradient_descent(learning_rate)
    return train_state.TrainState.create(
  id: totrans-377
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: apply_fn=cnn.apply, params=params, tx=tx)
  id: totrans-378
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Differentially Private SGD**'
  id: totrans-379
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Differentially Private SGD is used for training networks with sensitive data.
    It ensures that the models don't expose sensitive training data.
  id: totrans-380
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: from flax.training import train_state
  id: totrans-381
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'def create_train_state(rng, learning_rate):'
  id: totrans-382
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '"""Creates initial `TrainState`."""'
  id: totrans-383
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: cnn = CNN()
  id: totrans-384
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: params = cnn.init(rng, jnp.ones([1, size_image, size_image,
  id: totrans-385
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 3]))['params']
  id: totrans-386
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: tx = optax.dpsgd(learning_rate)
  id: totrans-387
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: return train_state.TrainState.create(
  id: totrans-388
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: apply_fn=cnn.apply, params=params, tx=tx)
  id: totrans-389
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**RMSProp**'
  id: totrans-390
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: RMSProp works by dividing the gradient of a running average of its recent magnitude–Hinton.from
    flax.training import train_state
  id: totrans-391
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'def create_train_state(rng, learning_rate):'
  id: totrans-392
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '"""Creates initial `TrainState`."""'
  id: totrans-393
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: cnn = CNN()
  id: totrans-394
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: params = cnn.init(rng, jnp.ones([1, size_image, size_image, tx = optax.rmsprop(learning_rate)
  id: totrans-395
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: return train_state.TrainState.create(
  id: totrans-396
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: apply_fn=cnn.apply, params=params, tx=tx)
  id: totrans-397
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Yogi**'
  id: totrans-398
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '`Yogi is a modified Adam optimizer for optimizing the stochastic nonconvex
    optimization problem.from flax.training import train_state`'
  id: totrans-399
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def create_train_state(rng, learning_rate):`'
  id: totrans-400
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '"""Creates initial `TrainState`."""'
  id: totrans-401
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`cnn = CNN()`'
  id: totrans-402
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`'
  id: totrans-403
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`3]))[''params'']`'
  id: totrans-404
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`tx = optax.yogi(learning_rate)`'
  id: totrans-405
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`return train_state.TrainState.create(`'
  id: totrans-406
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`apply_fn=cnn.apply, params=params, tx=tx)![](../images/00013.jpeg)` **Comparison
    of highly tuned RMSProp optimizer with YOGI for Inception-Resnet-v2 on Imagenet.
    First plot shows the mini-batch estimate of the loss during training, while the
    remaining two plots show top-1 and top-5 error rates on the held-out Imagenet
    validation set**  **## Final thoughts**'
  id: totrans-407
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'Choosing the right optimizer function determines how long training a network
    will take. It also determines how well the model performs. Choosing the appropriate
    optimizer functions is therefore paramount. This article discusses various optimizer
    functions that you can apply to your JAX and Flax networks. In particular, you
    walk away with nuggets about these optimizers:'
  id: totrans-408
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Adam optimizer in JAX.
  id: totrans-409
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: RMSProp optimizer in Flax.
  id: totrans-410
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Stochastic Gradient Descent in JAX.
  id: totrans-411
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`..to mention a few.`'
  id: totrans-412
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**JAX loss functions**'
  id: totrans-413
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Loss functions are at the core of training machine learning. They can be used
    to identify how well the model is performing on a dataset. Poor performance leads
    to a very high loss, while a well-performing model will have a lower loss. Therefore,
    the choice of a loss function is an important one when building machine learning
    models.
  id: totrans-414
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: In this article, we'll look at the loss functions available in JAX and how you
    can use them.
  id: totrans-415
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**What is a loss function?**'
  id: totrans-416
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Machine learning models learn by evaluating predictions against true values
    and adjusting the weights. The objective is to obtain the weights that minimize
    the **loss function**, that is the **error**. The loss function is also referred
    to as the **cost function**. The choice of a loss function depends on the problem.
    The two most common problems are classification and regression problems. Each
    will require a different set of loss functions.
  id: totrans-417
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Creating custom loss functions in JAX**'
  id: totrans-418
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: When training networks with JAX, you'll need to obtain the logits at the training
    stage. These logits are used for computing the loss. You'll then need to evaluate
    the loss function and its gradient. The gradient is used to update the model parameters.
    At this point, you can compute the training metrics for the model.
  id: totrans-419
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 💡 **What are logits?** Logits are unnormalized log probabilities.*
  id: totrans-420
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def compute_loss(params,images,labels):`'
  id: totrans-421
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`logits = CNN().apply({''params'': params}, images) loss = cross_entropy_loss(logits=logits,
    labels=labels) return loss, logits`'
  id: totrans-422
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`@jax.jitdef train_step(state,images, labels): """Train for a single step."""
    (_, logits), grads = jax.value_and_grad(compute_loss, has_aux`'
  id: totrans-423
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`=True)(state.params,images,labels)`'
  id: totrans-424
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`state = state.apply_gradients(grads=grads)`'
  id: totrans-425
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: metrics = compute_metrics(logits=logits, labels=labels) return state, metrics
  id: totrans-426
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: You can use JAX functions such as `[log_sigmoid]` and `[log_softmax]` to build
    custom loss functions. You can even write your loss functions from scratch without
    using these functions.
  id: totrans-427
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Here is an example of computing the sigmoid binary cross entropy loss.
  id: totrans-428
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import jax`'
  id: totrans-429
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def custom_sigmoid_binary_cross_entropy(logits, labels): log_p = jax.nn.log_sigmoid(logits)`'
  id: totrans-430
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`log_not_p = jax.nn.log_sigmoid(-logits)`'
  id: totrans-431
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`return -labels * log_p - (1\. - labels) * log_not_p`'
  id: totrans-432
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`custom_sigmoid_binary_cross_entropy(0.5,0.0) # DeviceArray(0.974077, dtype=float32,
    weak_type=True)`'
  id: totrans-433
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Which loss functions are available in JAX?**'
  id: totrans-434
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Building custom loss functions for your networks can introduce errors in your
    program. Furthermore, you have to take the burden of maintaining these functions.
    However, if the loss function you want is unavailable, there is a strong case
    for creating a custom loss function. Be that as it may, there is no need to reinvent
    the wheel and rewrite the already implemented loss functions.
  id: totrans-435
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: JAX doesn't ship with any loss functions. In JAX, we use `optax` for defining
    loss functions. It's important to ensure that you use JAXcompatible libraries
    to take advantage of functions such as `[JIT]`, `[vmap]` and `[pmap]` that make
    your programs faster.
  id: totrans-436
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Let's take a look at some of the loss functions available in `optax`.
  id: totrans-437
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Sigmoid binary cross entropy**'
  id: totrans-438
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: The sigmoid binary cross entropy loss is computed
  id: totrans-439
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: using `optax.sigmoid_binary_cross_entropy`. The function expects
  id: totrans-440
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`logits` and `class labels`. It is used in problems where the classes are not
    mutually exclusive. For example, the model can predict that the image contains
    two objects in an image classification problem.'
  id: totrans-441
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`optax.sigmoid_binary_cross_entropy(0.5,0.0)# DeviceArray(0.974077, dtype=float32,
    weak_type=True)`'
  id: totrans-442
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Softmax cross entropy**'
  id: totrans-443
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: The softmax cross entropy function is used where the classes are mutually exclusive.
    For example, in the MNIST dataset, each digit has exactly one label. The function
    expects an array of logits and probability distributions. The probability distribution
    sum to 1.
  id: totrans-444
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`logits = jnp.array([0.50,0.60,0.70,0.30,0.25]) labels = jnp.array([0.20,0.30,0.10,0.20,0.2])
    optax.softmax_cross_entropy(logits,labels) # DeviceArray(1.6341426, dtype=float32)`'
  id: totrans-445
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Cosine distance**'
  id: totrans-446
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: The cosine distance measures the cosine distance between targets and predictions.
  id: totrans-447
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`predictions = jnp.array([0.50,0.60,0.70,0.30,0.25]) targets = jnp.array([0.20,0.30,0.10,0.20,0.2])
    optax.cosine_distance(predictions,targets,epsilon=0.7) # DeviceArray(0.4128204,
    dtype=float32)`'
  id: totrans-448
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Cosine similarity**'
  id: totrans-449
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: The cosine similarity loss measures the cosine similarity between the true and
    predicted values. The cosine similarity is the cosine of the angle between two
    vectors. This is obtained by the dot product of the vectors divided by the product
    of their lengths.
  id: totrans-450
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The result is a number between -1 and 1\. 0 shows orthogonality, while numbers
    closer to -1 indicate similarity. Numbers close to  1 portray high dissimilarity.
  id: totrans-451
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`predictions = jnp.array([0.50,0.60,0.70,0.30,0.25]) targets = jnp.array([0.20,0.30,0.10,0.20,0.2])
    optax.cosine_similarity(predictions,targets,epsilon=0.5) # DeviceArray(0.8220514,
    dtype=float32)`'
  id: totrans-452
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**`Huber loss`**'
  id: totrans-453
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: The Huber loss is used for regression problems. It is less sensitive to outliers
    compared to the squared error loss. A variant of the Huber loss that can be used
    in classification problems exists.
  id: totrans-454
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`logits = jnp.array([0.50,0.60,0.70,0.30,0.25])`'
  id: totrans-455
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`labels = jnp.array([0.20,0.30,0.10,0.20,0.2])`'
  id: totrans-456
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`optax.huber_loss(logits,labels)`'
  id: totrans-457
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`DeviceArray([0.045 , 0.045 , 0.17999998, 0.005 ,`'
  id: totrans-458
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`0.00125 ], dtype=float32)`'
  id: totrans-459
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**`l2 loss`**'
  id: totrans-460
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: L2 loss function is the Least Square Errors. The L2 loss aims at minimizing
    the sum of the squared differences between the true and predicted values. The
    Mean Squared Error is the mean of all L2 loss values.
  id: totrans-461
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`predictions = jnp.array([0.50,0.60,0.70,0.30,0.25]) targets = jnp.array([0.20,0.30,0.10,0.20,0.2])`'
  id: totrans-462
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`optax.l2_loss(predictions,targets)`'
  id: totrans-463
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`DeviceArray([0.045 , 0.045 , 0.17999998, 0.005 ,`'
  id: totrans-464
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`0.00125 ], dtype=float32)`'
  id: totrans-465
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**`log cosh`**'
  id: totrans-466
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '`[`log_cosh`]` is the logarithm of the hyperbolic cosine of the prediction
    error.'
  id: totrans-467
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 💡 `log(cosh(x))` is approximately equal to `(x ** 2) / 2` for small `x` and
    to `abs(x) - log(2)` for large `x`. This means that 'logcosh' works mostly like
    the mean squared error, but will not be so strongly affected by the occasional
    wildly incorrect prediction. TensorFlow Docs*
  id: totrans-468
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`predictions = jnp.array([0.50,0.60,0.70,0.30,0.25]) targets = jnp.array([0.20,0.30,0.10,0.20,0.2])`'
  id: totrans-469
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`optax.log_cosh(predictions,targets)`'
  id: totrans-470
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`DeviceArray([0.04434085, 0.04434085, 0.17013526, 0.00499171,`'
  id: totrans-471
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`0.00124949], dtype=float32)`'
  id: totrans-472
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**`Smooth labels`**'
  id: totrans-473
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '`[`optax.smooth_labels`]`  is used together with a cross-entropy loss to smooth
    labels. It returns a smoothed version of the one hot input labels. Label smoothing
    has been applied in image classification, language translation, and speech recognition
    to prevent models from becoming overconfident.'
  id: totrans-474
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`labels = jnp.array([0.20,0.30,0.10,0.20,0.2])`'
  id: totrans-475
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`optax.smooth_labels(labels,alpha=0.4)`'
  id: totrans-476
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`DeviceArray([0.2 , 0.26, 0.14, 0.2 , 0.2 ], dtype=float32)`'
  id: totrans-477
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '**`Computing loss with JAX Metrics`**'
  id: totrans-478
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: JAX Metrics is an open-source package for computing losses and metrics in JAX.
    It provides a Keras-like API for computing model loss and metrics.
  id: totrans-479
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: For example, here is how you use the library to compute the crossentropy loss.
    Similar to Keras, the losses can be computed by either instantiating the [`Loss`] or [`loss`].
  id: totrans-480
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`pip install jax_metrics import jax_metrics as jm`'
  id: totrans-481
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`crossentropy = jm.losses.Crossentropy() logits = jnp.array([0.50,0.60,0.70,0.30,0.25])
    y = jnp.array([0.50,0.60,0.70,0.30,0.25]) crossentropy(target=y, preds=logits)`'
  id: totrans-482
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`DeviceArray(3.668735, dtype=float32)`'
  id: totrans-483
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: 变量名`logits`赋值`jnp.array([0.50,0.60,0.70,0.30,0.25])` `y`赋值`jnp.array([0.50,0.60,0.70,0.30,0.25])`
    `jm.losses.crossentropy`(`target=y`, `preds=logits`) `# DeviceArray(3.668735,
    dtype=float32)`
  id: totrans-484
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Here is what the code would like in a JAX training step.import jax_metrics
    as jmmetric = jm.metrics.Accuracy()@jax.jitdef init_step`(`metric: jm.Metric`)
    -> `jm.Metric`: return `metric.init()`'
  id: totrans-485
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '函数名`loss_fn`，参数`params`，`metric`，`x`，`y`: ...'
  id: totrans-486
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 变量名`metric`赋值`metric.update`，参数`target=y`，`preds=logits`...
  id: totrans-487
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: return `loss`, `metric`@jax.jitdef train_step`(`params`, `metric`, `x`, `y`):grads,
    metric = jax.grad`(`loss_fn`, `has_aux=True`)(
  id: totrans-488
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`params`, `metric`, `x`, `y`'
  id: totrans-489
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: )
  id: totrans-490
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '...'
  id: totrans-491
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: return `params`, `metric`
  id: totrans-492
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`@jax.jitdef reset_step`(`metric: jm.Metric`) -> `jm.Metric`: return `metric.reset()`The
    losses we have seen earlier can also be computed using JAX Metrics.'
  id: totrans-493
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`! pip install jax_metrics`'
  id: totrans-494
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import jax_metrics as jm`'
  id: totrans-495
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 变量名`target`赋值`jnp.array([50,60,70,30,25])` `preds`赋值`jnp.array([0.50,0.60,0.70,0.30,0.25])`
    `huber_loss`赋值`jm.losses.Huber()`
  id: totrans-496
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`huber_loss`(`target=target`, `preds=preds`) `# DeviceArray(46.030003, dtype=float32)`'
  id: totrans-497
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 变量名`target`赋值`jnp.array([50,60,70,30,25])`
  id: totrans-498
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 变量名`preds`赋值`jnp.array([0.50,0.60,0.70,0.30,0.25])`
  id: totrans-499
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`jm.losses.mean_absolute_error`(`target=target`, `preds=preds`) `# DeviceArray(46.530003,
    dtype=float32)`'
  id: totrans-500
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`rng`赋值`jax.random.PRNGKey(42)`'
  id: totrans-501
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 变量名`target`赋值`jax.random.randint(rng, shape=(2, 3), minval=0, maxval =2)`
  id: totrans-502
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 变量名`preds`赋值`jax.random.uniform(rng, shape=(2, 3))`
  id: totrans-503
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`jm.losses.cosine_similarity`(`target`, `preds`, `axis=1`)'
  id: totrans-504
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`DeviceArray`([-0.8602638 , -0.33731455], dtype=float32) `target`赋值`jnp.array([50,60,70,30,25])`'
  id: totrans-505
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: 变量名`preds`赋值`jnp.array([0.50,0.60,0.70,0.30,0.25])`
  id: totrans-506
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`jm.losses.mean_absolute_percentage_error`(`target=target`, `preds=p reds`)'
  id: totrans-507
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`DeviceArray(98.99999, dtype=float32)`'
  id: totrans-508
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: 变量名`target`赋值`jnp.array([50,60,70,30,25])`
  id: totrans-509
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 变量名`preds`赋值`jnp.array([0.50,0.60,0.70,0.30,0.25])`
  id: totrans-510
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`jm.losses.mean_squared_logarithmic_error`(`target=target`, `preds=p reds`)'
  id: totrans-511
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`DeviceArray(11.7779, dtype=float32)`'
  id: totrans-512
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`target`赋值`jnp.array([0.50,0.60,0.70,0.30,0.25])` `preds`赋值`jnp.array([0.50,0.60,0.70,0.30,0.25])`
    `jm.losses.mean_squared_error`(`target=target`, `preds=preds`) `# DeviceArray(0.,
    dtype=float32)`'
  id: totrans-513
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**How to monitor JAX loss functions**'
  id: totrans-514
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Monitoring the loss of your network is important because it indicates whether
    it's learning or not. A glance at the loss can tell you if there are any problems
    in the network, such as overfitting. One way to monitor the loss is to print the
    training and validation loss as the network is training.
  id: totrans-515
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`![](`../images/00014.jpeg`)`'
  id: totrans-516
  prefs: []
  stylish: true
  type: TYPE_IMG
- en: You can also plot the training and validation loss to represent the training
    visually.
  id: totrans-517
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`![](`../images/00015.jpeg`)`'
  id: totrans-518
  prefs: []
  stylish: true
  type: TYPE_IMG
- en: '**Why JAX loss nan happens**'
  id: totrans-519
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: JAX will not show errors when NANs occur in your program. This is by design
    because of the complexities involved in showing errors from accelerators. When
    debugging, you can turn on the NAN checker to show NAN errors. NANs should be
    fixed because the network stops learning when they occur.
  id: totrans-520
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from jax.config import config`'
  id: totrans-521
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`config.update`(`"jax_debug_nans"`, `True`)'
  id: totrans-522
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`jnp.divide`(`0.0`, `0.0`)'
  id: totrans-523
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`FloatingPointError`: invalid value (`nan`) encountered in `div`'
  id: totrans-524
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: 'However, what produces NANs in a network. There are various factors, not limited
    to:'
  id: totrans-525
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The dataset has not been scaled.
  id: totrans-526
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: There are NANs in the training set. The occurrence of infinite values in the
    training data.
  id: totrans-527
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Wrong optimizer function. Exploding gradients leading to large updates to training
    weights. Using a very large learning rate.
  id: totrans-528
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`Final thoughts`'
  id: totrans-529
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: 'In this article, we have seen that choosing the right loss function is critical
    to the learning of a network. We have also discussed various loss functions in
    JAX. More precisely, we have coved:'
  id: totrans-530
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: What is a loss function?
  id: totrans-531
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: How to create custom loss functions in JAX.
  id: totrans-532
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Loss functions available in JAX.
  id: totrans-533
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Computing loss with JAX Metrics.
  id: totrans-534
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Monitoring loss in JAX.
  id: totrans-535
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: How to avoid NANs in JAX.
  id: totrans-536
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`Activation functions in JAX and Flax`'
  id: totrans-537
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Activation functions are applied in neural networks to ensure that the network
    outputs the desired result. The activations functions cap the output within a
    specific range. For instance, when solving a binary classification problem, the
    outcome should be a number between 0 and 1\. This indicates the probability of
    an item belonging to either of the two classes.
  id: totrans-538
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: However, in a regression problem, you want the numerical prediction of a quantity,
    for example, the price of an item. You should, therefore, choose an appropriate
    activation function for the problem being solved.
  id: totrans-539
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Let's look at common activation functions in JAX and Flax.
  id: totrans-540
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`ReLU – Rectified linear unit`'
  id: totrans-541
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: The  **ReLU activation function** is primarily used in the hidden layers of
    neural networks to ensure non-linearity. The function caps all outputs to zero
    and above. Outputs below zero are returned as zero, while numbers above zero are
    returned as they are. This ensures that there are no negative numbers in the network.
  id: totrans-542
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: On line 9 we apply the ReLu activation function after the convolution layer.
  id: totrans-543
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import flaxfrom flax import linen as nnclass CNN(nn.Module):`'
  id: totrans-544
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`@nn.compact`'
  id: totrans-545
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def __call__(self, x):`'
  id: totrans-546
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`x = nn.Conv(features=32, kernel_size=(3, 3))(x)`'
  id: totrans-547
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`x = nn.relu(x)`'
  id: totrans-548
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))`'
  id: totrans-549
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`x = nn.Conv(features=64, kernel_size=(3, 3))(x)`'
  id: totrans-550
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`x = nn.relu(x)`'
  id: totrans-551
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))`'
  id: totrans-552
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`x = x.reshape((x.shape[0], -1))`'
  id: totrans-553
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`x = nn.Dense(features=256)(x)`'
  id: totrans-554
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`x = nn.relu(x)`'
  id: totrans-555
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`x = nn.Dense(features=2)(x)`'
  id: totrans-556
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`x = nn.log_softmax(x)`'
  id: totrans-557
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: return x
  id: totrans-558
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`PReLU– Parametric Rectified Linear Unit`'
  id: totrans-559
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '`Parametric Rectified Linear Unit`  is ReLU with extra parameters equal to
    the number of channels. It works by introducing *a– *a learnable parameter. PReLU
    allows for non-negative values.'
  id: totrans-560
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`![](../images/00016.gif)x = nn.PReLU(x)`'
  id: totrans-561
  prefs: []
  stylish: true
  type: TYPE_IMG
- en: '`Sigmoid`'
  id: totrans-562
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: The  **sigmoid activation function **caps output to a number between 0 and 1
    and is mainly used for binary classification tasks. Sigmoid is used where the
    classes are non-exclusive. For example, an image can have a car, a building, a
    tree, etc. Just because there is a car in the image doesn’t mean a tree can’t
    be in the picture. Use the sigmoid function when there is more than one correct
    answer.
  id: totrans-563
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: x = `nn.sigmoid(x)`
  id: totrans-564
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Log sigmoid**'
  id: totrans-565
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '**Log sigmoid** computes the log of the sigmoid activation, and its output
    is within the range of −∞ to 0.![](../images/00017.gif)x = `nn.log_sigmoid(x)`'
  id: totrans-566
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Softmax**'
  id: totrans-567
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: The  **softmax activation function** is a variant of the sigmoid function used
    in multi-class problems where labels are mutually exclusive. For example, a picture
    is either grayscale or color. Use the softmax activation when there is only one
    correct answer.
  id: totrans-568
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: x = `nn.softmax(x)`
  id: totrans-569
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Log softmax**'
  id: totrans-570
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '**Log softmax **computes the logarithm of the softmax function, which rescales
    elements to the range −∞ to 0.![](../images/00018.gif)x = `nn.log_softmax(x)`'
  id: totrans-571
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**ELU – Exponential linear unit activation**'
  id: totrans-572
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '**ELU activation**  function helps in solving the vanishing and exploding gradients
    problem. Unlike ReLu, ELU allows negative numbers pushing the mean unit activations
    closer to zero. ELUs may lead to faster training and better generalization in
    networks with more than five layers.'
  id: totrans-573
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: For values above zero the number is returned as is but for numbers below zeros
    they are a number that is less that but close to zero.![](../images/00019.gif)x
    = `nn.elu(x)`
  id: totrans-574
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**CELU – Continuouslydifferentiable exponential linear unit**'
  id: totrans-575
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '`CELU` is ELU that is continuously differentiable.![](../images/00020.gif)x
    = `nn.celu(x)`'
  id: totrans-576
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**GELU– Gaussian error linear unit activation**'
  id: totrans-577
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '**GELU** non-linearity weights inputs by their value rather than gates inputs
    by their sign as in ReLU– Source.![](../images/00021.gif)x = `nn.gelu(x)` ![](../images/00022.jpeg)'
  id: totrans-578
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**GLU – Gated linear unit activation**'
  id: totrans-579
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '**GLU**  is computed as [`GLU ( a , b )= a ⊗ σ ( b )`]. It has been applied
    in Gated CNNs for natural language processing. In the formula, the b gate controls
    what information is passed to the next layer. GLU helps tackle the vanishing gradient
    problem.'
  id: totrans-580
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: x = `nn.glu(x)`
  id: totrans-581
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Soft sign**'
  id: totrans-582
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: The  **Soft sign** activation function caps values between -1 and 1\. It is
    similar to the hyperbolic tangent activation function– tanh. The difference is
    that tanh converges exponentially while  Soft sign converges polynomially.
  id: totrans-583
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: x = `nn.soft_sign(x)` ![](../images/00023.gif)
  id: totrans-584
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Softplus**'
  id: totrans-585
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: The **Softplus activation** returns values as zero and above. It is a smooth
    version of the ReLu. x = `nn.soft_plus(x)` ![](../images/00024.gif)![](../images/00025.jpeg)**The
    Softplus activation ** **## Swish–Sigmoid Linear Unit( SiLU)**
  id: totrans-586
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The SiLU activation function is computed as [`x * sigmoid(beta * x)`] where
    beta is the hyperparameter for Swish activation function. SiLU, is, therefore,
    computed by multiplying the sigmoid function with its input.
  id: totrans-587
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: x = `nn.swish(x)`![](../images/00026.gif)
  id: totrans-588
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Custom activation functions in JAX and Flax**'
  id: totrans-589
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: 'You can also define custom activation functions in JAX. For example, here''s
    how you''d define the LeakyReLu activation function.from flax import linen as
    nnimport jax.numpy as jnpclass `LeakyReLU`(nn.Module):`alpha` : float = 0.1def
    `__call__`(self, `x`):return jnp.where(`x` > 0, `x`, `self.alpha` * `x`)'
  id: totrans-590
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Final thoughts**'
  id: totrans-591
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: You have learned about the various activation functions you can use in JAX and
    Flax. You have also seen that you can create new functions by creating a class
    that implements the`[__call__]`method.
  id: totrans-592
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: How to load datasets in`JAX`with`TensorFlow`
  id: totrans-593
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '`JAX`doesn''t ship with data loading utilities. This keeps`JAX`focused on providing
    a fast tool for building and training machine learning models. Loading data in`JAX`is
    done using'
  id: totrans-594
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: either`TensorFlow`or`PyTorch`. This article will focus on how to load datasets
    in`JAX`using`TensorFlow`.
  id: totrans-595
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Let's dive in!
  id: totrans-596
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: How to load text data in`JAX`
  id: totrans-597
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Let's use the`IMDB dataset from Kaggle`to illustrate how to load text datasets
    with`JAX`. We'll use the`Kaggle`Python`library to download the data. That requires
    your Kaggle username and key. Head over
  id: totrans-598
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`https://www.kaggle.com/your_username/ account to obtain the API`'
  id: totrans-599
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: key.
  id: totrans-600
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The library downloads the data as a zip file. We'll therefore extract it afterward.
  id: totrans-601
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: import`os`
  id: totrans-602
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Obtain from`https://www.kaggle.com/username/account os.environ["KAGGLE_USERNAME"]="YOUR_KAGGLE_USERNAME"
    os.environ["KAGGLE_KEY"]="YOUR_KAGGLE_KEY"`
  id: totrans-603
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: import`kaggle`
  id: totrans-604
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '!`kaggle datasets download lakshmi25npathi/imdb-dataset-of-50k-m ovie-reviews`'
  id: totrans-605
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: import`zipfile`
  id: totrans-606
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: with zipfile.ZipFile('imdb-dataset-of-50k-movie-reviews.zip',
  id: totrans-607
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '''r'') as zip_ref:zip_ref.extractall(''imdb-dataset-of-50k-movie-reviews'')Next,
    import the standard data science packages and view a sample of the data.'
  id: totrans-608
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: import`numpy as np`
  id: totrans-609
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: import`pandas`as`pd`
  id: totrans-610
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: from`numpy`import`array`
  id: totrans-611
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: import`tensorflow as tf`
  id: totrans-612
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: from`sklearn.model_selection`import`train_test_split`
  id: totrans-613
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: from`sklearn.preprocessing`import`LabelEncoder`
  id: totrans-614
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: import`matplotlib.pyplot`as`plt`
  id: totrans-615
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`df = pd.read_csv("imdb-dataset-of-50k-movie-reviews/IMDB Dataset.c df.head()`'
  id: totrans-616
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '!`[](../images/00027.jpeg)`'
  id: totrans-617
  prefs: []
  stylish: true
  type: TYPE_IMG
- en: Clean the text data
  id: totrans-618
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Let's do some processing of the data before we proceed to load it using`TensorFlow`.
    Standard processing in text problems is to remove stop words. Stop words are common
    words such as`a`,`the`that don't help the model in identifying the polarity of
    a
  id: totrans-619
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: sentence.`NLTK`provides the stops words. We can, therefore, write a function
    to remove them from the IMDB dataset.
  id: totrans-620
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: import`nltk`
  id: totrans-621
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: from`nltk.corpus`import`stopwords`
  id: totrans-622
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`nltk.download(''stopwords'')`'
  id: totrans-623
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def remove_stop_words(review):`'
  id: totrans-624
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`review_minus_sw = []`'
  id: totrans-625
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`stop_words = stopwords.words(''english'')`'
  id: totrans-626
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`review = review.split()`'
  id: totrans-627
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`cleaned_review = [review_minus_sw.append(word) for word in`'
  id: totrans-628
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`review if word not in stop_words]`'
  id: totrans-629
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`cleaned_review = '' ''.join(review_minus_sw)`'
  id: totrans-630
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`return cleaned_review`'
  id: totrans-631
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`df[''review''] = df[''review''].apply(remove_stop_words) view raw`'
  id: totrans-632
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Label encode the sentiment column
  id: totrans-633
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Convert the sentiment column to numerical representation using`Scikit-learn's
    label encoder. This is important because neural networks expect numerical data.
  id: totrans-634
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`labelencoder = LabelEncoder()`'
  id: totrans-635
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`df = df.assign(sentiment = labelencoder.fit_transform(df["senti ment"]))`'
  id: totrans-636
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '!`[](../images/00028.jpeg)`'
  id: totrans-637
  prefs: []
  stylish: true
  type: TYPE_IMG
- en: Text preprocessing with`TensorFlow`
  id: totrans-638
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: We have converted the sentiment column to a numerical
  id: totrans-639
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: representation. However, the reviews are still in text form. We need to convert
    them to numbers as well.
  id: totrans-640
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: We start by splitting the dataset into a training and testing set.
  id: totrans-641
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: from`sklearn.model_selection`import`train_test_split df = df.drop_duplicates()`
  id: totrans-642
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`docs = df[''review'']`'
  id: totrans-643
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`labels = array(df[''sentiment''])`'
  id: totrans-644
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`X_train, X_test , y_train, y_test = train_test_split(docs, labe ls , test_size
    = 0.20, random_state=0)`'
  id: totrans-645
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'Next, we use TensorFlow''s`TextVectorization`function to convert the text data
    to integer representations. The function expects:'
  id: totrans-646
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`[standardize]`used to specify how the text data is processed. For example,
    the`[lower_and_strip_punctuation]`option will lowercase the data and remove punctuations.'
  id: totrans-647
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`max_tokens`dictates the maximum size of the vocabulary. `[output_mode]`determines
    the output of the vectorization layer. Setting`[int]`outputs integers.'
  id: totrans-648
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`[output_sequence_length]`indicates the maximum length of the output sequence.
    This ensures that all sequences have the same length.'
  id: totrans-649
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: import`tensorflow as tf`
  id: totrans-650
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`max_features = 5000`# Maximum vocab size.'
  id: totrans-651
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`batch_size = 32`'
  id: totrans-652
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`max_len = 512`# Sequence length to pad the outputs to. `vectorize_layer =
    tf.keras.layers.TextVectorization(standardize =''lower_and_strip_punctuation'',max_tokens=max_features,output_m
    ode=''int'',output_sequence_length=max_len)`'
  id: totrans-653
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`vectorize_layer.adapt(X_train, batch_size=None)`'
  id: totrans-654
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Next, apply this layer to the training and testing data.`X_train_padded = vectorize_layer(X_train)
    X_test_padded = vectorize_layer(X_test)![](../images/00029.jpeg)`
  id: totrans-655
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Convert the data to a`TensorFlow dataset and create a function to fetch the
    data in batches. We also convert the data to NumPy arrays because JAX expects
    NumPy or JAX arrays.
  id: totrans-656
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: import`tensorflow_datasets`as`tfds`
  id: totrans-657
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`training_data = tf.data.Dataset.from_tensor_slices((X_train_padded, y_train))`'
  id: totrans-658
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`validation_data = tf.data.Dataset.from_tensor_slices((X_test_padded, y_test))`'
  id: totrans-659
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`training_data = training_data.batch(batch_size)`'
  id: totrans-660
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`validation_data = validation_data.batch(batch_size)`'
  id: totrans-661
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def get_train_batches():`'
  id: totrans-662
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`ds = training_data.prefetch(1)`'
  id: totrans-663
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`tfds.dataset_as_numpy`converts the`tf.data.Dataset`into an'
  id: totrans-664
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: iterable of NumPy arrays`return tfds.as_numpy(ds)`The data is now in the right
    format and to be passed to a Flax network.
  id: totrans-665
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Let's quickly walk through the rest of the steps required to train neural networks
    in Flax using this data.
  id: totrans-666
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: First, create a simple neural network in Flax.
  id: totrans-667
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: pip install`flax`
  id: totrans-668
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: import`flax`
  id: totrans-669
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: from`flax`import`linen as nn`
  id: totrans-670
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`class Model(nn.Module):`'
  id: totrans-671
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '@nn.compact'
  id: totrans-672
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def __call__(self, x): x = nn.Dense(features=256)(x) x = nn.relu(x)`'
  id: totrans-673
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`x = nn.Dense(features=2)(x) x = nn.log_softmax(x)`'
  id: totrans-674
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: return`x`
  id: totrans-675
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Define a function to compute the loss.
  id: totrans-676
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: import`optax`
  id: totrans-677
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: import`jax.numpy`as`jnp`
  id: totrans-678
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def cross_entropy_loss(*, logits, labels):`'
  id: totrans-679
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`labels_onehot = jax.nn.one_hot(labels, num_classes=2) return optax.softmax_cross_entropy(logits=logits,
    labels=labe ls_onehot).mean()`Next, define the function to compute the network
    metrics.'
  id: totrans-680
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def compute_metrics(*, logits, labels):`'
  id: totrans-681
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`loss = cross_entropy_loss(logits=logits, labels=labels)` `accuracy = jnp.mean(jnp.argmax(logits,
    -1) == labels)` `metrics = {`'
  id: totrans-682
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`''loss'': loss,`'
  id: totrans-683
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`''accuracy'': accuracy`,'
  id: totrans-684
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '}'
  id: totrans-685
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: return `metrics`
  id: totrans-686
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The training state is used to track the network training. It tracks the optimizer
    and model parameters and can be modified to track other things such as dropout
    and batch normalization statistics.
  id: totrans-687
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: from `flax.training import train_state` def `create_train_state(rng, learning_rate,
    momentum):` """Creates initial `TrainState`."""
  id: totrans-688
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`model = Model()`'
  id: totrans-689
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`params = model.init(rng, X_train_padded[0])[''params'']` `tx = optax.sgd(learning_rate,
    momentum)`'
  id: totrans-690
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: return `train_state.TrainState.create(
  id: totrans-691
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`apply_fn=model.apply, params=params, tx=tx)`In the training step, we [Apply] the
    model to obtain the loss. This is then used to compute the gradients that update
    the model parameters.'
  id: totrans-692
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def compute_loss(params,text,labels):`'
  id: totrans-693
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`logits = Model().apply({''params'': params}, text)` `loss = cross_entropy_loss(logits=logits,
    labels=labels)` return `loss, logits`'
  id: totrans-694
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '@jax.jit'
  id: totrans-695
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def train_step(state,text, labels):`'
  id: totrans-696
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '"""Train for a single step."""'
  id: totrans-697
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`(_, logits), grads = jax.value_and_grad(compute_loss, has_aux =True)(state.params,text,labels)`'
  id: totrans-698
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`state = state.apply_gradients(grads=grads)`'
  id: totrans-699
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`metrics = compute_metrics(logits=logits, labels=labels)`'
  id: totrans-700
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: return `state, metrics`
  id: totrans-701
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The evaluation step applies the model to the testing data to compute the test
    metrics.
  id: totrans-702
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '@jax.jit'
  id: totrans-703
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def eval_step(state, text, labels):`'
  id: totrans-704
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`logits = Model().apply({''params'': state.params}, text)`'
  id: totrans-705
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: return `compute_metrics(logits=logits, labels=labels)`
  id: totrans-706
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The evaluation function runs the above evaluation step to obtain the evaluation
    metrics.
  id: totrans-707
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def evaluate_model(state, text, test_lbls):`'
  id: totrans-708
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '"""Evaluate on the validation set."""'
  id: totrans-709
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`metrics = eval_step(state, text, test_lbls)` `metrics = jax.device_get(metrics)`'
  id: totrans-710
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`metrics = jax.tree_map(lambda x: x.item(), metrics)` return `metrics`'
  id: totrans-711
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: We use the `get_train_batches` function in the`train_epoch` method. We loop
    through the batches as we apply the `[train_step]` method. We obtain the train
    metrics and return them.
  id: totrans-712
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def train_one_epoch(state):`'
  id: totrans-713
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '"""Train for 1 epoch on the training set.""" `batch_metrics = []`'
  id: totrans-714
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: for `text, labels in get_train_batches():`
  id: totrans-715
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`state, metrics = train_step(state, text, labels)` `batch_metrics.append(metrics)`batch_metrics_np
    = jax.device_get(batch_metrics) epoch_metrics_np = {k: np.mean([metrics[k] for
    metrics in batch_metrics_n p])for k in batch_metrics_np[0] }return `state, epoch_metrics_np`'
  id: totrans-716
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The final step is to train the network on the training set and evaluate it on
    the test set. A training state is required before training the model. This is
    because JAX expects pure functions.
  id: totrans-717
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`rng = jax.random.PRNGKey(0)`，`rng, init_rng = jax.random.split(rng)`'
  id: totrans-718
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`learning_rate = 0.1 momentum = 0.9`'
  id: totrans-719
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`seed = 0`'
  id: totrans-720
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`state = create_train_state(init_rng, learning_rate, momentum)` del `init_rng
    # Must not be used anymore.`'
  id: totrans-721
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`num_epochs = 30`'
  id: totrans-722
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`(text, test_labels) = next(iter(validation_data))` `text = jnp.array(text)`'
  id: totrans-723
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`test_labels = jnp.array(test_labels)`'
  id: totrans-724
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`state` = `create_train_state(jax.random.PRNGKey(seed), learning_r ate, momentum)`'
  id: totrans-725
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`training_loss` = []'
  id: totrans-726
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`training_accuracy` = []'
  id: totrans-727
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`testing_loss` = []'
  id: totrans-728
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`testing_accuracy` = []'
  id: totrans-729
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`for epoch in range(1, num_epochs + 1):`'
  id: totrans-730
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`train_state, train_metrics = train_one_epoch(state) training_loss.append(train_metrics[''loss''])`'
  id: totrans-731
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`training_accuracy.append(train_metrics[''accuracy'']) print(f"Train epoch:
    {epoch}, loss: {train_metrics[''los`'
  id: totrans-732
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 's'']}, accuracy: {train_metrics[''accuracy''] * 100}")`'
  id: totrans-733
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`test_metrics` = `evaluate_model(train_state, text, test_label s)`'
  id: totrans-734
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`testing_loss.append(test_metrics[''loss''])`'
  id: totrans-735
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`testing_accuracy.append(test_metrics[''accuracy''])`'
  id: totrans-736
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`print(f"Test epoch: {epoch}, loss: {test_metrics[''loss'']}, accuracy: {test_metrics[''accuracy'']
    * 100}")`'
  id: totrans-737
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`![](../images/00030.jpeg)`'
  id: totrans-738
  prefs: []
  stylish: true
  type: TYPE_IMG
- en: '**How to load image data in JAX**'
  id: totrans-739
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Let's now see how we can load image data with TensorFlow. We'll use the popular cats
    and dogs images from Kaggle. We start by downloading the data.
  id: totrans-740
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import wget # pip install wgetimport zipfile`'
  id: totrans-741
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`wget.download("https://ml.machinelearningnuggets.com/train.zi p")`'
  id: totrans-742
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`with zipfile.ZipFile(''train.zip'', ''r'') as zip_ref:`'
  id: totrans-743
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`zip_ref.extractall(''.'')Next, create a Pandas DataFrame containing the labels
    and paths to the images.`'
  id: totrans-744
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import pandas as pd`'
  id: totrans-745
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`base_dir` = `''train''`'
  id: totrans-746
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`filenames = os.listdir(base_dir) categories = []`'
  id: totrans-747
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`for filename in filenames:`'
  id: totrans-748
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`category = filename.split(''.'')[0] if category == ''dog'':categories.append("dog")`'
  id: totrans-749
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`else:`'
  id: totrans-750
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`categories.append("cat") df = pd.DataFrame({''filename'': filenames,''category'':
    categorie s})`'
  id: totrans-751
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The next step is to define an `ImageDataGenerator` for scaling the images and
    performing simple augmentation.
  id: totrans-752
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from tensorflow.keras.preprocessing.image import ImageDataGener ator`'
  id: totrans-753
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`train_datagen` = `ImageDataGenerator(rescale=1./255,`'
  id: totrans-754
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`shear_range=0.2, zoom_range=0.2,`'
  id: totrans-755
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`horizontal_flip=True, width_shift_range=0.1, height_shift_range=0.1, validation_split=0.2
    )`'
  id: totrans-756
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`validation_gen` = `ImageDataGenerator(rescale=1./255,validation_s plit=0.2)`'
  id: totrans-757
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Load the images using the `[flow_from_dataframe]of these generators. This will
    match the image paths in the DataFrame to the images we downloaded.`
  id: totrans-758
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`image_size = (128, 128)`'
  id: totrans-759
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`batch_size = 128`'
  id: totrans-760
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`training_set` = `train_datagen.flow_from_dataframe(df,base_dir,`'
  id: totrans-761
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`seed=101, target_size=ima ge_size,`'
  id: totrans-762
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`batch_size=batc h_size,`'
  id: totrans-763
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`x_col=''filenam e''`,'
  id: totrans-764
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`y_col=''categor y''`'
  id: totrans-765
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: subset = `"train ing"`
  id: totrans-766
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`class_mode=''bin ary'')`'
  id: totrans-767
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`validation_set` = `validation_gen.flow_from_dataframe(df,base_di r,`'
  id: totrans-768
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`target_size=image _size`,'
  id: totrans-769
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`batch_size=batch_b size,`'
  id: totrans-770
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`x_col=''filename''`,'
  id: totrans-771
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`y_col=''categor y''`,'
  id: totrans-772
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: subset = `"validat ion"`
  id: totrans-773
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`class_mode=''binar y'')`'
  id: totrans-774
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Loop through the training set to confirm that a batch of images are being generated.
  id: totrans-775
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`for train_images, train_labels in training_set: print(''Train:'', train_images.shape,
    train_labels.shape) break`'
  id: totrans-776
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'Train: `(128, 128, 128, 3) (128,)`'
  id: totrans-777
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: The next step is to define a network and pass the data. The steps are similar
    to what we did for the text data above
  id: totrans-778
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**How to load CSV data in JAX**'
  id: totrans-779
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: You can use `Pandas` to load CSV data as we did for the text data at the beginning
    of the article. Convert the data to NumPy or `JAX arrays` once preprocessing is
    done. Passing Torch tensors or TensorFlow tensors to `JAX` neural networks will
    result in an error.
  id: totrans-780
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`**Final thoughts**`'
  id: totrans-781
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: This article shows how you can use TensorFlow to load datasets in `JAX` and
    `Flax` applications. We have walked through an example of loading text data with
    TensorFlow. After that, we discussed loading image and CSV data in `JAX`.
  id: totrans-782
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`**Image classification with JAX & Flax**`'
  id: totrans-783
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '`Flax` is a neural network library for `JAX`. `JAX` is a `Python` library that
    provides high-performance computing in machine learning research. `JAX` provides
    an API similar to `NumPy` making it easy to adopt. `JAX` also includes other functionalities
    for improving machine learning research. They include:'
  id: totrans-784
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`**Automatic differentiation**` . `JAX` supports forward and reverse automatic
    differential of numerical functions with functions such as `jacrev`, `grad`, `hessian` and `jacfwd`.'
  id: totrans-785
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`**Vectorization**` . `JAX` supports automatic vectorization via the `[vmap]` function.
    It also makes it easy to parallelize large-scale data processing via the `[pmap]` function.'
  id: totrans-786
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`**JIT compilation**` . `JAX` uses `XLA` for Just In Time (JIT) compilation
    and execution of code on GPUs and TPUs. In this article, let''s look at how you
    can use `JAX` and `Flax` to build a simple convolutional neural network.'
  id: totrans-787
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`**Loading the dataset**`'
  id: totrans-788
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: 'We''ll use the `cats and dogs dataset` from Kaggle. Let''s start by downloading
    and extracting it.`import wget # pip install wget import zipfile`'
  id: totrans-789
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`wget.download("https://ml.machinelearningnuggets.com/train.zi p")`'
  id: totrans-790
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`with zipfile.ZipFile(''train.zip'', ''r'') as zip_ref:`'
  id: totrans-791
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`zip_ref.extractall(''.'')`'
  id: totrans-792
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`Flax doesn''t ship with any data loading tools. You can use the data loaders
    from `PyTorch`'
  id: totrans-793
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`or `TensorFlow. In this case, let''s load the data using PyTorch. The first
    step is to define the dataset class.'
  id: totrans-794
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from PIL import Image`'
  id: totrans-795
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import pandas as pd`'
  id: totrans-796
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from torch.utils.data import Dataset`'
  id: totrans-797
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`class CatsDogsDataset(Dataset):`'
  id: totrans-798
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def __init__(self, root_dir, annotation_file, transform=Non`'
  id: totrans-799
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`e):`'
  id: totrans-800
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`self.root_dir = root_dir`'
  id: totrans-801
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`self.annotations = pd.read_csv(annotation_file) self.transform = transform`'
  id: totrans-802
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def __len__(self):return len(self.annotations) def __getitem__(self, index):img_id
    = self.annotations.iloc[index, 0]img = Image.open(os.path.join(self.root_dir,
    img_id)).c onvert("RGB")y_label = torch.tensor(float(self.annotations.iloc[inde
    x, 1]))if self.transform is not None: img = self.transform(img)return (img, y_label)`Next,
    we create a `Pandas DataFrame` that will contain the categories.`import osimport
    pandas as pd`'
  id: totrans-803
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`train_df = pd.DataFrame(columns=["img_path","label"]) train_df["img_path"]
    = os.listdir("train/") for idx, i in enumerate(os.listdir("train/")):`'
  id: totrans-804
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`if "cat" in i:`'
  id: totrans-805
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`train_df["label"][idx] = 0`'
  id: totrans-806
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`if "dog" in i:`'
  id: totrans-807
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`train_df["label"][idx] = 1`'
  id: totrans-808
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`train_df.to_csv (r''train_csv.csv'', index = False, header=True)`Define a
    function that will stack the data and return it as NumPy arrays.'
  id: totrans-809
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import numpy as np`'
  id: totrans-810
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: def `custom_collate_fn(batch)`
  id: totrans-811
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: transposed_data = `list(zip(*batch))`
  id: totrans-812
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: labels = `np.array(transposed_data[1])`
  id: totrans-813
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: imgs = `np.stack(transposed_data[0])`
  id: totrans-814
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: return `imgs, labels`
  id: totrans-815
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: We are now ready to define the training and test data and use that with the
    PyTorch DataLoader. We also define a PyTorch transformation for resizing the images.
  id: totrans-816
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: import `torch`
  id: totrans-817
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: from `torch.utils.data import DataLoader` from `torchvision import transforms`
    import `numpy as np`
  id: totrans-818
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: size_image = 64 batch_size = 32
  id: totrans-819
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: transform = `transforms.Compose([`
  id: totrans-820
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: transforms.Resize((size_image,size_image)),
  id: totrans-821
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`np.array]`'
  id: totrans-822
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: dataset = `CatsDogsDataset("train","train_csv.csv",transform=tra nsform)`
  id: totrans-823
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: train_set, validation_set = `torch.utils.data.random_split(datas et,[20000,5000])`
  id: totrans-824
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: train_loader = `DataLoader(dataset=train_set, collate_fn=custom_ collate_fn,shuffle=True,
    batch_size=batch_size)`
  id: totrans-825
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: validation_loader = `DataLoader(dataset=validation_set,collate_f n=custom_collate_fn,
    shuffle=False, batch_size=batch_size)`
  id: totrans-826
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Define Convolution Neural Network with Flax
  id: totrans-827
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Install Flax to create a simple neural network.`pip install flax`
  id: totrans-828
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Networks are created in Flax using the Linen API by
  id: totrans-829
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: subclassing Module.  All Flax modules are Python dataclasses. This means that
    they have  [`__.init__`] by default. You should, therefore, override [`setup()`]
    instead to initialize the network. However, you can use the compact wrapper
  id: totrans-830
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: to make the model definition more concise.
  id: totrans-831
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: import `flaxfrom flax import linen as nnclass CNN(nn.Module):`
  id: totrans-832
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '@`nn.compact`'
  id: totrans-833
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: def `__call__(self, x):`
  id: totrans-834
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: x = `nn.Conv(features=32, kernel_size=(3, 3))(x)`
  id: totrans-835
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: x = `nn.relu(x)`
  id: totrans-836
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: x = `nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))`
  id: totrans-837
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: x = `nn.Conv(features=64, kernel_size=(3, 3))(x)`
  id: totrans-838
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: x = `nn.relu(x)`
  id: totrans-839
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: x = `nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))`
  id: totrans-840
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: x = `x.reshape((x.shape[0], -1))`
  id: totrans-841
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: x = `nn.Dense(features=256)(x)`
  id: totrans-842
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: x = `nn.relu(x)`
  id: totrans-843
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: x = `nn.Dense(features=2)(x)`
  id: totrans-844
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: x = `nn.log_softmax(x)`
  id: totrans-845
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: return x
  id: totrans-846
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Define loss
  id: totrans-847
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: The loss can be computed using the Optax package. We one-hot encode the integer
    labels before passing them to the softmax crossentropy function. num_classes is
    2 because we are dealing with two classes.
  id: totrans-848
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: import `optax`
  id: totrans-849
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: def `cross_entropy_loss(*, logits, labels)`
  id: totrans-850
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: labels_onehot = `jax.nn.one_hot(labels, num_classes=2)`
  id: totrans-851
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: return `optax.softmax_cross_entropy(logits=logits, labels=labe ls_onehot).mean()`
  id: totrans-852
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Compute metrics
  id: totrans-853
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Next, we define a function that will use the above loss function to compute
    and return the loss. We also compute the accuracy in the same function.
  id: totrans-854
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: def `compute_metrics(*, logits, labels):`
  id: totrans-855
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: loss = `cross_entropy_loss(logits=logits, labels=labels)` accuracy = `jnp.mean(jnp.argmax(logits,
    -1) == labels)` metrics = {
  id: totrans-856
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '''loss'': loss,'
  id: totrans-857
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`''accuracy'': accuracy,`'
  id: totrans-858
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '}'
  id: totrans-859
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: return metrics
  id: totrans-860
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Create training state
  id: totrans-861
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: A training state holds the model variables such as parameters and optimizer
    state. These variables are modified at each iteration using the optimizer. You
    can subclass `[flax.training.train_state]` to track more data. You might want
    to do that for tracking the state of dropout and batch statistics if you include
    those layers in your model. For this simple model, the default class will suffice.
  id: totrans-862
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: from `flax.training` import `train_state`
  id: totrans-863
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'def `create_train_state`(rng, learning_rate, momentum): """Creates initial
    `TrainState`."""'
  id: totrans-864
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`cnn = CNN()`'
  id: totrans-865
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: params = `cnn.init`(rng, jnp.ones([1, size_image, size_image,
  id: totrans-866
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`3`]))[''params'']'
  id: totrans-867
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: tx = `optax.sgd`(learning_rate, momentum)
  id: totrans-868
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: return `train_state.TrainState.create`(
  id: totrans-869
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: apply_fn=`cnn.apply`, params=`params`, tx=`tx`)
  id: totrans-870
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Define training step**'
  id: totrans-871
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: In this function, we evaluate the model with a set of input images using the `Apply` method.
    We use the obtained logits to compute the loss. We then use  `value_and_grad`  to
    evaluate the loss function and its gradient. The gradients are then used to update
    the model parameters. Finally, it uses the `[compute_metrics]` function defined
    above to calculate the loss and accuracy.
  id: totrans-872
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'def `compute_loss`(params,images,labels):'
  id: totrans-873
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'logits = `CNN().apply`({''params'': params}, images) loss = `cross_entropy_loss`(logits=`logits`,
    labels=`labels`) return `loss`, `logits`'
  id: totrans-874
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '@`jax.jit`'
  id: totrans-875
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'def `train_step`(state,images, labels):'
  id: totrans-876
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '"""Train for a single step."""'
  id: totrans-877
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: (_, logits), grads = jax.value_and_grad(`compute_loss`, has_aux =`True`)(state.params,images,labels)
  id: totrans-878
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: state = `state.apply_gradients`(grads=`grads`)
  id: totrans-879
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: metrics = `compute_metrics`(logits=`logits`, labels=`labels`)
  id: totrans-880
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: return `state`, `metrics`
  id: totrans-881
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The function is decorated with the @`Jit` decorator to trace the function and
    compile just-in-time for faster computation.
  id: totrans-882
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Define evaluation step**'
  id: totrans-883
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: The evaluation function will use `[Apply]`  to evaluate the model on the test
    data.
  id: totrans-884
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '@`jax.jit`'
  id: totrans-885
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'def `eval_step`(state, images, labels):'
  id: totrans-886
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'logits = `CNN().apply`({''params'': state.params}, images)'
  id: totrans-887
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: return `compute_metrics`(logits=`logits`, labels=`labels`)
  id: totrans-888
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Training function**'
  id: totrans-889
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: In this function, we apply the training step we defined above.  We loop through
    each batch in the data loader and perform optimization for each batch. We use
    the `[jax.device_get]` to get the metrics and compute the mean.
  id: totrans-890
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'def `train_one_epoch`(state, dataloader):'
  id: totrans-891
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '"""Train for 1 epoch on the training set.""" batch_metrics = []'
  id: totrans-892
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'for cnt, (images, labels) in enumerate(dataloader):'
  id: totrans-893
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: images = images / `255.0`
  id: totrans-894
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: state, metrics = `train_step`(state, images, labels) batch_metrics.append(metrics)
  id: totrans-895
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'batch_metrics_np = jax.device_get(batch_metrics) epoch_metrics_np = {k: np.mean([metrics[k]
    for metrics in batch_metrics_n p])for k in batch_metrics_np[0] }return `state`,
    `epoch_metrics_np`'
  id: totrans-896
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Evaluate the model**'
  id: totrans-897
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: The evaluation function runs the evaluation step and returns the test metrics.
  id: totrans-898
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'def `evaluate_model`(state, test_imgs, test_lbls): """Evaluate on the validation
    set."""'
  id: totrans-899
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: metrics = `eval_step`(state, test_imgs, test_lbls) metrics = jax.device_get(metrics)
  id: totrans-900
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`metrics = jax.tree_map(lambda x: x.item(), metrics) return metrics`'
  id: totrans-901
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Train and evaluate the model
  id: totrans-902
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: We need to initialize the train state before training the model. The function
    to initialize the state requires a pseudo-random number (PRNG) key. Use the [PRNGKey] function
    to obtain a key and split it to get another key that you'll use for parameter
    initialization. Follow this link to learn more about JAX PRNG Design.
  id: totrans-903
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Pass this key to the [create_train_state] function together with the learning
    rate and momentum. You can now use the [train_one_epoch] function to train the
    model and the eval_modelfunction to evaluate the model.
  id: totrans-904
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import jaxrng = jax.random.PRNGKey(0)rng, init_rng = jax.random.split(rng)`'
  id: totrans-905
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`learning_rate = 0.1 momentum = 0.9`'
  id: totrans-906
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`seed = 0`'
  id: totrans-907
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`state = create_train_state(init_rng, learning_rate, momentum) del init_rng
    # Must not be used anymore.`'
  id: totrans-908
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`num_epochs = 30`'
  id: totrans-909
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`(test_images, test_labels) = next(iter(validation_loader)) test_images = test_images
    / 255.0`'
  id: totrans-910
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`state = create_train_state(jax.random.PRNGKey(seed), learning_r ate, momentum)`'
  id: totrans-911
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`training_loss = []`'
  id: totrans-912
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`training_accuracy = []`'
  id: totrans-913
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`testing_loss = []`'
  id: totrans-914
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`testing_accuracy = []`'
  id: totrans-915
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`for epoch in range(1, num_epochs + 1):`'
  id: totrans-916
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`train_state, train_metrics = train_one_epoch(state, train_l oader)`'
  id: totrans-917
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`training_loss.append(train_metrics[''loss''])`'
  id: totrans-918
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`training_accuracy.append(train_metrics[''accuracy''])`'
  id: totrans-919
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`print(f"Train epoch: {epoch}, loss: {train_metrics[''los s'']}, accuracy:
    {train_metrics[''accuracy''] * 100}")`'
  id: totrans-920
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`test_metrics = evaluate_model(train_state, test_images, tes t_labels)`'
  id: totrans-921
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`testing_loss.append(test_metrics[''loss''])`'
  id: totrans-922
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`testing_accuracy.append(test_metrics[''accuracy''])`'
  id: totrans-923
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`print(f"Test epoch: {epoch}, loss: {test_metrics[''loss'']}, accuracy: {test_metrics[''accuracy'']
    * 100}")`'
  id: totrans-924
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Model performance
  id: totrans-925
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: While the training is happening, we print the training and validation metrics.
    You can also use the resulting metrics to plot training and validation charts.
  id: totrans-926
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import matplotlib.pyplot as plt`'
  id: totrans-927
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`plt.plot(training_accuracy, label="Training") plt.plot(testing_accuracy, label="Test")
    plt.xlabel("Epoch")`'
  id: totrans-928
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`plt.ylabel("Accuracy")`'
  id: totrans-929
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`plt.legend()`'
  id: totrans-930
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`plt.show()`'
  id: totrans-931
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`plt.plot(training_loss, label="Training") plt.plot(testing_loss, label="Test")
    plt.xlabel("Epoch")`'
  id: totrans-932
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`plt.ylabel("Accuracy")`'
  id: totrans-933
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`plt.legend()`'
  id: totrans-934
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`plt.show()`'
  id: totrans-935
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Final thoughts
  id: totrans-936
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: In this article, we have seen how to set up a simple neural network with Flax
    and train it on the CPU.
  id: totrans-937
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Distributed training with JAX & Flax
  id: totrans-938
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Training models on accelerators with JAX and Flax differs slightly from training
    with CPU. For instance, the data needs to be replicated in the different devices
    when using multiple accelerators. After that, we need to execute the training
    on multiple devices and aggregate the results. Flax supports TPU and GPU accelerators.
  id: totrans-939
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: In the last article, we saw how to train models with the CPU. This article will
    focus on training models with Flax and JAX using GPUs and TPU.
  id: totrans-940
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Perform standard imports
  id: totrans-941
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: You'll need to install Flax for this illustration.pip install flaxLet's import
    all the packages we'll use in this project.
  id: totrans-942
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import wget`'
  id: totrans-943
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import zipfile`'
  id: totrans-944
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import torch`'
  id: totrans-945
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from torch.utils.data import DataLoader import os`'
  id: totrans-946
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from PIL import Image`'
  id: totrans-947
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from torchvision import transforms from torch.utils.data import Dataset import
    numpy as np`'
  id: totrans-948
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import pandas as pd`'
  id: totrans-949
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import matplotlib.pyplot as plt`'
  id: totrans-950
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import functools`'
  id: totrans-951
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import time`'
  id: totrans-952
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from tqdm.notebook import tqdm`'
  id: totrans-953
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: ignore harmless warnings
  id: totrans-954
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`import warnings`'
  id: totrans-955
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`warnings.filterwarnings("ignore") warnings.simplefilter(''ignore'') import
    jax`'
  id: totrans-956
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from jax import numpy as jnp`'
  id: totrans-957
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from flax import linen as nn`'
  id: totrans-958
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from flax.training import train_state import optax`'
  id: totrans-959
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import math`'
  id: totrans-960
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from flax import jax_utils`'
  id: totrans-961
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import jax.tools.colab_tpu`'
  id: totrans-962
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Setup TPUs on Colab**'
  id: totrans-963
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Change the runtime on Colab to TPUs. Next, run the code below to set up `JAX` to
    use TPUs.`jax.tools.colab_tpu.setup_tpu() jax.devices()![](../images/00031.gif)`
  id: totrans-964
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Download the dataset**'
  id: totrans-965
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: 'We''ll use the `cats and dogs dataset` from Kaggle. Let''s start by downloading
    and extracting it.`import wget # pip install wget import zipfile`'
  id: totrans-966
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`wget.download("https://ml.machinelearningnuggets.com/train.zi p")`'
  id: totrans-967
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`with zipfile.ZipFile(''train.zip'', ''r'') as zip_ref:`'
  id: totrans-968
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`zip_ref.extractall(''.'')`'
  id: totrans-969
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Load the dataset**'
  id: totrans-970
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: We'll use existing data loaders to load the data since `JAX` and Flax don't
    ship with any data loaders. In this case, let's use `PyTorch` to load the dataset.
    The first step is to set up a dataset class.
  id: totrans-971
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: class `CatsDogsDataset(Dataset):`
  id: totrans-972
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def __init__(self, root_dir, annotation_file, transform=Non`'
  id: totrans-973
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`e):`'
  id: totrans-974
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`self.root_dir = root_dir`'
  id: totrans-975
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`self.annotations = pd.read_csv(annotation_file) self.transform = transform`'
  id: totrans-976
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def __len__(self):return len(self.annotations)`'
  id: totrans-977
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def __getitem__(self, index):`'
  id: totrans-978
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`img_id = self.annotations.iloc[index, 0]`'
  id: totrans-979
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`img = Image.open(os.path.join(self.root_dir, img_id)).c`'
  id: totrans-980
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`onvert("RGB")`'
  id: totrans-981
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`y_label = torch.tensor(float(self.annotations.iloc[inde`'
  id: totrans-982
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`x, 1]))`'
  id: totrans-983
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`if self.transform is not None: img = self.transform(img)return (img, y_label)Next,
    we create a DataFrame containing the categories.`'
  id: totrans-984
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`train_df = pd.DataFrame(columns=["img_path","label"]) train_df["img_path"]
    = os.listdir("train/") for idx, i in enumerate(os.listdir("train/")):`'
  id: totrans-985
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`if "cat" in i:`'
  id: totrans-986
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`train_df["label"][idx] = 0`'
  id: totrans-987
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`if "dog" in i:`'
  id: totrans-988
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`train_df["label"][idx] = 1`'
  id: totrans-989
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`train_df.to_csv (r''train_csv.csv'', index = False, header=True)`'
  id: totrans-990
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: We then use the dataset class to create training and testing data. We also apply
    a custom function to return the data as NumPy arrays. Later, we'll use this `train_loader`
  id: totrans-991
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: when training the model. We'll then evaluate it on a batch of the test data.
  id: totrans-992
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def custom_collate_fn(batch):`'
  id: totrans-993
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`transposed_data = list(zip(*batch)) labels = np.array(transposed_data[1])
    imgs = np.stack(transposed_data[0]) return imgs, labels`'
  id: totrans-994
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`size_image = 224 batch_size = 64`'
  id: totrans-995
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`transform = transforms.Compose([`'
  id: totrans-996
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`transforms.Resize((size_image,size_image)),`'
  id: totrans-997
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`np.array])`'
  id: totrans-998
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`dataset = CatsDogsDataset("train","train_csv.csv",transform=tra nsform)`'
  id: totrans-999
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`train_set, validation_set = torch.utils.data.random_split(datas et,[20000,5000])`'
  id: totrans-1000
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`train_loader = DataLoader(dataset=train_set, collate_fn=custom_ collate_fn,shuffle=True,
    batch_size=batch_size)`'
  id: totrans-1001
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`validation_loader = DataLoader(dataset=validation_set,collate_f n=custom_collate_fn,
    shuffle=False, batch_size=batch_size)`'
  id: totrans-1002
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Define the model with Flax
  id: totrans-1003
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: In Flax, models are defined using the Linen API. It provides the building blocks
    for defining convolution layers, dropout, etc.
  id: totrans-1004
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Networks are created by subclassing `[Module]`.   Flax allows you to define
    your networks using `[setup]` or `[nn.compact]`. Both approaches behave the same
    way but `[nn.compact]`
  id: totrans-1005
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: is more concise.
  id: totrans-1006
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Create training state
  id: totrans-1007
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: We now need to create parallel versions of our functions. Parallelization in JAX is
    done using
  id: totrans-1008
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: the `[pmap]` function. `[pmap]` compiles a function with XLA and executes it
    on multiple devices.
  id: totrans-1009
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '```python'
  id: totrans-1010
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: cnn = CNN()
  id: totrans-1011
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image, 3]))[''params'']`'
  id: totrans-1012
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`tx = optax.sgd(learning_rate, momentum)`'
  id: totrans-1013
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`return train_state.TrainState.create(`'
  id: totrans-1014
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`apply_fn=cnn.apply, params=params, tx=tx)`'
  id: totrans-1015
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Apply the model
  id: totrans-1016
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: The next step is to define
  id: totrans-1017
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: parallel `apply_model` and `update_model`functions.
  id: totrans-1018
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'The `[apply_model]` function:'
  id: totrans-1019
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Computes the loss.
  id: totrans-1020
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Computes predictions from all devices by calculating the average of the probabilities
    using `[jax.lax.pmean()]`.```python
  id: totrans-1021
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'logits = CNN().apply({''params'': params}, images) one_hot = jax.nn.one_hot(labels,
    2)'
  id: totrans-1022
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`loss = optax.softmax_cross_entropy(logits=logits, labels=on`'
  id: totrans-1023
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: e_hot).mean()return loss, logits
  id: totrans-1024
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`grad_fn = jax.value_and_grad(loss_fn, has_aux=True) (loss, logits), grads
    = grad_fn(state.params)`'
  id: totrans-1025
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`probs = jax.lax.pmean(jax.nn.softmax(logits), axis_name=''ense`'
  id: totrans-1026
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '```python'
  id: totrans-1027
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Notice the use of the `[axis_name]`. You can give this any name. You'll need
    to specify that when computing the mean of the probabilities and accuracies.
  id: totrans-1028
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The `[update_model]` function updates the model parameters.
  id: totrans-1029
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Training function
  id: totrans-1030
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: 'The next step is to define the model training function. In the function, we:'
  id: totrans-1031
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Replicate the training data at batch level
  id: totrans-1032
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: using `jax_utils.replicat e`.
  id: totrans-1033
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`[apply_model]` to the replicated data.'
  id: totrans-1034
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Obtain the epoch loss and accuracy and unreplicate them
  id: totrans-1035
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: using `jax_utils.unreplicate`.
  id: totrans-1036
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Compute the mean of the loss and accuracy.
  id: totrans-1037
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`[apply_model]` to the test data and obtain test metrics.'
  id: totrans-1038
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Print the training and evaluation metrics per epoch. Append the training and
    test metrics to lists for visualization later.
  id: totrans-1039
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '```python'
  id: totrans-1040
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`epoch_loss = []`'
  id: totrans-1041
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`epoch_accuracy = []`'
  id: totrans-1042
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`testing_accuracy = []`'
  id: totrans-1043
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`testing_loss = []`'
  id: totrans-1044
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`for epoch in range(num_epochs):`'
  id: totrans-1045
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`for cnt, (images, labels) in tqdm(enumerate(dataloade r), total=(math.ceil(len(train_set)/batch_size))):
    images = images / 255.0`'
  id: totrans-1046
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`images` = `jax_utils.replicate(images)`'
  id: totrans-1047
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`labels` = `jax_utils.replicate(labels)`'
  id: totrans-1048
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`grads`, `loss`, `accuracy` = `apply_model(state, images`,'
  id: totrans-1049
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`labels)`'
  id: totrans-1050
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`state` = `update_model(state, grads)`'
  id: totrans-1051
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`epoch_loss.append(jax_utils.unreplicate(loss)) epoch_accuracy.append(jax_utils.unreplicate(accuracy))
    train_loss = np.mean(epoch_loss)`'
  id: totrans-1052
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`train_accuracy = np.mean(epoch_accuracy)`'
  id: totrans-1053
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`_, test_loss, test_accuracy` = `jax_utils.unreplicate(apply_model(state, test_images,
    test_labels))`'
  id: totrans-1054
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`testing_accuracy.append(test_accuracy)`'
  id: totrans-1055
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`testing_loss.append(test_loss)`'
  id: totrans-1056
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`print(f"Epoch: {epoch + 1}, train loss: {train_loss:.4 f}, train accuracy:
    {train_accuracy * 100:.4f}, test loss: {tes t_loss:.4f}, test accuracy: {test_accuracy*
    100:.4f}", flush=Tr ue)`'
  id: totrans-1057
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`return state, epoch_loss, epoch_accuracy, testing_accuracy, testing_loss`'
  id: totrans-1058
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Train the model
  id: totrans-1059
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: When creating the training state, we generate pseudo-random numbers equivalent
    to the number of devices. We also replicate a small batch of the test data for
    testing. The next step is to run the training function and unpack the training
    and test metrics.
  id: totrans-1060
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`learning_rate = 0.1 momentum = 0.9`'
  id: totrans-1061
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`seed = 0`'
  id: totrans-1062
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`num_epochs = 30`'
  id: totrans-1063
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`rng` = `jax.random.PRNGKey(0)`'
  id: totrans-1064
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`rng`, `init_rng` = `jax.random.split(rng)`'
  id: totrans-1065
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`state` = `create_train_state(jax.random.split(init_rng, jax.device_count()),learning_rate,
    momentum)` `del init_rng` # Must not be used anymore.'
  id: totrans-1066
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`(test_images, test_labels)` = `next(iter(validation_loader))` `test_images
    = test_images / 255.0`'
  id: totrans-1067
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`test_images` = `jax_utils.replicate(test_images)`'
  id: totrans-1068
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`test_labels` = `jax_utils.replicate(test_labels)`'
  id: totrans-1069
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`start = time.time()`'
  id: totrans-1070
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`state`, `epoch_loss`, `epoch_accuracy`, `testing_accuracy`, `testing_loss`
    = `train_one_epoch(state, train_loader,num_epochs)` `print("Total time: ", time.time()
    - start, "seconds")`'
  id: totrans-1071
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '![](../images/00032.jpeg)'
  id: totrans-1072
  prefs: []
  stylish: true
  type: TYPE_IMG
- en: Model evaluation
  id: totrans-1073
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: The metrics obtained above can be used to plot the metrics.
  id: totrans-1074
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`plt.plot(epoch_accuracy, label="Training")`'
  id: totrans-1075
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`plt.plot(testing_accuracy, label="Test")`'
  id: totrans-1076
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`plt.xlabel("Epoch")`'
  id: totrans-1077
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`plt.ylabel("Accuracy")`'
  id: totrans-1078
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`plt.legend()`'
  id: totrans-1079
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`plt.show()`'
  id: totrans-1080
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '![](../images/00033.jpeg)'
  id: totrans-1081
  prefs: []
  stylish: true
  type: TYPE_IMG
- en: Final thoughts
  id: totrans-1082
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: This article shows how you can use JAX and Flax to train machine learning models
    in parallel on multiple devices. You have seen that the process involves making
    a few functions parallel using JAX's pmap function. We have also covered how to
    replicate the training and test data on multiple devices.
  id: totrans-1083
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: How to use TensorBoard in JAX & Flax
  id: totrans-1084
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Tracking machine learning experiments makes understanding and visualizing the
    model's performance easy. It also makes it possible to spot any problems in the
    network. For example, you can quickly spot overfitting by looking at the training
    and validation charts. You can plot these charts using your favorite charts package,
    such as Matplotlib. However, you can also use more advanced tools such as TensorBoard.
  id: totrans-1085
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'TensorBoard is an open-source library that provides tools for experiment tracking
    in machine learning. You can use TensorBoard for:'
  id: totrans-1086
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Tracking and visualizing model evaluation metrics such as accuracy.
  id: totrans-1087
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Logging images.
  id: totrans-1088
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Visualize hyper-parameter tuning.
  id: totrans-1089
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Project embeddings such as word embedding in natural language processing problems.
  id: totrans-1090
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Visualize histograms of the model's weights and biases.
  id: totrans-1091
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Plot the architecture of the model.
  id: totrans-1092
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Profile the performance of the network.
  id: totrans-1093
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: You can use TensorBoard with popular machine learning libraries such as XGBoost, JAX, Flax,
    and PyTorch.
  id: totrans-1094
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: This article will focus on how to use TensorBoard when building networks with JAX and
    Flax.
  id: totrans-1095
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**How to use TensorBoard**'
  id: totrans-1096
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Let's start by exploring how to use TensorBoard.
  id: totrans-1097
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**How to install TensorBoard**'
  id: totrans-1098
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: The first step is to install TensorBoard from the Python Index. `pip install
    tensorboard`
  id: totrans-1099
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Using TensorBoard with Jupyter notebooks and Google Colab**'
  id: totrans-1100
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Once TensorBoard is installed, you need to load it in your environment, usually
    Google Colab or your local notebook.`%load_ext tensorboard`Next, inform TensorBoard
    which folder will contain the log information.`log_folder = "runs"`
  id: totrans-1101
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**How to launch TensorBoard**'
  id: totrans-1102
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Tensorboard is launched using the `[tensorboard]` magic command in notebook
    environments while specifying the `[logdir]`. `%tensorboard --logdir={log_folder}`
  id: totrans-1103
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'You can also launch TensorBoard on the command line using a similar pattern.
    Apart from viewing the terminal on the notebook environment, you can also view
    it on the browser by visiting: http://localhost:6006.'
  id: totrans-1104
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Tensorboard dashboards**'
  id: totrans-1105
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: TensorBoard has various dashboards for showing different types of information.
  id: totrans-1106
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The  **Scalars** dashboard tracks numerical information such as training metrics
    per epoch. You can use it to track other scalar values such as model training
    speed and learning rate.
  id: totrans-1107
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The **Graphs** dashboard is used for showing visualizations. For example, you
    can use it to check the architecture of the network.
  id: totrans-1108
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The  **Distributions and Histograms** dashboard show the distribution of tensors
    over time. Use it to check the weights and biases of the network.
  id: totrans-1109
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The **Images** dashboard shows the images you have logged to TensorBoard.
  id: totrans-1110
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The **HParams** dashboard visualizes hyperparameter optimization. It helps identify
    the best parameters for the network.
  id: totrans-1111
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The  **Embedding Projector** is used to visualize low-level embeddings, for
    example, text embeddings.
  id: totrans-1112
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The **What-If Tool** dashboard helps in understanding the performance of a model.
    It also enables the measurement of a model's fairness on data subsets.
  id: totrans-1113
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The  **TensorFlow Profiler** monitors the model training process. It also shows
    the events in the CPU and GPU during training. The TensorFlow profiler goes further
    to offer recommendations based on the data collected. You can also use it to debug
    performance issues in the input pipeline.
  id: totrans-1114
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**How to use TensorBoard with Flax**'
  id: totrans-1115
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: With TensorBoard installed and some basics out of the way, let's look at how
    you can use it in Flax. Let's use
  id: totrans-1116
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: the `[SummaryWriter]` from PyTorch to write to the log folder.
  id: totrans-1117
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**How to log images with TensorBoard in Flax**'
  id: totrans-1118
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: You may want to log sample images when solving computer vision problems. You
    can also log predictions while training the model. For example, you can log prediction
    images containing bounding boxes for an object detection network.
  id: totrans-1119
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'Let''s look at how we can log an image to TensorBoard.from`torch.utils.tensorboard`import`SummaryWriter`import`torchvision.transforms.functional`as`Fwriter
    = SummaryWriter(logdir)`def show(imgs):if not isinstance(imgs, list):'
  id: totrans-1120
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`imgs = [imgs]`'
  id: totrans-1121
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)` for i, img in enumerate(imgs):'
  id: totrans-1122
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`img = img.detach()`'
  id: totrans-1123
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`img = F.to_pil_image(img)`'
  id: totrans-1124
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`axs[0, i].imshow(np.asarray(img))`'
  id: totrans-1125
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`axs[0, i].set(xticklabels=[], yticklabels=[], xticks=`'
  id: totrans-1126
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '[], yticks=[])writer.flush() # Ensure that everything is written to diskNext,
    create a grid with the images that will be logged.'
  id: totrans-1127
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: from`torchvision.utils`import`make_grid`from`torchvision.io`import`read_image`from`pathlib`import`Path`
  id: totrans-1128
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`cat = read_image(str(Path(''train'') / ''cat.1.jpg'')) grid = make_grid(cat)`'
  id: totrans-1129
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`show(grid)`'
  id: totrans-1130
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The `[add_image]` function is used to write images to TensorBoard.`writer.add_image('sample_cat',
    grid)`Now, load the TensorBoard extension and point it to the logs folder.`%tensorboard
    --logdir={logdir}`The logged images will be visible on the Images dashboard. ![](../images/00034.gif)**TensorBoard
    Image dashboard ** **## How to log text with TensorBoard in Flax** Writing text
    to TensorBoard is done using the `[add_text]` function.`writer.add_text('Text',
    'Write image to TensorBoard', 0)` The logged data is available on the Text dashboard.![](../images/00035.jpeg)
  id: totrans-1131
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Track model training in JAX using TensorBoard**'
  id: totrans-1132
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: You can log the evaluation metrics when training machine learning models with
    JAX. They obtained at the training stage. At this point, you can log the metrics
    to TensorBoard. In the example below, we log the training and evaluation metrics.
  id: totrans-1133
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`for epoch in range(1, num_epochs + 1):`'
  id: totrans-1134
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`train_state, train_metrics = train_one_epoch(state, train_l`'
  id: totrans-1135
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`oader)`'
  id: totrans-1136
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`training_loss.append(train_metrics[''loss''])`'
  id: totrans-1137
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`training_accuracy.append(train_metrics[''accuracy'']) print(f"Train epoch:
    {epoch}, loss: {train_metrics[''los`'
  id: totrans-1138
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 's'']}, accuracy: {train_metrics[''accuracy''] * 100}")'
  id: totrans-1139
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`test_metrics = evaluate_model(train_state, test_images, tes t_labels)`'
  id: totrans-1140
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`testing_loss.append(test_metrics[''loss''])`'
  id: totrans-1141
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`testing_accuracy.append(test_metrics[''accuracy''])`'
  id: totrans-1142
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`writer.add_scalar(''Loss/train'', train_metrics[''loss''], epoc h)`'
  id: totrans-1143
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`writer.add_scalar(''Loss/test'', test_metrics[''loss''], epoch)`'
  id: totrans-1144
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`writer.add_scalar(''Accuracy/train'', train_metrics[''accuracy''], epoch)`writer.add_scalar(''Accuracy/test'',
    test_metrics[''accuracy''], epoch)print(f"Test epoch: {epoch}, loss: {test_metrics[''loss'']},
    accuracy: {test_metrics[''accuracy''] * 100}")These metrics will be available
    on the **Scalars** dashboard of TensorBoard.![](../images/00036.gif)'
  id: totrans-1145
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**How to profile JAX programs with TensorBoard**'
  id: totrans-1146
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: To profileJAX programs, send data to the TensorBoard profiler. The first step
    is to install the profile plugin.`pip install -U tensorboard-plugin-profile`
  id: totrans-1147
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Programmatic profiling**'
  id: totrans-1148
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Use `jax.profiler.start_trace()` to start a trace
  id: totrans-1149
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: and `jax.profiler.stop_trace()` to stop a trace.
  id: totrans-1150
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The `[start_trace()]` expects the path to the directory where the traces will
    be written.`import jax`jax.profiler.start_trace("runs")
  id: totrans-1151
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Run the operations to be profiled `key = jax.random.PRNGKey(0)`
  id: totrans-1152
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`x = jax.random.normal(key, (5000, 5000)) y = x @ x`'
  id: totrans-1153
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`y.block_until_ready()`'
  id: totrans-1154
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`jax.profiler.stop_trace()`'
  id: totrans-1155
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Manual profiling with `TensorBoard`**'
  id: totrans-1156
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: 'The second option is to profile the `JAX` program manually. `![](../images/00037.jpeg)`This
    is done in the following steps:'
  id: totrans-1157
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Initialize `TensorBoard` `tensorboard --logdir /runs`Start a `JAX` profiler
    server at the beginning of the program and stop the server at the end of the program.
  id: totrans-1158
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: import `jax.profiler`
  id: totrans-1159
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`jax.profiler.start_server(9999)`'
  id: totrans-1160
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`train_one_epoch(state, train_loader,num_epochs)`'
  id: totrans-1161
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`jax.profiler.stop_server()`'
  id: totrans-1162
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Open the Profile dashboard of `TensorBoard`. Click  **CAPTURE PROFILE** and
    enter the URL of the server that you started above, in this case localhost:9999\.
    Click CAPTURE to start profiling.
  id: totrans-1163
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Select  **trace_viewer** under **Tools** on the profile dashboard. Use the navigation
    tools here to click specific events to see more information about them.
  id: totrans-1164
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**How to profile `JAX` program on a remote machine**'
  id: totrans-1165
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: You can profile a `JAX` program on a remote server by executing the above instructions
    on the remote server. This involves starting a `TensorBoard` server on the remote
    machine, and port forwarding it to your local machine. You will then access `TensorBoard`
    locally via the web UI.
  id: totrans-1166
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`ssh -L 6006:localhost:6006 <remote server address>`'
  id: totrans-1167
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Share `TensorBoard` dashboards**'
  id: totrans-1168
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '`TensorBoard.dev` is a hosted version of `TensorBoard` that makes it easy to
    share your experiments. Let''s upload the above `TensorBoard` to `TensorBoard.dev`.'
  id: totrans-1169
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: On Colab or Jupyter nmotebook
  id: totrans-1170
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`!tensorboard dev upload --logdir ./runs \`'
  id: totrans-1171
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`--name "Flax experiments" \`'
  id: totrans-1172
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`--description "Logging model metrics with JAX" \`'
  id: totrans-1173
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`--one_shot`'
  id: totrans-1174
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: When you run the above code, you will get a prompt to authorize the upload.
    You should be keen not to share sensitive data because `TensorBoard.dev` experiments
    are public.
  id: totrans-1175
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: You can view the experiment on `TensorBoard.dev`.
  id: totrans-1176
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`![](../images/00038.jpeg)`'
  id: totrans-1177
  prefs: []
  stylish: true
  type: TYPE_IMG
- en: '**Final thoughts**'
  id: totrans-1178
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: 'In this article, we have seen how you can use `TensorBoard` to log your experiments
    in `Flax`. More specifically, you have learned: What is `TensorBoard`?'
  id: totrans-1179
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: How to install and launch `TensorBoard`.
  id: totrans-1180
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: How to log images and text to `TensorBoard`.
  id: totrans-1181
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: How to log model metrics to `TensorBoard`.
  id: totrans-1182
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: How to profile `JAX` and `Flax` programs using `TensorBoard` How to upload the
    log to `TensorBoard.dev`.
  id: totrans-1183
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Handling state in `JAX` & `Flax` (BatchNorm and DropOut layers)**'
  id: totrans-1184
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Jitting functions in `Flax` makes them faster but requires that the functions
    have no side effects. The fact that jitted functions can't have side effects introduces
    a challenge when dealing with stateful items such as model parameters and stateful
    layers such as batch normalization layers.
  id: totrans-1185
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: In this article, we'll create a network with the BatchNorm and DropOut layers.
    After that, we'll see how to deal with generating the random number for the DropOut
    layer and adding the batch statistics when training the network.
  id: totrans-1186
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Perform standard imports**'
  id: totrans-1187
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: We kick off by importing standard data science packages that we'll use in this
    article.
  id: totrans-1188
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import torch`'
  id: totrans-1189
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from torch.utils.data import DataLoader import os`'
  id: totrans-1190
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from PIL import Image`'
  id: totrans-1191
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from torchvision import transforms from torch.utils.data import Dataset import
    numpy as np`'
  id: totrans-1192
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import pandas as pd`'
  id: totrans-1193
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from typing import Any`'
  id: totrans-1194
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import matplotlib.pyplot as plt`'
  id: totrans-1195
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`%matplotlib inline`'
  id: totrans-1196
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: ignore harmless warnings
  id: totrans-1197
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`import warnings`'
  id: totrans-1198
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`warnings.filterwarnings("ignore") import jax`'
  id: totrans-1199
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from jax import numpy as jnp`'
  id: totrans-1200
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import flax`'
  id: totrans-1201
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from flax import linen as nn`'
  id: totrans-1202
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from flax.training import train_state import optax`'
  id: totrans-1203
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Download the dataset**'
  id: totrans-1204
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Let's illustrate how to include BatchNorm and DropOut layers in a Flax network
    by designing a simple Convolutional Neural Network using the cat and dogs dataset from
    Kaggle.
  id: totrans-1205
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Download and extract the dataset.
  id: totrans-1206
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import wget`'
  id: totrans-1207
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`wget.download("https://ml.machinelearningnuggets.com/train.zi p")`'
  id: totrans-1208
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import zipfilewith zipfile.ZipFile(''train.zip'', ''r'') as zip_ref: zip_ref.extractall(''.'')`'
  id: totrans-1209
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Loading datasets in JAX**'
  id: totrans-1210
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Since JAX doesn't ship with data loading tools, load the dataset using PyTorch.
    We start by creating a PyTorch Dataset class.
  id: totrans-1211
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`class CatsDogsDataset(Dataset):`'
  id: totrans-1212
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def __init__(self, root_dir, annotation_file, transform=Non`'
  id: totrans-1213
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`e):`'
  id: totrans-1214
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`self.root_dir = root_dir`'
  id: totrans-1215
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`self.annotations = pd.read_csv(annotation_file) self.transform = transform`'
  id: totrans-1216
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def __len__(self):return len(self.annotations)`'
  id: totrans-1217
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def __getitem__(self, index):`'
  id: totrans-1218
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`img_id = self.annotations.iloc[index, 0]`'
  id: totrans-1219
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`img = Image.open(os.path.join(self.root_dir, img_id)).c`'
  id: totrans-1220
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`onvert("RGB")`'
  id: totrans-1221
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`y_label = torch.tensor(float(self.annotations.iloc[inde`'
  id: totrans-1222
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`x, 1]))`'
  id: totrans-1223
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'if self.transform is not None: `img = self.transform(img)return (img, y_label)`Interested
    in learning more about loading datasets in JAX?👉 Check our How to load datasets
    in JAX with TensorFlow tutorial.'
  id: totrans-1224
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Next, create a `Pandas DataFrame` containing the image path and the labels.
  id: totrans-1225
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`train_df = pd.DataFrame(columns=["img_path","label"]) train_df["img_path"]
    = os.listdir("train/") for idx, i in enumerate(os.listdir("train/")):`'
  id: totrans-1226
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'if `"cat"` in `i`:'
  id: totrans-1227
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`train_df["label"][idx] = 0`'
  id: totrans-1228
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'if `"dog"` in `i`:'
  id: totrans-1229
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`train_df["label"][idx] = 1`'
  id: totrans-1230
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`train_df.to_csv (r''train_csv.csv'', index = False, header=True)`'
  id: totrans-1231
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Data processing with PyTorch**'
  id: totrans-1232
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Next, create a function to stack the dataset and return it as a `NumPy array`.
  id: totrans-1233
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def custom_collate_fn(batch):`'
  id: totrans-1234
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`transposed_data = list(zip(*batch)) labels = np.array(transposed_data[1])
    imgs = np.stack(transposed_data[0]) return imgs, labels`'
  id: totrans-1235
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: We then use PyTorch to create training and testing data loaders.`size_image
    = 224 batch_size = 64`
  id: totrans-1236
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`transform = transforms.Compose([`'
  id: totrans-1237
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`transforms.Resize((size_image,size_image)),`'
  id: totrans-1238
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`np.array])`'
  id: totrans-1239
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`dataset = CatsDogsDataset("train","train_csv.csv",transform=tra nsform)`'
  id: totrans-1240
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`train_set, validation_set = torch.utils.data.random_split(datas et,[20000,5000])`'
  id: totrans-1241
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`train_loader = DataLoader(dataset=train_set, collate_fn=custom_ collate_fn,shuffle=True,
    batch_size=batch_size)`'
  id: totrans-1242
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: x = `nn.relu(x)`
  id: totrans-1243
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: variables = `model.init(key, jnp.ones([1, size_image, size_imag e, 3]), training=False)`
  id: totrans-1244
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Define the Flax network with the BatchNorm and DropOut layers. In the network,
    we introduce the `[training]` variable to control when the batch stats should
    be updated. We ensure that they aren't updated during testing.
  id: totrans-1245
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`model = CNN()`'
  id: totrans-1246
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Define Flax model with BatchNorm and DropOut**'
  id: totrans-1247
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: x = `nn.Dense(features=2)(x)`
  id: totrans-1248
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The rate drop out probability.
  id: totrans-1249
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'Whether it''s deterministic. If deterministic inputs are scaled and masked.
    Otherwise, they are not masked and returned as they are.class `CNN(nn.Module)`:'
  id: totrans-1250
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`@nn.compact`'
  id: totrans-1251
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'def `__call__(self, x, training)`:'
  id: totrans-1252
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'The next step is to create the loss function. When applying the model, we:'
  id: totrans-1253
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: x = `nn.relu(x)`
  id: totrans-1254
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: x = `nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))`
  id: totrans-1255
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: x = `nn.Conv(features=64, kernel_size=(3, 3))(x)`
  id: totrans-1256
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: x = `nn.relu(x)`
  id: totrans-1257
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: x = `nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))`
  id: totrans-1258
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: x = `nn.Conv(features=32, kernel_size=(3, 3))(x)`
  id: totrans-1259
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: In the `BatchNorm` layer we set `use_running_average` to `False` meaning
  id: totrans-1260
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: x = `nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))`
  id: totrans-1261
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: x = `x.reshape((x.shape[0], -1))`
  id: totrans-1262
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: x = `nn.Dense(features=256)(x)`
  id: totrans-1263
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: x = `nn.Dense(features=128)(x)`
  id: totrans-1264
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: x = `nn.BatchNorm(use_running_average=not training)(x)`
  id: totrans-1265
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '}'
  id: totrans-1266
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`validation_loader = DataLoader(dataset=validation_set,collate_f n=custom_collate_fn,
    shuffle=False, batch_size=batch_size)`'
  id: totrans-1267
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Compute metrics**'
  id: totrans-1268
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: x = `nn.log_softmax(x)` return `x`
  id: totrans-1269
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Create loss function**'
  id: totrans-1270
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: 'The `[DropOut]` layer takes the following rate:'
  id: totrans-1271
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Pass the batch stats parameters.
  id: totrans-1272
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`[training]` as True. Set the `[batch_stats]` as mutable.'
  id: totrans-1273
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Set the random number for the `DropOut`
  id: totrans-1274
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'def `cross_entropy_loss(*, logits, labels)`:'
  id: totrans-1275
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`labels_onehot = jax.nn.one_hot(labels, num_classes=2)` return `optax.softmax_cross_entropy(logits=logits,
    labels=labe`'
  id: totrans-1276
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: x = `nn.Dropout(0.2, deterministic=not training)(x)`
  id: totrans-1277
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'def `compute_loss(params, batch_stats, images, labels)`:'
  id: totrans-1278
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`logits,batch_stats = CNN().apply({''params'': params,''batch_s tats'': batch_stats},images,
    training=True,rngs={''dropout'': jax. random.PRNGKey(0)}, mutable=[''batch_stats''])`'
  id: totrans-1279
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: loss = `cross_entropy_loss(logits=logits, labels=labels)` return `loss, (logits,
    batch_stats)`
  id: totrans-1280
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: The compute metrics function calculates the loss and accuracy and returns them.
  id: totrans-1281
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'def `compute_metrics(*, logits, labels)`:'
  id: totrans-1282
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: loss = `cross_entropy_loss(logits=logits, labels=labels)` accuracy = `jnp.mean(jnp.argmax(logits,
    -1) == labels)` metrics = {
  id: totrans-1283
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`''loss'': loss`,'
  id: totrans-1284
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`''accuracy'': accuracy`,'
  id: totrans-1285
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: x = `nn.relu(x)`
  id: totrans-1286
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: return `metrics`
  id: totrans-1287
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Create custom Flax training state**'
  id: totrans-1288
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Let's create a custom Flax training state that will store the batch stats information.
    To do that, create a new training state class that subclasses Flax's `TrainState`.
  id: totrans-1289
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: x = `nn.Conv(features=128, kernel_size=(3, 3))(x)`
  id: totrans-1290
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: that the stats stored in `[batch_stats]` will not be used, but batch stats of
    the input will be computed.
  id: totrans-1291
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`key = jax.random.PRNGKey(0)`'
  id: totrans-1292
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: initialize weights
  id: totrans-1293
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`class TrainState(train_state.TrainState): batch_stats: flax.core.FrozenDictTo
    define a Flax training state, use [TrainState.create] and pass the: Apply function.`'
  id: totrans-1294
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Model parameters.
  id: totrans-1295
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The optimizer function. The batch stats.
  id: totrans-1296
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`state = TrainState.create(`'
  id: totrans-1297
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: apply_fn = model.apply,
  id: totrans-1298
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`params = variables[''params''],`'
  id: totrans-1299
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: tx = optax.sgd(0.01),
  id: totrans-1300
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: batch_stats = variables['batch_stats'],
  id: totrans-1301
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: )
  id: totrans-1302
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Training step**'
  id: totrans-1303
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: In the training step, we compute the gradients with respect to the loss and
    model parameters– the **model parameters **and **batch statistics**. We use the
    gradients to update the model parameters and return the new state and model metrics.
    The function is decorated
  id: totrans-1304
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: with @jax.jit to make the computation faster.
  id: totrans-1305
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '@jax.jit'
  id: totrans-1306
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def train_step(state,images, labels):`'
  id: totrans-1307
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '"""Train for a single step."""'
  id: totrans-1308
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`(batch_loss, (logits, batch_stats)), grads= jax.value_and_gra d(compute_loss,
    has_aux=True)(state.params,state.batch_stats, i mages,labels)`'
  id: totrans-1309
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`state = state.apply_gradients(grads=grads)`'
  id: totrans-1310
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`metrics = compute_metrics(logits=logits, labels=labels) return state, metricsNext,
    define a function that applies the training step for one epoch. The functions:`'
  id: totrans-1311
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Loops through the training data.
  id: totrans-1312
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Passes each training batch the training step.
  id: totrans-1313
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Obtains the batch metrics. Computes the mean to obtain the epoch metrics.
  id: totrans-1314
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Returns the new state and metrics.
  id: totrans-1315
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def train_one_epoch(state, dataloader):`'
  id: totrans-1316
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '"""Train for 1 epoch on the training set.""" batch_metrics = []'
  id: totrans-1317
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'for cnt, (images, labels) in enumerate(dataloader):'
  id: totrans-1318
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`images = images / 255.0`'
  id: totrans-1319
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`state, metrics = train_step(state, images, labels) batch_metrics.append(metrics)`'
  id: totrans-1320
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`batch_metrics_np = jax.device_get(batch_metrics) epoch_metrics_np = {k: np.mean([metrics[k]
    for metrics in batch_metrics_n`'
  id: totrans-1321
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: p])
  id: totrans-1322
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: for k in batch_metrics_np[0] }
  id: totrans-1323
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`return state, epoch_metrics_np`'
  id: totrans-1324
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Evaluation step**'
  id: totrans-1325
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: We pass the test images and labels to the model in the evaluation step and obtain
    the evaluation metrics. The function is also jitted to take advantage of JAX's
    fast computation. During the evaluation, set [training] to [False] so that the
    model parameters are not updated. In this step, we also pass the batch stats and
    the random number generator for the [DropOut] layer.
  id: totrans-1326
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '@jax.jitdef eval_step(batch_stats, params, images, labels): logits = CNN().apply({''params'':
    params,''batch_stats'': batch _stats}, images, training=False,rngs={''dropout'':
    jax.random.PRN GKey(0)})return compute_metrics(logits=logits, labels=labels)The
    [evaluate_model] function applies the [eval_step] to the test data and returns
    the evaluation metrics.'
  id: totrans-1327
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def evaluate_model(state, test_imgs, test_lbls):`'
  id: totrans-1328
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '"""Evaluate on the validation set."""'
  id: totrans-1329
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`metrics = eval_step(state.batch_stats,state.params, test_im`'
  id: totrans-1330
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: gs, test_lbls)
  id: totrans-1331
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`metrics = jax.device_get(metrics)`'
  id: totrans-1332
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`metrics = jax.tree_map(lambda x: x.item(), metrics) return metrics`'
  id: totrans-1333
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Train Flax model**'
  id: totrans-1334
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: To train the model, we define another function that implements `[train_one_epoch]`.
    Let's start by defining the model evaluation data.(`test_images`, `test_labels`)
    = next(iter(validation_loader)) `test_images = test_images / 255.0`
  id: totrans-1335
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Set up TensorBoard in Flax**'
  id: totrans-1336
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: You can log the model metrics to TensorBoard by writing the scalars to TensorBoard.
  id: totrans-1337
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from torch.utils.tensorboard import SummaryWriter import torchvision.transforms.functional
    as F logdir = "flax_logs"`'
  id: totrans-1338
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`writer = SummaryWriter(logdir)`'
  id: totrans-1339
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Train model**'
  id: totrans-1340
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: We can also append the metrics to a list and visualize them with Matplotlib.
  id: totrans-1341
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`training_loss = [] training_accuracy = [] testing_loss = []`'
  id: totrans-1342
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`testing_accuracy = []`'
  id: totrans-1343
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'Next, define the training function that will: Train the Flax model for the
    specified number of epochs.'
  id: totrans-1344
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Evaluate the model on the test data.
  id: totrans-1345
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Append the metrics to a list. Write model metrics to TensorBoard.
  id: totrans-1346
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Print the metrics on every epoch. Return the trained model state`def train_model(epochs):for
    epoch in range(1, epochs + 1):train_state, train_metrics = train_one_epoch(state,
    tra`
  id: totrans-1347
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`in_loader)`'
  id: totrans-1348
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`training_loss.append(train_metrics[''loss''])` `training_accuracy.append(train_metrics[''accuracy''])`
    `test_metrics = evaluate_model(train_state, test_images,`'
  id: totrans-1349
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`test_labels`'
  id: totrans-1350
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`testing_loss.append(test_metrics[''loss''])`'
  id: totrans-1351
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`testing_accuracy.append(test_metrics[''accuracy''])`'
  id: totrans-1352
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`writer.add_scalar(''Loss/train'', train_metrics[''loss''], epoch)`'
  id: totrans-1353
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`writer.add_scalar(''Loss/test'', test_metrics[''loss''], epoch)`'
  id: totrans-1354
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`writer.add_scalar(''Accuracy/train'', train_metrics[''accuracy''], epoch)`'
  id: totrans-1355
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`writer.add_scalar(''Accuracy/test'', test_metrics[''accuracy''], epoch)`'
  id: totrans-1356
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`print(f"Epoch: {epoch}, training loss: {train_metrics [''loss'']}, training
    accuracy: {train_metrics[''accuracy''] * 10 0}, validation loss: {test_metrics[''loss'']},
    validation accuracy: {test_metrics[''accuracy''] * 100}")`'
  id: totrans-1357
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`return train_stateRun the training function to train the model. trained_model_state
    = train_model(30)`'
  id: totrans-1358
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Save Flax model**'
  id: totrans-1359
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: 'The `[save_checkpoint]` saves a Flax model. It expects:'
  id: totrans-1360
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The directory to save the model checkpoint.
  id: totrans-1361
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The Flax trained model, in this case `[trained_model_state]`. The model's prefix.
  id: totrans-1362
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Whether to overwrite existing models.
  id: totrans-1363
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from flax.training import checkpoints`'
  id: totrans-1364
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: ckpt_dir = 'model_checkpoint/'
  id: totrans-1365
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`checkpoints.save_checkpoint(ckpt_dir=ckpt_dir`'
  id: totrans-1366
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`target=trained_model_state`, `step=100,`'
  id: totrans-1367
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`prefix=''flax_model'', overwrite=True`'
  id: totrans-1368
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: )![](../images/00039.jpeg)
  id: totrans-1369
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Load Flax model**'
  id: totrans-1370
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: The `[restore_checkpoint]` method loads a saved Flax model from the saved location.
  id: totrans-1371
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`loaded_model = checkpoints.restore_checkpoint(`'
  id: totrans-1372
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`ckpt_dir=ckpt_dir`, `target=state`, `prefix=''flax_mode`'
  id: totrans-1373
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`l''`'
  id: totrans-1374
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Evaluate Flax model**'
  id: totrans-1375
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Run the `[evalaute_model]` function to check the performance of the model on
    test data. `evaluate_model(trained_model_state,test_images, test_labels)`
  id: totrans-1376
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Visualize Flax model performance**'
  id: totrans-1377
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: To visualize the performance of the Flax model, you can plot the metrics using
    Matplotlib or load TensorBoard and check the scalars tab.
  id: totrans-1378
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`%load_ext tensorboard` `%tensorboard --logdir={logdir}`'
  id: totrans-1379
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Final thoughts**'
  id: totrans-1380
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '`In this article, you have seen how to build networks in Flax containing BatchNorm
    and DropOut layers. You have also seen how to adjust the training process to cater
    to these new layers. Specifically, you have learned:`'
  id: totrans-1381
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`How to define Flax models with BatchNorm and DropOut layers.`'
  id: totrans-1382
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`How to create a custom Flax training state.`'
  id: totrans-1383
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`Training and evaluating a Flax model with BatchNorm and DropOut layers.`'
  id: totrans-1384
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`How to save and load a Flax model.`'
  id: totrans-1385
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`How to evaluate the performance of a Flax model`'
  id: totrans-1386
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`**LSTM in JAX & Flax**`'
  id: totrans-1387
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '`LSTMs are a class of neural networks used to solve sequence problems such
    as time series and natural language processing. The LSTMs maintain some internal
    state that is useful in solving these problems. LSTMs apply for loops to iterate
    over each time step. We can use functions from JAX and Flax instead of writing
    these for loops from scratch. In this article, we will build a natural language
    processing model using LSTMs in Flax.`'
  id: totrans-1388
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`Let''s get started.`'
  id: totrans-1389
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`**Dataset download**`'
  id: totrans-1390
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '`We''ll use the movie review dataset from Kaggle. We download the dataset using
    Kaggle''s Python package.`'
  id: totrans-1391
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import os`'
  id: totrans-1392
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 进入`#Obtain from https://www.kaggle.com/username/account` `os.environ["KAGGLE_USERNAME"]="KAGGLE_USERNAME"`
  id: totrans-1393
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`os.environ["KAGGLE_KEY"]="KAGGLE_KEY"`'
  id: totrans-1394
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import kaggle`'
  id: totrans-1395
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`kaggle datasets download lakshmi25npathi/imdb-dataset-of-50k-mo vie-reviews`'
  id: totrans-1396
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`Next, extract the dataset.import zipfilewith zipfile.ZipFile(''imdb-dataset-of-50k-movie-reviews.zip'',
    ''r'') as zip_ref:zip_ref.extractall(''imdb-dataset-of-50k-movie-reviews'')Load
    the dataset using Pandas and display a sample of the reviews.`'
  id: totrans-1397
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`df = pd.read_csv("imdb-dataset-of-50k-movie-reviews/IMDB Datase t.csv")`'
  id: totrans-1398
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`df.head()`'
  id: totrans-1399
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`![](../images/00040.gif)`'
  id: totrans-1400
  prefs: []
  stylish: true
  type: TYPE_IMG
- en: '`**Data processing with NLTK**`'
  id: totrans-1401
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '`The dataset contains unnecessary characters for predicting whether a movie
    review is negative or positive. For instance, punctuation marks and special characters.
    We, therefore, remove these from the reviews. We also need to convert the [sentiment] column
    into a numerical representation. This is achieved using  [LabelEncoder] from Scikitlearn.
    Let''s import that together with other packages we''ll use throughout this article.`'
  id: totrans-1402
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import numpy as np`'
  id: totrans-1403
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import pandas as pd`'
  id: totrans-1404
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from numpy import array`'
  id: totrans-1405
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import tensorflow as tf`'
  id: totrans-1406
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from sklearn.model_selection import train_test_split from sklearn.preprocessing
    import LabelEncoder import matplotlib.pyplot as plt`'
  id: totrans-1407
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`The reviews also contain words that are not useful in the sentiment prediction.
    These are common words in English, such as the, at, and, etc. These words are
    known as **stopwords**. We remove them with the help of the nltk library. Let''s
    start by defining a function to remove all the English stopwords.`'
  id: totrans-1408
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`pip install nltk`'
  id: totrans-1409
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`import nltk`'
  id: totrans-1410
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from nltk.corpus import stopwords`'
  id: totrans-1411
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`nltk.download(''stopwords'')`'
  id: totrans-1412
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def remove_stop_words(review):`'
  id: totrans-1413
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`review_minus_sw = []`'
  id: totrans-1414
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`stop_words = stopwords.words(''english'')`'
  id: totrans-1415
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`review = review.split()`'
  id: totrans-1416
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`cleaned_review = [review_minus_sw.append(word) for word in`'
  id: totrans-1417
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`review if word not in stop_words]`'
  id: totrans-1418
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`cleaned_review = '' ''.join(review_minus_sw)`'
  id: totrans-1419
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`return cleaned_review`'
  id: totrans-1420
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Apply the function to the sentiment column.`df['review'] = df['review'].apply(remove_stop_words)`Let's
    also convert the sentiment column to numerical representation.
  id: totrans-1421
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`labelencoder = LabelEncoder()`'
  id: totrans-1422
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`df = df.assign(sentiment = labelencoder.fit_transform(df["senti ment"]))`'
  id: totrans-1423
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Compare the reviews with the review with and without the stop words.![](../images/00041.jpeg)
  id: totrans-1424
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Looking at the third review, we notice that the
  id: totrans-1425
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`words [this], [was] and [a] have been dropped from the sentence. However,
    we can still see some special characters, such as [<br>] in the review. Let''s
    resolve that next.`'
  id: totrans-1426
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Text vectorization with Keras**'
  id: totrans-1427
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: The review data is still in text form. However, we need to convert it to a numeric
    representation like the sentiment column. Before we do that, let's split the dataset
    into a training and testing set.
  id: totrans-1428
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from sklearn.model_selection import train_test_split df = df.drop_duplicates()`'
  id: totrans-1429
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`docs = df[''review'']`'
  id: totrans-1430
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`labels = array(df[''sentiment''])`'
  id: totrans-1431
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`X_train, X_test , y_train, y_test = train_test_split(docs, labe ls , test_size
    = 0.20, random_state=0)`'
  id: totrans-1432
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'We use the Keras text vectorization layer to convert the reviews to integer
    form. This function lets us filter out all punctuation marks and convert the reviews
    to lowercase. We pass the following parameters:'
  id: totrans-1433
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`standardize` as `lower_and_strip_punctuation` to convert to'
  id: totrans-1434
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: lowercase and remove punctuation marks.
  id: totrans-1435
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`[output_mode]` to `[int]` to get the result as integers. `[tf_idf]` would
    apply the TF-IDF algorithm.'
  id: totrans-1436
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`[output_sequence_length]` as 50 to get sentences of that length. Change this
    number to see how it affects the model''s performance. I found 50 to five some
    good results. Sentences longer than the specified length will be truncated, while
    shorter ones will be padded with zeros.'
  id: totrans-1437
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`[max_tokens]` as 10,000 to have a vocabulary size of that number. Tweak this
    number and check how the model''s performance changes.'
  id: totrans-1438
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: After defining the vectorization layer, we apply it to the training data. This
    is done by calling the `adapt` function. The function computes the vocabulary
    from the provided dataset. The vocabulary will be truncated to `[max_tokens]`,
    if that is provided.
  id: totrans-1439
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import tensorflow as tf`'
  id: totrans-1440
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`max_features = 10000` # Maximum vocab size.'
  id: totrans-1441
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`batch_size = 128`'
  id: totrans-1442
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`max_len = 50` # Sequence length to pad the outputs to. `vectorize_layer =
    tf.keras.layers.TextVectorization(standardize =''lower_and_strip_punctuation'',max_tokens=max_features,output_m
    ode=''int'',output_sequence_length=max_len)`'
  id: totrans-1443
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`vectorize_layer.adapt(X_train)`'
  id: totrans-1444
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: To view the generated vocabulary, call the `get_vocabulary` function.`vectorize_layer.get_vocabulary()`![](../images/00042.jpeg)Convert
    the training and test data to numerical form using the trained vectorization layer.`X_train_padded
    = vectorize_layer(X_train)` `X_test_padded = vectorize_layer(X_test)`![](../images/00043.gif)
  id: totrans-1445
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Create tf.data dataset**'
  id: totrans-1446
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Let's generate and prefetch batches from the training and test set to make loading
    them to the LSTM model more efficient. We start by creating a `tf.data.Dataset`.
  id: totrans-1447
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`training_data = tf.data.Dataset.from_tensor_slices((X_train_padded, y_train))`'
  id: totrans-1448
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`validation_data = tf.data.Dataset.from_tensor_slices((X_test_padded, y_test))`'
  id: totrans-1449
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`training_data = training_data.batch(batch_size)`'
  id: totrans-1450
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`validation_data = validation_data.batch(batch_size)`'
  id: totrans-1451
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Next, we prefetch one batch, shuffle the data and return it as a `NumPy array.`
  id: totrans-1452
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`pip install tensorflow_datasets`'
  id: totrans-1453
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`import tensorflow_datasets as tfds`'
  id: totrans-1454
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def get_train_batches():`'
  id: totrans-1455
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`ds = training_data.prefetch(1)`'
  id: totrans-1456
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`ds = ds.shuffle(3, reshuffle_each_iteration=True)` # `tfds.dataset_as_numpy`
    converts the `tf.data.Dataset` into an'
  id: totrans-1457
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: iterable of NumPy arrays`return tfds.as_numpy(ds)`
  id: totrans-1458
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`Define LSTM model in Flax`'
  id: totrans-1459
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: We are now ready to define the LSTM model in Flax. To design LSTMs in Flax,
    we use the `LSTMCell` or the `OptimizedLSTMCell`.
  id: totrans-1460
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The `OptimizedLSTMCell` is the efficient `LSTMCell`.
  id: totrans-1461
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'The `[LSTMCell.initialize_carry]` function is used to initialize the hidden
    state of the LSTM cell. It expects:'
  id: totrans-1462
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: A random number.
  id: totrans-1463
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The batch dimensions.
  id: totrans-1464
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The number of units.
  id: totrans-1465
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'Let''s use the `setup method` to define the LSTM model. The LSTM contains the
    following layers:'
  id: totrans-1466
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: An `Embedding layer` with the same number of features and length as defined
    in the vectorization layer.
  id: totrans-1467
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: LSTM layers that pass data in one direction as specified by the `[reverse]`
    argument.
  id: totrans-1468
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: A couple of `Dense layers`.
  id: totrans-1469
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Final dense output layer.from flax import linen as nn
  id: totrans-1470
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`class LSTMModel(nn.Module):`'
  id: totrans-1471
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def setup(self):`'
  id: totrans-1472
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`self.embedding = nn.Embed(max_features, max_len) lstm_layer = nn.scan(nn.OptimizedLSTMCell,`'
  id: totrans-1473
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`variable_broadcast="params", split_rngs={"params": False}, in_axes=1,`'
  id: totrans-1474
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`out_axes=1,`'
  id: totrans-1475
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`length=max_len,`'
  id: totrans-1476
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`reverse=False)`'
  id: totrans-1477
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`self.lstm1 = lstm_layer()`'
  id: totrans-1478
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`self.dense1 = nn.Dense(256)`'
  id: totrans-1479
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`self.lstm2 = lstm_layer()`'
  id: totrans-1480
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`self.dense2 = nn.Dense(128)`'
  id: totrans-1481
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`self.lstm3 = lstm_layer()`'
  id: totrans-1482
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`self.dense3 = nn.Dense(64)`'
  id: totrans-1483
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`self.dense4 = nn.Dense(2)`'
  id: totrans-1484
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`@nn.rematdef __call__(self, x_batch): x = self.embedding(x_batch)`'
  id: totrans-1485
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`carry, hidden = nn.OptimizedLSTMCell.initialize_carry(jax.random.PRNGKey(0),
    batch_dims=(len(x_batch),), size=128)`'
  id: totrans-1486
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`(carry, hidden), x = self.lstm1((carry, hidden), x)`'
  id: totrans-1487
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`x = self.dense1(x) x = nn.relu(x)`'
  id: totrans-1488
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`carry, hidden = nn.OptimizedLSTMCell.initialize_carry(jax.random.PRNGKey(0),
    batch_dims=(len(x_batch),), size=64)`'
  id: totrans-1489
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`(carry, hidden), x = self.lstm2((carry, hidden), x) x = self.dense2(x) x =
    nn.relu(x)`'
  id: totrans-1490
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`carry, hidden = nn.OptimizedLSTMCell.initialize_carry(jax.random.PRNGKey(0),
    batch_dims=(len(x_batch),), size=32)`'
  id: totrans-1491
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`(carry, hidden), x = self.lstm3((carry, hidden), x)`'
  id: totrans-1492
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`x = self.dense3(x)`'
  id: totrans-1493
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`x = nn.relu(x)`'
  id: totrans-1494
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`x = self.dense4(x[:, -1]) return nn.log_softmax(x)`'
  id: totrans-1495
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: We apply the `scan function` to iterate over the data. It expects:`scan` the
    items to be looped over. They must be the same size and will be stacked along
    the scan axis.`carry` a carried value that is updated at each iteration. The value
    must be the same shape and `[dtype]` throughout the iteration.
  id: totrans-1496
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`[broadcast]` a value that is closed over by the loop ` [<axis:int>]` axis
    along which to scan.'
  id: totrans-1497
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`[split_rngs]` to define if to split the random number generator at each step.'
  id: totrans-1498
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The [nn.remat] call saves memory when using LSTMs to compute long sequences.
  id: totrans-1499
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Compute metrics in Flax
  id: totrans-1500
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Next, we define a function to compute the loss and accuracy of the network.
  id: totrans-1501
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: import optax
  id: totrans-1502
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: import jax.numpy as jnp
  id: totrans-1503
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'def compute_metrics(logits, labels):'
  id: totrans-1504
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: loss = jnp.mean(optax.softmax_cross_entropy(logits, jax.nn.on e_hot(labels,
    num_classes=2)))
  id: totrans-1505
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)
  id: totrans-1506
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: metrics = {
  id: totrans-1507
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '''loss'': loss,'
  id: totrans-1508
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '''accuracy'': accuracy'
  id: totrans-1509
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '}'
  id: totrans-1510
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: return metrics
  id: totrans-1511
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Create training state
  id: totrans-1512
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: 'The training state applies gradients and updates the parameters and optimizer
    state. Flax provides [train_state] for this purpose. We define a function that:'
  id: totrans-1513
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Creates an instance of the [LSTMModel].
  id: totrans-1514
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Initializes the model to obtain the [`params`] by passing a sample of the training
    data.
  id: totrans-1515
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Returns the created state after applying the Adam optimizer.from flax.training
    import train_state
  id: totrans-1516
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'def create_train_state(rng):'
  id: totrans-1517
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '"""Creates initial `TrainState`."""'
  id: totrans-1518
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: model = LSTMModel()
  id: totrans-1519
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: params = model.init(rng, jnp.array(X_train_padded[0]))['param
  id: totrans-1520
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: s']
  id: totrans-1521
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: tx = optax.adam(0.001,0.9,0.999,1e-07) return train_state.TrainState.create(
  id: totrans-1522
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: apply_fn=model.apply, params=params, tx=tx)
  id: totrans-1523
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Define training step
  id: totrans-1524
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: 'The training function does the following:'
  id: totrans-1525
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Compute the loss and logits from the model with the `apply` method.
  id: totrans-1526
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Compute the gradients using [value_and_grad]. Use the gradients to update the
    model parameters.
  id: totrans-1527
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Compute the metrics using the function defined earlier. Returns the state and
    metrics.
  id: totrans-1528
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'Applying [@jax.jit] makes the function run faster.`@jax.jit`def train_step(state,
    text, labels):def loss_fn(params):'
  id: totrans-1529
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'logits = LSTMModel().apply({''params'': params}, text) loss = jnp.mean(optax.softmax_cross_entropy(
    logits=logits,'
  id: totrans-1530
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: labels=jax.nn.one_hot(labels, num_classes=2))) return loss, logits
  id: totrans-1531
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: grad_fn = jax.value_and_grad(loss_fn, has_aux=True) (_, logits), grads = grad_fn(state.params)
  id: totrans-1532
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: state = state.apply_gradients(grads=grads)
  id: totrans-1533
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: metrics = compute_metrics(logits, labels)
  id: totrans-1534
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: return state, metrics
  id: totrans-1535
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Evaluate the Flax model
  id: totrans-1536
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: The [`eval_step`] evaluates the model's performance on the test set using `Module.apply`.
    It returns the loss and accuracy on the testing set.
  id: totrans-1537
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The [`evaluate_model`] function applies the [`eval_step`] , obtains the metrics
    from the device and returns them as a [`jax.tree_map`].
  id: totrans-1538
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '@jax.jit'
  id: totrans-1539
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'def eval_step(state, text, labels):'
  id: totrans-1540
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'logits = LSTMModel().apply({''params'': state.params}, text)'
  id: totrans-1541
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: return compute_metrics(logits=logits, labels=labels)
  id: totrans-1542
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'def evaluate_model(state, text, test_lbls): """Evaluate on the validation set."""'
  id: totrans-1543
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: metrics = eval_step(state, text, test_lbls) metrics = jax.device_get(metrics)
  id: totrans-1544
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'metrics = jax.tree_map(lambda x: x.item(), metrics) return metrics'
  id: totrans-1545
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Create training function
  id: totrans-1546
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Next, define a function that trains the Flax LSTM model on one epoch. The function
    applies  [train_step] to each batch in the training data. After each batch, it
    appends the metrics to a list.
  id: totrans-1547
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'def train_one_epoch(state):'
  id: totrans-1548
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Train for 1 epoch on the training set. batch_metrics = []
  id: totrans-1549
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'for text, labels in get_train_batches():'
  id: totrans-1550
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'state, metrics = `train_step(state, text, labels)` batch_metrics.append(metrics)batch_metrics_np
    = jax.device_get(batch_metrics) epoch_metrics_np = {k: np.mean([metrics[k] for
    metrics in batch_metrics_np[0] ])for k in batch_metrics_np[0] }return state, epoch_metrics_np'
  id: totrans-1551
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The function obtains the metrics from the device and computes the mean from
    all the trained batches. This gives the loss and accuracy for one epoch.
  id: totrans-1552
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Train LSTM model in Flax
  id: totrans-1553
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: To train the LSTM model, we run the [train_one_epoch] function for several iterations.
    Next, apply the [evaluate_model] to obtain the test metrics for each epoch. Before
    training starts, we create
  id: totrans-1554
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: a [create_train_state] to hold the training information. The function initializes
    the model parameters and the optimizer. This information is stored in the training
    state dataclass.
  id: totrans-1555
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'rng = jax.random.PRNGKey(0)rng, input_rng, init_rng = jax.random.split(rng,num=3)seed
    = 0state = `create_train_state(init_rng)` del init_rng # Must not be used anymore.'
  id: totrans-1556
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: num_epochs = 30
  id: totrans-1557
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: (text, test_labels) = next(iter(validation_data)) text = jnp.array(text)
  id: totrans-1558
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: test_labels = jnp.array(test_labels) training_loss = []
  id: totrans-1559
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: training_accuracy = []
  id: totrans-1560
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: testing_loss = []
  id: totrans-1561
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: testing_accuracy = []
  id: totrans-1562
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'def `train_model()`:'
  id: totrans-1563
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'for epoch in range(1, num_epochs + 1):'
  id: totrans-1564
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: train_state, train_metrics = `train_one_epoch(state)` training_loss.append(train_metrics['loss'])
    training_accuracy.append(train_metrics['accuracy']) test_metrics = `evaluate_model(train_state,
    text, test_l`
  id: totrans-1565
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: abels)
  id: totrans-1566
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: testing_loss.append(test_metrics['loss'])
  id: totrans-1567
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'testing_accuracy.append(test_metrics[''accuracy'']) print(f"Epoch: {epoch},
    train loss: {train_metrics[''los'
  id: totrans-1568
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 's'']}, train accuracy: {train_metrics[''accuracy''] * 100}, test l oss: {test_metrics[''loss'']},
    test accuracy: {test_metrics[''accu racy''] * 100}")'
  id: totrans-1569
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: return `train_statetrained_model_state = train_model()`After each epoch, we
    print the metrics and append them to a list.
  id: totrans-1570
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Visualize LSTM model performance in Flax
  id: totrans-1571
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: You can then use `Matplotlib` to visualize the metrics appended to the list.
    The training is not quite smooth, but you can tweak the architecture of the network,
    the length of each review, and the vocabulary size to improve performance.
  id: totrans-1572
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Save LSTM model
  id: totrans-1573
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: 'To save a Flax model checkpoint, use the [save_checkpoint] method. It expects:'
  id: totrans-1574
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The directory to save the checkpoint files.
  id: totrans-1575
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The Flax object to be saved, that is, [target].
  id: totrans-1576
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The prefix of the checkpoint file name.
  id: totrans-1577
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Whether to overwrite previous checkpoints
  id: totrans-1578
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: from flax.training import `checkpoints`
  id: totrans-1579
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: checkpoints.save_checkpoint(ckpt_dir='lstm_model_checkpoint/', target=trained_model_state,
  id: totrans-1580
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: step=100,
  id: totrans-1581
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: prefix='lstm_model',
  id: totrans-1582
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: overwrite=False
  id: totrans-1583
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: )
  id: totrans-1584
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: To restore the saved model, use [restore_checkpoint] method.
  id: totrans-1585
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: loaded_model = `checkpoints.restore_checkpoint(`
  id: totrans-1586
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: ckpt_dir='lstm_mod
  id: totrans-1587
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: el_checkpoint/',
  id: totrans-1588
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: target=state, prefix='lstm_mode
  id: totrans-1589
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: l'
  id: totrans-1590
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: )
  id: totrans-1591
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: loaded_model ![](../images/00044.gif)This model can be used to make predictions
    right away.![](../images/00045.gif)
  id: totrans-1592
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Final thoughts
  id: totrans-1593
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '`You have learned to solve natural language processing problems with JAX and
    Flax in this article. In particular, the nuggets you have covered include:`'
  id: totrans-1594
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`How to process text data with NLTK.`'
  id: totrans-1595
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`Text vectorization with Keras.`'
  id: totrans-1596
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Creating batches of text data with Keras and TensorFlow.
  id: totrans-1597
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`How to create LSTM models in JAX and Flax. How to train and evaluate the LSTM
    model in Flax. Saving and restoring Flax LSTM models.`'
  id: totrans-1598
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Flax vs. TensorFlow**'
  id: totrans-1599
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '`Flax is the neural network library for JAX. TensorFlow is a deep learning
    library with a large ecosystem of tools and resources. Flax and TensorFlow are
    similar but different in some ways. For instance, both Flax and TensorFlow can
    run on XLA.`'
  id: totrans-1600
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`Let''s look at the differences between Flax and TensorFlow from my perspective
    as a user of both libraries.`'
  id: totrans-1601
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Random number generation in TensorFlow and Flax**'
  id: totrans-1602
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '`In TensorFlow, you can set global or function level seeds. Generating random
    numbers in TensorFlow is quite straightforward.`tf.random.set_seed(6853)`'
  id: totrans-1603
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: However, this is not the case in Flax. Flax is built on top of JAX. JAX expects
    pure functions, meaning functions without any side effects. To achieve this JAX
    introduces stateless pseudo-random number generators (PRNGs). For example, calling
    the random number generator from NumPy will result in a different number every
    time.
  id: totrans-1604
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import numpy as np`'
  id: totrans-1605
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`print(np.random.random()) print(np.random.random()) print(np.random.random())`'
  id: totrans-1606
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`![](../images/00046.jpeg)`'
  id: totrans-1607
  prefs: []
  stylish: true
  type: TYPE_IMG
- en: '`In JAX and Flax, the result should be the same on every call. We, therefore,
    generate random numbers from a random state. The state should not be re-used.
    It can be split to obtain several pseudo-random numbers.`'
  id: totrans-1608
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import jax`'
  id: totrans-1609
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`key = jax.random.PRNGKey(0)`'
  id: totrans-1610
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`key1, key2, key3 = jax.random.split(key, num=3)`'
  id: totrans-1611
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`![](../images/00047.gif)`'
  id: totrans-1612
  prefs: []
  stylish: true
  type: TYPE_IMG
- en: '**Model definition in Flax and TensorFlow**'
  id: totrans-1613
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '`Model definition in TensorFlow is made easy by the Keras API. You can use
    Keras to define Sequential or Functional networks. Keras has many layers for designing
    various types of networks, such as CNNs, and LSTMS.`'
  id: totrans-1614
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`In Flax, networks are designed using the setup or compact way. The setup method
    is explicit, while the compact way is in-line. Setup is very similar to how networks
    are designed in PyTorch. For example, here is a network designed with the setup
    way.`'
  id: totrans-1615
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`class MLP(nn.Module):def setup(self):`'
  id: totrans-1616
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`Submodule names are derived by the attributes you assign to. In this`'
  id: totrans-1617
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`case`, `"dense1"` and `"dense2"`. This follows the logic in Py Torch.'
  id: totrans-1618
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`self.dense1 = nn.Dense(32)`'
  id: totrans-1619
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`self.dense2 = nn.Dense(32)`'
  id: totrans-1620
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def __call__(self, x): x = self.dense1(x) x = nn.relu(x)`'
  id: totrans-1621
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`x = self.dense2(x) return x`'
  id: totrans-1622
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Here's the same network designed in a compact way. The compact way is more straightforward
    because there is less code duplicity.`class MLP(nn.Module):`
  id: totrans-1623
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`@nn.compact`'
  id: totrans-1624
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def __call__(self, x):`'
  id: totrans-1625
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`x = nn.Dense(32, name="dense1")(x)`'
  id: totrans-1626
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`x = nn.relu(x)`'
  id: totrans-1627
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`x = nn.Dense(32, name="dense2")(x)`'
  id: totrans-1628
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`return x`'
  id: totrans-1629
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Activations in Flax and TensorFlow**'
  id: totrans-1630
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: The `[tf.keras.activations]` module in TensorFlow provides most of the activations
    needed when designing networks. In Flax, activation functions are available via
    the linen module.
  id: totrans-1631
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Optimizers in Flax and TensorFlow**'
  id: totrans-1632
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: The `[tf.keras.optimizers]` in TensorFlow has popular optimizer functions. However,
    Flax doesn't ship with any optimizer functions. Optimizers used in Flax are provided
    by another library known as Optax.
  id: totrans-1633
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Metrics in Flax and TensorFlow**'
  id: totrans-1634
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: In TensorFlow, metrics are available via
  id: totrans-1635
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: the `[tf.keras.metrics]` module. As of this writing, Flax has no metrics module.
    You'll need to define metric functions for your networks or use other third-party
    libraries.
  id: totrans-1636
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import optax`'
  id: totrans-1637
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import jax.numpy as jnp`'
  id: totrans-1638
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def compute_metrics(logits, labels):`'
  id: totrans-1639
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`loss = jnp.mean(optax.softmax_cross_entropy(logits, jax.nn.on e_hot(labels,
    num_classes=2)))`'
  id: totrans-1640
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)`'
  id: totrans-1641
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`metrics = {`'
  id: totrans-1642
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`''loss'': loss,`'
  id: totrans-1643
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`''accuracy'': accuracy`'
  id: totrans-1644
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`}`'
  id: totrans-1645
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`return metrics`'
  id: totrans-1646
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Computing gradients in Flax and TensorFlow**'
  id: totrans-1647
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: The `[jax.grad]` function is used to compute gradients in Flax. It offers the
    ability to return auxillary data. For example, you can return loss and gradients
    at the same time.
  id: totrans-1648
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`@jax.jitdef sum_logistic(x):return jnp.sum(1.0 / (1.0 + jnp.exp(-x))),(x +
    1)`'
  id: totrans-1649
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`x_small = jnp.arange(6.)`'
  id: totrans-1650
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`derivative_fn = jax.grad(sum_logistic, has_aux=True) print(derivative_fn(x_small))`'
  id: totrans-1651
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`(DeviceArray([0.25 , 0.19661194, 0.10499357, 0.04517666, 0.01766271,`'
  id: totrans-1652
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`0.00664806], dtype=float32), DeviceArray([1., 2.,`'
  id: totrans-1653
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`3., 4., 5., 6.], dtype=float32))Advanced automatic differentiation can also
    be done`'
  id: totrans-1654
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: using `jax.vjp()` and `jax.jvp()`.
  id: totrans-1655
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: In TensorFlow, gradients are computed using `[tf.GradientTape].def grad(model,
    inputs, targets):with tf.GradientTape() as tape:loss_value = loss(model, inputs,
    targets, training=True) return loss_value, tape.gradient(loss_value, model.trainable_
    variables)`
  id: totrans-1656
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Unless you are creating custom training loops in TensorFlow, you will not define
    a gradient function. This is done automatically when you train the network.
  id: totrans-1657
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Loading datasets in Flax and TensorFlow**'
  id: totrans-1658
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: TensorFlow provides utilities for loading data. Flax doesn't ship with any data
    loaders. You have to use the data loaders from other libraries such as TensorFlow.
    As long as the data is in JAX NumPy or regular arrays and has the proper shape,
    it can be passed to Flax networks.
  id: totrans-1659
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Training model in Flax vs. TensorFlow**'
  id: totrans-1660
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Training models in TensorFlow is done by compiling the network and calling the
    fit method. However, in Flax, we create a training state to hold the training
    information and then pass data to the network.
  id: totrans-1661
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from flax.training import train_state`'
  id: totrans-1662
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def create_train_state(rng):`'
  id: totrans-1663
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '"""Creates initial `TrainState`."""'
  id: totrans-1664
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`model = LSTMModel()`'
  id: totrans-1665
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`params = model.init(rng, jnp.array(X_train_padded[0]))[''param`'
  id: totrans-1666
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`s'']`'
  id: totrans-1667
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`tx = optax.adam(0.001,0.9,0.999,1e-07)`'
  id: totrans-1668
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`return train_state.TrainState.create(`'
  id: totrans-1669
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`apply_fn=model.apply, params=params, tx=tx)`'
  id: totrans-1670
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: After that, we define a training step that will compute the loss and gradients.
    It then uses these gradients to update the model parameters and returns the model
    metrics and the new state.
  id: totrans-1671
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`@jax.jitdef train_step(state, text, labels):def loss_fn(params):`'
  id: totrans-1672
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`logits = LSTMModel().apply({''params'': params}, text) loss = jnp.mean(optax.softmax_cross_entropy(
    logits=logits,`'
  id: totrans-1673
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`labels=jax.nn.one_hot(labels, num_classes=2))) return loss, logits`'
  id: totrans-1674
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`grad_fn = jax.value_and_grad(loss_fn, has_aux=True) (_, logits), grads = grad_fn(state.params)`'
  id: totrans-1675
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`state = state.apply_gradients(grads=grads)`'
  id: totrans-1676
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`metrics = compute_metrics(logits, labels)`'
  id: totrans-1677
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`return state, metrics`'
  id: totrans-1678
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Use `Elegy` to train networks like in Keras. Elegy is a high-level API for JAX
    neural network libraries.
  id: totrans-1679
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Distributed training in Flax and TensorFlow**'
  id: totrans-1680
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Training networks in TensorFlow in a distributed manner is done by creating `distributed
    strategy.mirrored_strategy = tf.distribute.MirroredStrategy()`
  id: totrans-1681
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`with mirrored_strategy.scope():`'
  id: totrans-1682
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_s`'
  id: totrans-1683
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`hape=(1,))])`'
  id: totrans-1684
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`model.compile(loss=''mse'', optimizer=''sgd'')`'
  id: totrans-1685
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: To train networks in a distributed way in Flax, we define distributed versions
    of our Flax functions. This is done using the [`pmap`] function that executes
    a function on multiple devices. You'll then compute predictions from all devices
    and get the average
  id: totrans-1686
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: using [`jax.lax.pmean()`]. You also need to replicate the data on all the devices
    using [`jax_utils.replicate`] . To obtain metrics from the
  id: totrans-1687
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: device use `jax_utils.unreplicate`.
  id: totrans-1688
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Working with TPU accelerators**'
  id: totrans-1689
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: You can use Flax and TensorFlow with TPU and GPU accelerators. To use Flax with
    TPUs on Colab, you'll need to set it up:`jax.tools.colab_tpu.setup_tpu() jax.devices()![](../images/00048.gif)`For
    TensorFlow, set up the `TPU distributed strategy.cluster_resolver = tf.distribute.cluster_resolver.TPUClusterRes
    olver(tpu=tpu_address)`
  id: totrans-1690
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`tf.config.experimental_connect_to_cluster(cluster_resolver) tf.tpu.experimental.initialize_tpu_system(cluster_resolver)
    tpu_strategy = tf.distribute.TPUStrategy(cluster_resolver)`'
  id: totrans-1691
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Model evaluation**'
  id: totrans-1692
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: TensorFlow provides the [`evaluate`] function for evaluating networks. Flax
    doesn't ship with such a function. You'll need to create a function that applies
    the model and returns the test metrics. `Elegy` provides Keras-like functions
    such as the `evaluate` method.
  id: totrans-1693
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`@jax.jit`'
  id: totrans-1694
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def eval_step(state, text, labels):`'
  id: totrans-1695
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`logits = LSTMModel().apply({''params'': state.params}, text)`'
  id: totrans-1696
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`return compute_metrics(logits=logits, labels=labels)`'
  id: totrans-1697
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def evaluate_model(state, text, test_lbls): """Evaluate on the validation
    set."""`'
  id: totrans-1698
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`metrics = eval_step(state, text, test_lbls) metrics = jax.device_get(metrics)`'
  id: totrans-1699
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`metrics = jax.tree_map(lambda x: x.item(), metrics) return metrics`'
  id: totrans-1700
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Visualize model performance**'
  id: totrans-1701
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Model visualizing is similar in Flax and TensorFlow. Once you obtain the metrics,
    you can use a package such as `Matplotlib` to visualize the model's performance.
    You can also use `TensorBoard` in both Flax and TensorFlow.
  id: totrans-1702
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Final thoughts**'
  id: totrans-1703
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: You have seen the differences between the Flax and TensorFlow libraries. In
    particular, have seen the difference in model definition and training.
  id: totrans-1704
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Train ResNet in Flax from scratch(Distributed ResNet training)
  id: totrans-1705
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Apart from designing custom CNN architectures, you can use architectures that
    have already been built. ResNet is one such popular architecture. In most cases,
    you'll achieve better performance by using such architectures. In this article,
    you will learn how to perform distributed training of a ResNet model in Flax.
  id: totrans-1706
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Install Flax models
  id: totrans-1707
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: 'The [flaxmodels] package provides pre-trained models for Jax and Flax, including:'
  id: totrans-1708
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: StyleGAN2
  id: totrans-1709
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`GPT2`'
  id: totrans-1710
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`VGG`'
  id: totrans-1711
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`ResNet`'
  id: totrans-1712
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`git clone https://github.com/matthias-wright/flaxmodels.git pip install -r
    flaxmodels/training/resnet/requirements.txt`'
  id: totrans-1713
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: In this project, we will train the model from scratch– meaning that we will
    not use the pre-trained weights. In a separate article, we have covered how to
    perform transfer learning with ResNet.
  id: totrans-1714
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Perform standard imports
  id: totrans-1715
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: With [flaxmodels] installed, let's import the standard libraries used in this
    article.
  id: totrans-1716
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: import`wget`# pip install wget
  id: totrans-1717
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: import`zipfile`
  id: totrans-1718
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import torch`'
  id: totrans-1719
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: from`torch.utils.data`import`DataLoader import os`
  id: totrans-1720
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: from`PIL`import`Image`
  id: totrans-1721
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: from`torchvision`import`transforms from torch.utils.data import Dataset import
    numpy as np`
  id: totrans-1722
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import pandas as pd`'
  id: totrans-1723
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import matplotlib.pyplot as plt`'
  id: totrans-1724
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '%matplotlib inline'
  id: totrans-1725
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: ignore harmless warnings
  id: totrans-1726
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`import warnings`'
  id: totrans-1727
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`warnings.filterwarnings("ignore") import jax`'
  id: totrans-1728
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: from`jax`import`numpy as jnp`
  id: totrans-1729
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import flax`'
  id: totrans-1730
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: from`flax`import`linen as nn`
  id: totrans-1731
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: from`flax.training`import`train_state import optax`
  id: totrans-1732
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: import`time`
  id: totrans-1733
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from tqdm.notebook import tqdm`'
  id: totrans-1734
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import math`'
  id: totrans-1735
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: from`flax`import`jax_utils`
  id: totrans-1736
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Download dataset
  id: totrans-1737
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: We will train the ResNet model to predict two classes from the cats and dogs dataset.
    Download and extract the cat and dog images.
  id: totrans-1738
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`wget.download("https://ml.machinelearningnuggets.com/train.zi p")`'
  id: totrans-1739
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: with`zipfile.ZipFile('train.zip', 'r') as zip_ref:`
  id: totrans-1740
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`zip_ref.extractall(''.'')`'
  id: totrans-1741
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Loading dataset in Flax
  id: totrans-1742
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Since JAX and Flax don't ship with any data loaders, we use data loading utilities
    from PyTorch or TensorFlow. When using PyTorch, we start by creating a dataset
    class.
  id: totrans-1743
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`class CatsDogsDataset(Dataset):`'
  id: totrans-1744
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def __init__(self, root_dir, annotation_file, transform=Non`'
  id: totrans-1745
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'e):'
  id: totrans-1746
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`self.root_dir = root_dir`'
  id: totrans-1747
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`self.annotations = pd.read_csv(annotation_file) self.transform = transform`'
  id: totrans-1748
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def __len__(self):return len(self.annotations)`'
  id: totrans-1749
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def __getitem__(self, index):`'
  id: totrans-1750
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`img_id = self.annotations.iloc[index, 0]`'
  id: totrans-1751
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`img = Image.open(os.path.join(self.root_dir, img_id)).c`'
  id: totrans-1752
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`convert("RGB")`'
  id: totrans-1753
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`y_label = torch.tensor(float(self.annotations.iloc[inde`'
  id: totrans-1754
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: x, 1]))
  id: totrans-1755
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'if self.transform is not None: img = self.transform(img)return (img, y_label)Next,
    create a Pandas DataFrame containing the image paths and labels.'
  id: totrans-1756
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`train_df = pd.DataFrame(columns=["img_path","label"]) train_df["img_path"]
    = os.listdir("train/") for idx, i in enumerate(os.listdir("train/")):`'
  id: totrans-1757
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`if "cat" in i:`'
  id: totrans-1758
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`train_df["label"][idx] = 0`'
  id: totrans-1759
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`if "dog" in i:`'
  id: totrans-1760
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`train_df["label"][idx] = 1`'
  id: totrans-1761
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`train_df.to_csv (r''train_csv.csv'', index = False, header=True)`'
  id: totrans-1762
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Data transformation in Flax
  id: totrans-1763
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Define a function that will stack the data and return it as a NumPy array.
  id: totrans-1764
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def custom_collate_fn(batch):`'
  id: totrans-1765
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`transposed_data = list(zip(*batch)) labels = np.array(transposed_data[1])
    imgs = np.stack(transposed_data[0]) return imgs, labels`'
  id: totrans-1766
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Create a transformation for resizing the images. Next, apply the transformation
    to the dataset created earlier.`size_image = 224`
  id: totrans-1767
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`transform = transforms.Compose([`'
  id: totrans-1768
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`transforms.Resize((size_image,size_image)), np.array])`'
  id: totrans-1769
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`dataset = CatsDogsDataset("train","train_csv.csv",transform=transform)`'
  id: totrans-1770
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Split this dataset into a training and testing set and create data loaders for
    each set.`batch_size = 32`
  id: totrans-1771
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`train_set, validation_set = torch.utils.data.random_split(dataset,[20000,5000])`'
  id: totrans-1772
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`train_loader = DataLoader(dataset=train_set, collate_fn=custom_collate_fn,shuffle=True,
    batch_size=batch_size)`'
  id: totrans-1773
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`validation_loader = DataLoader(dataset=validation_set,collate_fn=custom_collate_fn,
    shuffle=False, batch_size=batch_size)`'
  id: totrans-1774
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Instantiate Flax ResNet model
  id: totrans-1775
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: 'With the data in place, instantiate the Flax ResNet model using the `[flaxmodels]` package.
    The instantiation requires:'
  id: totrans-1776
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The desired number of classes.
  id: totrans-1777
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The type of output.
  id: totrans-1778
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The data type.
  id: totrans-1779
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Whether the model is pre-trained– in this case `[False]`.import jax.numpy as
    jnp import flaxmodels as fm
  id: totrans-1780
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`num_classes = 2`'
  id: totrans-1781
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`dtype = jnp.float32 model = fm.ResNet50(output=''log_softmax'', pretrained=None,
    num_classes=num_classes, dtype=dtype)`'
  id: totrans-1782
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Compute metrics
  id: totrans-1783
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Define the metrics for evaluating the model during training. Let's start by
    creating the loss function.
  id: totrans-1784
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def cross_entropy_loss(*, logits, labels):`'
  id: totrans-1785
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`labels_onehot = jax.nn.one_hot(labels, num_classes=num_classes)`'
  id: totrans-1786
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`s)`'
  id: totrans-1787
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`return optax.softmax_cross_entropy(logits=logits, labels=labels)`'
  id: totrans-1788
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`ls_onehot).mean()`'
  id: totrans-1789
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Next, define a function that computes and returns the loss and accuracy.
  id: totrans-1790
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def compute_metrics(*, logits, labels):`'
  id: totrans-1791
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`loss = cross_entropy_loss(logits=logits, labels=labels) accuracy = jnp.mean(jnp.argmax(logits,
    -1) == labels) metrics = {`'
  id: totrans-1792
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`''loss'': loss,`'
  id: totrans-1793
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`''accuracy'': accuracy`,'
  id: totrans-1794
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`}`'
  id: totrans-1795
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`return metrics`'
  id: totrans-1796
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Create Flax model training state
  id: totrans-1797
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Flax provides a training state for storing training information. The training
    state can be modified to add new information. In this case, we need to alter the
    training state to add the batch statistics since the ResNet model computes `[batch_stats]`.
  id: totrans-1798
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`class TrainState(train_state.TrainState): batch_stats: flax.core.FrozenDict`'
  id: totrans-1799
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: We need the model parameters and batch statistics to create the training state
    function. We can access these by initializing the model with the `[train]` as `[False]`.
  id: totrans-1800
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`key = jax.random.PRNGKey(0)`'
  id: totrans-1801
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`variables = model.init(key, jnp.ones([1, size_image, size_image, 3]), train=False)`'
  id: totrans-1802
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'The `create` method of TrainState expects the following parameters: The `[apply_fn]` –
    model apply function. The model parameters– `[variables[''params'']]`.'
  id: totrans-1803
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The optimizer, usually defined using Optax.
  id: totrans-1804
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The batch statistics– `variables['batch_stats']`.
  id: totrans-1805
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: We apply `[pmap]` to this function to create a distributed version of the training
    state.  `[pmap]` compiles the function for execution on multiple devices such
    as multiple GPUs and TPUs.
  id: totrans-1806
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import functools`'
  id: totrans-1807
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`@functools.partial(jax.pmap)`'
  id: totrans-1808
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def create_train_state(rng):`'
  id: totrans-1809
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '"""Creates initial `TrainState`."""'
  id: totrans-1810
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`return TrainState.create(apply_fn = model.apply,params = variables[''params''],tx
    = optax.adam(0.01,0.9),batch_stats = variables[''batch_stats''])`'
  id: totrans-1811
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Apply model function**'
  id: totrans-1812
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: 'Next, define a parallel model training function. Pass an `[axis_name]` so you
    can use that to aggregate the metrics from all the devices. The function:'
  id: totrans-1813
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Computes the loss.
  id: totrans-1814
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Computes predictions from all devices by calculating the average of the probabilities
    using `[jax.lax.pmean()]` .
  id: totrans-1815
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: When applying the model, we also include the batch statistics and the random
    number for  `[DropOut]`. Since this is the training function, the `[train]` parameter
    is `[True]`. The `[batch_stats]` are also included when computing the gradients.
    The `[update_model]` function applies the computed gradients– updates the model
    parameters.
  id: totrans-1816
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`@functools.partial(jax.pmap, axis_name=''ensemble'')` `def apply_model(state,
    images, labels):` `def loss_fn(params,batch_stats):`'
  id: totrans-1817
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`logits, batch_stats = model.apply({''params'': params, ''batch_stats'': batch_stats},
    images, train=True, rngs={''dropout'': jax.random.PRNGKey(0)}, mutable=[''batch_stats''])'
  id: totrans-1818
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`one_hot = jax.nn.one_hot(labels, num_classes)` `loss = optax.softmax_cross_entropy(logits=logits,
    labels=one_hot).mean()` return loss, `(logits, batch_stats)` `(loss, (logits,
    batch_stats)), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params,state.batch_stats)`
    `probs = jax.lax.pmean(jax.nn.softmax(logits), axis_name=''ensemble'')` `accuracy
    = jnp.mean(jnp.argmax(probs, -1) == labels)` return grads, `loss, accuracy@jax.pmap`
    `def update_model(state, grads):` return state.apply_gradients(grads=grads)'
  id: totrans-1819
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**TensorBoard in Flax**'
  id: totrans-1820
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: The next step is to train the ResNet model. However, you might be interested
    in tracking the training using `[TensorBoard]`. In that case, you have to configure
    TensorBoard. You can write the metrics to TensorBoard using the PyTorch `SummaryWriter`.
  id: totrans-1821
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`rm -rf ./flax_logs/`'
  id: totrans-1822
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from torch.utils.tensorboard import SummaryWriter` `import torchvision.transforms.functional
    as F` `logdir = "flax_logs"`'
  id: totrans-1823
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`writer = SummaryWriter(logdir)`'
  id: totrans-1824
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Train Flax ResNet model**'
  id: totrans-1825
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Let's train the ResNet model on the entire training set and evaluate it on a
    subset of the test set. You can also evaluate it on the whole test set. Replicate
    the test set to the available devices.
  id: totrans-1826
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`(test_images, test_labels) = next(iter(validation_loader))` `test_images =
    test_images / 255.0`'
  id: totrans-1827
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`test_images = np.array(jax_utils.replicate(test_images))` `test_labels = np.array(jax_utils.replicate(test_labels))`'
  id: totrans-1828
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Create some lists to hold the training and evaluation metrics.
  id: totrans-1829
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`epoch_loss = []`'
  id: totrans-1830
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`epoch_accuracy = []` `testing_accuracy = []` `testing_loss = []`'
  id: totrans-1831
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'Next, define the ResNet model training function. The function does the following:'
  id: totrans-1832
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Loops through the training dataset and scales it.
  id: totrans-1833
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Replicates the data on the available devices.
  id: totrans-1834
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Applies the model on the dataset and computes the metrics. Obtains the metrics
    from the devices
  id: totrans-1835
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`using jax_utils.unreplicate.`'
  id: totrans-1836
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Appends the metrics to a list.
  id: totrans-1837
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Computes the mean of the loss and accuracy to obtain the metrics for each epoch.
  id: totrans-1838
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Applies the model to the test set and obtains the metrics.
  id: totrans-1839
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Append the test metrics to a list.
  id: totrans-1840
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Writes the training and evaluation metrics to TensorBaord.
  id: totrans-1841
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'Prints the training and evaluation metrics.def `train_one_epoch(state, dataloader,num_epochs):
    """Train for 1 epoch on the training set."""for epoch in range(num_epochs):for
    cnt, (images, labels) in tqdm(enumerate(dataloade r), total=(math.ceil(len(train_set)/batch_size))):`'
  id: totrans-1842
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`images = images / 255.0`'
  id: totrans-1843
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`images = jax_utils.replicate(images)`'
  id: totrans-1844
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`labels = jax_utils.replicate(labels)`'
  id: totrans-1845
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`grads, loss, accuracy = apply_model(state, images,`'
  id: totrans-1846
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`labels)state = update_model(state, grads)`'
  id: totrans-1847
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`epoch_loss.append(jax_utils.unreplicate(loss)) epoch_accuracy.append(jax_utils.unreplicate(accuracy))
    train_loss = np.mean(epoch_loss)`'
  id: totrans-1848
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`train_accuracy = np.mean(epoch_accuracy)`'
  id: totrans-1849
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`_`, `test_loss`, `test_accuracy` = `jax_utils.unreplicate(app ly_model(state,
    test_images, test_labels))`'
  id: totrans-1850
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`testing_accuracy.append(test_accuracy)`'
  id: totrans-1851
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`testing_loss.append(test_loss)`'
  id: totrans-1852
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`writer.add_scalar(''Loss/train'', np.array(train_loss), e poch)`'
  id: totrans-1853
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`writer.add_scalar(''Loss/test'', np.array(test_loss), epo ch)`'
  id: totrans-1854
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`writer.add_scalar(''Accuracy/train'', np.array(train_accu racy), epoch)`'
  id: totrans-1855
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`writer.add_scalar(''Accuracy/test'', np.array(test_accura cy), epoch)`'
  id: totrans-1856
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`print(f"Epoch: {epoch + 1}, train loss: {train_loss:.4 f}, train accuracy:
    {train_accuracy * 100:.4f}, test loss: {tes t_loss:.4f}, test accuracy: {test_accuracy*
    100:.4f}", flush=Tr ue)`'
  id: totrans-1857
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: return `state, epoch_loss, epoch_accuracy, testing_accuracy, testing_lossCreate
    a training state by generating random numbers equivalent to the number of devices.`
  id: totrans-1858
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`seed = 0`'
  id: totrans-1859
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`rng = jax.random.PRNGKey(seed)`'
  id: totrans-1860
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`rng, init_rng = jax.random.split(rng)`'
  id: totrans-1861
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`state = create_train_state(jax.random.split(init_rng, jax.devic e_count()))`'
  id: totrans-1862
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`del init_rng # Must not be used anymore.`'
  id: totrans-1863
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Train the ResNet model by passing the training data and the number of epochs.
  id: totrans-1864
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`start = time.time()`'
  id: totrans-1865
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`num_epochs = 30`'
  id: totrans-1866
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`state, epoch_loss, epoch_accuracy, testing_accuracy, testing_lo ss = train_one_epoch(state,
    train_loader,num_epochs) print("Total time: ", time.time() - start, "seconds")`'
  id: totrans-1867
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`![](../images/00049.jpeg)`'
  id: totrans-1868
  prefs: []
  stylish: true
  type: TYPE_IMG
- en: '**Evaluate model with TensorBoard**'
  id: totrans-1869
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Run TensorBoard to see the logged scalars on TensorBoard.%load_ext tensorboard%tensorboard
    --logdir={logdir}![](../images/00050.jpeg)![](../images/00051.jpeg)
  id: totrans-1870
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Visualize Flax model performance**'
  id: totrans-1871
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: The metrics that were stored in a list can be plotted using Matplotlib.
  id: totrans-1872
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`plt.plot(epoch_accuracy, label="Training")`'
  id: totrans-1873
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`plt.plot(testing_accuracy, label="Test")`'
  id: totrans-1874
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`plt.xlabel("Epoch")`'
  id: totrans-1875
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`plt.ylabel("Accuracy")`'
  id: totrans-1876
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`plt.legend()`'
  id: totrans-1877
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`plt.show()`'
  id: totrans-1878
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`plt.plot(epoch_loss, label="Training")`'
  id: totrans-1879
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`plt.plot(testing_loss, label="Test")`'
  id: totrans-1880
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`plt.xlabel("Epoch")`'
  id: totrans-1881
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`plt.ylabel("Accuracy")`'
  id: totrans-1882
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`plt.legend()`'
  id: totrans-1883
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`plt.show()`'
  id: totrans-1884
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`![](../images/00052.jpeg)![](../images/00053.jpeg)`'
  id: totrans-1885
  prefs: []
  stylish: true
  type: TYPE_IMG
- en: Save Flax ResNet model
  id: totrans-1886
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '`To save the trained Flax ResNet model use`'
  id: totrans-1887
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'the save_checkpoint function. The function expects:'
  id: totrans-1888
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The folder where the ResNet model will be saved.
  id: totrans-1889
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The model to be saved– [target]. The step – training step number.
  id: totrans-1890
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The model prefix. Whether to overwrite existing models.
  id: totrans-1891
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`!pip install tensorstore`'
  id: totrans-1892
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from flax.training import checkpoints`'
  id: totrans-1893
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`ckpt_dir = ''model_checkpoint/''`'
  id: totrans-1894
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`checkpoints.save_checkpoint(ckpt_dir=ckpt_dir,`'
  id: totrans-1895
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`target=state, step=100,`'
  id: totrans-1896
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`prefix=''flax_model'', overwrite=True`'
  id: totrans-1897
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`)![](../images/00054.jpeg)`'
  id: totrans-1898
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Load Flax RestNet model
  id: totrans-1899
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: 'The saved ResNet Flax model can also be loaded to make predictions. Flax models
    are loaded using the [restore_checkpoint] function. The function expects:'
  id: totrans-1900
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The target state.
  id: totrans-1901
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The folder containing the saved model.
  id: totrans-1902
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The model's prefix.
  id: totrans-1903
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`loaded_model = checkpoints.restore_checkpoint(`'
  id: totrans-1904
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`ckpt_dir=ckpt_dir, target=state, prefix=''flax_mode`'
  id: totrans-1905
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`l'') ![](../images/00055.gif)`'
  id: totrans-1906
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Final thoughts
  id: totrans-1907
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: 'In this article, you have learned how to train a ResNet model from scratch
    in Flax. In particular, you have covered:'
  id: totrans-1908
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Creating a ResNet model in Flax.
  id: totrans-1909
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Defining the training state for the ResNet Flax model.
  id: totrans-1910
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Training the Flax ResNet model in a distributed manner. Track the performance
    of the Flax ResNet model with TensorBoard.
  id: totrans-1911
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Saving and loading the Flax ResNet model.
  id: totrans-1912
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Transfer learning with JAX & Flax
  id: totrans-1913
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Training large neural networks can take days or weeks. Once these networks are
    trained, you can take advantage of their weights and apply them to new tasks– transfer
    learning. As a result, you finetune a new network and get good results in a short
    period. Let's look at how you can fine-tune a pre-trained ResNet network in JAX
    and Flax.
  id: totrans-1914
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Install JAX ResNet
  id: totrans-1915
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '`We''ll use ResNet checkpoints provided by the jax-resnet library.pip install
    jax-resnetLet''s import it together with other packages used in this article.`'
  id: totrans-1916
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`pip install flax`'
  id: totrans-1917
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`import numpy as np`'
  id: totrans-1918
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import pandas as pd`'
  id: totrans-1919
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from PIL import Image`'
  id: totrans-1920
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import jax`'
  id: totrans-1921
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import optax`'
  id: totrans-1922
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import flax`'
  id: totrans-1923
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import jax.numpy as jnp`'
  id: totrans-1924
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from jax_resnet import pretrained_resnet, slice_variables, Sequ ential`'
  id: totrans-1925
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from flax.training import train_state from flax import linen as nn`'
  id: totrans-1926
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from flax.core import FrozenDict,frozen_dict from functools import partial`'
  id: totrans-1927
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import os`'
  id: totrans-1928
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import torch`'
  id: totrans-1929
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from torch.utils.data import DataLoader from torchvision import transforms`'
  id: totrans-1930
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from torch.utils.data import Dataset import matplotlib.pyplot as plt`'
  id: totrans-1931
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`%matplotlib inline`'
  id: totrans-1932
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: ignore harmless warnings
  id: totrans-1933
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`import warnings`'
  id: totrans-1934
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`warnings.filterwarnings("ignore")`'
  id: totrans-1935
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Download dataset
  id: totrans-1936
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: We will fine-tune the ResNet model to predict two classes from the cats and
    dogs dataset. Download and extract the cat and dog images.
  id: totrans-1937
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`pip install wget`'
  id: totrans-1938
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import wget`'
  id: totrans-1939
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`wget.download("https://ml.machinelearningnuggets.com/train.zi p")`'
  id: totrans-1940
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import zipfile`'
  id: totrans-1941
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`with zipfile.ZipFile(''train.zip'', ''r'') as zip_ref:`'
  id: totrans-1942
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`zip_ref.extractall(''.'')`'
  id: totrans-1943
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Data loading in JAX
  id: totrans-1944
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: JAX doesn't ship with data loading utilities. We use existing data loaders in TensorFlow and
    PyTorch to load the data. Let's use PyTorch to load the image data.
  id: totrans-1945
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The first step is to create a PyTorch [Dataset] class.
  id: totrans-1946
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: class CatsDogsDataset(Dataset):`
  id: totrans-1947
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: def __init__(self, root_dir, annotation_file, transform=Non
  id: totrans-1948
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'e):'
  id: totrans-1949
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: self.root_dir = root_dir
  id: totrans-1950
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: self.annotations = pd.read_csv(annotation_file) self.transform = transform
  id: totrans-1951
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def __len__(self):return len(self.annotations)`'
  id: totrans-1952
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def __getitem__(self, index):`'
  id: totrans-1953
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: img_id = self.annotations.iloc[index, 0]
  id: totrans-1954
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: img = Image.open(os.path.join(self.root_dir, img_id)).c
  id: totrans-1955
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`onvert("RGB")`'
  id: totrans-1956
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`y_label = torch.tensor(float(self.annotations.iloc[inde`'
  id: totrans-1957
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`x, 1])`'
  id: totrans-1958
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'if self.transform is not None: img = self.transform(img) return (img, y_label)'
  id: totrans-1959
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Data processing**'
  id: totrans-1960
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Next, create a Pandas DataFrame with the image paths and labels.
  id: totrans-1961
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`train_df = pd.DataFrame(columns=["img_path","label"]) train_df["img_path"]
    = os.listdir("train/") for idx, i in enumerate(os.listdir("train/")):`'
  id: totrans-1962
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: if "cat" in i:`train_df["label"][idx] = 0if "dog" in i:train_df["label"][idx]
    = 1train_df.to_csv (r'train_csv.csv', index = False, header=True)![](../images/00056.jpeg)Define
    a function to stack the data and return the images and labels as a NumPy array.
  id: totrans-1963
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def custom_collate_fn(batch):`'
  id: totrans-1964
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`transposed_data = list(zip(*batch))`'
  id: totrans-1965
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`labels = np.array(transposed_data[1])`'
  id: totrans-1966
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: imgs = np.stack(transposed_data[0])
  id: totrans-1967
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: return imgs, labels
  id: totrans-1968
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Let's also resize the images to ensure they are the same size. Define the size
    in a configuration dictionary. We'll use the other config variables later.
  id: totrans-1969
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: config = {
  id: totrans-1970
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '''NUM_LABELS'': 2,'
  id: totrans-1971
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '''BATCH_SIZE'': 32,'
  id: totrans-1972
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '''N_EPOCHS'': 5,'
  id: totrans-1973
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`''LR'': 0.001,`'
  id: totrans-1974
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`''IMAGE_SIZE'': 224,`'
  id: totrans-1975
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`''WEIGHT_DECAY'': 1e-5, ''FREEZE_BACKBONE'': True,`'
  id: totrans-1976
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '}'
  id: totrans-1977
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Resize the images using PyTorch transforms. Next, use the CatsDogsDataset class
    to define the training and testing data loaders.
  id: totrans-1978
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`transform = transforms.Compose([`'
  id: totrans-1979
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: transforms.Resize((config["IMAGE_SIZE"],config["IMAGE_SIZ
  id: totrans-1980
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`E"]))`,'
  id: totrans-1981
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: np.array])
  id: totrans-1982
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: dataset = CatsDogsDataset("train","train_csv.csv",transform=tra
  id: totrans-1983
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: nsform)
  id: totrans-1984
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`train_set`, `validation_set = torch.utils.data.random_split(datas`'
  id: totrans-1985
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`et,[20000,5000])`'
  id: totrans-1986
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`train_loader = DataLoader(dataset=train_set, collate_fn=custom_`'
  id: totrans-1987
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: collate_fn,shuffle=True, batch_size=config["BATCH_SIZE"])
  id: totrans-1988
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`validation_loader = DataLoader(dataset=validation_set,collate_f`'
  id: totrans-1989
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: n=custom_collate_fn, shuffle=False, batch_size=config["BATCH_SI
  id: totrans-1990
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`ZE"])`'
  id: totrans-1991
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**ResNet model definition**'
  id: totrans-1992
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Pre-trained ResNet models are trained on many classes. However, the dataset
    we have has two classes. We, therefore, use the ResNet as the backbone and define
    a custom classification layer.
  id: totrans-1993
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Create head network**'
  id: totrans-1994
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Create a head network with output as per the problem, in this case, a binary
    image classification.
  id: totrans-1995
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '"""'
  id: totrans-1996
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: reference - https://www.kaggle.com/code/alexlwh/happywhale-flax
  id: totrans-1997
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`-jax-tpu-gpu-resnet-baseline`'
  id: totrans-1998
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '"""'
  id: totrans-1999
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`class Head(nn.Module):`'
  id: totrans-2000
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '''''''head model''''''`batch_norm_cls: partial = partial(nn.BatchNorm, momentum=0.`'
  id: totrans-2001
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 9)
  id: totrans-2002
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '@nn.compact'
  id: totrans-2003
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def __call__(self, inputs, train: bool):`'
  id: totrans-2004
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: output_n = inputs.shape[-1]x = self.batch_norm_cls(use_running_average=not train)
  id: totrans-2005
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`(inputs)'
  id: totrans-2006
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`x = nn.Dropout(rate=0.25)(x, deterministic=not train) x = nn.Dense(features=output_n)(x)'
  id: totrans-2007
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`x = nn.relu(x)'
  id: totrans-2008
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`x = self.batch_norm_cls(use_running_average=not train)'
  id: totrans-2009
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`(x)'
  id: totrans-2010
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`x = nn.Dropout(rate=0.5)(x, deterministic=not train) x = nn.Dense(features=config["NUM_LABELS"])(x)
    return x'
  id: totrans-2011
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Combine ResNet backbone with head**'
  id: totrans-2012
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Combine the pre-trained ResNet backbone with the custom head you created above.
  id: totrans-2013
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`class Model(nn.Module):'
  id: totrans-2014
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '```Combines backbone and head model``` backbone: Sequential'
  id: totrans-2015
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'head: Head'
  id: totrans-2016
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'def __call__(self, inputs, train: bool): x = self.backbone(inputs)'
  id: totrans-2017
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: average pool layer
  id: totrans-2018
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: x = jnp.mean(x, axis=(1, 2)) x = self.head(x, train)
  id: totrans-2019
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: return x
  id: totrans-2020
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Load pre-trained ResNet 50**'
  id: totrans-2021
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Next, create a function that loads the pre-trained ResNet model. Omit the last
    two layers of the network because we have defined a custom head. The function
    returns the ResNet model and its parameters. The model parameters are obtained
    using the [slice_variables] function.
  id: totrans-2022
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '```def get_backbone_and_params(model_arch: str):```'
  id: totrans-2023
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '```'
  id: totrans-2024
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Get backbone and params
  id: totrans-2025
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 1\. Loads pretrained model (resnet50)
  id: totrans-2026
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '2\. Get model and param structure except last 2 layers 3\. Extract the corresponding
    subset of the variables dict INPUT : model_arch'
  id: totrans-2027
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: RETURNS backbone , backbone_params
  id: totrans-2028
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '```'
  id: totrans-2029
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`if model_arch == ''resnet50'':'
  id: totrans-2030
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: resnet_tmpl, params = pretrained_resnet(50) model = resnet_tmpl()else:raise
    NotImplementedError
  id: totrans-2031
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: get model & param structure for backbone start, end = 0, len(model.layers) -
    2 backbone = Sequential(model.layers[start:end]) backbone_params = slice_variables(params,
    start, end) return backbone, backbone_params
  id: totrans-2032
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: Get model and variables
  id: totrans-2033
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: 'Use the above function to create the final model. Define a function that:'
  id: totrans-2034
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Initializes the network's input.
  id: totrans-2035
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Obtains the ResNet backbone and its parameters. Passes the input to the backbone
    and gets the output.
  id: totrans-2036
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Initializes the network's head. Creates the final model using backbone and head.
  id: totrans-2037
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Combines the parameters from backbone and head.
  id: totrans-2038
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '```def get_model_and_variables(model_arch: str, head_init_key: in t):```'
  id: totrans-2039
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '```'
  id: totrans-2040
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Get model and variables
  id: totrans-2041
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 1\. Initialise inputs(shape=(1,image_size,image_size,3))
  id: totrans-2042
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 2\. Get backbone and params
  id: totrans-2043
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 3\. Apply backbone model and get outputs
  id: totrans-2044
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 4\. Initialise head
  id: totrans-2045
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 5\. Create final model using backbone and head
  id: totrans-2046
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 6\. Combine params from backbone and head
  id: totrans-2047
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: INPUT model_arch, head_init_key RETURNS model, variables '''
  id: totrans-2048
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '#backboneinputs = jnp.ones((1, config[''IMAGE_SIZE''],config[''IMAGE_SI ZE''],
    3), jnp.float32)backbone, backbone_params = get_backbone_and_params(model_a rch)
    key = jax.random.PRNGKey(head_init_key)backbone_output = backbone.apply(backbone_params,
    inputs, m utable=False)#headhead_inputs = jnp.ones((1, backbone_output.shape[-1]),
    jnp. float32)head = Head()head_params = head.init(key, head_inputs, train=False)'
  id: totrans-2049
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '#final model'
  id: totrans-2050
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: model = Model(backbone, head)
  id: totrans-2051
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: variables = FrozenDict({
  id: totrans-2052
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`''params'': {'
  id: totrans-2053
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`''backbone'': backbone_params[''params''], ''head'': head_params[''params'']'
  id: totrans-2054
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '},'
  id: totrans-2055
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`''batch_stats'': {'
  id: totrans-2056
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '''backbone'': backbone_params[''batch_stats''], ''head'': head_params[''batch_stats'']'
  id: totrans-2057
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2058
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '})'
  id: totrans-2059
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: return model, variables
  id: totrans-2060
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: All names relating to the backbone network are prefixed with the name [backbone].
    You can use any name, but all backbone variable names should be the same. This
    is important when freezing layers, as we'll see later.
  id: totrans-2061
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Next, use the function defined above to create the model.model, variables =
    get_model_and_variables('resnet50', 0) ![](../images/00057.jpeg)
  id: totrans-2062
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Zero gradients
  id: totrans-2063
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Since we are applying transfer learning, we need to ensure that the backbone
    is not updated. Otherwise, we'll be training the network from scratch. We want
    to take advantage of the pre-trained weights and use them as a feature extractor for
    the network. To achieve this, we freeze the parameters of all layers whose name
    starts
  id: totrans-2064
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: with [backbone]. As a result, these parameters will not be updated during training.
  id: totrans-2065
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '"""'
  id: totrans-2066
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: reference - https://github.com/deepmind/optax/issues/159#issuec omment-896459491
  id: totrans-2067
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '"""'
  id: totrans-2068
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'def zero_grads():'
  id: totrans-2069
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: ''''''''
  id: totrans-2070
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Zero out the previous gradient computation
  id: totrans-2071
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '```'
  id: totrans-2072
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'def init_fn(_):'
  id: totrans-2073
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: return ()
  id: totrans-2074
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'def update_fn(updates, state, params=None):'
  id: totrans-2075
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: return jax.tree_map(jnp.zeros_like, updates), () return optax.GradientTransformation(init_fn,
    update_fn)
  id: totrans-2076
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '"""'
  id: totrans-2077
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: reference - https://colab.research.google.com/drive/1g_pt2Rc3bv 6H6qchvGHD-BpgF-Pt4vrC#scrollTo=TqDvTL_tIQCH&line=2&uniqifier=1
    """
  id: totrans-2078
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'def create_mask(params, label_fn):'
  id: totrans-2079
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'def _map(params, mask, label_fn):for k in params:if label_fn(k):'
  id: totrans-2080
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: mask[k] = 'zero'
  id: totrans-2081
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'else:'
  id: totrans-2082
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'if isinstance(params[k], FrozenDict): mask[k] = {}'
  id: totrans-2083
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: _map(params[k], mask[k], label_fn)
  id: totrans-2084
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'else:'
  id: totrans-2085
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: mask[k] = 'adam'
  id: totrans-2086
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: mask = {}
  id: totrans-2087
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: _map(params, mask, label_fn)
  id: totrans-2088
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: return frozen_dict.freeze(mask)
  id: totrans-2089
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Define Flax optimizer
  id: totrans-2090
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Create an optimizer that will only be applied to the head and not backbone layers.
    This is done using the optax.multi_transform while passing the desired transformations.
  id: totrans-2091
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: adamw = optax.adamw(
  id: totrans-2092
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: learning_rate=config['LR'],
  id: totrans-2093
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: b1=0.9, b2=0.999,
  id: totrans-2094
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: eps=1e-6, weight_decay=1e-2
  id: totrans-2095
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: )
  id: totrans-2096
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: optimizer = optax.multi_transform(
  id: totrans-2097
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '{''adam'': adamw, ''zero'': zero_grads()},'
  id: totrans-2098
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'create_mask(variables[''params''], lambda s: s.startswith(''ba ckbone''))'
  id: totrans-2099
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: )
  id: totrans-2100
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Define Flax loss function
  id: totrans-2101
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Next, define the function to compute the loss function.
  id: totrans-2102
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'def cross_entropy_loss(*, logits, labels):'
  id: totrans-2103
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: labels_onehot = jax.nn.one_hot(labels, num_classes=config["NU
  id: totrans-2104
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: M_LABELS"])
  id: totrans-2105
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: return optax.softmax_cross_entropy(logits=logits, labels=labe
  id: totrans-2106
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: ls_onehot).mean()
  id: totrans-2107
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'When computing the loss during training, set [train] to [True]. You also have
    to:'
  id: totrans-2108
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Set the batch_statsDefine the random number for the [dropout] layers. Set the [batch_stats] as
    mutable.
  id: totrans-2109
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'def compute_loss(params, batch_stats, images, labels): logits,batch_stats =
    model.apply({''params'': params,''batch_s'
  id: totrans-2110
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'tats'': batch_stats},images, train=True,rngs={''dropout'': jax.ran'
  id: totrans-2111
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: dom.PRNGKey(0)}, mutable=['batch_stats'])
  id: totrans-2112
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: loss = cross_entropy_loss(logits=logits, labels=labels) return loss, (logits,
    batch_stats)
  id: totrans-2113
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Define Flax metrics
  id: totrans-2114
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '`Using the loss function, define a function that will return the loss and accuracy
    during training.`'
  id: totrans-2115
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def compute_metrics(*, logits, labels):`'
  id: totrans-2116
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`loss = cross_entropy_loss(logits=logits, labels=labels) accuracy = jnp.mean(jnp.argmax(logits,
    -1) == labels) metrics = {`'
  id: totrans-2117
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`''loss'': loss,`'
  id: totrans-2118
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`''accuracy'': accuracy,`'
  id: totrans-2119
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`}`'
  id: totrans-2120
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`return metrics`'
  id: totrans-2121
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Create Flax training state**'
  id: totrans-2122
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '`Flax provides a training state for storing training information. In this case,
    we add the [batch_stats] information.class TrainState(train_state.TrainState):
    batch_stats: FrozenDict`'
  id: totrans-2123
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`state = TrainState.create(`'
  id: totrans-2124
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`apply_fn = model.apply,`'
  id: totrans-2125
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`params = variables[''params''],`'
  id: totrans-2126
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`tx = optimizer,`'
  id: totrans-2127
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`batch_stats = variables[''batch_stats''],`'
  id: totrans-2128
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`)`'
  id: totrans-2129
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Training step**'
  id: totrans-2130
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '`The training step receives the images and labels and computes the gradient
    with respect to the model parameters. It then returns the new state and the model
    metrics.`'
  id: totrans-2131
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`@jax.jit`'
  id: totrans-2132
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def train_step(state: TrainState,images, labels):`'
  id: totrans-2133
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '"""Train for a single step."""'
  id: totrans-2134
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`(batch_loss, (logits, batch_stats)), grads= jax.value_and_gra d(compute_loss,
    has_aux=True)(state.params,state.batch_stats, i mages,labels)`'
  id: totrans-2135
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`state = state.apply_gradients(grads=grads)`'
  id: totrans-2136
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`metrics = compute_metrics(logits=logits, labels=labels)`'
  id: totrans-2137
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`return state, metrics`'
  id: totrans-2138
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`To train the network for one epoch, loop through the training data while applying
    the training step.`'
  id: totrans-2139
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def train_one_epoch(state, dataloader):`'
  id: totrans-2140
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '"""Train for 1 epoch on the training set.""" batch_metrics = []'
  id: totrans-2141
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`for cnt, (images, labels) in enumerate(dataloader):`'
  id: totrans-2142
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`images = images / 255.0`'
  id: totrans-2143
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`state, metrics = train_step(state, images, labels) batch_metrics.append(metrics)`'
  id: totrans-2144
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`batch_metrics_np = jax.device_get(batch_metrics) epoch_metrics_np = {k: np.mean([metrics[k]
    for metrics in batch_metrics_n`'
  id: totrans-2145
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`p])`'
  id: totrans-2146
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`for k in batch_metrics_np[0] }`'
  id: totrans-2147
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`return state, epoch_metrics_np`'
  id: totrans-2148
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Evaluation step**'
  id: totrans-2149
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '`The model evaluation steps accept the test labels and images and applies them
    to the network. It then returns the model evaluation metrics. During evaluation,
    set the [train] parameter to [False]. You''ll also define the [batch_stats] and
    the random number for the [dropout] layer.`'
  id: totrans-2150
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`@jax.jitdef eval_step(batch_stats, params, images, labels): logits = model.apply({''params'':
    params,''batch_stats'': batch _stats}, images, train=False,rngs={''dropout'':
    jax.random.PRNGKe y(0)})return compute_metrics(logits=logits, labels=labels)`'
  id: totrans-2151
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def evaluate_model(state, test_imgs, test_lbls):`'
  id: totrans-2152
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '"""Evaluate on the validation set."""'
  id: totrans-2153
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`metrics = eval_step(state.batch_stats,state.params, test_im`'
  id: totrans-2154
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`gs, test_lbls)`'
  id: totrans-2155
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`metrics = jax.device_get(metrics)`'
  id: totrans-2156
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`metrics = jax.tree_map(lambda x: x.item(), metrics) return metrics`'
  id: totrans-2157
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Train ResNet model in Flax**'
  id: totrans-2158
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '`Train the ResNet model by applying the [train_one_epoch] function for the
    desired number of epochs. This is a few epochs since we are finetuning the network.`'
  id: totrans-2159
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Set up TensorBoard in Flax**'
  id: totrans-2160
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '`To monitor model training via TensorBoard, you can write the training and
    validation metrics to TensorBoard.`'
  id: totrans-2161
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from torch.utils.tensorboard import SummaryWriter import torchvision.transforms.functional
    as F logdir = "flax_logs"`'
  id: totrans-2162
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`writer = SummaryWriter(logdir)`'
  id: totrans-2163
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Train model
  id: totrans-2164
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '`Define a function to train and evaluate the model while writing the metrics
    to TensorBoard.(test_images, test_labels) = next(iter(validation_loader)) test_images
    = test_images / 255.0`'
  id: totrans-2165
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`training_loss = [] training_accuracy = [] testing_loss = []`'
  id: totrans-2166
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`testing_accuracy = []`'
  id: totrans-2167
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def train_model(epochs):for epoch in range(1, epochs + 1):train_state, train_metrics
    = train_one_epoch(state, tra`'
  id: totrans-2168
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`in_loader)`'
  id: totrans-2169
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`training_loss.append(train_metrics[''loss'']) training_accuracy.append(train_metrics[''accuracy''])`'
  id: totrans-2170
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`test_metrics = evaluate_model(train_state, test_images, test_labels)`'
  id: totrans-2171
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`testing_loss.append(test_metrics[''loss''])`'
  id: totrans-2172
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`testing_accuracy.append(test_metrics[''accuracy''])`'
  id: totrans-2173
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`writer.add_scalar(''Loss/train'', train_metrics[''loss''], epoch)`'
  id: totrans-2174
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`writer.add_scalar(''Loss/test'', test_metrics[''loss''], epoch)`'
  id: totrans-2175
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`writer.add_scalar(''Accuracy/train'', train_metrics[''accu racy''], epoch)`'
  id: totrans-2176
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`writer.add_scalar(''Accuracy/test'', test_metrics[''accura cy''], epoch)`'
  id: totrans-2177
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`print(f"Epoch: {epoch}, training loss: {train_metrics [''loss'']}, training
    accuracy: {train_metrics[''accuracy''] * 10 0}, validation loss: {test_metrics[''loss'']},
    validation accurac y: {test_metrics[''accuracy''] * 100}")`'
  id: totrans-2178
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`return train_stateRun the training function. trained_model_state = train_model(config["N_EPOCHS"])`'
  id: totrans-2179
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Save Flax model
  id: totrans-2180
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Use the `[save_checkpoint]` to save a trained Flax model.
  id: totrans-2181
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from flax.training import checkpoints`'
  id: totrans-2182
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`ckpt_dir = ''model_checkpoint/''`'
  id: totrans-2183
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`checkpoints.save_checkpoint(ckpt_dir=ckpt_dir,`'
  id: totrans-2184
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`target=trained_model_state, step=100,`'
  id: totrans-2185
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`prefix=''resnet_model'', overwrite=True`'
  id: totrans-2186
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`)`'
  id: totrans-2187
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Load saved Flax model
  id: totrans-2188
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: A saved Flax model is loaded using the `[restore_checkpoint]` method.
  id: totrans-2189
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`loaded_model = checkpoints.restore_checkpoint('
  id: totrans-2190
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`ckpt_dir=ckpt_dir, target=state, prefix=''resnet_mod`'
  id: totrans-2191
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`el'') loaded_model`'
  id: totrans-2192
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Evaluate Flax ResNet model
  id: totrans-2193
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '`To evaluate a Flax model, pass the test and training data to`'
  id: totrans-2194
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`the evalaute_model function.evaluate_model(loaded_model,test_images, test_labels)
    ![](../images/00058.gif)`'
  id: totrans-2195
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Visualize model performance
  id: totrans-2196
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: You can check the network's performance via TensorBoard or plot the metrics
    using Matplotlib.
  id: totrans-2197
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Final thoughts
  id: totrans-2198
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: 'You can apply transfer learning to take advantage of pre-trained models and
    get results with minimal effort. You have learned how to train a ResNet model
    in Flax. Specially, you have covered:'
  id: totrans-2199
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`How to define the ResNet model in Flax.`'
  id: totrans-2200
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: How to freeze the layers of the ResNet network.
  id: totrans-2201
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`Training a ResNet model on custom data in Flax.`'
  id: totrans-2202
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`Saving and loading a ResNet model in Flax.`'
  id: totrans-2203
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Elegy(High-level API for deep learning in JAX & Flax)
  id: totrans-2204
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '`Training deep learning networks in Flax is done in a couple of steps. It involves
    creating the following functions:`'
  id: totrans-2205
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Model definition.
  id: totrans-2206
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`Compute metrics.`'
  id: totrans-2207
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`Training state.`'
  id: totrans-2208
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Training step.
  id: totrans-2209
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Training and evaluation function.
  id: totrans-2210
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`Flax and JAX give more control in defining and training deep learning networks.
    However, this comes with more verbosity. Enter Elegy. Elegy is a high-level API
    for creating deep learning networks in JAX. Elegy''s API is like the one in Keras.`'
  id: totrans-2211
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`Let''s look at how to use Elegy to define and train deep learning networks
    in Flax.`'
  id: totrans-2212
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`Data pre-processing`'
  id: totrans-2213
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '`To make this illustration concrete, we''ll use the movie review data from
    Kaggle to create an LSTM network in Flax.`'
  id: totrans-2214
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`The first step is to download and extract the data.`'
  id: totrans-2215
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import os`'
  id: totrans-2216
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import kaggle`'
  id: totrans-2217
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`Obtain from https://www.kaggle.com/username/account os.environ["KAGGLE_USERNAME"]="KAGGLE_USERNAME"`'
  id: totrans-2218
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`os.environ["KAGGLE_KEY"]="KAGGLE_KEY"`'
  id: totrans-2219
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`!kaggle datasets download lakshmi25npathi/imdb-dataset-of-50k-m ovie-reviews`'
  id: totrans-2220
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import zipfile`'
  id: totrans-2221
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`with zipfile.ZipFile(''imdb-dataset-of-50k-movie-reviews.zip'',`'
  id: totrans-2222
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`''r'') as zip_ref:zip_ref.extractall(''imdb-dataset-of-50k-movie-reviews'')Next,
    we define the following processing steps:`'
  id: totrans-2223
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`Split the data into a training and testing set. Remove stopwords from the
    data.`'
  id: totrans-2224
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`Clean the data by removing punctuations and other special characters.`'
  id: totrans-2225
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`Convert the data to a TensorFlow dataset.`'
  id: totrans-2226
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`Conver the data to numerical representation using the Keras vectorization
    layer.`'
  id: totrans-2227
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import numpy as np`'
  id: totrans-2228
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import pandas as pd`'
  id: totrans-2229
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from numpy import array`'
  id: totrans-2230
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import tensorflow_datasets as tfds`'
  id: totrans-2231
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import tensorflow as tf`'
  id: totrans-2232
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from sklearn.model_selection import train_test_split from sklearn.preprocessing
    import LabelEncoder import matplotlib.pyplot as plt`'
  id: totrans-2233
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from sklearn.model_selection import train_test_split import tensorflow as
    tf`'
  id: totrans-2234
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`df = pd.read_csv("imdb-dataset-of-50k-movie-reviews/IMDB Datase t.csv")`'
  id: totrans-2235
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import nltk`'
  id: totrans-2236
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`from nltk.corpus import stopwords`'
  id: totrans-2237
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`nltk.download(''stopwords'')`'
  id: totrans-2238
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def remove_stop_words(review):`'
  id: totrans-2239
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`review_minus_sw = []`'
  id: totrans-2240
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`stop_words = stopwords.words(''english'')`'
  id: totrans-2241
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`review = review.split()`'
  id: totrans-2242
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`cleaned_review = [review_minus_sw.append(word) for word in`'
  id: totrans-2243
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`review if word not in stop_words]`'
  id: totrans-2244
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`cleaned_review = '' ''.join(review_minus_sw)`'
  id: totrans-2245
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`return cleaned_review`'
  id: totrans-2246
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`df[''review''] = df[''review''].apply(remove_stop_words) labelencoder = LabelEncoder()`'
  id: totrans-2247
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`df = df.assign(sentiment = labelencoder.fit_transform(df["senti ment"]))`'
  id: totrans-2248
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`df = df.drop_duplicates()`'
  id: totrans-2249
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`docs = df[''review'']`'
  id: totrans-2250
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`labels = array(df[''sentiment''])`'
  id: totrans-2251
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`X_train, X_test , y_train, y_test = train_test_split(docs, labe ls , test_size
    = 0.20, random_state=0)`'
  id: totrans-2252
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`max_features = 10000 # Maximum vocab size.`'
  id: totrans-2253
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`batch_size = 128`'
  id: totrans-2254
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`max_len = 50 # Sequence length to pad the outputs to. vectorize_layer = tf.keras.layers.TextVectorization(standardize
    =''lower_and_strip_punctuation'',max_tokens=max_features,output_m ode=''int'',output_sequence_length=max_len)`'
  id: totrans-2255
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`vectorize_layer.adapt(X_train)`'
  id: totrans-2256
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`X_train_padded = vectorize_layer(X_train)`'
  id: totrans-2257
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`X_test_padded = vectorize_layer(X_test)`'
  id: totrans-2258
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`training_data = tf.data.Dataset.from_tensor_slices((X_train_pad ded, y_train))`'
  id: totrans-2259
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`validation_data = tf.data.Dataset.from_tensor_slices((X_test_pa dded, y_test))`'
  id: totrans-2260
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`training_data = training_data.batch(batch_size)`'
  id: totrans-2261
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`validation_data = validation_data.batch(batch_size) def get_train_batches():`'
  id: totrans-2262
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: ds = training_data.prefetch(1)
  id: totrans-2263
  prefs: []
  type: TYPE_NORMAL
- en: ds = ds.repeat(3)
  id: totrans-2264
  prefs: []
  type: TYPE_NORMAL
- en: 'ds = ds.shuffle(3, reshuffle_each_iteration=True) # tfds.dataset_as_numpy converts
    the tf.data.Dataset into an'
  id: totrans-2265
  prefs: []
  type: TYPE_NORMAL
- en: iterable of NumPy arraysreturn tfds.as_numpy(ds)
  id: totrans-2266
  prefs: []
  type: TYPE_NORMAL
- en: '**Model definition in Elegy**'
  id: totrans-2267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Start by installing Elegy, Flax, and JAX.pip install -U elegy flax jax jaxlibNext,
    define the LSTM model.
  id: totrans-2268
  prefs: []
  type: TYPE_NORMAL
- en: import jax
  id: totrans-2269
  prefs: []
  type: TYPE_NORMAL
- en: import jax.numpy as jnp import elegy as eg
  id: totrans-2270
  prefs: []
  type: TYPE_NORMAL
- en: from flax import linen as nn
  id: totrans-2271
  prefs: []
  type: TYPE_NORMAL
- en: 'class LSTMModel(nn.Module):'
  id: totrans-2272
  prefs: []
  type: TYPE_NORMAL
- en: 'def setup(self):'
  id: totrans-2273
  prefs: []
  type: TYPE_NORMAL
- en: 'self.embedding = nn.Embed(max_features, max_len) lstm_layer = nn.scan(nn.OptimizedLSTMCell,
    variable_broadcast="params", split_rngs={"params": False}, in_axes=1,'
  id: totrans-2274
  prefs: []
  type: TYPE_NORMAL
- en: out_axes=1,
  id: totrans-2275
  prefs: []
  type: TYPE_NORMAL
- en: length=max_len,
  id: totrans-2276
  prefs: []
  type: TYPE_NORMAL
- en: reverse=False)
  id: totrans-2277
  prefs: []
  type: TYPE_NORMAL
- en: self.lstm1 = lstm_layer()
  id: totrans-2278
  prefs: []
  type: TYPE_NORMAL
- en: self.dense1 = nn.Dense(256)
  id: totrans-2279
  prefs: []
  type: TYPE_NORMAL
- en: self.lstm2 = lstm_layer()
  id: totrans-2280
  prefs: []
  type: TYPE_NORMAL
- en: self.dense2 = nn.Dense(128)
  id: totrans-2281
  prefs: []
  type: TYPE_NORMAL
- en: self.lstm3 = lstm_layer()
  id: totrans-2282
  prefs: []
  type: TYPE_NORMAL
- en: self.dense3 = nn.Dense(64)
  id: totrans-2283
  prefs: []
  type: TYPE_NORMAL
- en: self.dense4 = nn.Dense(2)
  id: totrans-2284
  prefs: []
  type: TYPE_NORMAL
- en: '@nn.rematdef __call__(self, x_batch): x = self.embedding(x_batch)'
  id: totrans-2285
  prefs: []
  type: TYPE_NORMAL
- en: carry, hidden = nn.OptimizedLSTMCell.initialize_carry(j ax.random.PRNGKey(0),
    batch_dims=(len(x_batch),), size=128)
  id: totrans-2286
  prefs: []
  type: TYPE_NORMAL
- en: (carry, hidden), x = self.lstm1((carry, hidden), x)
  id: totrans-2287
  prefs: []
  type: TYPE_NORMAL
- en: x = self.dense1(x) x = nn.relu(x)
  id: totrans-2288
  prefs: []
  type: TYPE_NORMAL
- en: carry, hidden = nn.OptimizedLSTMCell.initialize_carry(j ax.random.PRNGKey(0),
    batch_dims=(len(x_batch),), size=64)
  id: totrans-2289
  prefs: []
  type: TYPE_NORMAL
- en: (carry, hidden), x = self.lstm2((carry, hidden), x)
  id: totrans-2290
  prefs: []
  type: TYPE_NORMAL
- en: x = self.dense2(x) x = nn.relu(x)
  id: totrans-2291
  prefs: []
  type: TYPE_NORMAL
- en: carry, hidden = nn.OptimizedLSTMCell.initialize_carry(j ax.random.PRNGKey(0),
    batch_dims=(len(x_batch),), size=32)
  id: totrans-2292
  prefs: []
  type: TYPE_NORMAL
- en: (carry, hidden), x = self.lstm3((carry, hidden), x)
  id: totrans-2293
  prefs: []
  type: TYPE_NORMAL
- en: x = self.dense3(x)
  id: totrans-2294
  prefs: []
  type: TYPE_NORMAL
- en: x = nn.relu(x)
  id: totrans-2295
  prefs: []
  type: TYPE_NORMAL
- en: x = self.dense4(x[:, -1]) return nn.log_softmax(x)
  id: totrans-2296
  prefs: []
  type: TYPE_NORMAL
- en: Let's now create an Elegy model using the above network. As you can see, the
    loss and metrics are defined like in Keras. The model compilation is done in the
    constructor, so you don't have to do this manually.
  id: totrans-2297
  prefs: []
  type: TYPE_NORMAL
- en: import optax
  id: totrans-2298
  prefs: []
  type: TYPE_NORMAL
- en: model = eg.Model(
  id: totrans-2299
  prefs: []
  type: TYPE_NORMAL
- en: module=LSTMModel(),
  id: totrans-2300
  prefs: []
  type: TYPE_NORMAL
- en: loss=[
  id: totrans-2301
  prefs: []
  type: TYPE_NORMAL
- en: eg.losses.Crossentropy(), eg.regularizers.L2(l=1e-4), ],
  id: totrans-2302
  prefs: []
  type: TYPE_NORMAL
- en: metrics=eg.metrics.Accuracy(),
  id: totrans-2303
  prefs: []
  type: TYPE_NORMAL
- en: optimizer=optax.adam(1e-3), )
  id: totrans-2304
  prefs: []
  type: TYPE_NORMAL
- en: '**Elegy model summary**'
  id: totrans-2305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Like in Keras, we can print the model's summary. model.summary(jnp.array(X_train_padded[:64]))
    ![](../images/00059.jpeg)
  id: totrans-2306
  prefs: []
  type: TYPE_NORMAL
- en: '**Distributed training in Elegy**'
  id: totrans-2307
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To train models in a distributed manner in Flax, we define parallel versions
    of our model training functions.
  id: totrans-2308
  prefs: []
  type: TYPE_NORMAL
- en: However, with Elegy, we call the [distributed] method.model = model.distributed()
  id: totrans-2309
  prefs: []
  type: TYPE_NORMAL
- en: '**Keras-like callbacks in Flax**'
  id: totrans-2310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Elegy supports callbacks similar to Keras callbacks. In this case, we train
    the model with the following callbacks:'
  id: totrans-2311
  prefs: []
  type: TYPE_NORMAL
- en: TensorBoard.
  id: totrans-2312
  prefs: []
  type: TYPE_NORMAL
- en: Model checkpoint.
  id: totrans-2313
  prefs: []
  type: TYPE_NORMAL
- en: Early stopping.callbacks = [ eg.callbacks.TensorBoard("summaries"), eg.callbacks.ModelCheckpoint("models/high-level",
    save_best_only=True),eg.callbacks.EarlyStopping(monitor = 'val_loss',pa tience=10)]
  id: totrans-2314
  prefs: []
  type: TYPE_NORMAL
- en: '**Train Elegy models**'
  id: totrans-2315
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Elegy provides the [fit] method for training models. The method supports the
    following data sources:'
  id: totrans-2316
  prefs: []
  type: TYPE_NORMAL
- en: Tensorflow Dataset.
  id: totrans-2317
  prefs: []
  type: TYPE_NORMAL
- en: Pytorch DataLoader
  id: totrans-2318
  prefs: []
  type: TYPE_NORMAL
- en: Elegy DataLoader, and
  id: totrans-2319
  prefs: []
  type: TYPE_NORMAL
- en: Python Generators.
  id: totrans-2320
  prefs: []
  type: TYPE_NORMAL
- en: history = model.fit(
  id: totrans-2321
  prefs: []
  type: TYPE_NORMAL
- en: training_data,
  id: totrans-2322
  prefs: []
  type: TYPE_NORMAL
- en: epochs=100,
  id: totrans-2323
  prefs: []
  type: TYPE_NORMAL
- en: validation_data=(validation_data), callbacks=callbacks,
  id: totrans-2324
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: )![](../images/00060.gif)
  id: totrans-2325
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Evaluate Elegy models**'
  id: totrans-2326
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: To evaluate Elegy models, use the [evaluate] function.model.evaluate(validation_data)![](../images/00061.jpeg)
  id: totrans-2327
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Visualize Elegy model with TensorBoard**'
  id: totrans-2328
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Since we applied the TensorBoard callback, we can view the performance of the
    model in TensorBoard.%load_ext tensorboard%tensorboard --logdir summaries ![](../images/00062.jpeg)
  id: totrans-2329
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Plot model performance with Matplotlib**'
  id: totrans-2330
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: We can also plot the performance of the model using Matplotlib.import matplotlib.pyplot
    as plt
  id: totrans-2331
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'def plot_history(history):'
  id: totrans-2332
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: n_plots = len(history.history.keys()) // 2
  id: totrans-2333
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: plt.figure(figsize=(14, 24))
  id: totrans-2334
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: for i, key in enumerate(list(history.history.keys())[:n_plo
  id: totrans-2335
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'ts]):'
  id: totrans-2336
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: metric = history.history[`key`]
  id: totrans-2337
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: val_metric = history.history[f`val_{key}`]
  id: totrans-2338
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: plt.subplot(n_plots, 1, i + 1)
  id: totrans-2339
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: plt.plot(metric, label=f`Training {key}`) plt.plot(val_metric, label=f`Validation
    {key}`) plt.legend(loc=`lower right`)
  id: totrans-2340
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: plt.ylabel(`key`)
  id: totrans-2341
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: plt.title(`Training and Validation {key}`)
  id: totrans-2342
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: plt.show()plot_history(history) ![](../images/00063.jpeg)
  id: totrans-2343
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Making predictions with Elegy models**'
  id: totrans-2344
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Like Keras, Elegy provides the [predict] method for making predictions.(text,
    test_labels) = next(iter(validation_data)) y_pred = model.predict(jnp.array(text))
    ![](../images/00064.jpeg)
  id: totrans-2345
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Saving and loading Elegy models**'
  id: totrans-2346
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Elegy models can also be saved like Keras models and used to make predictions
    immediately.
  id: totrans-2347
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: You can use can use `save` but `ModelCheckpoint already seria lized the model
  id: totrans-2348
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: model.save(`model`)
  id: totrans-2349
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: current model referenceprint(`current model id:`, id(model))
  id: totrans-2350
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: load model from disk
  id: totrans-2351
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: model = eg.load(`models/high-level`)
  id: totrans-2352
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: new model reference
  id: totrans-2353
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: 'print(`new model id: `, id(model))'
  id: totrans-2354
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: check that it works!model.evaluate(validation_data) ![](../images/00065.jpeg)
  id: totrans-2355
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '**Final thoughts**'
  id: totrans-2356
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: 'This article has been a quick dive into Elegy– a JAX high-level API that you
    can use to build and train Flax networks. You have seen that Elegy is very similar
    to Keras and has a simple API for Flax. It also contains similar functions to
    Keras, like:'
  id: totrans-2357
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Model training.
  id: totrans-2358
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Making predictions.
  id: totrans-2359
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Creating callbacks.
  id: totrans-2360
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Defining model loss and metrics.
  id: totrans-2361
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Appendix**'
  id: totrans-2362
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: This book is provided in line with our terms and privacy policy.
  id: totrans-2363
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Disclaimer**'
  id: totrans-2364
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: The information in this eBook is not meant to be applied as is in a production
  id: totrans-2365
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: environment. By applying it to a production environment, you take full responsibility
  id: totrans-2366
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: for your actions.
  id: totrans-2367
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The author has made every effort to ensure the accuracy of the information within
  id: totrans-2368
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: this book was correct at the time of publication. The author does not assume
    and
  id: totrans-2369
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: hereby disclaims any liability to any party for any loss, damage, or disruption
    caused
  id: totrans-2370
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: by errors or omissions, whether such errors or omissions result from accident,
  id: totrans-2371
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: negligence, or any other cause.
  id: totrans-2372
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: No part of this eBook may be reproduced or transmitted in any form or by any
  id: totrans-2373
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: means, electronic or mechanical, recording or by any information storage and
  id: totrans-2374
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: retrieval system, without written permission from the author.
  id: totrans-2375
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Copyright**'
  id: totrans-2376
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: JAX and Flax book— Deep learning with Flax and JAX © Copyright Derrick Mwiti.
    All Rights Reserved.
  id: totrans-2377
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '**Other things to learn**'
  id: totrans-2378
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Learn Python
  id: totrans-2379
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Learn data science
  id: totrans-2380
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Learn Streamlit
  id: totrans-2381
  prefs: []
  stylish: true
  type: TYPE_NORMAL
