- en: '**JAX & Flax ebook**'
  id: totrans-0
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '**JAX & Flax 电子书**'
- en: '![](`../images/00001.jpeg`)'
  id: totrans-1
  prefs: []
  stylish: true
  type: TYPE_IMG
  zh: '![](`../images/00001.jpeg`)'
- en: Download all notebooks JAX (What it is and how to use it in Python) What is
    XLA? Installing JAX **Setting up TPUs on Google Colab Data types in JAX Ways to
    create JAX arrays Generating random numbers with JAX Pure functions JAX NumPy
    operations JAX arrays are immutable Out-of-Bounds Indexing Data placement on devices
    in JAX How fast is JAX? Using jit() to speed up functions How JIT works Taking
    derivatives with grad() Auto-vectorization with vmap Parallelization with pmap
    Debugging NANs in JAX Double (64bit) precision What is a pytree? Handling state
    in JAX Loading datasets with JAX Building neural networks with JAX Final thoughts**
    Optimizers in JAX and Flax **Adaptive vs stochastic gradient descent (SGD) optimizers
    AdaBelief AdaGrad Adam – Adaptive moment estimation AdamW RAdam – Rectified Adam
    optimizer AdaFactor Fromage Lamb – Layerwise adaptive large batch optimization
    Lars – Layer-wise Adaptive Rate Scaling SM3 - Square-root of Minima of Sums of
    Maxima of Squared-gradients Method SGD– Stochastic Gradient Descent Noisy SGD
    Optimistic GD Differentially Private SGD RMSProp Yogi Final thoughts JAX loss
    functions What is a loss function? Creating custom loss functions in JAX Which
    loss functions are available in JAX? Sigmoid binary cross entropy Softmax cross
    entropy Cosine distance Cosine similarity Huber loss l2 loss log cosh Smooth labels
    Computing loss with JAX Metrics How to monitor JAX loss functions Why JAX loss
    nan happens Final thoughts Activation functions in JAX and Flax ReLU – Rectified
    linear unit PReLU– Parametric Rectified Linear Unit Sigmoid Log sigmoid Softmax
    Log softmax ELU – Exponential linear unit activation CELU – Continuously-differentiable
    exponential linear unit GELU– Gaussian error linear unit activation GLU – Gated
    linear unit activation Soft sign Softplus Swish–Sigmoid Linear Unit( SiLU) Custom
    activation functions in JAX and Flax Final thoughts How to load datasets in JAX
    with TensorFlow How to load text data in JAX Clean the text data Label encode
    the sentiment column Text preprocessing with TensorFlow How to load image data
    in JAX How to load CSV data in JAX Final thoughts Image classification with JAX
    & Flax Loading the dataset Define Convolution Neural Network with Flax Define
    loss Compute metrics Create training state Define training step Define evaluation
    step Training function Evaluate the model Train and evaluate the model Model performance
    Final thoughts Distributed training with JAX & Flax Perform standard imports Setup
    TPUs on Colab Download the dataset Load the dataset Define the model with Flax
    Create training state Apply the model Training function Train the model Model
    evaluation Final thoughts How to use TensorBoard in JAX & Flax How to use TensorBoard
    How to install TensorBoard Using TensorBoard with Jupyter notebooks and Google
    Colab How to launch TensorBoard Tensorboard dashboards How to use TensorBoard
    with Flax How to log images with TensorBoard in Flax How to log text with TensorBoard
    in Flax Track model training in JAX using TensorBoard How to profile JAX programs
    with TensorBoard Programmatic profiling Manual profiling with TensorBoard How
    to profile JAX program on a remote machine Share TensorBoard dashboards Final
    thoughts Handling state in JAX & Flax (BatchNorm and DropOut layers) Perform standard
    imports Download the dataset Loading datasets in JAX Data processing with PyTorch
    Define Flax model with BatchNorm and DropOut Create loss function Compute metrics
    Create custom Flax training state Training step Evaluation step Train Flax model
    Set up TensorBoard in Flax Train model Save Flax model Load Flax model Evaluate
    Flax model Visualize Flax model performance Final thoughts LSTM in JAX & Flax
    Dataset download Data processing with NLTK Text vectorization with Keras Create
    tf.data dataset Define LSTM model in Flax Compute metrics in Flax Create training
    state Define training step Evaluate the Flax model Create training function Train
    LSTM model in Flax Visualize LSTM model performance in Flax Save LSTM model Final
    thoughts Flax vs. TensorFlow Random number generation in TensorFlow and Flax Model
    definition in Flax and TensorFlow Activations in Flax and TensorFlow Optimizers
    in Flax and TensorFlow Metrics in Flax and TensorFlow Computing gradients in Flax
    and TensorFlow Loading datasets in Flax and TensorFlow Training model in Flax
    vs. TensorFlow Distributed training in Flax and TensorFlow Working with TPU accelerators
    Model evaluation Visualize model performance Final thoughts Train ResNet in Flax
    from scratch(Distributed ResNet training) Install Flax models Perform standard
    imports Download dataset Loading dataset in Flax Data transformation in Flax Instantiate
    Flax ResNet model Compute metrics Create Flax model training state Apply model
    function TensorBoard in Flax Train Flax ResNet model Evaluate model with TensorBoard
    Visualize Flax model performance Save Flax ResNet model Load Flax RestNet model
    Final thoughts Transfer learning with JAX & Flax Install JAX ResNet Download dataset
    Data loading in JAX Data processing ResNet model definition Create head network
    Combine ResNet backbone with head Load pre-trained ResNet 50 Get model and variables
    Zero gradients Define Flax optimizer Define Flax loss function Compute Flax metrics
    Create Flax training state Training step Evaluation step Train ResNet model in
    Flax Set up TensorBoard in Flax Train model Save Flax model Load saved Flax model
    Evaluate Flax ResNet model Visualize model performance Final thoughts Elegy(High-level
    API for deep learning in JAX & Flax) Data pre-processing Model definition in Elegy
    Elegy model summary Distributed training in Elegy Keras-like callbacks in Flax
    Train Elegy models Evaluate Elegy models Visualize Elegy model with TensorBoard
    Plot model performance with Matplotlib Making predictions with Elegy models Saving
    and loading Elegy models Final thoughts** Appendix Disclaimer Copyright Other
    things to learn
  id: totrans-2
  prefs:
  - PREF_H3
  stylish: true
  type: TYPE_NORMAL
  zh: 下载所有 JAX 笔记本（它是什么以及如何在 Python 中使用它）什么是 XLA？安装 JAX **在 Google Colab 上设置 TPU JAX
    中的数据类型 使用 JAX 创建数组 用 JAX 生成随机数 纯函数 JAX NumPy 操作 JAX 数组是不可变的 超出边界索引 JAX 中的数据放置
    JAX 有多快？ 使用 jit() 加速函数 JIT 如何工作 用 grad() 求导 自动向量化与 vmap 使用 pmap 并行化 在 JAX 中调试
    NANs 双精度（64位）是什么？ 什么是 pytree？ 在 JAX 中处理状态 使用 JAX 加载数据集 使用 JAX 构建神经网络 总结** JAX
    和 Flax 中的优化器 ** 自适应与随机梯度下降（SGD）优化器 AdaBelief AdaGrad Adam – 自适应矩估计 AdamW RAdam
    – 修正的 Adam 优化器 AdaFactor Fromage Lamb – 层次自适应大批量优化 Lars – 层次自适应速率缩放 SM3 - 平方根最小值之和的最大值的平方梯度法
    SGD – 随机梯度下降 Noisy SGD 乐观的 GD 差分隐私 SGD RMSProp Yogi 总结 JAX 损失函数 什么是损失函数？ 在 JAX
    中创建自定义损失函数 JAX 中可用的损失函数有哪些？ Sigmoid 二元交叉熵 Softmax 交叉熵 余弦距离 余弦相似度 Huber 损失 l2 损失
    log cosh 平滑标签 使用 JAX 计算损失函数 指标 如何监控 JAX 损失函数 JAX 损失为 NAN 的原因 总结 JAX 和 Flax 中的激活函数
    ReLU – 矫正线性单元 PReLU– 参数化矫正线性单元 Sigmoid Log sigmoid Softmax Log softmax ELU – 指数线性单元
    CELU – 连续可微指数线性单元 GELU– 高斯误差线性单元 GLU – 门控线性单元 Soft sign Softplus Swish–Sigmoid Linear
    Unit( SiLU) JAX 和 Flax 中的自定义激活函数 总结 如何在 JAX 中使用 TensorFlow 加载数据集 如何在 JAX 中加载文本数据
    清理文本数据 标签编码情感列 使用 TensorFlow 进行文本预处理 如何在 JAX 中加载图像数据 如何在 JAX 中加载 CSV 数据 总结 JAX
    和 Flax 中的图像分类 定义 Convolution Neural Network 使用 Flax 定义损失 计算指标 创建训练状态 定义训练步骤 定义评估步骤
    训练函数 评估模型 训练和评估模型 模型性能 总结 JAX 和 Flax 中的分布式训练 执行标准导入 在 Colab 上设置 TPUs 下载数据集 加载数据集
    使用 Flax 定义模型 创建训练状态 应用模型 训练函数 训练模型 模型评估 总结 如何在 JAX 和 Flax 中使用 TensorBoard 如何使用
    TensorBoard 如何安装 TensorBoard 在 Jupyter notebooks 和 Google Colab 中使用 TensorBoard
    如何启动 TensorBoard Tensorboard 仪表板 如何在 Flax 中使用 TensorBoard 如何记录图像到 Flax 的 TensorBoard
    如何记录文本到 Flax 的 TensorBoard 使用 TensorBoard 在 JAX 中跟踪模型训练 如何使用 TensorBoard 对 JAX
    程序进行性能分析 通过 TensorBoard 进行编程性能分析 手动使用 TensorBoard 进行性能分析 如何在远程机器上对 JAX 程序进行性能分析
    分享 TensorBoard 仪表板 总结 JAX 和 Flax 中的状态处理（BatchNorm 和 DropOut 层） 执行标准导入 下载数据集 在
    JAX 中加载数据集 使用 PyTorch 处理数据 使用 BatchNorm 和 DropOut 层定义 Flax 模型 创建损失函数 计算指标 创建自定义
    Flax 训练状态 训练步骤 评估步骤 训练 Flax 模型 在 Flax 中设置 TensorBoard 训练模型 保存 Flax 模型 加载 Flax
    模型 评估 Flax 模型 可视化 Flax 模型性能 总结 JAX 和 Flax 中的 LSTM 数据集下载 使用 NLTK 处理文本 数据向量化 使用
    Keras 创建 tf.data 数据集 在 Flax 中定义 LSTM 模型 计算 Flax 中的指标 创建训练状态 定义训练步骤 评估 Flax 模型
    创建训练函数 在 Flax 中训练 LSTM 模型 可视化 Flax 中的 LSTM 模型性能 保存 LSTM 模型 总结 Flax vs. TensorFlow
    TensorFlow 和 Flax 中的随机数生成 在 Flax 和 TensorFlow 中定义模型 在 Flax 和 TensorFlow 中的激活函数
    在 Flax 和 TensorFlow 中的优化器 在 Flax 和 TensorFlow 中的指标 在 Flax 和 TensorFlow 中计算梯度 在
    Flax 和 TensorFlow 中加载数据集 在 Flax 和 TensorFlow 中训练模型 在 Flax 和 TensorFlow 中的分布式训练
    使用 TPU 加速器 模型评估 可视化模型性能 总结 从头开始在 Flax 中训练 ResNet（分布式 ResNet 训练） 安装 Flax 模型 执行标准导入
    下载数据集 在 Flax 中加载数据集 数据转换 在 Flax 中实例化 ResNet 模型 计算指标 创建 Flax 模型训练状态 应用模型函数 Flax
    中的 TensorBoard 训练 Flax ResNet 模型 评估带有 TensorBoard 的模型 可视化 Flax 模型性能 保存 Flax ResNet
    模型 加载 Flax ResNet 模型 总结 JAX 和 Flax 中的迁移学习 安装 JAX ResNet 下载数据集 在 JAX 中加载数据 处理数据
    在 Flax 中定义 ResNet 模型 计算 Flax 中的指标 创建 Flax 优化器 定义 Flax 损失函数 计算 Flax 指标 创建 Flax
    训练状态 训练步骤 评估步骤 在 Flax 中训练 ResNet 模型 设置 Flax 中的 TensorBoard 训练模型 保存 Flax 模型 加载保存的
    Flax 模型 评估 Flax ResNet 模型 可视化模型性能 总结 Elegy（JAX 和 Flax 中的高级 API） 数据预处理 在 Elegy
    中定义模型 Elegy 模型摘要 Elegy 中的分布式训练 Flax 中的 Keras 回调 在 Flax 中训练 Elegy 模型 评估 Elegy 模型
    使用 TensorBoard 可视化 Elegy 模型 用 Matplotlib 绘制模型性能 使用 Elegy 模型进行预测 保存和加载 Elegy 模型
    总结** 附录 免责声明 版权 其他要学习的事项
- en: Download all notebooks
  id: totrans-3
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 下载所有笔记本
- en: Link to download all notebooks. The password for the ZIP file is FDCPJx0D5A6SO#%Qsg
  id: totrans-4
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 下载所有笔记本的链接。ZIP 文件的密码是 FDCPJx0D5A6SO#%Qsg
- en: JAX (What it is and how to use it in Python)
  id: totrans-5
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: JAX（它是什么以及如何在 Python 中使用它）
- en: 'JAX is a Python library offering high performance in machine learning with
    XLA and Just In Time (JIT) compilation. Its API is similar to NumPy''s with a
    few differences. JAX ships with functionalities that aim to improve and increase
    speed in machine learning research. These functionalities include:'
  id: totrans-6
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAX 是一个 Python 库，利用 XLA 和即时（JIT）编译提供高性能机器学习。其 API 类似于 NumPy，但有一些不同之处。JAX 配备了旨在改善和提高机器学习研究速度的功能。这些功能包括：
- en: Automatic differentiation
  id: totrans-7
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 自动微分
- en: Vectorization
  id: totrans-8
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 向量化
- en: JIT compilation
  id: totrans-9
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JIT 编译
- en: This article will cover these functionalities and other JAX concepts. Let's
    get started.
  id: totrans-10
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 本文将涵盖这些功能和其他 JAX 概念。让我们开始吧。
- en: What is XLA?
  id: totrans-11
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 什么是 XLA？
- en: XLA (Accelerated Linear Algebra) is a linear algebra compiler for accelerating
    machine learning models. It leads to an increase in the speed of model execution
    and reduced memory usage. XLA programs can be generated by JAX, PyTorch, Julia,
    and NX.
  id: totrans-12
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: XLA（加速线性代数）是用于加速机器学习模型的线性代数编译器。它导致模型执行速度的提高和内存使用量的减少。JAX、PyTorch、Julia 和 NX
    都可以生成 XLA 程序。
- en: Installing JAX
  id: totrans-13
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 安装 JAX
- en: 'JAX can be installed from the Python Package Index using: `pip install jax`
    JAX is pre-installed on Google Colab. See the link below for other installation
    options.'
  id: totrans-14
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAX 可以通过 Python 包索引进行安装： `pip install jax` JAX 已预装在 Google Colab 上。查看下面的链接获取其他安装选项。
- en: Setting up TPUs on Google Colab
  id: totrans-15
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 在 Google Colab 上设置 TPUs
- en: You need to set up JAX to use TPUs on Colab. That is done by executing the following
    code. Ensure that you have changed the runtime to TPU by going to Runtime-> Change
    Runtime Type. If no accelerator is available, JAX will use the CPU.
  id: totrans-16
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 您需要设置 JAX 以在 Colab 上使用 TPUs。通过执行以下代码来完成。确保您已经将运行时更改为 TPU，方法是转到运行时->更改运行时类型。如果没有可用的加速器，JAX
    将使用 CPU。
- en: '`import jax.tools.colab_tpu jax.tools.colab_tpu.setup_tpu() jax.devices()`'
  id: totrans-17
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import jax.tools.colab_tpu jax.tools.colab_tpu.setup_tpu() jax.devices()`'
- en: Data types in JAX
  id: totrans-18
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: JAX 中的数据类型
- en: The data types in NumPy are similar to those in JAX arrays. For instance, here
    is how you can create float and int data in JAX.
  id: totrans-19
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: NumPy 中的数据类型与 JAX 数组中的类似。例如，以下是如何在 JAX 中创建浮点数和整数数据的方式。
- en: '`import jax.numpy as jnp x = jnp.float32(1.25844) x = jnp.int32(45.25844)`'
  id: totrans-20
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import jax.numpy as jnp x = jnp.float32(1.25844) x = jnp.int32(45.25844)`'
- en: When you check the type of the data, you will see that it's
  id: totrans-21
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 当您检查数据类型时，您会发现它是
- en: a `DeviceArray.DeviceArray` in JAX is the equivalent of `numpy.ndarray` in NumPy.
  id: totrans-22
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAX 中的 `DeviceArray.DeviceArray` 相当于 NumPy 中的 `numpy.ndarray`。
- en: '`jax.numpy` provides an interface similar to NumPy''s. However, JAX also provides
    `jax.lax` a low-level API that is more powerful and stricter. For example, with
    `[jax.numpy]` you can add numbers that have mixed types but `[jax.lax]` will not
    allow this.'
  id: totrans-23
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jax.numpy` 提供了与 NumPy 类似的接口。但是，JAX 还提供了 `jax.lax`，这是一个更强大和更严格的低级 API。例如，使用
    `[jax.numpy]` 可以添加具有混合类型的数字，但 `[jax.lax]` 不允许这样做。'
- en: Ways to create JAX arrays
  id: totrans-24
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 创建 JAX 数组的方法
- en: You can create JAX arrays like you would in NumPy. For example, can use:arange
  id: totrans-25
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 您可以像在 NumPy 中一样创建 JAX 数组。例如，可以使用：arange
- en: linspacePython lists.ones.zeros.identity.
  id: totrans-26
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: linspacePython lists.ones.zeros.identity.
- en: '`jnp.arange(10)`'
  id: totrans-27
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jnp.arange(10)`'
- en: '`jnp.arange(0,10)`'
  id: totrans-28
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jnp.arange(0,10)`'
- en: '`scores = [50,60,70,30,25,70] scores_array = jnp.array(scores) jnp.zeros(5)`'
  id: totrans-29
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`scores = [50,60,70,30,25,70] scores_array = jnp.array(scores) jnp.zeros(5)`'
- en: '`jnp.ones(5)`'
  id: totrans-30
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jnp.ones(5)`'
- en: '`jnp.eye(5)`'
  id: totrans-31
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jnp.eye(5)`'
- en: '`jnp.identity(5)`'
  id: totrans-32
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jnp.identity(5)`'
- en: '`![](../images/00002.jpeg)`'
  id: totrans-33
  prefs: []
  stylish: true
  type: TYPE_IMG
  zh: '`![](../images/00002.jpeg)`'
- en: Generating random numbers with JAX
  id: totrans-34
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 使用 JAX 生成随机数
- en: Random number generation is one main difference between JAX and NumPy. JAX is
    meant to be used with functional programs. JAX expects these functions to be pure.
    A **pure function** has no side effects and expects the output to only come from
    its inputs. JAX transformation functions expect pure functions.
  id: totrans-35
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 随机数生成是 JAX 与 NumPy 的一个主要区别。JAX 旨在与功能程序一起使用。JAX 期望这些函数是纯函数。**纯函数** 没有副作用，并期望输出仅来自其输入。JAX
    转换函数期望纯函数。
- en: Therefore, when working with JAX, all input should be passed through function
    parameters, while all output should come from the function results. Hence, something
    like Python's print function is not pure.
  id: totrans-36
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 因此，在使用 JAX 时，所有输入都应通过函数参数传递，而所有输出都应来自函数结果。因此，类似于 Python 的打印函数不是纯函数。
- en: A pure function returns the same results when called with the same inputs. This
    is not possible with [np.random.random()] because it is stateful and returns different
    results when called several times.
  id: totrans-37
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 纯函数在使用相同的输入调用时返回相同的结果。这对于[np.random.random()]是不可能的，因为它是有状态的，在多次调用时返回不同的结果。
- en: '`print(np.random.random()) print(np.random.random()) print(np.random.random())`'
  id: totrans-38
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`print(np.random.random()) print(np.random.random()) print(np.random.random())`'
- en: '`![](../images/00003.jpeg)`'
  id: totrans-39
  prefs: []
  stylish: true
  type: TYPE_IMG
  zh: '`![](../images/00003.jpeg)`'
- en: JAX implements random number generation using a random state. This random state
    is referred to as a `key🔑`. JAX generates pseudorandom numbers from the pseudorandom
    number generator (PRNGs)  state.
  id: totrans-40
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAX使用随机状态来实现随机数生成。这个随机状态被称为`key🔑`。JAX从伪随机数生成器（PRNGs）状态中生成伪随机数。
- en: '`seed = 98`'
  id: totrans-41
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`seed = 98`'
- en: '`key = jax.random.PRNGKey(seed) jax.random.uniform(key)`'
  id: totrans-42
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`key = jax.random.PRNGKey(seed) jax.random.uniform(key)`'
- en: You should, therefore, not reuse the same state. Instead, you should split the
    PRNG to obtain as many sub keys as you need.`key, subkey = jax.random.split(key)
    Using the same key will always generate the same output. ![](../images/00004.gif)`
  id: totrans-43
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 因此，你不应该重用相同的状态。相反，你应该分割PRNG以获得所需数量的子键。`key, subkey = jax.random.split(key) 使用相同的键将始终生成相同的输出。![](../images/00004.gif)`
- en: Pure functions
  id: totrans-44
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 纯函数
- en: We have mentioned that the output of a pure function should only come from the
    result of the function. Therefore, something like Python's [print] function introduces
    impurity. This can be demonstrated using this function.
  id: totrans-45
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 我们已经提到纯函数的输出应该只来自函数的结果。因此，像Python的[print]函数这样的东西会引入不纯性。这可以通过这个函数来演示。
- en: '`def impure_print_side_effect(x):`'
  id: totrans-46
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def impure_print_side_effect(x):`'
- en: '`print("Executing function")` # This is a side-effect return x'
  id: totrans-47
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`print("Executing function")` # 这是一个副作用返回x'
- en: 'The side-effects appear during the first run`print ("First call: ", jax.jit(impure_print_side_effect)(4.))`'
  id: totrans-48
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '副作用出现在第一次运行时`print ("First call: ", jax.jit(impure_print_side_effect)(4.))`'
- en: Subsequent runs with parameters of same type and shape may no t show the side-effect
  id: totrans-49
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: 相同类型和形状的参数进行后续运行可能不会显示副作用。
- en: This is because JAX now invokes a cached compilation of the f unction
  id: totrans-50
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: 这是因为JAX现在调用了函数的缓存编译
- en: '`print ("Second call: ", jax.jit(impure_print_side_effect)(5.))`'
  id: totrans-51
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`print ("Second call: ", jax.jit(impure_print_side_effect)(5.))`'
- en: JAX re-runs the Python function when the type or shape of the argument changes
  id: totrans-52
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: 当参数的类型或形状发生变化时，JAX会重新运行Python函数
- en: '`print ("Third call, different type: ", jax.jit(impure_print_sid e_effect)(jnp.array([5.])))`'
  id: totrans-53
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`print ("Third call, different type: ", jax.jit(impure_print_sid e_effect)(jnp.array([5.])))`'
- en: '`![](../images/00005.gif)`'
  id: totrans-54
  prefs: []
  stylish: true
  type: TYPE_IMG
  zh: '`![](../images/00005.gif)`'
- en: We can see the printed statement the first time the function is executed. However,
    we don't see that print statement in consecutive runs because it is cached. We
    only see the statement again after changing the data's shape, which forces JAX
    to recompile the function. More on jax.jit in a moment.
  id: totrans-55
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 第一次执行函数时，我们可以看到打印的语句。然而，在连续运行中我们看不到这个打印语句，因为它被缓存了。只有在改变数据形状后，强制JAX重新编译函数时，我们才会再次看到这个语句。稍后我们会详细讨论`jax.jit`。
- en: JAX NumPy operations
  id: totrans-56
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: JAX NumPy操作
- en: Operations on JAX arrays are similar to operations with NumPy arrays. For example,
    you can [max], [argmax], and [sum] like in NumPy.
  id: totrans-57
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 对JAX数组的操作类似于对NumPy数组的操作。例如，你可以像在NumPy中一样使用[max]、[argmax]和[sum]。
- en: '`matrix = matrix.reshape(4,4) jnp.max(matrix)`'
  id: totrans-58
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`matrix = matrix.reshape(4,4) jnp.max(matrix)`'
- en: '`jnp.argmax(matrix)`'
  id: totrans-59
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jnp.argmax(matrix)`'
- en: '`jnp.min(matrix)`'
  id: totrans-60
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jnp.min(matrix)`'
- en: '`jnp.argmin(matrix)`'
  id: totrans-61
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jnp.argmin(matrix)`'
- en: '`jnp.sum(matrix)`'
  id: totrans-62
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jnp.sum(matrix)`'
- en: '`jnp.sqrt(matrix)`'
  id: totrans-63
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jnp.sqrt(matrix)`'
- en: '`matrix.transpose()`'
  id: totrans-64
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`matrix.transpose()`'
- en: '`![](../images/00006.jpeg)`However, JAX doesn''t allow operations on non-array
    input like NumPy. For example, passing Python lists or tuples will lead to an
    error.'
  id: totrans-65
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`![](../images/00006.jpeg)`然而，JAX不允许对非数组输入（如NumPy中的Python列表或元组）进行操作，这会导致错误。'
- en: '`try:`'
  id: totrans-66
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`try:`'
- en: '`jnp.sum([1, 2, 3])`'
  id: totrans-67
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jnp.sum([1, 2, 3])`'
- en: '`except TypeError as e:`'
  id: totrans-68
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`except TypeError as e:`'
- en: '`print(f"TypeError: {e}")`'
  id: totrans-69
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`print(f"TypeError: {e}")`'
- en: '`TypeError: sum requires ndarray or scalar arguments, got <c`'
  id: totrans-70
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`TypeError: sum requires ndarray or scalar arguments, got <c`'
- en: '`lass ''list''> at position 0.`'
  id: totrans-71
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`lass ''list''> at position 0.`'
- en: JAX arrays are immutable
  id: totrans-72
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: JAX数组是不可变的
- en: Unlike in NumPy, JAX arrays can not be modified in place. This is because JAX
    expects pure functions.
  id: totrans-73
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 与NumPy不同，JAX数组不能就地修改。这是因为JAX期望纯函数。
- en: '`scores = [50,60,70,30,25]`'
  id: totrans-74
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`scores = [50,60,70,30,25]`'
- en: '`scores_array = jnp.array(scores)`'
  id: totrans-75
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`scores_array = jnp.array(scores)`'
- en: '`scores_array[0:3] = [20,40,90]`'
  id: totrans-76
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`scores_array[0:3] = [20,40,90]`'
- en: '`TypeError: ''<class ''jaxlib.xla_extension.DeviceArray''>'' objec t does not
    support item assignment.`'
  id: totrans-77
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`TypeError: ''<class ''jaxlib.xla_extension.DeviceArray''>'' objec t does not
    support item assignment.`'
- en: JAX arrays are immutable. Instead of ``x[idx] = y``, use ``x
  id: totrans-78
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: JAX数组是不可变的。而不是``x[idx] = y``，请使用``x
- en: '`= x.at[idx].set(y)`` or another `.at[]`'
  id: totrans-79
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`= x.at[idx].set(y)`` 或者另一个 `.at[]`'
- en: 'method: `https://jax.readthedocs.io/en/latest/_autosummary/ja x.numpy.ndarray.at.html`'
  id: totrans-80
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: 方法：`https://jax.readthedocs.io/en/latest/_autosummary/ja x.numpy.ndarray.at.html`
- en: Array updates in JAX are performed using `[x.at[idx].set(y)]`. This returns
    a new array while the old array stays unaltered.
  id: totrans-81
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAX中的数组更新使用`[x.at[idx].set(y)]`进行。这将返回一个新数组，而旧数组保持不变。
- en: '`try:`'
  id: totrans-82
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`try:`'
- en: '`jnp.sum([1, 2, 3])`'
  id: totrans-83
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jnp.sum([1, 2, 3])`'
- en: '`except TypeError as e:`'
  id: totrans-84
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`except TypeError as e:`'
- en: '`print(f"TypeError: {e}")`'
  id: totrans-85
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`print(f"TypeError: {e}")`'
- en: '`TypeError: sum requires ndarray or scalar arguments, got <c`'
  id: totrans-86
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`TypeError: sum需要ndarray或标量参数，得到<c`'
- en: '`lass ''list''> at position 0.`'
  id: totrans-87
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`在位置0处''list''列表。`'
- en: '**Out-of-Bounds Indexing**'
  id: totrans-88
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**越界索引**'
- en: '`NumPy` usually throws an error when you try to get an item in an array that
    is out of bounds. JAX doesn''t throw any error but returns the last item in the
    array.'
  id: totrans-89
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`NumPy`通常在您尝试获取数组中超出边界的项时会抛出错误。JAX不会抛出任何错误，而是返回数组中的最后一项。'
- en: '`matrix = jnp.arange(1,17) matrix[20]`'
  id: totrans-90
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`matrix = jnp.arange(1,17) matrix[20]`'
- en: '`DeviceArray(16, dtype=int32)`'
  id: totrans-91
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`DeviceArray(16, dtype=int32)`'
- en: JAX is designed like this because throwing errors in accelerators can be challenging.
  id: totrans-92
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAX设计如此，因为在加速器中抛出错误可能会很具有挑战性。
- en: '**Data placement on devices in JAX**'
  id: totrans-93
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**JAX中的数据放置**'
- en: JAX arrays are placed in the first device, `[jax.devices()[0]]`that is, GPU,
    TPU, or CPU. Data can be placed on a particular device
  id: totrans-94
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAX数组被放置在第一个设备上，`[jax.devices()[0]]`即GPU、TPU或CPU。数据可以被放置在特定的设备上
- en: using `jax.device_put()`.
  id: totrans-95
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 使用`jax.device_put()`。
- en: from `jax` import `device_put`
  id: totrans-96
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from jax import device_put`'
- en: import `numpy` as `np`
  id: totrans-97
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import numpy as np`'
- en: '`size = 5000`'
  id: totrans-98
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`size = 5000`'
- en: '`x = np.random.normal(size=(size, size)).astype(np.float32) x = device_put(x)`'
  id: totrans-99
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = np.random.normal(size=(size, size)).astype(np.float32) x = device_put(x)`'
- en: The data becomes committed to that device, and operations on it are also committed
    on the same device.
  id: totrans-100
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 数据将提交到该设备，并且对其进行的操作也将提交到同一设备。
- en: '**How fast is JAX?**'
  id: totrans-101
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**JAX的速度有多快？**'
- en: JAX uses `asynchronous` dispatch, meaning that it does not wait for computation
    to complete to give control back to the `Python program`. Therefore, when you
    perform an execution, JAX will return a future. JAX forces Python to wait for
    the execution when you want to print the output or if you convert the result to
    a `NumPy array`.
  id: totrans-102
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAX使用`asynchronous`分发，这意味着它不会等待计算完成就将控制权交还给`Python程序`。因此，当您执行一个操作时，JAX会返回一个future。当您想要打印输出或将结果转换为`NumPy数组`时，JAX会强制Python等待执行。
- en: Therefore, if you want to compute the time of execution of a program you'll
    have to convert the result to a `NumPy` array
  id: totrans-103
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 因此，如果您想计算程序的执行时间，您将不得不将结果转换为`NumPy`数组
- en: using `[block_until_ready()]` to wait for the execution to complete. Generally
    speaking, `NumPy` will outperform JAX on the CPU, but JAX will outperform `NumPy`
    on accelerators and when using jitted functions.
  id: totrans-104
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 使用`[block_until_ready()]`等待执行完成。一般来说，在CPU上，`NumPy`的性能优于JAX，但在加速器和使用jitted函数时，JAX的性能优于`NumPy`。
- en: '**Using jit() to speed up functions**'
  id: totrans-105
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**使用jit()加速函数**'
- en: '`[jit]` performs just-in-time compilation with XLA. `[jax.jit]` expects a pure
    function. Any side effects in the function will only be executed once. Let''s
    create a pure function and time its execution time without `jit`.'
  id: totrans-106
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[jit]`使用XLA进行即时编译。`[jax.jit]`期望一个纯函数。函数中的任何副作用将只执行一次。让我们创建一个纯函数，并计算其在没有`jit`的情况下的执行时间。'
- en: '`def test_fn(sample_rate=3000,frequency=3):`'
  id: totrans-107
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def test_fn(sample_rate=3000,frequency=3):`'
- en: '`x = jnp.arange(sample_rate)`'
  id: totrans-108
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = jnp.arange(sample_rate)`'
- en: '`y = np.sin(2*jnp.pi*frequency * (frequency/sample_rate)) return jnp.dot(x,y)`'
  id: totrans-109
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`y = np.sin(2*jnp.pi*frequency * (frequency/sample_rate)) return jnp.dot(x,y)`'
- en: '`%timeit test_fn()`'
  id: totrans-110
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`%timeit test_fn()`'
- en: '`best of 5: 76.1 µs per loop`'
  id: totrans-111
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`最佳结果为 5 次：每次循环 76.1 µs`'
- en: Let's now use `jit` and time the execution of the same function. In this case,
    we can see that using `jit` makes the execution almost 20 times faster.
  id: totrans-112
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 现在让我们使用`jit`并计算相同函数的执行时间。在这种情况下，我们可以看到使用`jit`使执行速度几乎快了20倍。
- en: '`test_fn_jit = jax.jit(test_fn)`'
  id: totrans-113
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`test_fn_jit = jax.jit(test_fn)`'
- en: '`%timeit test_fn_jit().block_until_ready()` # best of 5: 4.54 µs per loop'
  id: totrans-114
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`%timeit test_fn_jit().block_until_ready()` # 最佳结果为 5 次：每次循环 4.54 µs'
- en: In the above example, `[test_fn_jit]` is the jit-compiled version of the function.
    JAX then created code that is optimized for GPU or TPU. The optimized code is
    what will be used the next time this function is called.
  id: totrans-115
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在上面的例子中，`[test_fn_jit]`是函数的jit编译版本。然后JAX创建了针对GPU或TPU优化的代码。优化后的代码将在下次调用此函数时使用。
- en: '**How JIT works**'
  id: totrans-116
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**JIT的工作原理**'
- en: JAX works by converting Python functions into an intermediate language called
    jaxpr (JAX Expression). The `[jax.make_jaxpr]` can be used to show the jaxpr representation
    of a Python function. If the function has any side effects, they are not recorded
    by jaxpr. We saw earlier that any side effects, for example, printing, will only
    be shown during the first call.
  id: totrans-117
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAX通过将Python函数转换为一种称为jaxpr（JAX表达式）的中间语言来工作。`[jax.make_jaxpr]`可用于显示Python函数的jaxpr表示。如果函数有任何副作用，它们不会被jaxpr记录。我们之前看到，例如打印的任何副作用只会在第一次调用时显示出来。
- en: '`def sum_logistic(x):`'
  id: totrans-118
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def sum_logistic(x):`'
- en: '`print("printed x:", x)`'
  id: totrans-119
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`print("printed x:", x)`'
- en: '`return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))`'
  id: totrans-120
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))`'
- en: '`x_small = jnp.arange(6.)print(jax.make_jaxpr(sum_logistic)(x_small))`'
  id: totrans-121
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x_small = jnp.arange(6.)print(jax.make_jaxpr(sum_logistic)(x_small))`'
- en: JAX creates the jaxpr through tracing. Each argument in the function is wrapped
    with a tracer object. The purpose of these tracers is to record all JAX operations
    performed on them when the function is called. JAX uses the tracer records to
    rebuild the function, which leads to jaxpr. Python side-effects don't show up
    in the jaxpr because the tracers do not record them.
  id: totrans-122
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAX通过追踪创建jaxpr。函数中的每个参数都会被包装成追踪器对象。这些追踪器的目的是在调用函数时记录对它们执行的所有JAX操作。JAX使用追踪器记录重建函数，从而得到jaxpr。Python的副作用不会出现在jaxpr中，因为追踪器不记录它们。
- en: JAX requires arrays shapes to be static and known at compile time. Decorating
    a function conditioned on a value with jit results in error. Therefore, not all
    code can be jit-compiled.
  id: totrans-123
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAX要求数组形状在编译时是静态和已知的。将带有值条件的函数使用jit修饰会导致错误。因此，并非所有代码都可以jit编译。
- en: '`@jax.jitdef f(boolean, x):return -x if boolean else x`'
  id: totrans-124
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`@jax.jitdef f(boolean, x):return -x if boolean else x`'
- en: '`f(True, 1)`'
  id: totrans-125
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`f(True, 1)`'
- en: '`ConcretizationTypeError`: Abstract tracer value encountered where concrete
    value is expected: `Traced<ShapedArray(bool[], weak _type=True)>with<DynamicJaxprTrace(level=0/1)>`'
  id: totrans-126
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`ConcretizationTypeError`: 遇到抽象追踪器值，但需要具体值：`Traced<ShapedArray(bool[], weak
    _type=True)>with<DynamicJaxprTrace(level=0/1)>`'
- en: 'There are a couple of solutions to this problem:'
  id: totrans-127
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 对于这个问题有几个解决方案：
- en: Remove conditionals on the value. Use JAX control flow operators such as `[jax.lax.cond]`.
  id: totrans-128
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 移除值条件。使用JAX控制流操作符如`[jax.lax.cond]`。
- en: Jit only a part of the function. Make parameters static.
  id: totrans-129
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 仅对函数的一部分使用jit。使参数静态化。
- en: We can implement the last option and make the boolean parameter static. This
    is done by specifying `[static_argnums]` or `[static_argnames]`. This forces JAX
    to recompile the function when the value of the static parameter changes. This
    is not a good strategy if the function will get many values for the static argument.
    You don't want to recompile the function too many times.
  id: totrans-130
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 我们可以实现最后一种选项并使布尔参数静态化。这可以通过指定`[static_argnums]`或`[static_argnames]`来完成。当静态参数的值变化时，这会强制JAX重新编译函数。如果函数会获得许多静态参数的值，这不是一个好策略。你不希望函数重新编译太多次。
- en: You can pass the static arguments using Python’s `[functools.partial]`. `from
    functools import partial@partial(jax.jit, static_argnums=(0,)) def f(boolean,
    x):return -x if boolean else xf(True, 1)`
  id: totrans-131
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 你可以使用Python的`[functools.partial]`传递静态参数。`from functools import partial@partial(jax.jit,
    static_argnums=(0,)) def f(boolean, x):return -x if boolean else xf(True, 1)`
- en: '**Taking derivatives with `grad()`**'
  id: totrans-132
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**使用`grad()`计算导数**'
- en: Computing derivatives in JAX is done using `jax.grad.` `@jax.jitdef sum_logistic(x):return
    jnp.sum(1.0 / (1.0 + jnp.exp(-x)))`
  id: totrans-133
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在JAX中计算导数是通过`jax.grad.`完成的。`@jax.jitdef sum_logistic(x):return jnp.sum(1.0 /
    (1.0 + jnp.exp(-x)))`
- en: '`x_small = jnp.arange(6.)`'
  id: totrans-134
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x_small = jnp.arange(6.)`'
- en: '`derivative_fn = jax.grad(sum_logistic) print(derivative_fn(x_small))`'
  id: totrans-135
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`derivative_fn = jax.grad(sum_logistic) print(derivative_fn(x_small))`'
- en: The `[grad]` function has a `[has_aux]` argument that allows you to return auxiliary
    data. For example, when building machine learning models, you can use it to return
    loss and gradients.
  id: totrans-136
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[grad]`函数有一个`[has_aux]`参数，允许你返回辅助数据。例如，在构建机器学习模型时，你可以用它返回损失和梯度。'
- en: '`@jax.jitdef sum_logistic(x):return jnp.sum(1.0 / (1.0 + jnp.exp(-x))),(x +
    1)`'
  id: totrans-137
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`@jax.jitdef sum_logistic(x):return jnp.sum(1.0 / (1.0 + jnp.exp(-x))),(x +
    1)`'
- en: '`x_small = jnp.arange(6.)`'
  id: totrans-138
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x_small = jnp.arange(6.)`'
- en: '`derivative_fn = jax.grad(sum_logistic, has_aux=True) print(derivative_fn(x_small))`'
  id: totrans-139
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`derivative_fn = jax.grad(sum_logistic, has_aux=True) print(derivative_fn(x_small))`'
- en: '`(DeviceArray([0.25 , 0.19661194, 0.10499357, 0.04517666, 0.01766271,`'
  id: totrans-140
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`(DeviceArray([0.25 , 0.19661194, 0.10499357, 0.04517666, 0.01766271,`'
- en: '`0.00664806], dtype=float32), DeviceArray([1., 2.,`'
  id: totrans-141
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`0.00664806], dtype=float32), DeviceArray([1., 2.,`'
- en: '`3., 4., 5., 6.], dtype=float32))`You can perform advanced automatic differentiation
    using **`jax.vjp()`**and **`jax.jvp()`**. **## Auto-vectorization with vmap**'
  id: totrans-142
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`3., 4., 5., 6.], dtype=float32))`使用 **`jax.vjp()`** 和 **`jax.jvp()`** 可以执行高级自动微分。
    **## 使用 vmap 进行自动向量化**'
- en: vmap(Vectorizing map) allows you write a function that can be applied to a single
    data and then vmap will map it to a batch of data. Without vmap the solution would
    be to loop through the batches while applying the function. Using jit with for
    loops is a little complicated and may be slower.
  id: totrans-143
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: vmap（向量化映射）允许你编写一个可以应用于单个数据的函数，然后 vmap 将其映射到一批数据中。如果没有 vmap，则解决方案将是通过批处理循环应用函数。在使用
    jit 和循环的情况下，解决方案会更加复杂且可能会更慢。
- en: '`mat = jax.random.normal(key, (150, 100)) batched_x = jax.random.normal(key,
    (10, 100)) def apply_matrix(v):`'
  id: totrans-144
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`mat = jax.random.normal(key, (150, 100)) batched_x = jax.random.normal(key,
    (10, 100)) def apply_matrix(v):`'
- en: '`return jnp.dot(mat, v)`'
  id: totrans-145
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return jnp.dot(mat, v)`'
- en: '`@jax.jit`'
  id: totrans-146
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`@jax.jit`'
- en: '`def vmap_batched_apply_matrix(v_batched):`'
  id: totrans-147
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def vmap_batched_apply_matrix(v_batched):`'
- en: '`return jax.vmap(apply_matrix)(v_batched)`'
  id: totrans-148
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return jax.vmap(apply_matrix)(v_batched)`'
- en: '`print(''Auto-vectorized with vmap'')`'
  id: totrans-149
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`print(''使用 vmap 进行自动向量化'')`'
- en: '`%timeit vmap_batched_apply_matrix(batched_x).block_until_ready ()`'
  id: totrans-150
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`%timeit vmap_batched_apply_matrix(batched_x).block_until_ready ()`'
- en: In JAX, the `[jax.vmap]` transformation is designed to generate a vectorized
    implementation of a function automatically. It does this by tracing the function
    similarly to `[jax.jit]`, and automatically adding batch axes at the beginning
    of each input. If the batch dimension is not the first, you may use the `[in_axes]`
    and `[out_axes]` arguments to specify the location of the batch dimension in inputs
    and outputs. These may be an integer if the batch axis is the same for all inputs
    and outputs, or lists, otherwise. `Matteo Hessel, JAX author.`
  id: totrans-151
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在 JAX 中，`[jax.vmap]` 转换设计为自动生成函数的向量化实现。它通过类似于 `[jax.jit]` 的跟踪功能实现，自动在每个输入的开头添加批处理轴。如果批处理维度不是第一个，可以使用
    `[in_axes]` 和 `[out_axes]` 参数来指定输入和输出中批处理维度的位置。如果所有输入和输出的批处理轴相同，可以使用整数，否则可以使用列表。`Matteo
    Hessel，JAX 作者。`
- en: Parallelization with `[pmap]`
  id: totrans-152
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 使用 `[pmap]` 进行并行化
- en: The working of `jax.pmap` is similar to `jax.vmap`. The difference is that `jax.pmap`
    is meant for parallel execution, that is, computation on multiple devices. This
    is applicable when training a machine learning model on batches of data.
  id: totrans-153
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jax.pmap` 的工作方式类似于 `jax.vmap`。不同之处在于 `jax.pmap` 用于并行执行，即在多个设备上进行计算。这在对数据批次进行训练时是适用的。'
- en: Computation on batches can occur in different devices then the results are aggregated.
    The `[pmap]`ed function returns a `[ShardedDeviceArray]`. This is because the
    arrays are split across all the devices. There is no need to decorate the function
    with jit because the function is jitcompiled by default when using `[pmap]`.
  id: totrans-154
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 计算在不同设备上的批处理可能会发生，然后将结果聚合。被 `[pmap]` 的函数返回一个 `[ShardedDeviceArray]`。这是因为数组分布在所有设备上。不需要用
    jit 装饰函数，因为使用 `[pmap]` 时函数默认为 jit 编译。
- en: '`x = np.arange(5)w = np.array([2., 3., 4.])`'
  id: totrans-155
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = np.arange(5)w = np.array([2., 3., 4.])`'
- en: '`def convolve(x, w):`'
  id: totrans-156
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def convolve(x, w):`'
- en: '`output = []`'
  id: totrans-157
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`output = []`'
- en: '`for i in range(1, len(x)-1):`'
  id: totrans-158
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`for i in range(1, len(x)-1):`'
- en: '`output.append(jnp.dot(x[i-1:i+2], w)) return jnp.array(output)`'
  id: totrans-159
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`output.append(jnp.dot(x[i-1:i+2], w)) return jnp.array(output)`'
- en: '`convolve(x, w) n_devices = jax.local_device_count()`'
  id: totrans-160
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`convolve(x, w) n_devices = jax.local_device_count()`'
- en: '`xs = np.arange(5 * n_devices).reshape(-1, 5)`'
  id: totrans-161
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`xs = np.arange(5 * n_devices).reshape(-1, 5)`'
- en: '`ws = np.stack([w] * n_devices)`'
  id: totrans-162
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`ws = np.stack([w] * n_devices)`'
- en: '`jax.pmap(convolve)(xs, ws)`'
  id: totrans-163
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jax.pmap(convolve)(xs, ws)`'
- en: '`ShardedDeviceArray([[ 11., 20., 29.],`'
  id: totrans-164
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`ShardedDeviceArray([[ 11., 20., 29.],`'
- en: '`.................`'
  id: totrans-165
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`.................`'
- en: '`[326., 335., 344.]], dtype=float32)`'
  id: totrans-166
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`[326., 335., 344.]], dtype=float32)`'
- en: You may need to aggregate data using one of the `[collective operators]`, for
    example, to compute the mean of the accuracy or mean of the logits. In that case,
    you'll need to specify an `[axis_name]`. This name is important to achieve communication
    between devices.
  id: totrans-167
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 你可能需要使用其中一个 `[collective operators]` 来聚合数据，例如，计算准确度的平均值或 logits 的平均值。在这种情况下，你需要指定一个
    `[axis_name]`。这个名称在设备之间的通信中很重要。
- en: Debugging NANs in JAX
  id: totrans-168
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 在 JAX 中调试 NANs
- en: By default, the occurrence of NANs in JAX program will not lead to an error.
    `jnp.divide(0.0,0.0)# DeviceArray(nan, dtype=float32, weak_type=True)`
  id: totrans-169
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在 JAX 程序中，默认情况下，出现 NANs 不会导致错误。`jnp.divide(0.0,0.0)# DeviceArray(nan, dtype=float32,
    weak_type=True)`
- en: You can turn on the NAN checker and your program will error out at the occurrence
    of NANs. You should only use the NAN checker for debugging because it leads to
    performance issues. Also, it doesn't work with `[pmap]`, use `[vmap]` instead.
  id: totrans-170
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 您可以打开 NAN 检查器，程序将在出现 NAN 时报错。您应仅在调试时使用 NAN 检查器，因为它会导致性能问题。此外，它不适用于 `[pmap]`，请改用
    `[vmap]`。
- en: '`from jax.config import config`'
  id: totrans-171
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from jax.config import config`'
- en: '`config.update("jax_debug_nans", True)`'
  id: totrans-172
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`config.update("jax_debug_nans", True)`'
- en: '`jnp.divide(0.0,0.0)`'
  id: totrans-173
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jnp.divide(0.0,0.0)`'
- en: '`FloatingPointError: invalid value (nan) encountered in div`'
  id: totrans-174
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`FloatingPointError: 在除法中遇到无效值 (nan)`'
- en: '**Double (64bit) precision**'
  id: totrans-175
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**双精度（64 位）**'
- en: JAX enforces single-precision of numbers. For example, you will get a warning
    when you create a `[float64]` number. If you check the type of the number, you
    will notice that it's `[float32]`.
  id: totrans-176
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAX 强制使用单精度数。例如，创建 `[float64]` 数字时会收到警告。如果检查数字类型，会发现它是 `[float32]`。
- en: '`x = jnp.float64(1.25844)`'
  id: totrans-177
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = jnp.float64(1.25844)`'
- en: '`/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_num py.py:1806`:
    UserWarning: Explicitly requested dtype `float64` requested in array is not available,
    and will be truncated to dtype `float32`. To enable more dtypes, set the `jax_enable_x64`
    configuration option or the `JAX_ENABLE_X64` shell environment variable. See [https://github.com/google/jax#current-gotchas](https://github.com/google/jax#current-gotchas)
    for more. `# lax_internal._check_user_dtype_supported(dtype, "array")` `# DeviceArray(1.25844,
    dtype=float32)`'
  id: totrans-178
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_num py.py:1806`:
    UserWarning: 你在数组中显式请求的 `float64` 类型不可用，将被截断为 `float32` 类型。要启用更多数据类型，请设置 `jax_enable_x64`
    配置选项或 `JAX_ENABLE_X64` shell 环境变量。详情请见 [https://github.com/google/jax#current-gotchas](https://github.com/google/jax#current-gotchas)。
    `# lax_internal._check_user_dtype_supported(dtype, "array")` `# DeviceArray(1.25844,
    dtype=float32)`'
- en: You can use double-precision numbers by setting that in the configuration using
    `[jax_enable_x64]`.
  id: totrans-179
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 您可以通过在配置中设置 `[jax_enable_x64]` 来使用双精度数字。
- en: '`set this config at the begining of the program from jax.config import config`'
  id: totrans-180
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`在程序开头设置此配置 from jax.config import config`'
- en: '`config.update("jax_enable_x64", True)`'
  id: totrans-181
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`config.update("jax_enable_x64", True)`'
- en: '`x = jnp.float64(1.25844)`'
  id: totrans-182
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = jnp.float64(1.25844)`'
- en: '`x`'
  id: totrans-183
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x`'
- en: '`DeviceArray(1.25844, dtype=float64)`'
  id: totrans-184
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`DeviceArray(1.25844, dtype=float64)`'
- en: '**What is a pytree?**'
  id: totrans-185
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**什么是 pytree？**'
- en: A pytree is a container that holds Python objects. In JAX, it can hold arrays,
    tuples, lists, dictionaries, etc. A Pytree contains leaves. For example, model
    parameters in JAX are pytrees.
  id: totrans-186
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: Pytree 是一个容器，可以容纳 Python 对象。在 JAX 中，它可以包含数组、元组、列表、字典等。Pytree 包含叶子节点。例如，在 JAX
    中，模型参数就是 pytree。
- en: '`example_trees = [`'
  id: totrans-187
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`example_trees = [`'
- en: '`[1, ''a'', object()],`'
  id: totrans-188
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[1, ''a'', object()],`'
- en: '`(1, (2, 3), ())`'
  id: totrans-189
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`(1, (2, 3), ())`'
- en: '`[1, {''k1'': 2, ''k2'': (3, 4)}, 5], {''a'': 2, ''b'': (2, 3)},`'
  id: totrans-190
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[1, {''k1'': 2, ''k2'': (3, 4)}, 5], {''a'': 2, ''b'': (2, 3)},`'
- en: '`jnp.array([1, 2, 3]),`'
  id: totrans-191
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jnp.array([1, 2, 3]),`'
- en: '`]`'
  id: totrans-192
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`]`'
- en: 'Let''s see how many leaves they have:'
  id: totrans-193
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: 让我们看看它们各自有多少个叶子节点：
- en: '`for pytree in example_trees:`'
  id: totrans-194
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`for pytree in example_trees:`'
- en: '`leaves = jax.tree_leaves(pytree)`'
  id: totrans-195
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`leaves = jax.tree_leaves(pytree)`'
- en: '`print(f"{repr(pytree):<45} has {len(leaves)} leaves: {leave s}")`'
  id: totrans-196
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`print(f"{repr(pytree):<45} 有 {len(leaves)} 个叶子节点: {leave s}")`'
- en: '`[1, ''a'', <object object at 0x7f280a01f6d0>] [1, ''a'', <object object at
    0x7f280a01f6d0>]`'
  id: totrans-197
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`[1, ''a'', <object object at 0x7f280a01f6d0>] [1, ''a'', <object object at
    0x7f280a01f6d0>]`'
- en: '`(1, (2, 3), ())`'
  id: totrans-198
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`(1, (2, 3), ())`'
- en: '`[1, 2, 3]`'
  id: totrans-199
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[1, 2, 3]`'
- en: '`[1, {''k1'': 2, ''k2'': (3, 4)}, 5]`'
  id: totrans-200
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`[1, {''k1'': 2, ''k2'': (3, 4)}, 5]`'
- en: '`[1, 2, 3, 4, 5]`'
  id: totrans-201
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[1, 2, 3, 4, 5]`'
- en: '`{''a'': 2, ''b'': (2, 3)}`'
  id: totrans-202
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`{''a'': 2, ''b'': (2, 3)}`'
- en: '`[2, 2, 3]`'
  id: totrans-203
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[2, 2, 3]`'
- en: '`DeviceArray([1, 2, 3], dtype=int64) [DeviceArray([1, 2, 3], dtype=int64)]
    has 3 leaves:`'
  id: totrans-204
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`DeviceArray([1, 2, 3], dtype=int64) [DeviceArray([1, 2, 3], dtype=int64)]
    有 3 个叶子节点:`'
- en: 'has 3 leaves:has 5 leaves:has 3 leaves: has 1 leaves:'
  id: totrans-205
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '有 3 个叶子节点: 有 5 个叶子节点: 有 3 个叶子节点: 有 1 个叶子节点:'
- en: '**Handling state in JAX**'
  id: totrans-206
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**在 JAX 中处理状态**'
- en: Training machine learning models will often involve state in areas such as model
    parameters, optimizer state, and stateful Layer such as BatchNorm. However, jit-compiled
    functions must have no side effects. We, therefore, need a way to track and update
    model parameters, optimizer state, and stateful layers. The solution is to define
    the state explicitly.
  id: totrans-207
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 训练机器学习模型通常涉及状态，例如模型参数、优化器状态和类似 BatchNorm 的有状态层。然而，jit 编译的函数必须没有副作用。因此，我们需要一种方法来跟踪和更新模型参数、优化器状态和有状态层。解决方案是显式定义状态。
- en: '**Loading datasets with JAX**'
  id: totrans-208
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**使用 JAX 加载数据集**'
- en: JAX doesn't ship with any data loading tools. However, JAX recommends using
    data loaders from `PyTorch` and `TensorFlow`.
  id: totrans-209
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAX 并不附带任何数据加载工具。不过，JAX 建议使用来自 `PyTorch` 和 `TensorFlow` 的数据加载器。
- en: import `tensorflow` as `tf`
  id: totrans-210
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: import `tensorflow` as `tf`
- en: Ensure TF does not see GPU and grab all GPU memory. `tf.config.set_visible_devices([],
    device_type='GPU')`
  id: totrans-211
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: 确保 TF 看不到 GPU 并抢占所有 GPU 内存。`tf.config.set_visible_devices([], device_type='GPU')`
- en: import `tensorflow_datasets` as `tfdsdata_dir = '/tmp/tfds'`
  id: totrans-212
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: import `tensorflow_datasets` as `tfdsdata_dir = '/tmp/tfds'`
- en: Fetch full datasets for evaluation
  id: totrans-213
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: 获取完整数据集以进行评估
- en: '`tfds.load returns tf.Tensors (or tf.data.Datasets if batch_size != -1)`'
  id: totrans-214
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`tfds.load 返回 tf.Tensors（如果 batch_size != -1 则为 tf.data.Datasets）`'
- en: You can convert them to NumPy arrays (or iterables of NumPy arrays) with `tfds.dataset_as_numpy`
  id: totrans-215
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: 您可以使用 `tfds.dataset_as_numpy` 将其转换为 NumPy 数组（或 NumPy 数组的可迭代对象）。
- en: '`mnist_data, info = tfds.load(name="mnist", batch_size=-1, data_dir=data_dir,
    with_info=True)`'
  id: totrans-216
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`mnist_data, info = tfds.load(name="mnist", batch_size=-1, data_dir=data_dir,
    with_info=True)`'
- en: '`mnist_data = tfds.as_numpy(mnist_data)`'
  id: totrans-217
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`mnist_data = tfds.as_numpy(mnist_data)`'
- en: '`train_data`, `test_data` = `mnist_data[''train'']`, `mnist_data[''test'']`
    `num_labels = info.features[''label''].num_classes`'
  id: totrans-218
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_data`, `test_data` = `mnist_data[''train'']`, `mnist_data[''test'']`
    `num_labels = info.features[''label''].num_classes`'
- en: '`h, w, c = info.features[''image''].shape`'
  id: totrans-219
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`h, w, c = info.features[''image''].shape`'
- en: '`num_pixels = h * w * c`'
  id: totrans-220
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`num_pixels = h * w * c`'
- en: Full train set
  id: totrans-221
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: 完整的训练集
- en: '`train_images`, `train_labels` = `train_data[''image'']`, `train_data[''l abel'']`'
  id: totrans-222
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_images`, `train_labels` = `train_data[''image'']`, `train_data[''l abel'']`'
- en: '`train_images = jnp.reshape(train_images, (len(train_images), nu m_pixels))`'
  id: totrans-223
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_images = jnp.reshape(train_images, (len(train_images), nu m_pixels))`'
- en: '`train_labels = one_hot(train_labels, num_labels)`'
  id: totrans-224
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_labels = one_hot(train_labels, num_labels)`'
- en: Full test set
  id: totrans-225
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: 完整的测试集
- en: '`test_images`, `test_labels` = `test_data[''image'']`, `test_data[''labe l'']`'
  id: totrans-226
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`test_images`, `test_labels` = `test_data[''image'']`, `test_data[''labe l'']`'
- en: '`test_images = jnp.reshape(test_images, (len(test_images), num_p ixels))`'
  id: totrans-227
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`test_images = jnp.reshape(test_images, (len(test_images), num_p ixels))`'
- en: '`test_labels = one_hot(test_labels, num_labels)`'
  id: totrans-228
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`test_labels = one_hot(test_labels, num_labels)`'
- en: '`print(''Train:'', train_images.shape, train_labels.shape)` `print(''Test:'',
    test_images.shape, test_labels.shape)`'
  id: totrans-229
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`print(''Train:'', train_images.shape, train_labels.shape)` `print(''Test:'',
    test_images.shape, test_labels.shape)`'
- en: 'Train: `(60000, 784)` `(60000, 10)` # Test: `(10000, 784)` `(10000, 10)`'
  id: totrans-230
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: 'Train: `(60000, 784)` `(60000, 10)` # Test: `(10000, 784)` `(10000, 10)`'
- en: Building neural networks with JAX
  id: totrans-231
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 使用 JAX 构建神经网络
- en: You can build a model from scratch using JAX. However, various neural network
    libraries are built on top of JAX to make building neural networks with JAX easier.
    The Image classification with JAX & Flax article shows how to load data with PyTorch
    and build a convolutional neural network with Jax and Flax.
  id: totrans-232
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 您可以使用 JAX 从头开始构建模型。然而，各种神经网络库都是基于 JAX 构建的，以使使用 JAX 构建神经网络变得更加容易。《JAX 和 Flax
    图像分类》文章展示了如何使用 PyTorch 加载数据，并使用 JAX 和 Flax 构建卷积神经网络。
- en: Final thoughts
  id: totrans-233
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 总结思考
- en: 'In this article, we have covered the basics of JAX. We have seen that JAX uses
    XLA and just-in-time compilation to improve the performance of Python functions.
    Specifically, we have covered:'
  id: totrans-234
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了 JAX 的基础知识。我们看到 JAX 使用 XLA 和即时编译来提高 Python 函数的性能。具体来说，我们介绍了：
- en: Setting up JAX to use TPUs on Google Colab.
  id: totrans-235
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在 Google Colab 上配置 JAX 以使用 TPUs。
- en: Comparison between data types in JAX and NumPy. Creating arrays in JAX.
  id: totrans-236
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAX 和 NumPy 中数据类型的比较。在 JAX 中创建数组。
- en: How to generate random numbers in JAX.
  id: totrans-237
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 如何在 JAX 中生成随机数。
- en: Operations on JAX arrays.
  id: totrans-238
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAX 数组上的操作。
- en: Gotchas in JAX, such as using pure functions and the immutability of JAX arrays.
  id: totrans-239
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAX 中的注意事项，如使用纯函数和 JAX 数组的不可变性。
- en: Placing JAX arrays in GPUs or TPUs.
  id: totrans-240
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 将 JAX 数组放入 GPU 或 TPU。
- en: How to use JIT to speed up functions.
  id: totrans-241
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 如何使用 JIT 加速函数。
- en: '...and so much more'
  id: totrans-242
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '... 以及更多内容'
- en: Optimizers in JAX and Flax
  id: totrans-243
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: JAX 和 Flax 中的优化器
- en: Optimizers are applied when training neural networks to reduce the error between
    the true and predicted values. This optimization is done via gradient descent.
    Gradient descent adjusts errors in the network through a cost function. In JAX,
    optimizers are applied from the Optax library.
  id: totrans-244
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在训练神经网络时应用优化器以减少真实值和预测值之间的误差。这种优化通过梯度下降完成。梯度下降通过成本函数调整网络中的误差。在 JAX 中，优化器来自 Optax
    库。
- en: 'Optimizers can be classified into two broad categories:'
  id: totrans-245
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 优化器可以分为两大类：
- en: Adaptive such as Adam, Adagrad, AdaDelta, and RMSProp.
  id: totrans-246
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 包括 Adam、Adagrad、AdaDelta 和 RMSProp 在内的自适应优化器。
- en: Accelerated stochastic gradient descent (SGD), for example, SGD with momentum,
    heavy-ball method (HB), and Nesterov accelerated gradient (NAG).
  id: totrans-247
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 加速随机梯度下降（SGD），例如带动量的 SGD、重球方法（HB）和 Nesterov 加速梯度（NAG）。
- en: Let's look at common optimizer functions used in JAX and Flax.
  id: totrans-248
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 让我们来看看在 JAX 和 Flax 中常用的优化器函数。
- en: Adaptive vs stochastic gradient descent (SGD) optimizers
  id: totrans-249
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 自适应 vs 随机梯度下降（SGD）优化器
- en: When performing optimization, adaptive optimizers start with large update steps
    but reduce the step size as they get close to the global minimum. This ensures
    that they don't miss the global minimum.
  id: totrans-250
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在执行优化时，自适应优化器以较大的更新步骤开始，但在接近全局最小值时减小步长。这确保它们不会错过全局最小值。
- en: 'Adaptive optimizers such as Adam are quite common because they converge faster,
    but they may have poor generalization. `SGD-based optimizers apply a global learning
    rate on all parameters, while adaptive optimizers calculate a learning rate for
    each parameter.` `## AdaBelief` The authors of AdaBelief introduced the optimizer
    to:'
  id: totrans-251
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 自适应优化器如Adam非常常见，因为它们收敛速度快，但可能泛化能力较差。`SGD-based optimizers apply a global learning
    rate on all parameters, while adaptive optimizers calculate a learning rate for
    each parameter.` `## AdaBelief` AdaBelief的作者引入了这个优化器来：
- en: Converge fast as in adaptive methods.
  id: totrans-252
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 与自适应方法一样快速收敛。
- en: Have good generalization like SGD.
  id: totrans-253
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 具有像SGD一样的良好泛化能力。
- en: Be stable during training.
  id: totrans-254
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在训练期间保持稳定。
- en: '`AdaBelief` works on the concept of " **belief**" in the current gradient direction.
    If it results in good performance, then that direction is trusted, and large updates
    are applied. Otherwise, it''s distrusted and the step size is reduced.'
  id: totrans-255
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`AdaBelief` 基于对当前梯度方向的“**信念**”工作。如果它导致良好的性能，则信任该方向，并应用大幅更新。否则，它被不信任，步长被减小。'
- en: Let's look at a `Flax training state` that applies the `AdaBelief` optimizer.from
    `flax.training` import `train_state`
  id: totrans-256
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 让我们来看一个应用`AdaBelief`优化器的`Flax训练状态`。从`flax.training`导入`train_state`
- en: 'def `create_train_state`(rng, learning_rate):'
  id: totrans-257
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def `create_train_state`(rng, learning_rate):'
- en: '"""Creates initial `TrainState`."""'
  id: totrans-258
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""创建初始`TrainState`。"""'
- en: cnn = `CNN()``
  id: totrans-259
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: cnn = `CNN()``
- en: params = `cnn.init`(rng, `jnp.ones([1, size_image, size_image,`
  id: totrans-260
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: params = `cnn.init`(rng, `jnp.ones([1, size_image, size_image,`
- en: '`3]))[''params'']`'
  id: totrans-261
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`3]))[''params'']`'
- en: tx = `optax.adabelief`(learning_rate)
  id: totrans-262
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: tx = `optax.adabelief`(learning_rate)
- en: return `train_state.TrainState.create`(
  id: totrans-263
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 返回`train_state.TrainState.create`(
- en: apply_fn=`cnn.apply`, params=params, tx=tx)Here's the performance of `AdaBelief`
    on various tasks as provided by its authors.![](../images/00007.jpeg)`
  id: totrans-264
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: apply_fn=`cnn.apply`, params=params, tx=tx)这是`AdaBelief`在各种任务上的表现，由其作者提供！[](../images/00007.jpeg)`
- en: '**`AdaGrad`**'
  id: totrans-265
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**`AdaGrad`**'
- en: '`AdaGrad` works well in situations leading to sparse gradients. `Adagrad` is
    an algorithm for gradient-based optimization that anneals the learning rate for
    each parameter during training– `Optax`.'
  id: totrans-266
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`AdaGrad` 在导致稀疏梯度的情况下效果很好。`Adagrad`是一种基于梯度的优化算法，在训练期间为每个参数退火学习率– `Optax`.'
- en: from `flax.training` import `train_state`
  id: totrans-267
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: from `flax.training` import `train_state`
- en: 'def `create_train_state`(rng, learning_rate):'
  id: totrans-268
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def `create_train_state`(rng, learning_rate):'
- en: '"""Creates initial `TrainState`."""'
  id: totrans-269
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""创建初始`TrainState`。"""'
- en: cnn = `CNN()``
  id: totrans-270
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: cnn = `CNN()``
- en: params = `cnn.init`(rng, `jnp.ones([1, size_image, size_image,`
  id: totrans-271
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: params = `cnn.init`(rng, `jnp.ones([1, size_image, size_image,`
- en: '`3]))[''params'']`'
  id: totrans-272
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`3]))[''params'']`'
- en: tx = `optax.AdaGrad`(learning_rate)
  id: totrans-273
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: tx = `optax.AdaGrad`(learning_rate)
- en: return `train_state.TrainState.create`(
  id: totrans-274
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 返回`train_state.TrainState.create`(
- en: apply_fn=`cnn.apply`, params=params, tx=tx)
  id: totrans-275
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: apply_fn=`cnn.apply`, params=params, tx=tx)
- en: '**`Adam – Adaptive moment estimation`**'
  id: totrans-276
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**`Adam – 自适应矩估计`**'
- en: '`Adam` is a common optimizer in deep learning because it gives good results
    with default parameters, is computationally inexpensive, and uses little memory.'
  id: totrans-277
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Adam` 是深度学习中常见的优化器，因为它使用默认参数能够获得良好的结果，计算成本低，内存使用少。'
- en: from `flax.training` import `train_state`
  id: totrans-278
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: from `flax.training` import `train_state`
- en: 'def `create_train_state`(rng):'
  id: totrans-279
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def `create_train_state`(rng):'
- en: '"""Creates initial `TrainState`."""'
  id: totrans-280
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""创建初始`TrainState`。"""'
- en: model = `LSTMModel()``
  id: totrans-281
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: model = `LSTMModel()``
- en: params = `model.init`(rng, `jnp.array(X_train_padded[0]))['param`
  id: totrans-282
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: params = `model.init`(rng, `jnp.array(X_train_padded[0]))['param`
- en: '`s'']`'
  id: totrans-283
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`s'']`'
- en: tx = `optax.adam`(0.001,0.9,0.999,1e-07)
  id: totrans-284
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: tx = `optax.adam`(0.001,0.9,0.999,1e-07)
- en: return `train_state.TrainState.create`(
  id: totrans-285
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 返回`train_state.TrainState.create`(
- en: apply_fn=`model.apply`, params=params, tx=tx)![](../images/00008.jpeg) **`Training
    of multilayer neural networks on MNIST images. (a) Neural networks using dropout
    stochastic regularization. (b) Neural networks with deterministic cost function`**  **##
    `AdamW`**
  id: totrans-286
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: apply_fn=`model.apply`, params=params, tx=tx)![](../images/00008.jpeg) **`在MNIST图像上训练多层神经网络。（a）使用dropout随机正则化的神经网络。（b）具有确定性成本函数的神经网络`**  **##
    `AdamW`**
- en: '`AdamW` is `Adam` with weight decay regularization. Weight decay regularization
    penalizes the cost function making the weights smaller during backpropagarion.
    It results in small weights that lead to better generalization. In some cases,
    `Adam` with decoupled weight decay leads to better results compared `Adam` with
    `L2 regularization`.'
  id: totrans-287
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`AdamW是带有权重衰减正则化的Adam。权重衰减正则化通过惩罚成本函数在反向传播过程中使权重变小。这导致权重变小，从而导致更好的泛化。在某些情况下，与L2正则化的Adam相比，使用解耦权重衰减的Adam会导致更好的结果。`'
- en: from `flax.training` import `train_state`
  id: totrans-288
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: from `flax.training` import `train_state`
- en: 'def `create_train_state`(rng):'
  id: totrans-289
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def `create_train_state`(rng):`'
- en: '"""Creates initial `TrainState`."""'
  id: totrans-290
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""创建初始`TrainState`。"""'
- en: model = `LSTMModel()``
  id: totrans-291
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: model = `LSTMModel()``
- en: params = `model.init`(rng, `jnp.array(X_train_padded[0]))['param`
  id: totrans-292
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: params = `model.init`(rng, `jnp.array(X_train_padded[0]))['param`
- en: '`s'']tx = optax.adamw(0.001,0.9,0.999,1e-07)return train_state.TrainState.create(apply_fn=model.apply,
    params=params, tx=tx)![](../images/00009.jpeg) **`Learning curves (top row) and
    generalization results (bottom row) obtained by a 26 2x96d ResNet trained with
    Adam and AdamW on CIFAR-10`**  **## `RAdam – Rectified Adam optimizer`** `RAdam` aims
    to solve large variances during the early training stages when applying an adaptive
    learning rate.from `flax.training` import `train_state`'
  id: totrans-293
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`s'']tx = optax.adamw(0.001,0.9,0.999,1e-07)return train_state.TrainState.create(apply_fn=model.apply,
    params=params, tx=tx)![](../images/00009.jpeg) **`学习曲线（顶部行）和在CIFAR-10上使用Adam和AdamW训练的26
    2x96d ResNet的泛化结果（底部行）`**  **## `RAdam – 矫正的Adam优化器`** `RAdam旨在解决应用自适应学习率时在早期训练阶段出现的大方差。from
    `flax.training` import `train_state`'
- en: '`def create_train_state(rng, learning_rate):`'
  id: totrans-294
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def create_train_state(rng, learning_rate):`'
- en: '"""Creates initial `TrainState`."""'
  id: totrans-295
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""创建初始`TrainState`。"""'
- en: '`cnn = CNN()`'
  id: totrans-296
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`cnn = CNN()`'
- en: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`'
  id: totrans-297
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`'
- en: '`3]))[''params'']`'
  id: totrans-298
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`3]))[''params'']`'
- en: '`tx = optax.radam(learning_rate)`'
  id: totrans-299
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`tx = optax.radam(learning_rate)`'
- en: '`return train_state.TrainState.create(`'
  id: totrans-300
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return train_state.TrainState.create(`'
- en: '`apply_fn=cnn.apply, params=params, tx=tx)`'
  id: totrans-301
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`apply_fn=cnn.apply, params=params, tx=tx)`'
- en: '**AdaFactor**'
  id: totrans-302
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**AdaFactor**'
- en: '`AdaFactor is used for training large neural networks because it is implemented
    to reduce memory utilization.from flax.training import train_state`'
  id: totrans-303
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`AdaFactor用于训练大型神经网络，因为它实现了减少内存使用。from flax.training import train_state`'
- en: '`def create_train_state(rng, learning_rate):`'
  id: totrans-304
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def create_train_state(rng, learning_rate):`'
- en: '"""Creates initial `TrainState`."""'
  id: totrans-305
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""创建初始`TrainState`。"""'
- en: '`cnn = CNN()`'
  id: totrans-306
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`cnn = CNN()`'
- en: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`'
  id: totrans-307
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`'
- en: '`3]))[''params'']`'
  id: totrans-308
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`3]))[''params'']`'
- en: '`tx = optax.adafactor(learning_rate)`'
  id: totrans-309
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`tx = optax.adafactor(learning_rate)`'
- en: '`return train_state.TrainState.create(`'
  id: totrans-310
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return train_state.TrainState.create(`'
- en: '`apply_fn=cnn.apply, params=params, tx=tx)`'
  id: totrans-311
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`apply_fn=cnn.apply, params=params, tx=tx)`'
- en: '**Fromage**'
  id: totrans-312
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**Fromage**'
- en: '`Fromage introduces a distance function on deep neural networks called* **deep
    relative trust**. *It requires little to no learning rate tuning.from flax.training
    import train_state`'
  id: totrans-313
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Fromage引入了一种称为“深度相对信任”的深度神经网络上的距离函数。它几乎不需要学习率调整。from flax.training import
    train_state`'
- en: '`def create_train_state(rng, learning_rate): """Creates initial `TrainState`."""
    cnn = CNN()`'
  id: totrans-314
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def create_train_state(rng, learning_rate): """创建初始`TrainState`。""" cnn =
    CNN()`'
- en: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`'
  id: totrans-315
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`'
- en: '`3]))[''params'']`'
  id: totrans-316
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`3]))[''params'']`'
- en: '`tx = optax.fromage(learning_rate)`'
  id: totrans-317
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`tx = optax.fromage(learning_rate)`'
- en: '`return train_state.TrainState.create(`'
  id: totrans-318
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return train_state.TrainState.create(`'
- en: '`apply_fn=cnn.apply, params=params, tx=tx)`'
  id: totrans-319
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`apply_fn=cnn.apply, params=params, tx=tx)`'
- en: '**Lamb – Layerwise adaptive large batch optimization**'
  id: totrans-320
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**Lamb – 层自适应大批量优化**'
- en: '`Lamb aims to enable the training of deep neural networks by computing gradients
    using large mini-batches. It leads to good performance on attention-based models
    such as Transformers and ResNet-50.`'
  id: totrans-321
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Lamb旨在通过使用大型小批量计算梯度来训练深度神经网络。它在基于注意力的模型（如Transformers和ResNet-50）上表现良好。`'
- en: '`from flax.training import train_state`'
  id: totrans-322
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from flax.training import train_state`'
- en: '`def create_train_state(rng, learning_rate):`'
  id: totrans-323
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def create_train_state(rng, learning_rate):`'
- en: '"""Creates initial `TrainState`."""'
  id: totrans-324
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""创建初始`TrainState`。"""'
- en: '`cnn = CNN()`'
  id: totrans-325
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`cnn = CNN()`'
- en: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`'
  id: totrans-326
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`'
- en: '`3]))[''params'']`'
  id: totrans-327
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`3]))[''params'']`'
- en: '`tx = optax.lamb(learning_rate)`'
  id: totrans-328
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`tx = optax.lamb(learning_rate)`'
- en: '`return train_state.TrainState.create(`'
  id: totrans-329
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return train_state.TrainState.create(`'
- en: '`apply_fn=cnn.apply, params=params, tx=tx)`'
  id: totrans-330
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`apply_fn=cnn.apply, params=params, tx=tx)`'
- en: '**Lars – Layer-wise Adaptive Rate Scaling**'
  id: totrans-331
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**Lars – 层自适应速率缩放**'
- en: '`Lars is inspired by Lamb to scale SGD to large batch sizes. Lars has been
    used to train AlexNet with an 8K batch size and Resnet-50 with a 32K batch size
    without degrading the accuracy. from flax.training import train_state`'
  id: totrans-332
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Lars is inspired by Lamb to scale SGD to large batch sizes. Lars has been
    used to train AlexNet with an 8K batch size and Resnet-50 with a 32K batch size
    without degrading the accuracy. from flax.training import train_state`'
- en: '`def create_train_state(rng, learning_rate):`'
  id: totrans-333
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def create_train_state(rng, learning_rate):`'
- en: '"""Creates initial `TrainState`."""'
  id: totrans-334
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""Creates initial `TrainState`."""'
- en: '`cnn = CNN()`'
  id: totrans-335
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`cnn = CNN()`'
- en: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`'
  id: totrans-336
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`'
- en: '`3]))[''params'']`'
  id: totrans-337
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`3]))[''params'']`'
- en: '`tx = optax.lars(learning_rate)`'
  id: totrans-338
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`tx = optax.lars(learning_rate)`'
- en: '`return train_state.TrainState.create(`'
  id: totrans-339
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return train_state.TrainState.create(`'
- en: '`apply_fn=cnn.apply, params=params, tx=tx)![](../images/00010.jpeg)`**LARS:
    Alexnet-BN with B=8K ** **## SM3 - Square-root of Minima of Sums of Maxima of
    Squaredgradients Method**'
  id: totrans-340
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`apply_fn=cnn.apply, params=params, tx=tx)![](../images/00010.jpeg)`**LARS:
    Alexnet-BN with B=8K ** **## SM3 - Square-root of Minima of Sums of Maxima of
    Squaredgradients Method**'
- en: '`SM3 was designed to reduce memory utilization when training very large models
    such as Transformer for machine translation, BERT for language modeling, and AmoebaNet-D
    for image classification`'
  id: totrans-341
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`SM3 was designed to reduce memory utilization when training very large models
    such as Transformer for machine translation, BERT for language modeling, and AmoebaNet-D
    for image classification`'
- en: '`from flax.training import train_state`'
  id: totrans-342
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from flax.training import train_state`'
- en: '`def create_train_state(rng, learning_rate):`'
  id: totrans-343
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def create_train_state(rng, learning_rate):`'
- en: '"""Creates initial `TrainState`."""'
  id: totrans-344
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""Creates initial `TrainState`."""'
- en: '`cnn = CNN()`'
  id: totrans-345
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`cnn = CNN()`'
- en: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`'
  id: totrans-346
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`'
- en: '`3]))[''params'']`'
  id: totrans-347
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`3]))[''params'']`'
- en: '`tx = optax.sm3(learning_rate)`'
  id: totrans-348
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`tx = optax.sm3(learning_rate)`'
- en: '`return train_state.TrainState.create(`'
  id: totrans-349
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return train_state.TrainState.create(`'
- en: apply_fn=cnn.apply, params=params, tx=tx)![](../images/00011.jpeg)**Top-1 (left)
    and Top-5 (right) test accuracy of AmoebaNet-D on ImageNet ** **## SGD– Stochastic
    Gradient Descent**
  id: totrans-350
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: apply_fn=cnn.apply, params=params, tx=tx)![](../images/00011.jpeg)**Top-1 (left)
    and Top-5 (right) test accuracy of AmoebaNet-D on ImageNet ** **## SGD– Stochastic
    Gradient Descent**
- en: SDG implements stochastic gradient descent with support for momentum and Nesterov
    acceleration. Momentum makes obtaining optimal model weights faster by accelerating
    gradient descent in a certain direction.
  id: totrans-351
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: SDG implements stochastic gradient descent with support for momentum and Nesterov
    acceleration. Momentum makes obtaining optimal model weights faster by accelerating
    gradient descent in a certain direction.
- en: from flax.training import train_state
  id: totrans-352
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: from flax.training import train_state
- en: 'def create_train_state(rng, learning_rate):'
  id: totrans-353
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def create_train_state(rng, learning_rate):'
- en: '"""Creates initial `TrainState`."""'
  id: totrans-354
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""Creates initial `TrainState`."""'
- en: cnn = CNN()
  id: totrans-355
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: cnn = CNN()
- en: params = cnn.init(rng, jnp.ones([1, size_image, size_image,
  id: totrans-356
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: params = cnn.init(rng, jnp.ones([1, size_image, size_image,
- en: 3]))['params']
  id: totrans-357
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 3]))['params']
- en: tx = optax.sgd(learning_rate)
  id: totrans-358
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: tx = optax.sgd(learning_rate)
- en: return train_state.TrainState.create(
  id: totrans-359
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: return train_state.TrainState.create(
- en: apply_fn=cnn.apply, params=params, tx=tx)
  id: totrans-360
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: apply_fn=cnn.apply, params=params, tx=tx)
- en: '**Noisy SGD**'
  id: totrans-361
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**Noisy SGD**'
- en: Noisy SGD is SGD with added noise. Adding noise to gradients can prevent overfitting
    and improve training error and generalization in deep architectures.
  id: totrans-362
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: Noisy SGD is SGD with added noise. Adding noise to gradients can prevent overfitting
    and improve training error and generalization in deep architectures.
- en: from flax.training import train_state
  id: totrans-363
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: from flax.training import train_state
- en: 'def create_train_state(rng, learning_rate):'
  id: totrans-364
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def create_train_state(rng, learning_rate):'
- en: '"""Creates initial `TrainState`."""'
  id: totrans-365
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""Creates initial `TrainState`."""'
- en: cnn = CNN()
  id: totrans-366
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: cnn = CNN()
- en: params = cnn.init(rng, jnp.ones([1, size_image, size_image,
  id: totrans-367
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: params = cnn.init(rng, jnp.ones([1, size_image, size_image,
- en: 3]))['params']
  id: totrans-368
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 3]))['params']
- en: tx = optax.noisy_sgd(learning_rate)
  id: totrans-369
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: tx = optax.noisy_sgd(learning_rate)
- en: return train_state.TrainState.create(
  id: totrans-370
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: return train_state.TrainState.create(
- en: 'apply_fn=cnn.apply, params=params, tx=tx)![](../images/00012.jpeg) **: Noise
    vs. No Noise in our experiment with tables containing 5 columns. The models trained
    with noise generalizes almost always better**  **## Optimistic GD** An Optimistic
    Gradient Descent optimizer.'
  id: totrans-371
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'apply_fn=cnn.apply, params=params, tx=tx)![](../images/00012.jpeg) **: Noise
    vs. No Noise in our experiment with tables containing 5 columns. The models trained
    with noise generalizes almost always better**  **## Optimistic GD** An Optimistic
    Gradient Descent optimizer.'
- en: '*Optimistic gradient descent is an approximation of extra-gradient methods
    which require multiple gradient calls to compute the next update. It has strong
    formal guarantees for last-iterate convergence in min-max games, for which standard
    gradient descent can oscillate or even diverge– Optax.*'
  id: totrans-372
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '*Optimistic gradient descent is an approximation of extra-gradient methods
    which require multiple gradient calls to compute the next update. It has strong
    formal guarantees for last-iterate convergence in min-max games, for which standard
    gradient descent can oscillate or even diverge– Optax.*'
- en: from flax.training import train_state
  id: totrans-373
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: from flax.training import train_state
- en: 'def create_train_state(rng, learning_rate):'
  id: totrans-374
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def create_train_state(rng, learning_rate):'
- en: '"""Creates initial `TrainState`."""'
  id: totrans-375
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""Creates initial `TrainState`."""'
- en: cnn = CNN()
  id: totrans-376
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: cnn = CNN()
- en: params = cnn.init(rng, jnp.ones([1, size_image, size_image, tx = optax.optimistic_gradient_descent(learning_rate)
    return train_state.TrainState.create(
  id: totrans-377
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image, tx = optax.optimistic_gradient_descent(learning_rate)
    return train_state.TrainState.create(`'
- en: apply_fn=cnn.apply, params=params, tx=tx)
  id: totrans-378
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: apply_fn=cnn.apply, params=params, tx=tx)
- en: '**Differentially Private SGD**'
  id: totrans-379
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**差分隐私SGD**'
- en: Differentially Private SGD is used for training networks with sensitive data.
    It ensures that the models don't expose sensitive training data.
  id: totrans-380
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 差分隐私SGD用于训练具有敏感数据的网络。确保模型不会泄露敏感的训练数据。
- en: from flax.training import train_state
  id: totrans-381
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from flax.training import train_state`'
- en: 'def create_train_state(rng, learning_rate):'
  id: totrans-382
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def create_train_state(rng, learning_rate):`'
- en: '"""Creates initial `TrainState`."""'
  id: totrans-383
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""创建初始`TrainState`。"""'
- en: cnn = CNN()
  id: totrans-384
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: cnn = CNN()
- en: params = cnn.init(rng, jnp.ones([1, size_image, size_image,
  id: totrans-385
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`'
- en: 3]))['params']
  id: totrans-386
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 3]))['params']
- en: tx = optax.dpsgd(learning_rate)
  id: totrans-387
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`tx = optax.dpsgd(learning_rate)`'
- en: return train_state.TrainState.create(
  id: totrans-388
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 返回train_state.TrainState.create(
- en: apply_fn=cnn.apply, params=params, tx=tx)
  id: totrans-389
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: apply_fn=cnn.apply, params=params, tx=tx)
- en: '**RMSProp**'
  id: totrans-390
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**RMSProp**'
- en: RMSProp works by dividing the gradient of a running average of its recent magnitude–Hinton.from
    flax.training import train_state
  id: totrans-391
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: RMSProp通过将其最近幅度的运行平均值的梯度分割来工作 - Hinton.来自flax.training import train_state
- en: 'def create_train_state(rng, learning_rate):'
  id: totrans-392
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def create_train_state(rng, learning_rate):`'
- en: '"""Creates initial `TrainState`."""'
  id: totrans-393
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""创建初始`TrainState`。"""'
- en: cnn = CNN()
  id: totrans-394
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: cnn = CNN()
- en: params = cnn.init(rng, jnp.ones([1, size_image, size_image, tx = optax.rmsprop(learning_rate)
  id: totrans-395
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image, tx = optax.rmsprop(learning_rate)`'
- en: return train_state.TrainState.create(
  id: totrans-396
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_state.TrainState.create(`的返回'
- en: apply_fn=cnn.apply, params=params, tx=tx)
  id: totrans-397
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: apply_fn=cnn.apply, params=params, tx=tx)
- en: '**Yogi**'
  id: totrans-398
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**Yogi**'
- en: '`Yogi is a modified Adam optimizer for optimizing the stochastic nonconvex
    optimization problem.from flax.training import train_state`'
  id: totrans-399
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Yogi是用于优化随机非凸优化问题的修改版Adam优化器。来自flax.training import train_state`'
- en: '`def create_train_state(rng, learning_rate):`'
  id: totrans-400
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def create_train_state(rng, learning_rate):`'
- en: '"""Creates initial `TrainState`."""'
  id: totrans-401
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""创建初始`TrainState`。"""'
- en: '`cnn = CNN()`'
  id: totrans-402
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`cnn = CNN()`'
- en: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`'
  id: totrans-403
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`'
- en: '`3]))[''params'']`'
  id: totrans-404
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`3]))[''params'']`'
- en: '`tx = optax.yogi(learning_rate)`'
  id: totrans-405
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`tx = optax.yogi(learning_rate)`'
- en: '`return train_state.TrainState.create(`'
  id: totrans-406
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return train_state.TrainState.create(`'
- en: '`apply_fn=cnn.apply, params=params, tx=tx)![](../images/00013.jpeg)` **Comparison
    of highly tuned RMSProp optimizer with YOGI for Inception-Resnet-v2 on Imagenet.
    First plot shows the mini-batch estimate of the loss during training, while the
    remaining two plots show top-1 and top-5 error rates on the held-out Imagenet
    validation set**  **## Final thoughts**'
  id: totrans-407
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`apply_fn=cnn.apply, params=params, tx=tx)![](../images/00013.jpeg)` **比较高度调整的RMSProp优化器与YOGI在Imagenet上的Inception-Resnet-v2。第一张图显示了训练期间损失的小批量估计，而其余两张图显示了在保留的Imagenet验证集上的top-1和top-5错误率**  **##
    最后的思考**'
- en: 'Choosing the right optimizer function determines how long training a network
    will take. It also determines how well the model performs. Choosing the appropriate
    optimizer functions is therefore paramount. This article discusses various optimizer
    functions that you can apply to your JAX and Flax networks. In particular, you
    walk away with nuggets about these optimizers:'
  id: totrans-408
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 选择正确的优化器函数决定了训练网络所需的时间。它还决定了模型的表现如何。因此，在构建JAX和Flax网络时选择适当的优化器函数至关重要。本文讨论了可以应用于您的网络的各种优化器函数。特别是，您将了解以下优化器的精华：
- en: Adam optimizer in JAX.
  id: totrans-409
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAX中的Adam优化器。
- en: RMSProp optimizer in Flax.
  id: totrans-410
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: Flax中的RMSProp优化器。
- en: Stochastic Gradient Descent in JAX.
  id: totrans-411
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAX中的随机梯度下降。
- en: '`..to mention a few.`'
  id: totrans-412
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`..提及一些。`'
- en: '**JAX loss functions**'
  id: totrans-413
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**JAX损失函数**'
- en: Loss functions are at the core of training machine learning. They can be used
    to identify how well the model is performing on a dataset. Poor performance leads
    to a very high loss, while a well-performing model will have a lower loss. Therefore,
    the choice of a loss function is an important one when building machine learning
    models.
  id: totrans-414
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 损失函数是训练机器学习的核心。它们可以用来识别模型在数据集上的表现如何。表现不佳会导致非常高的损失，而表现良好的模型将具有较低的损失。因此，在构建机器学习模型时，选择损失函数是一个重要的决定。
- en: In this article, we'll look at the loss functions available in JAX and how you
    can use them.
  id: totrans-415
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将看看JAX中可用的损失函数以及如何使用它们。
- en: '**What is a loss function?**'
  id: totrans-416
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**什么是损失函数？**'
- en: Machine learning models learn by evaluating predictions against true values
    and adjusting the weights. The objective is to obtain the weights that minimize
    the **loss function**, that is the **error**. The loss function is also referred
    to as the **cost function**. The choice of a loss function depends on the problem.
    The two most common problems are classification and regression problems. Each
    will require a different set of loss functions.
  id: totrans-417
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 机器学习模型通过评估预测与真实值的差异并调整权重来学习。目标是获得能够最小化**损失函数**（即**误差**）的权重。损失函数也称为**成本函数**。选择损失函数取决于问题的性质。最常见的问题是分类和回归问题。每个问题需要不同的损失函数。
- en: '**Creating custom loss functions in JAX**'
  id: totrans-418
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**在JAX中创建自定义损失函数**'
- en: When training networks with JAX, you'll need to obtain the logits at the training
    stage. These logits are used for computing the loss. You'll then need to evaluate
    the loss function and its gradient. The gradient is used to update the model parameters.
    At this point, you can compute the training metrics for the model.
  id: totrans-419
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在训练JAX网络时，您需要在训练阶段获取logits。这些logits用于计算损失。然后需要评估损失函数及其梯度。梯度用于更新模型参数。此时，您可以为模型计算训练指标。
- en: 💡 **What are logits?** Logits are unnormalized log probabilities.*
  id: totrans-420
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 💡 **Logits 是什么？** Logits 是未归一化的对数概率。*
- en: '`def compute_loss(params,images,labels):`'
  id: totrans-421
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def compute_loss(params,images,labels):`'
- en: '`logits = CNN().apply({''params'': params}, images) loss = cross_entropy_loss(logits=logits,
    labels=labels) return loss, logits`'
  id: totrans-422
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`logits = CNN().apply({''params'': params}, images) loss = cross_entropy_loss(logits=logits,
    labels=labels) return loss, logits`'
- en: '`@jax.jitdef train_step(state,images, labels): """Train for a single step."""
    (_, logits), grads = jax.value_and_grad(compute_loss, has_aux`'
  id: totrans-423
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`@jax.jitdef train_step(state,images, labels): """单步训练。""" (_, logits), grads
    = jax.value_and_grad(compute_loss, has_aux`'
- en: '`=True)(state.params,images,labels)`'
  id: totrans-424
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`=True)(state.params,images,labels)`'
- en: '`state = state.apply_gradients(grads=grads)`'
  id: totrans-425
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`state = state.apply_gradients(grads=grads)`'
- en: metrics = compute_metrics(logits=logits, labels=labels) return state, metrics
  id: totrans-426
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`metrics = compute_metrics(logits=logits, labels=labels) return state, metrics`'
- en: You can use JAX functions such as `[log_sigmoid]` and `[log_softmax]` to build
    custom loss functions. You can even write your loss functions from scratch without
    using these functions.
  id: totrans-427
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 您可以使用JAX函数如`[log_sigmoid]`和`[log_softmax]`来构建自定义损失函数。甚至可以不使用这些函数从头开始编写您自己的损失函数。
- en: Here is an example of computing the sigmoid binary cross entropy loss.
  id: totrans-428
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 这是计算Sigmoid二元交叉熵损失的一个例子。
- en: '`import jax`'
  id: totrans-429
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import jax`'
- en: '`def custom_sigmoid_binary_cross_entropy(logits, labels): log_p = jax.nn.log_sigmoid(logits)`'
  id: totrans-430
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def custom_sigmoid_binary_cross_entropy(logits, labels): log_p = jax.nn.log_sigmoid(logits)`'
- en: '`log_not_p = jax.nn.log_sigmoid(-logits)`'
  id: totrans-431
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`log_not_p = jax.nn.log_sigmoid(-logits)`'
- en: '`return -labels * log_p - (1\. - labels) * log_not_p`'
  id: totrans-432
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return -labels * log_p - (1\. - labels) * log_not_p`'
- en: '`custom_sigmoid_binary_cross_entropy(0.5,0.0) # DeviceArray(0.974077, dtype=float32,
    weak_type=True)`'
  id: totrans-433
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`custom_sigmoid_binary_cross_entropy(0.5,0.0) # DeviceArray(0.974077, dtype=float32,
    weak_type=True)`'
- en: '**Which loss functions are available in JAX?**'
  id: totrans-434
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**JAX中有哪些损失函数可用？**'
- en: Building custom loss functions for your networks can introduce errors in your
    program. Furthermore, you have to take the burden of maintaining these functions.
    However, if the loss function you want is unavailable, there is a strong case
    for creating a custom loss function. Be that as it may, there is no need to reinvent
    the wheel and rewrite the already implemented loss functions.
  id: totrans-435
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 为您的网络构建自定义损失函数可能会在程序中引入错误。此外，您必须承担维护这些函数的负担。然而，如果您需要的损失函数不可用，则有理由创建自定义损失函数。尽管如此，没有必要重新发明轮子并重写已经实现的损失函数。
- en: JAX doesn't ship with any loss functions. In JAX, we use `optax` for defining
    loss functions. It's important to ensure that you use JAXcompatible libraries
    to take advantage of functions such as `[JIT]`, `[vmap]` and `[pmap]` that make
    your programs faster.
  id: totrans-436
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAX不提供任何损失函数。在JAX中，我们使用 `optax` 来定义损失函数。确保您使用与JAX兼容的库以利用诸如 `[JIT]`、 `[vmap]` 和 `[pmap]` 等函数，这些函数能够加快程序运行速度。
- en: Let's take a look at some of the loss functions available in `optax`.
  id: totrans-437
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 让我们看看在`optax`中可用的一些损失函数。
- en: '**Sigmoid binary cross entropy**'
  id: totrans-438
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**Sigmoid 二元交叉熵**'
- en: The sigmoid binary cross entropy loss is computed
  id: totrans-439
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 计算Sigmoid二元交叉熵损失
- en: using `optax.sigmoid_binary_cross_entropy`. The function expects
  id: totrans-440
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 使用 `optax.sigmoid_binary_cross_entropy`。该函数期望
- en: '`logits` and `class labels`. It is used in problems where the classes are not
    mutually exclusive. For example, the model can predict that the image contains
    two objects in an image classification problem.'
  id: totrans-441
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`logits` 和 `class labels`。它用于类别不互斥的问题。例如，模型可以预测图像中包含两个对象的图像分类问题。'
- en: '`optax.sigmoid_binary_cross_entropy(0.5,0.0)# DeviceArray(0.974077, dtype=float32,
    weak_type=True)`'
  id: totrans-442
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`optax.sigmoid_binary_cross_entropy(0.5,0.0)# DeviceArray(0.974077, dtype=float32,
    weak_type=True)`'
- en: '**Softmax cross entropy**'
  id: totrans-443
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**Softmax 交叉熵**'
- en: The softmax cross entropy function is used where the classes are mutually exclusive.
    For example, in the MNIST dataset, each digit has exactly one label. The function
    expects an array of logits and probability distributions. The probability distribution
    sum to 1.
  id: totrans-444
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: Softmax 交叉熵函数用于类别互斥的问题。例如，在 MNIST 数据集中，每个数字正好有一个标签。该函数期望一个 logits 数组和概率分布。概率分布总和为1。
- en: '`logits = jnp.array([0.50,0.60,0.70,0.30,0.25]) labels = jnp.array([0.20,0.30,0.10,0.20,0.2])
    optax.softmax_cross_entropy(logits,labels) # DeviceArray(1.6341426, dtype=float32)`'
  id: totrans-445
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`logits = jnp.array([0.50,0.60,0.70,0.30,0.25]) labels = jnp.array([0.20,0.30,0.10,0.20,0.2])
    optax.softmax_cross_entropy(logits,labels) # DeviceArray(1.6341426, dtype=float32)`'
- en: '**Cosine distance**'
  id: totrans-446
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**余弦距离**'
- en: The cosine distance measures the cosine distance between targets and predictions.
  id: totrans-447
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 余弦距离衡量目标与预测之间的余弦距离。
- en: '`predictions = jnp.array([0.50,0.60,0.70,0.30,0.25]) targets = jnp.array([0.20,0.30,0.10,0.20,0.2])
    optax.cosine_distance(predictions,targets,epsilon=0.7) # DeviceArray(0.4128204,
    dtype=float32)`'
  id: totrans-448
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`predictions = jnp.array([0.50,0.60,0.70,0.30,0.25]) targets = jnp.array([0.20,0.30,0.10,0.20,0.2])
    optax.cosine_distance(predictions,targets,epsilon=0.7) # DeviceArray(0.4128204,
    dtype=float32)`'
- en: '**Cosine similarity**'
  id: totrans-449
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**余弦相似度**'
- en: The cosine similarity loss measures the cosine similarity between the true and
    predicted values. The cosine similarity is the cosine of the angle between two
    vectors. This is obtained by the dot product of the vectors divided by the product
    of their lengths.
  id: totrans-450
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 余弦相似度损失函数衡量真实值和预测值之间的余弦相似度。余弦相似度是两个向量之间角度的余弦。通过向量的点积除以它们长度的乘积得到。
- en: The result is a number between -1 and 1\. 0 shows orthogonality, while numbers
    closer to -1 indicate similarity. Numbers close to  1 portray high dissimilarity.
  id: totrans-451
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 结果是-1到1之间的数。0表示正交，接近-1表示相似度高。接近1表示高度不相似。
- en: '`predictions = jnp.array([0.50,0.60,0.70,0.30,0.25]) targets = jnp.array([0.20,0.30,0.10,0.20,0.2])
    optax.cosine_similarity(predictions,targets,epsilon=0.5) # DeviceArray(0.8220514,
    dtype=float32)`'
  id: totrans-452
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`predictions = jnp.array([0.50,0.60,0.70,0.30,0.25]) targets = jnp.array([0.20,0.30,0.10,0.20,0.2])
    optax.cosine_similarity(predictions,targets,epsilon=0.5) # DeviceArray(0.8220514,
    dtype=float32)`'
- en: '**`Huber loss`**'
  id: totrans-453
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**Huber 损失**'
- en: The Huber loss is used for regression problems. It is less sensitive to outliers
    compared to the squared error loss. A variant of the Huber loss that can be used
    in classification problems exists.
  id: totrans-454
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: Huber 损失函数用于回归问题。与平方误差损失相比，它对异常值不太敏感。存在可用于分类问题的 Huber 损失函数的变体。
- en: '`logits = jnp.array([0.50,0.60,0.70,0.30,0.25])`'
  id: totrans-455
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`logits = jnp.array([0.50,0.60,0.70,0.30,0.25])`'
- en: '`labels = jnp.array([0.20,0.30,0.10,0.20,0.2])`'
  id: totrans-456
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`labels = jnp.array([0.20,0.30,0.10,0.20,0.2])`'
- en: '`optax.huber_loss(logits,labels)`'
  id: totrans-457
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`optax.huber_loss(logits,labels)`'
- en: '`DeviceArray([0.045 , 0.045 , 0.17999998, 0.005 ,`'
  id: totrans-458
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`DeviceArray([0.045 , 0.045 , 0.17999998, 0.005 ,`'
- en: '`0.00125 ], dtype=float32)`'
  id: totrans-459
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`0.00125 ], dtype=float32)`'
- en: '**`l2 loss`**'
  id: totrans-460
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**l2 损失**'
- en: L2 loss function is the Least Square Errors. The L2 loss aims at minimizing
    the sum of the squared differences between the true and predicted values. The
    Mean Squared Error is the mean of all L2 loss values.
  id: totrans-461
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: L2 损失函数是最小二乘误差。L2 损失旨在最小化真实值与预测值之间的平方差的总和。均方误差是所有 L2 损失值的平均值。
- en: '`predictions = jnp.array([0.50,0.60,0.70,0.30,0.25]) targets = jnp.array([0.20,0.30,0.10,0.20,0.2])`'
  id: totrans-462
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`predictions = jnp.array([0.50,0.60,0.70,0.30,0.25]) targets = jnp.array([0.20,0.30,0.10,0.20,0.2])`'
- en: '`optax.l2_loss(predictions,targets)`'
  id: totrans-463
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`optax.l2_loss(predictions,targets)`'
- en: '`DeviceArray([0.045 , 0.045 , 0.17999998, 0.005 ,`'
  id: totrans-464
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`DeviceArray([0.045 , 0.045 , 0.17999998, 0.005 ,`'
- en: '`0.00125 ], dtype=float32)`'
  id: totrans-465
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`0.00125 ], dtype=float32)`'
- en: '**`log cosh`**'
  id: totrans-466
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**`log cosh`**'
- en: '`[`log_cosh`]` is the logarithm of the hyperbolic cosine of the prediction
    error.'
  id: totrans-467
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[`log_cosh`]` 是预测误差的双曲余弦的对数。'
- en: 💡 `log(cosh(x))` is approximately equal to `(x ** 2) / 2` for small `x` and
    to `abs(x) - log(2)` for large `x`. This means that 'logcosh' works mostly like
    the mean squared error, but will not be so strongly affected by the occasional
    wildly incorrect prediction. TensorFlow Docs*
  id: totrans-468
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 💡 `log(cosh(x))` is approximately equal to `(x ** 2) / 2` for small `x` and
    to `abs(x) - log(2)` for large `x`. This means that 'logcosh' works mostly like
    the mean squared error, but will not be so strongly affected by the occasional
    wildly incorrect prediction. TensorFlow Docs*
- en: '`predictions = jnp.array([0.50,0.60,0.70,0.30,0.25]) targets = jnp.array([0.20,0.30,0.10,0.20,0.2])`'
  id: totrans-469
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`predictions = jnp.array([0.50,0.60,0.70,0.30,0.25]) targets = jnp.array([0.20,0.30,0.10,0.20,0.2])`'
- en: '`optax.log_cosh(predictions,targets)`'
  id: totrans-470
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`optax.log_cosh(predictions,targets)`'
- en: '`DeviceArray([0.04434085, 0.04434085, 0.17013526, 0.00499171,`'
  id: totrans-471
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`DeviceArray([0.04434085, 0.04434085, 0.17013526, 0.00499171,`'
- en: '`0.00124949], dtype=float32)`'
  id: totrans-472
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`0.00124949], dtype=float32)`'
- en: '**`Smooth labels`**'
  id: totrans-473
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**`Smooth labels`**'
- en: '`[`optax.smooth_labels`]`  is used together with a cross-entropy loss to smooth
    labels. It returns a smoothed version of the one hot input labels. Label smoothing
    has been applied in image classification, language translation, and speech recognition
    to prevent models from becoming overconfident.'
  id: totrans-474
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[`optax.smooth_labels`]`  与交叉熵损失一起使用以平滑标签。它返回输入标签的平滑版本。标签平滑已应用于图像分类、语言翻译和语音识别，以防止模型过于自信。'
- en: '`labels = jnp.array([0.20,0.30,0.10,0.20,0.2])`'
  id: totrans-475
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`labels = jnp.array([0.20,0.30,0.10,0.20,0.2])`'
- en: '`optax.smooth_labels(labels,alpha=0.4)`'
  id: totrans-476
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`optax.smooth_labels(labels,alpha=0.4)`'
- en: '`DeviceArray([0.2 , 0.26, 0.14, 0.2 , 0.2 ], dtype=float32)`'
  id: totrans-477
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`DeviceArray([0.2 , 0.26, 0.14, 0.2 , 0.2 ], dtype=float32)`'
- en: '**`Computing loss with JAX Metrics`**'
  id: totrans-478
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**`Computing loss with JAX Metrics`**'
- en: JAX Metrics is an open-source package for computing losses and metrics in JAX.
    It provides a Keras-like API for computing model loss and metrics.
  id: totrans-479
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAX Metrics 是一个用于在 JAX 中计算损失和指标的开源包。它提供了类似 Keras 的 API 来计算模型的损失和指标。
- en: For example, here is how you use the library to compute the crossentropy loss.
    Similar to Keras, the losses can be computed by either instantiating the [`Loss`] or [`loss`].
  id: totrans-480
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 例如，这里是如何使用库来计算交叉熵损失的。与 Keras 类似，可以通过实例化 [`Loss`] 或 [`loss`] 来计算损失。
- en: '`pip install jax_metrics import jax_metrics as jm`'
  id: totrans-481
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`pip install jax_metrics import jax_metrics as jm`'
- en: '`crossentropy = jm.losses.Crossentropy() logits = jnp.array([0.50,0.60,0.70,0.30,0.25])
    y = jnp.array([0.50,0.60,0.70,0.30,0.25]) crossentropy(target=y, preds=logits)`'
  id: totrans-482
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`crossentropy = jm.losses.Crossentropy() logits = jnp.array([0.50,0.60,0.70,0.30,0.25])
    y = jnp.array([0.50,0.60,0.70,0.30,0.25]) crossentropy(target=y, preds=logits)`'
- en: '`DeviceArray(3.668735, dtype=float32)`'
  id: totrans-483
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`DeviceArray(3.668735, dtype=float32)`'
- en: 变量名`logits`赋值`jnp.array([0.50,0.60,0.70,0.30,0.25])` `y`赋值`jnp.array([0.50,0.60,0.70,0.30,0.25])`
    `jm.losses.crossentropy`(`target=y`, `preds=logits`) `# DeviceArray(3.668735,
    dtype=float32)`
  id: totrans-484
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 变量名`logits`赋值`jnp.array([0.50,0.60,0.70,0.30,0.25])` `y`赋值`jnp.array([0.50,0.60,0.70,0.30,0.25])`
    `jm.losses.crossentropy`(`target=y`, `preds=logits`) `# DeviceArray(3.668735,
    dtype=float32)`
- en: '**Here is what the code would like in a JAX training step.import jax_metrics
    as jmmetric = jm.metrics.Accuracy()@jax.jitdef init_step`(`metric: jm.Metric`)
    -> `jm.Metric`: return `metric.init()`'
  id: totrans-485
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '**Here is what the code would like in a JAX training step.import jax_metrics
    as jmmetric = jm.metrics.Accuracy()@jax.jitdef init_step`(`metric: jm.Metric`)
    -> `jm.Metric`: return `metric.init()`'
- en: '函数名`loss_fn`，参数`params`，`metric`，`x`，`y`: ...'
  id: totrans-486
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '函数名`loss_fn`，参数`params`，`metric`，`x`，`y`: ...'
- en: 变量名`metric`赋值`metric.update`，参数`target=y`，`preds=logits`...
  id: totrans-487
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 变量名`metric`赋值`metric.update`，参数`target=y`，`preds=logits`...
- en: return `loss`, `metric`@jax.jitdef train_step`(`params`, `metric`, `x`, `y`):grads,
    metric = jax.grad`(`loss_fn`, `has_aux=True`)(
  id: totrans-488
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: return `loss`, `metric`@jax.jitdef train_step`(`params`, `metric`, `x`, `y`):grads,
    metric = jax.grad`(`loss_fn`, `has_aux=True`)(
- en: '`params`, `metric`, `x`, `y`'
  id: totrans-489
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`params`, `metric`, `x`, `y`'
- en: )
  id: totrans-490
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: )
- en: '...'
  id: totrans-491
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '...'
- en: return `params`, `metric`
  id: totrans-492
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: return `params`, `metric`
- en: '`@jax.jitdef reset_step`(`metric: jm.Metric`) -> `jm.Metric`: return `metric.reset()`The
    losses we have seen earlier can also be computed using JAX Metrics.'
  id: totrans-493
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`@jax.jitdef reset_step`(`metric: jm.Metric`) -> `jm.Metric`: return `metric.reset()`The
    losses we have seen earlier can also be computed using JAX Metrics.'
- en: '`! pip install jax_metrics`'
  id: totrans-494
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`! pip install jax_metrics`'
- en: '`import jax_metrics as jm`'
  id: totrans-495
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import jax_metrics as jm`'
- en: 变量名`target`赋值`jnp.array([50,60,70,30,25])` `preds`赋值`jnp.array([0.50,0.60,0.70,0.30,0.25])`
    `huber_loss`赋值`jm.losses.Huber()`
  id: totrans-496
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 变量名`target`赋值`jnp.array([50,60,70,30,25])` `preds`赋值`jnp.array([0.50,0.60,0.70,0.30,0.25])`
    `huber_loss`赋值`jm.losses.Huber()`
- en: '`huber_loss`(`target=target`, `preds=preds`) `# DeviceArray(46.030003, dtype=float32)`'
  id: totrans-497
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`huber_loss`(`target=target`, `preds=preds`) `# DeviceArray(46.030003, dtype=float32)`'
- en: 变量名`target`赋值`jnp.array([50,60,70,30,25])`
  id: totrans-498
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 变量名`target`赋值`jnp.array([50,60,70,30,25])`
- en: 变量名`preds`赋值`jnp.array([0.50,0.60,0.70,0.30,0.25])`
  id: totrans-499
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 变量名`preds`赋值`jnp.array([0.50,0.60,0.70,0.30,0.25])`
- en: '`jm.losses.mean_absolute_error`(`target=target`, `preds=preds`) `# DeviceArray(46.530003,
    dtype=float32)`'
  id: totrans-500
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jm.losses.mean_absolute_error`(`target=target`, `preds=preds`) `# DeviceArray(46.530003,
    dtype=float32)`'
- en: '`rng`赋值`jax.random.PRNGKey(42)`'
  id: totrans-501
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`rng`赋值`jax.random.PRNGKey(42)`'
- en: 变量名`target`赋值`jax.random.randint(rng, shape=(2, 3), minval=0, maxval =2)`
  id: totrans-502
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 变量名`target`赋值`jax.random.randint(rng, shape=(2, 3), minval=0, maxval =2)`
- en: 变量名`preds`赋值`jax.random.uniform(rng, shape=(2, 3))`
  id: totrans-503
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 变量名`preds`赋值`jax.random.uniform(rng, shape=(2, 3))`
- en: '`jm.losses.cosine_similarity`(`target`, `preds`, `axis=1`)'
  id: totrans-504
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jm.losses.cosine_similarity`(`target`, `preds`, `axis=1`)'
- en: '`DeviceArray`([-0.8602638 , -0.33731455], dtype=float32) `target`赋值`jnp.array([50,60,70,30,25])`'
  id: totrans-505
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`DeviceArray`([-0.8602638 , -0.33731455], dtype=float32) `target`赋值`jnp.array([50,60,70,30,25])`'
- en: 变量名`preds`赋值`jnp.array([0.50,0.60,0.70,0.30,0.25])`
  id: totrans-506
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 变量名`preds`赋值`jnp.array([0.50,0.60,0.70,0.30,0.25])`
- en: '`jm.losses.mean_absolute_percentage_error`(`target=target`, `preds=p reds`)'
  id: totrans-507
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jm.losses.mean_absolute_percentage_error`(`target=target`, `preds=preds`)'
- en: '`DeviceArray(98.99999, dtype=float32)`'
  id: totrans-508
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`DeviceArray(98.99999, dtype=float32)`'
- en: 变量名`target`赋值`jnp.array([50,60,70,30,25])`
  id: totrans-509
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 变量名`target`赋值`jnp.array([50,60,70,30,25])`
- en: 变量名`preds`赋值`jnp.array([0.50,0.60,0.70,0.30,0.25])`
  id: totrans-510
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 变量名`preds`赋值`jnp.array([0.50,0.60,0.70,0.30,0.25])`
- en: '`jm.losses.mean_squared_logarithmic_error`(`target=target`, `preds=p reds`)'
  id: totrans-511
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jm.losses.mean_squared_logarithmic_error`(`target=target`, `preds=preds`)'
- en: '`DeviceArray(11.7779, dtype=float32)`'
  id: totrans-512
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`DeviceArray(11.7779, dtype=float32)`'
- en: '`target`赋值`jnp.array([0.50,0.60,0.70,0.30,0.25])` `preds`赋值`jnp.array([0.50,0.60,0.70,0.30,0.25])`
    `jm.losses.mean_squared_error`(`target=target`, `preds=preds`) `# DeviceArray(0.,
    dtype=float32)`'
  id: totrans-513
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`target`赋值`jnp.array([0.50,0.60,0.70,0.30,0.25])` `preds`赋值`jnp.array([0.50,0.60,0.70,0.30,0.25])`
    `jm.losses.mean_squared_error`(`target=target`, `preds=preds`) `# DeviceArray(0.,
    dtype=float32)`'
- en: '**How to monitor JAX loss functions**'
  id: totrans-514
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**如何监控JAX损失函数**'
- en: Monitoring the loss of your network is important because it indicates whether
    it's learning or not. A glance at the loss can tell you if there are any problems
    in the network, such as overfitting. One way to monitor the loss is to print the
    training and validation loss as the network is training.
  id: totrans-515
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 监控网络的损失很重要，因为它表明网络是否在学习。损失的一瞥可以告诉您网络中是否存在问题，比如过拟合。监控损失的一种方式是在网络训练时打印训练和验证损失。
- en: '`![](`../images/00014.jpeg`)`'
  id: totrans-516
  prefs: []
  stylish: true
  type: TYPE_IMG
  zh: '`![](`../images/00014.jpeg`)`'
- en: You can also plot the training and validation loss to represent the training
    visually.
  id: totrans-517
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 您还可以绘制训练和验证损失，以可视化地表示训练过程。
- en: '`![](`../images/00015.jpeg`)`'
  id: totrans-518
  prefs: []
  stylish: true
  type: TYPE_IMG
  zh: '`![](`../images/00015.jpeg`)`'
- en: '**Why JAX loss nan happens**'
  id: totrans-519
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**为什么JAX损失会出现NaN**'
- en: JAX will not show errors when NANs occur in your program. This is by design
    because of the complexities involved in showing errors from accelerators. When
    debugging, you can turn on the NAN checker to show NAN errors. NANs should be
    fixed because the network stops learning when they occur.
  id: totrans-520
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 当您的程序中出现NAN时，JAX不会显示错误。这是设计上的考虑，因为从加速器显示错误涉及复杂性。在调试时，您可以打开NAN检查器以显示NAN错误。NAN应该被修复，因为当它们出现时，网络停止学习。
- en: '`from jax.config import config`'
  id: totrans-521
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from jax.config import config`'
- en: '`config.update`(`"jax_debug_nans"`, `True`)'
  id: totrans-522
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`config.update`(`"jax_debug_nans"`, `True`)'
- en: '`jnp.divide`(`0.0`, `0.0`)'
  id: totrans-523
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jnp.divide`(`0.0`, `0.0`)'
- en: '`FloatingPointError`: invalid value (`nan`) encountered in `div`'
  id: totrans-524
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`FloatingPointError`: 遇到了无效值 (`nan`) 在 `div` 中'
- en: 'However, what produces NANs in a network. There are various factors, not limited
    to:'
  id: totrans-525
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 然而，什么导致网络中出现NaN？有各种因素，不限于：
- en: The dataset has not been scaled.
  id: totrans-526
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 数据集尚未缩放。
- en: There are NANs in the training set. The occurrence of infinite values in the
    training data.
  id: totrans-527
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 训练集中存在NaN。训练数据中的无限值出现。
- en: Wrong optimizer function. Exploding gradients leading to large updates to training
    weights. Using a very large learning rate.
  id: totrans-528
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 错误的优化器函数。梯度爆炸导致训练权重大幅更新。使用非常大的学习率。
- en: '`Final thoughts`'
  id: totrans-529
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '`最后的思考`'
- en: 'In this article, we have seen that choosing the right loss function is critical
    to the learning of a network. We have also discussed various loss functions in
    JAX. More precisely, we have coved:'
  id: totrans-530
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在本文中，我们已经看到选择正确的损失函数对网络的学习至关重要。我们还讨论了JAX中的各种损失函数。更准确地说，我们涵盖了：
- en: What is a loss function?
  id: totrans-531
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 什么是损失函数？
- en: How to create custom loss functions in JAX.
  id: totrans-532
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 如何在JAX中创建自定义损失函数。
- en: Loss functions available in JAX.
  id: totrans-533
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAX中可用的损失函数。
- en: Computing loss with JAX Metrics.
  id: totrans-534
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 使用JAX指标计算损失。
- en: Monitoring loss in JAX.
  id: totrans-535
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在JAX中监控损失。
- en: How to avoid NANs in JAX.
  id: totrans-536
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 如何在JAX中避免NaN。
- en: '`Activation functions in JAX and Flax`'
  id: totrans-537
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '`在JAX和Flax中的激活函数`'
- en: Activation functions are applied in neural networks to ensure that the network
    outputs the desired result. The activations functions cap the output within a
    specific range. For instance, when solving a binary classification problem, the
    outcome should be a number between 0 and 1\. This indicates the probability of
    an item belonging to either of the two classes.
  id: totrans-538
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 激活函数被应用在神经网络中，以确保网络输出所需的结果。激活函数将输出限制在特定范围内。例如，在解决二元分类问题时，结果应该是一个介于0和1之间的数字。这表示物品属于两个类别的概率。
- en: However, in a regression problem, you want the numerical prediction of a quantity,
    for example, the price of an item. You should, therefore, choose an appropriate
    activation function for the problem being solved.
  id: totrans-539
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 然而，在回归问题中，您希望数量的数值预测，例如物品的价格。因此，您应该为所解决的问题选择合适的激活函数。
- en: Let's look at common activation functions in JAX and Flax.
  id: totrans-540
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 让我们来看看JAX和Flax中常见的激活函数。
- en: '`ReLU – Rectified linear unit`'
  id: totrans-541
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '`ReLU – 矫正线性单元`'
- en: The  **ReLU activation function** is primarily used in the hidden layers of
    neural networks to ensure non-linearity. The function caps all outputs to zero
    and above. Outputs below zero are returned as zero, while numbers above zero are
    returned as they are. This ensures that there are no negative numbers in the network.
  id: totrans-542
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '**ReLU激活函数**主要用于神经网络的隐藏层，确保非线性。该函数将所有输出限制在零及以上。小于零的输出被返回为零，而大于零的数则原样返回。这确保了网络中没有负数。'
- en: On line 9 we apply the ReLu activation function after the convolution layer.
  id: totrans-543
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在第9行，我们在卷积层之后应用ReLU激活函数。
- en: '`import flaxfrom flax import linen as nnclass CNN(nn.Module):`'
  id: totrans-544
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import flaxfrom flax import linen as nnclass CNN(nn.Module):`'
- en: '`@nn.compact`'
  id: totrans-545
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`@nn.compact`'
- en: '`def __call__(self, x):`'
  id: totrans-546
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def __call__(self, x):`'
- en: '`x = nn.Conv(features=32, kernel_size=(3, 3))(x)`'
  id: totrans-547
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = nn.Conv(features=32, kernel_size=(3, 3))(x)`'
- en: '`x = nn.relu(x)`'
  id: totrans-548
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = nn.relu(x)`'
- en: '`x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))`'
  id: totrans-549
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))`'
- en: '`x = nn.Conv(features=64, kernel_size=(3, 3))(x)`'
  id: totrans-550
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = nn.Conv(features=64, kernel_size=(3, 3))(x)`'
- en: '`x = nn.relu(x)`'
  id: totrans-551
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = nn.relu(x)`'
- en: '`x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))`'
  id: totrans-552
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))`'
- en: '`x = x.reshape((x.shape[0], -1))`'
  id: totrans-553
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = x.reshape((x.shape[0], -1))`'
- en: '`x = nn.Dense(features=256)(x)`'
  id: totrans-554
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = nn.Dense(features=256)(x)`'
- en: '`x = nn.relu(x)`'
  id: totrans-555
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = nn.relu(x)`'
- en: '`x = nn.Dense(features=2)(x)`'
  id: totrans-556
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = nn.Dense(features=2)(x)`'
- en: '`x = nn.log_softmax(x)`'
  id: totrans-557
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = nn.log_softmax(x)`'
- en: return x
  id: totrans-558
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: return x
- en: '`PReLU– Parametric Rectified Linear Unit`'
  id: totrans-559
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '`PReLU – 参数化修正线性单元`'
- en: '`Parametric Rectified Linear Unit`  is ReLU with extra parameters equal to
    the number of channels. It works by introducing *a– *a learnable parameter. PReLU
    allows for non-negative values.'
  id: totrans-560
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`参数化修正线性单元`是带有额外参数的ReLU，参数数量等于通道数。它通过引入一个可学习参数 *a* 来工作。PReLU允许非负值。'
- en: '`![](../images/00016.gif)x = nn.PReLU(x)`'
  id: totrans-561
  prefs: []
  stylish: true
  type: TYPE_IMG
  zh: '`![](../images/00016.gif)x = nn.PReLU(x)`'
- en: '`Sigmoid`'
  id: totrans-562
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '`Sigmoid`'
- en: The  **sigmoid activation function **caps output to a number between 0 and 1
    and is mainly used for binary classification tasks. Sigmoid is used where the
    classes are non-exclusive. For example, an image can have a car, a building, a
    tree, etc. Just because there is a car in the image doesn’t mean a tree can’t
    be in the picture. Use the sigmoid function when there is more than one correct
    answer.
  id: totrans-563
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '**Sigmoid激活函数**将输出限制在0到1之间，主要用于二元分类任务。当类别不是互斥的时候使用sigmoid。例如，一张图片可能同时有汽车、建筑和树等多种物体。使用sigmoid函数来处理这种情况。'
- en: x = `nn.sigmoid(x)`
  id: totrans-564
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.sigmoid(x)`
- en: '**Log sigmoid**'
  id: totrans-565
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**Log sigmoid**'
- en: '**Log sigmoid** computes the log of the sigmoid activation, and its output
    is within the range of −∞ to 0.![](../images/00017.gif)x = `nn.log_sigmoid(x)`'
  id: totrans-566
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '**Log sigmoid**计算sigmoid激活的对数，其输出在范围−∞到0之间。![](../images/00017.gif)x = `nn.log_sigmoid(x)`'
- en: '**Softmax**'
  id: totrans-567
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**Softmax**'
- en: The  **softmax activation function** is a variant of the sigmoid function used
    in multi-class problems where labels are mutually exclusive. For example, a picture
    is either grayscale or color. Use the softmax activation when there is only one
    correct answer.
  id: totrans-568
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '**Softmax激活函数**是sigmoid函数的一种变体，用于多类问题，其中标签是相互排斥的。例如，一张图片只能是灰度或者彩色。当只有一个正确答案时使用softmax激活。'
- en: x = `nn.softmax(x)`
  id: totrans-569
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.softmax(x)`
- en: '**Log softmax**'
  id: totrans-570
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**Log softmax**'
- en: '**Log softmax **computes the logarithm of the softmax function, which rescales
    elements to the range −∞ to 0.![](../images/00018.gif)x = `nn.log_softmax(x)`'
  id: totrans-571
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '**Log softmax**计算softmax函数的对数，将元素重新缩放到范围−∞到0之间。![](../images/00018.gif)x =
    `nn.log_softmax(x)`'
- en: '**ELU – Exponential linear unit activation**'
  id: totrans-572
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**ELU – 指数线性单元激活**'
- en: '**ELU activation**  function helps in solving the vanishing and exploding gradients
    problem. Unlike ReLu, ELU allows negative numbers pushing the mean unit activations
    closer to zero. ELUs may lead to faster training and better generalization in
    networks with more than five layers.'
  id: totrans-573
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '**ELU激活函数**有助于解决梯度消失和梯度爆炸问题。与ReLU不同，ELU允许负数，从而将单位均值激活推向零附近。ELU可能导致训练速度更快，以及在多层网络中更好的泛化能力。'
- en: For values above zero the number is returned as is but for numbers below zeros
    they are a number that is less that but close to zero.![](../images/00019.gif)x
    = `nn.elu(x)`
  id: totrans-574
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 对于大于零的值，返回原数值，但对于小于零的数值，它们会变成接近零但稍小的数。![](../images/00019.gif)x = `nn.elu(x)`
- en: '**CELU – Continuouslydifferentiable exponential linear unit**'
  id: totrans-575
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**CELU – 连续可微的指数线性单元**'
- en: '`CELU` is ELU that is continuously differentiable.![](../images/00020.gif)x
    = `nn.celu(x)`'
  id: totrans-576
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`CELU`是连续可微的ELU激活函数的变种。![](../images/00020.gif)x = `nn.celu(x)`'
- en: '**GELU– Gaussian error linear unit activation**'
  id: totrans-577
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**GELU – 高斯误差线性单元激活**'
- en: '**GELU** non-linearity weights inputs by their value rather than gates inputs
    by their sign as in ReLU– Source.![](../images/00021.gif)x = `nn.gelu(x)` ![](../images/00022.jpeg)'
  id: totrans-578
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '-   **GELU** 非线性按其值加权输入，而不是像 ReLU 那样按其符号门控输入– 来源.![](../images/00021.gif)x
    = `nn.gelu(x)` ![](../images/00022.jpeg)'
- en: '**GLU – Gated linear unit activation**'
  id: totrans-579
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '-   **GLU – 门控线性单元激活**'
- en: '**GLU**  is computed as [`GLU ( a , b )= a ⊗ σ ( b )`]. It has been applied
    in Gated CNNs for natural language processing. In the formula, the b gate controls
    what information is passed to the next layer. GLU helps tackle the vanishing gradient
    problem.'
  id: totrans-580
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '-   **GLU** 是计算为 [`GLU ( a , b )= a ⊗ σ ( b )`]. 它已被应用于用于自然语言处理的门控 CNNs。在公式中，b
    门控制着传递到下一层的信息。GLU 有助于解决消失梯度问题。'
- en: x = `nn.glu(x)`
  id: totrans-581
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.glu(x)`
- en: '**Soft sign**'
  id: totrans-582
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '-   **Soft sign**'
- en: The  **Soft sign** activation function caps values between -1 and 1\. It is
    similar to the hyperbolic tangent activation function– tanh. The difference is
    that tanh converges exponentially while  Soft sign converges polynomially.
  id: totrans-583
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '**Soft sign** 激活函数将值限制在 -1 到 1 之间。它类似于双曲正切激活函数– tanh。不同之处在于 tanh 指数级收敛，而 Soft
    sign 多项式级收敛。'
- en: x = `nn.soft_sign(x)` ![](../images/00023.gif)
  id: totrans-584
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.soft_sign(x)` ![](../images/00023.gif)
- en: '**Softplus**'
  id: totrans-585
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '-   **Softplus**'
- en: The **Softplus activation** returns values as zero and above. It is a smooth
    version of the ReLu. x = `nn.soft_plus(x)` ![](../images/00024.gif)![](../images/00025.jpeg)**The
    Softplus activation ** **## Swish–Sigmoid Linear Unit( SiLU)**
  id: totrans-586
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '-   **Softplus 激活** 将返回值为零及以上。它是 ReLu 的平滑版本。x = `nn.soft_plus(x)` ![](../images/00024.gif)![](../images/00025.jpeg)**The
    Softplus activation ** **## Swish–Sigmoid Linear Unit( SiLU)**'
- en: The SiLU activation function is computed as [`x * sigmoid(beta * x)`] where
    beta is the hyperparameter for Swish activation function. SiLU, is, therefore,
    computed by multiplying the sigmoid function with its input.
  id: totrans-587
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: SiLU 激活函数计算为 [`x * sigmoid(beta * x)`]，其中 beta 是 Swish 激活函数的超参数。因此，SiLU 是通过将其输入与
    sigmoid 函数相乘来计算的。
- en: x = `nn.swish(x)`![](../images/00026.gif)
  id: totrans-588
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.swish(x)`![](../images/00026.gif)
- en: '**Custom activation functions in JAX and Flax**'
  id: totrans-589
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '-   **在 JAX 和 Flax 中的自定义激活函数**'
- en: 'You can also define custom activation functions in JAX. For example, here''s
    how you''d define the LeakyReLu activation function.from flax import linen as
    nnimport jax.numpy as jnpclass `LeakyReLU`(nn.Module):`alpha` : float = 0.1def
    `__call__`(self, `x`):return jnp.where(`x` > 0, `x`, `self.alpha` * `x`)'
  id: totrans-590
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '您还可以在 JAX 中定义自定义激活函数。例如，以下是如何定义 LeakyReLU 激活函数。from flax import linen as nnimport
    jax.numpy as jnpclass `LeakyReLU`(nn.Module):`alpha` : float = 0.1def `__call__`(self,
    `x`):return jnp.where(`x` > 0, `x`, `self.alpha` * `x`)'
- en: '**Final thoughts**'
  id: totrans-591
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '-   **最终思考**'
- en: You have learned about the various activation functions you can use in JAX and
    Flax. You have also seen that you can create new functions by creating a class
    that implements the`[__call__]`method.
  id: totrans-592
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 您已经了解了可以在 JAX 和 Flax 中使用的各种激活函数。您还看到，可以通过创建一个实现`[__call__]`方法的类来创建新函数。
- en: How to load datasets in`JAX`with`TensorFlow`
  id: totrans-593
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '-   如何在`JAX`中加载数据集与`TensorFlow`'
- en: '`JAX`doesn''t ship with data loading utilities. This keeps`JAX`focused on providing
    a fast tool for building and training machine learning models. Loading data in`JAX`is
    done using'
  id: totrans-594
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`JAX` 不提供数据加载实用程序。这使`JAX`专注于提供一个快速构建和训练机器学习模型的工具。在`JAX`中加载数据是使用'
- en: either`TensorFlow`or`PyTorch`. This article will focus on how to load datasets
    in`JAX`using`TensorFlow`.
  id: totrans-595
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 要么`TensorFlow`要么`PyTorch`。本文将重点介绍如何使用`TensorFlow`在`JAX`中加载数据集。
- en: Let's dive in!
  id: totrans-596
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '-   让我们深入探讨！'
- en: How to load text data in`JAX`
  id: totrans-597
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 如何在`JAX`中加载文本数据
- en: Let's use the`IMDB dataset from Kaggle`to illustrate how to load text datasets
    with`JAX`. We'll use the`Kaggle`Python`library to download the data. That requires
    your Kaggle username and key. Head over
  id: totrans-598
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 让我们使用`Kaggle`的`IMDB 数据集`来说明如何在`JAX`中加载文本数据集。我们将使用`Kaggle`Python`库下载数据。这需要您的
    Kaggle 用户名和密钥。前往
- en: '`https://www.kaggle.com/your_username/ account to obtain the API`'
  id: totrans-599
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`https://www.kaggle.com/your_username/ account to obtain the API`'
- en: key.
  id: totrans-600
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 关键。
- en: The library downloads the data as a zip file. We'll therefore extract it afterward.
  id: totrans-601
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 该库将数据下载为 zip 文件。因此，我们将在之后提取它。
- en: import`os`
  id: totrans-602
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: import`os`
- en: Obtain from`https://www.kaggle.com/username/account os.environ["KAGGLE_USERNAME"]="YOUR_KAGGLE_USERNAME"
    os.environ["KAGGLE_KEY"]="YOUR_KAGGLE_KEY"`
  id: totrans-603
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: Obtain from`https://www.kaggle.com/username/account os.environ["KAGGLE_USERNAME"]="YOUR_KAGGLE_USERNAME"
    os.environ["KAGGLE_KEY"]="YOUR_KAGGLE_KEY"`
- en: import`kaggle`
  id: totrans-604
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: import`kaggle`
- en: '!`kaggle datasets download lakshmi25npathi/imdb-dataset-of-50k-m ovie-reviews`'
  id: totrans-605
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '!`kaggle datasets download lakshmi25npathi/imdb-dataset-of-50k-movie-reviews`'
- en: import`zipfile`
  id: totrans-606
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: import`zipfile`
- en: with zipfile.ZipFile('imdb-dataset-of-50k-movie-reviews.zip',
  id: totrans-607
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: with zipfile.ZipFile('imdb-dataset-of-50k-movie-reviews.zip',
- en: '''r'') as zip_ref:zip_ref.extractall(''imdb-dataset-of-50k-movie-reviews'')Next,
    import the standard data science packages and view a sample of the data.'
  id: totrans-608
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '''r'') as zip_ref:zip_ref.extractall(''imdb-dataset-of-50k-movie-reviews'')接下来，导入标准的数据科学包，并查看数据的样本。'
- en: import`numpy as np`
  id: totrans-609
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import`numpy as np`'
- en: import`pandas`as`pd`
  id: totrans-610
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import`pandas`as`pd`'
- en: from`numpy`import`array`
  id: totrans-611
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from`numpy`import`array`'
- en: import`tensorflow as tf`
  id: totrans-612
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import`tensorflow as tf`'
- en: from`sklearn.model_selection`import`train_test_split`
  id: totrans-613
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from`sklearn.model_selection`import`train_test_split`'
- en: from`sklearn.preprocessing`import`LabelEncoder`
  id: totrans-614
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from`sklearn.preprocessing`import`LabelEncoder`'
- en: import`matplotlib.pyplot`as`plt`
  id: totrans-615
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import`matplotlib.pyplot`as`plt`'
- en: '`df = pd.read_csv("imdb-dataset-of-50k-movie-reviews/IMDB Dataset.c df.head()`'
  id: totrans-616
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`df = pd.read_csv("imdb-dataset-of-50k-movie-reviews/IMDB Dataset.c df.head()`'
- en: '!`[](../images/00027.jpeg)`'
  id: totrans-617
  prefs: []
  stylish: true
  type: TYPE_IMG
  zh: '!`[](../images/00027.jpeg)`'
- en: Clean the text data
  id: totrans-618
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 清理文本数据
- en: Let's do some processing of the data before we proceed to load it using`TensorFlow`.
    Standard processing in text problems is to remove stop words. Stop words are common
    words such as`a`,`the`that don't help the model in identifying the polarity of
    a
  id: totrans-619
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在使用`TensorFlow`加载数据之前，让我们对数据进行一些处理。文本问题的标准处理是去除停用词。停用词是如`a`、`the`等常见词汇，它们对模型识别情感极性没有帮助。
- en: sentence.`NLTK`provides the stops words. We can, therefore, write a function
    to remove them from the IMDB dataset.
  id: totrans-620
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 句子。`NLTK`提供了停用词。因此，我们可以编写一个函数从 IMDB 数据集中去除它们。
- en: import`nltk`
  id: totrans-621
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import`nltk`'
- en: from`nltk.corpus`import`stopwords`
  id: totrans-622
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from`nltk.corpus`import`stopwords`'
- en: '`nltk.download(''stopwords'')`'
  id: totrans-623
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`nltk.download(''stopwords'')`'
- en: '`def remove_stop_words(review):`'
  id: totrans-624
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def remove_stop_words(review):`'
- en: '`review_minus_sw = []`'
  id: totrans-625
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`review_minus_sw = []`'
- en: '`stop_words = stopwords.words(''english'')`'
  id: totrans-626
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`stop_words = stopwords.words(''english'')`'
- en: '`review = review.split()`'
  id: totrans-627
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`review = review.split()`'
- en: '`cleaned_review = [review_minus_sw.append(word) for word in`'
  id: totrans-628
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[cleaned_review = [review_minus_sw.append(word) for word in`'
- en: '`review if word not in stop_words]`'
  id: totrans-629
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`review if word not in stop_words]`'
- en: '`cleaned_review = '' ''.join(review_minus_sw)`'
  id: totrans-630
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`cleaned_review = '' ''.join(review_minus_sw)`'
- en: '`return cleaned_review`'
  id: totrans-631
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return cleaned_review`'
- en: '`df[''review''] = df[''review''].apply(remove_stop_words) view raw`'
  id: totrans-632
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`df[''review''] = df[''review''].apply(remove_stop_words) view raw`'
- en: Label encode the sentiment column
  id: totrans-633
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 对情感列进行标签编码
- en: Convert the sentiment column to numerical representation using`Scikit-learn's
    label encoder. This is important because neural networks expect numerical data.
  id: totrans-634
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 使用`Scikit-learn`的标签编码将情感列转换为数值表示。这很重要，因为神经网络期望数值数据。
- en: '`labelencoder = LabelEncoder()`'
  id: totrans-635
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`labelencoder = LabelEncoder()`'
- en: '`df = df.assign(sentiment = labelencoder.fit_transform(df["senti ment"]))`'
  id: totrans-636
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`df = df.assign(sentiment = labelencoder.fit_transform(df["senti ment"]))`'
- en: '!`[](../images/00028.jpeg)`'
  id: totrans-637
  prefs: []
  stylish: true
  type: TYPE_IMG
  zh: '!`[](../images/00028.jpeg)`'
- en: Text preprocessing with`TensorFlow`
  id: totrans-638
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 使用`TensorFlow`进行文本预处理
- en: We have converted the sentiment column to a numerical
  id: totrans-639
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 我们已将情感列转换为数字
- en: representation. However, the reviews are still in text form. We need to convert
    them to numbers as well.
  id: totrans-640
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 表示。然而，评论仍然是文本形式。我们也需要将它们转换为数字。
- en: We start by splitting the dataset into a training and testing set.
  id: totrans-641
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 我们首先将数据集分割为训练集和测试集。
- en: from`sklearn.model_selection`import`train_test_split df = df.drop_duplicates()`
  id: totrans-642
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from`sklearn.model_selection`import`train_test_split df = df.drop_duplicates()`'
- en: '`docs = df[''review'']`'
  id: totrans-643
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`docs = df[''review'']`'
- en: '`labels = array(df[''sentiment''])`'
  id: totrans-644
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`labels = array(df[''sentiment''])`'
- en: '`X_train, X_test , y_train, y_test = train_test_split(docs, labe ls , test_size
    = 0.20, random_state=0)`'
  id: totrans-645
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`X_train, X_test , y_train, y_test = train_test_split(docs, labe ls , test_size
    = 0.20, random_state=0)`'
- en: 'Next, we use TensorFlow''s`TextVectorization`function to convert the text data
    to integer representations. The function expects:'
  id: totrans-646
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 接下来，我们使用 TensorFlow 的`TextVectorization`函数将文本数据转换为整数表示。该函数期望：
- en: '`[standardize]`used to specify how the text data is processed. For example,
    the`[lower_and_strip_punctuation]`option will lowercase the data and remove punctuations.'
  id: totrans-647
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[standardize]`用于指定如何处理文本数据。例如，选项`[lower_and_strip_punctuation]`将文本转换为小写并删除标点符号。'
- en: '`max_tokens`dictates the maximum size of the vocabulary. `[output_mode]`determines
    the output of the vectorization layer. Setting`[int]`outputs integers.'
  id: totrans-648
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`max_tokens`决定词汇表的最大大小。`[output_mode]`确定向量化层的输出。设置`[int]`将输出整数。'
- en: '`[output_sequence_length]`indicates the maximum length of the output sequence.
    This ensures that all sequences have the same length.'
  id: totrans-649
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[output_sequence_length]`指示输出序列的最大长度。这确保所有序列具有相同的长度。'
- en: import`tensorflow as tf`
  id: totrans-650
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import`tensorflow as tf`'
- en: '`max_features = 5000`# Maximum vocab size.'
  id: totrans-651
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`max_features = 5000`# 最大词汇量大小。'
- en: '`batch_size = 32`'
  id: totrans-652
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`batch_size = 32`'
- en: '`max_len = 512`# Sequence length to pad the outputs to. `vectorize_layer =
    tf.keras.layers.TextVectorization(standardize =''lower_and_strip_punctuation'',max_tokens=max_features,output_m
    ode=''int'',output_sequence_length=max_len)`'
  id: totrans-653
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`max_len = 512`# 将输出填充到的序列长度。`vectorize_layer = tf.keras.layers.TextVectorization(standardize
    =''lower_and_strip_punctuation'',max_tokens=max_features,output_m ode=''int'',output_sequence_length=max_len)`'
- en: '`vectorize_layer.adapt(X_train, batch_size=None)`'
  id: totrans-654
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`vectorize_layer.adapt(X_train, batch_size=None)`'
- en: Next, apply this layer to the training and testing data.`X_train_padded = vectorize_layer(X_train)
    X_test_padded = vectorize_layer(X_test)![](../images/00029.jpeg)`
  id: totrans-655
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 接下来，将此层应用于训练和测试数据。`X_train_padded = vectorize_layer(X_train) X_test_padded =
    vectorize_layer(X_test)![](../images/00029.jpeg)`
- en: Convert the data to a`TensorFlow dataset and create a function to fetch the
    data in batches. We also convert the data to NumPy arrays because JAX expects
    NumPy or JAX arrays.
  id: totrans-656
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 将数据转换为`TensorFlow数据集，并创建一个函数来批量获取数据。我们还将数据转换为NumPy数组，因为JAX期望使用NumPy或JAX数组。
- en: import`tensorflow_datasets`as`tfds`
  id: totrans-657
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: import`tensorflow_datasets`as`tfds`
- en: '`training_data = tf.data.Dataset.from_tensor_slices((X_train_padded, y_train))`'
  id: totrans-658
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`training_data = tf.data.Dataset.from_tensor_slices((X_train_padded, y_train))`'
- en: '`validation_data = tf.data.Dataset.from_tensor_slices((X_test_padded, y_test))`'
  id: totrans-659
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`validation_data = tf.data.Dataset.from_tensor_slices((X_test_padded, y_test))`'
- en: '`training_data = training_data.batch(batch_size)`'
  id: totrans-660
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`training_data = training_data.batch(batch_size)`'
- en: '`validation_data = validation_data.batch(batch_size)`'
  id: totrans-661
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`validation_data = validation_data.batch(batch_size)`'
- en: '`def get_train_batches():`'
  id: totrans-662
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def get_train_batches():`'
- en: '`ds = training_data.prefetch(1)`'
  id: totrans-663
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`ds = training_data.prefetch(1)`'
- en: '`tfds.dataset_as_numpy`converts the`tf.data.Dataset`into an'
  id: totrans-664
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`tfds.dataset_as_numpy`将`tf.data.Dataset`转换为'
- en: iterable of NumPy arrays`return tfds.as_numpy(ds)`The data is now in the right
    format and to be passed to a Flax network.
  id: totrans-665
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 可迭代的NumPy数组`return tfds.as_numpy(ds)`现在数据格式正确，并将传递给Flax网络。
- en: Let's quickly walk through the rest of the steps required to train neural networks
    in Flax using this data.
  id: totrans-666
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 让我们快速浏览一下使用这些数据在Flax中训练神经网络所需的其余步骤。
- en: First, create a simple neural network in Flax.
  id: totrans-667
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 首先，在Flax中创建一个简单的神经网络。
- en: pip install`flax`
  id: totrans-668
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: pip install`flax`
- en: import`flax`
  id: totrans-669
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: import`flax`
- en: from`flax`import`linen as nn`
  id: totrans-670
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: from`flax`import`linen as nn`
- en: '`class Model(nn.Module):`'
  id: totrans-671
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`class Model(nn.Module):`'
- en: '@nn.compact'
  id: totrans-672
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '@nn.compact'
- en: '`def __call__(self, x): x = nn.Dense(features=256)(x) x = nn.relu(x)`'
  id: totrans-673
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def __call__(self, x): x = nn.Dense(features=256)(x) x = nn.relu(x)`'
- en: '`x = nn.Dense(features=2)(x) x = nn.log_softmax(x)`'
  id: totrans-674
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = nn.Dense(features=2)(x) x = nn.log_softmax(x)`'
- en: return`x`
  id: totrans-675
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: return`x`
- en: Define a function to compute the loss.
  id: totrans-676
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 定义一个计算损失的函数。
- en: import`optax`
  id: totrans-677
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: import`optax`
- en: import`jax.numpy`as`jnp`
  id: totrans-678
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: import`jax.numpy`as`jnp`
- en: '`def cross_entropy_loss(*, logits, labels):`'
  id: totrans-679
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def cross_entropy_loss(*, logits, labels):`'
- en: '`labels_onehot = jax.nn.one_hot(labels, num_classes=2) return optax.softmax_cross_entropy(logits=logits,
    labels=labe ls_onehot).mean()`Next, define the function to compute the network
    metrics.'
  id: totrans-680
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`labels_onehot = jax.nn.one_hot(labels, num_classes=2) return optax.softmax_cross_entropy(logits=logits,
    labels=labe ls_onehot).mean()`接下来，定义计算网络指标的函数。'
- en: '`def compute_metrics(*, logits, labels):`'
  id: totrans-681
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def compute_metrics(*, logits, labels):`'
- en: '`loss = cross_entropy_loss(logits=logits, labels=labels)` `accuracy = jnp.mean(jnp.argmax(logits,
    -1) == labels)` `metrics = {`'
  id: totrans-682
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`loss = cross_entropy_loss(logits=logits, labels=labels)` `accuracy = jnp.mean(jnp.argmax(logits,
    -1) == labels)` `metrics = {`'
- en: '`''loss'': loss,`'
  id: totrans-683
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`''loss'': loss,`'
- en: '`''accuracy'': accuracy`,'
  id: totrans-684
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`''accuracy'': accuracy`,'
- en: '}'
  id: totrans-685
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '}'
- en: return `metrics`
  id: totrans-686
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: return `metrics`
- en: The training state is used to track the network training. It tracks the optimizer
    and model parameters and can be modified to track other things such as dropout
    and batch normalization statistics.
  id: totrans-687
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 训练状态用于跟踪网络训练。它跟踪优化器和模型参数，并可以修改以跟踪其他内容，例如dropout和批归一化统计信息。
- en: from `flax.training import train_state` def `create_train_state(rng, learning_rate,
    momentum):` """Creates initial `TrainState`."""
  id: totrans-688
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: from `flax.training import train_state` def `create_train_state(rng, learning_rate,
    momentum):` """创建初始`TrainState`。"""
- en: '`model = Model()`'
  id: totrans-689
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`model = Model()`'
- en: '`params = model.init(rng, X_train_padded[0])[''params'']` `tx = optax.sgd(learning_rate,
    momentum)`'
  id: totrans-690
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`params = model.init(rng, X_train_padded[0])[''params'']` `tx = optax.sgd(learning_rate,
    momentum)`'
- en: return `train_state.TrainState.create(
  id: totrans-691
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: return `train_state.TrainState.create(
- en: '`apply_fn=model.apply, params=params, tx=tx)`In the training step, we [Apply] the
    model to obtain the loss. This is then used to compute the gradients that update
    the model parameters.'
  id: totrans-692
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`apply_fn=model.apply, params=params, tx=tx)`在训练步骤中，我们[应用]模型以获取损失。然后用这个损失来计算更新模型参数的梯度。'
- en: '`def compute_loss(params,text,labels):`'
  id: totrans-693
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def compute_loss(params,text,labels):`'
- en: '`logits = Model().apply({''params'': params}, text)` `loss = cross_entropy_loss(logits=logits,
    labels=labels)` return `loss, logits`'
  id: totrans-694
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`logits = Model().apply({''params'': params}, text)` `loss = cross_entropy_loss(logits=logits,
    labels=labels)` return `loss, logits`'
- en: '@jax.jit'
  id: totrans-695
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '@jax.jit'
- en: '`def train_step(state,text, labels):`'
  id: totrans-696
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def train_step(state,text, labels):`'
- en: '"""Train for a single step."""'
  id: totrans-697
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""对单个步骤进行训练。"""'
- en: '`(_, logits), grads = jax.value_and_grad(compute_loss, has_aux =True)(state.params,text,labels)`'
  id: totrans-698
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`(_, logits), grads = jax.value_and_grad(compute_loss, has_aux =True)(state.params,text,labels)`'
- en: '`state = state.apply_gradients(grads=grads)`'
  id: totrans-699
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`state = state.apply_gradients(grads=grads)`'
- en: '`metrics = compute_metrics(logits=logits, labels=labels)`'
  id: totrans-700
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`metrics = compute_metrics(logits=logits, labels=labels)`'
- en: return `state, metrics`
  id: totrans-701
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: return `state, metrics`
- en: The evaluation step applies the model to the testing data to compute the test
    metrics.
  id: totrans-702
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 评估步骤将模型应用于测试数据以计算测试指标。
- en: '@jax.jit'
  id: totrans-703
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '@jax.jit'
- en: '`def eval_step(state, text, labels):`'
  id: totrans-704
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def eval_step(state, text, labels):`'
- en: '`logits = Model().apply({''params'': state.params}, text)`'
  id: totrans-705
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`logits = Model().apply({''params'': state.params}, text)`'
- en: return `compute_metrics(logits=logits, labels=labels)`
  id: totrans-706
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: return `compute_metrics(logits=logits, labels=labels)`
- en: The evaluation function runs the above evaluation step to obtain the evaluation
    metrics.
  id: totrans-707
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 评估函数运行上述评估步骤以获取评估指标。
- en: '`def evaluate_model(state, text, test_lbls):`'
  id: totrans-708
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def evaluate_model(state, text, test_lbls):`'
- en: '"""Evaluate on the validation set."""'
  id: totrans-709
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""在验证集上进行评估。"""'
- en: '`metrics = eval_step(state, text, test_lbls)` `metrics = jax.device_get(metrics)`'
  id: totrans-710
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`metrics = eval_step(state, text, test_lbls)` `metrics = jax.device_get(metrics)`'
- en: '`metrics = jax.tree_map(lambda x: x.item(), metrics)` return `metrics`'
  id: totrans-711
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`metrics = jax.tree_map(lambda x: x.item(), metrics)` return `metrics`'
- en: We use the `get_train_batches` function in the`train_epoch` method. We loop
    through the batches as we apply the `[train_step]` method. We obtain the train
    metrics and return them.
  id: totrans-712
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 我们在`train_epoch`方法中使用`get_train_batches`函数。我们通过应用`[train_step]`方法循环遍历批次。我们获取训练指标并返回它们。
- en: '`def train_one_epoch(state):`'
  id: totrans-713
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def train_one_epoch(state):`'
- en: '"""Train for 1 epoch on the training set.""" `batch_metrics = []`'
  id: totrans-714
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""在训练集上进行1轮训练。""" `batch_metrics = []`'
- en: for `text, labels in get_train_batches():`
  id: totrans-715
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: for `text, labels in get_train_batches():`
- en: '`state, metrics = train_step(state, text, labels)` `batch_metrics.append(metrics)`batch_metrics_np
    = jax.device_get(batch_metrics) epoch_metrics_np = {k: np.mean([metrics[k] for
    metrics in batch_metrics_n p])for k in batch_metrics_np[0] }return `state, epoch_metrics_np`'
  id: totrans-716
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`state, metrics = train_step(state, text, labels)` `batch_metrics.append(metrics)`batch_metrics_np
    = jax.device_get(batch_metrics) epoch_metrics_np = {k: np.mean([metrics[k] for
    metrics in batch_metrics_n p])for k in batch_metrics_np[0] }return `state, epoch_metrics_np`'
- en: The final step is to train the network on the training set and evaluate it on
    the test set. A training state is required before training the model. This is
    because JAX expects pure functions.
  id: totrans-717
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 最后一步是在训练集上训练网络，并在测试集上进行评估。在训练模型之前需要一个训练状态。这是因为JAX期望纯函数。
- en: '`rng = jax.random.PRNGKey(0)`，`rng, init_rng = jax.random.split(rng)`'
  id: totrans-718
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`rng = jax.random.PRNGKey(0)`，`rng, init_rng = jax.random.split(rng)`'
- en: '`learning_rate = 0.1 momentum = 0.9`'
  id: totrans-719
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`learning_rate = 0.1 momentum = 0.9`'
- en: '`seed = 0`'
  id: totrans-720
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`seed = 0`'
- en: '`state = create_train_state(init_rng, learning_rate, momentum)` del `init_rng
    # Must not be used anymore.`'
  id: totrans-721
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`state = create_train_state(init_rng, learning_rate, momentum)` del `init_rng
    # 不再使用。`'
- en: '`num_epochs = 30`'
  id: totrans-722
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`num_epochs = 30`'
- en: '`(text, test_labels) = next(iter(validation_data))` `text = jnp.array(text)`'
  id: totrans-723
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`(text, test_labels) = next(iter(validation_data))` `text = jnp.array(text)`'
- en: '`test_labels = jnp.array(test_labels)`'
  id: totrans-724
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`test_labels = jnp.array(test_labels)`'
- en: '`state` = `create_train_state(jax.random.PRNGKey(seed), learning_r ate, momentum)`'
  id: totrans-725
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`state` = `create_train_state(jax.random.PRNGKey(seed), learning_r ate, momentum)`'
- en: '`training_loss` = []'
  id: totrans-726
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`training_loss` = []'
- en: '`training_accuracy` = []'
  id: totrans-727
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`training_accuracy` = []'
- en: '`testing_loss` = []'
  id: totrans-728
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_loss` = []'
- en: '`testing_accuracy` = []'
  id: totrans-729
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_accuracy` = []'
- en: '`for epoch in range(1, num_epochs + 1):`'
  id: totrans-730
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'for epoch in range(1, num_epochs + 1):'
- en: '`train_state, train_metrics = train_one_epoch(state) training_loss.append(train_metrics[''loss''])`'
  id: totrans-731
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_state, train_metrics = train_one_epoch(state) training_loss.append(train_metrics[''loss''])`'
- en: '`training_accuracy.append(train_metrics[''accuracy'']) print(f"Train epoch:
    {epoch}, loss: {train_metrics[''los`'
  id: totrans-732
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`training_accuracy.append(train_metrics[''accuracy'']) print(f"Train epoch:
    {epoch}, loss: {train_metrics[''los`'
- en: 's'']}, accuracy: {train_metrics[''accuracy''] * 100}")`'
  id: totrans-733
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 's'']}, accuracy: {train_metrics[''accuracy''] * 100}")`'
- en: '`test_metrics` = `evaluate_model(train_state, text, test_label s)`'
  id: totrans-734
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`test_metrics` = `evaluate_model(train_state, text, test_label s)`'
- en: '`testing_loss.append(test_metrics[''loss''])`'
  id: totrans-735
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_loss.append(test_metrics[''loss''])`'
- en: '`testing_accuracy.append(test_metrics[''accuracy''])`'
  id: totrans-736
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_accuracy.append(test_metrics[''accuracy''])`'
- en: '`print(f"Test epoch: {epoch}, loss: {test_metrics[''loss'']}, accuracy: {test_metrics[''accuracy'']
    * 100}")`'
  id: totrans-737
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`print(f"Test epoch: {epoch}, loss: {test_metrics[''loss'']}, accuracy: {test_metrics[''accuracy'']
    * 100}")`'
- en: '`![](../images/00030.jpeg)`'
  id: totrans-738
  prefs: []
  stylish: true
  type: TYPE_IMG
  zh: '`![](../images/00030.jpeg)`'
- en: '**How to load image data in JAX**'
  id: totrans-739
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**如何在JAX中加载图像数据**'
- en: Let's now see how we can load image data with TensorFlow. We'll use the popular cats
    and dogs images from Kaggle. We start by downloading the data.
  id: totrans-740
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 现在让我们看看如何使用TensorFlow加载图像数据。我们将使用来自Kaggle的流行的猫和狗图像。我们首先下载数据。
- en: '`import wget # pip install wgetimport zipfile`'
  id: totrans-741
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import wget # pip install wgetimport zipfile`'
- en: '`wget.download("https://ml.machinelearningnuggets.com/train.zi p")`'
  id: totrans-742
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`wget.download("https://ml.machinelearningnuggets.com/train.zi p")`'
- en: '`with zipfile.ZipFile(''train.zip'', ''r'') as zip_ref:`'
  id: totrans-743
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`with zipfile.ZipFile(''train.zip'', ''r'') as zip_ref:`'
- en: '`zip_ref.extractall(''.'')Next, create a Pandas DataFrame containing the labels
    and paths to the images.`'
  id: totrans-744
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`zip_ref.extractall(''.'')接下来，创建一个包含标签和图像路径的`Pandas DataFrame`。'
- en: '`import pandas as pd`'
  id: totrans-745
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import pandas as pd`'
- en: '`base_dir` = `''train''`'
  id: totrans-746
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`base_dir` = `''train''`'
- en: '`filenames = os.listdir(base_dir) categories = []`'
  id: totrans-747
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`filenames = os.listdir(base_dir) categories = []`'
- en: '`for filename in filenames:`'
  id: totrans-748
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`for filename in filenames:`'
- en: '`category = filename.split(''.'')[0] if category == ''dog'':categories.append("dog")`'
  id: totrans-749
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`category = filename.split(''.'')[0] if category == ''dog'':categories.append("dog")`'
- en: '`else:`'
  id: totrans-750
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`else:`'
- en: '`categories.append("cat") df = pd.DataFrame({''filename'': filenames,''category'':
    categorie s})`'
  id: totrans-751
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`categories.append("cat") df = pd.DataFrame({''filename'': filenames,''category'':
    categorie s})`'
- en: The next step is to define an `ImageDataGenerator` for scaling the images and
    performing simple augmentation.
  id: totrans-752
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 下一步是为图像缩放和执行简单数据增强定义一个`ImageDataGenerator`。
- en: '`from tensorflow.keras.preprocessing.image import ImageDataGener ator`'
  id: totrans-753
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from tensorflow.keras.preprocessing.image import ImageDataGener ator`'
- en: '`train_datagen` = `ImageDataGenerator(rescale=1./255,`'
  id: totrans-754
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_datagen` = `ImageDataGenerator(rescale=1./255,`'
- en: '`shear_range=0.2, zoom_range=0.2,`'
  id: totrans-755
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`shear_range=0.2, zoom_range=0.2,`'
- en: '`horizontal_flip=True, width_shift_range=0.1, height_shift_range=0.1, validation_split=0.2
    )`'
  id: totrans-756
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`horizontal_flip=True, width_shift_range=0.1, height_shift_range=0.1, validation_split=0.2
    )`'
- en: '`validation_gen` = `ImageDataGenerator(rescale=1./255,validation_s plit=0.2)`'
  id: totrans-757
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`validation_gen` = `ImageDataGenerator(rescale=1./255,validation_s plit=0.2)`'
- en: Load the images using the `[flow_from_dataframe]of these generators. This will
    match the image paths in the DataFrame to the images we downloaded.`
  id: totrans-758
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 使用这些生成器的`[flow_from_dataframe]`加载图像。这将把 DataFrame 中的图像路径与我们下载的图像进行匹配。
- en: '`image_size = (128, 128)`'
  id: totrans-759
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`image_size = (128, 128)`'
- en: '`batch_size = 128`'
  id: totrans-760
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`batch_size = 128`'
- en: '`training_set` = `train_datagen.flow_from_dataframe(df,base_dir,`'
  id: totrans-761
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`training_set` = `train_datagen.flow_from_dataframe(df,base_dir,`'
- en: '`seed=101, target_size=ima ge_size,`'
  id: totrans-762
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`seed=101, target_size=ima ge_size,`'
- en: '`batch_size=batc h_size,`'
  id: totrans-763
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`batch_size=batc h_size,`'
- en: '`x_col=''filenam e''`,'
  id: totrans-764
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x_col=''filenam e''`,'
- en: '`y_col=''categor y''`'
  id: totrans-765
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`y_col=''分类''`'
- en: subset = `"train ing"`
  id: totrans-766
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: subset = `"训练"`
- en: '`class_mode=''bin ary'')`'
  id: totrans-767
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`class_mode=''bin ary'')`'
- en: '`validation_set` = `validation_gen.flow_from_dataframe(df,base_di r,`'
  id: totrans-768
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`validation_set` = `validation_gen.flow_from_dataframe(df,base_di r,`'
- en: '`target_size=image _size`,'
  id: totrans-769
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`target_size=image _size`,'
- en: '`batch_size=batch_b size,`'
  id: totrans-770
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`batch_size=batch_b size,`'
- en: '`x_col=''filename''`,'
  id: totrans-771
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x_col=''文件名''`,'
- en: '`y_col=''categor y''`,'
  id: totrans-772
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`y_col=''分类''`,'
- en: subset = `"validat ion"`
  id: totrans-773
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: subset = `"验证"`
- en: '`class_mode=''binar y'')`'
  id: totrans-774
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`class_mode=''binar y'')`'
- en: Loop through the training set to confirm that a batch of images are being generated.
  id: totrans-775
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 遍历训练集以确认是否生成了一批图像。
- en: '`for train_images, train_labels in training_set: print(''Train:'', train_images.shape,
    train_labels.shape) break`'
  id: totrans-776
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`for train_images, train_labels in training_set: print(''Train:'', train_images.shape,
    train_labels.shape) break`'
- en: 'Train: `(128, 128, 128, 3) (128,)`'
  id: totrans-777
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: 训练：`(128, 128, 128, 3) (128,)`
- en: The next step is to define a network and pass the data. The steps are similar
    to what we did for the text data above
  id: totrans-778
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 下一步是定义一个神经网络并传递数据。步骤与我们上面对文本数据所做的步骤类似
- en: '**How to load CSV data in JAX**'
  id: totrans-779
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**如何在 JAX 中加载 CSV 数据**'
- en: You can use `Pandas` to load CSV data as we did for the text data at the beginning
    of the article. Convert the data to NumPy or `JAX arrays` once preprocessing is
    done. Passing Torch tensors or TensorFlow tensors to `JAX` neural networks will
    result in an error.
  id: totrans-780
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 你可以使用`Pandas`来加载 CSV 数据，就像我们在文章开头的文本数据那样。在预处理完成后，将数据转换为 NumPy 或`JAX 数组`。将 Torch
    张量或 TensorFlow 张量传递给`JAX`神经网络会导致错误。
- en: '`**Final thoughts**`'
  id: totrans-781
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '`**最后的思考**`'
- en: This article shows how you can use TensorFlow to load datasets in `JAX` and
    `Flax` applications. We have walked through an example of loading text data with
    TensorFlow. After that, we discussed loading image and CSV data in `JAX`.
  id: totrans-782
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 本文展示了如何使用 TensorFlow 在`JAX`和`Flax`应用程序中加载数据集。我们已经介绍了如何使用 TensorFlow 加载文本数据的示例。之后，我们讨论了在`JAX`中加载图像和
    CSV 数据。
- en: '`**Image classification with JAX & Flax**`'
  id: totrans-783
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '`**使用 JAX 和 Flax 进行图像分类**`'
- en: '`Flax` is a neural network library for `JAX`. `JAX` is a `Python` library that
    provides high-performance computing in machine learning research. `JAX` provides
    an API similar to `NumPy` making it easy to adopt. `JAX` also includes other functionalities
    for improving machine learning research. They include:'
  id: totrans-784
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Flax`是一个面向`JAX`的神经网络库。`JAX`是一个提供高性能计算的`Python`库，用于机器学习研究。`JAX`提供了与`NumPy`类似的
    API，使其易于采用。`JAX`还包括其他用于改进机器学习研究的功能。其中包括：'
- en: '`**Automatic differentiation**` . `JAX` supports forward and reverse automatic
    differential of numerical functions with functions such as `jacrev`, `grad`, `hessian` and `jacfwd`.'
  id: totrans-785
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '**自动微分** . `JAX` 支持使用 `jacrev`, `grad`, `hessian` 和 `jacfwd` 等函数进行数值函数的前向和反向自动微分。'
- en: '`**Vectorization**` . `JAX` supports automatic vectorization via the `[vmap]` function.
    It also makes it easy to parallelize large-scale data processing via the `[pmap]` function.'
  id: totrans-786
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '**向量化** . `JAX` 通过 `[vmap]` 函数支持自动向量化。它还通过 `[pmap]` 函数使得大规模数据处理易于并行化。'
- en: '`**JIT compilation**` . `JAX` uses `XLA` for Just In Time (JIT) compilation
    and execution of code on GPUs and TPUs. In this article, let''s look at how you
    can use `JAX` and `Flax` to build a simple convolutional neural network.'
  id: totrans-787
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '**即时编译** . `JAX` 使用 `XLA` 来进行代码的即时编译和在 GPU 和 TPU 上执行。在本文中，让我们看看如何使用 `JAX` 和
    `Flax` 来构建一个简单的卷积神经网络。'
- en: '`**Loading the dataset**`'
  id: totrans-788
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**加载数据集**'
- en: 'We''ll use the `cats and dogs dataset` from Kaggle. Let''s start by downloading
    and extracting it.`import wget # pip install wget import zipfile`'
  id: totrans-789
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '我们将使用来自 Kaggle 的`猫狗数据集`。让我们从下载和解压开始。`import wget # pip install wget import
    zipfile`'
- en: '`wget.download("https://ml.machinelearningnuggets.com/train.zi p")`'
  id: totrans-790
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`wget.download("https://ml.machinelearningnuggets.com/train.zi p")`'
- en: '`with zipfile.ZipFile(''train.zip'', ''r'') as zip_ref:`'
  id: totrans-791
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`with zipfile.ZipFile(''train.zip'', ''r'') as zip_ref:`'
- en: '`zip_ref.extractall(''.'')`'
  id: totrans-792
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`zip_ref.extractall(''.'')`'
- en: '`Flax doesn''t ship with any data loading tools. You can use the data loaders
    from `PyTorch`'
  id: totrans-793
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: Flax 没有任何数据加载工具。你可以使用来自 `PyTorch` 的数据加载器。
- en: '`or `TensorFlow. In this case, let''s load the data using PyTorch. The first
    step is to define the dataset class.'
  id: totrans-794
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 或者使用 `TensorFlow`。在这种情况下，让我们使用 PyTorch 加载数据。首先要定义数据集类。
- en: '`from PIL import Image`'
  id: totrans-795
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from PIL import Image`'
- en: '`import pandas as pd`'
  id: totrans-796
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import pandas as pd`'
- en: '`from torch.utils.data import Dataset`'
  id: totrans-797
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from torch.utils.data import Dataset`'
- en: '`class CatsDogsDataset(Dataset):`'
  id: totrans-798
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`class CatsDogsDataset(Dataset):`'
- en: '`def __init__(self, root_dir, annotation_file, transform=Non`'
  id: totrans-799
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def __init__(self, root_dir, annotation_file, transform=Non`'
- en: '`e):`'
  id: totrans-800
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`e):`'
- en: '`self.root_dir = root_dir`'
  id: totrans-801
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.root_dir = root_dir`'
- en: '`self.annotations = pd.read_csv(annotation_file) self.transform = transform`'
  id: totrans-802
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.annotations = pd.read_csv(annotation_file) self.transform = transform`'
- en: '`def __len__(self):return len(self.annotations) def __getitem__(self, index):img_id
    = self.annotations.iloc[index, 0]img = Image.open(os.path.join(self.root_dir,
    img_id)).c onvert("RGB")y_label = torch.tensor(float(self.annotations.iloc[inde
    x, 1]))if self.transform is not None: img = self.transform(img)return (img, y_label)`Next,
    we create a `Pandas DataFrame` that will contain the categories.`import osimport
    pandas as pd`'
  id: totrans-803
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def __len__(self):return len(self.annotations) def __getitem__(self, index):img_id
    = self.annotations.iloc[index, 0]img = Image.open(os.path.join(self.root_dir,
    img_id)).c onvert("RGB")y_label = torch.tensor(float(self.annotations.iloc[inde
    x, 1]))if self.transform is not None: img = self.transform(img)return (img, y_label)`接下来，我们创建一个将包含类别的 `Pandas
    DataFrame`。`import osimport pandas as pd`'
- en: '`train_df = pd.DataFrame(columns=["img_path","label"]) train_df["img_path"]
    = os.listdir("train/") for idx, i in enumerate(os.listdir("train/")):`'
  id: totrans-804
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_df = pd.DataFrame(columns=["img_path","label"]) train_df["img_path"]
    = os.listdir("train/") for idx, i in enumerate(os.listdir("train/")):`'
- en: '`if "cat" in i:`'
  id: totrans-805
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`if "cat" in i:`'
- en: '`train_df["label"][idx] = 0`'
  id: totrans-806
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_df["label"][idx] = 0`'
- en: '`if "dog" in i:`'
  id: totrans-807
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`if "dog" in i:`'
- en: '`train_df["label"][idx] = 1`'
  id: totrans-808
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_df["label"][idx] = 1`'
- en: '`train_df.to_csv (r''train_csv.csv'', index = False, header=True)`Define a
    function that will stack the data and return it as NumPy arrays.'
  id: totrans-809
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_df.to_csv (r''train_csv.csv'', index = False, header=True)`定义一个函数，将堆叠数据并以
    NumPy 数组形式返回。'
- en: '`import numpy as np`'
  id: totrans-810
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import numpy as np`'
- en: def `custom_collate_fn(batch)`
  id: totrans-811
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: def `custom_collate_fn(batch)`
- en: transposed_data = `list(zip(*batch))`
  id: totrans-812
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: transposed_data = `list(zip(*batch))`
- en: labels = `np.array(transposed_data[1])`
  id: totrans-813
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: labels = `np.array(transposed_data[1])`
- en: imgs = `np.stack(transposed_data[0])`
  id: totrans-814
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: imgs = `np.stack(transposed_data[0])`
- en: return `imgs, labels`
  id: totrans-815
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: return `imgs, labels`
- en: We are now ready to define the training and test data and use that with the
    PyTorch DataLoader. We also define a PyTorch transformation for resizing the images.
  id: totrans-816
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 现在我们准备好定义训练和测试数据，并使用 PyTorch 的 DataLoader。我们还定义了一个用于调整图像大小的 PyTorch 转换。
- en: import `torch`
  id: totrans-817
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: import `torch`
- en: from `torch.utils.data import DataLoader` from `torchvision import transforms`
    import `numpy as np`
  id: totrans-818
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: from `torch.utils.data import DataLoader` from `torchvision import transforms`
    import `numpy as np`
- en: size_image = 64 batch_size = 32
  id: totrans-819
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: size_image = 64 batch_size = 32
- en: transform = `transforms.Compose([`
  id: totrans-820
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: transform = `transforms.Compose([`
- en: transforms.Resize((size_image,size_image)),
  id: totrans-821
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: transforms.Resize((size_image,size_image)),
- en: '`np.array]`'
  id: totrans-822
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`np.array]`'
- en: dataset = `CatsDogsDataset("train","train_csv.csv",transform=tra nsform)`
  id: totrans-823
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: dataset = `CatsDogsDataset("train","train_csv.csv",transform=tra nsform)`
- en: train_set, validation_set = `torch.utils.data.random_split(datas et,[20000,5000])`
  id: totrans-824
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 训练集、验证集 = `torch.utils.data.random_split(datas et,[20000,5000])`
- en: train_loader = `DataLoader(dataset=train_set, collate_fn=custom_ collate_fn,shuffle=True,
    batch_size=batch_size)`
  id: totrans-825
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: train_loader = `DataLoader(dataset=train_set, collate_fn=custom_collate_fn,shuffle=True,
    batch_size=batch_size)`
- en: validation_loader = `DataLoader(dataset=validation_set,collate_f n=custom_collate_fn,
    shuffle=False, batch_size=batch_size)`
  id: totrans-826
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: validation_loader = `DataLoader(dataset=validation_set,collate_f n=custom_collate_fn,
    shuffle=False, batch_size=batch_size)`
- en: Define Convolution Neural Network with Flax
  id: totrans-827
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 使用 Flax 定义卷积神经网络
- en: Install Flax to create a simple neural network.`pip install flax`
  id: totrans-828
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 安装 Flax 来创建一个简单的神经网络。`pip install flax`
- en: Networks are created in Flax using the Linen API by
  id: totrans-829
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在 Flax 中使用 Linen API 创建网络
- en: subclassing Module.  All Flax modules are Python dataclasses. This means that
    they have  [`__.init__`] by default. You should, therefore, override [`setup()`]
    instead to initialize the network. However, you can use the compact wrapper
  id: totrans-830
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 子类化 Module。所有的 Flax 模块都是 Python 的 dataclasses。这意味着它们默认具有 `__init__` 方法。因此，你应该覆盖
    `setup()` 方法来初始化网络。但是，你可以使用紧凑的包装器
- en: to make the model definition more concise.
  id: totrans-831
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 使模型定义更加简洁。
- en: import `flaxfrom flax import linen as nnclass CNN(nn.Module):`
  id: totrans-832
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 导入 `flaxfrom flax import linen as nnclass CNN(nn.Module):`
- en: '@`nn.compact`'
  id: totrans-833
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '@`nn.compact`'
- en: def `__call__(self, x):`
  id: totrans-834
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: def `__call__(self, x):`
- en: x = `nn.Conv(features=32, kernel_size=(3, 3))(x)`
  id: totrans-835
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.Conv(features=32, kernel_size=(3, 3))(x)`
- en: x = `nn.relu(x)`
  id: totrans-836
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.relu(x)`
- en: x = `nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))`
  id: totrans-837
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))`
- en: x = `nn.Conv(features=64, kernel_size=(3, 3))(x)`
  id: totrans-838
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.Conv(features=64, kernel_size=(3, 3))(x)`
- en: x = `nn.relu(x)`
  id: totrans-839
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.relu(x)`
- en: x = `nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))`
  id: totrans-840
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))`
- en: x = `x.reshape((x.shape[0], -1))`
  id: totrans-841
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `x.reshape((x.shape[0], -1))`
- en: x = `nn.Dense(features=256)(x)`
  id: totrans-842
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.Dense(features=256)(x)`
- en: x = `nn.relu(x)`
  id: totrans-843
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.relu(x)`
- en: x = `nn.Dense(features=2)(x)`
  id: totrans-844
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.Dense(features=2)(x)`
- en: x = `nn.log_softmax(x)`
  id: totrans-845
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.log_softmax(x)`
- en: return x
  id: totrans-846
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 返回 x
- en: Define loss
  id: totrans-847
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 定义损失
- en: The loss can be computed using the Optax package. We one-hot encode the integer
    labels before passing them to the softmax crossentropy function. num_classes is
    2 because we are dealing with two classes.
  id: totrans-848
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 可以使用 Optax 包计算损失。我们在传递给 softmax 交叉熵函数之前对整数标签进行了 one-hot 编码。num_classes 为 2，因为我们处理的是两类问题。
- en: import `optax`
  id: totrans-849
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 导入 `optax`
- en: def `cross_entropy_loss(*, logits, labels)`
  id: totrans-850
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: def `cross_entropy_loss(*, logits, labels)`
- en: labels_onehot = `jax.nn.one_hot(labels, num_classes=2)`
  id: totrans-851
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: labels_onehot = `jax.nn.one_hot(labels, num_classes=2)`
- en: return `optax.softmax_cross_entropy(logits=logits, labels=labe ls_onehot).mean()`
  id: totrans-852
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 返回 `optax.softmax_cross_entropy(logits=logits, labels=labels_onehot).mean()`
- en: Compute metrics
  id: totrans-853
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 计算指标
- en: Next, we define a function that will use the above loss function to compute
    and return the loss. We also compute the accuracy in the same function.
  id: totrans-854
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个函数，使用上述损失函数计算并返回损失。我们还在同一个函数中计算准确率。
- en: def `compute_metrics(*, logits, labels):`
  id: totrans-855
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: def `compute_metrics(*, logits, labels):`
- en: loss = `cross_entropy_loss(logits=logits, labels=labels)` accuracy = `jnp.mean(jnp.argmax(logits,
    -1) == labels)` metrics = {
  id: totrans-856
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: loss = `cross_entropy_loss(logits=logits, labels=labels)` accuracy = `jnp.mean(jnp.argmax(logits,
    -1) == labels)` metrics = {
- en: '''loss'': loss,'
  id: totrans-857
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '''loss'': loss,'
- en: '`''accuracy'': accuracy,`'
  id: totrans-858
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`''accuracy'': accuracy,`'
- en: '}'
  id: totrans-859
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '}'
- en: return metrics
  id: totrans-860
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 返回指标
- en: Create training state
  id: totrans-861
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 创建训练状态
- en: A training state holds the model variables such as parameters and optimizer
    state. These variables are modified at each iteration using the optimizer. You
    can subclass `[flax.training.train_state]` to track more data. You might want
    to do that for tracking the state of dropout and batch statistics if you include
    those layers in your model. For this simple model, the default class will suffice.
  id: totrans-862
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 训练状态保存模型变量，如参数和优化器状态。这些变量在每次迭代中使用优化器进行修改。如果在模型中包含 dropout 和批处理统计信息，你可以子类化 `flax.training.train_state`
    来跟踪更多数据。对于这个简单的模型，默认的类就足够了。
- en: from `flax.training` import `train_state`
  id: totrans-863
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 从 `flax.training` 导入 `train_state`
- en: 'def `create_train_state`(rng, learning_rate, momentum): """Creates initial
    `TrainState`."""'
  id: totrans-864
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def `create_train_state`(rng, learning_rate, momentum): """Creates initial
    `TrainState`."""'
- en: '`cnn = CNN()`'
  id: totrans-865
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`cnn = CNN()`'
- en: params = `cnn.init`(rng, jnp.ones([1, size_image, size_image,
  id: totrans-866
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: params = `cnn.init`(rng, jnp.ones([1, size_image, size_image,
- en: '`3`]))[''params'']'
  id: totrans-867
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`3`]))[''params'']'
- en: tx = `optax.sgd`(learning_rate, momentum)
  id: totrans-868
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: tx = `optax.sgd`(learning_rate, momentum)
- en: return `train_state.TrainState.create`(
  id: totrans-869
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 返回 `train_state.TrainState.create`(
- en: apply_fn=`cnn.apply`, params=`params`, tx=`tx`)
  id: totrans-870
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: apply_fn=`cnn.apply`, params=`params`, tx=`tx`)
- en: '**Define training step**'
  id: totrans-871
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**定义训练步骤**'
- en: In this function, we evaluate the model with a set of input images using the `Apply` method.
    We use the obtained logits to compute the loss. We then use  `value_and_grad`  to
    evaluate the loss function and its gradient. The gradients are then used to update
    the model parameters. Finally, it uses the `[compute_metrics]` function defined
    above to calculate the loss and accuracy.
  id: totrans-872
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在这个函数中，我们使用 `Apply` 方法对一组输入图像评估模型。我们使用获取的logits计算损失。然后我们使用 `value_and_grad` 来评估损失函数及其梯度。然后使用梯度来更新模型参数。最后，它使用上面定义的 `[compute_metrics]` 函数来计算损失和准确率。
- en: 'def `compute_loss`(params,images,labels):'
  id: totrans-873
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def `compute_loss`(params,images,labels):'
- en: 'logits = `CNN().apply`({''params'': params}, images) loss = `cross_entropy_loss`(logits=`logits`,
    labels=`labels`) return `loss`, `logits`'
  id: totrans-874
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'logits = `CNN().apply`({''params'': params}, images) loss = `cross_entropy_loss`(logits=`logits`,
    labels=`labels`) return `loss`, `logits`'
- en: '@`jax.jit`'
  id: totrans-875
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '@`jax.jit`'
- en: 'def `train_step`(state,images, labels):'
  id: totrans-876
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def `train_step`(state,images, labels):'
- en: '"""Train for a single step."""'
  id: totrans-877
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""单步训练。"""'
- en: (_, logits), grads = jax.value_and_grad(`compute_loss`, has_aux =`True`)(state.params,images,labels)
  id: totrans-878
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: (_, logits), grads = jax.value_and_grad(`compute_loss`, has_aux =`True`)(state.params,images,labels)
- en: state = `state.apply_gradients`(grads=`grads`)
  id: totrans-879
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: state = `state.apply_gradients`(grads=`grads`)
- en: metrics = `compute_metrics`(logits=`logits`, labels=`labels`)
  id: totrans-880
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: metrics = `compute_metrics`(logits=`logits`, labels=`labels`)
- en: return `state`, `metrics`
  id: totrans-881
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: return `state`, `metrics`
- en: The function is decorated with the @`Jit` decorator to trace the function and
    compile just-in-time for faster computation.
  id: totrans-882
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 函数被 @`Jit` 修饰，以跟踪函数并即时编译，以提高计算速度。
- en: '**Define evaluation step**'
  id: totrans-883
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**定义评估步骤**'
- en: The evaluation function will use `[Apply]`  to evaluate the model on the test
    data.
  id: totrans-884
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 评估函数将使用 `[Apply]`  来在测试数据上评估模型。
- en: '@`jax.jit`'
  id: totrans-885
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '@`jax.jit`'
- en: 'def `eval_step`(state, images, labels):'
  id: totrans-886
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def `eval_step`(state, images, labels):'
- en: 'logits = `CNN().apply`({''params'': state.params}, images)'
  id: totrans-887
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'logits = `CNN().apply`({''params'': state.params}, images)'
- en: return `compute_metrics`(logits=`logits`, labels=`labels`)
  id: totrans-888
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: return `compute_metrics`(logits=`logits`, labels=`labels`)
- en: '**Training function**'
  id: totrans-889
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**训练函数**'
- en: In this function, we apply the training step we defined above.  We loop through
    each batch in the data loader and perform optimization for each batch. We use
    the `[jax.device_get]` to get the metrics and compute the mean.
  id: totrans-890
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在此函数中，我们应用了上面定义的训练步骤。我们遍历数据加载器中的每个批次并对每个批次进行优化。我们使用 `[jax.device_get]` 来获取指标并计算均值。
- en: 'def `train_one_epoch`(state, dataloader):'
  id: totrans-891
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def `train_one_epoch`(state, dataloader):'
- en: '"""Train for 1 epoch on the training set.""" batch_metrics = []'
  id: totrans-892
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""在训练集上训练 1 个 epoch。""" batch_metrics = []'
- en: 'for cnt, (images, labels) in enumerate(dataloader):'
  id: totrans-893
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 对于 cnt, (images, labels) 在 dataloader 中的每一个枚举：
- en: images = images / `255.0`
  id: totrans-894
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: images = images / `255.0`
- en: state, metrics = `train_step`(state, images, labels) batch_metrics.append(metrics)
  id: totrans-895
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: state, metrics = `train_step`(state, images, labels) batch_metrics.append(metrics)
- en: 'batch_metrics_np = jax.device_get(batch_metrics) epoch_metrics_np = {k: np.mean([metrics[k]
    for metrics in batch_metrics_n p])for k in batch_metrics_np[0] }return `state`,
    `epoch_metrics_np`'
  id: totrans-896
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'batch_metrics_np = jax.device_get(batch_metrics) epoch_metrics_np = {k: np.mean([metrics[k]
    for metrics in batch_metrics_n p])for k in batch_metrics_np[0] }return `state`,
    `epoch_metrics_np`'
- en: '**Evaluate the model**'
  id: totrans-897
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**评估模型**'
- en: The evaluation function runs the evaluation step and returns the test metrics.
  id: totrans-898
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 评估函数运行评估步骤并返回测试指标。
- en: 'def `evaluate_model`(state, test_imgs, test_lbls): """Evaluate on the validation
    set."""'
  id: totrans-899
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def `evaluate_model`(state, test_imgs, test_lbls): """在验证集上评估。"""'
- en: metrics = `eval_step`(state, test_imgs, test_lbls) metrics = jax.device_get(metrics)
  id: totrans-900
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: metrics = `eval_step`(state, test_imgs, test_lbls) metrics = jax.device_get(metrics)
- en: '`metrics = jax.tree_map(lambda x: x.item(), metrics) return metrics`'
  id: totrans-901
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`metrics = jax.tree_map(lambda x: x.item(), metrics) return metrics`'
- en: Train and evaluate the model
  id: totrans-902
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 训练和评估模型
- en: We need to initialize the train state before training the model. The function
    to initialize the state requires a pseudo-random number (PRNG) key. Use the [PRNGKey] function
    to obtain a key and split it to get another key that you'll use for parameter
    initialization. Follow this link to learn more about JAX PRNG Design.
  id: totrans-903
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在训练模型之前，我们需要初始化训练状态。初始化状态函数需要一个伪随机数（PRNG）密钥。使用 [PRNGKey] 函数获取密钥并拆分它以获得另一个用于参数初始化的密钥。点击此链接了解更多关于
    JAX PRNG 设计的信息。
- en: Pass this key to the [create_train_state] function together with the learning
    rate and momentum. You can now use the [train_one_epoch] function to train the
    model and the eval_modelfunction to evaluate the model.
  id: totrans-904
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 将此密钥与学习率和动量一起传递给 [create_train_state] 函数。现在可以使用 [train_one_epoch] 函数训练模型以及 eval_model
    函数评估模型。
- en: '`import jaxrng = jax.random.PRNGKey(0)rng, init_rng = jax.random.split(rng)`'
  id: totrans-905
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import jaxrng = jax.random.PRNGKey(0)rng, init_rng = jax.random.split(rng)`'
- en: '`learning_rate = 0.1 momentum = 0.9`'
  id: totrans-906
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`learning_rate = 0.1 momentum = 0.9`'
- en: '`seed = 0`'
  id: totrans-907
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`seed = 0`'
- en: '`state = create_train_state(init_rng, learning_rate, momentum) del init_rng
    # Must not be used anymore.`'
  id: totrans-908
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`state = create_train_state(init_rng, learning_rate, momentum) del init_rng
    # 不再使用。`'
- en: '`num_epochs = 30`'
  id: totrans-909
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`num_epochs = 30`'
- en: '`(test_images, test_labels) = next(iter(validation_loader)) test_images = test_images
    / 255.0`'
  id: totrans-910
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`(test_images, test_labels) = next(iter(validation_loader)) test_images = test_images
    / 255.0`'
- en: '`state = create_train_state(jax.random.PRNGKey(seed), learning_r ate, momentum)`'
  id: totrans-911
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`state = create_train_state(jax.random.PRNGKey(seed), learning_r ate, momentum)`'
- en: '`training_loss = []`'
  id: totrans-912
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`training_loss = []`'
- en: '`training_accuracy = []`'
  id: totrans-913
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`training_accuracy = []`'
- en: '`testing_loss = []`'
  id: totrans-914
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_loss = []`'
- en: '`testing_accuracy = []`'
  id: totrans-915
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_accuracy = []`'
- en: '`for epoch in range(1, num_epochs + 1):`'
  id: totrans-916
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`for epoch in range(1, num_epochs + 1):`'
- en: '`train_state, train_metrics = train_one_epoch(state, train_l oader)`'
  id: totrans-917
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_state, train_metrics = train_one_epoch(state, train_l oader)`'
- en: '`training_loss.append(train_metrics[''loss''])`'
  id: totrans-918
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`training_loss.append(train_metrics[''loss''])`'
- en: '`training_accuracy.append(train_metrics[''accuracy''])`'
  id: totrans-919
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`training_accuracy.append(train_metrics[''accuracy''])`'
- en: '`print(f"Train epoch: {epoch}, loss: {train_metrics[''los s'']}, accuracy:
    {train_metrics[''accuracy''] * 100}")`'
  id: totrans-920
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`print(f"训练周期：{epoch}，损失：{train_metrics[''loss'']}，准确率：{train_metrics[''accuracy'']
    * 100}")`'
- en: '`test_metrics = evaluate_model(train_state, test_images, tes t_labels)`'
  id: totrans-921
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`test_metrics = evaluate_model(train_state, test_images, tes t_labels)`'
- en: '`testing_loss.append(test_metrics[''loss''])`'
  id: totrans-922
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_loss.append(test_metrics[''loss''])`'
- en: '`testing_accuracy.append(test_metrics[''accuracy''])`'
  id: totrans-923
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_accuracy.append(test_metrics[''accuracy''])`'
- en: '`print(f"Test epoch: {epoch}, loss: {test_metrics[''loss'']}, accuracy: {test_metrics[''accuracy'']
    * 100}")`'
  id: totrans-924
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`print(f"测试周期：{epoch}，损失：{test_metrics[''loss'']}，准确率：{test_metrics[''accuracy'']
    * 100}")`'
- en: Model performance
  id: totrans-925
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 模型性能
- en: While the training is happening, we print the training and validation metrics.
    You can also use the resulting metrics to plot training and validation charts.
  id: totrans-926
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在训练过程中，我们会打印训练和验证指标。您也可以使用这些指标绘制训练和验证图表。
- en: '`import matplotlib.pyplot as plt`'
  id: totrans-927
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import matplotlib.pyplot as plt`'
- en: '`plt.plot(training_accuracy, label="Training") plt.plot(testing_accuracy, label="Test")
    plt.xlabel("Epoch")`'
  id: totrans-928
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.plot(training_accuracy, label="Training") plt.plot(testing_accuracy, label="Test")
    plt.xlabel("Epoch")`'
- en: '`plt.ylabel("Accuracy")`'
  id: totrans-929
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.ylabel("Accuracy")`'
- en: '`plt.legend()`'
  id: totrans-930
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.legend()`'
- en: '`plt.show()`'
  id: totrans-931
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.show()`'
- en: '`plt.plot(training_loss, label="Training") plt.plot(testing_loss, label="Test")
    plt.xlabel("Epoch")`'
  id: totrans-932
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.plot(training_loss, label="Training") plt.plot(testing_loss, label="Test")
    plt.xlabel("Epoch")`'
- en: '`plt.ylabel("Accuracy")`'
  id: totrans-933
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.ylabel("Accuracy")`'
- en: '`plt.legend()`'
  id: totrans-934
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.legend()`'
- en: '`plt.show()`'
  id: totrans-935
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.show()`'
- en: Final thoughts
  id: totrans-936
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 总结思考
- en: In this article, we have seen how to set up a simple neural network with Flax
    and train it on the CPU.
  id: totrans-937
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在本文中，我们看到了如何在Flax上设置一个简单的神经网络并在CPU上训练它。
- en: Distributed training with JAX & Flax
  id: totrans-938
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 使用JAX和Flax进行分布式训练
- en: Training models on accelerators with JAX and Flax differs slightly from training
    with CPU. For instance, the data needs to be replicated in the different devices
    when using multiple accelerators. After that, we need to execute the training
    on multiple devices and aggregate the results. Flax supports TPU and GPU accelerators.
  id: totrans-939
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 使用JAX和Flax在加速器上训练模型与在CPU上训练略有不同。例如，在使用多个加速器时，数据需要在不同设备之间复制。之后，我们需要在多个设备上执行训练并聚合结果。Flax支持TPU和GPU加速器。
- en: In the last article, we saw how to train models with the CPU. This article will
    focus on training models with Flax and JAX using GPUs and TPU.
  id: totrans-940
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在上一篇文章中，我们看到了如何使用CPU训练模型。本文将专注于使用Flax和JAX在GPU和TPU上训练模型。
- en: Perform standard imports
  id: totrans-941
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 执行标准导入
- en: You'll need to install Flax for this illustration.pip install flaxLet's import
    all the packages we'll use in this project.
  id: totrans-942
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 为了进行演示，您需要安装Flax。可以通过pip install flax进行安装。让我们导入这个项目中将要使用的所有包。
- en: '`import wget`'
  id: totrans-943
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import wget`'
- en: '`import zipfile`'
  id: totrans-944
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import zipfile`'
- en: '`import torch`'
  id: totrans-945
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import torch`'
- en: '`from torch.utils.data import DataLoader import os`'
  id: totrans-946
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from torch.utils.data import DataLoader import os`'
- en: '`from PIL import Image`'
  id: totrans-947
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from PIL import Image`'
- en: '`from torchvision import transforms from torch.utils.data import Dataset import
    numpy as np`'
  id: totrans-948
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from torchvision import transforms from torch.utils.data import Dataset import
    numpy as np`'
- en: '`import pandas as pd`'
  id: totrans-949
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import pandas as pd`'
- en: '`import matplotlib.pyplot as plt`'
  id: totrans-950
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import matplotlib.pyplot as plt`'
- en: '`import functools`'
  id: totrans-951
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import functools`'
- en: '`import time`'
  id: totrans-952
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import time`'
- en: '`from tqdm.notebook import tqdm`'
  id: totrans-953
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from tqdm.notebook import tqdm`'
- en: ignore harmless warnings
  id: totrans-954
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: 忽略无害的警告
- en: '`import warnings`'
  id: totrans-955
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import warnings`'
- en: '`warnings.filterwarnings("ignore") warnings.simplefilter(''ignore'') import
    jax`'
  id: totrans-956
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`warnings.filterwarnings("ignore") warnings.simplefilter(''ignore'') import
    jax`'
- en: '`from jax import numpy as jnp`'
  id: totrans-957
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from jax import numpy as jnp`'
- en: '`from flax import linen as nn`'
  id: totrans-958
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from flax import linen as nn`'
- en: '`from flax.training import train_state import optax`'
  id: totrans-959
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from flax.training import train_state import optax`'
- en: '`import math`'
  id: totrans-960
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import math`'
- en: '`from flax import jax_utils`'
  id: totrans-961
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from flax import jax_utils`'
- en: '`import jax.tools.colab_tpu`'
  id: totrans-962
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import jax.tools.colab_tpu`'
- en: '**Setup TPUs on Colab**'
  id: totrans-963
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**在Colab上设置TPUs**'
- en: Change the runtime on Colab to TPUs. Next, run the code below to set up `JAX` to
    use TPUs.`jax.tools.colab_tpu.setup_tpu() jax.devices()![](../images/00031.gif)`
  id: totrans-964
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在 Colab 上切换到 TPUs 运行时。接下来，运行以下代码设置`JAX`以使用 TPUs。`jax.tools.colab_tpu.setup_tpu()
    jax.devices()![](../images/00031.gif)`
- en: '**Download the dataset**'
  id: totrans-965
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**下载数据集**'
- en: 'We''ll use the `cats and dogs dataset` from Kaggle. Let''s start by downloading
    and extracting it.`import wget # pip install wget import zipfile`'
  id: totrans-966
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '我们将使用来自 Kaggle 的`猫和狗数据集`。让我们从下载和解压开始。`import wget # pip install wget import
    zipfile`'
- en: '`wget.download("https://ml.machinelearningnuggets.com/train.zi p")`'
  id: totrans-967
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`wget.download("https://ml.machinelearningnuggets.com/train.zi p")`'
- en: '`with zipfile.ZipFile(''train.zip'', ''r'') as zip_ref:`'
  id: totrans-968
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`with zipfile.ZipFile(''train.zip'', ''r'') as zip_ref:`'
- en: '`zip_ref.extractall(''.'')`'
  id: totrans-969
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`zip_ref.extractall(''.'')`'
- en: '**Load the dataset**'
  id: totrans-970
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**加载数据集**'
- en: We'll use existing data loaders to load the data since `JAX` and Flax don't
    ship with any data loaders. In this case, let's use `PyTorch` to load the dataset.
    The first step is to set up a dataset class.
  id: totrans-971
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 由于`JAX`和Flax没有任何数据加载器，我们将使用现有的数据加载器来加载数据。在这种情况下，让我们使用`PyTorch`来加载数据集。第一步是设置一个数据集类。
- en: class `CatsDogsDataset(Dataset):`
  id: totrans-972
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`CatsDogsDataset`类：'
- en: '`def __init__(self, root_dir, annotation_file, transform=Non`'
  id: totrans-973
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`__init__`方法：'
- en: '`e):`'
  id: totrans-974
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`e):`'
- en: '`self.root_dir = root_dir`'
  id: totrans-975
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.root_dir = root_dir`'
- en: '`self.annotations = pd.read_csv(annotation_file) self.transform = transform`'
  id: totrans-976
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.annotations = pd.read_csv(annotation_file) self.transform = transform`'
- en: '`def __len__(self):return len(self.annotations)`'
  id: totrans-977
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`__len__`方法：'
- en: '`def __getitem__(self, index):`'
  id: totrans-978
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`__getitem__`方法：'
- en: '`img_id = self.annotations.iloc[index, 0]`'
  id: totrans-979
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`img_id = self.annotations.iloc[index, 0]`'
- en: '`img = Image.open(os.path.join(self.root_dir, img_id)).c`'
  id: totrans-980
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`img = Image.open(os.path.join(self.root_dir, img_id)).c`'
- en: '`onvert("RGB")`'
  id: totrans-981
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`onvert("RGB")`'
- en: '`y_label = torch.tensor(float(self.annotations.iloc[inde`'
  id: totrans-982
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`y_label = torch.tensor(float(self.annotations.iloc[inde`'
- en: '`x, 1]))`'
  id: totrans-983
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x, 1]))`'
- en: '`if self.transform is not None: img = self.transform(img)return (img, y_label)Next,
    we create a DataFrame containing the categories.`'
  id: totrans-984
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`if self.transform is not None: img = self.transform(img)return (img, y_label)接下来，我们创建一个包含类别的
    DataFrame。`'
- en: '`train_df = pd.DataFrame(columns=["img_path","label"]) train_df["img_path"]
    = os.listdir("train/") for idx, i in enumerate(os.listdir("train/")):`'
  id: totrans-985
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_df = pd.DataFrame(columns=["img_path","label"]) train_df["img_path"]
    = os.listdir("train/") for idx, i in enumerate(os.listdir("train/")):`'
- en: '`if "cat" in i:`'
  id: totrans-986
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`if "cat" in i:`'
- en: '`train_df["label"][idx] = 0`'
  id: totrans-987
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_df["label"][idx] = 0`'
- en: '`if "dog" in i:`'
  id: totrans-988
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`if "dog" in i:`'
- en: '`train_df["label"][idx] = 1`'
  id: totrans-989
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_df["label"][idx] = 1`'
- en: '`train_df.to_csv (r''train_csv.csv'', index = False, header=True)`'
  id: totrans-990
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_df.to_csv (r''train_csv.csv'', index = False, header=True)`'
- en: We then use the dataset class to create training and testing data. We also apply
    a custom function to return the data as NumPy arrays. Later, we'll use this `train_loader`
  id: totrans-991
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 然后，我们使用数据集类创建训练和测试数据。我们还应用一个自定义函数将数据返回为`NumPy`数组。稍后，我们将使用这个`train_loader`
- en: when training the model. We'll then evaluate it on a batch of the test data.
  id: totrans-992
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在训练模型时。然后我们将在一批测试数据上评估它。
- en: '`def custom_collate_fn(batch):`'
  id: totrans-993
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`custom_collate_fn`函数：'
- en: '`transposed_data = list(zip(*batch)) labels = np.array(transposed_data[1])
    imgs = np.stack(transposed_data[0]) return imgs, labels`'
  id: totrans-994
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`transposed_data = list(zip(*batch)) labels = np.array(transposed_data[1])
    imgs = np.stack(transposed_data[0]) return imgs, labels`'
- en: '`size_image = 224 batch_size = 64`'
  id: totrans-995
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`size_image = 224 batch_size = 64`'
- en: '`transform = transforms.Compose([`'
  id: totrans-996
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`transform = transforms.Compose([`'
- en: '`transforms.Resize((size_image,size_image)),`'
  id: totrans-997
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`transforms.Resize((size_image,size_image)),`'
- en: '`np.array])`'
  id: totrans-998
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`np.array])`'
- en: '`dataset = CatsDogsDataset("train","train_csv.csv",transform=tra nsform)`'
  id: totrans-999
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`dataset = CatsDogsDataset("train","train_csv.csv",transform=tra nsform)`'
- en: '`train_set, validation_set = torch.utils.data.random_split(datas et,[20000,5000])`'
  id: totrans-1000
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_set, validation_set = torch.utils.data.random_split(datas et,[20000,5000])`'
- en: '`train_loader = DataLoader(dataset=train_set, collate_fn=custom_ collate_fn,shuffle=True,
    batch_size=batch_size)`'
  id: totrans-1001
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_loader = DataLoader(dataset=train_set, collate_fn=custom_ collate_fn,shuffle=True,
    batch_size=batch_size)`'
- en: '`validation_loader = DataLoader(dataset=validation_set,collate_f n=custom_collate_fn,
    shuffle=False, batch_size=batch_size)`'
  id: totrans-1002
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`validation_loader = DataLoader(dataset=validation_set,collate_f n=custom_collate_fn,
    shuffle=False, batch_size=batch_size)`'
- en: Define the model with Flax
  id: totrans-1003
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 使用 Flax 定义模型
- en: In Flax, models are defined using the Linen API. It provides the building blocks
    for defining convolution layers, dropout, etc.
  id: totrans-1004
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在 Flax 中，模型是使用 Linen API 定义的。它提供了定义卷积层、dropout 等的基本构件。
- en: Networks are created by subclassing `[Module]`.   Flax allows you to define
    your networks using `[setup]` or `[nn.compact]`. Both approaches behave the same
    way but `[nn.compact]`
  id: totrans-1005
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 网络通过子类化`[Module]`创建。Flax 允许您使用`[setup]`或`[nn.compact]`定义网络。这两种方法的行为相同，但`[nn.compact]`
- en: is more concise.
  id: totrans-1006
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 更简洁。
- en: Create training state
  id: totrans-1007
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 创建训练状态
- en: We now need to create parallel versions of our functions. Parallelization in JAX is
    done using
  id: totrans-1008
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 现在我们需要创建函数的并行版本。在 JAX 中，使用并行化
- en: the `[pmap]` function. `[pmap]` compiles a function with XLA and executes it
    on multiple devices.
  id: totrans-1009
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[pmap]` 函数。 `[pmap]` 会使用 XLA 编译函数并在多个设备上执行。'
- en: '```python'
  id: totrans-1010
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '```python'
- en: cnn = CNN()
  id: totrans-1011
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`cnn = CNN()`'
- en: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image, 3]))[''params'']`'
  id: totrans-1012
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image, 3]))[''params'']`'
- en: '`tx = optax.sgd(learning_rate, momentum)`'
  id: totrans-1013
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`tx = optax.sgd(learning_rate, momentum)`'
- en: '`return train_state.TrainState.create(`'
  id: totrans-1014
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return train_state.TrainState.create(`'
- en: '`apply_fn=cnn.apply, params=params, tx=tx)`'
  id: totrans-1015
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`apply_fn=cnn.apply, params=params, tx=tx)`'
- en: Apply the model
  id: totrans-1016
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 应用模型
- en: The next step is to define
  id: totrans-1017
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 下一步是定义
- en: parallel `apply_model` and `update_model`functions.
  id: totrans-1018
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 并行应用 `apply_model` 和 `update_model` 函数。
- en: 'The `[apply_model]` function:'
  id: totrans-1019
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[apply_model]` 函数：'
- en: Computes the loss.
  id: totrans-1020
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 计算损失。
- en: Computes predictions from all devices by calculating the average of the probabilities
    using `[jax.lax.pmean()]`.```python
  id: totrans-1021
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 通过计算使用 `[jax.lax.pmean()]` 计算所有设备的概率平均值来生成预测。```python
- en: 'logits = CNN().apply({''params'': params}, images) one_hot = jax.nn.one_hot(labels,
    2)'
  id: totrans-1022
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'logits = CNN().apply({''params'': params}, images) one_hot = jax.nn.one_hot(labels,
    2)'
- en: '`loss = optax.softmax_cross_entropy(logits=logits, labels=on`'
  id: totrans-1023
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`loss = optax.softmax_cross_entropy(logits=logits, labels=on`'
- en: e_hot).mean()return loss, logits
  id: totrans-1024
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: e_hot).mean()return loss, logits
- en: '`grad_fn = jax.value_and_grad(loss_fn, has_aux=True) (loss, logits), grads
    = grad_fn(state.params)`'
  id: totrans-1025
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`grad_fn = jax.value_and_grad(loss_fn, has_aux=True) (loss, logits), grads
    = grad_fn(state.params)`'
- en: '`probs = jax.lax.pmean(jax.nn.softmax(logits), axis_name=''ense`'
  id: totrans-1026
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`probs = jax.lax.pmean(jax.nn.softmax(logits), axis_name=''ense`'
- en: '```python'
  id: totrans-1027
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '```python'
- en: Notice the use of the `[axis_name]`. You can give this any name. You'll need
    to specify that when computing the mean of the probabilities and accuracies.
  id: totrans-1028
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 注意使用了 `[axis_name]`。你可以随意指定名称。在计算概率和准确率的平均值时，需要指定这个名称。
- en: The `[update_model]` function updates the model parameters.
  id: totrans-1029
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[update_model]` 函数更新模型参数。'
- en: Training function
  id: totrans-1030
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 训练函数
- en: 'The next step is to define the model training function. In the function, we:'
  id: totrans-1031
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 下一步是定义模型训练函数。在该函数中，我们：
- en: Replicate the training data at batch level
  id: totrans-1032
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在批级别上复制训练数据
- en: using `jax_utils.replicat e`.
  id: totrans-1033
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 使用 `jax_utils.replicate`。
- en: '`[apply_model]` to the replicated data.'
  id: totrans-1034
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[apply_model]` 应用于复制的数据。'
- en: Obtain the epoch loss and accuracy and unreplicate them
  id: totrans-1035
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 获取 epoch 损失和准确率，并对其进行解复制
- en: using `jax_utils.unreplicate`.
  id: totrans-1036
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 使用 `jax_utils.unreplicate`。
- en: Compute the mean of the loss and accuracy.
  id: totrans-1037
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 计算损失和准确率的均值。
- en: '`[apply_model]` to the test data and obtain test metrics.'
  id: totrans-1038
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[apply_model]` 应用于测试数据并获取测试指标。'
- en: Print the training and evaluation metrics per epoch. Append the training and
    test metrics to lists for visualization later.
  id: totrans-1039
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 打印每个 epoch 的训练和评估指标。将训练和测试指标附加到列表中，以便稍后可视化。
- en: '```python'
  id: totrans-1040
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '```python'
- en: '`epoch_loss = []`'
  id: totrans-1041
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`epoch_loss = []`'
- en: '`epoch_accuracy = []`'
  id: totrans-1042
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`epoch_accuracy = []`'
- en: '`testing_accuracy = []`'
  id: totrans-1043
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_accuracy = []`'
- en: '`testing_loss = []`'
  id: totrans-1044
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_loss = []`'
- en: '`for epoch in range(num_epochs):`'
  id: totrans-1045
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`for epoch in range(num_epochs):`'
- en: '`for cnt, (images, labels) in tqdm(enumerate(dataloade r), total=(math.ceil(len(train_set)/batch_size))):
    images = images / 255.0`'
  id: totrans-1046
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`for cnt, (images, labels) in tqdm(enumerate(dataloade r), total=(math.ceil(len(train_set)/batch_size))):
    images = images / 255.0`'
- en: '`images` = `jax_utils.replicate(images)`'
  id: totrans-1047
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`images` = `jax_utils.replicate(images)`'
- en: '`labels` = `jax_utils.replicate(labels)`'
  id: totrans-1048
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`labels` = `jax_utils.replicate(labels)`'
- en: '`grads`, `loss`, `accuracy` = `apply_model(state, images`,'
  id: totrans-1049
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`grads`, `loss`, `accuracy` = `apply_model(state, images`,'
- en: '`labels)`'
  id: totrans-1050
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`labels)`'
- en: '`state` = `update_model(state, grads)`'
  id: totrans-1051
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`state` = `update_model(state, grads)`'
- en: '`epoch_loss.append(jax_utils.unreplicate(loss)) epoch_accuracy.append(jax_utils.unreplicate(accuracy))
    train_loss = np.mean(epoch_loss)`'
  id: totrans-1052
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`epoch_loss.append(jax_utils.unreplicate(loss)) epoch_accuracy.append(jax_utils.unreplicate(accuracy))
    train_loss = np.mean(epoch_loss)`'
- en: '`train_accuracy = np.mean(epoch_accuracy)`'
  id: totrans-1053
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_accuracy = np.mean(epoch_accuracy)`'
- en: '`_, test_loss, test_accuracy` = `jax_utils.unreplicate(apply_model(state, test_images,
    test_labels))`'
  id: totrans-1054
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`_, test_loss, test_accuracy` = `jax_utils.unreplicate(apply_model(state, test_images,
    test_labels))`'
- en: '`testing_accuracy.append(test_accuracy)`'
  id: totrans-1055
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_accuracy.append(test_accuracy)`'
- en: '`testing_loss.append(test_loss)`'
  id: totrans-1056
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_loss.append(test_loss)`'
- en: '`print(f"Epoch: {epoch + 1}, train loss: {train_loss:.4 f}, train accuracy:
    {train_accuracy * 100:.4f}, test loss: {tes t_loss:.4f}, test accuracy: {test_accuracy*
    100:.4f}", flush=Tr ue)`'
  id: totrans-1057
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`print(f"Epoch: {epoch + 1}, train loss: {train_loss:.4 f}, train accuracy:
    {train_accuracy * 100:.4f}, test loss: {tes t_loss:.4f}, test accuracy: {test_accuracy*
    100:.4f}", flush=Tr ue)`'
- en: '`return state, epoch_loss, epoch_accuracy, testing_accuracy, testing_loss`'
  id: totrans-1058
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return state, epoch_loss, epoch_accuracy, testing_accuracy, testing_loss`'
- en: Train the model
  id: totrans-1059
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 训练模型
- en: When creating the training state, we generate pseudo-random numbers equivalent
    to the number of devices. We also replicate a small batch of the test data for
    testing. The next step is to run the training function and unpack the training
    and test metrics.
  id: totrans-1060
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 创建训练状态时，我们生成与设备数量相当的伪随机数。我们还为测试复制了一小批测试数据。下一步是运行训练函数并解压缩训练和测试指标。
- en: '`learning_rate = 0.1 momentum = 0.9`'
  id: totrans-1061
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`learning_rate = 0.1 momentum = 0.9`'
- en: '`seed = 0`'
  id: totrans-1062
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`seed = 0`'
- en: '`num_epochs = 30`'
  id: totrans-1063
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`num_epochs = 30`'
- en: '`rng` = `jax.random.PRNGKey(0)`'
  id: totrans-1064
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`rng` = `jax.random.PRNGKey(0)`'
- en: '`rng`, `init_rng` = `jax.random.split(rng)`'
  id: totrans-1065
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`rng`, `init_rng` = `jax.random.split(rng)`'
- en: '`state` = `create_train_state(jax.random.split(init_rng, jax.device_count()),learning_rate,
    momentum)` `del init_rng` # Must not be used anymore.'
  id: totrans-1066
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`state` = `create_train_state(jax.random.split(init_rng, jax.device_count()),learning_rate,
    momentum)` `del init_rng` # 不再使用。'
- en: '`(test_images, test_labels)` = `next(iter(validation_loader))` `test_images
    = test_images / 255.0`'
  id: totrans-1067
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`(test_images, test_labels)` = `next(iter(validation_loader))` `test_images
    = test_images / 255.0`'
- en: '`test_images` = `jax_utils.replicate(test_images)`'
  id: totrans-1068
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`test_images` = `jax_utils.replicate(test_images)`'
- en: '`test_labels` = `jax_utils.replicate(test_labels)`'
  id: totrans-1069
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`test_labels` = `jax_utils.replicate(test_labels)`'
- en: '`start = time.time()`'
  id: totrans-1070
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`start = time.time()`'
- en: '`state`, `epoch_loss`, `epoch_accuracy`, `testing_accuracy`, `testing_loss`
    = `train_one_epoch(state, train_loader,num_epochs)` `print("Total time: ", time.time()
    - start, "seconds")`'
  id: totrans-1071
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`state`, `epoch_loss`, `epoch_accuracy`, `testing_accuracy`, `testing_loss`
    = `train_one_epoch(state, train_loader,num_epochs)` `print("Total time: ", time.time()
    - start, "seconds")`'
- en: '![](../images/00032.jpeg)'
  id: totrans-1072
  prefs: []
  stylish: true
  type: TYPE_IMG
  zh: '![](../images/00032.jpeg)'
- en: Model evaluation
  id: totrans-1073
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 模型评估
- en: The metrics obtained above can be used to plot the metrics.
  id: totrans-1074
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 上面获得的指标可用于绘制指标。
- en: '`plt.plot(epoch_accuracy, label="Training")`'
  id: totrans-1075
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.plot(epoch_accuracy, label="Training")`'
- en: '`plt.plot(testing_accuracy, label="Test")`'
  id: totrans-1076
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.plot(testing_accuracy, label="Test")`'
- en: '`plt.xlabel("Epoch")`'
  id: totrans-1077
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.xlabel("Epoch")`'
- en: '`plt.ylabel("Accuracy")`'
  id: totrans-1078
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.ylabel("Accuracy")`'
- en: '`plt.legend()`'
  id: totrans-1079
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.legend()`'
- en: '`plt.show()`'
  id: totrans-1080
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.show()`'
- en: '![](../images/00033.jpeg)'
  id: totrans-1081
  prefs: []
  stylish: true
  type: TYPE_IMG
  zh: '![](../images/00033.jpeg)'
- en: Final thoughts
  id: totrans-1082
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 最终想法
- en: This article shows how you can use JAX and Flax to train machine learning models
    in parallel on multiple devices. You have seen that the process involves making
    a few functions parallel using JAX's pmap function. We have also covered how to
    replicate the training and test data on multiple devices.
  id: totrans-1083
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 本文展示了如何使用 JAX 和 Flax 在多设备上并行训练机器学习模型。您已经看到该过程涉及使用 JAX 的 pmap 函数将几个函数并行化。我们还介绍了如何在多个设备上复制训练和测试数据。
- en: How to use TensorBoard in JAX & Flax
  id: totrans-1084
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 如何在 JAX 和 Flax 中使用 TensorBoard
- en: Tracking machine learning experiments makes understanding and visualizing the
    model's performance easy. It also makes it possible to spot any problems in the
    network. For example, you can quickly spot overfitting by looking at the training
    and validation charts. You can plot these charts using your favorite charts package,
    such as Matplotlib. However, you can also use more advanced tools such as TensorBoard.
  id: totrans-1085
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 跟踪机器学习实验使理解和可视化模型性能变得简单。它还可以帮助您快速发现网络中的任何问题。例如，通过查看训练和验证图表，您可以快速发现过拟合问题。您可以使用自己喜欢的图表包（如
    Matplotlib）绘制这些图表。但是，您也可以使用更先进的工具，如 TensorBoard。
- en: 'TensorBoard is an open-source library that provides tools for experiment tracking
    in machine learning. You can use TensorBoard for:'
  id: totrans-1086
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: TensorBoard 是一个开源库，提供了机器学习实验跟踪工具。您可以使用 TensorBoard 进行：
- en: Tracking and visualizing model evaluation metrics such as accuracy.
  id: totrans-1087
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 跟踪和可视化模型评估指标，如准确性。
- en: Logging images.
  id: totrans-1088
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 记录图像。
- en: Visualize hyper-parameter tuning.
  id: totrans-1089
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 可视化超参数调整。
- en: Project embeddings such as word embedding in natural language processing problems.
  id: totrans-1090
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 项目嵌入，例如自然语言处理问题中的词嵌入。
- en: Visualize histograms of the model's weights and biases.
  id: totrans-1091
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 可视化模型权重和偏差的直方图。
- en: Plot the architecture of the model.
  id: totrans-1092
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 绘制模型的架构。
- en: Profile the performance of the network.
  id: totrans-1093
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 评估网络性能。
- en: You can use TensorBoard with popular machine learning libraries such as XGBoost, JAX, Flax,
    and PyTorch.
  id: totrans-1094
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 您可以在流行的机器学习库（如 XGBoost、JAX、Flax 和 PyTorch）中使用 TensorBoard。
- en: This article will focus on how to use TensorBoard when building networks with JAX and
    Flax.
  id: totrans-1095
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 本文将重点介绍在使用 JAX 和 Flax 构建网络时如何使用 TensorBoard。
- en: '**How to use TensorBoard**'
  id: totrans-1096
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**如何使用 TensorBoard**'
- en: Let's start by exploring how to use TensorBoard.
  id: totrans-1097
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 让我们从探索如何使用 TensorBoard 开始。
- en: '**How to install TensorBoard**'
  id: totrans-1098
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**如何安装 TensorBoard**'
- en: The first step is to install TensorBoard from the Python Index. `pip install
    tensorboard`
  id: totrans-1099
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 第一步是从 Python Index 安装 TensorBoard。`pip install tensorboard`
- en: '**Using TensorBoard with Jupyter notebooks and Google Colab**'
  id: totrans-1100
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**在 Jupyter 笔记本和 Google Colab 中使用 TensorBoard**'
- en: Once TensorBoard is installed, you need to load it in your environment, usually
    Google Colab or your local notebook.`%load_ext tensorboard`Next, inform TensorBoard
    which folder will contain the log information.`log_folder = "runs"`
  id: totrans-1101
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 安装了 TensorBoard 后，您需要在环境中加载它，通常是在 Google Colab 或本地笔记本中。 `%load_ext tensorboard`
    接下来，告诉 TensorBoard 哪个文件夹将包含日志信息。 `log_folder = "runs"`
- en: '**How to launch TensorBoard**'
  id: totrans-1102
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**如何启动 TensorBoard**'
- en: Tensorboard is launched using the `[tensorboard]` magic command in notebook
    environments while specifying the `[logdir]`. `%tensorboard --logdir={log_folder}`
  id: totrans-1103
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: Tensorboard 使用 `[tensorboard]` 魔术命令在笔记本环境中启动，同时指定 `[logdir]`。 `%tensorboard
    --logdir={log_folder}`
- en: 'You can also launch TensorBoard on the command line using a similar pattern.
    Apart from viewing the terminal on the notebook environment, you can also view
    it on the browser by visiting: http://localhost:6006.'
  id: totrans-1104
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 您还可以使用类似的模式在命令行上启动 TensorBoard。除了在笔记本环境中查看终端外，您还可以通过访问以下地址在浏览器中查看：http://localhost:6006。
- en: '**Tensorboard dashboards**'
  id: totrans-1105
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**Tensorboard 仪表板**'
- en: TensorBoard has various dashboards for showing different types of information.
  id: totrans-1106
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: TensorBoard 拥有各种仪表板，用于显示不同类型的信息。
- en: The  **Scalars** dashboard tracks numerical information such as training metrics
    per epoch. You can use it to track other scalar values such as model training
    speed and learning rate.
  id: totrans-1107
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '**Scalars** 仪表板跟踪数字信息，如每个 epoch 的训练指标。您可以使用它来跟踪模型训练速度和学习率等其他标量值。'
- en: The **Graphs** dashboard is used for showing visualizations. For example, you
    can use it to check the architecture of the network.
  id: totrans-1108
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '**Graphs** 仪表板用于显示可视化。例如，您可以使用它来检查网络的架构。'
- en: The  **Distributions and Histograms** dashboard show the distribution of tensors
    over time. Use it to check the weights and biases of the network.
  id: totrans-1109
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '**Distributions and Histograms** 仪表板显示张量随时间的分布。用它来检查网络的权重和偏置。'
- en: The **Images** dashboard shows the images you have logged to TensorBoard.
  id: totrans-1110
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '**Images** 仪表板显示您已记录到 TensorBoard 的图像。'
- en: The **HParams** dashboard visualizes hyperparameter optimization. It helps identify
    the best parameters for the network.
  id: totrans-1111
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '**HParams** 仪表板可视化超参数优化。它帮助确定网络的最佳参数。'
- en: The  **Embedding Projector** is used to visualize low-level embeddings, for
    example, text embeddings.
  id: totrans-1112
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '**Embedding Projector** 用于可视化低级嵌入，例如文本嵌入。'
- en: The **What-If Tool** dashboard helps in understanding the performance of a model.
    It also enables the measurement of a model's fairness on data subsets.
  id: totrans-1113
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '**What-If Tool** 仪表板帮助理解模型的性能。它还能够在数据子集上测量模型的公平性。'
- en: The  **TensorFlow Profiler** monitors the model training process. It also shows
    the events in the CPU and GPU during training. The TensorFlow profiler goes further
    to offer recommendations based on the data collected. You can also use it to debug
    performance issues in the input pipeline.
  id: totrans-1114
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '**TensorFlow Profiler** 监控模型训练过程。它还显示了训练期间 CPU 和 GPU 上的事件。TensorFlow 分析器进一步根据收集的数据提供建议。您还可以使用它来调试输入管道中的性能问题。'
- en: '**How to use TensorBoard with Flax**'
  id: totrans-1115
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**如何在 Flax 中使用 TensorBoard**'
- en: With TensorBoard installed and some basics out of the way, let's look at how
    you can use it in Flax. Let's use
  id: totrans-1116
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 安装了 TensorBoard 并完成了一些基础设置后，让我们看看如何在 Flax 中使用它。 让我们使用
- en: the `[SummaryWriter]` from PyTorch to write to the log folder.
  id: totrans-1117
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 使用 PyTorch 中的 `[SummaryWriter]` 向日志文件夹写入。
- en: '**How to log images with TensorBoard in Flax**'
  id: totrans-1118
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**如何在 Flax 中使用 TensorBoard 记录图像**'
- en: You may want to log sample images when solving computer vision problems. You
    can also log predictions while training the model. For example, you can log prediction
    images containing bounding boxes for an object detection network.
  id: totrans-1119
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在解决计算机视觉问题时，您可能希望记录样本图像。您还可以在训练模型时记录预测。例如，您可以记录包含对象检测网络边界框的预测图像。
- en: 'Let''s look at how we can log an image to TensorBoard.from`torch.utils.tensorboard`import`SummaryWriter`import`torchvision.transforms.functional`as`Fwriter
    = SummaryWriter(logdir)`def show(imgs):if not isinstance(imgs, list):'
  id: totrans-1120
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '让我们看看如何将图像记录到 TensorBoard。 from `torch.utils.tensorboard` import `SummaryWriter`
    import `torchvision.transforms.functional` as `Fwriter = SummaryWriter(logdir)`
    def show(imgs): if not isinstance(imgs, list):'
- en: '`imgs = [imgs]`'
  id: totrans-1121
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`imgs = [imgs]`'
- en: '`fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)` for i, img in enumerate(imgs):'
  id: totrans-1122
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)` for i, img in enumerate(imgs):'
- en: '`img = img.detach()`'
  id: totrans-1123
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`img = img.detach()`'
- en: '`img = F.to_pil_image(img)`'
  id: totrans-1124
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`img = F.to_pil_image(img)`'
- en: '`axs[0, i].imshow(np.asarray(img))`'
  id: totrans-1125
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`axs[0, i].imshow(np.asarray(img))`'
- en: '`axs[0, i].set(xticklabels=[], yticklabels=[], xticks=`'
  id: totrans-1126
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`axs[0, i].set(xticklabels=[], yticklabels=[], xticks=`'
- en: '[], yticks=[])writer.flush() # Ensure that everything is written to diskNext,
    create a grid with the images that will be logged.'
  id: totrans-1127
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '[], yticks=[])writer.flush() # 确保所有内容都已写入磁盘接下来，创建将记录的图像的网格。'
- en: from`torchvision.utils`import`make_grid`from`torchvision.io`import`read_image`from`pathlib`import`Path`
  id: totrans-1128
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: from`torchvision.utils`import`make_grid`from`torchvision.io`import`read_image`from`pathlib`import`Path`
- en: '`cat = read_image(str(Path(''train'') / ''cat.1.jpg'')) grid = make_grid(cat)`'
  id: totrans-1129
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`cat = read_image(str(Path(''train'') / ''cat.1.jpg'')) grid = make_grid(cat)`'
- en: '`show(grid)`'
  id: totrans-1130
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`show(grid)`'
- en: The `[add_image]` function is used to write images to TensorBoard.`writer.add_image('sample_cat',
    grid)`Now, load the TensorBoard extension and point it to the logs folder.`%tensorboard
    --logdir={logdir}`The logged images will be visible on the Images dashboard. ![](../images/00034.gif)**TensorBoard
    Image dashboard ** **## How to log text with TensorBoard in Flax** Writing text
    to TensorBoard is done using the `[add_text]` function.`writer.add_text('Text',
    'Write image to TensorBoard', 0)` The logged data is available on the Text dashboard.![](../images/00035.jpeg)
  id: totrans-1131
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 使用`[add_image]`函数将图像写入TensorBoard。`writer.add_image('sample_cat', grid)`现在，加载TensorBoard扩展并将其指向日志文件夹。`%tensorboard
    --logdir={logdir}`记录的图像将在图像仪表板上可见。![](../images/00034.gif)**TensorBoard 图像仪表板**
    **## 如何在Flax中使用TensorBoard记录文本** 使用`[add_text]`函数向TensorBoard写入文本。`writer.add_text('Text',
    'Write image to TensorBoard', 0)`记录的数据在文本仪表板上可用。![](../images/00035.jpeg)
- en: '**Track model training in JAX using TensorBoard**'
  id: totrans-1132
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**使用TensorBoard跟踪JAX模型训练**'
- en: You can log the evaluation metrics when training machine learning models with
    JAX. They obtained at the training stage. At this point, you can log the metrics
    to TensorBoard. In the example below, we log the training and evaluation metrics.
  id: totrans-1133
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在训练机器学习模型时，可以记录评估指标。它们在训练阶段获取。此时，您可以将指标记录到TensorBoard。在下面的示例中，我们记录训练和评估指标。
- en: '`for epoch in range(1, num_epochs + 1):`'
  id: totrans-1134
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`for epoch in range(1, num_epochs + 1):`'
- en: '`train_state, train_metrics = train_one_epoch(state, train_l`'
  id: totrans-1135
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_state, train_metrics = train_one_epoch(state, train_l`'
- en: '`oader)`'
  id: totrans-1136
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`oader)`'
- en: '`training_loss.append(train_metrics[''loss''])`'
  id: totrans-1137
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`training_loss.append(train_metrics[''loss''])`'
- en: '`training_accuracy.append(train_metrics[''accuracy'']) print(f"Train epoch:
    {epoch}, loss: {train_metrics[''los`'
  id: totrans-1138
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`training_accuracy.append(train_metrics[''accuracy'']) print(f"Train epoch:
    {epoch}, loss: {train_metrics[''los`'
- en: 's'']}, accuracy: {train_metrics[''accuracy''] * 100}")'
  id: totrans-1139
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 's'']}, 准确率: {train_metrics[''accuracy''] * 100}")'
- en: '`test_metrics = evaluate_model(train_state, test_images, tes t_labels)`'
  id: totrans-1140
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`test_metrics = evaluate_model(train_state, test_images, tes t_labels)`'
- en: '`testing_loss.append(test_metrics[''loss''])`'
  id: totrans-1141
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_loss.append(test_metrics[''loss''])`'
- en: '`testing_accuracy.append(test_metrics[''accuracy''])`'
  id: totrans-1142
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_accuracy.append(test_metrics[''accuracy''])`'
- en: '`writer.add_scalar(''Loss/train'', train_metrics[''loss''], epoc h)`'
  id: totrans-1143
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`writer.add_scalar(''Loss/train'', train_metrics[''loss''], epoc h)`'
- en: '`writer.add_scalar(''Loss/test'', test_metrics[''loss''], epoch)`'
  id: totrans-1144
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`writer.add_scalar(''Loss/test'', test_metrics[''loss''], epoch)`'
- en: '`writer.add_scalar(''Accuracy/train'', train_metrics[''accuracy''], epoch)`writer.add_scalar(''Accuracy/test'',
    test_metrics[''accuracy''], epoch)print(f"Test epoch: {epoch}, loss: {test_metrics[''loss'']},
    accuracy: {test_metrics[''accuracy''] * 100}")These metrics will be available
    on the **Scalars** dashboard of TensorBoard.![](../images/00036.gif)'
  id: totrans-1145
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`writer.add_scalar(''Accuracy/train'', train_metrics[''accuracy''], epoch)`writer.add_scalar(''Accuracy/test'',
    test_metrics[''accuracy''], epoch)print(f"Test epoch: {epoch}, loss: {test_metrics[''loss'']},
    accuracy: {test_metrics[''accuracy''] * 100}")这些指标将在TensorBoard的**Scalars**仪表板上可用。![](../images/00036.gif)'
- en: '**How to profile JAX programs with TensorBoard**'
  id: totrans-1146
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**如何使用TensorBoard分析JAX程序**'
- en: To profileJAX programs, send data to the TensorBoard profiler. The first step
    is to install the profile plugin.`pip install -U tensorboard-plugin-profile`
  id: totrans-1147
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 要分析JAX程序，请将数据发送到TensorBoard分析器。第一步是安装分析插件。`pip install -U tensorboard-plugin-profile`
- en: '**Programmatic profiling**'
  id: totrans-1148
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**程序化分析**'
- en: Use `jax.profiler.start_trace()` to start a trace
  id: totrans-1149
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 使用`jax.profiler.start_trace()`来启动跟踪
- en: and `jax.profiler.stop_trace()` to stop a trace.
  id: totrans-1150
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 和`jax.profiler.stop_trace()`来停止跟踪。
- en: The `[start_trace()]` expects the path to the directory where the traces will
    be written.`import jax`jax.profiler.start_trace("runs")
  id: totrans-1151
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[start_trace()]`期望将跟踪写入的目录路径。`import jax`jax.profiler.start_trace("runs")'
- en: Run the operations to be profiled `key = jax.random.PRNGKey(0)`
  id: totrans-1152
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: 运行要分析的操作 `key = jax.random.PRNGKey(0)`
- en: '`x = jax.random.normal(key, (5000, 5000)) y = x @ x`'
  id: totrans-1153
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = jax.random.normal(key, (5000, 5000)) y = x @ x`'
- en: '`y.block_until_ready()`'
  id: totrans-1154
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`y.block_until_ready()`'
- en: '`jax.profiler.stop_trace()`'
  id: totrans-1155
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jax.profiler.stop_trace()`'
- en: '**Manual profiling with `TensorBoard`**'
  id: totrans-1156
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**使用`TensorBoard`进行手动分析**'
- en: 'The second option is to profile the `JAX` program manually. `![](../images/00037.jpeg)`This
    is done in the following steps:'
  id: totrans-1157
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 第二个选项是手动分析`JAX`程序。 `![](../images/00037.jpeg)`以下是操作步骤：
- en: Initialize `TensorBoard` `tensorboard --logdir /runs`Start a `JAX` profiler
    server at the beginning of the program and stop the server at the end of the program.
  id: totrans-1158
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在程序开始时初始化`TensorBoard`，使用`tensorboard --logdir /runs`启动一个`JAX`性能分析服务器，并在程序结束时停止服务器。
- en: import `jax.profiler`
  id: totrans-1159
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 导入`jax.profiler`。
- en: '`jax.profiler.start_server(9999)`'
  id: totrans-1160
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jax.profiler.start_server(9999)`。'
- en: '`train_one_epoch(state, train_loader,num_epochs)`'
  id: totrans-1161
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_one_epoch(state, train_loader,num_epochs)`。'
- en: '`jax.profiler.stop_server()`'
  id: totrans-1162
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jax.profiler.stop_server()`。'
- en: Open the Profile dashboard of `TensorBoard`. Click  **CAPTURE PROFILE** and
    enter the URL of the server that you started above, in this case localhost:9999\.
    Click CAPTURE to start profiling.
  id: totrans-1163
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 打开`TensorBoard`的Profile仪表板。点击 **CAPTURE PROFILE**，输入上述启动服务器的 URL，例如 localhost:9999。点击
    CAPTURE 开始性能分析。
- en: Select  **trace_viewer** under **Tools** on the profile dashboard. Use the navigation
    tools here to click specific events to see more information about them.
  id: totrans-1164
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在Profile仪表板上的**工具**下选择 **trace_viewer**。使用导航工具点击特定事件以查看更多信息。
- en: '**How to profile `JAX` program on a remote machine**'
  id: totrans-1165
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**如何在远程机器上对`JAX`程序进行性能分析**。'
- en: You can profile a `JAX` program on a remote server by executing the above instructions
    on the remote server. This involves starting a `TensorBoard` server on the remote
    machine, and port forwarding it to your local machine. You will then access `TensorBoard`
    locally via the web UI.
  id: totrans-1166
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 你可以通过在远程服务器上执行上述指令来对一个`JAX`程序进行性能分析。这涉及在远程机器上启动`TensorBoard`服务器，并将其端口转发到本地机器。然后，你可以通过
    web UI 在本地访问`TensorBoard`。
- en: '`ssh -L 6006:localhost:6006 <remote server address>`'
  id: totrans-1167
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`ssh -L 6006:localhost:6006 <remote server address>`。'
- en: '**Share `TensorBoard` dashboards**'
  id: totrans-1168
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**分享`TensorBoard`仪表板**。'
- en: '`TensorBoard.dev` is a hosted version of `TensorBoard` that makes it easy to
    share your experiments. Let''s upload the above `TensorBoard` to `TensorBoard.dev`.'
  id: totrans-1169
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`TensorBoard.dev` 是`TensorBoard`的托管版本，方便分享你的实验。让我们将上述`TensorBoard`上传到`TensorBoard.dev`。'
- en: On Colab or Jupyter nmotebook
  id: totrans-1170
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: 在Colab或Jupyter笔记本上
- en: '`!tensorboard dev upload --logdir ./runs \`'
  id: totrans-1171
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`!tensorboard dev upload --logdir ./runs \`'
- en: '`--name "Flax experiments" \`'
  id: totrans-1172
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`--name "Flax experiments" \`'
- en: '`--description "Logging model metrics with JAX" \`'
  id: totrans-1173
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`--description "Logging model metrics with JAX" \`'
- en: '`--one_shot`'
  id: totrans-1174
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`--one_shot`。'
- en: When you run the above code, you will get a prompt to authorize the upload.
    You should be keen not to share sensitive data because `TensorBoard.dev` experiments
    are public.
  id: totrans-1175
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 当你运行以上代码时，你将收到一个授权上传的提示。请注意不要分享敏感数据，因为`TensorBoard.dev`实验是公开的。
- en: You can view the experiment on `TensorBoard.dev`.
  id: totrans-1176
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 你可以在`TensorBoard.dev`上查看实验。
- en: '`![](../images/00038.jpeg)`'
  id: totrans-1177
  prefs: []
  stylish: true
  type: TYPE_IMG
  zh: '`![](../images/00038.jpeg)`。'
- en: '**Final thoughts**'
  id: totrans-1178
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**最后的思考**。'
- en: 'In this article, we have seen how you can use `TensorBoard` to log your experiments
    in `Flax`. More specifically, you have learned: What is `TensorBoard`?'
  id: totrans-1179
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在本文中，我们已经看到如何使用`TensorBoard`来记录你在`Flax`中的实验。更具体地说，你学到了：什么是`TensorBoard`？
- en: How to install and launch `TensorBoard`.
  id: totrans-1180
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 如何安装和启动`TensorBoard`。
- en: How to log images and text to `TensorBoard`.
  id: totrans-1181
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 如何将图像和文本日志记录到`TensorBoard`。
- en: How to log model metrics to `TensorBoard`.
  id: totrans-1182
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 如何将模型指标记录到`TensorBoard`。
- en: How to profile `JAX` and `Flax` programs using `TensorBoard` How to upload the
    log to `TensorBoard.dev`.
  id: totrans-1183
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 如何使用`TensorBoard`来对`JAX`和`Flax`程序进行性能分析。如何将日志上传到`TensorBoard.dev`。
- en: '**Handling state in `JAX` & `Flax` (BatchNorm and DropOut layers)**'
  id: totrans-1184
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**处理`JAX`和`Flax`中的状态（BatchNorm和DropOut层）**。'
- en: Jitting functions in `Flax` makes them faster but requires that the functions
    have no side effects. The fact that jitted functions can't have side effects introduces
    a challenge when dealing with stateful items such as model parameters and stateful
    layers such as batch normalization layers.
  id: totrans-1185
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在`Flax`中对函数进行`JIT`编译可以使其运行更快，但要求函数没有副作用。`JIT`函数不能有副作用的事实在处理状态项（如模型参数）和状态层（如批量归一化层）时带来了挑战。
- en: In this article, we'll create a network with the BatchNorm and DropOut layers.
    After that, we'll see how to deal with generating the random number for the DropOut
    layer and adding the batch statistics when training the network.
  id: totrans-1186
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在本文中，我们将创建一个带有BatchNorm和DropOut层的网络。然后，我们将看到如何处理生成DropOut层的随机数以及在训练网络时添加批次统计信息。
- en: '**Perform standard imports**'
  id: totrans-1187
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**执行标准导入操作**。'
- en: We kick off by importing standard data science packages that we'll use in this
    article.
  id: totrans-1188
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 我们从导入本文中将要使用的标准数据科学包开始。
- en: '`import torch`'
  id: totrans-1189
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import torch`。'
- en: '`from torch.utils.data import DataLoader import os`'
  id: totrans-1190
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from torch.utils.data import DataLoader import os`。'
- en: '`from PIL import Image`'
  id: totrans-1191
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from PIL import Image`。'
- en: '`from torchvision import transforms from torch.utils.data import Dataset import
    numpy as np`'
  id: totrans-1192
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from torchvision import transforms from torch.utils.data import Dataset import
    numpy as np`。'
- en: '`import pandas as pd`'
  id: totrans-1193
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import pandas as pd`。'
- en: '`from typing import Any`'
  id: totrans-1194
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from typing import Any`。'
- en: '`import matplotlib.pyplot as plt`'
  id: totrans-1195
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import matplotlib.pyplot as plt`'
- en: '`%matplotlib inline`'
  id: totrans-1196
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`%matplotlib inline`'
- en: ignore harmless warnings
  id: totrans-1197
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: 忽略无害的警告
- en: '`import warnings`'
  id: totrans-1198
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import warnings`'
- en: '`warnings.filterwarnings("ignore") import jax`'
  id: totrans-1199
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`warnings.filterwarnings("ignore") import jax`'
- en: '`from jax import numpy as jnp`'
  id: totrans-1200
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from jax import numpy as jnp`'
- en: '`import flax`'
  id: totrans-1201
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import flax`'
- en: '`from flax import linen as nn`'
  id: totrans-1202
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from flax import linen as nn`'
- en: '`from flax.training import train_state import optax`'
  id: totrans-1203
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from flax.training import train_state import optax`'
- en: '**Download the dataset**'
  id: totrans-1204
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**下载数据集**'
- en: Let's illustrate how to include BatchNorm and DropOut layers in a Flax network
    by designing a simple Convolutional Neural Network using the cat and dogs dataset from
    Kaggle.
  id: totrans-1205
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 让我们通过设计一个简单的使用Kaggle的猫和狗数据集的卷积神经网络来说明如何在Flax网络中包含BatchNorm和DropOut层。
- en: Download and extract the dataset.
  id: totrans-1206
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 下载并提取数据集。
- en: '`import wget`'
  id: totrans-1207
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import wget`'
- en: '`wget.download("https://ml.machinelearningnuggets.com/train.zi p")`'
  id: totrans-1208
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`wget.download("https://ml.machinelearningnuggets.com/train.zi p")`'
- en: '`import zipfilewith zipfile.ZipFile(''train.zip'', ''r'') as zip_ref: zip_ref.extractall(''.'')`'
  id: totrans-1209
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import zipfilewith zipfile.ZipFile(''train.zip'', ''r'') as zip_ref: zip_ref.extractall(''.'')`'
- en: '**Loading datasets in JAX**'
  id: totrans-1210
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**在JAX中加载数据集**'
- en: Since JAX doesn't ship with data loading tools, load the dataset using PyTorch.
    We start by creating a PyTorch Dataset class.
  id: totrans-1211
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 由于JAX不包含数据加载工具，使用PyTorch加载数据集。我们首先创建一个PyTorch数据集类。
- en: '`class CatsDogsDataset(Dataset):`'
  id: totrans-1212
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`class CatsDogsDataset(Dataset):`'
- en: '`def __init__(self, root_dir, annotation_file, transform=Non`'
  id: totrans-1213
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def __init__(self, root_dir, annotation_file, transform=Non`'
- en: '`e):`'
  id: totrans-1214
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`e):`'
- en: '`self.root_dir = root_dir`'
  id: totrans-1215
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.root_dir = root_dir`'
- en: '`self.annotations = pd.read_csv(annotation_file) self.transform = transform`'
  id: totrans-1216
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.annotations = pd.read_csv(annotation_file) self.transform = transform`'
- en: '`def __len__(self):return len(self.annotations)`'
  id: totrans-1217
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def __len__(self):return len(self.annotations)`'
- en: '`def __getitem__(self, index):`'
  id: totrans-1218
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def __getitem__(self, index):`'
- en: '`img_id = self.annotations.iloc[index, 0]`'
  id: totrans-1219
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`img_id = self.annotations.iloc[index, 0]`'
- en: '`img = Image.open(os.path.join(self.root_dir, img_id)).c`'
  id: totrans-1220
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`img = Image.open(os.path.join(self.root_dir, img_id)).c`'
- en: '`onvert("RGB")`'
  id: totrans-1221
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`onvert("RGB")`'
- en: '`y_label = torch.tensor(float(self.annotations.iloc[inde`'
  id: totrans-1222
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`y_label = torch.tensor(float(self.annotations.iloc[inde`'
- en: '`x, 1]))`'
  id: totrans-1223
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x, 1]))`'
- en: 'if self.transform is not None: `img = self.transform(img)return (img, y_label)`Interested
    in learning more about loading datasets in JAX?👉 Check our How to load datasets
    in JAX with TensorFlow tutorial.'
  id: totrans-1224
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 如果`self.transform`不为`None`：`img = self.transform(img)return (img, y_label)`有兴趣了解如何在JAX中加载数据集？👉查看我们的《如何使用TensorFlow在JAX中加载数据集》教程。
- en: Next, create a `Pandas DataFrame` containing the image path and the labels.
  id: totrans-1225
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 接下来，创建一个包含图像路径和标签的`Pandas DataFrame`。
- en: '`train_df = pd.DataFrame(columns=["img_path","label"]) train_df["img_path"]
    = os.listdir("train/") for idx, i in enumerate(os.listdir("train/")):`'
  id: totrans-1226
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_df = pd.DataFrame(columns=["img_path","label"]) train_df["img_path"]
    = os.listdir("train/") for idx, i in enumerate(os.listdir("train/")):`'
- en: 'if `"cat"` in `i`:'
  id: totrans-1227
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 如果`"cat"`在`i`中：
- en: '`train_df["label"][idx] = 0`'
  id: totrans-1228
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_df["label"][idx] = 0`'
- en: 'if `"dog"` in `i`:'
  id: totrans-1229
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 如果`"dog"`在`i`中：
- en: '`train_df["label"][idx] = 1`'
  id: totrans-1230
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_df["label"][idx] = 1`'
- en: '`train_df.to_csv (r''train_csv.csv'', index = False, header=True)`'
  id: totrans-1231
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_df.to_csv (r''train_csv.csv'', index = False, header=True)`'
- en: '**Data processing with PyTorch**'
  id: totrans-1232
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**使用PyTorch进行数据处理**'
- en: Next, create a function to stack the dataset and return it as a `NumPy array`.
  id: totrans-1233
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 接下来，创建一个函数来堆叠数据集并将其作为`NumPy array`返回。
- en: '`def custom_collate_fn(batch):`'
  id: totrans-1234
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def custom_collate_fn(batch):`'
- en: '`transposed_data = list(zip(*batch)) labels = np.array(transposed_data[1])
    imgs = np.stack(transposed_data[0]) return imgs, labels`'
  id: totrans-1235
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`transposed_data = list(zip(*batch)) labels = np.array(transposed_data[1])
    imgs = np.stack(transposed_data[0]) return imgs, labels`'
- en: We then use PyTorch to create training and testing data loaders.`size_image
    = 224 batch_size = 64`
  id: totrans-1236
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 然后，我们使用PyTorch创建训练和测试数据加载器。`size_image = 224 batch_size = 64`
- en: '`transform = transforms.Compose([`'
  id: totrans-1237
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`transform = transforms.Compose([`'
- en: '`transforms.Resize((size_image,size_image)),`'
  id: totrans-1238
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`transforms.Resize((size_image,size_image)),`'
- en: '`np.array])`'
  id: totrans-1239
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`np.array])`'
- en: '`dataset = CatsDogsDataset("train","train_csv.csv",transform=tra nsform)`'
  id: totrans-1240
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`dataset = CatsDogsDataset("train","train_csv.csv",transform=tra nsform)`'
- en: '`train_set, validation_set = torch.utils.data.random_split(datas et,[20000,5000])`'
  id: totrans-1241
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_set, validation_set = torch.utils.data.random_split(datas et,[20000,5000])`'
- en: '`train_loader = DataLoader(dataset=train_set, collate_fn=custom_ collate_fn,shuffle=True,
    batch_size=batch_size)`'
  id: totrans-1242
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_loader = DataLoader(dataset=train_set, collate_fn=custom_collate_fn,shuffle=True,
    batch_size=batch_size)`'
- en: x = `nn.relu(x)`
  id: totrans-1243
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.relu(x)`
- en: variables = `model.init(key, jnp.ones([1, size_image, size_imag e, 3]), training=False)`
  id: totrans-1244
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 变量 = `model.init(key, jnp.ones([1, size_image, size_imag e, 3]), training=False)`
- en: Define the Flax network with the BatchNorm and DropOut layers. In the network,
    we introduce the `[training]` variable to control when the batch stats should
    be updated. We ensure that they aren't updated during testing.
  id: totrans-1245
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在网络中使用BatchNorm和DropOut层定义Flax网络。在网络中，我们引入了`[training]`变量来控制何时更新批次统计信息。我们确保在测试期间不更新它们。
- en: '`model = CNN()`'
  id: totrans-1246
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`model = CNN()`'
- en: '**Define Flax model with BatchNorm and DropOut**'
  id: totrans-1247
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '**定义具有 BatchNorm 和 DropOut 的 Flax 模型**'
- en: x = `nn.Dense(features=2)(x)`
  id: totrans-1248
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.Dense(features=2)(x)`
- en: The rate drop out probability.
  id: totrans-1249
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 概率放弃率。
- en: 'Whether it''s deterministic. If deterministic inputs are scaled and masked.
    Otherwise, they are not masked and returned as they are.class `CNN(nn.Module)`:'
  id: totrans-1250
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '是否是确定性的。如果是确定性的输入则被缩放和掩码。否则，它们不会被掩码并原样返回。class `CNN(nn.Module)`:'
- en: '`@nn.compact`'
  id: totrans-1251
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`@nn.compact`'
- en: 'def `__call__(self, x, training)`:'
  id: totrans-1252
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def `__call__(self, x, training)`:'
- en: 'The next step is to create the loss function. When applying the model, we:'
  id: totrans-1253
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 下一步是创建损失函数。在应用模型时，我们：
- en: x = `nn.relu(x)`
  id: totrans-1254
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.relu(x)`
- en: x = `nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))`
  id: totrans-1255
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))`
- en: x = `nn.Conv(features=64, kernel_size=(3, 3))(x)`
  id: totrans-1256
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.Conv(features=64, kernel_size=(3, 3))(x)`
- en: x = `nn.relu(x)`
  id: totrans-1257
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.relu(x)`
- en: x = `nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))`
  id: totrans-1258
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))`
- en: x = `nn.Conv(features=32, kernel_size=(3, 3))(x)`
  id: totrans-1259
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.Conv(features=32, kernel_size=(3, 3))(x)`
- en: In the `BatchNorm` layer we set `use_running_average` to `False` meaning
  id: totrans-1260
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在 `BatchNorm` 层中，我们将 `use_running_average` 设置为 `False`，意味着
- en: x = `nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))`
  id: totrans-1261
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))`
- en: x = `x.reshape((x.shape[0], -1))`
  id: totrans-1262
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `x.reshape((x.shape[0], -1))`
- en: x = `nn.Dense(features=256)(x)`
  id: totrans-1263
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.Dense(features=256)(x)`
- en: x = `nn.Dense(features=128)(x)`
  id: totrans-1264
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.Dense(features=128)(x)`
- en: x = `nn.BatchNorm(use_running_average=not training)(x)`
  id: totrans-1265
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.BatchNorm(use_running_average=not training)(x)`
- en: '}'
  id: totrans-1266
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '}'
- en: '`validation_loader = DataLoader(dataset=validation_set,collate_f n=custom_collate_fn,
    shuffle=False, batch_size=batch_size)`'
  id: totrans-1267
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`validation_loader = DataLoader(dataset=validation_set,collate_f n=custom_collate_fn,
    shuffle=False, batch_size=batch_size)`'
- en: '**Compute metrics**'
  id: totrans-1268
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '**计算指标**'
- en: x = `nn.log_softmax(x)` return `x`
  id: totrans-1269
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.log_softmax(x)` return `x`
- en: '**Create loss function**'
  id: totrans-1270
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**创建损失函数**'
- en: 'The `[DropOut]` layer takes the following rate:'
  id: totrans-1271
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[DropOut]` 层使用以下比例：'
- en: Pass the batch stats parameters.
  id: totrans-1272
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 传递批量统计参数。
- en: '`[training]` as True. Set the `[batch_stats]` as mutable.'
  id: totrans-1273
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[training]` 为 True。将 `[batch_stats]` 设置为 mutable。'
- en: Set the random number for the `DropOut`
  id: totrans-1274
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 设置用于 `DropOut` 的随机数
- en: 'def `cross_entropy_loss(*, logits, labels)`:'
  id: totrans-1275
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def `cross_entropy_loss(*, logits, labels)`:'
- en: '`labels_onehot = jax.nn.one_hot(labels, num_classes=2)` return `optax.softmax_cross_entropy(logits=logits,
    labels=labe`'
  id: totrans-1276
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`labels_onehot = jax.nn.one_hot(labels, num_classes=2)` return `optax.softmax_cross_entropy(logits=logits,
    labels=labe`'
- en: x = `nn.Dropout(0.2, deterministic=not training)(x)`
  id: totrans-1277
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.Dropout(0.2, deterministic=not training)(x)`
- en: 'def `compute_loss(params, batch_stats, images, labels)`:'
  id: totrans-1278
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def `compute_loss(params, batch_stats, images, labels)`:'
- en: '`logits,batch_stats = CNN().apply({''params'': params,''batch_s tats'': batch_stats},images,
    training=True,rngs={''dropout'': jax. random.PRNGKey(0)}, mutable=[''batch_stats''])`'
  id: totrans-1279
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`logits,batch_stats = CNN().apply({''params'': params,''batch_s tats'': batch_stats},images,
    training=True,rngs={''dropout'': jax.random.PRNGKey(0)}, mutable=[''batch_stats''])`'
- en: loss = `cross_entropy_loss(logits=logits, labels=labels)` return `loss, (logits,
    batch_stats)`
  id: totrans-1280
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: loss = `cross_entropy_loss(logits=logits, labels=labels)` return `loss, (logits,
    batch_stats)`
- en: The compute metrics function calculates the loss and accuracy and returns them.
  id: totrans-1281
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 计算指标函数计算损失和准确性并返回它们。
- en: 'def `compute_metrics(*, logits, labels)`:'
  id: totrans-1282
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def `compute_metrics(*, logits, labels)`:'
- en: loss = `cross_entropy_loss(logits=logits, labels=labels)` accuracy = `jnp.mean(jnp.argmax(logits,
    -1) == labels)` metrics = {
  id: totrans-1283
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: loss = `cross_entropy_loss(logits=logits, labels=labels)` 准确率 = `jnp.mean(jnp.argmax(logits,
    -1) == labels)` metrics = {
- en: '`''loss'': loss`,'
  id: totrans-1284
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`''loss'': loss`,'
- en: '`''accuracy'': accuracy`,'
  id: totrans-1285
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`''accuracy'': accuracy`,'
- en: x = `nn.relu(x)`
  id: totrans-1286
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.relu(x)`
- en: return `metrics`
  id: totrans-1287
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: return `metrics`
- en: '**Create custom Flax training state**'
  id: totrans-1288
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**创建自定义的 Flax 训练状态**'
- en: Let's create a custom Flax training state that will store the batch stats information.
    To do that, create a new training state class that subclasses Flax's `TrainState`.
  id: totrans-1289
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 让我们创建一个自定义的 Flax 训练状态，用于存储批量统计信息。为此，创建一个新的训练状态类，它是 Flax 的 `TrainState` 的子类。
- en: x = `nn.Conv(features=128, kernel_size=(3, 3))(x)`
  id: totrans-1290
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.Conv(features=128, kernel_size=(3, 3))(x)`
- en: that the stats stored in `[batch_stats]` will not be used, but batch stats of
    the input will be computed.
  id: totrans-1291
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 存储在 `[batch_stats]` 中的统计信息不会被使用，但会计算输入的批次统计。
- en: '`key = jax.random.PRNGKey(0)`'
  id: totrans-1292
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`key = jax.random.PRNGKey(0)`'
- en: initialize weights
  id: totrans-1293
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 初始化权重
- en: '`class TrainState(train_state.TrainState): batch_stats: flax.core.FrozenDictTo
    define a Flax training state, use [TrainState.create] and pass the: Apply function.`'
  id: totrans-1294
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`class TrainState(train_state.TrainState): batch_stats: flax.core.FrozenDict要定义一个
    Flax 训练状态，使用 [TrainState.create] 并传递：应用函数。`'
- en: Model parameters.
  id: totrans-1295
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 模型参数。
- en: The optimizer function. The batch stats.
  id: totrans-1296
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 优化器函数。批量统计。
- en: '`state = TrainState.create(`'
  id: totrans-1297
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`state = TrainState.create(`'
- en: apply_fn = model.apply,
  id: totrans-1298
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: apply_fn = model.apply,
- en: '`params = variables[''params''],`'
  id: totrans-1299
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`params = variables[''params''],`'
- en: tx = optax.sgd(0.01),
  id: totrans-1300
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: tx = optax.sgd(0.01),
- en: batch_stats = variables['batch_stats'],
  id: totrans-1301
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: batch_stats = variables['batch_stats'],
- en: )
  id: totrans-1302
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: )
- en: '**Training step**'
  id: totrans-1303
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**训练步骤**'
- en: In the training step, we compute the gradients with respect to the loss and
    model parameters– the **model parameters **and **batch statistics**. We use the
    gradients to update the model parameters and return the new state and model metrics.
    The function is decorated
  id: totrans-1304
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在训练步骤中，我们计算相对于损失和模型参数的梯度 - **模型参数**和**批次统计**。我们使用这些梯度来更新模型参数并返回新的状态和模型指标。这个函数被装饰
- en: with @jax.jit to make the computation faster.
  id: totrans-1305
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 以 @jax.jit 使计算更快。
- en: '@jax.jit'
  id: totrans-1306
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '@jax.jit'
- en: '`def train_step(state,images, labels):`'
  id: totrans-1307
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def train_step(state,images, labels):`'
- en: '"""Train for a single step."""'
  id: totrans-1308
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""进行单步训练。"""'
- en: '`(batch_loss, (logits, batch_stats)), grads= jax.value_and_gra d(compute_loss,
    has_aux=True)(state.params,state.batch_stats, i mages,labels)`'
  id: totrans-1309
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`(batch_loss, (logits, batch_stats)), grads= jax.value_and_gra d(compute_loss,
    has_aux=True)(state.params,state.batch_stats, i mages,labels)`'
- en: '`state = state.apply_gradients(grads=grads)`'
  id: totrans-1310
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`state = state.apply_gradients(grads=grads)`'
- en: '`metrics = compute_metrics(logits=logits, labels=labels) return state, metricsNext,
    define a function that applies the training step for one epoch. The functions:`'
  id: totrans-1311
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`metrics = compute_metrics(logits=logits, labels=labels) return state, metricsNext,
    define a function that applies the training step for one epoch. The functions:`'
- en: Loops through the training data.
  id: totrans-1312
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 通过训练数据循环。
- en: Passes each training batch the training step.
  id: totrans-1313
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 将每个训练批次传递给训练步骤。
- en: Obtains the batch metrics. Computes the mean to obtain the epoch metrics.
  id: totrans-1314
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 获取批次指标。 计算均值以获得 epoch 指标。
- en: Returns the new state and metrics.
  id: totrans-1315
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 返回新状态和度量。
- en: '`def train_one_epoch(state, dataloader):`'
  id: totrans-1316
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def train_one_epoch(state, dataloader):`'
- en: '"""Train for 1 epoch on the training set.""" batch_metrics = []'
  id: totrans-1317
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""在训练集上进行 1 个 epoch 的训练。""" batch_metrics = []'
- en: 'for cnt, (images, labels) in enumerate(dataloader):'
  id: totrans-1318
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'for cnt, (images, labels) in enumerate(dataloader):'
- en: '`images = images / 255.0`'
  id: totrans-1319
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`images = images / 255.0`'
- en: '`state, metrics = train_step(state, images, labels) batch_metrics.append(metrics)`'
  id: totrans-1320
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`state, metrics = train_step(state, images, labels) batch_metrics.append(metrics)`'
- en: '`batch_metrics_np = jax.device_get(batch_metrics) epoch_metrics_np = {k: np.mean([metrics[k]
    for metrics in batch_metrics_n`'
  id: totrans-1321
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`batch_metrics_np = jax.device_get(batch_metrics) epoch_metrics_np = {k: np.mean([metrics[k]
    for metrics in batch_metrics_n`'
- en: p])
  id: totrans-1322
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: p])
- en: for k in batch_metrics_np[0] }
  id: totrans-1323
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 对于 `batch_metrics_np[0]` 中的`k` }
- en: '`return state, epoch_metrics_np`'
  id: totrans-1324
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return state, epoch_metrics_np`'
- en: '**Evaluation step**'
  id: totrans-1325
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**评估步骤**'
- en: We pass the test images and labels to the model in the evaluation step and obtain
    the evaluation metrics. The function is also jitted to take advantage of JAX's
    fast computation. During the evaluation, set [training] to [False] so that the
    model parameters are not updated. In this step, we also pass the batch stats and
    the random number generator for the [DropOut] layer.
  id: totrans-1326
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 我们在评估步骤中将测试图像和标签传递给模型并获取评估指标。该函数还通过JAX的快速计算利用JIT编译。在评估中，将 [training] 设置为 [False]，以便不更新模型参数。在这一步中，我们还传递批次统计和
    [DropOut] 层的随机数生成器。
- en: '@jax.jitdef eval_step(batch_stats, params, images, labels): logits = CNN().apply({''params'':
    params,''batch_stats'': batch _stats}, images, training=False,rngs={''dropout'':
    jax.random.PRN GKey(0)})return compute_metrics(logits=logits, labels=labels)The
    [evaluate_model] function applies the [eval_step] to the test data and returns
    the evaluation metrics.'
  id: totrans-1327
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '@jax.jitdef eval_step(batch_stats, params, images, labels): logits = CNN().apply({''params'':
    params,''batch_stats'': batch _stats}, images, training=False,rngs={''dropout'':
    jax.random.PRN GKey(0)})return compute_metrics(logits=logits, labels=labels)`[evaluate_model]`
    函数将`[eval_step]` 应用于测试数据，并返回评估指标。'
- en: '`def evaluate_model(state, test_imgs, test_lbls):`'
  id: totrans-1328
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def evaluate_model(state, test_imgs, test_lbls):`'
- en: '"""Evaluate on the validation set."""'
  id: totrans-1329
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""在验证集上评估。"""'
- en: '`metrics = eval_step(state.batch_stats,state.params, test_im`'
  id: totrans-1330
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`metrics = eval_step(state.batch_stats,state.params, test_im`'
- en: gs, test_lbls)
  id: totrans-1331
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: gs, test_lbls)
- en: '`metrics = jax.device_get(metrics)`'
  id: totrans-1332
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`metrics = jax.device_get(metrics)`'
- en: '`metrics = jax.tree_map(lambda x: x.item(), metrics) return metrics`'
  id: totrans-1333
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`metrics = jax.tree_map(lambda x: x.item(), metrics) return metrics`'
- en: '**Train Flax model**'
  id: totrans-1334
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**训练 Flax 模型**'
- en: To train the model, we define another function that implements `[train_one_epoch]`.
    Let's start by defining the model evaluation data.(`test_images`, `test_labels`)
    = next(iter(validation_loader)) `test_images = test_images / 255.0`
  id: totrans-1335
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 为了训练模型，我们定义另一个函数来实施`[train_one_epoch]`。首先定义模型评估数据。(`test_images`, `test_labels`)
    = next(iter(validation_loader)) `test_images = test_images / 255.0`
- en: '**Set up TensorBoard in Flax**'
  id: totrans-1336
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**在 Flax 中设置 TensorBoard**'
- en: You can log the model metrics to TensorBoard by writing the scalars to TensorBoard.
  id: totrans-1337
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 您可以将模型指标记录到TensorBoard中，方法是将标量写入TensorBoard。
- en: '`from torch.utils.tensorboard import SummaryWriter import torchvision.transforms.functional
    as F logdir = "flax_logs"`'
  id: totrans-1338
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from torch.utils.tensorboard import SummaryWriter import torchvision.transforms.functional
    as F logdir = "flax_logs"`'
- en: '`writer = SummaryWriter(logdir)`'
  id: totrans-1339
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`writer = SummaryWriter(logdir)`'
- en: '**Train model**'
  id: totrans-1340
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**训练模型**'
- en: We can also append the metrics to a list and visualize them with Matplotlib.
  id: totrans-1341
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 我们还可以将指标附加到列表中，并使用 Matplotlib 进行可视化。
- en: '`training_loss = [] training_accuracy = [] testing_loss = []`'
  id: totrans-1342
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`training_loss = [] training_accuracy = [] testing_loss = []`'
- en: '`testing_accuracy = []`'
  id: totrans-1343
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_accuracy = []`'
- en: 'Next, define the training function that will: Train the Flax model for the
    specified number of epochs.'
  id: totrans-1344
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 接下来，定义将训练 Flax 模型指定轮数的训练函数。
- en: Evaluate the model on the test data.
  id: totrans-1345
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在测试数据上评估模型。
- en: Append the metrics to a list. Write model metrics to TensorBoard.
  id: totrans-1346
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 将指标附加到列表中。将模型指标写入 TensorBoard。
- en: Print the metrics on every epoch. Return the trained model state`def train_model(epochs):for
    epoch in range(1, epochs + 1):train_state, train_metrics = train_one_epoch(state,
    tra`
  id: totrans-1347
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在每个 epoch 打印指标。返回训练后的模型状态`def train_model(epochs):for epoch in range(1, epochs
    + 1):train_state, train_metrics = train_one_epoch(state, tra`
- en: '`in_loader)`'
  id: totrans-1348
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`in_loader)`'
- en: '`training_loss.append(train_metrics[''loss''])` `training_accuracy.append(train_metrics[''accuracy''])`
    `test_metrics = evaluate_model(train_state, test_images,`'
  id: totrans-1349
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`training_loss.append(train_metrics[''loss''])` `training_accuracy.append(train_metrics[''accuracy''])`
    `test_metrics = evaluate_model(train_state, test_images,`'
- en: '`test_labels`'
  id: totrans-1350
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`test_labels`'
- en: '`testing_loss.append(test_metrics[''loss''])`'
  id: totrans-1351
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_loss.append(test_metrics[''loss''])`'
- en: '`testing_accuracy.append(test_metrics[''accuracy''])`'
  id: totrans-1352
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_accuracy.append(test_metrics[''accuracy''])`'
- en: '`writer.add_scalar(''Loss/train'', train_metrics[''loss''], epoch)`'
  id: totrans-1353
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`writer.add_scalar(''Loss/train'', train_metrics[''loss''], epoch)`'
- en: '`writer.add_scalar(''Loss/test'', test_metrics[''loss''], epoch)`'
  id: totrans-1354
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`writer.add_scalar(''Loss/test'', test_metrics[''loss''], epoch)`'
- en: '`writer.add_scalar(''Accuracy/train'', train_metrics[''accuracy''], epoch)`'
  id: totrans-1355
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`writer.add_scalar(''Accuracy/train'', train_metrics[''accuracy''], epoch)`'
- en: '`writer.add_scalar(''Accuracy/test'', test_metrics[''accuracy''], epoch)`'
  id: totrans-1356
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`writer.add_scalar(''Accuracy/test'', test_metrics[''accuracy''], epoch)`'
- en: '`print(f"Epoch: {epoch}, training loss: {train_metrics [''loss'']}, training
    accuracy: {train_metrics[''accuracy''] * 10 0}, validation loss: {test_metrics[''loss'']},
    validation accuracy: {test_metrics[''accuracy''] * 100}")`'
  id: totrans-1357
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`print(f"Epoch: {epoch}, training loss: {train_metrics [''loss'']}, training
    accuracy: {train_metrics[''accuracy''] * 10 0}, validation loss: {test_metrics[''loss'']},
    validation accuracy: {test_metrics[''accuracy''] * 100}")`'
- en: '`return train_stateRun the training function to train the model. trained_model_state
    = train_model(30)`'
  id: totrans-1358
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return train_stateRun the training function to train the model. trained_model_state
    = train_model(30)`'
- en: '**Save Flax model**'
  id: totrans-1359
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**保存 Flax 模型**'
- en: 'The `[save_checkpoint]` saves a Flax model. It expects:'
  id: totrans-1360
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[save_checkpoint]` 保存 Flax 模型。它期望：'
- en: The directory to save the model checkpoint.
  id: totrans-1361
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 保存模型检查点的目录。
- en: The Flax trained model, in this case `[trained_model_state]`. The model's prefix.
  id: totrans-1362
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在这种情况下，Flax 训练的模型是 `[trained_model_state]`。该模型的前缀。
- en: Whether to overwrite existing models.
  id: totrans-1363
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 是否覆盖现有模型。
- en: '`from flax.training import checkpoints`'
  id: totrans-1364
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from flax.training import checkpoints`'
- en: ckpt_dir = 'model_checkpoint/'
  id: totrans-1365
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ckpt_dir = 'model_checkpoint/'
- en: '`checkpoints.save_checkpoint(ckpt_dir=ckpt_dir`'
  id: totrans-1366
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`checkpoints.save_checkpoint(ckpt_dir=ckpt_dir)`'
- en: '`target=trained_model_state`, `step=100,`'
  id: totrans-1367
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`target=trained_model_state`, `step=100,`'
- en: '`prefix=''flax_model'', overwrite=True`'
  id: totrans-1368
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`prefix=''flax_model'', overwrite=True`'
- en: )![](../images/00039.jpeg)
  id: totrans-1369
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: )![](../images/00039.jpeg)
- en: '**Load Flax model**'
  id: totrans-1370
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**加载 Flax 模型**'
- en: The `[restore_checkpoint]` method loads a saved Flax model from the saved location.
  id: totrans-1371
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[restore_checkpoint]` 方法从保存的位置加载已保存的 Flax 模型。'
- en: '`loaded_model = checkpoints.restore_checkpoint(`'
  id: totrans-1372
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`loaded_model = checkpoints.restore_checkpoint(`'
- en: '`ckpt_dir=ckpt_dir`, `target=state`, `prefix=''flax_mode`'
  id: totrans-1373
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`ckpt_dir=ckpt_dir`, `target=state`, `prefix=''flax_mode`'
- en: '`l''`'
  id: totrans-1374
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`l''`'
- en: '**Evaluate Flax model**'
  id: totrans-1375
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**评估 Flax 模型**'
- en: Run the `[evalaute_model]` function to check the performance of the model on
    test data. `evaluate_model(trained_model_state,test_images, test_labels)`
  id: totrans-1376
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 运行 `[evalaute_model]` 函数来检查模型在测试数据上的性能。`evaluate_model(trained_model_state,test_images,
    test_labels)`
- en: '**Visualize Flax model performance**'
  id: totrans-1377
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**可视化 Flax 模型的性能**'
- en: To visualize the performance of the Flax model, you can plot the metrics using
    Matplotlib or load TensorBoard and check the scalars tab.
  id: totrans-1378
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 要可视化 Flax 模型的性能，可以使用 Matplotlib 绘制指标图表或加载 TensorBoard 并检查标量选项卡。
- en: '`%load_ext tensorboard` `%tensorboard --logdir={logdir}`'
  id: totrans-1379
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`%load_ext tensorboard` `%tensorboard --logdir={logdir}`'
- en: '**Final thoughts**'
  id: totrans-1380
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**最终思考**'
- en: '`In this article, you have seen how to build networks in Flax containing BatchNorm
    and DropOut layers. You have also seen how to adjust the training process to cater
    to these new layers. Specifically, you have learned:`'
  id: totrans-1381
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`在本文中，您已经看到如何在 Flax 中构建包含 BatchNorm 和 DropOut 层的网络。您还学习了如何调整训练过程以适应这些新层。具体来说，您学到了：`'
- en: '`How to define Flax models with BatchNorm and DropOut layers.`'
  id: totrans-1382
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 如何定义包含 BatchNorm 和 DropOut 层的 Flax 模型。
- en: '`How to create a custom Flax training state.`'
  id: totrans-1383
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`如何创建自定义的 Flax 训练状态。`'
- en: '`Training and evaluating a Flax model with BatchNorm and DropOut layers.`'
  id: totrans-1384
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 使用 BatchNorm 和 DropOut 层训练和评估 Flax 模型。
- en: '`How to save and load a Flax model.`'
  id: totrans-1385
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`如何保存和加载 Flax 模型。`'
- en: '`How to evaluate the performance of a Flax model`'
  id: totrans-1386
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`如何评估Flax模型的性能`'
- en: '`**LSTM in JAX & Flax**`'
  id: totrans-1387
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '`**JAX和Flax中的LSTM**`'
- en: '`LSTMs are a class of neural networks used to solve sequence problems such
    as time series and natural language processing. The LSTMs maintain some internal
    state that is useful in solving these problems. LSTMs apply for loops to iterate
    over each time step. We can use functions from JAX and Flax instead of writing
    these for loops from scratch. In this article, we will build a natural language
    processing model using LSTMs in Flax.`'
  id: totrans-1388
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`LSTM是一类用于解决序列问题（如时间序列和自然语言处理）的神经网络。LSTM保持一些内部状态，在解决这些问题时非常有用。LSTM应用于循环遍历每个时间步。我们可以使用JAX和Flax中的函数，而不是从头编写这些循环。在本文中，我们将使用Flax中的LSTM构建自然语言处理模型。`'
- en: '`Let''s get started.`'
  id: totrans-1389
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`让我们开始吧。`'
- en: '`**Dataset download**`'
  id: totrans-1390
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '`**数据集下载**`'
- en: '`We''ll use the movie review dataset from Kaggle. We download the dataset using
    Kaggle''s Python package.`'
  id: totrans-1391
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`我们将使用Kaggle提供的电影评论数据集。我们使用Kaggle的Python包下载数据集。`'
- en: '`import os`'
  id: totrans-1392
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import os`'
- en: 进入`#Obtain from https://www.kaggle.com/username/account` `os.environ["KAGGLE_USERNAME"]="KAGGLE_USERNAME"`
  id: totrans-1393
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 进入`#Obtain from https://www.kaggle.com/username/account` `os.environ["KAGGLE_USERNAME"]="KAGGLE_USERNAME"`
- en: '`os.environ["KAGGLE_KEY"]="KAGGLE_KEY"`'
  id: totrans-1394
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`os.environ["KAGGLE_KEY"]="KAGGLE_KEY"`'
- en: '`import kaggle`'
  id: totrans-1395
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import kaggle`'
- en: '`kaggle datasets download lakshmi25npathi/imdb-dataset-of-50k-mo vie-reviews`'
  id: totrans-1396
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`kaggle datasets download lakshmi25npathi/imdb-dataset-of-50k-mo vie-reviews`'
- en: '`Next, extract the dataset.import zipfilewith zipfile.ZipFile(''imdb-dataset-of-50k-movie-reviews.zip'',
    ''r'') as zip_ref:zip_ref.extractall(''imdb-dataset-of-50k-movie-reviews'')Load
    the dataset using Pandas and display a sample of the reviews.`'
  id: totrans-1397
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`接下来，提取数据集。使用zipfile解压缩zip文件''imdb-dataset-of-50k-movie-reviews.zip''，然后使用Pandas加载数据并显示部分评论。`'
- en: '`df = pd.read_csv("imdb-dataset-of-50k-movie-reviews/IMDB Datase t.csv")`'
  id: totrans-1398
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`df = pd.read_csv("imdb-dataset-of-50k-movie-reviews/IMDB Datase t.csv")`'
- en: '`df.head()`'
  id: totrans-1399
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`df.head()`'
- en: '`![](../images/00040.gif)`'
  id: totrans-1400
  prefs: []
  stylish: true
  type: TYPE_IMG
  zh: '`![](../images/00040.gif)`'
- en: '`**Data processing with NLTK**`'
  id: totrans-1401
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '`**使用NLTK进行数据处理**`'
- en: '`The dataset contains unnecessary characters for predicting whether a movie
    review is negative or positive. For instance, punctuation marks and special characters.
    We, therefore, remove these from the reviews. We also need to convert the [sentiment] column
    into a numerical representation. This is achieved using  [LabelEncoder] from Scikitlearn.
    Let''s import that together with other packages we''ll use throughout this article.`'
  id: totrans-1402
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`该数据集包含一些不必要的字符，用于预测电影评论是负面还是正面。例如，标点符号和特殊字符。因此，我们从评论中去除这些字符。我们还需要将[sentiment]列转换为数值表示。这可以使用Scikitlearn中的[LabelEncoder]完成。让我们导入这些及其他我们在本文中将使用的包。`'
- en: '`import numpy as np`'
  id: totrans-1403
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import numpy as np`'
- en: '`import pandas as pd`'
  id: totrans-1404
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import pandas as pd`'
- en: '`from numpy import array`'
  id: totrans-1405
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from numpy import array`'
- en: '`import tensorflow as tf`'
  id: totrans-1406
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import tensorflow as tf`'
- en: '`from sklearn.model_selection import train_test_split from sklearn.preprocessing
    import LabelEncoder import matplotlib.pyplot as plt`'
  id: totrans-1407
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from sklearn.model_selection import train_test_split from sklearn.preprocessing
    import LabelEncoder import matplotlib.pyplot as plt`'
- en: '`The reviews also contain words that are not useful in the sentiment prediction.
    These are common words in English, such as the, at, and, etc. These words are
    known as **stopwords**. We remove them with the help of the nltk library. Let''s
    start by defining a function to remove all the English stopwords.`'
  id: totrans-1408
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`评论中还包含对情感预测无用的词语。这些是英语中常见的词，如the，at，and等。这些词称为**停用词**。我们使用nltk库帮助去除它们。让我们开始定义一个函数，以删除所有英文停用词。`'
- en: '`pip install nltk`'
  id: totrans-1409
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`pip install nltk`'
- en: '`import nltk`'
  id: totrans-1410
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import nltk`'
- en: '`from nltk.corpus import stopwords`'
  id: totrans-1411
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from nltk.corpus import stopwords`'
- en: '`nltk.download(''stopwords'')`'
  id: totrans-1412
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`nltk.download(''stopwords'')`'
- en: '`def remove_stop_words(review):`'
  id: totrans-1413
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def remove_stop_words(review):`'
- en: '`review_minus_sw = []`'
  id: totrans-1414
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`review_minus_sw = []`'
- en: '`stop_words = stopwords.words(''english'')`'
  id: totrans-1415
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`stop_words = stopwords.words(''english'')`'
- en: '`review = review.split()`'
  id: totrans-1416
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`review = review.split()`'
- en: '`cleaned_review = [review_minus_sw.append(word) for word in`'
  id: totrans-1417
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[review_minus_sw.append(word) for word in cleaned_review]`'
- en: '`review if word not in stop_words]`'
  id: totrans-1418
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`review if word not in stop_words]`'
- en: '`cleaned_review = '' ''.join(review_minus_sw)`'
  id: totrans-1419
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`cleaned_review = '' ''.join(review_minus_sw)`'
- en: '`return cleaned_review`'
  id: totrans-1420
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return cleaned_review`'
- en: Apply the function to the sentiment column.`df['review'] = df['review'].apply(remove_stop_words)`Let's
    also convert the sentiment column to numerical representation.
  id: totrans-1421
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`将该函数应用于情感列。`df[''review''] = df[''review''].apply(remove_stop_words)`让我们还将情感列转换为数值表示。`'
- en: '`labelencoder = LabelEncoder()`'
  id: totrans-1422
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`labelencoder = LabelEncoder()`'
- en: '`df = df.assign(sentiment = labelencoder.fit_transform(df["senti ment"]))`'
  id: totrans-1423
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`df = df.assign(sentiment = labelencoder.fit_transform(df["senti ment"]))`'
- en: Compare the reviews with the review with and without the stop words.![](../images/00041.jpeg)
  id: totrans-1424
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 将具有和不具有停用词的评论进行比较！[](../images/00041.jpeg)
- en: Looking at the third review, we notice that the
  id: totrans-1425
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 查看第三条评论时，我们注意到
- en: '`words [this], [was] and [a] have been dropped from the sentence. However,
    we can still see some special characters, such as [<br>] in the review. Let''s
    resolve that next.`'
  id: totrans-1426
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`words [this], [was] and [a] have been dropped from the sentence. However,
    we can still see some special characters, such as [<br>] in the review. Let''s
    resolve that next.`'
- en: '**Text vectorization with Keras**'
  id: totrans-1427
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**使用 Keras 进行文本向量化**'
- en: The review data is still in text form. However, we need to convert it to a numeric
    representation like the sentiment column. Before we do that, let's split the dataset
    into a training and testing set.
  id: totrans-1428
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 评论数据仍然是文本形式。但是，我们需要将其转换为类似情感列的数值表示。在这之前，让我们将数据集拆分为训练集和测试集。
- en: '`from sklearn.model_selection import train_test_split df = df.drop_duplicates()`'
  id: totrans-1429
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from sklearn.model_selection import train_test_split df = df.drop_duplicates()`'
- en: '`docs = df[''review'']`'
  id: totrans-1430
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`docs = df[''review'']`'
- en: '`labels = array(df[''sentiment''])`'
  id: totrans-1431
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`labels = array(df[''sentiment''])`'
- en: '`X_train, X_test , y_train, y_test = train_test_split(docs, labe ls , test_size
    = 0.20, random_state=0)`'
  id: totrans-1432
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`X_train, X_test , y_train, y_test = train_test_split(docs, labe ls , test_size
    = 0.20, random_state=0)`'
- en: 'We use the Keras text vectorization layer to convert the reviews to integer
    form. This function lets us filter out all punctuation marks and convert the reviews
    to lowercase. We pass the following parameters:'
  id: totrans-1433
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 我们使用 Keras 文本向量化层将评论转换为整数形式。此函数可以过滤掉所有标点符号并将评论转换为小写。我们传递以下参数：
- en: '`standardize` as `lower_and_strip_punctuation` to convert to'
  id: totrans-1434
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`standardize` 设置为 `lower_and_strip_punctuation` 以转换为'
- en: lowercase and remove punctuation marks.
  id: totrans-1435
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 转换为小写并删除标点符号。
- en: '`[output_mode]` to `[int]` to get the result as integers. `[tf_idf]` would
    apply the TF-IDF algorithm.'
  id: totrans-1436
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[output_mode]` 转换为 `[int]` 以获得整数结果。 `[tf_idf]` 将应用 TF-IDF 算法。'
- en: '`[output_sequence_length]` as 50 to get sentences of that length. Change this
    number to see how it affects the model''s performance. I found 50 to five some
    good results. Sentences longer than the specified length will be truncated, while
    shorter ones will be padded with zeros.'
  id: totrans-1437
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[output_sequence_length]` 设置为 50，以获得该长度的句子。更改此数字以查看它如何影响模型的性能。我发现 50 给出了一些不错的结果。超过指定长度的句子将被截断，而较短的句子将用零填充。'
- en: '`[max_tokens]` as 10,000 to have a vocabulary size of that number. Tweak this
    number and check how the model''s performance changes.'
  id: totrans-1438
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[max_tokens]` 设置为 10,000，以获得该数量的词汇量。调整此数字并检查模型性能的变化。'
- en: After defining the vectorization layer, we apply it to the training data. This
    is done by calling the `adapt` function. The function computes the vocabulary
    from the provided dataset. The vocabulary will be truncated to `[max_tokens]`,
    if that is provided.
  id: totrans-1439
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 定义向量化层后，我们将其应用于训练数据。通过调用 `adapt` 函数来实现。该函数从提供的数据集中计算词汇表。如果提供了 `[max_tokens]`，则词汇表将被截断。
- en: '`import tensorflow as tf`'
  id: totrans-1440
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import tensorflow as tf`'
- en: '`max_features = 10000` # Maximum vocab size.'
  id: totrans-1441
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`max_features = 10000` # 最大词汇量大小。'
- en: '`batch_size = 128`'
  id: totrans-1442
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`batch_size = 128`'
- en: '`max_len = 50` # Sequence length to pad the outputs to. `vectorize_layer =
    tf.keras.layers.TextVectorization(standardize =''lower_and_strip_punctuation'',max_tokens=max_features,output_m
    ode=''int'',output_sequence_length=max_len)`'
  id: totrans-1443
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`max_len = 50` # 序列长度，用于填充输出。`vectorize_layer = tf.keras.layers.TextVectorization(standardize
    =''lower_and_strip_punctuation'',max_tokens=max_features,output_m ode=''int'',output_sequence_length=max_len)`'
- en: '`vectorize_layer.adapt(X_train)`'
  id: totrans-1444
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`vectorize_layer.adapt(X_train)`'
- en: To view the generated vocabulary, call the `get_vocabulary` function.`vectorize_layer.get_vocabulary()`![](../images/00042.jpeg)Convert
    the training and test data to numerical form using the trained vectorization layer.`X_train_padded
    = vectorize_layer(X_train)` `X_test_padded = vectorize_layer(X_test)`![](../images/00043.gif)
  id: totrans-1445
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 要查看生成的词汇表，请调用 `get_vocabulary` 函数。`vectorize_layer.get_vocabulary()`！[](../images/00042.jpeg)使用训练好的向量化层将训练和测试数据转换为数值形式。`X_train_padded
    = vectorize_layer(X_train)` `X_test_padded = vectorize_layer(X_test)`！[](../images/00043.gif)
- en: '**Create tf.data dataset**'
  id: totrans-1446
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**创建 tf.data 数据集**'
- en: Let's generate and prefetch batches from the training and test set to make loading
    them to the LSTM model more efficient. We start by creating a `tf.data.Dataset`.
  id: totrans-1447
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 让我们从训练集和测试集中生成和预取批次，以使加载到 LSTM 模型更高效。我们首先创建一个 `tf.data.Dataset`。
- en: '`training_data = tf.data.Dataset.from_tensor_slices((X_train_padded, y_train))`'
  id: totrans-1448
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`training_data = tf.data.Dataset.from_tensor_slices((X_train_padded, y_train))`'
- en: '`validation_data = tf.data.Dataset.from_tensor_slices((X_test_padded, y_test))`'
  id: totrans-1449
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`validation_data = tf.data.Dataset.from_tensor_slices((X_test_padded, y_test))`'
- en: '`training_data = training_data.batch(batch_size)`'
  id: totrans-1450
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`training_data = training_data.batch(batch_size)`'
- en: '`validation_data = validation_data.batch(batch_size)`'
  id: totrans-1451
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`validation_data = validation_data.batch(batch_size)`'
- en: Next, we prefetch one batch, shuffle the data and return it as a `NumPy array.`
  id: totrans-1452
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 接下来，我们预取一个批次，洗牌数据，并将其作为 `NumPy array` 返回。
- en: '`pip install tensorflow_datasets`'
  id: totrans-1453
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`pip install tensorflow_datasets`'
- en: '`import tensorflow_datasets as tfds`'
  id: totrans-1454
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import tensorflow_datasets as tfds`'
- en: '`def get_train_batches():`'
  id: totrans-1455
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def get_train_batches():`'
- en: '`ds = training_data.prefetch(1)`'
  id: totrans-1456
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`ds = training_data.prefetch(1)`'
- en: '`ds = ds.shuffle(3, reshuffle_each_iteration=True)` # `tfds.dataset_as_numpy`
    converts the `tf.data.Dataset` into an'
  id: totrans-1457
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`ds = ds.shuffle(3, reshuffle_each_iteration=True)` # `tfds.dataset_as_numpy`
    将 `tf.data.Dataset` 转换为一个'
- en: iterable of NumPy arrays`return tfds.as_numpy(ds)`
  id: totrans-1458
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 由 NumPy 数组组成的迭代器 `return tfds.as_numpy(ds)`
- en: '`Define LSTM model in Flax`'
  id: totrans-1459
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '`在 Flax 中定义 LSTM 模型`'
- en: We are now ready to define the LSTM model in Flax. To design LSTMs in Flax,
    we use the `LSTMCell` or the `OptimizedLSTMCell`.
  id: totrans-1460
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 我们现在可以在 Flax 中定义 LSTM 模型了。要在 Flax 中设计 LSTM，我们使用 `LSTMCell` 或 `OptimizedLSTMCell`。
- en: The `OptimizedLSTMCell` is the efficient `LSTMCell`.
  id: totrans-1461
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`OptimizedLSTMCell` 是高效的 `LSTMCell`。'
- en: 'The `[LSTMCell.initialize_carry]` function is used to initialize the hidden
    state of the LSTM cell. It expects:'
  id: totrans-1462
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[LSTMCell.initialize_carry]` 函数用于初始化 LSTM 单元的隐藏状态。它期望：'
- en: A random number.
  id: totrans-1463
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 一个随机数。
- en: The batch dimensions.
  id: totrans-1464
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 批次维度。
- en: The number of units.
  id: totrans-1465
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 单元的数量。
- en: 'Let''s use the `setup method` to define the LSTM model. The LSTM contains the
    following layers:'
  id: totrans-1466
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 让我们使用 `setup 方法` 来定义 LSTM 模型。LSTM 包含以下层：
- en: An `Embedding layer` with the same number of features and length as defined
    in the vectorization layer.
  id: totrans-1467
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 一个与向量化层中定义的特征数和长度相同的 `Embedding layer`。
- en: LSTM layers that pass data in one direction as specified by the `[reverse]`
    argument.
  id: totrans-1468
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: LSTM 层根据 `[reverse]` 参数在一个方向上传递数据。
- en: A couple of `Dense layers`.
  id: totrans-1469
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 一对 `Dense layers`。
- en: Final dense output layer.from flax import linen as nn
  id: totrans-1470
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 最终的密集输出层。from flax import linen as nn
- en: '`class LSTMModel(nn.Module):`'
  id: totrans-1471
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`class LSTMModel(nn.Module):`'
- en: '`def setup(self):`'
  id: totrans-1472
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def setup(self):`'
- en: '`self.embedding = nn.Embed(max_features, max_len) lstm_layer = nn.scan(nn.OptimizedLSTMCell,`'
  id: totrans-1473
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.embedding = nn.Embed(max_features, max_len) lstm_layer = nn.scan(nn.OptimizedLSTMCell,`'
- en: '`variable_broadcast="params", split_rngs={"params": False}, in_axes=1,`'
  id: totrans-1474
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`variable_broadcast="params", split_rngs={"params": False}, in_axes=1,`'
- en: '`out_axes=1,`'
  id: totrans-1475
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`out_axes=1,`'
- en: '`length=max_len,`'
  id: totrans-1476
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`length=max_len,`'
- en: '`reverse=False)`'
  id: totrans-1477
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`reverse=False)`'
- en: '`self.lstm1 = lstm_layer()`'
  id: totrans-1478
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.lstm1 = lstm_layer()`'
- en: '`self.dense1 = nn.Dense(256)`'
  id: totrans-1479
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.dense1 = nn.Dense(256)`'
- en: '`self.lstm2 = lstm_layer()`'
  id: totrans-1480
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.lstm2 = lstm_layer()`'
- en: '`self.dense2 = nn.Dense(128)`'
  id: totrans-1481
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.dense2 = nn.Dense(128)`'
- en: '`self.lstm3 = lstm_layer()`'
  id: totrans-1482
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.lstm3 = lstm_layer()`'
- en: '`self.dense3 = nn.Dense(64)`'
  id: totrans-1483
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.dense3 = nn.Dense(64)`'
- en: '`self.dense4 = nn.Dense(2)`'
  id: totrans-1484
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.dense4 = nn.Dense(2)`'
- en: '`@nn.rematdef __call__(self, x_batch): x = self.embedding(x_batch)`'
  id: totrans-1485
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`@nn.rematdef __call__(self, x_batch): x = self.embedding(x_batch)`'
- en: '`carry, hidden = nn.OptimizedLSTMCell.initialize_carry(jax.random.PRNGKey(0),
    batch_dims=(len(x_batch),), size=128)`'
  id: totrans-1486
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`carry, hidden = nn.OptimizedLSTMCell.initialize_carry(jax.random.PRNGKey(0),
    batch_dims=(len(x_batch),), size=128)`'
- en: '`(carry, hidden), x = self.lstm1((carry, hidden), x)`'
  id: totrans-1487
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`(carry, hidden), x = self.lstm1((carry, hidden), x)`'
- en: '`x = self.dense1(x) x = nn.relu(x)`'
  id: totrans-1488
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = self.dense1(x) x = nn.relu(x)`'
- en: '`carry, hidden = nn.OptimizedLSTMCell.initialize_carry(jax.random.PRNGKey(0),
    batch_dims=(len(x_batch),), size=64)`'
  id: totrans-1489
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`carry, hidden = nn.OptimizedLSTMCell.initialize_carry(jax.random.PRNGKey(0),
    batch_dims=(len(x_batch),), size=64)`'
- en: '`(carry, hidden), x = self.lstm2((carry, hidden), x) x = self.dense2(x) x =
    nn.relu(x)`'
  id: totrans-1490
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`(carry, hidden), x = self.lstm2((carry, hidden), x) x = self.dense2(x) x =
    nn.relu(x)`'
- en: '`carry, hidden = nn.OptimizedLSTMCell.initialize_carry(jax.random.PRNGKey(0),
    batch_dims=(len(x_batch),), size=32)`'
  id: totrans-1491
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`carry, hidden = nn.OptimizedLSTMCell.initialize_carry(jax.random.PRNGKey(0),
    batch_dims=(len(x_batch),), size=32)`'
- en: '`(carry, hidden), x = self.lstm3((carry, hidden), x)`'
  id: totrans-1492
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`(carry, hidden), x = self.lstm3((carry, hidden), x)`'
- en: '`x = self.dense3(x)`'
  id: totrans-1493
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = self.dense3(x)`'
- en: '`x = nn.relu(x)`'
  id: totrans-1494
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = nn.relu(x)`'
- en: '`x = self.dense4(x[:, -1]) return nn.log_softmax(x)`'
  id: totrans-1495
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = self.dense4(x[:, -1]) return nn.log_softmax(x)`'
- en: We apply the `scan function` to iterate over the data. It expects:`scan` the
    items to be looped over. They must be the same size and will be stacked along
    the scan axis.`carry` a carried value that is updated at each iteration. The value
    must be the same shape and `[dtype]` throughout the iteration.
  id: totrans-1496
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 我们将 `scan 函数` 应用于数据的迭代。它期望：`scan` 待循环的项目。它们必须具有相同的大小，并且将沿着扫描轴堆叠。`carry` 一个在每次迭代中更新的传递值。该值在整个迭代过程中必须具有相同的形状和
    `[dtype]`。
- en: '`[broadcast]` a value that is closed over by the loop ` [<axis:int>]` axis
    along which to scan.'
  id: totrans-1497
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[广播]` 一个在循环中封闭的值 ` [<axis:int>]` 扫描的轴。'
- en: '`[split_rngs]` to define if to split the random number generator at each step.'
  id: totrans-1498
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[split_rngs]` 用于定义是否在每一步分割随机数生成器。'
- en: The [nn.remat] call saves memory when using LSTMs to compute long sequences.
  id: totrans-1499
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在使用 LSTMs 计算长序列时，`[nn.remat]` 调用节省内存。
- en: Compute metrics in Flax
  id: totrans-1500
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 在 Flax 中计算指标
- en: Next, we define a function to compute the loss and accuracy of the network.
  id: totrans-1501
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个函数来计算网络的损失和准确率。
- en: import optax
  id: totrans-1502
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: import optax
- en: import jax.numpy as jnp
  id: totrans-1503
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: import jax.numpy as jnp
- en: 'def compute_metrics(logits, labels):'
  id: totrans-1504
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def compute_metrics(logits, labels):'
- en: loss = jnp.mean(optax.softmax_cross_entropy(logits, jax.nn.on e_hot(labels,
    num_classes=2)))
  id: totrans-1505
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: loss = jnp.mean(optax.softmax_cross_entropy(logits, jax.nn.on e_hot(labels,
    num_classes=2)))
- en: accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)
  id: totrans-1506
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)
- en: metrics = {
  id: totrans-1507
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: metrics = {
- en: '''loss'': loss,'
  id: totrans-1508
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '''loss'': loss,'
- en: '''accuracy'': accuracy'
  id: totrans-1509
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '''accuracy'': accuracy'
- en: '}'
  id: totrans-1510
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '}'
- en: return metrics
  id: totrans-1511
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: return metrics
- en: Create training state
  id: totrans-1512
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 创建训练状态
- en: 'The training state applies gradients and updates the parameters and optimizer
    state. Flax provides [train_state] for this purpose. We define a function that:'
  id: totrans-1513
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 训练状态应用梯度并更新参数和优化器状态。Flax提供了[train_state]用于此目的。我们定义一个函数：
- en: Creates an instance of the [LSTMModel].
  id: totrans-1514
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 创建[LSTMModel]的一个实例。
- en: Initializes the model to obtain the [`params`] by passing a sample of the training
    data.
  id: totrans-1515
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 初始化模型以通过训练数据样本获取[`params`]。
- en: Returns the created state after applying the Adam optimizer.from flax.training
    import train_state
  id: totrans-1516
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在应用Adam优化器后返回创建的状态。from flax.training import train_state
- en: 'def create_train_state(rng):'
  id: totrans-1517
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def create_train_state(rng):'
- en: '"""Creates initial `TrainState`."""'
  id: totrans-1518
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""创建初始`TrainState`。"""'
- en: model = LSTMModel()
  id: totrans-1519
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: model = LSTMModel()
- en: params = model.init(rng, jnp.array(X_train_padded[0]))['param
  id: totrans-1520
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: params = model.init(rng, jnp.array(X_train_padded[0]))['param
- en: s']
  id: totrans-1521
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: s']
- en: tx = optax.adam(0.001,0.9,0.999,1e-07) return train_state.TrainState.create(
  id: totrans-1522
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: tx = optax.adam(0.001,0.9,0.999,1e-07) return train_state.TrainState.create(
- en: apply_fn=model.apply, params=params, tx=tx)
  id: totrans-1523
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: apply_fn=model.apply, params=params, tx=tx)
- en: Define training step
  id: totrans-1524
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 定义训练步骤
- en: 'The training function does the following:'
  id: totrans-1525
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '训练函数执行以下操作:'
- en: Compute the loss and logits from the model with the `apply` method.
  id: totrans-1526
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 使用`apply`方法从模型计算损失和logits。
- en: Compute the gradients using [value_and_grad]. Use the gradients to update the
    model parameters.
  id: totrans-1527
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 使用[value_and_grad]计算梯度。使用梯度更新模型参数。
- en: Compute the metrics using the function defined earlier. Returns the state and
    metrics.
  id: totrans-1528
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 使用先前定义的函数计算metrics。返回状态和metrics。
- en: 'Applying [@jax.jit] makes the function run faster.`@jax.jit`def train_step(state,
    text, labels):def loss_fn(params):'
  id: totrans-1529
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '应用[@jax.jit]使函数运行更快。`@jax.jit`def train_step(state, text, labels):def loss_fn(params):'
- en: 'logits = LSTMModel().apply({''params'': params}, text) loss = jnp.mean(optax.softmax_cross_entropy(
    logits=logits,'
  id: totrans-1530
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'logits = LSTMModel().apply({''params'': params}, text) loss = jnp.mean(optax.softmax_cross_entropy(
    logits=logits,'
- en: labels=jax.nn.one_hot(labels, num_classes=2))) return loss, logits
  id: totrans-1531
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: labels=jax.nn.one_hot(labels, num_classes=2))) return loss, logits
- en: grad_fn = jax.value_and_grad(loss_fn, has_aux=True) (_, logits), grads = grad_fn(state.params)
  id: totrans-1532
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: grad_fn = jax.value_and_grad(loss_fn, has_aux=True) (_, logits), grads = grad_fn(state.params)
- en: state = state.apply_gradients(grads=grads)
  id: totrans-1533
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: state = state.apply_gradients(grads=grads)
- en: metrics = compute_metrics(logits, labels)
  id: totrans-1534
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: metrics = compute_metrics(logits, labels)
- en: return state, metrics
  id: totrans-1535
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 返回状态和metrics
- en: Evaluate the Flax model
  id: totrans-1536
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 评估Flax模型
- en: The [`eval_step`] evaluates the model's performance on the test set using `Module.apply`.
    It returns the loss and accuracy on the testing set.
  id: totrans-1537
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '[`eval_step`]使用`Module.apply`评估模型在测试集上的表现。它返回测试集上的损失和准确率。'
- en: The [`evaluate_model`] function applies the [`eval_step`] , obtains the metrics
    from the device and returns them as a [`jax.tree_map`].
  id: totrans-1538
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '[`evaluate_model`]函数应用[`eval_step`]，从设备获取metrics，并作为[`jax.tree_map`]返回它们。'
- en: '@jax.jit'
  id: totrans-1539
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '@jax.jit'
- en: 'def eval_step(state, text, labels):'
  id: totrans-1540
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def eval_step(state, text, labels):'
- en: 'logits = LSTMModel().apply({''params'': state.params}, text)'
  id: totrans-1541
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'logits = LSTMModel().apply({''params'': state.params}, text)'
- en: return compute_metrics(logits=logits, labels=labels)
  id: totrans-1542
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 返回使用logits和labels计算metrics的结果
- en: 'def evaluate_model(state, text, test_lbls): """Evaluate on the validation set."""'
  id: totrans-1543
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def evaluate_model(state, text, test_lbls): """在验证集上评估模型。"""'
- en: metrics = eval_step(state, text, test_lbls) metrics = jax.device_get(metrics)
  id: totrans-1544
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: metrics = eval_step(state, text, test_lbls) metrics = jax.device_get(metrics)
- en: 'metrics = jax.tree_map(lambda x: x.item(), metrics) return metrics'
  id: totrans-1545
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'metrics = jax.tree_map(lambda x: x.item(), metrics) return metrics'
- en: Create training function
  id: totrans-1546
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 创建训练函数
- en: Next, define a function that trains the Flax LSTM model on one epoch. The function
    applies  [train_step] to each batch in the training data. After each batch, it
    appends the metrics to a list.
  id: totrans-1547
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 接下来，定义一个函数，在一个epoch上训练Flax LSTM模型。该函数对训练数据中的每个批次应用[train_step]。在每个批次之后，它将metrics附加到一个列表中。
- en: 'def train_one_epoch(state):'
  id: totrans-1548
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def train_one_epoch(state):'
- en: Train for 1 epoch on the training set. batch_metrics = []
  id: totrans-1549
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在训练集上训练1个epoch。batch_metrics = []
- en: 'for text, labels in get_train_batches():'
  id: totrans-1550
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '对于text, labels in get_train_batches():'
- en: 'state, metrics = `train_step(state, text, labels)` batch_metrics.append(metrics)batch_metrics_np
    = jax.device_get(batch_metrics) epoch_metrics_np = {k: np.mean([metrics[k] for
    metrics in batch_metrics_np[0] ])for k in batch_metrics_np[0] }return state, epoch_metrics_np'
  id: totrans-1551
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'state, metrics = `train_step(state, text, labels)` batch_metrics.append(metrics)batch_metrics_np
    = jax.device_get(batch_metrics) epoch_metrics_np = {k: np.mean([metrics[k] for
    metrics in batch_metrics_np[0] ])for k in batch_metrics_np[0] }return state, epoch_metrics_np'
- en: The function obtains the metrics from the device and computes the mean from
    all the trained batches. This gives the loss and accuracy for one epoch.
  id: totrans-1552
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 函数从设备获取指标并计算所有训练批次的平均值。这提供了一个时期的损失和准确性。
- en: Train LSTM model in Flax
  id: totrans-1553
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 在 Flax 中训练 LSTM 模型
- en: To train the LSTM model, we run the [train_one_epoch] function for several iterations.
    Next, apply the [evaluate_model] to obtain the test metrics for each epoch. Before
    training starts, we create
  id: totrans-1554
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 要训练 LSTM 模型，我们运行 [train_one_epoch] 函数进行多次迭代。接下来，应用 [evaluate_model] 获取每个时期的测试指标。在开始训练之前，我们创建
- en: a [create_train_state] to hold the training information. The function initializes
    the model parameters and the optimizer. This information is stored in the training
    state dataclass.
  id: totrans-1555
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 使用 [create_train_state] 来保存训练信息。该函数初始化模型参数和优化器。此信息存储在训练状态 dataclass 中。
- en: 'rng = jax.random.PRNGKey(0)rng, input_rng, init_rng = jax.random.split(rng,num=3)seed
    = 0state = `create_train_state(init_rng)` del init_rng # Must not be used anymore.'
  id: totrans-1556
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'rng = jax.random.PRNGKey(0)rng, input_rng, init_rng = jax.random.split(rng,num=3)seed
    = 0state = `create_train_state(init_rng)` del init_rng # 不再使用。'
- en: num_epochs = 30
  id: totrans-1557
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: num_epochs = 30
- en: (text, test_labels) = next(iter(validation_data)) text = jnp.array(text)
  id: totrans-1558
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: (text, test_labels) = next(iter(validation_data)) text = jnp.array(text)
- en: test_labels = jnp.array(test_labels) training_loss = []
  id: totrans-1559
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: test_labels = jnp.array(test_labels) training_loss = []
- en: training_accuracy = []
  id: totrans-1560
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: training_accuracy = []
- en: testing_loss = []
  id: totrans-1561
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: testing_loss = []
- en: testing_accuracy = []
  id: totrans-1562
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: testing_accuracy = []
- en: 'def `train_model()`:'
  id: totrans-1563
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def `train_model()`:'
- en: 'for epoch in range(1, num_epochs + 1):'
  id: totrans-1564
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'for epoch in range(1, num_epochs + 1):'
- en: train_state, train_metrics = `train_one_epoch(state)` training_loss.append(train_metrics['loss'])
    training_accuracy.append(train_metrics['accuracy']) test_metrics = `evaluate_model(train_state,
    text, test_l`
  id: totrans-1565
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: train_state, train_metrics = `train_one_epoch(state)` training_loss.append(train_metrics['loss'])
    training_accuracy.append(train_metrics['accuracy']) test_metrics = `evaluate_model(train_state,
    text, test_l`
- en: abels)
  id: totrans-1566
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: abels)
- en: testing_loss.append(test_metrics['loss'])
  id: totrans-1567
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: testing_loss.append(test_metrics['loss'])
- en: 'testing_accuracy.append(test_metrics[''accuracy'']) print(f"Epoch: {epoch},
    train loss: {train_metrics[''los'
  id: totrans-1568
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'testing_accuracy.append(test_metrics[''accuracy'']) print(f"Epoch: {epoch},
    train loss: {train_metrics[''los'
- en: 's'']}, train accuracy: {train_metrics[''accuracy''] * 100}, test l oss: {test_metrics[''loss'']},
    test accuracy: {test_metrics[''accu racy''] * 100}")'
  id: totrans-1569
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 's'']}, train accuracy: {train_metrics[''accuracy''] * 100}, test l oss: {test_metrics[''loss'']},
    test accuracy: {test_metrics[''accu racy''] * 100}")'
- en: return `train_statetrained_model_state = train_model()`After each epoch, we
    print the metrics and append them to a list.
  id: totrans-1570
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 返回 `train_statetrained_model_state = train_model()`每个时期结束后，我们打印指标并将其附加到列表中。
- en: Visualize LSTM model performance in Flax
  id: totrans-1571
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 在 Flax 中可视化 LSTM 模型性能
- en: You can then use `Matplotlib` to visualize the metrics appended to the list.
    The training is not quite smooth, but you can tweak the architecture of the network,
    the length of each review, and the vocabulary size to improve performance.
  id: totrans-1572
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 然后，您可以使用 `Matplotlib` 来可视化附加到列表中的指标。训练过程不是很平稳，但您可以调整网络的架构、每个评审的长度和词汇量大小以提高性能。
- en: Save LSTM model
  id: totrans-1573
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 保存 LSTM 模型
- en: 'To save a Flax model checkpoint, use the [save_checkpoint] method. It expects:'
  id: totrans-1574
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 要保存 Flax 模型检查点，请使用 [save_checkpoint] 方法。它需要：
- en: The directory to save the checkpoint files.
  id: totrans-1575
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 保存检查点文件的目录。
- en: The Flax object to be saved, that is, [target].
  id: totrans-1576
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 要保存的 Flax 对象，即 [target]。
- en: The prefix of the checkpoint file name.
  id: totrans-1577
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 检查点文件名的前缀。
- en: Whether to overwrite previous checkpoints
  id: totrans-1578
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 是否覆盖先前的检查点
- en: from flax.training import `checkpoints`
  id: totrans-1579
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: from flax.training import `checkpoints`
- en: checkpoints.save_checkpoint(ckpt_dir='lstm_model_checkpoint/', target=trained_model_state,
  id: totrans-1580
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: checkpoints.save_checkpoint(ckpt_dir='lstm_model_checkpoint/', target=trained_model_state,
- en: step=100,
  id: totrans-1581
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: step=100,
- en: prefix='lstm_model',
  id: totrans-1582
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: prefix='lstm_model',
- en: overwrite=False
  id: totrans-1583
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: overwrite=False
- en: )
  id: totrans-1584
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: )
- en: To restore the saved model, use [restore_checkpoint] method.
  id: totrans-1585
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 要恢复保存的模型，请使用 [restore_checkpoint] 方法。
- en: loaded_model = `checkpoints.restore_checkpoint(`
  id: totrans-1586
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: loaded_model = `checkpoints.restore_checkpoint(`
- en: ckpt_dir='lstm_mod
  id: totrans-1587
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ckpt_dir='lstm_mod
- en: el_checkpoint/',
  id: totrans-1588
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: el_checkpoint/',
- en: target=state, prefix='lstm_mode
  id: totrans-1589
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: target=state, prefix='lstm_mode
- en: l'
  id: totrans-1590
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: l'
- en: )
  id: totrans-1591
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: )
- en: loaded_model ![](../images/00044.gif)This model can be used to make predictions
    right away.![](../images/00045.gif)
  id: totrans-1592
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: loaded_model ![](../images/00044.gif)此模型可立即用于进行预测！[](../images/00045.gif)
- en: Final thoughts
  id: totrans-1593
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 最终想法
- en: '`You have learned to solve natural language processing problems with JAX and
    Flax in this article. In particular, the nuggets you have covered include:`'
  id: totrans-1594
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`在本文中，您已经学会了如何在 JAX 和 Flax 中解决自然语言处理问题。特别是您涵盖的关键点包括：`'
- en: '`How to process text data with NLTK.`'
  id: totrans-1595
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`如何使用 NLTK 处理文本数据。`'
- en: '`Text vectorization with Keras.`'
  id: totrans-1596
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`使用 Keras 进行文本向量化。`'
- en: Creating batches of text data with Keras and TensorFlow.
  id: totrans-1597
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 使用 Keras 和 TensorFlow 创建文本数据批处理。
- en: '`How to create LSTM models in JAX and Flax. How to train and evaluate the LSTM
    model in Flax. Saving and restoring Flax LSTM models.`'
  id: totrans-1598
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`如何在 JAX 和 Flax 中创建 LSTM 模型。如何在 Flax 中训练和评估 LSTM 模型。保存和恢复 Flax LSTM 模型。`'
- en: '**Flax vs. TensorFlow**'
  id: totrans-1599
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**Flax vs. TensorFlow**'
- en: '`Flax is the neural network library for JAX. TensorFlow is a deep learning
    library with a large ecosystem of tools and resources. Flax and TensorFlow are
    similar but different in some ways. For instance, both Flax and TensorFlow can
    run on XLA.`'
  id: totrans-1600
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Flax 是建立在 JAX 上的神经网络库。TensorFlow 是一个拥有大量工具和资源的深度学习库。Flax 和 TensorFlow 在某些方面相似但又不同。例如，Flax
    和 TensorFlow 都可以在 XLA 上运行。`'
- en: '`Let''s look at the differences between Flax and TensorFlow from my perspective
    as a user of both libraries.`'
  id: totrans-1601
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 让我们从使用这两个库的用户角度来看一下 Flax 和 TensorFlow 的区别。
- en: '**Random number generation in TensorFlow and Flax**'
  id: totrans-1602
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**TensorFlow 和 Flax 中的随机数生成**'
- en: '`In TensorFlow, you can set global or function level seeds. Generating random
    numbers in TensorFlow is quite straightforward.`tf.random.set_seed(6853)`'
  id: totrans-1603
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`在 TensorFlow 中，您可以设置全局或函数级种子。在 TensorFlow 中生成随机数非常简单。`tf.random.set_seed(6853)`'
- en: However, this is not the case in Flax. Flax is built on top of JAX. JAX expects
    pure functions, meaning functions without any side effects. To achieve this JAX
    introduces stateless pseudo-random number generators (PRNGs). For example, calling
    the random number generator from NumPy will result in a different number every
    time.
  id: totrans-1604
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 然而，在 Flax 中并非如此。Flax 是建立在 JAX 之上的。JAX 期望纯函数，即没有任何副作用的函数。为了实现这一点，JAX 引入了无状态的伪随机数生成器（PRNGs）。例如，从
    NumPy 调用随机数生成器每次都会得到不同的数字。
- en: '`import numpy as np`'
  id: totrans-1605
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import numpy as np`'
- en: '`print(np.random.random()) print(np.random.random()) print(np.random.random())`'
  id: totrans-1606
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`print(np.random.random()) print(np.random.random()) print(np.random.random())`'
- en: '`![](../images/00046.jpeg)`'
  id: totrans-1607
  prefs: []
  stylish: true
  type: TYPE_IMG
  zh: '`![](../images/00046.jpeg)`'
- en: '`In JAX and Flax, the result should be the same on every call. We, therefore,
    generate random numbers from a random state. The state should not be re-used.
    It can be split to obtain several pseudo-random numbers.`'
  id: totrans-1608
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`在 JAX 和 Flax 中，每次调用时结果应该相同。因此，我们从随机状态生成随机数。状态不应该被重用。可以拆分状态以获取多个伪随机数。`'
- en: '`import jax`'
  id: totrans-1609
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import jax`'
- en: '`key = jax.random.PRNGKey(0)`'
  id: totrans-1610
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`key = jax.random.PRNGKey(0)`'
- en: '`key1, key2, key3 = jax.random.split(key, num=3)`'
  id: totrans-1611
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`key1, key2, key3 = jax.random.split(key, num=3)`'
- en: '`![](../images/00047.gif)`'
  id: totrans-1612
  prefs: []
  stylish: true
  type: TYPE_IMG
  zh: '`![](../images/00047.gif)`'
- en: '**Model definition in Flax and TensorFlow**'
  id: totrans-1613
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**Flax 和 TensorFlow 的模型定义**'
- en: '`Model definition in TensorFlow is made easy by the Keras API. You can use
    Keras to define Sequential or Functional networks. Keras has many layers for designing
    various types of networks, such as CNNs, and LSTMS.`'
  id: totrans-1614
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`在 TensorFlow 中，通过 Keras API 可以轻松定义模型。您可以使用 Keras 定义顺序或功能型网络。Keras 提供了许多层用于设计各种类型的网络，例如
    CNN 和 LSTM。`'
- en: '`In Flax, networks are designed using the setup or compact way. The setup method
    is explicit, while the compact way is in-line. Setup is very similar to how networks
    are designed in PyTorch. For example, here is a network designed with the setup
    way.`'
  id: totrans-1615
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在 Flax 中，网络可以使用 setup 或紧凑方式设计。setup 方法显式，而紧凑方式内联。Setup 非常类似于 PyTorch 中设计网络的方式。例如，这里是使用
    setup 方式设计的网络。
- en: '`class MLP(nn.Module):def setup(self):`'
  id: totrans-1616
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`class MLP(nn.Module):def setup(self):`'
- en: '`Submodule names are derived by the attributes you assign to. In this`'
  id: totrans-1617
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`子模块名称由您分配的属性派生而来。在这`'
- en: '`case`, `"dense1"` and `"dense2"`. This follows the logic in Py Torch.'
  id: totrans-1618
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`case`, `"dense1"` 和 `"dense2"`。这遵循了 PyTorch 的逻辑。'
- en: '`self.dense1 = nn.Dense(32)`'
  id: totrans-1619
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.dense1 = nn.Dense(32)`'
- en: '`self.dense2 = nn.Dense(32)`'
  id: totrans-1620
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.dense2 = nn.Dense(32)`'
- en: '`def __call__(self, x): x = self.dense1(x) x = nn.relu(x)`'
  id: totrans-1621
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def __call__(self, x): x = self.dense1(x) x = nn.relu(x)`'
- en: '`x = self.dense2(x) return x`'
  id: totrans-1622
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = self.dense2(x) return x`'
- en: Here's the same network designed in a compact way. The compact way is more straightforward
    because there is less code duplicity.`class MLP(nn.Module):`
  id: totrans-1623
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 这里是同一个网络的紧凑设计方式。紧凑的方式更为直接，因为代码重复较少。`class MLP(nn.Module):`
- en: '`@nn.compact`'
  id: totrans-1624
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`@nn.compact`'
- en: '`def __call__(self, x):`'
  id: totrans-1625
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def __call__(self, x):`'
- en: '`x = nn.Dense(32, name="dense1")(x)`'
  id: totrans-1626
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = nn.Dense(32, name="dense1")(x)`'
- en: '`x = nn.relu(x)`'
  id: totrans-1627
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = nn.relu(x)`'
- en: '`x = nn.Dense(32, name="dense2")(x)`'
  id: totrans-1628
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = nn.Dense(32, name="dense2")(x)`'
- en: '`return x`'
  id: totrans-1629
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return x`'
- en: '**Activations in Flax and TensorFlow**'
  id: totrans-1630
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**Flax 和 TensorFlow 中的激活函数**'
- en: The `[tf.keras.activations]` module in TensorFlow provides most of the activations
    needed when designing networks. In Flax, activation functions are available via
    the linen module.
  id: totrans-1631
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[tf.keras.activations]`模块在TensorFlow中提供设计网络时所需的大部分激活函数。在Flax中，激活函数通过linen模块提供。'
- en: '**Optimizers in Flax and TensorFlow**'
  id: totrans-1632
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**在Flax和TensorFlow中的优化器**'
- en: The `[tf.keras.optimizers]` in TensorFlow has popular optimizer functions. However,
    Flax doesn't ship with any optimizer functions. Optimizers used in Flax are provided
    by another library known as Optax.
  id: totrans-1633
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[tf.keras.optimizers]`中的优化器在TensorFlow中有流行的优化器函数。但是，Flax不提供任何优化器函数。Flax中使用的优化器由另一个名为Optax的库提供。'
- en: '**Metrics in Flax and TensorFlow**'
  id: totrans-1634
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**在Flax和TensorFlow中的指标**'
- en: In TensorFlow, metrics are available via
  id: totrans-1635
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在TensorFlow中，指标可以通过
- en: the `[tf.keras.metrics]` module. As of this writing, Flax has no metrics module.
    You'll need to define metric functions for your networks or use other third-party
    libraries.
  id: totrans-1636
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[tf.keras.metrics]`模块。截至目前，Flax没有指标模块。您需要为您的网络定义指标函数或使用其他第三方库。'
- en: '`import optax`'
  id: totrans-1637
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import optax`'
- en: '`import jax.numpy as jnp`'
  id: totrans-1638
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import jax.numpy as jnp`'
- en: '`def compute_metrics(logits, labels):`'
  id: totrans-1639
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def compute_metrics(logits, labels):`'
- en: '`loss = jnp.mean(optax.softmax_cross_entropy(logits, jax.nn.on e_hot(labels,
    num_classes=2)))`'
  id: totrans-1640
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`loss = jnp.mean(optax.softmax_cross_entropy(logits, jax.nn.on e_hot(labels,
    num_classes=2)))`'
- en: '`accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)`'
  id: totrans-1641
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)`'
- en: '`metrics = {`'
  id: totrans-1642
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`metrics = {`'
- en: '`''loss'': loss,`'
  id: totrans-1643
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`''loss'': loss,`'
- en: '`''accuracy'': accuracy`'
  id: totrans-1644
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`''accuracy'': accuracy`'
- en: '`}`'
  id: totrans-1645
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`}`'
- en: '`return metrics`'
  id: totrans-1646
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return metrics`'
- en: '**Computing gradients in Flax and TensorFlow**'
  id: totrans-1647
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**在Flax和TensorFlow中计算梯度**'
- en: The `[jax.grad]` function is used to compute gradients in Flax. It offers the
    ability to return auxillary data. For example, you can return loss and gradients
    at the same time.
  id: totrans-1648
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[jax.grad]`函数用于在Flax中计算梯度。它提供同时返回损失和梯度的能力，例如，您可以同时返回损失和梯度。'
- en: '`@jax.jitdef sum_logistic(x):return jnp.sum(1.0 / (1.0 + jnp.exp(-x))),(x +
    1)`'
  id: totrans-1649
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`@jax.jitdef sum_logistic(x):return jnp.sum(1.0 / (1.0 + jnp.exp(-x))),(x +
    1)`'
- en: '`x_small = jnp.arange(6.)`'
  id: totrans-1650
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x_small = jnp.arange(6.)`'
- en: '`derivative_fn = jax.grad(sum_logistic, has_aux=True) print(derivative_fn(x_small))`'
  id: totrans-1651
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`derivative_fn = jax.grad(sum_logistic, has_aux=True) print(derivative_fn(x_small))`'
- en: '`(DeviceArray([0.25 , 0.19661194, 0.10499357, 0.04517666, 0.01766271,`'
  id: totrans-1652
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`(DeviceArray([0.25 , 0.19661194, 0.10499357, 0.04517666, 0.01766271,`'
- en: '`0.00664806], dtype=float32), DeviceArray([1., 2.,`'
  id: totrans-1653
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`0.00664806], dtype=float32), DeviceArray([1., 2.,`'
- en: '`3., 4., 5., 6.], dtype=float32))Advanced automatic differentiation can also
    be done`'
  id: totrans-1654
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`3., 4., 5., 6.], dtype=float32))高级自动微分也可以完成`'
- en: using `jax.vjp()` and `jax.jvp()`.
  id: totrans-1655
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 使用`jax.vjp()`和`jax.jvp()`。
- en: In TensorFlow, gradients are computed using `[tf.GradientTape].def grad(model,
    inputs, targets):with tf.GradientTape() as tape:loss_value = loss(model, inputs,
    targets, training=True) return loss_value, tape.gradient(loss_value, model.trainable_
    variables)`
  id: totrans-1656
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在TensorFlow中，使用`[tf.GradientTape]`计算梯度。def grad(model, inputs, targets):with
    tf.GradientTape() as tape:loss_value = loss(model, inputs, targets, training=True)
    return loss_value, tape.gradient(loss_value, model.trainable_ variables)`
- en: Unless you are creating custom training loops in TensorFlow, you will not define
    a gradient function. This is done automatically when you train the network.
  id: totrans-1657
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 除非您在TensorFlow中创建自定义训练循环，否则不会定义梯度函数。当您训练网络时，这是自动完成的。
- en: '**Loading datasets in Flax and TensorFlow**'
  id: totrans-1658
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**在Flax和TensorFlow中加载数据集**'
- en: TensorFlow provides utilities for loading data. Flax doesn't ship with any data
    loaders. You have to use the data loaders from other libraries such as TensorFlow.
    As long as the data is in JAX NumPy or regular arrays and has the proper shape,
    it can be passed to Flax networks.
  id: totrans-1659
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: TensorFlow提供了加载数据的实用程序。Flax不附带任何数据加载器。您必须使用来自其他库（如TensorFlow）的数据加载器。只要数据是JAX
    NumPy或常规数组，并且具有适当的形状，就可以传递给Flax网络。
- en: '**Training model in Flax vs. TensorFlow**'
  id: totrans-1660
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**在Flax与TensorFlow中训练模型**'
- en: Training models in TensorFlow is done by compiling the network and calling the
    fit method. However, in Flax, we create a training state to hold the training
    information and then pass data to the network.
  id: totrans-1661
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在TensorFlow中，通过编译网络并调用fit方法来训练模型。然而，在Flax中，我们创建一个训练状态来保存训练信息，然后将数据传递给网络。
- en: '`from flax.training import train_state`'
  id: totrans-1662
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from flax.training import train_state`'
- en: '`def create_train_state(rng):`'
  id: totrans-1663
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def create_train_state(rng):`'
- en: '"""Creates initial `TrainState`."""'
  id: totrans-1664
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""创建初始`TrainState`。"""'
- en: '`model = LSTMModel()`'
  id: totrans-1665
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`model = LSTMModel()`'
- en: '`params = model.init(rng, jnp.array(X_train_padded[0]))[''param`'
  id: totrans-1666
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`params = model.init(rng, jnp.array(X_train_padded[0]))[''param`'
- en: '`s'']`'
  id: totrans-1667
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`s'']`'
- en: '`tx = optax.adam(0.001,0.9,0.999,1e-07)`'
  id: totrans-1668
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`tx = optax.adam(0.001,0.9,0.999,1e-07)`'
- en: '`return train_state.TrainState.create(`'
  id: totrans-1669
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return train_state.TrainState.create(`'
- en: '`apply_fn=model.apply, params=params, tx=tx)`'
  id: totrans-1670
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`apply_fn=model.apply, params=params, tx=tx)`'
- en: After that, we define a training step that will compute the loss and gradients.
    It then uses these gradients to update the model parameters and returns the model
    metrics and the new state.
  id: totrans-1671
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 之后，我们定义一个训练步骤，计算损失和梯度。然后使用这些梯度来更新模型参数并返回模型指标和新状态。
- en: '`@jax.jitdef train_step(state, text, labels):def loss_fn(params):`'
  id: totrans-1672
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`@jax.jitdef train_step(state, text, labels):def loss_fn(params):`'
- en: '`logits = LSTMModel().apply({''params'': params}, text) loss = jnp.mean(optax.softmax_cross_entropy(
    logits=logits,`'
  id: totrans-1673
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`logits = LSTMModel().apply({''params'': params}, text) loss = jnp.mean(optax.softmax_cross_entropy(
    logits=logits,`'
- en: '`labels=jax.nn.one_hot(labels, num_classes=2))) return loss, logits`'
  id: totrans-1674
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`labels=jax.nn.one_hot(labels, num_classes=2))) return loss, logits`'
- en: '`grad_fn = jax.value_and_grad(loss_fn, has_aux=True) (_, logits), grads = grad_fn(state.params)`'
  id: totrans-1675
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`grad_fn = jax.value_and_grad(loss_fn, has_aux=True) (_, logits), grads = grad_fn(state.params)`'
- en: '`state = state.apply_gradients(grads=grads)`'
  id: totrans-1676
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`state = state.apply_gradients(grads=grads)`'
- en: '`metrics = compute_metrics(logits, labels)`'
  id: totrans-1677
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`metrics = compute_metrics(logits, labels)`'
- en: '`return state, metrics`'
  id: totrans-1678
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return state, metrics`'
- en: Use `Elegy` to train networks like in Keras. Elegy is a high-level API for JAX
    neural network libraries.
  id: totrans-1679
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 使用`Elegy`来训练类似于Keras的网络。Elegy是一个基于JAX神经网络库的高级API。
- en: '**Distributed training in Flax and TensorFlow**'
  id: totrans-1680
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**Flax和TensorFlow中的分布式训练**'
- en: Training networks in TensorFlow in a distributed manner is done by creating `distributed
    strategy.mirrored_strategy = tf.distribute.MirroredStrategy()`
  id: totrans-1681
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在TensorFlow中以分布式方式训练网络是通过创建`distributed strategy.mirrored_strategy = tf.distribute.MirroredStrategy()`来完成的。
- en: '`with mirrored_strategy.scope():`'
  id: totrans-1682
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`with mirrored_strategy.scope():`'
- en: '`model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_s`'
  id: totrans-1683
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_s`'
- en: '`hape=(1,))])`'
  id: totrans-1684
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`hape=(1,))])`'
- en: '`model.compile(loss=''mse'', optimizer=''sgd'')`'
  id: totrans-1685
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`model.compile(loss=''mse'', optimizer=''sgd'')`'
- en: To train networks in a distributed way in Flax, we define distributed versions
    of our Flax functions. This is done using the [`pmap`] function that executes
    a function on multiple devices. You'll then compute predictions from all devices
    and get the average
  id: totrans-1686
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 要在Flax中以分布式方式训练网络，我们定义我们的Flax函数的分布式版本。这通过`pmap`函数完成，该函数在多个设备上执行函数。然后，您将计算所有设备上的预测并获得平均值。
- en: using [`jax.lax.pmean()`]. You also need to replicate the data on all the devices
    using [`jax_utils.replicate`] . To obtain metrics from the
  id: totrans-1687
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 使用[`jax.lax.pmean()`]。你还需要使用[`jax_utils.replicate`]在所有设备上复制数据，以获取指标。
- en: device use `jax_utils.unreplicate`.
  id: totrans-1688
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 设备使用`jax_utils.unreplicate`。
- en: '**Working with TPU accelerators**'
  id: totrans-1689
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**使用TPU加速器**'
- en: You can use Flax and TensorFlow with TPU and GPU accelerators. To use Flax with
    TPUs on Colab, you'll need to set it up:`jax.tools.colab_tpu.setup_tpu() jax.devices()![](../images/00048.gif)`For
    TensorFlow, set up the `TPU distributed strategy.cluster_resolver = tf.distribute.cluster_resolver.TPUClusterRes
    olver(tpu=tpu_address)`
  id: totrans-1690
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 您可以使用Flax和TensorFlow与TPU和GPU加速器。要在Colab上使用Flax与TPU，您需要设置它：`jax.tools.colab_tpu.setup_tpu()
    jax.devices()![](../images/00048.gif)`对于TensorFlow，设置`TPU distributed strategy.cluster_resolver
    = tf.distribute.cluster_resolver.TPUClusterRes olver(tpu=tpu_address)`
- en: '`tf.config.experimental_connect_to_cluster(cluster_resolver) tf.tpu.experimental.initialize_tpu_system(cluster_resolver)
    tpu_strategy = tf.distribute.TPUStrategy(cluster_resolver)`'
  id: totrans-1691
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`tf.config.experimental_connect_to_cluster(cluster_resolver) tf.tpu.experimental.initialize_tpu_system(cluster_resolver)
    tpu_strategy = tf.distribute.TPUStrategy(cluster_resolver)`'
- en: '**Model evaluation**'
  id: totrans-1692
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**模型评估**'
- en: TensorFlow provides the [`evaluate`] function for evaluating networks. Flax
    doesn't ship with such a function. You'll need to create a function that applies
    the model and returns the test metrics. `Elegy` provides Keras-like functions
    such as the `evaluate` method.
  id: totrans-1693
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: TensorFlow提供了`evaluate`函数用于评估网络。Flax没有提供这样的函数。您需要创建一个应用模型并返回测试指标的函数。`Elegy`提供类似Keras的功能，例如`evaluate`方法。
- en: '`@jax.jit`'
  id: totrans-1694
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`@jax.jit`'
- en: '`def eval_step(state, text, labels):`'
  id: totrans-1695
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def eval_step(state, text, labels):`'
- en: '`logits = LSTMModel().apply({''params'': state.params}, text)`'
  id: totrans-1696
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`logits = LSTMModel().apply({''params'': state.params}, text)`'
- en: '`return compute_metrics(logits=logits, labels=labels)`'
  id: totrans-1697
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return compute_metrics(logits=logits, labels=labels)`'
- en: '`def evaluate_model(state, text, test_lbls): """Evaluate on the validation
    set."""`'
  id: totrans-1698
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def evaluate_model(state, text, test_lbls): """Evaluate on the validation
    set."""`'
- en: '`metrics = eval_step(state, text, test_lbls) metrics = jax.device_get(metrics)`'
  id: totrans-1699
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`metrics = eval_step(state, text, test_lbls) metrics = jax.device_get(metrics)`'
- en: '`metrics = jax.tree_map(lambda x: x.item(), metrics) return metrics`'
  id: totrans-1700
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`metrics = jax.tree_map(lambda x: x.item(), metrics) return metrics`'
- en: '**Visualize model performance**'
  id: totrans-1701
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**可视化模型性能**'
- en: Model visualizing is similar in Flax and TensorFlow. Once you obtain the metrics,
    you can use a package such as `Matplotlib` to visualize the model's performance.
    You can also use `TensorBoard` in both Flax and TensorFlow.
  id: totrans-1702
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 模型的可视化在 Flax 和 TensorFlow 中是类似的。一旦获得指标，可以使用诸如 `Matplotlib` 之类的软件包来可视化模型的性能。你也可以在
    Flax 和 TensorFlow 中使用 `TensorBoard`。
- en: '**Final thoughts**'
  id: totrans-1703
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**最后的思考**'
- en: You have seen the differences between the Flax and TensorFlow libraries. In
    particular, have seen the difference in model definition and training.
  id: totrans-1704
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 你已经看到了 Flax 和 TensorFlow 库之间的差异。特别是，在模型定义和训练方面有所不同。
- en: Train ResNet in Flax from scratch(Distributed ResNet training)
  id: totrans-1705
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 在 Flax 中从头开始训练 ResNet（分布式 ResNet 训练）
- en: Apart from designing custom CNN architectures, you can use architectures that
    have already been built. ResNet is one such popular architecture. In most cases,
    you'll achieve better performance by using such architectures. In this article,
    you will learn how to perform distributed training of a ResNet model in Flax.
  id: totrans-1706
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 除了设计定制的 CNN 架构外，您还可以使用已经构建好的架构。ResNet 就是这样一个流行的架构之一。在大多数情况下，使用这样的架构会获得更好的性能。在本文中，您将学习如何在
    Flax 中进行 ResNet 模型的分布式训练。
- en: Install Flax models
  id: totrans-1707
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 安装 Flax 模型
- en: 'The [flaxmodels] package provides pre-trained models for Jax and Flax, including:'
  id: totrans-1708
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '[flaxmodels] 包为 Jax 和 Flax 提供了预训练模型，包括：'
- en: StyleGAN2
  id: totrans-1709
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: StyleGAN2
- en: '`GPT2`'
  id: totrans-1710
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`GPT2`'
- en: '`VGG`'
  id: totrans-1711
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`VGG`'
- en: '`ResNet`'
  id: totrans-1712
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`ResNet`'
- en: '`git clone https://github.com/matthias-wright/flaxmodels.git pip install -r
    flaxmodels/training/resnet/requirements.txt`'
  id: totrans-1713
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`git clone https://github.com/matthias-wright/flaxmodels.git pip install -r
    flaxmodels/training/resnet/requirements.txt`'
- en: In this project, we will train the model from scratch– meaning that we will
    not use the pre-trained weights. In a separate article, we have covered how to
    perform transfer learning with ResNet.
  id: totrans-1714
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在这个项目中，我们将从头开始训练模型——也就是说，我们不会使用预训练的权重。在另一篇文章中，我们已经讨论了如何使用 ResNet 进行迁移学习。
- en: Perform standard imports
  id: totrans-1715
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 执行标准导入
- en: With [flaxmodels] installed, let's import the standard libraries used in this
    article.
  id: totrans-1716
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 安装了 [flaxmodels] 后，让我们导入本文中使用的标准库。
- en: import`wget`# pip install wget
  id: totrans-1717
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: import`wget`# pip install wget
- en: import`zipfile`
  id: totrans-1718
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: import`zipfile`
- en: '`import torch`'
  id: totrans-1719
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import torch`'
- en: from`torch.utils.data`import`DataLoader import os`
  id: totrans-1720
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: from`torch.utils.data`import`DataLoader import os`
- en: from`PIL`import`Image`
  id: totrans-1721
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: from`PIL`import`Image`
- en: from`torchvision`import`transforms from torch.utils.data import Dataset import
    numpy as np`
  id: totrans-1722
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: from`torchvision`import`transforms from torch.utils.data import Dataset import
    numpy as np`
- en: '`import pandas as pd`'
  id: totrans-1723
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import pandas as pd`'
- en: '`import matplotlib.pyplot as plt`'
  id: totrans-1724
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import matplotlib.pyplot as plt`'
- en: '%matplotlib inline'
  id: totrans-1725
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '%matplotlib inline'
- en: ignore harmless warnings
  id: totrans-1726
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: 忽略无害的警告
- en: '`import warnings`'
  id: totrans-1727
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import warnings`'
- en: '`warnings.filterwarnings("ignore") import jax`'
  id: totrans-1728
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`warnings.filterwarnings("ignore") import jax`'
- en: from`jax`import`numpy as jnp`
  id: totrans-1729
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: from`jax`import`numpy as jnp`
- en: '`import flax`'
  id: totrans-1730
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import flax`'
- en: from`flax`import`linen as nn`
  id: totrans-1731
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: from`flax`import`linen as nn`
- en: from`flax.training`import`train_state import optax`
  id: totrans-1732
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: from`flax.training`import`train_state import optax`
- en: import`time`
  id: totrans-1733
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: import`time`
- en: '`from tqdm.notebook import tqdm`'
  id: totrans-1734
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from tqdm.notebook import tqdm`'
- en: '`import math`'
  id: totrans-1735
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import math`'
- en: from`flax`import`jax_utils`
  id: totrans-1736
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: from`flax`import`jax_utils`
- en: Download dataset
  id: totrans-1737
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 下载数据集
- en: We will train the ResNet model to predict two classes from the cats and dogs dataset.
    Download and extract the cat and dog images.
  id: totrans-1738
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 我们将训练 ResNet 模型来预测来自猫和狗数据集的两类。下载并提取猫和狗的图像。
- en: '`wget.download("https://ml.machinelearningnuggets.com/train.zi p")`'
  id: totrans-1739
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`wget.download("https://ml.machinelearningnuggets.com/train.zi p")`'
- en: with`zipfile.ZipFile('train.zip', 'r') as zip_ref:`
  id: totrans-1740
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: with`zipfile.ZipFile('train.zip', 'r') as zip_ref:`
- en: '`zip_ref.extractall(''.'')`'
  id: totrans-1741
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`zip_ref.extractall(''.'')`'
- en: Loading dataset in Flax
  id: totrans-1742
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 在 Flax 中加载数据集
- en: Since JAX and Flax don't ship with any data loaders, we use data loading utilities
    from PyTorch or TensorFlow. When using PyTorch, we start by creating a dataset
    class.
  id: totrans-1743
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 由于 JAX 和 Flax 不包含任何数据加载器，我们使用 PyTorch 或 TensorFlow 中的数据加载工具。当使用 PyTorch 时，我们首先创建一个数据集类。
- en: '`class CatsDogsDataset(Dataset):`'
  id: totrans-1744
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`class CatsDogsDataset(Dataset):`'
- en: '`def __init__(self, root_dir, annotation_file, transform=Non`'
  id: totrans-1745
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def __init__(self, root_dir, annotation_file, transform=Non`'
- en: 'e):'
  id: totrans-1746
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'e):'
- en: '`self.root_dir = root_dir`'
  id: totrans-1747
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.root_dir = root_dir`'
- en: '`self.annotations = pd.read_csv(annotation_file) self.transform = transform`'
  id: totrans-1748
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.annotations = pd.read_csv(annotation_file) self.transform = transform`'
- en: '`def __len__(self):return len(self.annotations)`'
  id: totrans-1749
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def __len__(self):return len(self.annotations)`'
- en: '`def __getitem__(self, index):`'
  id: totrans-1750
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def __getitem__(self, index):`'
- en: '`img_id = self.annotations.iloc[index, 0]`'
  id: totrans-1751
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`img_id = self.annotations.iloc[index, 0]`'
- en: '`img = Image.open(os.path.join(self.root_dir, img_id)).c`'
  id: totrans-1752
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`img = Image.open(os.path.join(self.root_dir, img_id)).c`'
- en: '`convert("RGB")`'
  id: totrans-1753
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`convert("RGB")`'
- en: '`y_label = torch.tensor(float(self.annotations.iloc[inde`'
  id: totrans-1754
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`y_label = torch.tensor(float(self.annotations.iloc[inde`'
- en: x, 1]))
  id: totrans-1755
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x, 1]))
- en: 'if self.transform is not None: img = self.transform(img)return (img, y_label)Next,
    create a Pandas DataFrame containing the image paths and labels.'
  id: totrans-1756
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 如果存在转换，则进行转换：img = self.transform(img)return (img, y_label)接下来，创建包含图像路径和标签的Pandas
    DataFrame。
- en: '`train_df = pd.DataFrame(columns=["img_path","label"]) train_df["img_path"]
    = os.listdir("train/") for idx, i in enumerate(os.listdir("train/")):`'
  id: totrans-1757
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_df = pd.DataFrame(columns=["img_path","label"]) train_df["img_path"]
    = os.listdir("train/") for idx, i in enumerate(os.listdir("train/")):`'
- en: '`if "cat" in i:`'
  id: totrans-1758
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`if "cat" in i:`'
- en: '`train_df["label"][idx] = 0`'
  id: totrans-1759
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_df["label"][idx] = 0`'
- en: '`if "dog" in i:`'
  id: totrans-1760
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`if "dog" in i:`'
- en: '`train_df["label"][idx] = 1`'
  id: totrans-1761
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_df["label"][idx] = 1`'
- en: '`train_df.to_csv (r''train_csv.csv'', index = False, header=True)`'
  id: totrans-1762
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_df.to_csv (r''train_csv.csv'', index = False, header=True)`'
- en: Data transformation in Flax
  id: totrans-1763
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: Flax中的数据转换
- en: Define a function that will stack the data and return it as a NumPy array.
  id: totrans-1764
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 定义一个将数据堆叠并作为NumPy数组返回的函数。
- en: '`def custom_collate_fn(batch):`'
  id: totrans-1765
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def custom_collate_fn(batch):`'
- en: '`transposed_data = list(zip(*batch)) labels = np.array(transposed_data[1])
    imgs = np.stack(transposed_data[0]) return imgs, labels`'
  id: totrans-1766
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`transposed_data = list(zip(*batch)) labels = np.array(transposed_data[1])
    imgs = np.stack(transposed_data[0]) return imgs, labels`'
- en: Create a transformation for resizing the images. Next, apply the transformation
    to the dataset created earlier.`size_image = 224`
  id: totrans-1767
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 创建用于调整图像大小的转换。接下来，将该转换应用于早期创建的数据集。`size_image = 224`
- en: '`transform = transforms.Compose([`'
  id: totrans-1768
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`transform = transforms.Compose([`'
- en: '`transforms.Resize((size_image,size_image)), np.array])`'
  id: totrans-1769
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`transforms.Resize((size_image,size_image)), np.array])`'
- en: '`dataset = CatsDogsDataset("train","train_csv.csv",transform=transform)`'
  id: totrans-1770
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`dataset = CatsDogsDataset("train","train_csv.csv",transform=transform)`'
- en: Split this dataset into a training and testing set and create data loaders for
    each set.`batch_size = 32`
  id: totrans-1771
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 将此数据集拆分为训练集和测试集，并为每个集创建数据加载器。`batch_size = 32`
- en: '`train_set, validation_set = torch.utils.data.random_split(dataset,[20000,5000])`'
  id: totrans-1772
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_set, validation_set = torch.utils.data.random_split(dataset,[20000,5000])`'
- en: '`train_loader = DataLoader(dataset=train_set, collate_fn=custom_collate_fn,shuffle=True,
    batch_size=batch_size)`'
  id: totrans-1773
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_loader = DataLoader(dataset=train_set, collate_fn=custom_collate_fn,shuffle=True,
    batch_size=batch_size)`'
- en: '`validation_loader = DataLoader(dataset=validation_set,collate_fn=custom_collate_fn,
    shuffle=False, batch_size=batch_size)`'
  id: totrans-1774
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`validation_loader = DataLoader(dataset=validation_set,collate_fn=custom_collate_fn,
    shuffle=False, batch_size=batch_size)`'
- en: Instantiate Flax ResNet model
  id: totrans-1775
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 实例化Flax ResNet模型
- en: 'With the data in place, instantiate the Flax ResNet model using the `[flaxmodels]` package.
    The instantiation requires:'
  id: totrans-1776
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 有了数据准备就绪，使用`[flaxmodels]`包实例化Flax ResNet模型。实例化需要：
- en: The desired number of classes.
  id: totrans-1777
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 所需的类别数。
- en: The type of output.
  id: totrans-1778
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 输出类型。
- en: The data type.
  id: totrans-1779
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 数据类型。
- en: Whether the model is pre-trained– in this case `[False]`.import jax.numpy as
    jnp import flaxmodels as fm
  id: totrans-1780
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 是否预训练模型 - 在这种情况下`[False]`。import jax.numpy as jnp import flaxmodels as fm
- en: '`num_classes = 2`'
  id: totrans-1781
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`num_classes = 2`'
- en: '`dtype = jnp.float32 model = fm.ResNet50(output=''log_softmax'', pretrained=None,
    num_classes=num_classes, dtype=dtype)`'
  id: totrans-1782
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`dtype = jnp.float32 model = fm.ResNet50(output=''log_softmax'', pretrained=None,
    num_classes=num_classes, dtype=dtype)`'
- en: Compute metrics
  id: totrans-1783
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 计算指标
- en: Define the metrics for evaluating the model during training. Let's start by
    creating the loss function.
  id: totrans-1784
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 定义评估模型训练期间的指标。让我们首先创建损失函数。
- en: '`def cross_entropy_loss(*, logits, labels):`'
  id: totrans-1785
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def cross_entropy_loss(*, logits, labels):`'
- en: '`labels_onehot = jax.nn.one_hot(labels, num_classes=num_classes)`'
  id: totrans-1786
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`labels_onehot = jax.nn.one_hot(labels, num_classes=num_classes)`'
- en: '`s)`'
  id: totrans-1787
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`s)`'
- en: '`return optax.softmax_cross_entropy(logits=logits, labels=labels)`'
  id: totrans-1788
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return optax.softmax_cross_entropy(logits=logits, labels=labels)`'
- en: '`ls_onehot).mean()`'
  id: totrans-1789
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`ls_onehot).mean()`'
- en: Next, define a function that computes and returns the loss and accuracy.
  id: totrans-1790
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 接下来，定义一个计算并返回损失和准确率的函数。
- en: '`def compute_metrics(*, logits, labels):`'
  id: totrans-1791
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def compute_metrics(*, logits, labels):`'
- en: '`loss = cross_entropy_loss(logits=logits, labels=labels) accuracy = jnp.mean(jnp.argmax(logits,
    -1) == labels) metrics = {`'
  id: totrans-1792
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`loss = cross_entropy_loss(logits=logits, labels=labels) accuracy = jnp.mean(jnp.argmax(logits,
    -1) == labels) metrics = {`'
- en: '`''loss'': loss,`'
  id: totrans-1793
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`''loss'': loss'',`'
- en: '`''accuracy'': accuracy`,'
  id: totrans-1794
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`''accuracy'': accuracy`,'
- en: '`}`'
  id: totrans-1795
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`}`'
- en: '`return metrics`'
  id: totrans-1796
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return metrics`'
- en: Create Flax model training state
  id: totrans-1797
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 创建Flax模型训练状态
- en: Flax provides a training state for storing training information. The training
    state can be modified to add new information. In this case, we need to alter the
    training state to add the batch statistics since the ResNet model computes `[batch_stats]`.
  id: totrans-1798
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: Flax提供了用于存储训练信息的训练状态。可以修改训练状态以添加新信息。在这种情况下，我们需要修改训练状态以添加批次统计信息，因为ResNet模型计算`[batch_stats]`。
- en: '`class TrainState(train_state.TrainState): batch_stats: flax.core.FrozenDict`'
  id: totrans-1799
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`class TrainState(train_state.TrainState): batch_stats: flax.core.FrozenDict`'
- en: We need the model parameters and batch statistics to create the training state
    function. We can access these by initializing the model with the `[train]` as `[False]`.
  id: totrans-1800
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 我们需要模型参数和批次统计来创建训练状态函数。我们可以通过将模型初始化为`[train]`为`[False]`来访问这些内容。
- en: '`key = jax.random.PRNGKey(0)`'
  id: totrans-1801
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`key = jax.random.PRNGKey(0)`'
- en: '`variables = model.init(key, jnp.ones([1, size_image, size_image, 3]), train=False)`'
  id: totrans-1802
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`variables = model.init(key, jnp.ones([1, size_image, size_image, 3]), train=False)`'
- en: 'The `create` method of TrainState expects the following parameters: The `[apply_fn]` –
    model apply function. The model parameters– `[variables[''params'']]`.'
  id: totrans-1803
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`TrainState`的`create`方法需要以下参数：`[apply_fn]`– 模型应用函数。模型参数– `variables[''params'']`。'
- en: The optimizer, usually defined using Optax.
  id: totrans-1804
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 优化器通常使用Optax定义。
- en: The batch statistics– `variables['batch_stats']`.
  id: totrans-1805
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 批次统计– `variables['batch_stats']`。
- en: We apply `[pmap]` to this function to create a distributed version of the training
    state.  `[pmap]` compiles the function for execution on multiple devices such
    as multiple GPUs and TPUs.
  id: totrans-1806
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 我们对这个函数应用`[pmap]`来创建一个分布式版本的训练状态。 `[pmap]` 编译该函数以在多个设备上执行，例如多个GPU和TPU。
- en: '`import functools`'
  id: totrans-1807
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import functools`'
- en: '`@functools.partial(jax.pmap)`'
  id: totrans-1808
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`@functools.partial(jax.pmap)`'
- en: '`def create_train_state(rng):`'
  id: totrans-1809
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def create_train_state(rng):`'
- en: '"""Creates initial `TrainState`."""'
  id: totrans-1810
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""创建初始的`TrainState`。"""'
- en: '`return TrainState.create(apply_fn = model.apply,params = variables[''params''],tx
    = optax.adam(0.01,0.9),batch_stats = variables[''batch_stats''])`'
  id: totrans-1811
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return TrainState.create(apply_fn = model.apply,params = variables[''params''],tx
    = optax.adam(0.01,0.9),batch_stats = variables[''batch_stats''])`'
- en: '**Apply model function**'
  id: totrans-1812
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**应用模型函数**'
- en: 'Next, define a parallel model training function. Pass an `[axis_name]` so you
    can use that to aggregate the metrics from all the devices. The function:'
  id: totrans-1813
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 接下来，定义一个并行模型训练函数。传递一个`[axis_name]`以便您可以使用它来聚合来自所有设备的指标。该函数：
- en: Computes the loss.
  id: totrans-1814
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 计算损失。
- en: Computes predictions from all devices by calculating the average of the probabilities
    using `[jax.lax.pmean()]` .
  id: totrans-1815
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 通过计算使用`[jax.lax.pmean()]` 的概率的平均值来从所有设备上计算预测。
- en: When applying the model, we also include the batch statistics and the random
    number for  `[DropOut]`. Since this is the training function, the `[train]` parameter
    is `[True]`. The `[batch_stats]` are also included when computing the gradients.
    The `[update_model]` function applies the computed gradients– updates the model
    parameters.
  id: totrans-1816
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在应用模型时，我们还包括了批次统计和`[DropOut]`的随机数。由于这是训练函数，`[train]`参数是`[True]`。在计算梯度时，也包括了`[batch_stats]`。`[update_model]`函数应用计算得到的梯度–
    更新模型参数。
- en: '`@functools.partial(jax.pmap, axis_name=''ensemble'')` `def apply_model(state,
    images, labels):` `def loss_fn(params,batch_stats):`'
  id: totrans-1817
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`@functools.partial(jax.pmap, axis_name=''ensemble'')` `def apply_model(state,
    images, labels):` `def loss_fn(params,batch_stats):`'
- en: '`logits, batch_stats = model.apply({''params'': params, ''batch_stats'': batch_stats},
    images, train=True, rngs={''dropout'': jax.random.PRNGKey(0)}, mutable=[''batch_stats''])'
  id: totrans-1818
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`logits, batch_stats = model.apply({''params'': params, ''batch_stats'': batch_stats},
    images, train=True, rngs={''dropout'': jax.random.PRNGKey(0)}, mutable=[''batch_stats''])`'
- en: '`one_hot = jax.nn.one_hot(labels, num_classes)` `loss = optax.softmax_cross_entropy(logits=logits,
    labels=one_hot).mean()` return loss, `(logits, batch_stats)` `(loss, (logits,
    batch_stats)), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params,state.batch_stats)`
    `probs = jax.lax.pmean(jax.nn.softmax(logits), axis_name=''ensemble'')` `accuracy
    = jnp.mean(jnp.argmax(probs, -1) == labels)` return grads, `loss, accuracy@jax.pmap`
    `def update_model(state, grads):` return state.apply_gradients(grads=grads)'
  id: totrans-1819
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`one_hot = jax.nn.one_hot(labels, num_classes)` `loss = optax.softmax_cross_entropy(logits=logits,
    labels=one_hot).mean()` 返回损失，`(logits, batch_stats)` `(loss, (logits, batch_stats)),
    grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params,state.batch_stats)`
    `probs = jax.lax.pmean(jax.nn.softmax(logits), axis_name=''ensemble'')` `accuracy
    = jnp.mean(jnp.argmax(probs, -1) == labels)` 返回梯度，`损失，准确率@jax.pmap` `def update_model(state,
    grads):` 应用梯度 `return state.apply_gradients(grads=grads)`'
- en: '**TensorBoard in Flax**'
  id: totrans-1820
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**在Flax中使用TensorBoard**'
- en: The next step is to train the ResNet model. However, you might be interested
    in tracking the training using `[TensorBoard]`. In that case, you have to configure
    TensorBoard. You can write the metrics to TensorBoard using the PyTorch `SummaryWriter`.
  id: totrans-1821
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 下一步是训练ResNet模型。但是，您可能有兴趣使用`[TensorBoard]`跟踪训练。在这种情况下，您需要配置TensorBoard。您可以使用PyTorch的`SummaryWriter`将指标写入TensorBoard。
- en: '`rm -rf ./flax_logs/`'
  id: totrans-1822
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`rm -rf ./flax_logs/`'
- en: '`from torch.utils.tensorboard import SummaryWriter` `import torchvision.transforms.functional
    as F` `logdir = "flax_logs"`'
  id: totrans-1823
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from torch.utils.tensorboard import SummaryWriter` `import torchvision.transforms.functional
    as F` `logdir = "flax_logs"`'
- en: '`writer = SummaryWriter(logdir)`'
  id: totrans-1824
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`writer = SummaryWriter(logdir)`'
- en: '**Train Flax ResNet model**'
  id: totrans-1825
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**训练Flax ResNet模型**'
- en: Let's train the ResNet model on the entire training set and evaluate it on a
    subset of the test set. You can also evaluate it on the whole test set. Replicate
    the test set to the available devices.
  id: totrans-1826
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 让我们在整个训练集上训练 ResNet 模型，并在测试集的子集上进行评估。您也可以在整个测试集上进行评估。将测试集复制到可用设备上。
- en: '`(test_images, test_labels) = next(iter(validation_loader))` `test_images =
    test_images / 255.0`'
  id: totrans-1827
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`(test_images, test_labels) = next(iter(validation_loader))` `test_images =
    test_images / 255.0`'
- en: '`test_images = np.array(jax_utils.replicate(test_images))` `test_labels = np.array(jax_utils.replicate(test_labels))`'
  id: totrans-1828
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`test_images = np.array(jax_utils.replicate(test_images))` `test_labels = np.array(jax_utils.replicate(test_labels))`'
- en: Create some lists to hold the training and evaluation metrics.
  id: totrans-1829
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 创建一些列表以保存训练和评估指标。
- en: '`epoch_loss = []`'
  id: totrans-1830
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`epoch_loss = []`'
- en: '`epoch_accuracy = []` `testing_accuracy = []` `testing_loss = []`'
  id: totrans-1831
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`epoch_accuracy = []` `testing_accuracy = []` `testing_loss = []`'
- en: 'Next, define the ResNet model training function. The function does the following:'
  id: totrans-1832
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 接下来，定义 ResNet 模型训练函数。该函数执行以下操作：
- en: Loops through the training dataset and scales it.
  id: totrans-1833
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 循环遍历训练数据集并对其进行缩放。
- en: Replicates the data on the available devices.
  id: totrans-1834
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在可用设备上复制数据。
- en: Applies the model on the dataset and computes the metrics. Obtains the metrics
    from the devices
  id: totrans-1835
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在数据集上应用模型并计算指标。从设备获取指标
- en: '`using jax_utils.unreplicate.`'
  id: totrans-1836
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`using jax_utils.unreplicate.`'
- en: Appends the metrics to a list.
  id: totrans-1837
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 将指标附加到列表。
- en: Computes the mean of the loss and accuracy to obtain the metrics for each epoch.
  id: totrans-1838
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 计算损失和准确度的均值以获取每个 epoch 的指标。
- en: Applies the model to the test set and obtains the metrics.
  id: totrans-1839
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 将模型应用于测试集并获取指标。
- en: Append the test metrics to a list.
  id: totrans-1840
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 将测试指标附加到列表。
- en: Writes the training and evaluation metrics to TensorBaord.
  id: totrans-1841
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 将训练和评估指标写入 TensorBaord。
- en: 'Prints the training and evaluation metrics.def `train_one_epoch(state, dataloader,num_epochs):
    """Train for 1 epoch on the training set."""for epoch in range(num_epochs):for
    cnt, (images, labels) in tqdm(enumerate(dataloade r), total=(math.ceil(len(train_set)/batch_size))):`'
  id: totrans-1842
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 打印训练和评估指标。
- en: '`images = images / 255.0`'
  id: totrans-1843
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`images = images / 255.0`'
- en: '`images = jax_utils.replicate(images)`'
  id: totrans-1844
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`images = jax_utils.replicate(images)`'
- en: '`labels = jax_utils.replicate(labels)`'
  id: totrans-1845
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`labels = jax_utils.replicate(labels)`'
- en: '`grads, loss, accuracy = apply_model(state, images,`'
  id: totrans-1846
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`grads, loss, accuracy = apply_model(state, images,`'
- en: '`labels)state = update_model(state, grads)`'
  id: totrans-1847
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`labels)state = update_model(state, grads)`'
- en: '`epoch_loss.append(jax_utils.unreplicate(loss)) epoch_accuracy.append(jax_utils.unreplicate(accuracy))
    train_loss = np.mean(epoch_loss)`'
  id: totrans-1848
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`epoch_loss.append(jax_utils.unreplicate(loss)) epoch_accuracy.append(jax_utils.unreplicate(accuracy))
    train_loss = np.mean(epoch_loss)`'
- en: '`train_accuracy = np.mean(epoch_accuracy)`'
  id: totrans-1849
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_accuracy = np.mean(epoch_accuracy)`'
- en: '`_`, `test_loss`, `test_accuracy` = `jax_utils.unreplicate(app ly_model(state,
    test_images, test_labels))`'
  id: totrans-1850
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`_`, `test_loss`, `test_accuracy` = `jax_utils.unreplicate(app ly_model(state,
    test_images, test_labels))`'
- en: '`testing_accuracy.append(test_accuracy)`'
  id: totrans-1851
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_accuracy.append(test_accuracy)`'
- en: '`testing_loss.append(test_loss)`'
  id: totrans-1852
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_loss.append(test_loss)`'
- en: '`writer.add_scalar(''Loss/train'', np.array(train_loss), e poch)`'
  id: totrans-1853
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`writer.add_scalar(''Loss/train'', np.array(train_loss), e poch)`'
- en: '`writer.add_scalar(''Loss/test'', np.array(test_loss), epo ch)`'
  id: totrans-1854
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`writer.add_scalar(''Loss/test'', np.array(test_loss), epo ch)`'
- en: '`writer.add_scalar(''Accuracy/train'', np.array(train_accu racy), epoch)`'
  id: totrans-1855
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`writer.add_scalar(''Accuracy/train'', np.array(train_accu racy), epoch)`'
- en: '`writer.add_scalar(''Accuracy/test'', np.array(test_accura cy), epoch)`'
  id: totrans-1856
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`writer.add_scalar(''Accuracy/test'', np.array(test_accura cy), epoch)`'
- en: '`print(f"Epoch: {epoch + 1}, train loss: {train_loss:.4 f}, train accuracy:
    {train_accuracy * 100:.4f}, test loss: {tes t_loss:.4f}, test accuracy: {test_accuracy*
    100:.4f}", flush=Tr ue)`'
  id: totrans-1857
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`print(f"Epoch: {epoch + 1}, train loss: {train_loss:.4 f}, train accuracy:
    {train_accuracy * 100:.4f}, test loss: {tes t_loss:.4f}, test accuracy: {test_accuracy*
    100:.4f}", flush=Tr ue)`'
- en: return `state, epoch_loss, epoch_accuracy, testing_accuracy, testing_lossCreate
    a training state by generating random numbers equivalent to the number of devices.`
  id: totrans-1858
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 返回 `state, epoch_loss, epoch_accuracy, testing_accuracy, testing_loss` 通过生成与设备数量相同的随机数来创建训练状态。
- en: '`seed = 0`'
  id: totrans-1859
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`seed = 0`'
- en: '`rng = jax.random.PRNGKey(seed)`'
  id: totrans-1860
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`rng = jax.random.PRNGKey(seed)`'
- en: '`rng, init_rng = jax.random.split(rng)`'
  id: totrans-1861
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`rng, init_rng = jax.random.split(rng)`'
- en: '`state = create_train_state(jax.random.split(init_rng, jax.devic e_count()))`'
  id: totrans-1862
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`state = create_train_state(jax.random.split(init_rng, jax.devic e_count()))`'
- en: '`del init_rng # Must not be used anymore.`'
  id: totrans-1863
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`del init_rng # Must not be used anymore.`'
- en: Train the ResNet model by passing the training data and the number of epochs.
  id: totrans-1864
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 通过传递训练数据和 epochs 数量来训练 ResNet 模型。
- en: '`start = time.time()`'
  id: totrans-1865
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`start = time.time()`'
- en: '`num_epochs = 30`'
  id: totrans-1866
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`num_epochs = 30`'
- en: '`state, epoch_loss, epoch_accuracy, testing_accuracy, testing_lo ss = train_one_epoch(state,
    train_loader,num_epochs) print("Total time: ", time.time() - start, "seconds")`'
  id: totrans-1867
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`state, epoch_loss, epoch_accuracy, testing_accuracy, testing_lo ss = train_one_epoch(state,
    train_loader,num_epochs) print("Total time: ", time.time() - start, "seconds")`'
- en: '`![](../images/00049.jpeg)`'
  id: totrans-1868
  prefs: []
  stylish: true
  type: TYPE_IMG
  zh: '`![](../images/00049.jpeg)`'
- en: '**Evaluate model with TensorBoard**'
  id: totrans-1869
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**使用 TensorBoard 评估模型**'
- en: Run TensorBoard to see the logged scalars on TensorBoard.%load_ext tensorboard%tensorboard
    --logdir={logdir}![](../images/00050.jpeg)![](../images/00051.jpeg)
  id: totrans-1870
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 运行 TensorBoard 查看 TensorBoard 上记录的标量。%load_ext tensorboard%tensorboard --logdir={logdir}![](../images/00050.jpeg)![](../images/00051.jpeg)
- en: '**Visualize Flax model performance**'
  id: totrans-1871
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**可视化 Flax 模型性能**'
- en: The metrics that were stored in a list can be plotted using Matplotlib.
  id: totrans-1872
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 存储在列表中的指标可以使用 Matplotlib 绘制。
- en: '`plt.plot(epoch_accuracy, label="Training")`'
  id: totrans-1873
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.plot(epoch_accuracy, label="Training")`'
- en: '`plt.plot(testing_accuracy, label="Test")`'
  id: totrans-1874
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.plot(testing_accuracy, label="Test")`'
- en: '`plt.xlabel("Epoch")`'
  id: totrans-1875
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.xlabel("Epoch")`'
- en: '`plt.ylabel("Accuracy")`'
  id: totrans-1876
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.ylabel("Accuracy")`'
- en: '`plt.legend()`'
  id: totrans-1877
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.legend()`'
- en: '`plt.show()`'
  id: totrans-1878
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.show()`'
- en: '`plt.plot(epoch_loss, label="Training")`'
  id: totrans-1879
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.plot(epoch_loss, label="Training")`'
- en: '`plt.plot(testing_loss, label="Test")`'
  id: totrans-1880
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.plot(testing_loss, label="Test")`'
- en: '`plt.xlabel("Epoch")`'
  id: totrans-1881
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.xlabel("Epoch")`'
- en: '`plt.ylabel("Accuracy")`'
  id: totrans-1882
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.ylabel("Accuracy")`'
- en: '`plt.legend()`'
  id: totrans-1883
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.legend()`'
- en: '`plt.show()`'
  id: totrans-1884
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.show()`'
- en: '`![](../images/00052.jpeg)![](../images/00053.jpeg)`'
  id: totrans-1885
  prefs: []
  stylish: true
  type: TYPE_IMG
  zh: '`![](../images/00052.jpeg)![](../images/00053.jpeg)`'
- en: Save Flax ResNet model
  id: totrans-1886
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 保存 Flax ResNet 模型
- en: '`To save the trained Flax ResNet model use`'
  id: totrans-1887
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`保存训练好的 Flax ResNet 模型使用`'
- en: 'the save_checkpoint function. The function expects:'
  id: totrans-1888
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: save_checkpoint 函数。该函数期望：
- en: The folder where the ResNet model will be saved.
  id: totrans-1889
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 将保存 ResNet 模型的文件夹。
- en: The model to be saved– [target]. The step – training step number.
  id: totrans-1890
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 要保存的模型– [目标]。训练步骤– 训练步骤编号。
- en: The model prefix. Whether to overwrite existing models.
  id: totrans-1891
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 模型前缀。是否覆盖现有模型。
- en: '`!pip install tensorstore`'
  id: totrans-1892
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`!pip install tensorstore`'
- en: '`from flax.training import checkpoints`'
  id: totrans-1893
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from flax.training import checkpoints`'
- en: '`ckpt_dir = ''model_checkpoint/''`'
  id: totrans-1894
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`ckpt_dir = ''model_checkpoint/''`'
- en: '`checkpoints.save_checkpoint(ckpt_dir=ckpt_dir,`'
  id: totrans-1895
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`checkpoints.save_checkpoint(ckpt_dir=ckpt_dir,`'
- en: '`target=state, step=100,`'
  id: totrans-1896
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`target=state, step=100,`'
- en: '`prefix=''flax_model'', overwrite=True`'
  id: totrans-1897
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`prefix=''flax_model'', overwrite=True`'
- en: '`)![](../images/00054.jpeg)`'
  id: totrans-1898
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`)![](../images/00054.jpeg)`'
- en: Load Flax RestNet model
  id: totrans-1899
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 加载 Flax RestNet 模型
- en: 'The saved ResNet Flax model can also be loaded to make predictions. Flax models
    are loaded using the [restore_checkpoint] function. The function expects:'
  id: totrans-1900
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 保存的 ResNet Flax 模型也可以加载以进行预测。Flax 模型使用 [restore_checkpoint] 函数加载。该函数期望：
- en: The target state.
  id: totrans-1901
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 目标状态。
- en: The folder containing the saved model.
  id: totrans-1902
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 包含保存模型的文件夹。
- en: The model's prefix.
  id: totrans-1903
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 模型的前缀。
- en: '`loaded_model = checkpoints.restore_checkpoint(`'
  id: totrans-1904
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`loaded_model = checkpoints.restore_checkpoint(`'
- en: '`ckpt_dir=ckpt_dir, target=state, prefix=''flax_mode`'
  id: totrans-1905
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`ckpt_dir=ckpt_dir, target=state, prefix=''flax_mode`'
- en: '`l'') ![](../images/00055.gif)`'
  id: totrans-1906
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`l'') ![](../images/00055.gif)`'
- en: Final thoughts
  id: totrans-1907
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 最后的想法
- en: 'In this article, you have learned how to train a ResNet model from scratch
    in Flax. In particular, you have covered:'
  id: totrans-1908
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在本文中，您已经学习了如何在 Flax 中从头开始训练 ResNet 模型。具体来说，您已经涵盖了：
- en: Creating a ResNet model in Flax.
  id: totrans-1909
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在 Flax 中创建 ResNet 模型。
- en: Defining the training state for the ResNet Flax model.
  id: totrans-1910
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 为 ResNet Flax 模型定义训练状态。
- en: Training the Flax ResNet model in a distributed manner. Track the performance
    of the Flax ResNet model with TensorBoard.
  id: totrans-1911
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 在分布式方式下训练 Flax ResNet 模型。使用 TensorBoard 跟踪 Flax ResNet 模型的性能。
- en: Saving and loading the Flax ResNet model.
  id: totrans-1912
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 保存和加载 Flax ResNet 模型。
- en: Transfer learning with JAX & Flax
  id: totrans-1913
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 使用 JAX 和 Flax 进行迁移学习
- en: Training large neural networks can take days or weeks. Once these networks are
    trained, you can take advantage of their weights and apply them to new tasks– transfer
    learning. As a result, you finetune a new network and get good results in a short
    period. Let's look at how you can fine-tune a pre-trained ResNet network in JAX
    and Flax.
  id: totrans-1914
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 训练大型神经网络可能需要几天或几周。一旦这些网络被训练好，您可以利用它们的权重并将它们应用于新任务– 迁移学习。因此，您可以微调一个预训练的 ResNet
    网络，并在短时间内获得良好的结果。让我们看看如何在 JAX 和 Flax 中对预训练的 ResNet 网络进行微调。
- en: Install JAX ResNet
  id: totrans-1915
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 安装 JAX ResNet
- en: '`We''ll use ResNet checkpoints provided by the jax-resnet library.pip install
    jax-resnetLet''s import it together with other packages used in this article.`'
  id: totrans-1916
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`我们将使用 jax-resnet 库提供的 ResNet 检查点。pip install jax-resnet让我们一起导入它和本文中使用的其他软件包。`'
- en: '`pip install flax`'
  id: totrans-1917
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`pip install flax`'
- en: '`import numpy as np`'
  id: totrans-1918
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import numpy as np`'
- en: '`import pandas as pd`'
  id: totrans-1919
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import pandas as pd`'
- en: '`from PIL import Image`'
  id: totrans-1920
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from PIL import Image`'
- en: '`import jax`'
  id: totrans-1921
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import jax`'
- en: '`import optax`'
  id: totrans-1922
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import optax`'
- en: '`import flax`'
  id: totrans-1923
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import flax`'
- en: '`import jax.numpy as jnp`'
  id: totrans-1924
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import jax.numpy as jnp`'
- en: '`from jax_resnet import pretrained_resnet, slice_variables, Sequ ential`'
  id: totrans-1925
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from jax_resnet import pretrained_resnet, slice_variables, Sequ ential`'
- en: '`from flax.training import train_state from flax import linen as nn`'
  id: totrans-1926
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from flax.training import train_state from flax import linen as nn`'
- en: '`from flax.core import FrozenDict,frozen_dict from functools import partial`'
  id: totrans-1927
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from flax.core import FrozenDict,frozen_dict from functools import partial`'
- en: '`import os`'
  id: totrans-1928
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import os`'
- en: '`import torch`'
  id: totrans-1929
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import torch`'
- en: '`from torch.utils.data import DataLoader from torchvision import transforms`'
  id: totrans-1930
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from torch.utils.data import DataLoader from torchvision import transforms`'
- en: '`from torch.utils.data import Dataset import matplotlib.pyplot as plt`'
  id: totrans-1931
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from torch.utils.data import Dataset import matplotlib.pyplot as plt`'
- en: '`%matplotlib inline`'
  id: totrans-1932
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`%matplotlib inline`'
- en: ignore harmless warnings
  id: totrans-1933
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: 忽略无害的警告
- en: '`import warnings`'
  id: totrans-1934
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import warnings`'
- en: '`warnings.filterwarnings("ignore")`'
  id: totrans-1935
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`warnings.filterwarnings("ignore")`'
- en: Download dataset
  id: totrans-1936
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 下载数据集
- en: We will fine-tune the ResNet model to predict two classes from the cats and
    dogs dataset. Download and extract the cat and dog images.
  id: totrans-1937
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 我们将微调 ResNet 模型以预测来自猫狗数据集的两个类。下载并提取猫和狗的图像。
- en: '`pip install wget`'
  id: totrans-1938
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`pip install wget`'
- en: '`import wget`'
  id: totrans-1939
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import wget`'
- en: '`wget.download("https://ml.machinelearningnuggets.com/train.zi p")`'
  id: totrans-1940
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`wget.download("https://ml.machinelearningnuggets.com/train.zi p")`'
- en: '`import zipfile`'
  id: totrans-1941
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import zipfile`'
- en: '`with zipfile.ZipFile(''train.zip'', ''r'') as zip_ref:`'
  id: totrans-1942
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`with zipfile.ZipFile(''train.zip'', ''r'') as zip_ref:`'
- en: '`zip_ref.extractall(''.'')`'
  id: totrans-1943
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`zip_ref.extractall(''.'')`'
- en: Data loading in JAX
  id: totrans-1944
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 在 JAX 中加载数据
- en: JAX doesn't ship with data loading utilities. We use existing data loaders in TensorFlow and
    PyTorch to load the data. Let's use PyTorch to load the image data.
  id: totrans-1945
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAX 不包含数据加载工具。我们使用现有的 TensorFlow 和 PyTorch 数据加载器加载数据。让我们使用 PyTorch 加载图像数据。
- en: The first step is to create a PyTorch [Dataset] class.
  id: totrans-1946
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 第一步是创建一个 PyTorch [数据集] 类。
- en: class CatsDogsDataset(Dataset):`
  id: totrans-1947
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: class CatsDogsDataset(Dataset):`
- en: def __init__(self, root_dir, annotation_file, transform=Non
  id: totrans-1948
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: def __init__(self, root_dir, annotation_file, transform=Non
- en: 'e):'
  id: totrans-1949
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'e):'
- en: self.root_dir = root_dir
  id: totrans-1950
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: self.root_dir = root_dir
- en: self.annotations = pd.read_csv(annotation_file) self.transform = transform
  id: totrans-1951
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: self.annotations = pd.read_csv(annotation_file) self.transform = transform
- en: '`def __len__(self):return len(self.annotations)`'
  id: totrans-1952
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def __len__(self):return len(self.annotations)`'
- en: '`def __getitem__(self, index):`'
  id: totrans-1953
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def __getitem__(self, index):`'
- en: img_id = self.annotations.iloc[index, 0]
  id: totrans-1954
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: img_id = self.annotations.iloc[index, 0]
- en: img = Image.open(os.path.join(self.root_dir, img_id)).c
  id: totrans-1955
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: img = Image.open(os.path.join(self.root_dir, img_id)).c
- en: '`onvert("RGB")`'
  id: totrans-1956
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`onvert("RGB")`'
- en: '`y_label = torch.tensor(float(self.annotations.iloc[inde`'
  id: totrans-1957
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`y_label = torch.tensor(float(self.annotations.iloc[inde`'
- en: '`x, 1])`'
  id: totrans-1958
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x, 1])`'
- en: 'if self.transform is not None: img = self.transform(img) return (img, y_label)'
  id: totrans-1959
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 如果 self.transform 不为空：img = self.transform(img) return (img, y_label)
- en: '**Data processing**'
  id: totrans-1960
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**数据处理**'
- en: Next, create a Pandas DataFrame with the image paths and labels.
  id: totrans-1961
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 接下来，使用图像路径和标签创建一个 Pandas DataFrame。
- en: '`train_df = pd.DataFrame(columns=["img_path","label"]) train_df["img_path"]
    = os.listdir("train/") for idx, i in enumerate(os.listdir("train/")):`'
  id: totrans-1962
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_df = pd.DataFrame(columns=["img_path","label"]) train_df["img_path"]
    = os.listdir("train/") for idx, i in enumerate(os.listdir("train/")):`'
- en: if "cat" in i:`train_df["label"][idx] = 0if "dog" in i:train_df["label"][idx]
    = 1train_df.to_csv (r'train_csv.csv', index = False, header=True)![](../images/00056.jpeg)Define
    a function to stack the data and return the images and labels as a NumPy array.
  id: totrans-1963
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 如果 "cat" 在 i 中：`train_df["label"][idx] = 0if "dog" 在 i 中：train_df["label"][idx]
    = 1train_df.to_csv (r'train_csv.csv', index = False, header=True)![](../images/00056.jpeg)定义一个函数来堆叠数据并将图像和标签作为
    NumPy 数组返回。
- en: '`def custom_collate_fn(batch):`'
  id: totrans-1964
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def custom_collate_fn(batch):`'
- en: '`transposed_data = list(zip(*batch))`'
  id: totrans-1965
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`transposed_data = list(zip(*batch))`'
- en: '`labels = np.array(transposed_data[1])`'
  id: totrans-1966
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`labels = np.array(transposed_data[1])`'
- en: imgs = np.stack(transposed_data[0])
  id: totrans-1967
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: imgs = np.stack(transposed_data[0])
- en: return imgs, labels
  id: totrans-1968
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 返回图像和标签作为 NumPy 数组
- en: Let's also resize the images to ensure they are the same size. Define the size
    in a configuration dictionary. We'll use the other config variables later.
  id: totrans-1969
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 我们还需要调整图像大小以确保它们的尺寸一致。在配置字典中定义尺寸。稍后我们将使用其他配置变量。
- en: config = {
  id: totrans-1970
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: config = {
- en: '''NUM_LABELS'': 2,'
  id: totrans-1971
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '''标签数量'': 2,'
- en: '''BATCH_SIZE'': 32,'
  id: totrans-1972
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '''批处理大小'': 32,'
- en: '''N_EPOCHS'': 5,'
  id: totrans-1973
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '''训练周期'': 5,'
- en: '`''LR'': 0.001,`'
  id: totrans-1974
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`''学习速率'': 0.001,`'
- en: '`''IMAGE_SIZE'': 224,`'
  id: totrans-1975
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`''图像尺寸'': 224,`'
- en: '`''WEIGHT_DECAY'': 1e-5, ''FREEZE_BACKBONE'': True,`'
  id: totrans-1976
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`''权重衰减'': 1e-5, ''冻结骨干网络'': True,`'
- en: '}'
  id: totrans-1977
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '}'
- en: Resize the images using PyTorch transforms. Next, use the CatsDogsDataset class
    to define the training and testing data loaders.
  id: totrans-1978
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 使用 PyTorch 转换调整图像大小。接下来，使用 CatsDogsDataset 类定义训练和测试数据加载器。
- en: '`transform = transforms.Compose([`'
  id: totrans-1979
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`transform = transforms.Compose([`'
- en: transforms.Resize((config["IMAGE_SIZE"],config["IMAGE_SIZ
  id: totrans-1980
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: transforms.Resize((config["IMAGE_SIZE"],config["IMAGE_SIZ
- en: '`E"]))`,'
  id: totrans-1981
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`E"]))`,'
- en: np.array])
  id: totrans-1982
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`np.array`'
- en: dataset = CatsDogsDataset("train","train_csv.csv",transform=tra
  id: totrans-1983
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: dataset = CatsDogsDataset("train","train_csv.csv",transform=tra
- en: nsform)
  id: totrans-1984
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: nsform)
- en: '`train_set`, `validation_set = torch.utils.data.random_split(datas`'
  id: totrans-1985
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_set`, `validation_set = torch.utils.data.random_split(datas`'
- en: '`et,[20000,5000])`'
  id: totrans-1986
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`et,[20000,5000])`'
- en: '`train_loader = DataLoader(dataset=train_set, collate_fn=custom_`'
  id: totrans-1987
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_loader = DataLoader(dataset=train_set, collate_fn=custom_`'
- en: collate_fn,shuffle=True, batch_size=config["BATCH_SIZE"])
  id: totrans-1988
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: collate_fn,shuffle=True, batch_size=config["BATCH_SIZE"])
- en: '`validation_loader = DataLoader(dataset=validation_set,collate_f`'
  id: totrans-1989
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`validation_loader = DataLoader(dataset=validation_set,collate_f`'
- en: n=custom_collate_fn, shuffle=False, batch_size=config["BATCH_SI
  id: totrans-1990
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: n=custom_collate_fn, shuffle=False, batch_size=config["BATCH_SI
- en: '`ZE"])`'
  id: totrans-1991
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`ZE"])`'
- en: '**ResNet model definition**'
  id: totrans-1992
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**ResNet模型定义**'
- en: Pre-trained ResNet models are trained on many classes. However, the dataset
    we have has two classes. We, therefore, use the ResNet as the backbone and define
    a custom classification layer.
  id: totrans-1993
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 预训练的ResNet模型在许多类上进行了训练。但是，我们的数据集只有两类。因此，我们使用ResNet作为主干，并定义一个自定义分类层。
- en: '**Create head network**'
  id: totrans-1994
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**创建头部网络**'
- en: Create a head network with output as per the problem, in this case, a binary
    image classification.
  id: totrans-1995
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 创建一个头部网络，输出与问题相符，本例中为二进制图像分类。
- en: '"""'
  id: totrans-1996
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""'
- en: reference - https://www.kaggle.com/code/alexlwh/happywhale-flax
  id: totrans-1997
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 参考 - https://www.kaggle.com/code/alexlwh/happywhale-flax
- en: '`-jax-tpu-gpu-resnet-baseline`'
  id: totrans-1998
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`-jax-tpu-gpu-resnet-baseline`'
- en: '"""'
  id: totrans-1999
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""'
- en: '`class Head(nn.Module):`'
  id: totrans-2000
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`class Head(nn.Module):`'
- en: '''''''head model''''''`batch_norm_cls: partial = partial(nn.BatchNorm, momentum=0.`'
  id: totrans-2001
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '''''''head model''''''`batch_norm_cls: partial = partial(nn.BatchNorm, momentum=0.`'
- en: 9)
  id: totrans-2002
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 9)
- en: '@nn.compact'
  id: totrans-2003
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '@nn.compact'
- en: '`def __call__(self, inputs, train: bool):`'
  id: totrans-2004
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def __call__(self, inputs, train: bool):`'
- en: output_n = inputs.shape[-1]x = self.batch_norm_cls(use_running_average=not train)
  id: totrans-2005
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: output_n = inputs.shape[-1]x = self.batch_norm_cls(use_running_average=not train)
- en: '`(inputs)'
  id: totrans-2006
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`(inputs)'
- en: '`x = nn.Dropout(rate=0.25)(x, deterministic=not train) x = nn.Dense(features=output_n)(x)'
  id: totrans-2007
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = nn.Dropout(rate=0.25)(x, deterministic=not train) x = nn.Dense(features=output_n)(x)'
- en: '`x = nn.relu(x)'
  id: totrans-2008
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = nn.relu(x)'
- en: '`x = self.batch_norm_cls(use_running_average=not train)'
  id: totrans-2009
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = self.batch_norm_cls(use_running_average=not train)'
- en: '`(x)'
  id: totrans-2010
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`(x)'
- en: '`x = nn.Dropout(rate=0.5)(x, deterministic=not train) x = nn.Dense(features=config["NUM_LABELS"])(x)
    return x'
  id: totrans-2011
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = nn.Dropout(rate=0.5)(x, deterministic=not train) x = nn.Dense(features=config["NUM_LABELS"])(x)
    return x'
- en: '**Combine ResNet backbone with head**'
  id: totrans-2012
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**将ResNet主干与头部模型结合**'
- en: Combine the pre-trained ResNet backbone with the custom head you created above.
  id: totrans-2013
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 将预训练的ResNet主干与您上面创建的自定义头部结合。
- en: '`class Model(nn.Module):'
  id: totrans-2014
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`class Model(nn.Module):'
- en: '```Combines backbone and head model``` backbone: Sequential'
  id: totrans-2015
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '```结合主干和头部模型``` backbone: Sequential'
- en: 'head: Head'
  id: totrans-2016
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'head: Head'
- en: 'def __call__(self, inputs, train: bool): x = self.backbone(inputs)'
  id: totrans-2017
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def __call__(self, inputs, train: bool): x = self.backbone(inputs)'
- en: average pool layer
  id: totrans-2018
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: 平均池化层
- en: x = jnp.mean(x, axis=(1, 2)) x = self.head(x, train)
  id: totrans-2019
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = jnp.mean(x, axis=(1, 2)) x = self.head(x, train)
- en: return x
  id: totrans-2020
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 返回 x
- en: '**Load pre-trained ResNet 50**'
  id: totrans-2021
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**加载预训练的ResNet 50**'
- en: Next, create a function that loads the pre-trained ResNet model. Omit the last
    two layers of the network because we have defined a custom head. The function
    returns the ResNet model and its parameters. The model parameters are obtained
    using the [slice_variables] function.
  id: totrans-2022
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 接下来，创建一个函数，加载预训练的ResNet模型。省略网络的最后两层，因为我们已定义了一个自定义头部。该函数返回ResNet模型及其参数。使用 [slice_variables] 函数获取模型参数。
- en: '```def get_backbone_and_params(model_arch: str):```'
  id: totrans-2023
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '```def get_backbone_and_params(model_arch: str):```'
- en: '```'
  id: totrans-2024
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '```'
- en: Get backbone and params
  id: totrans-2025
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 获取主干和参数
- en: 1\. Loads pretrained model (resnet50)
  id: totrans-2026
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 1\. 加载预训练模型(resnet50)
- en: '2\. Get model and param structure except last 2 layers 3\. Extract the corresponding
    subset of the variables dict INPUT : model_arch'
  id: totrans-2027
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 2\. 获取模型和参数结构，除了最后2层 3\. 提取变量字典的相应子集 输入：model_arch
- en: RETURNS backbone , backbone_params
  id: totrans-2028
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: RETURNS backbone , backbone_params
- en: '```'
  id: totrans-2029
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '```'
- en: '`if model_arch == ''resnet50'':'
  id: totrans-2030
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`if model_arch == ''resnet50'':'
- en: resnet_tmpl, params = pretrained_resnet(50) model = resnet_tmpl()else:raise
    NotImplementedError
  id: totrans-2031
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: resnet_tmpl, params = pretrained_resnet(50) model = resnet_tmpl()else:raise
    NotImplementedError
- en: get model & param structure for backbone start, end = 0, len(model.layers) -
    2 backbone = Sequential(model.layers[start:end]) backbone_params = slice_variables(params,
    start, end) return backbone, backbone_params
  id: totrans-2032
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: 获取主干模型及其参数结构的起始点和结束点 = 0, len(model.layers) - 2 backbone = Sequential(model.layers[start:end])
    backbone_params = slice_variables(params, start, end) return backbone, backbone_params
- en: Get model and variables
  id: totrans-2033
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 获取模型和变量
- en: 'Use the above function to create the final model. Define a function that:'
  id: totrans-2034
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 使用上述函数创建最终模型。定义一个函数：
- en: Initializes the network's input.
  id: totrans-2035
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 初始化网络的输入。
- en: Obtains the ResNet backbone and its parameters. Passes the input to the backbone
    and gets the output.
  id: totrans-2036
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 获取ResNet主干及其参数。将输入传递给主干并获取输出。
- en: Initializes the network's head. Creates the final model using backbone and head.
  id: totrans-2037
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 初始化网络的头部。使用主干和头部创建最终模型。
- en: Combines the parameters from backbone and head.
  id: totrans-2038
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 结合来自主干和头部的参数。
- en: '```def get_model_and_variables(model_arch: str, head_init_key: in t):```'
  id: totrans-2039
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '```def get_model_and_variables(model_arch: str, head_init_key: in t):```'
- en: '```'
  id: totrans-2040
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '```'
- en: Get model and variables
  id: totrans-2041
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: Get model and variables
- en: 1\. Initialise inputs(shape=(1,image_size,image_size,3))
  id: totrans-2042
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 1\. 初始化输入(shape=(1,image_size,image_size,3))
- en: 2\. Get backbone and params
  id: totrans-2043
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 2\. Get backbone and params
- en: 3\. Apply backbone model and get outputs
  id: totrans-2044
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 3\. Apply backbone model and get outputs
- en: 4\. Initialise head
  id: totrans-2045
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 4\. Initialise head
- en: 5\. Create final model using backbone and head
  id: totrans-2046
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 5\. Create final model using backbone and head
- en: 6\. Combine params from backbone and head
  id: totrans-2047
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 6\. Combine params from backbone and head
- en: INPUT model_arch, head_init_key RETURNS model, variables '''
  id: totrans-2048
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: INPUT model_arch, head_init_key RETURNS model, variables '''
- en: '#backboneinputs = jnp.ones((1, config[''IMAGE_SIZE''],config[''IMAGE_SI ZE''],
    3), jnp.float32)backbone, backbone_params = get_backbone_and_params(model_a rch)
    key = jax.random.PRNGKey(head_init_key)backbone_output = backbone.apply(backbone_params,
    inputs, m utable=False)#headhead_inputs = jnp.ones((1, backbone_output.shape[-1]),
    jnp. float32)head = Head()head_params = head.init(key, head_inputs, train=False)'
  id: totrans-2049
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '#backboneinputs = jnp.ones((1, config[''IMAGE_SIZE''],config[''IMAGE_SI ZE''],
    3), jnp.float32)backbone, backbone_params = get_backbone_and_params(model_a rch)
    key = jax.random.PRNGKey(head_init_key)backbone_output = backbone.apply(backbone_params,
    inputs, m utable=False)#headhead_inputs = jnp.ones((1, backbone_output.shape[-1]),
    jnp. float32)head = Head()head_params = head.init(key, head_inputs, train=False)'
- en: '#final model'
  id: totrans-2050
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '#final model'
- en: model = Model(backbone, head)
  id: totrans-2051
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: model = Model(backbone, head)
- en: variables = FrozenDict({
  id: totrans-2052
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: variables = FrozenDict({
- en: '`''params'': {'
  id: totrans-2053
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`''params'': {'
- en: '`''backbone'': backbone_params[''params''], ''head'': head_params[''params'']'
  id: totrans-2054
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`''backbone'': backbone_params[''params''], ''head'': head_params[''params'']'
- en: '},'
  id: totrans-2055
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '},'
- en: '`''batch_stats'': {'
  id: totrans-2056
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`''batch_stats'': {'
- en: '''backbone'': backbone_params[''batch_stats''], ''head'': head_params[''batch_stats'']'
  id: totrans-2057
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`''backbone'': backbone_params[''batch_stats''], ''head'': head_params[''batch_stats'']'
- en: '}'
  id: totrans-2058
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '}'
- en: '})'
  id: totrans-2059
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '})'
- en: return model, variables
  id: totrans-2060
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: return model, variables
- en: All names relating to the backbone network are prefixed with the name [backbone].
    You can use any name, but all backbone variable names should be the same. This
    is important when freezing layers, as we'll see later.
  id: totrans-2061
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 所有与骨干网络相关的名称都以“backbone”作为前缀。您可以使用任何名称，但所有骨干变量名称应该相同。在冻结层时，这一点非常重要，正如我们将在后面看到的那样。
- en: Next, use the function defined above to create the model.model, variables =
    get_model_and_variables('resnet50', 0) ![](../images/00057.jpeg)
  id: totrans-2062
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 接下来，使用上述定义的函数创建模型。model, variables = get_model_and_variables('resnet50', 0)
    ![](../images/00057.jpeg)
- en: Zero gradients
  id: totrans-2063
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: Zero gradients
- en: Since we are applying transfer learning, we need to ensure that the backbone
    is not updated. Otherwise, we'll be training the network from scratch. We want
    to take advantage of the pre-trained weights and use them as a feature extractor for
    the network. To achieve this, we freeze the parameters of all layers whose name
    starts
  id: totrans-2064
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 由于我们正在应用迁移学习，需要确保不更新骨干网络。否则，我们将从头开始训练网络。我们希望利用预训练的权重，并将它们用作网络的特征提取器。为此，我们冻结所有以“backbone”和“head”开头的层的参数。
- en: with [backbone]. As a result, these parameters will not be updated during training.
  id: totrans-2065
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: with [backbone]. As a result, these parameters will not be updated during training.
- en: '"""'
  id: totrans-2066
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""'
- en: reference - https://github.com/deepmind/optax/issues/159#issuec omment-896459491
  id: totrans-2067
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 参考 - https://github.com/deepmind/optax/issues/159#issuec omment-896459491
- en: '"""'
  id: totrans-2068
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""'
- en: 'def zero_grads():'
  id: totrans-2069
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def zero_grads():'
- en: ''''''''
  id: totrans-2070
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ''''''''
- en: Zero out the previous gradient computation
  id: totrans-2071
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: Zero out the previous gradient computation
- en: '```'
  id: totrans-2072
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '```'
- en: 'def init_fn(_):'
  id: totrans-2073
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def init_fn(_):'
- en: return ()
  id: totrans-2074
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 返回()
- en: 'def update_fn(updates, state, params=None):'
  id: totrans-2075
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def update_fn(updates, state, params=None):'
- en: return jax.tree_map(jnp.zeros_like, updates), () return optax.GradientTransformation(init_fn,
    update_fn)
  id: totrans-2076
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: return jax.tree_map(jnp.zeros_like, updates), () return optax.GradientTransformation(init_fn,
    update_fn)
- en: '"""'
  id: totrans-2077
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""'
- en: reference - https://colab.research.google.com/drive/1g_pt2Rc3bv 6H6qchvGHD-BpgF-Pt4vrC#scrollTo=TqDvTL_tIQCH&line=2&uniqifier=1
    """
  id: totrans-2078
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 参考 - https://colab.research.google.com/drive/1g_pt2Rc3bv 6H6qchvGHD-BpgF-Pt4vrC#scrollTo=TqDvTL_tIQCH&line=2&uniqifier=1
    """
- en: 'def create_mask(params, label_fn):'
  id: totrans-2079
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def create_mask(params, label_fn):'
- en: 'def _map(params, mask, label_fn):for k in params:if label_fn(k):'
  id: totrans-2080
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def _map(params, mask, label_fn):for k in params:if label_fn(k):'
- en: mask[k] = 'zero'
  id: totrans-2081
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: mask[k] = 'zero'
- en: 'else:'
  id: totrans-2082
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'else:'
- en: 'if isinstance(params[k], FrozenDict): mask[k] = {}'
  id: totrans-2083
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'if isinstance(params[k], FrozenDict): mask[k] = {}'
- en: _map(params[k], mask[k], label_fn)
  id: totrans-2084
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: _map(params[k], mask[k], label_fn)
- en: 'else:'
  id: totrans-2085
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'else:'
- en: mask[k] = 'adam'
  id: totrans-2086
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: mask[k] = 'adam'
- en: mask = {}
  id: totrans-2087
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: mask = {}
- en: _map(params, mask, label_fn)
  id: totrans-2088
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: _map(params, mask, label_fn)
- en: return frozen_dict.freeze(mask)
  id: totrans-2089
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: return frozen_dict.freeze(mask)
- en: Define Flax optimizer
  id: totrans-2090
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 定义 Flax 优化器
- en: Create an optimizer that will only be applied to the head and not backbone layers.
    This is done using the optax.multi_transform while passing the desired transformations.
  id: totrans-2091
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 创建一个仅应用于头部而不是骨干层的优化器。这是通过 optax.multi_transform 实现的，同时传递所需的变换。
- en: adamw = optax.adamw(
  id: totrans-2092
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: adamw = optax.adamw(
- en: learning_rate=config['LR'],
  id: totrans-2093
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: learning_rate=config['LR'],
- en: b1=0.9, b2=0.999,
  id: totrans-2094
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: b1=0.9, b2=0.999,
- en: eps=1e-6, weight_decay=1e-2
  id: totrans-2095
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: eps=1e-6, weight_decay=1e-2
- en: )
  id: totrans-2096
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: )
- en: optimizer = optax.multi_transform(
  id: totrans-2097
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: optimizer = optax.multi_transform(
- en: '{''adam'': adamw, ''zero'': zero_grads()},'
  id: totrans-2098
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '{''adam'': adamw, ''zero'': zero_grads()},'
- en: 'create_mask(variables[''params''], lambda s: s.startswith(''ba ckbone''))'
  id: totrans-2099
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'create_mask(variables[''params''], lambda s: s.startswith(''ba ckbone''))'
- en: )
  id: totrans-2100
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: )
- en: Define Flax loss function
  id: totrans-2101
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 定义 Flax 损失函数
- en: Next, define the function to compute the loss function.
  id: totrans-2102
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 下一步，定义计算损失函数的函数。
- en: 'def cross_entropy_loss(*, logits, labels):'
  id: totrans-2103
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def cross_entropy_loss(*, logits, labels):'
- en: labels_onehot = jax.nn.one_hot(labels, num_classes=config["NU
  id: totrans-2104
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: labels_onehot = jax.nn.one_hot(labels, num_classes=config["NU
- en: M_LABELS"])
  id: totrans-2105
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: M_LABELS"])
- en: return optax.softmax_cross_entropy(logits=logits, labels=labe
  id: totrans-2106
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: return optax.softmax_cross_entropy(logits=logits, labels=labe
- en: ls_onehot).mean()
  id: totrans-2107
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ls_onehot).mean()
- en: 'When computing the loss during training, set [train] to [True]. You also have
    to:'
  id: totrans-2108
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`在训练过程中计算损失时，将[train]设置为[True]。您还需要：`'
- en: Set the batch_statsDefine the random number for the [dropout] layers. Set the [batch_stats] as
    mutable.
  id: totrans-2109
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 设置[batch_stats]定义[dropout]层的随机数。将[batch_stats]设置为可变。
- en: 'def compute_loss(params, batch_stats, images, labels): logits,batch_stats =
    model.apply({''params'': params,''batch_s'
  id: totrans-2110
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def compute_loss(params, batch_stats, images, labels): logits,batch_stats =
    model.apply({''params'': params,''batch_s'
- en: 'tats'': batch_stats},images, train=True,rngs={''dropout'': jax.ran'
  id: totrans-2111
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'tats'': batch_stats},images, train=True,rngs={''dropout'': jax.ran'
- en: dom.PRNGKey(0)}, mutable=['batch_stats'])
  id: totrans-2112
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: dom.PRNGKey(0)}, mutable=['batch_stats'])
- en: loss = cross_entropy_loss(logits=logits, labels=labels) return loss, (logits,
    batch_stats)
  id: totrans-2113
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: loss = cross_entropy_loss(logits=logits, labels=labels) return loss, (logits,
    batch_stats)
- en: Define Flax metrics
  id: totrans-2114
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 定义Flax指标
- en: '`Using the loss function, define a function that will return the loss and accuracy
    during training.`'
  id: totrans-2115
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`使用损失函数，定义一个函数，在训练期间返回损失和准确度。`'
- en: '`def compute_metrics(*, logits, labels):`'
  id: totrans-2116
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def compute_metrics(*, logits, labels):`'
- en: '`loss = cross_entropy_loss(logits=logits, labels=labels) accuracy = jnp.mean(jnp.argmax(logits,
    -1) == labels) metrics = {`'
  id: totrans-2117
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`loss = cross_entropy_loss(logits=logits, labels=labels) accuracy = jnp.mean(jnp.argmax(logits,
    -1) == labels) metrics = {`'
- en: '`''loss'': loss,`'
  id: totrans-2118
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`''loss'': loss,`'
- en: '`''accuracy'': accuracy,`'
  id: totrans-2119
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`''accuracy'': accuracy,`'
- en: '`}`'
  id: totrans-2120
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`}`'
- en: '`return metrics`'
  id: totrans-2121
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return metrics`'
- en: '**Create Flax training state**'
  id: totrans-2122
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**创建Flax训练状态**'
- en: '`Flax provides a training state for storing training information. In this case,
    we add the [batch_stats] information.class TrainState(train_state.TrainState):
    batch_stats: FrozenDict`'
  id: totrans-2123
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Flax提供一个训练状态来存储训练信息。在这种情况下，我们添加[batch_stats]信息。`'
- en: '`state = TrainState.create(`'
  id: totrans-2124
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`state = TrainState.create(`'
- en: '`apply_fn = model.apply,`'
  id: totrans-2125
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`apply_fn = model.apply,`'
- en: '`params = variables[''params''],`'
  id: totrans-2126
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`params = variables[''params''],`'
- en: '`tx = optimizer,`'
  id: totrans-2127
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`tx = optimizer,`'
- en: '`batch_stats = variables[''batch_stats''],`'
  id: totrans-2128
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`batch_stats = variables[''batch_stats''],`'
- en: '`)`'
  id: totrans-2129
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`)`'
- en: '**Training step**'
  id: totrans-2130
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**训练步骤**'
- en: '`The training step receives the images and labels and computes the gradient
    with respect to the model parameters. It then returns the new state and the model
    metrics.`'
  id: totrans-2131
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`训练步骤接收图像和标签，并计算相对于模型参数的梯度。然后返回新的状态和模型指标。`'
- en: '`@jax.jit`'
  id: totrans-2132
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`@jax.jit`'
- en: '`def train_step(state: TrainState,images, labels):`'
  id: totrans-2133
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def train_step(state: TrainState,images, labels):`'
- en: '"""Train for a single step."""'
  id: totrans-2134
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""训练单步。"""'
- en: '`(batch_loss, (logits, batch_stats)), grads= jax.value_and_gra d(compute_loss,
    has_aux=True)(state.params,state.batch_stats, i mages,labels)`'
  id: totrans-2135
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`(batch_loss, (logits, batch_stats)), grads= jax.value_and_gra d(compute_loss,
    has_aux=True)(state.params,state.batch_stats, i mages,labels)`'
- en: '`state = state.apply_gradients(grads=grads)`'
  id: totrans-2136
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`state = state.apply_gradients(grads=grads)`'
- en: '`metrics = compute_metrics(logits=logits, labels=labels)`'
  id: totrans-2137
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`metrics = compute_metrics(logits=logits, labels=labels)`'
- en: '`return state, metrics`'
  id: totrans-2138
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return state, metrics`'
- en: '`To train the network for one epoch, loop through the training data while applying
    the training step.`'
  id: totrans-2139
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`要训练网络一个epoch，循环遍历训练数据并应用训练步骤。`'
- en: '`def train_one_epoch(state, dataloader):`'
  id: totrans-2140
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def train_one_epoch(state, dataloader):`'
- en: '"""Train for 1 epoch on the training set.""" batch_metrics = []'
  id: totrans-2141
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""在训练集上训练1个epoch。""" batch_metrics = []'
- en: '`for cnt, (images, labels) in enumerate(dataloader):`'
  id: totrans-2142
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`for cnt, (images, labels) in enumerate(dataloader):`'
- en: '`images = images / 255.0`'
  id: totrans-2143
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`images = images / 255.0`'
- en: '`state, metrics = train_step(state, images, labels) batch_metrics.append(metrics)`'
  id: totrans-2144
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`state, metrics = train_step(state, images, labels) batch_metrics.append(metrics)`'
- en: '`batch_metrics_np = jax.device_get(batch_metrics) epoch_metrics_np = {k: np.mean([metrics[k]
    for metrics in batch_metrics_n`'
  id: totrans-2145
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`batch_metrics_np = jax.device_get(batch_metrics) epoch_metrics_np = {k: np.mean([metrics[k]
    for metrics in batch_metrics_n`'
- en: '`p])`'
  id: totrans-2146
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`p])`'
- en: '`for k in batch_metrics_np[0] }`'
  id: totrans-2147
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`for k in batch_metrics_np[0] }`'
- en: '`return state, epoch_metrics_np`'
  id: totrans-2148
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return state, epoch_metrics_np`'
- en: '**Evaluation step**'
  id: totrans-2149
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**评估步骤**'
- en: '`The model evaluation steps accept the test labels and images and applies them
    to the network. It then returns the model evaluation metrics. During evaluation,
    set the [train] parameter to [False]. You''ll also define the [batch_stats] and
    the random number for the [dropout] layer.`'
  id: totrans-2150
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`模型评估步骤接受测试标签和图像，并将其应用于网络。然后返回模型评估指标。在评估过程中，将[train]参数设为[False]。还需定义[batch_stats]和[dropout]层的随机数。`'
- en: '`@jax.jitdef eval_step(batch_stats, params, images, labels): logits = model.apply({''params'':
    params,''batch_stats'': batch _stats}, images, train=False,rngs={''dropout'':
    jax.random.PRNGKe y(0)})return compute_metrics(logits=logits, labels=labels)`'
  id: totrans-2151
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`@jax.jitdef eval_step(batch_stats, params, images, labels): logits = model.apply({''params'':
    params,''batch_stats'': batch _stats}, images, train=False,rngs={''dropout'':
    jax.random.PRNGKe y(0)})return compute_metrics(logits=logits, labels=labels)`'
- en: '`def evaluate_model(state, test_imgs, test_lbls):`'
  id: totrans-2152
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def evaluate_model(state, test_imgs, test_lbls):`'
- en: '"""Evaluate on the validation set."""'
  id: totrans-2153
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""在验证集上进行评估。"""'
- en: '`metrics = eval_step(state.batch_stats,state.params, test_im`'
  id: totrans-2154
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`metrics = eval_step(state.batch_stats,state.params, test_im`'
- en: '`gs, test_lbls)`'
  id: totrans-2155
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`gs, test_lbls)`'
- en: '`metrics = jax.device_get(metrics)`'
  id: totrans-2156
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`metrics = jax.device_get(metrics)`'
- en: '`metrics = jax.tree_map(lambda x: x.item(), metrics) return metrics`'
  id: totrans-2157
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`metrics = jax.tree_map(lambda x: x.item(), metrics) return metrics`'
- en: '**Train ResNet model in Flax**'
  id: totrans-2158
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**在Flax中训练ResNet模型**'
- en: '`Train the ResNet model by applying the [train_one_epoch] function for the
    desired number of epochs. This is a few epochs since we are finetuning the network.`'
  id: totrans-2159
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 通过应用`[train_one_epoch]`函数来训练ResNet模型。由于我们在微调网络，所以只需要几个epochs。
- en: '**Set up TensorBoard in Flax**'
  id: totrans-2160
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**在Flax中设置TensorBoard**'
- en: '`To monitor model training via TensorBoard, you can write the training and
    validation metrics to TensorBoard.`'
  id: totrans-2161
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`要通过TensorBoard监控模型训练，可以将训练和验证指标写入TensorBoard。`'
- en: '`from torch.utils.tensorboard import SummaryWriter import torchvision.transforms.functional
    as F logdir = "flax_logs"`'
  id: totrans-2162
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from torch.utils.tensorboard import SummaryWriter import torchvision.transforms.functional
    as F logdir = "flax_logs"`'
- en: '`writer = SummaryWriter(logdir)`'
  id: totrans-2163
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`writer = SummaryWriter(logdir)`'
- en: Train model
  id: totrans-2164
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 训练模型
- en: '`Define a function to train and evaluate the model while writing the metrics
    to TensorBoard.(test_images, test_labels) = next(iter(validation_loader)) test_images
    = test_images / 255.0`'
  id: totrans-2165
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`定义一个函数来训练和评估模型，并将指标写入TensorBoard。(test_images, test_labels) = next(iter(validation_loader))
    test_images = test_images / 255.0`'
- en: '`training_loss = [] training_accuracy = [] testing_loss = []`'
  id: totrans-2166
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`training_loss = [] training_accuracy = [] testing_loss = []`'
- en: '`testing_accuracy = []`'
  id: totrans-2167
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_accuracy = []`'
- en: '`def train_model(epochs):for epoch in range(1, epochs + 1):train_state, train_metrics
    = train_one_epoch(state, tra`'
  id: totrans-2168
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def train_model(epochs):for epoch in range(1, epochs + 1):train_state, train_metrics
    = train_one_epoch(state, tra`'
- en: '`in_loader)`'
  id: totrans-2169
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`in_loader)`'
- en: '`training_loss.append(train_metrics[''loss'']) training_accuracy.append(train_metrics[''accuracy''])`'
  id: totrans-2170
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`training_loss.append(train_metrics[''loss'']) training_accuracy.append(train_metrics[''accuracy''])`'
- en: '`test_metrics = evaluate_model(train_state, test_images, test_labels)`'
  id: totrans-2171
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`test_metrics = evaluate_model(train_state, test_images, test_labels)`'
- en: '`testing_loss.append(test_metrics[''loss''])`'
  id: totrans-2172
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_loss.append(test_metrics[''loss''])`'
- en: '`testing_accuracy.append(test_metrics[''accuracy''])`'
  id: totrans-2173
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_accuracy.append(test_metrics[''accuracy''])`'
- en: '`writer.add_scalar(''Loss/train'', train_metrics[''loss''], epoch)`'
  id: totrans-2174
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`writer.add_scalar(''Loss/train'', train_metrics[''loss''], epoch)`'
- en: '`writer.add_scalar(''Loss/test'', test_metrics[''loss''], epoch)`'
  id: totrans-2175
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`writer.add_scalar(''Loss/test'', test_metrics[''loss''], epoch)`'
- en: '`writer.add_scalar(''Accuracy/train'', train_metrics[''accu racy''], epoch)`'
  id: totrans-2176
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`writer.add_scalar(''Accuracy/train'', train_metrics[''accu racy''], epoch)`'
- en: '`writer.add_scalar(''Accuracy/test'', test_metrics[''accura cy''], epoch)`'
  id: totrans-2177
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`writer.add_scalar(''Accuracy/test'', test_metrics[''accura cy''], epoch)`'
- en: '`print(f"Epoch: {epoch}, training loss: {train_metrics [''loss'']}, training
    accuracy: {train_metrics[''accuracy''] * 10 0}, validation loss: {test_metrics[''loss'']},
    validation accurac y: {test_metrics[''accuracy''] * 100}")`'
  id: totrans-2178
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`print(f"Epoch: {epoch}, training loss: {train_metrics [''loss'']}, training
    accuracy: {train_metrics[''accuracy''] * 10 0}, validation loss: {test_metrics[''loss'']},
    validation accurac y: {test_metrics[''accuracy''] * 100}")`'
- en: '`return train_stateRun the training function. trained_model_state = train_model(config["N_EPOCHS"])`'
  id: totrans-2179
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return train_stateRun the training function. trained_model_state = train_model(config["N_EPOCHS"])`'
- en: Save Flax model
  id: totrans-2180
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 保存Flax模型
- en: Use the `[save_checkpoint]` to save a trained Flax model.
  id: totrans-2181
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 使用`[save_checkpoint]`保存训练后的Flax模型。
- en: '`from flax.training import checkpoints`'
  id: totrans-2182
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from flax.training import checkpoints`'
- en: '`ckpt_dir = ''model_checkpoint/''`'
  id: totrans-2183
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`ckpt_dir = ''model_checkpoint/''`'
- en: '`checkpoints.save_checkpoint(ckpt_dir=ckpt_dir,`'
  id: totrans-2184
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`checkpoints.save_checkpoint(ckpt_dir=ckpt_dir,`'
- en: '`target=trained_model_state, step=100,`'
  id: totrans-2185
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`target=trained_model_state, step=100,`'
- en: '`prefix=''resnet_model'', overwrite=True`'
  id: totrans-2186
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`prefix=''resnet_model'', overwrite=True`'
- en: '`)`'
  id: totrans-2187
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`)`'
- en: Load saved Flax model
  id: totrans-2188
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 加载保存的Flax模型
- en: A saved Flax model is loaded using the `[restore_checkpoint]` method.
  id: totrans-2189
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 使用`[restore_checkpoint]`方法加载保存的Flax模型。
- en: '`loaded_model = checkpoints.restore_checkpoint('
  id: totrans-2190
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`loaded_model = checkpoints.restore_checkpoint('
- en: '`ckpt_dir=ckpt_dir, target=state, prefix=''resnet_mod`'
  id: totrans-2191
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`ckpt_dir=ckpt_dir, target=state, prefix=''resnet_mod`'
- en: '`el'') loaded_model`'
  id: totrans-2192
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`el'') loaded_model`'
- en: Evaluate Flax ResNet model
  id: totrans-2193
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 评估Flax ResNet模型
- en: '`To evaluate a Flax model, pass the test and training data to`'
  id: totrans-2194
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`要评估Flax模型，请将测试和训练数据传递给`'
- en: '`the evalaute_model function.evaluate_model(loaded_model,test_images, test_labels)
    ![](../images/00058.gif)`'
  id: totrans-2195
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`the evalaute_model function.evaluate_model(loaded_model,test_images, test_labels)
    ![](../images/00058.gif)`'
- en: Visualize model performance
  id: totrans-2196
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: 可视化模型性能
- en: You can check the network's performance via TensorBoard or plot the metrics
    using Matplotlib.
  id: totrans-2197
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 你可以通过TensorBoard检查网络的性能，或者使用Matplotlib绘制指标。
- en: Final thoughts
  id: totrans-2198
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: Final thoughts
- en: 'You can apply transfer learning to take advantage of pre-trained models and
    get results with minimal effort. You have learned how to train a ResNet model
    in Flax. Specially, you have covered:'
  id: totrans-2199
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 可以应用迁移学习来利用预训练模型，并且只需付出最小的努力就能获得结果。你已经学会了如何在Flax中训练ResNet模型。具体来说，你已经覆盖了：
- en: '`How to define the ResNet model in Flax.`'
  id: totrans-2200
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`How to define the ResNet model in Flax.`'
- en: How to freeze the layers of the ResNet network.
  id: totrans-2201
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 如何冻结ResNet网络的层。
- en: '`Training a ResNet model on custom data in Flax.`'
  id: totrans-2202
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Training a ResNet model on custom data in Flax.`'
- en: '`Saving and loading a ResNet model in Flax.`'
  id: totrans-2203
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Saving and loading a ResNet model in Flax.`'
- en: Elegy(High-level API for deep learning in JAX & Flax)
  id: totrans-2204
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: Elegy(High-level API for deep learning in JAX & Flax)
- en: '`Training deep learning networks in Flax is done in a couple of steps. It involves
    creating the following functions:`'
  id: totrans-2205
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Training deep learning networks in Flax is done in a couple of steps. It involves
    creating the following functions:`'
- en: Model definition.
  id: totrans-2206
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: Model definition.
- en: '`Compute metrics.`'
  id: totrans-2207
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Compute metrics.`'
- en: '`Training state.`'
  id: totrans-2208
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Training state.`'
- en: Training step.
  id: totrans-2209
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: Training step.
- en: Training and evaluation function.
  id: totrans-2210
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: Training and evaluation function.
- en: '`Flax and JAX give more control in defining and training deep learning networks.
    However, this comes with more verbosity. Enter Elegy. Elegy is a high-level API
    for creating deep learning networks in JAX. Elegy''s API is like the one in Keras.`'
  id: totrans-2211
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Flax and JAX give more control in defining and training deep learning networks.
    However, this comes with more verbosity. Enter Elegy. Elegy is a high-level API
    for creating deep learning networks in JAX. Elegy''s API is like the one in Keras.`'
- en: '`Let''s look at how to use Elegy to define and train deep learning networks
    in Flax.`'
  id: totrans-2212
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Let''s look at how to use Elegy to define and train deep learning networks
    in Flax.`'
- en: '`Data pre-processing`'
  id: totrans-2213
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '`Data pre-processing`'
- en: '`To make this illustration concrete, we''ll use the movie review data from
    Kaggle to create an LSTM network in Flax.`'
  id: totrans-2214
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`To make this illustration concrete, we''ll use the movie review data from
    Kaggle to create an LSTM network in Flax.`'
- en: '`The first step is to download and extract the data.`'
  id: totrans-2215
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`The first step is to download and extract the data.`'
- en: '`import os`'
  id: totrans-2216
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import os`'
- en: '`import kaggle`'
  id: totrans-2217
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import kaggle`'
- en: '`Obtain from https://www.kaggle.com/username/account os.environ["KAGGLE_USERNAME"]="KAGGLE_USERNAME"`'
  id: totrans-2218
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`Obtain from https://www.kaggle.com/username/account os.environ["KAGGLE_USERNAME"]="KAGGLE_USERNAME"`'
- en: '`os.environ["KAGGLE_KEY"]="KAGGLE_KEY"`'
  id: totrans-2219
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`os.environ["KAGGLE_KEY"]="KAGGLE_KEY"`'
- en: '`!kaggle datasets download lakshmi25npathi/imdb-dataset-of-50k-m ovie-reviews`'
  id: totrans-2220
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`!kaggle datasets download lakshmi25npathi/imdb-dataset-of-50k-m ovie-reviews`'
- en: '`import zipfile`'
  id: totrans-2221
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import zipfile`'
- en: '`with zipfile.ZipFile(''imdb-dataset-of-50k-movie-reviews.zip'',`'
  id: totrans-2222
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`with zipfile.ZipFile(''imdb-dataset-of-50k-movie-reviews.zip'',`'
- en: '`''r'') as zip_ref:zip_ref.extractall(''imdb-dataset-of-50k-movie-reviews'')Next,
    we define the following processing steps:`'
  id: totrans-2223
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`''r'') as zip_ref:zip_ref.extractall(''imdb-dataset-of-50k-movie-reviews'')Next,
    we define the following processing steps:`'
- en: '`Split the data into a training and testing set. Remove stopwords from the
    data.`'
  id: totrans-2224
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Split the data into a training and testing set. Remove stopwords from the
    data.`'
- en: '`Clean the data by removing punctuations and other special characters.`'
  id: totrans-2225
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 清理数据，去除标点和其他特殊字符。
- en: '`Convert the data to a TensorFlow dataset.`'
  id: totrans-2226
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Convert the data to a TensorFlow dataset.`'
- en: '`Conver the data to numerical representation using the Keras vectorization
    layer.`'
  id: totrans-2227
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Conver the data to numerical representation using the Keras vectorization
    layer.`'
- en: '`import numpy as np`'
  id: totrans-2228
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import numpy as np`'
- en: '`import pandas as pd`'
  id: totrans-2229
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import pandas as pd`'
- en: '`from numpy import array`'
  id: totrans-2230
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from numpy import array`'
- en: '`import tensorflow_datasets as tfds`'
  id: totrans-2231
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import tensorflow_datasets as tfds`'
- en: '`import tensorflow as tf`'
  id: totrans-2232
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import tensorflow as tf`'
- en: '`from sklearn.model_selection import train_test_split from sklearn.preprocessing
    import LabelEncoder import matplotlib.pyplot as plt`'
  id: totrans-2233
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from sklearn.model_selection import train_test_split from sklearn.preprocessing
    import LabelEncoder import matplotlib.pyplot as plt`'
- en: '`from sklearn.model_selection import train_test_split import tensorflow as
    tf`'
  id: totrans-2234
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from sklearn.model_selection import train_test_split import tensorflow as
    tf`'
- en: '`df = pd.read_csv("imdb-dataset-of-50k-movie-reviews/IMDB Datase t.csv")`'
  id: totrans-2235
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`df = pd.read_csv("imdb-dataset-of-50k-movie-reviews/IMDB Datase t.csv")`'
- en: '`import nltk`'
  id: totrans-2236
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import nltk`'
- en: '`from nltk.corpus import stopwords`'
  id: totrans-2237
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from nltk.corpus import stopwords`'
- en: '`nltk.download(''stopwords'')`'
  id: totrans-2238
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`nltk.download(''stopwords'')`'
- en: '`def remove_stop_words(review):`'
  id: totrans-2239
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def remove_stop_words(review):`'
- en: '`review_minus_sw = []`'
  id: totrans-2240
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`review_minus_sw = []`'
- en: '`stop_words = stopwords.words(''english'')`'
  id: totrans-2241
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`stop_words = stopwords.words(''english'')`'
- en: '`review = review.split()`'
  id: totrans-2242
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`review = review.split()`'
- en: '`cleaned_review = [review_minus_sw.append(word) for word in`'
  id: totrans-2243
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`cleaned_review = [review_minus_sw.append(word) for word in`'
- en: '`review if word not in stop_words]`'
  id: totrans-2244
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`review if word not in stop_words]`'
- en: '`cleaned_review = '' ''.join(review_minus_sw)`'
  id: totrans-2245
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`cleaned_review = '' ''.join(review_minus_sw)`'
- en: '`return cleaned_review`'
  id: totrans-2246
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return cleaned_review`'
- en: '`df[''review''] = df[''review''].apply(remove_stop_words) labelencoder = LabelEncoder()`'
  id: totrans-2247
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`df[''review''] = df[''review''].apply(remove_stop_words) labelencoder = LabelEncoder()`'
- en: '`df = df.assign(sentiment = labelencoder.fit_transform(df["senti ment"]))`'
  id: totrans-2248
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`df = df.assign(sentiment = labelencoder.fit_transform(df["senti ment"]))`'
- en: '`df = df.drop_duplicates()`'
  id: totrans-2249
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`df = df.drop_duplicates()`'
- en: '`docs = df[''review'']`'
  id: totrans-2250
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`docs = df[''review'']`'
- en: '`labels = array(df[''sentiment''])`'
  id: totrans-2251
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`labels = array(df[''sentiment''])`'
- en: '`X_train, X_test , y_train, y_test = train_test_split(docs, labe ls , test_size
    = 0.20, random_state=0)`'
  id: totrans-2252
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`X_train, X_test , y_train, y_test = train_test_split(docs, labe ls , test_size
    = 0.20, random_state=0)`'
- en: '`max_features = 10000 # Maximum vocab size.`'
  id: totrans-2253
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`max_features = 10000 # 最大词汇量大小。`'
- en: '`batch_size = 128`'
  id: totrans-2254
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`batch_size = 128`'
- en: '`max_len = 50 # Sequence length to pad the outputs to. vectorize_layer = tf.keras.layers.TextVectorization(standardize
    =''lower_and_strip_punctuation'',max_tokens=max_features,output_m ode=''int'',output_sequence_length=max_len)`'
  id: totrans-2255
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`max_len = 50 # 序列长度，用于填充输出。vectorize_layer = tf.keras.layers.TextVectorization(standardize
    =''lower_and_strip_punctuation'',max_tokens=max_features,output_m ode=''int'',output_sequence_length=max_len)`'
- en: '`vectorize_layer.adapt(X_train)`'
  id: totrans-2256
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`vectorize_layer.adapt(X_train)`'
- en: '`X_train_padded = vectorize_layer(X_train)`'
  id: totrans-2257
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`X_train_padded = vectorize_layer(X_train)`'
- en: '`X_test_padded = vectorize_layer(X_test)`'
  id: totrans-2258
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`X_test_padded = vectorize_layer(X_test)`'
- en: '`training_data = tf.data.Dataset.from_tensor_slices((X_train_pad ded, y_train))`'
  id: totrans-2259
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`training_data = tf.data.Dataset.from_tensor_slices((X_train_pad ded, y_train))`'
- en: '`validation_data = tf.data.Dataset.from_tensor_slices((X_test_pa dded, y_test))`'
  id: totrans-2260
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`validation_data = tf.data.Dataset.from_tensor_slices((X_test_pa dded, y_test))`'
- en: '`training_data = training_data.batch(batch_size)`'
  id: totrans-2261
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`training_data = training_data.batch(batch_size)`'
- en: '`validation_data = validation_data.batch(batch_size) def get_train_batches():`'
  id: totrans-2262
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`validation_data = validation_data.batch(batch_size) def get_train_batches():`'
- en: '`ds = training_data.prefetch(1)`'
  id: totrans-2263
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`ds = training_data.prefetch(1)`'
- en: '`ds = ds.repeat(3)`'
  id: totrans-2264
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`ds = ds.repeat(3)`'
- en: '`ds = ds.shuffle(3, reshuffle_each_iteration=True)` # `tfds.dataset_as_numpy`
    converts the `tf.data.Dataset` into an'
  id: totrans-2265
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`ds = ds.shuffle(3, reshuffle_each_iteration=True)` # `tfds.dataset_as_numpy`
    converts the `tf.data.Dataset` into an'
- en: '`iterable of NumPy arraysreturn tfds.as_numpy(ds)`'
  id: totrans-2266
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`iterable of NumPy arraysreturn tfds.as_numpy(ds)`'
- en: '**Model definition in Elegy**'
  id: totrans-2267
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**Elegy模型定义**'
- en: Start by installing `Elegy`, `Flax`, and `JAX`.`pip install -U elegy flax jax
    jaxlib`Next, define the LSTM model.
  id: totrans-2268
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 首先安装`Elegy`，`Flax`和`JAX`。`pip install -U elegy flax jax jaxlib`接下来，定义LSTM模型。
- en: '`import jax`'
  id: totrans-2269
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import jax`'
- en: '`import jax.numpy as jnp import elegy as eg`'
  id: totrans-2270
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import jax.numpy as jnp import elegy as eg`'
- en: '`from flax import linen as nn`'
  id: totrans-2271
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from flax import linen as nn`'
- en: '`class LSTMModel(nn.Module):`'
  id: totrans-2272
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`class LSTMModel(nn.Module):`'
- en: '`def setup(self):`'
  id: totrans-2273
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def setup(self):`'
- en: '`self.embedding = nn.Embed(max_features, max_len)` `lstm_layer = nn.scan(nn.OptimizedLSTMCell,
    variable_broadcast="params", split_rngs={"params": False}, in_axes=1,`'
  id: totrans-2274
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.embedding = nn.Embed(max_features, max_len)` `lstm_layer = nn.scan(nn.OptimizedLSTMCell,
    variable_broadcast="params", split_rngs={"params": False}, in_axes=1,`'
- en: '`out_axes=1,`'
  id: totrans-2275
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`out_axes=1,`'
- en: '`length=max_len,`'
  id: totrans-2276
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`length=max_len,`'
- en: '`reverse=False)`'
  id: totrans-2277
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`reverse=False)`'
- en: '`self.lstm1 = lstm_layer()`'
  id: totrans-2278
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.lstm1 = lstm_layer()`'
- en: '`self.dense1 = nn.Dense(256)`'
  id: totrans-2279
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.dense1 = nn.Dense(256)`'
- en: '`self.lstm2 = lstm_layer()`'
  id: totrans-2280
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.lstm2 = lstm_layer()`'
- en: '`self.dense2 = nn.Dense(128)`'
  id: totrans-2281
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.dense2 = nn.Dense(128)`'
- en: '`self.lstm3 = lstm_layer()`'
  id: totrans-2282
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.lstm3 = lstm_layer()`'
- en: '`self.dense3 = nn.Dense(64)`'
  id: totrans-2283
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.dense3 = nn.Dense(64)`'
- en: '`self.dense4 = nn.Dense(2)`'
  id: totrans-2284
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.dense4 = nn.Dense(2)`'
- en: '`@nn.rematdef __call__(self, x_batch): x = self.embedding(x_batch)`'
  id: totrans-2285
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`@nn.rematdef __call__(self, x_batch): x = self.embedding(x_batch)`'
- en: '`carry, hidden = nn.OptimizedLSTMCell.initialize_carry(j ax.random.PRNGKey(0),
    batch_dims=(len(x_batch),), size=128)`'
  id: totrans-2286
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`carry, hidden = nn.OptimizedLSTMCell.initialize_carry(j ax.random.PRNGKey(0),
    batch_dims=(len(x_batch),), size=128)`'
- en: '`(carry, hidden)`, `x = self.lstm1((carry, hidden), x)`'
  id: totrans-2287
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`(carry, hidden)`, `x = self.lstm1((carry, hidden), x)`'
- en: '`x = self.dense1(x) x = nn.relu(x)`'
  id: totrans-2288
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = self.dense1(x) x = nn.relu(x)`'
- en: '`carry, hidden = nn.OptimizedLSTMCell.initialize_carry(j ax.random.PRNGKey(0),
    batch_dims=(len(x_batch),), size=64)`'
  id: totrans-2289
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`carry, hidden = nn.OptimizedLSTMCell.initialize_carry(j ax.random.PRNGKey(0),
    batch_dims=(len(x_batch),), size=64)`'
- en: '`(carry, hidden), x = self.lstm2((carry, hidden), x)`'
  id: totrans-2290
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`(carry, hidden), x = self.lstm2((carry, hidden), x)`'
- en: '`x = self.dense2(x) x = nn.relu(x)`'
  id: totrans-2291
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = self.dense2(x) x = nn.relu(x)`'
- en: '`carry, hidden = nn.OptimizedLSTMCell.initialize_carry(j ax.random.PRNGKey(0),
    batch_dims=(len(x_batch),), size=32)`'
  id: totrans-2292
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`carry, hidden = nn.OptimizedLSTMCell.initialize_carry(j ax.random.PRNGKey(0),
    batch_dims=(len(x_batch),), size=32)`'
- en: '`(carry, hidden), x = self.lstm3((carry, hidden), x)`'
  id: totrans-2293
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`(carry, hidden), x = self.lstm3((carry, hidden), x)`'
- en: '`x = self.dense3(x)`'
  id: totrans-2294
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = self.dense3(x)`'
- en: '`x = nn.relu(x)`'
  id: totrans-2295
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = nn.relu(x)`'
- en: '`x = self.dense4(x[:, -1]) return nn.log_softmax(x)`'
  id: totrans-2296
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = self.dense4(x[:, -1]) return nn.log_softmax(x)`'
- en: Let's now create an Elegy model using the above network. As you can see, the
    loss and metrics are defined like in `Keras`. The model compilation is done in
    the constructor, so you don't have to do this manually.
  id: totrans-2297
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 现在让我们使用上述网络创建一个Elegy模型。正如你所见，损失和指标的定义类似于`Keras`。模型的编译在构造函数中完成，因此您无需手动执行此操作。
- en: '`import optax`'
  id: totrans-2298
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import optax`'
- en: '`model = eg.Model(`'
  id: totrans-2299
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`model = eg.Model(`'
- en: '`module=LSTMModel(),`'
  id: totrans-2300
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`module=LSTMModel(),`'
- en: '`loss=[`'
  id: totrans-2301
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`loss=[`'
- en: '`eg.losses.Crossentropy()`, `eg.regularizers.L2(l=1e-4)`, ],'
  id: totrans-2302
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`eg.losses.Crossentropy()`, `eg.regularizers.L2(l=1e-4)`, ],'
- en: '`metrics=eg.metrics.Accuracy(),`'
  id: totrans-2303
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`metrics=eg.metrics.Accuracy(),`'
- en: '`optimizer=optax.adam(1e-3), )`'
  id: totrans-2304
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`optimizer=optax.adam(1e-3), )`'
- en: '**Elegy model summary**'
  id: totrans-2305
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**Elegy模型摘要**'
- en: Like in Keras, we can print the model's summary. `model.summary(jnp.array(X_train_padded[:64]))`
    ![](../images/00059.jpeg)
  id: totrans-2306
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 像在 Keras 中一样，我们可以打印模型的摘要。`model.summary(jnp.array(X_train_padded[:64]))` ![](../images/00059.jpeg)
- en: '**Distributed training in Elegy**'
  id: totrans-2307
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**Elegy 中的分布式训练**'
- en: To train models in a distributed manner in Flax, we define parallel versions
    of our model training functions.
  id: totrans-2308
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 要在 Flax 中以分布式方式训练模型，我们定义模型训练函数的并行版本。
- en: However, with Elegy, we call the `[distributed]` method.`model = model.distributed()`
  id: totrans-2309
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 但是，在 Elegy 中，我们调用 `[distributed]` 方法来进行分布式方法。`model = model.distributed()`
- en: '**Keras-like callbacks in Flax**'
  id: totrans-2310
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**Flax 中类似 Keras 的回调**'
- en: 'Elegy supports callbacks similar to Keras callbacks. In this case, we train
    the model with the following callbacks:'
  id: totrans-2311
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: Elegy 支持类似 Keras 回调的回调函数。在这种情况下，我们使用以下回调函数训练模型：
- en: '`TensorBoard.`'
  id: totrans-2312
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`TensorBoard.`'
- en: '`Model checkpoint.`'
  id: totrans-2313
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`模型检查点.`'
- en: '`Early stopping.callbacks = [ eg.callbacks.TensorBoard("summaries"), eg.callbacks.ModelCheckpoint("models/high-level",
    save_best_only=True),eg.callbacks.EarlyStopping(monitor = ''val_loss'',pa tience=10)]`'
  id: totrans-2314
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Early stopping.callbacks = [ eg.callbacks.TensorBoard("summaries"), eg.callbacks.ModelCheckpoint("models/high-level",
    save_best_only=True),eg.callbacks.EarlyStopping(monitor = ''val_loss'',pa tience=10)]`'
- en: '**Train Elegy models**'
  id: totrans-2315
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**训练 Elegy 模型**'
- en: 'Elegy provides the `[fit]` method for training models. The method supports
    the following data sources:'
  id: totrans-2316
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: Elegy 提供了 `[fit]` 方法来训练模型。该方法支持以下数据源：
- en: '`Tensorflow Dataset.`'
  id: totrans-2317
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Tensorflow 数据集.`'
- en: '`Pytorch DataLoader`'
  id: totrans-2318
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Pytorch DataLoader`'
- en: '`Elegy DataLoader, and`'
  id: totrans-2319
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Elegy DataLoader，并且`'
- en: '`Python Generators.`'
  id: totrans-2320
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Python 生成器.`'
- en: '`history = model.fit(`'
  id: totrans-2321
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`history = model.fit(`'
- en: '`training_data,`'
  id: totrans-2322
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`training_data,`'
- en: '`epochs=100,`'
  id: totrans-2323
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`epochs=100,`'
- en: validation_data=(validation_data), callbacks=callbacks,
  id: totrans-2324
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: validation_data=(validation_data), callbacks=callbacks,
- en: )![](../images/00060.gif)
  id: totrans-2325
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: )![](../images/00060.gif)
- en: '**Evaluate Elegy models**'
  id: totrans-2326
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**评估 Elegy 模型**'
- en: To evaluate Elegy models, use the [evaluate] function.model.evaluate(validation_data)![](../images/00061.jpeg)
  id: totrans-2327
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 要评估 Elegy 模型，请使用 [evaluate] 函数。model.evaluate(validation_data)![](../images/00061.jpeg)
- en: '**Visualize Elegy model with TensorBoard**'
  id: totrans-2328
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**使用 TensorBoard 可视化 Elegy 模型**'
- en: Since we applied the TensorBoard callback, we can view the performance of the
    model in TensorBoard.%load_ext tensorboard%tensorboard --logdir summaries ![](../images/00062.jpeg)
  id: totrans-2329
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 由于我们应用了 TensorBoard 回调，我们可以在 TensorBoard 中查看模型的性能。%load_ext tensorboard%tensorboard
    --logdir summaries ![](../images/00062.jpeg)
- en: '**Plot model performance with Matplotlib**'
  id: totrans-2330
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**使用 Matplotlib 绘制模型性能**'
- en: We can also plot the performance of the model using Matplotlib.import matplotlib.pyplot
    as plt
  id: totrans-2331
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 我们还可以使用 Matplotlib 绘制模型的性能。import matplotlib.pyplot as plt
- en: 'def plot_history(history):'
  id: totrans-2332
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def plot_history(history):'
- en: n_plots = len(history.history.keys()) // 2
  id: totrans-2333
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: n_plots = len(history.history.keys()) // 2
- en: plt.figure(figsize=(14, 24))
  id: totrans-2334
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: plt.figure(figsize=(14, 24))
- en: for i, key in enumerate(list(history.history.keys())[:n_plo
  id: totrans-2335
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: for i, key in enumerate(list(history.history.keys())[:n_plo
- en: 'ts]):'
  id: totrans-2336
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'ts]):'
- en: metric = history.history[`key`]
  id: totrans-2337
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: metric = history.history[`key`]
- en: val_metric = history.history[f`val_{key}`]
  id: totrans-2338
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: val_metric = history.history[f`val_{key}`]
- en: plt.subplot(n_plots, 1, i + 1)
  id: totrans-2339
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: plt.subplot(n_plots, 1, i + 1)
- en: plt.plot(metric, label=f`Training {key}`) plt.plot(val_metric, label=f`Validation
    {key}`) plt.legend(loc=`lower right`)
  id: totrans-2340
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: plt.plot(metric, label=f`Training {key}`) plt.plot(val_metric, label=f`Validation
    {key}`) plt.legend(loc=`lower right`)
- en: plt.ylabel(`key`)
  id: totrans-2341
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: plt.ylabel(`key`)
- en: plt.title(`Training and Validation {key}`)
  id: totrans-2342
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: plt.title(`Training and Validation {key}`)
- en: plt.show()plot_history(history) ![](../images/00063.jpeg)
  id: totrans-2343
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: plt.show()plot_history(history) ![](../images/00063.jpeg)
- en: '**Making predictions with Elegy models**'
  id: totrans-2344
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**使用 Elegy 模型进行预测**'
- en: Like Keras, Elegy provides the [predict] method for making predictions.(text,
    test_labels) = next(iter(validation_data)) y_pred = model.predict(jnp.array(text))
    ![](../images/00064.jpeg)
  id: totrans-2345
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 像 Keras 一样，Elegy 提供了进行预测的 [predict] 方法。(text, test_labels) = next(iter(validation_data))
    y_pred = model.predict(jnp.array(text)) ![](../images/00064.jpeg)
- en: '**Saving and loading Elegy models**'
  id: totrans-2346
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**保存和加载 Elegy 模型**'
- en: Elegy models can also be saved like Keras models and used to make predictions
    immediately.
  id: totrans-2347
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: Elegy 模型也可以像 Keras 模型一样保存，并立即用于预测。
- en: You can use can use `save` but `ModelCheckpoint already seria lized the model
  id: totrans-2348
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: 您可以使用 `save` 但是 `ModelCheckpoint` 已经序列化了模型
- en: model.save(`model`)
  id: totrans-2349
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: model.save(`model`)
- en: current model referenceprint(`current model id:`, id(model))
  id: totrans-2350
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: 当前模型参考print(`current model id:`, id(model))
- en: load model from disk
  id: totrans-2351
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: 从磁盘加载模型
- en: model = eg.load(`models/high-level`)
  id: totrans-2352
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: model = eg.load(`models/high-level`)
- en: new model reference
  id: totrans-2353
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: 新模型参考
- en: 'print(`new model id: `, id(model))'
  id: totrans-2354
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'print(`new model id: `, id(model))'
- en: check that it works!model.evaluate(validation_data) ![](../images/00065.jpeg)
  id: totrans-2355
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: 检查其是否正常工作！model.evaluate(validation_data) ![](../images/00065.jpeg)
- en: '**Final thoughts**'
  id: totrans-2356
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**最终想法**'
- en: 'This article has been a quick dive into Elegy– a JAX high-level API that you
    can use to build and train Flax networks. You have seen that Elegy is very similar
    to Keras and has a simple API for Flax. It also contains similar functions to
    Keras, like:'
  id: totrans-2357
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 这篇文章简要介绍了 Elegy —— 一种您可以用来构建和训练 Flax 网络的 JAX 高级 API。您已经看到 Elegy 非常类似于 Keras，并且具有用于
    Flax 的简单 API。它还包含类似于 Keras 的功能，如：
- en: Model training.
  id: totrans-2358
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 模型训练。
- en: Making predictions.
  id: totrans-2359
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 进行预测。
- en: Creating callbacks.
  id: totrans-2360
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 创建回调。
- en: Defining model loss and metrics.
  id: totrans-2361
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 定义模型损失和指标。
- en: '**Appendix**'
  id: totrans-2362
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**附录**'
- en: This book is provided in line with our terms and privacy policy.
  id: totrans-2363
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 本书的提供符合我们的条款和隐私政策。
- en: '**Disclaimer**'
  id: totrans-2364
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**免责声明**'
- en: The information in this eBook is not meant to be applied as is in a production
  id: totrans-2365
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 本电子书中的信息不适用于直接在生产中应用
- en: environment. By applying it to a production environment, you take full responsibility
  id: totrans-2366
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 环境。将其应用于生产环境时，您需承担全部责任
- en: for your actions.
  id: totrans-2367
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 由于您的行为。
- en: The author has made every effort to ensure the accuracy of the information within
  id: totrans-2368
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 作者已尽一切努力确保信息的准确性
- en: this book was correct at the time of publication. The author does not assume
    and
  id: totrans-2369
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 本书出版时的信息是正确的。作者不承担任何责任，并且
- en: hereby disclaims any liability to any party for any loss, damage, or disruption
    caused
  id: totrans-2370
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 特此声明对任何因损失、损坏或造成中断而导致的责任不承担。
- en: by errors or omissions, whether such errors or omissions result from accident,
  id: totrans-2371
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 由于意外、
- en: negligence, or any other cause.
  id: totrans-2372
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 疏忽或其他原因。
- en: No part of this eBook may be reproduced or transmitted in any form or by any
  id: totrans-2373
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 未经任何形式或任何
- en: means, electronic or mechanical, recording or by any information storage and
  id: totrans-2374
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 手段，电子或机械，录制或通过任何信息存储和
- en: retrieval system, without written permission from the author.
  id: totrans-2375
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 检索系统，未经作者书面许可。
- en: '**Copyright**'
  id: totrans-2376
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**版权**'
- en: JAX and Flax book— Deep learning with Flax and JAX © Copyright Derrick Mwiti.
    All Rights Reserved.
  id: totrans-2377
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAX 和 Flax 书籍—— 使用 Flax 和 JAX 进行深度学习 © 版权 Derrick Mwiti。保留所有权利。
- en: '**Other things to learn**'
  id: totrans-2378
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**其他学习内容**'
- en: Learn Python
  id: totrans-2379
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 学习 Python
- en: Learn data science
  id: totrans-2380
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 学习数据科学
- en: Learn Streamlit
  id: totrans-2381
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 学习 Streamlit
