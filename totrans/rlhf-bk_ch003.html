<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <title>ch003.xhtml</title>
  <link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css" />
</head>
<body epub:type="bodymatter">
<section id="definitions-background" class="level1">
<h1>Definitions &amp; Background</h1>
<p>This chapter includes all the definitions, symbols, and operations frequently used in the RLHF process and with a quick overview of language models, which is the guiding application of this book.</p>
<section id="language-modeling-overview" class="level2">
<h2>Language Modeling Overview</h2>
<p>The majority of modern language models are trained to learn the joint probability distribution of sequences of tokens (words, subwords, or characters) in an autoregressive manner. Autoregression simply means that each next prediction depends on the previous entities in the sequence. Given a sequence of tokens <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mo stretchy="false" form="prefix">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><msub><mi>x</mi><mi>T</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">x = (x_1, x_2, \ldots, x_T)</annotation></semantics></math>, the model factorizes the probability of the entire sequence into a product of conditional distributions:</p>
<p><span id="eq:llming"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><munderover><mo>‚àè</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><msub><mi>P</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>‚à£</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></msub><mo stretchy="false" form="postfix">)</mo><mi>.</mi><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P_{\theta}(x) = \prod_{t=1}^{T} P_{\theta}(x_{t} \mid x_{1}, \ldots, x_{t-1}).\qquad{(1)}</annotation></semantics></math></span></p>
<p>In order to fit a model that accurately predicts this, the goal is often to maximize the likelihood of the training data as predicted by the current model. To do so, we can minimize a negative log-likelihood (NLL) loss:</p>
<p><span id="eq:nll"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>‚Ñí</mi><mtext mathvariant="normal">LM</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>‚àí</mi><mspace width="0.167em"></mspace><msub><mi>ùîº</mi><mrow><mi>x</mi><mo>‚àº</mo><mi>ùíü</mi></mrow></msub><mrow><mo stretchy="true" form="prefix">[</mo><munderover><mo>‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mi mathvariant="normal">log</mi><msub><mi>P</mi><mi>Œ∏</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>‚à£</mo><msub><mi>x</mi><mrow><mo>&lt;</mo><mi>t</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mi>.</mi><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>2</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}_{\text{LM}}(\theta)=-\,\mathbb{E}_{x \sim \mathcal{D}}\left[\sum_{t=1}^{T}\log P_{\theta}\left(x_t \mid x_{&lt;t}\right)\right]. \qquad{(2)}</annotation></semantics></math></span></p>
<p>In practice, one uses a cross-entropy loss with respect to each next-token prediction, computed by comparing the true token in a sequence to what was predicted by the model.</p>
<p>Language models come in many architectures with different trade-offs in terms of knowledge, speed, and other performance characteristics. Modern LMs, including ChatGPT, Claude, Gemini, etc., most often use <strong>decoder-only Transformers</strong> <span class="citation" data-cites="Vaswani2017AttentionIA"><a href="ch021.xhtml#ref-Vaswani2017AttentionIA">[49]</a></span>. The core innovation of the Transformer was heavily utilizing the <strong>self-attention</strong> <span class="citation" data-cites="Bahdanau2014NeuralMT"><a href="ch021.xhtml#ref-Bahdanau2014NeuralMT">[50]</a></span> mechanism to allow the model to directly attend to concepts in context and learn complex mappings. Throughout this book, particularly when covering reward models in Chapter 7, we will discuss adding new heads or modifying a language modeling (LM) head of the transformer. The LM head is a final linear projection layer that maps from the model‚Äôs internal embedding space to the tokenizer space (a.k.a. vocabulary). We‚Äôll see in this book that different ‚Äúheads‚Äù of a language model can be applied to finetune the model to different purposes ‚Äì in RLHF this is most often done when training a reward model, which is highlighted in Chapter 7.</p>
</section>
<section id="ml-definitions" class="level2">
<h2>ML Definitions</h2>
<ul>
<li><strong>Kullback-Leibler (KL) divergence (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ùíü</mi><mtext mathvariant="normal">KL</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>P</mi><mo stretchy="false" form="prefix">|</mo><mo stretchy="false" form="prefix">|</mo><mi>Q</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{D}_{\text{KL}}(P || Q)</annotation></semantics></math>)</strong>, also known as KL divergence, is a measure of the difference between two probability distributions. For discrete probability distributions <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math> defined on the same probability space <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ùí≥</mi><annotation encoding="application/x-tex">\mathcal{X}</annotation></semantics></math>, the KL distance from <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math> to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> is defined as:</li>
</ul>
<p><span id="eq:def_kl"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ùíü</mi><mtext mathvariant="normal">KL</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>P</mi><mo stretchy="false" form="prefix">|</mo><mo stretchy="false" form="prefix">|</mo><mi>Q</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><munder><mo>‚àë</mo><mrow><mi>x</mi><mo>‚àà</mo><mi>ùí≥</mi></mrow></munder><mi>P</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>P</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><mi>Q</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>3</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex"> \mathcal{D}_{\text{KL}}(P || Q) = \sum_{x \in \mathcal{X}} P(x) \log \left(\frac{P(x)}{Q(x)}\right) \qquad{(3)}</annotation></semantics></math></span></p>
</section>
<section id="nlp-definitions" class="level2">
<h2>NLP Definitions</h2>
<ul>
<li><p><strong>Prompt (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>)</strong>: The input text given to a language model to generate a response or completion.</p></li>
<li><p><strong>Completion (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>)</strong>: The output text generated by a language model in response to a prompt. Often the completion is denoted as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>‚à£</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">y\mid x</annotation></semantics></math>. Rewards and other values are often computed as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">r(y\mid x)</annotation></semantics></math> or <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">P(y\mid x)</annotation></semantics></math>.</p></li>
<li><p><strong>Chosen Completion (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mi>c</mi></msub><annotation encoding="application/x-tex">y_c</annotation></semantics></math>)</strong>: The completion that is selected or preferred over other alternatives, often denoted as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mrow><mi>c</mi><mi>h</mi><mi>o</mi><mi>s</mi><mi>e</mi><mi>n</mi></mrow></msub><annotation encoding="application/x-tex">y_{chosen}</annotation></semantics></math>.</p></li>
<li><p><strong>Rejected Completion (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mi>r</mi></msub><annotation encoding="application/x-tex">y_r</annotation></semantics></math>)</strong>: The disfavored completion in a pairwise setting.</p></li>
<li><p><strong>Preference Relation (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>‚âª</mo><annotation encoding="application/x-tex">\succ</annotation></semantics></math>)</strong>: A symbol indicating that one completion is preferred over another, e.g., <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mrow><mi>c</mi><mi>h</mi><mi>o</mi><mi>s</mi><mi>e</mi><mi>n</mi></mrow></msub><mo>‚âª</mo><msub><mi>y</mi><mrow><mi>r</mi><mi>e</mi><mi>j</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>e</mi><mi>d</mi></mrow></msub></mrow><annotation encoding="application/x-tex">y_{chosen} \succ y_{rejected}</annotation></semantics></math>. E.g. a reward model predicts the probability of a preference relation, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>‚âª</mo><msub><mi>y</mi><mi>r</mi></msub><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">P(y_c \succ y_r \mid x)</annotation></semantics></math>.</p></li>
<li><p><strong>Policy (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>œÄ</mi><annotation encoding="application/x-tex">\pi</annotation></semantics></math>)</strong>: A probability distribution over possible completions, parameterized by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ∏</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\pi_\theta(y\mid x)</annotation></semantics></math>.</p></li>
</ul>
</section>
<section id="rl-definitions" class="level2">
<h2>RL Definitions</h2>
<ul>
<li><p><strong>Reward (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math>)</strong>: A scalar value indicating the desirability of an action or state, typically denoted as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math>.</p></li>
<li><p><strong>Action (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics></math>)</strong>: A decision or move made by an agent in an environment, often represented as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>‚àà</mo><mi>A</mi></mrow><annotation encoding="application/x-tex">a \in A</annotation></semantics></math>, where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> is the set of possible actions.</p></li>
<li><p><strong>State (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math>)</strong>: The current configuration or situation of the environment, usually denoted as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mo>‚àà</mo><mi>S</mi></mrow><annotation encoding="application/x-tex">s \in S</annotation></semantics></math>, where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math> is the state space.</p></li>
<li><p><strong>Trajectory (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>œÑ</mi><annotation encoding="application/x-tex">\tau</annotation></semantics></math>)</strong>: A trajectory <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>œÑ</mi><annotation encoding="application/x-tex">\tau</annotation></semantics></math> is a sequence of states, actions, and rewards experienced by an agent: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>œÑ</mi><mo>=</mo><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo>,</mo><msub><mi>a</mi><mn>0</mn></msub><mo>,</mo><msub><mi>r</mi><mn>0</mn></msub><mo>,</mo><msub><mi>s</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>r</mi><mn>1</mn></msub><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi><mo>,</mo><msub><mi>s</mi><mi>T</mi></msub><mo>,</mo><msub><mi>a</mi><mi>T</mi></msub><mo>,</mo><msub><mi>r</mi><mi>T</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\tau = (s_0, a_0, r_0, s_1, a_1, r_1, ..., s_T, a_T, r_T)</annotation></semantics></math>.</p></li>
<li><p><strong>Trajectory Distribution (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><mi>œÑ</mi><mo>‚à£</mo><mi>œÄ</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\tau\mid\pi)</annotation></semantics></math>)</strong>: The probability of a trajectory under policy <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>œÄ</mi><annotation encoding="application/x-tex">\pi</annotation></semantics></math> is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false" form="prefix">(</mo><mi>œÑ</mi><mo>‚à£</mo><mi>œÄ</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>p</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo stretchy="false" form="postfix">)</mo><msubsup><mo>‚àè</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>T</mi></msubsup><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>‚à£</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mi>p</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>‚à£</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">P(\tau\mid\pi) = p(s_0)\prod_{t=0}^T \pi(a_t\mid s_t)p(s_{t+1}\mid s_t,a_t)</annotation></semantics></math>, where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">p(s_0)</annotation></semantics></math> is the prior state distribution and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>‚à£</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">p(s_{t+1}\mid s_t,a_t)</annotation></semantics></math> is the transition probability.</p></li>
<li><p><strong>Policy (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>œÄ</mi><annotation encoding="application/x-tex">\pi</annotation></semantics></math>)</strong>, also called the <strong>policy model</strong> in RLHF: In RL, a policy is a strategy or rule that the agent follows to decide which action to take in a given state: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo>‚à£</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\pi(a\mid s)</annotation></semantics></math>.</p></li>
<li><p><strong>Discount Factor (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ≥</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math>)</strong>: A scalar <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn><mo>‚â§</mo><mi>Œ≥</mi><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">0 \le \gamma &lt; 1</annotation></semantics></math> that exponentially down-weights future rewards in the return, trading off immediacy versus long-term gain and guaranteeing convergence for infinite-horizon sums. Sometimes discounting is not used, which is equivalent to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ≥</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\gamma=1</annotation></semantics></math>.</p></li>
<li><p><strong>Value Function (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation></semantics></math>)</strong>: A function that estimates the expected cumulative reward from a given state: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>ùîº</mi><mo stretchy="false" form="prefix">[</mo><msubsup><mo>‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>‚àû</mi></msubsup><msup><mi>Œ≥</mi><mi>t</mi></msup><msub><mi>r</mi><mi>t</mi></msub><mo>‚à£</mo><msub><mi>s</mi><mn>0</mn></msub><mo>=</mo><mi>s</mi><mo stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">V(s) = \mathbb{E}[\sum_{t=0}^{\infty} \gamma^t r_t \mid s_0 = s]</annotation></semantics></math>.</p></li>
<li><p><strong>Q-Function (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>)</strong>: A function that estimates the expected cumulative reward from taking a specific action in a given state: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>ùîº</mi><mo stretchy="false" form="prefix">[</mo><msubsup><mo>‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>‚àû</mi></msubsup><msup><mi>Œ≥</mi><mi>t</mi></msup><msub><mi>r</mi><mi>t</mi></msub><mo>‚à£</mo><msub><mi>s</mi><mn>0</mn></msub><mo>=</mo><mi>s</mi><mo>,</mo><msub><mi>a</mi><mn>0</mn></msub><mo>=</mo><mi>a</mi><mo stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">Q(s,a) = \mathbb{E}[\sum_{t=0}^{\infty} \gamma^t r_t \mid s_0 = s, a_0 = a]</annotation></semantics></math>.</p></li>
<li><p><strong>Advantage Function (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>)</strong>: The advantage function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">A(s,a)</annotation></semantics></math> quantifies the relative benefit of taking action <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics></math> in state <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math> compared to the average action. It‚Äôs defined as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>Q</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo><mo>‚àí</mo><mi>V</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">A(s,a) = Q(s,a) - V(s)</annotation></semantics></math>. Advantage functions (and value functions) can depend on a specific policy, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>A</mi><mi>œÄ</mi></msup><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">A^\pi(s,a)</annotation></semantics></math>.</p></li>
<li><p><strong>Policy-conditioned Values (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">[</mo><msup><mo stretchy="false" form="postfix">]</mo><mrow><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><mi>‚ãÖ</mi><mo stretchy="false" form="postfix">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">[]^{\pi(\cdot)}</annotation></semantics></math>)</strong>: Across RL derivations and implementations, a crucial component of the theory and practice is collecting data or values conditioned on a specific policy. Throughout this book we will switch between the simpler notation of value functions (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mo>,</mo><mi>A</mi><mo>,</mo><mi>Q</mi><mo>,</mo><mi>G</mi></mrow><annotation encoding="application/x-tex">V,A,Q,G</annotation></semantics></math>) and their specific policy-conditioned values (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>V</mi><mi>œÄ</mi></msup><mo>,</mo><msup><mi>A</mi><mi>œÄ</mi></msup><mo>,</mo><msup><mi>Q</mi><mi>œÄ</mi></msup></mrow><annotation encoding="application/x-tex">V^\pi,A^\pi,Q^\pi</annotation></semantics></math>). Also crucial in the expected value computation is sampling from data <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math>, that is conditioned on a specific policy, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>d</mi><mi>œÄ</mi></msub><annotation encoding="application/x-tex">d_\pi</annotation></semantics></math> (e.g., <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mo>‚àº</mo><msub><mi>d</mi><mi>œÄ</mi></msub></mrow><annotation encoding="application/x-tex">s \sim d_\pi</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>‚àº</mo><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><mi>‚ãÖ</mi><mo>‚à£</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">a \sim \pi(\cdot\mid s)</annotation></semantics></math> when estimating <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ùîº</mi><mrow><mi>s</mi><mo>‚àº</mo><msub><mi>d</mi><mi>œÄ</mi></msub><mo>,</mo><mspace width="0.167em"></mspace><mi>a</mi><mo>‚àº</mo><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><mi>‚ãÖ</mi><mo>‚à£</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msub><mo stretchy="false" form="prefix">[</mo><msup><mi>A</mi><mi>œÄ</mi></msup><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">\mathbb{E}_{s\sim d_\pi,\,a\sim\pi(\cdot\mid s)}[A^\pi(s,a)]</annotation></semantics></math>).</p></li>
<li><p><strong>Expectation of Reward Optimization</strong>: The primary goal in RL, which involves maximizing the expected cumulative reward:</p>
<p><span id="eq:expect_reward_opt"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><munder><mi mathvariant="normal">max</mi><mi>Œ∏</mi></munder><msub><mi>ùîº</mi><mrow><mi>s</mi><mo>‚àº</mo><msub><mi>œÅ</mi><mi>œÄ</mi></msub><mo>,</mo><mi>a</mi><mo>‚àº</mo><msub><mi>œÄ</mi><mi>Œ∏</mi></msub></mrow></msub><mo stretchy="false" form="prefix">[</mo><munderover><mo>‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mo accent="false">‚àû</mo></munderover><msup><mi>Œ≥</mi><mi>t</mi></msup><msub><mi>r</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">]</mo><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>4</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\max_{\theta} \mathbb{E}_{s \sim \rho_\pi, a \sim \pi_\theta}[\sum_{t=0}^{\infty} \gamma^t r_t]\qquad{(4)}</annotation></semantics></math></span></p>
<p>where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>œÅ</mi><mi>œÄ</mi></msub><annotation encoding="application/x-tex">\rho_\pi</annotation></semantics></math> is the state distribution under policy <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>œÄ</mi><annotation encoding="application/x-tex">\pi</annotation></semantics></math>, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ≥</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math> is the discount factor.</p></li>
<li><p><strong>Finite Horizon Reward (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">J(\pi_\theta)</annotation></semantics></math>)</strong>: The expected finite-horizon discounted return of the policy <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><annotation encoding="application/x-tex">\pi_\theta</annotation></semantics></math>, parameterized by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ∏</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math> is defined as:</p>
<p><span id="eq:finite_horizon_return"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ùîº</mi><mrow><mi>œÑ</mi><mo>‚àº</mo><msub><mi>œÄ</mi><mi>Œ∏</mi></msub></mrow></msub><mrow><mo stretchy="true" form="prefix">[</mo><munderover><mo>‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>T</mi></munderover><msup><mi>Œ≥</mi><mi>t</mi></msup><msub><mi>r</mi><mi>t</mi></msub><mo stretchy="true" form="postfix">]</mo></mrow><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>5</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">J(\pi_\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \gamma^t r_t \right]\qquad{(5)}</annotation></semantics></math></span></p>
<p>where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>œÑ</mi><mo>‚àº</mo><msub><mi>œÄ</mi><mi>Œ∏</mi></msub></mrow><annotation encoding="application/x-tex">\tau \sim \pi_\theta</annotation></semantics></math> denotes trajectories sampled by following policy <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><annotation encoding="application/x-tex">\pi_\theta</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math> is the finite horizon.</p></li>
<li><p><strong>On-policy</strong>: In RLHF, particularly in the debate between RL and Direct Alignment Algorithms, the discussion of <strong>on-policy</strong> data is common. In the RL literature, on-policy means that the data is generated <em>exactly</em> by the current form of the agent, but in the general preference-tuning literature, on-policy is expanded to mean generations from that edition of model ‚Äì e.g.¬†a instruction-tuned checkpoint before running any preference fine-tuning. In this context, off-policy could be data generated by any other language model being used in post-training.</p></li>
</ul>
</section>
<section id="rlhf-only-definitions" class="level2">
<h2>RLHF Only Definitions</h2>
<ul>
<li><strong>Reference Model (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><annotation encoding="application/x-tex">\pi_{\text{ref}}</annotation></semantics></math>)</strong>: This is a saved set of parameters used in RLHF where outputs of it are used to regularize the optimization.</li>
</ul>
</section>
<section id="extended-glossary" class="level2">
<h2>Extended Glossary</h2>
<ul>
<li><strong>Synthetic Data</strong>: This is any training data for an AI model that is the output from another AI system. This could be anything from text generated from an open-ended prompt of a model to a model rewriting existing content.</li>
<li><strong>Distillation</strong>: Distillation is a general set of practices in training AI models where a model is trained on the outputs of a stronger model. This is a type of synthetic data known to make strong, smaller models. Most models make the rules around distillation clear through either the license, for open weight models, or the terms of service, for models accessible only via API. The term distillation is now overloaded with a specific technical definition from the ML literature.</li>
<li><strong>(Teacher-student) Knowledge Distillation</strong>: Knowledge distillation from a specific teacher to a student model is a specific type of distillation above and where the term originated. It is a specific deep learning method where a neural network loss is modified to learn from the log-probabilities of the teacher model over multiple potential tokens/logits, instead of learning directly from a chosen output <span class="citation" data-cites="hinton2015distilling"><a href="ch021.xhtml#ref-hinton2015distilling">[51]</a></span>. An example of a modern series of models trained with Knowledge Distillation is Gemma 2 <span class="citation" data-cites="team2024gemma"><a href="ch021.xhtml#ref-team2024gemma">[52]</a></span> or Gemma 3. For a language modeling setup, the next-token loss function can be modified as follows <span class="citation" data-cites="agarwal2024policy"><a href="ch021.xhtml#ref-agarwal2024policy">[53]</a></span>, where the student model <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>P</mi><mi>Œ∏</mi></msub><annotation encoding="application/x-tex">P_\theta</annotation></semantics></math> learns from the teacher distribution <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>P</mi><mi>œï</mi></msub><annotation encoding="application/x-tex">P_\phi</annotation></semantics></math>:</li>
</ul>
<p><span id="eq:knowledge_distillation"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>‚Ñí</mi><mtext mathvariant="normal">KD</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>‚àí</mi><mspace width="0.167em"></mspace><msub><mi>ùîº</mi><mrow><mi>x</mi><mo>‚àº</mo><mi>ùíü</mi></mrow></msub><mrow><mo stretchy="true" form="prefix">[</mo><munderover><mo>‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><msub><mi>P</mi><mi>œï</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>‚à£</mo><msub><mi>x</mi><mrow><mo>&lt;</mo><mi>t</mi></mrow></msub><mo stretchy="false" form="postfix">)</mo><mi mathvariant="normal">log</mi><msub><mi>P</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>‚à£</mo><msub><mi>x</mi><mrow><mo>&lt;</mo><mi>t</mi></mrow></msub><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow><mi>.</mi><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>6</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}_{\text{KD}}(\theta) = -\,\mathbb{E}_{x \sim \mathcal{D}}\left[\sum_{t=1}^{T} P_{\phi}(x_t \mid x_{&lt;t}) \log P_{\theta}(x_t \mid x_{&lt;t})\right]. \qquad{(6)}</annotation></semantics></math></span></p>
<ul>
<li><strong>In-context Learning (ICL)</strong>: In-context here refers to any information within the context window of the language model. Usually, this is information added to the prompt. The simplest form of in-context learning is adding examples of a similar form before the prompt. Advanced versions can learn which information to include for a specific use-case.</li>
<li><strong>Chain of Thought (CoT)</strong>: Chain of thought is a specific behavior of language models where they are steered towards a behavior that breaks down a problem in a step-by-step form. The original version of this was through the prompt ‚ÄúLet‚Äôs think step-by-step‚Äù <span class="citation" data-cites="wei2022chain"><a href="ch021.xhtml#ref-wei2022chain">[54]</a></span>.</li>
</ul>
</section>
</section>
</body>
</html>
