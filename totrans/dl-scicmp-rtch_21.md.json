["```r\nlibrary(torch)\nlibrary(torchvision)\nlibrary(luz)\n\ndir <- \"~/.torch-datasets\"\n\nvalid_ds <- mnist_dataset(\n dir,\n download = TRUE,\n train = FALSE,\n transform = transform_to_tensor\n)\n\nvalid_dl <- dataloader(valid_ds, batch_size = 128)\n\n# a convenient way to obtain individual images without \n# manual iteration\ntest_images <- coro::collect(\n valid_dl, 1\n)[[1]]$x[1:32, 1, , ] %>% as.array()\n\npar(mfrow = c(4, 8), mar = rep(0, 4), mai = rep(0, 4))\ntest_images %>%\n purrr::array_tree(1) %>%\n purrr::map(as.raster) %>%\n purrr::iwalk(~ {\n plot(.x)\n })\n```", "```r\ntrain_ds <- mnist_dataset(\n dir,\n download = TRUE,\n transform = . %>%\n transform_to_tensor() %>%\n # flip horizontally, with a probability of 0.5\n transform_random_horizontal_flip(p = 0.5) %>%\n # flip vertically, with a probability of 0.5\n transform_random_vertical_flip(p = 0.5) %>%\n # (1) rotate to the left or the right,\n #     up to respective angles of 45 degrees\n # (2) translate vertically or horizontally,\n #     not exceeding 10% of total image width/height\n transform_random_affine(\n degrees = c(-45, 45),\n translate = c(0.1, 0.1)\n )\n)\n```", "```r\ntrain_dl <- dataloader(\n train_ds,\n batch_size = 128,\n shuffle = TRUE\n)\n\ntrain_images <- coro::collect(\n train_dl, 1\n)[[1]]$x[1:32, 1, , ] %>% as.array()\n\npar(mfrow = c(4, 8), mar = rep(0, 4), mai = rep(0, 4))\ntrain_images %>%\n purrr::array_tree(1) %>%\n purrr::map(as.raster) %>%\n purrr::iwalk(~ {\n plot(.x)\n })\n```", "```r\ntrain_ds <- mnist_dataset(\n dir,\n download = TRUE,\n transform = . %>%\n transform_to_tensor() %>%\n transform_random_affine(\n degrees = c(-45, 45), translate = c(0.1, 0.1)\n )\n)\n\ntrain_dl <- dataloader(train_ds,\n batch_size = 128,\n shuffle = TRUE\n)\n```", "```r\nconvnet <- nn_module(\n \"convnet\",\n initialize = function() {\n # nn_conv2d(in_channels, out_channels, kernel_size, stride)\n self$conv1 <- nn_conv2d(1, 32, 3, 1)\n self$conv2 <- nn_conv2d(32, 64, 3, 2)\n self$conv3 <- nn_conv2d(64, 128, 3, 1)\n self$conv4 <- nn_conv2d(128, 256, 3, 2)\n self$conv5 <- nn_conv2d(256, 10, 3, 2)\n },\n forward = function(x) {\n x %>%\n self$conv1() %>%\n nnf_relu() %>%\n self$conv2() %>%\n nnf_relu() %>%\n self$conv3() %>%\n nnf_relu() %>%\n self$conv4() %>%\n nnf_relu() %>%\n self$conv5() %>%\n torch_squeeze()\n }\n)\n\nfitted <- convnet %>%\n setup(\n loss = nn_cross_entropy_loss(),\n optimizer = optim_adam,\n metrics = list(\n luz_metric_accuracy()\n )\n ) %>%\n fit(train_dl, epochs = 5, valid_data = valid_dl)\n```", "```r\nfirst_batch <- coro::collect(valid_dl, 1)[[1]]\n\nmixed <- nnf_mixup(x = first_batch$x,\n y = first_batch$y,\n weight = torch_tensor(rep(0.9, 128)))\n```", "```r\nmixed <- nnf_mixup(x = first_batch$x,\n y = first_batch$y,\n weight = torch_tensor(rep(0.7, 128)))\n```", "```r\nmixed <- nnf_mixup(x = first_batch$x,\n y = first_batch$y,\n weight = torch_tensor(rep(0.5, 128)))\n```", "```r\n# redefine the training set to not use augmentation\ntrain_ds <- mnist_dataset(\n dir,\n download = TRUE,\n transform = transform_to_tensor\n)\n\ntrain_dl <- dataloader(train_ds,\n batch_size = 128,\n shuffle = TRUE\n)\n\nfitted <- convnet %>%\n setup(\n loss = nn_mixup_loss(torch::nn_cross_entropy_loss()),\n optimizer = optim_adam\n ) %>%\n fit(\n train_dl,\n epochs = 5,\n valid_data = valid_dl,\n callbacks = list(luz_callback_mixup())\n )\n```", "```r\nconvnet <- nn_module(\n \"convnet\",\n\n initialize = function() {\n # nn_conv2d(in_channels, out_channels, kernel_size, stride)\n self$conv1 <- nn_conv2d(1, 32, 3, 1)\n self$conv2 <- nn_conv2d(32, 64, 3, 2)\n self$conv3 <- nn_conv2d(64, 128, 3, 1)\n self$conv4 <- nn_conv2d(128, 256, 3, 2)\n self$conv5 <- nn_conv2d(256, 10, 3, 2)\n\n self$drop1 <- nn_dropout(p = 0.2)\n self$drop2 <- nn_dropout(p = 0.2)\n },\n forward = function(x) {\n x %>% \n self$conv1() %>% \n nnf_relu() %>%\n self$conv2() %>%\n nnf_relu() %>% \n self$drop1() %>%\n self$conv3() %>% \n nnf_relu() %>% \n self$conv4() %>% \n nnf_relu() %>% \n self$drop2() %>%\n self$conv5() %>%\n torch_squeeze()\n }\n)\n```", "```r\nfitted <- convnet %>%\n setup(\n loss = nn_cross_entropy_loss(),\n optimizer = optim_adam,\n metrics = list(luz_metric_accuracy())\n ) %>%\n set_opt_hparams(weight_decay = 0.00001) %>%\n fit(train_dl, epochs = 5, valid_data = valid_dl)\n```", "```r\nfitted <- convnet %>%\n setup(\n loss = nn_cross_entropy_loss(),\n optimizer = optim_adam,\n metrics = list(luz_metric_accuracy())\n ) %>%\n fit(train_dl,\n epochs = 5,\n valid_data = valid_dl,\n callbacks = list(\n luz_callback_early_stopping()\n )\n )\n```"]