- en: Synthetic Data & Distillation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement learning from *human feedback* is deeply rooted in the idea of
    keeping a human influence on the models we are building. When the first models
    were trained successfully with RLHF, human data was *the only* viable way to improve
    the models in this way.
  prefs: []
  type: TYPE_NORMAL
- en: Humans were the only way to create high enough quality responses to questions
    to train on them. Humans were the only way to collect reliable and specific feedback
    data to train reward models.
  prefs: []
  type: TYPE_NORMAL
- en: As AI models got better, this assumption rapidly broke down. The possibility
    of synthetic data, which is far cheaper and easier to iterate on, enabled the
    proliferation from RLHF being the center of attention to the idea of a broader
    “post-training” shaping the models. This chapter provides a cursory overview of
    how and why synthetic data is replacing or expanding many pieces of the RLHF pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: We begin by starting with a common criticism of synthetic data, in order to
    highlight what the data is actually extremely capable at. Many reports have been
    made on how synthetic data causes “model collapse” or other issues in models [[302]](ch021.xhtml#ref-shumailov2024ai),
    but this has been emphatically rebuked in leading language models [[303]](ch021.xhtml#ref-gerstgrasser2024model)
    [[304]](ch021.xhtml#ref-feng2024beyond). Synthetic data *can* cause models to
    have performance issues, but this is caused by using repetitive data or solely
    data outputted by the model being trained (narrowing its potential distribution)
    rather than well-rounded data sources.
  prefs: []
  type: TYPE_NORMAL
- en: The leading models **need synthetic data** to reach the best performance. Synthetic
    data in modern post-training encompasses many pieces of training – language models
    are used to generate new training prompts from seed examples [[305]](ch021.xhtml#ref-wang2022self),
    modify existing prompts, generate completions to prompts [[306]](ch021.xhtml#ref-numina_math_7b),
    provide AI feedback to create preference data [[23]](ch021.xhtml#ref-cui2023ultrafeedback),
    filter completions [[307]](ch021.xhtml#ref-li2024superfiltering), and much more.
    Synthetic data is key to post-training.
  prefs: []
  type: TYPE_NORMAL
- en: The ability for synthetic data to be impactful to this extent emerged with GPT-4
    class models. With early language models, such as Llama 2 and GPT-3.5-Turbo, the
    models were not reliable enough in generating or supervising data pipelines. Within
    1-2 years, language models were far superior to humans for generating answers.
    In the transition from GPT-3.5 to GPT-4 class models, the ability for models to
    perform LLM-as-a-judge tasks also emerged. GPT-4 or better models are far more
    robust and consistent in generating feedback or scores with respect to a piece
    of content.
  prefs: []
  type: TYPE_NORMAL
- en: Since this transition, the role of synthetic data has only grown in language
    model training. Otherwise, there are two clear areas where human data continues
    to be important.
  prefs: []
  type: TYPE_NORMAL
- en: The role of human data continues to be at the fringe of capabilities in models
    – humans must generate data where AI’s do not yet have any ability. Once the first
    strong model exists, synthetic data proliferates.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Human preference data is still used in the leading models, even though academic
    work shows synthetic versions to perform just as well. The role of human preferences
    is still being established in the literature.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The term distillation has been the most powerful form of discussion around the
    role of synthetic data in language models. Distillation as a term comes from a
    technical definition of teacher-student knowledge distillation from the deep learning
    literature [[51]](ch021.xhtml#ref-hinton2015distilling).
  prefs: []
  type: TYPE_NORMAL
- en: 'Distillation colloquially refers to using the outputs from a stronger model
    to train a smaller model. In post-training, this general notion of distillation
    takes two common forms:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a data engine to use across wide swaths of the post-training process: Completions
    for instructions, preference data (or Constitutional AI), or verification for
    RL.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To transfer specific skills from a stronger model to a weaker model, which is
    often done for specific skill such as mathematic reasoning or coding.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first strategy has grown in popularity as language models evolved to be
    more reliable than humans at writing answers to a variety of tasks. GPT-4 class
    models expanded the scope of this to use distillation of stronger models for complex
    tasks such as math and code (as mentioned above). Here, distillation motivates
    having a model suite where often a laboratory will train a large internal model,
    such as Claude Opus or Gemini Ultra, which is not released publicly and just used
    internally to make stronger models. With open models, common practice is to distill
    training data from closed API models into smaller, openly available weights [[21]](ch021.xhtml#ref-tunstall2023zephyr).
    Within this, curating high-quality prompts and filtering responses from the teacher
    model is crucial to maximize performance.
  prefs: []
  type: TYPE_NORMAL
- en: Transferring specific skills into smaller language models uses the same principles
    of distillation – get the best data possible for training. Here, many papers have
    studied using limited datasets from stronger models to improve alignment [[13]](ch021.xhtml#ref-zhou2023lima),
    mathematic reasoning [[308]](ch021.xhtml#ref-shridhar2023distilling) [[309]](ch021.xhtml#ref-hsieh2023distilling),
    and test-time scaling [[258]](ch021.xhtml#ref-muennighoff2025s1).
  prefs: []
  type: TYPE_NORMAL
