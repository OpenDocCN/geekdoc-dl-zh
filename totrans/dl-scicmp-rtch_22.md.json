["```r\nlibrary(torch)\nlibrary(torchvision)\nlibrary(luz)\n\nconvnet <- nn_module(\n \"convnet\",\n initialize = function() {\n # nn_conv2d(in_channels, out_channels, kernel_size, stride)\n self$conv1 <- nn_conv2d(1, 32, 3, 1)\n self$conv2 <- nn_conv2d(32, 64, 3, 2)\n self$conv3 <- nn_conv2d(64, 128, 3, 1)\n self$conv4 <- nn_conv2d(128, 256, 3, 2)\n self$conv5 <- nn_conv2d(256, 10, 3, 2)\n\n self$bn1 <- nn_batch_norm2d(32)\n self$bn2 <- nn_batch_norm2d(64)\n self$bn3 <- nn_batch_norm2d(128)\n self$bn4 <- nn_batch_norm2d(256)\n },\n forward = function(x) {\n x %>%\n self$conv1() %>%\n nn_relu() %>%\n self$bn1() %>%\n self$conv2() %>%\n nn_relu() %>%\n self$bn2() %>%\n self$conv3() %>%\n nn_relu() %>%\n self$bn3() %>%\n self$conv4() %>%\n nn_relu() %>%\n self$bn4() %>%\n self$conv5() %>%\n torch_squeeze()\n }\n)\n```", "```r\nlibrary(torch)\nlibrary(torchvision)\nlibrary(luz)\n\ndir <- \"~/.torch-datasets\" \n\ntrain_ds <- mnist_dataset(\n dir,\n download = TRUE,\n transform = . %>%\n transform_to_tensor() %>%\n transform_random_affine(\n degrees = c(-45, 45), translate = c(0.1, 0.1)\n )\n)\n\ntrain_dl <- dataloader(train_ds, batch_size = 128, shuffle = TRUE)\n\nvalid_ds <- mnist_dataset(\n dir,\n train = FALSE,\n transform = transform_to_tensor\n)\n\nvalid_dl <- dataloader(valid_ds, batch_size = 128)\n\nconvnet <- nn_module(\n \"convnet\",\n initialize = function() {\n # nn_conv2d(in_channels, out_channels, kernel_size, stride)\n self$conv1 <- nn_conv2d(1, 32, 3, 1)\n self$conv2 <- nn_conv2d(32, 64, 3, 2)\n self$conv3 <- nn_conv2d(64, 128, 3, 1)\n self$conv4 <- nn_conv2d(128, 256, 3, 2)\n self$conv5 <- nn_conv2d(256, 10, 3, 2)\n\n self$bn1 <- nn_batch_norm2d(32)\n self$bn2 <- nn_batch_norm2d(64)\n self$bn3 <- nn_batch_norm2d(128)\n self$bn4 <- nn_batch_norm2d(256)\n },\n forward = function(x) {\n x %>%\n self$conv1() %>%\n nnf_relu() %>%\n self$bn1() %>%\n self$conv2() %>%\n nnf_relu() %>%\n self$bn2() %>%\n self$conv3() %>%\n nnf_relu() %>%\n self$bn3() %>%\n self$conv4() %>%\n nnf_relu() %>%\n self$bn4() %>%\n self$conv5() %>%\n torch_squeeze()\n }\n)\n\nmodel <- convnet %>%\n setup(\n loss = nn_cross_entropy_loss(),\n optimizer = optim_adam,\n metrics = list(luz_metric_accuracy())\n )\n```", "```r\nrates_and_losses <- model %>%\n lr_finder(train_dl, start_lr = 0.0001, end_lr = 0.3)\n```", "```r\nrates_and_losses %>% plot()\n```", "```r\nnum_epochs <- 5 \n\n# the model has already been setup(), we continue from there\nfitted <- model %>%\n fit(train_dl,\n epochs = num_epochs,\n valid_data = valid_dl,\n callbacks = list(\n luz_callback_lr_scheduler(\n lr_one_cycle,\n max_lr = 0.01,\n epochs = num_epochs,\n steps_per_epoch = length(train_dl),\n call_on = \"on_batch_end\"\n )\n )\n )\n```", "```r\nresnet <- model_resnet18(pretrained = TRUE)\nresnet\n```", "```r\nAn `nn_module` containing 11,689,512 parameters.\n\n── Modules ───────────────────────────────────────\n• conv1: <nn_conv2d> #9,408 parameters\n• bn1: <nn_batch_norm2d> #128 parameters\n• relu: <nn_relu> #0 parameters\n• maxpool: <nn_max_pool2d> #0 parameters\n• layer1: <nn_sequential> #147,968 parameters\n• layer2: <nn_sequential> #525,568 parameters\n• layer3: <nn_sequential> #2,099,712 parameters\n• layer4: <nn_sequential> #8,393,728 parameters\n• avgpool: <nn_adaptive_avg_pool2d> #0 parameters\n• fc: <nn_linear> #513,000 parameters\n```", "```r\nresnet$fc\n```", "```r\nresnet$fc <- nn_linear(resnet$fc$in_features, 10)\nresnet$fc\n```", "```r\nAn `nn_module` containing 5,130 parameters.\n\n── Parameters ───────────────────────────────────────────────────────────────────────────────────────────────\n• weight: Float [1:10, 1:512]\n• bias: Float [1:10]\n```", "```r\nconvnet <- nn_module(\n initialize = function() {\n self$model <- model_resnet18(pretrained = TRUE)\n for (par in self$parameters) {\n par$requires_grad_(FALSE)\n }\n self$model$fc <- nn_linear(self$model$fc$in_features, 10)\n },\n forward = function(x) {\n self$model(x)\n }\n)\n```", "```r\ntrain_ds <- mnist_dataset(\n dir,\n download = TRUE,\n transform = . %>%\n transform_to_tensor() %>%\n (function(x) x$expand(c(3, 28, 28))) %>%\n transform_random_affine(\n degrees = c(-45, 45), translate = c(0.1, 0.1)\n )\n)\n\ntrain_dl <- dataloader(\n train_ds,\n batch_size = 128,\n shuffle = TRUE\n)\n\nvalid_ds <- mnist_dataset(\n dir,\n train = FALSE,\n transform = . %>%\n transform_to_tensor %>%\n (function(x) x$expand(c(3, 28, 28)))\n)\n\nvalid_dl <- dataloader(valid_ds, batch_size = 128)\n```", "```r\nmodel <- convnet %>%\n setup(\n loss = nn_cross_entropy_loss(),\n optimizer = optim_adam,\n metrics = list(luz_metric_accuracy())\n ) %>%\n fit(train_dl,\n epochs = 5,\n valid_data = valid_dl)\n```"]