- en: Key Related Works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RLHF and its related methods are very new. We highlight history to show how
    recently the procedures were formalized, and how much of this documentation is
    in the academic literature. With this, we want to emphasize that RLHF is very
    rapidly evolving, so the chapter sets the stage for a book that will express uncertainty
    over certain methods and an expectation that some details can change around a
    few, core practices. Otherwise, the papers and methods listed here showcase why
    many pieces of the RLHF pipeline are what they are, as some of the seminal papers
    were for applications totally distinct from modern language models.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we detail the key papers and projects that got the RLHF field
    to where it is today. This is not intended to be a comprehensive review of RLHF
    and the related fields, but rather a starting point and retelling of how we got
    to today. It is intentionally focused on recent work that led to ChatGPT. There
    is substantial further work in the RL literature on learning from preferences
    [[26]](ch021.xhtml#ref-wirth2017survey). For a more exhaustive list, you should
    use a proper survey paper [[27]](ch021.xhtml#ref-kaufmann2023survey),[[28]](ch021.xhtml#ref-casper2023open).
  prefs: []
  type: TYPE_NORMAL
- en: 'Origins to 2018: RL on Preferences'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The field has recently been popularized with the growth of Deep Reinforcement
    Learning and has grown into a broader study of the applications of LLMs from many
    large technology companies. Still, many of the techniques used today are deeply
    related to core techniques from early literature on RL from preferences.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the first papers with an approach similar to modern RLHF was *TAMER*.
    *TAMER: Training an Agent Manually via Evaluative Reinforcement* proposed an approach
    in which humans iteratively scored an agent’s actions to learn a reward model,
    which was used to learn the action policy [[29]](ch021.xhtml#ref-knox2008tamer).
    Other concurrent or soon after work proposed an actor-critic algorithm, COACH,
    where human feedback (both positive and negative) is used to tune the advantage
    function [[30]](ch021.xhtml#ref-macglashan2017interactive).'
  prefs: []
  type: TYPE_NORMAL
- en: The primary reference, Christiano et al. 2017, is an application of RLHF applied
    to preferences between trajectories of agents within Atari games [[1]](ch021.xhtml#ref-christiano2017deep).
    This work introducing RLHF followed soon after DeepMind’s seminal work in reinforcement
    learning on Deep Q-Networks (DQN), which showed that RL agents can solve popular
    video games learning from scratch. The work shows that humans choosing between
    trajectories can be more effective in some domains than directly interacting with
    the environment. This uses some clever conditions, but is impressive nonetheless.
    This method was expanded upon with more direct reward modeling [[31]](ch021.xhtml#ref-ibarz2018reward)
    and the adoption of deep learning within early RLHF work was capped by an extension
    to TAMER with neural network models just one year later [[32]](ch021.xhtml#ref-warnell2018deep).
  prefs: []
  type: TYPE_NORMAL
- en: This era began to transition, as reward models as a general notion were proposed
    as a method for studying alignment, rather than just a tool for solving RL problems
    [[33]](ch021.xhtml#ref-leike2018scalable).
  prefs: []
  type: TYPE_NORMAL
- en: '2019 to 2022: RL from Human Preferences on Language Models'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reinforcement learning from human feedback, also referred to regularly as reinforcement
    learning from human preferences in its early days, was quickly adopted by AI labs
    increasingly turning to scaling large language models. A large portion of this
    work began between GPT-2, in 2018, and GPT-3, in 2020\. The earliest work in 2019,
    *Fine-Tuning Language Models from Human Preferences* has many striking similarities
    to modern work on RLHF and the content that we will cover in this book [[34]](ch021.xhtml#ref-ziegler2019fine).
    Many canonical terms, such as learning reward models, KL distances, feedback diagrams,
    etc. were formalized in this paper – just the evaluation tasks for the final models,
    and capabilities, were different to what people are doing today. From here, RLHF
    was applied to a variety of tasks. Important examples include general summarization
    [[2]](ch021.xhtml#ref-stiennon2020learning), recursive summarization of books
    [[35]](ch021.xhtml#ref-wu2021recursively), instruction following (InstructGPT)
    [[3]](ch021.xhtml#ref-ouyang2022training), browser-assisted question-answering
    (WebGPT) [[4]](ch021.xhtml#ref-nakano2021webgpt), supporting answers with citations
    (GopherCite) [[36]](ch021.xhtml#ref-menick2022teaching), and general dialogue
    (Sparrow) [[37]](ch021.xhtml#ref-glaese2022improving).
  prefs: []
  type: TYPE_NORMAL
- en: 'Aside from applications, a number of seminal papers defined key areas for the
    future of RLHF, including those on:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Reward model over-optimization [[38]](ch021.xhtml#ref-gao2023scaling): The
    ability for RL optimizers to over-fit to models trained on preference data,'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Language models as a general area of study for alignment [[18]](ch021.xhtml#ref-askell2021general),
    and
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Red teaming [[39]](ch021.xhtml#ref-ganguli2022red) – the process of assessing
    the safety of a language model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Work continued on refining RLHF for application to chat models. Anthropic continued
    to use it extensively for early versions of Claude [[5]](ch021.xhtml#ref-bai2022training)
    and early RLHF open-source tools emerged [[40]](ch021.xhtml#ref-ramamurthy2022reinforcement),[[41]](ch021.xhtml#ref-havrilla-etal-2023-trlx),[[42]](ch021.xhtml#ref-vonwerra2022trl).
  prefs: []
  type: TYPE_NORMAL
- en: '2023 to Present: ChatGPT Era'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The announcement of ChatGPT was very clear about the role of RLHF in its training
    [[43]](ch021.xhtml#ref-openai2022chatgpt):'
  prefs: []
  type: TYPE_NORMAL
- en: We trained this model using Reinforcement Learning from Human Feedback (RLHF),
    using the same methods as InstructGPT, but with slight differences in the data
    collection setup.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Since then, RLHF has been used extensively in leading language models. It is
    well known to be used in Anthropic’s Constitutional AI for Claude [[19]](ch021.xhtml#ref-bai2022constitutional),
    Meta’s Llama 2 [[44]](ch021.xhtml#ref-touvron2023llama) and Llama 3 [[24]](ch021.xhtml#ref-dubey2024llama),
    Nvidia’s Nemotron [[25]](ch021.xhtml#ref-adler2024nemotron), Ai2’s Tülu 3 [[6]](ch021.xhtml#ref-lambert2024t),
    and more.
  prefs: []
  type: TYPE_NORMAL
- en: Today, RLHF is growing into a broader field of preference fine-tuning (PreFT),
    including new applications such as process reward for intermediate reasoning steps
    [[45]](ch021.xhtml#ref-lightman2023let), covered in Chapter 7; direct alignment
    algorithms inspired by Direct Preference Optimization (DPO) [[20]](ch021.xhtml#ref-rafailov2024direct),
    covered in Chapter 12; learning from execution feedback from code or math [[46]](ch021.xhtml#ref-kumar2024training),[[47]](ch021.xhtml#ref-singh2023beyond)
    and other online reasoning methods inspired by OpenAI’s o1 [[48]](ch021.xhtml#ref-openai2024o1),
    covered in Chapter 14.
  prefs: []
  type: TYPE_NORMAL
