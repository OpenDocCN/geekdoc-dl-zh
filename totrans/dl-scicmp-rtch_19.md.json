["```r\nlibrary(torch)\nlibrary(luz)\n\n# input dimensionality (number of input features)\nd_in <- 3\n# number of observations in training set\nn <- 1000\n\nx <- torch_randn(n, d_in)\ncoefs <- c(0.2, -1.3, -0.5)\ny <- x$matmul(coefs)$unsqueeze(2) + torch_randn(n, 1)\n\nds <- tensor_dataset(x, y)\n\ndl <- dataloader(ds, batch_size = 100, shuffle = TRUE)\n```", "```r\n# dimensionality of hidden layer\nd_hidden <- 32\n# output dimensionality (number of predicted features)\nd_out <- 1\n\nnet <- nn_module(\n initialize = function(d_in, d_hidden, d_out) {\n self$net <- nn_sequential(\n nn_linear(d_in, d_hidden),\n nn_relu(),\n nn_linear(d_hidden, d_out)\n )\n },\n forward = function(x) {\n self$net(x)\n }\n)\n```", "```r\nfitted <- net %>%\n setup(loss = nn_mse_loss(), optimizer = optim_adam) %>%\n set_hparams(\n d_in = d_in,\n d_hidden = d_hidden, d_out = d_out\n ) %>%\n fit(dl, epochs = 200)\n```", "```r\nEpoch 1/200\nTrain metrics: Loss: 3.0343                                                                               \nEpoch 2/200\nTrain metrics: Loss: 2.5387                                                                               \nEpoch 3/200\nTrain metrics: Loss: 2.2758                                                                               \n...\n...\nEpoch 198/200\nTrain metrics: Loss: 0.891                                                                                \nEpoch 199/200\nTrain metrics: Loss: 0.8879                                                                               \nEpoch 200/200\nTrain metrics: Loss: 0.9036 \n```", "```r\nfitted <- net %>%\n setup(loss = nn_mse_loss(), optimizer = optim_adam) %>%\n set_hparams(\n d_in = d_in,\n d_hidden = d_hidden, d_out = d_out\n ) %>%\n fit(ds, epochs = 200)\n```", "```r\nfitted <- net %>%\n setup(loss = nn_mse_loss(), optimizer = optim_adam) %>%\n set_hparams(\n d_in = d_in,\n d_hidden = d_hidden, d_out = d_out\n ) %>%\n fit(list(x, y), epochs = 200)\n```", "```r\nfitted <- net %>%\n setup(loss = nn_mse_loss(), optimizer = optim_adam) %>%\n set_hparams(\n d_in = d_in,\n d_hidden = d_hidden, d_out = d_out\n ) %>%\n fit(list(as.matrix(x), as.matrix(y)), epochs = 200)\n```", "```r\nfitted <- net %>%\n setup(\n loss = nn_mse_loss(),\n optimizer = optim_adam,\n metrics = list(luz_metric_mae())\n ) %>%\n fit(...)\n```", "```r\ntrain_ids <- sample(1:length(ds), size = 0.6 * length(ds))\nvalid_ids <- sample(\n setdiff(1:length(ds), train_ids),\n size = 0.2 * length(ds)\n)\ntest_ids <- setdiff(\n 1:length(ds),\n union(train_ids, valid_ids)\n)\n\ntrain_ds <- dataset_subset(ds, indices = train_ids)\nvalid_ds <- dataset_subset(ds, indices = valid_ids)\ntest_ds <- dataset_subset(ds, indices = test_ids)\n\ntrain_dl <- dataloader(train_ds,\n batch_size = 100, shuffle = TRUE\n)\nvalid_dl <- dataloader(valid_ds, batch_size = 100)\ntest_dl <- dataloader(test_ds, batch_size = 100)\n```", "```r\nfitted <- net %>%\n setup(\n loss = nn_mse_loss(),\n optimizer = optim_adam,\n metrics = list(luz_metric_mae())\n ) %>%\n set_hparams(\n d_in = d_in,\n d_hidden = d_hidden, d_out = d_out\n ) %>%\n fit(train_dl, epochs = 200, valid_data = valid_dl)\n```", "```r\nEpoch 1/200\nTrain metrics: Loss: 2.5863 - MAE: 1.2832                                       \nValid metrics: Loss: 2.487 - MAE: 1.2365\nEpoch 2/200\nTrain metrics: Loss: 2.4943 - MAE: 1.26                                          \nValid metrics: Loss: 2.4049 - MAE: 1.2161\nEpoch 3/200\nTrain metrics: Loss: 2.4036 - MAE: 1.236                                         \nValid metrics: Loss: 2.3261 - MAE: 1.1962\n...\n...\nEpoch 198/200\nTrain metrics: Loss: 0.8947 - MAE: 0.7504\nValid metrics: Loss: 1.0572 - MAE: 0.8287\nEpoch 199/200\nTrain metrics: Loss: 0.8948 - MAE: 0.7503\nValid metrics: Loss: 1.0569 - MAE: 0.8286\nEpoch 200/200\nTrain metrics: Loss: 0.8944 - MAE: 0.75\nValid metrics: Loss: 1.0579 - MAE: 0.8292\n```", "```r\nfitted %>% predict(test_dl)\n```", "```r\ntorch_tensor\n 0.7799\n 1.7839\n-1.1294\n-1.3002\n-1.8169\n-1.6762\n-0.7548\n-1.2041\n 2.9613\n-0.9551\n 0.7714\n-0.8265\n 1.1334\n-2.8406\n-1.1679\n 0.8350\n 2.0134\n 2.1083\n 1.4093\n 0.6962\n-0.3669\n-0.5292\n 2.0310\n-0.5814\n 2.7494\n 0.7855\n-0.5263\n-1.1257\n-3.3117\n 0.6157\n... [the output was truncated (use n=-1 to disable)]\n[ CPUFloatType{200,1} ]\n```", "```r\nfitted %>% evaluate(test_dl)\n```", "```r\nA `luz_module_evaluation`\n── Results \nloss: 0.9271\nmae: 0.7348\n```", "```r\nfitted <- net %>%\n setup(\n loss = nn_mse_loss(),\n optimizer = optim_adam,\n metrics = list(luz_metric_mae())\n ) %>%\n set_hparams(d_in = d_in,\n d_hidden = d_hidden,\n d_out = d_out) %>%\n fit(\n train_dl,\n epochs = 200,\n valid_data = valid_dl,\n callbacks = list(\n luz_callback_model_checkpoint(path = \"./models/\",\n save_best_only = TRUE),\n luz_callback_early_stopping(patience = 10)\n )\n )\n```", "```r\nEpoch 1/200\nTrain metrics: Loss: 2.5803 - MAE: 1.2547\nValid metrics: Loss: 3.3763 - MAE: 1.4232\nEpoch 2/200\nTrain metrics: Loss: 2.4767 - MAE: 1.229\nValid metrics: Loss: 3.2334 - MAE: 1.3909\n...\n...\nEpoch 110/200\nTrain metrics: Loss: 1.011 - MAE: 0.8034\nValid metrics: Loss: 1.1673 - MAE: 0.8578\nEpoch 111/200\nTrain metrics: Loss: 1.0108 - MAE: 0.8032\nValid metrics: Loss: 1.167 - MAE: 0.8578\nEarly stopping at epoch 111 of 200\n```", "```r\nfitted %>% predict(test_dl)\n```", "```r\n# input dimensionality (number of input features)\nd_in <- 3\n# number of observations in training set\nn <- 1000\n\nx <- torch_randn(n, d_in)\ncoefs <- c(0.2, -1.3, -0.5)\ny <- x$matmul(coefs)$unsqueeze(2) + torch_randn(n, 1)\n\nds <- tensor_dataset(x, y)\n\ndl <- dataloader(ds, batch_size = 100, shuffle = TRUE)\n\ntrain_ids <- sample(1:length(ds), size = 0.6 * length(ds))\nvalid_ids <- sample(setdiff(\n 1:length(ds),\n train_ids\n), size = 0.2 * length(ds))\ntest_ids <- setdiff(1:length(ds), union(train_ids, valid_ids))\n\ntrain_ds <- dataset_subset(ds, indices = train_ids)\nvalid_ds <- dataset_subset(ds, indices = valid_ids)\ntest_ds <- dataset_subset(ds, indices = test_ids)\n\ntrain_dl <- dataloader(train_ds,\n batch_size = 100,\n shuffle = TRUE\n)\nvalid_dl <- dataloader(valid_ds, batch_size = 100)\ntest_dl <- dataloader(test_ds, batch_size = 100)\n\n# dimensionality of hidden layer\nd_hidden <- 32\n# output dimensionality (number of predicted features)\nd_out <- 1\n\nnet <- nn_module(\n initialize = function(d_in, d_hidden, d_out) {\n self$net <- nn_sequential(\n nn_linear(d_in, d_hidden),\n nn_relu(),\n nn_linear(d_hidden, d_out)\n )\n },\n forward = function(x) {\n self$net(x)\n }\n)\n```", "```r\nfitted <- net %>%\n setup(\n loss = nn_mse_loss(),\n optimizer = optim_adam\n ) %>%\n set_hparams(\n d_in = d_in,\n d_hidden = d_hidden, d_out = d_out\n ) %>%\n fit(train_dl, epochs = 200, valid_data = valid_dl)\n```", "```r\ndevice <- torch_device(if\n(cuda_is_available()) {\n \"cuda\"\n} else {\n \"cpu\"\n})\n\nmodel <- net(d_in = d_in, d_hidden = d_hidden, d_out = d_out)\nmodel <- model$to(device = device)\n```", "```r\noptimizer <- optim_adam(model$parameters)\n```", "```r\ntrain_batch <- function(b) {\n optimizer$zero_grad()\n output <- model(b[[1]]$to(device = device))\n target <- b[[2]]$to(device = device)\n\n loss <- nn_mse_loss(output, target)\n loss$backward()\n optimizer$step()\n\n loss$item()\n}\n\nvalid_batch <- function(b) {\n output <- model(b[[1]]$to(device = device))\n target <- b[[2]]$to(device = device)\n\n loss <- nn_mse_loss(output, target)\n loss$item()\n}\n```", "```r\nnum_epochs <- 200\n\nfor (epoch in 1:num_epochs) {\n model$train()\n train_loss <- c()\n\n # use coro::loop() for stability and performance\n coro::loop(for (b in train_dl) {\n loss <- train_batch(b)\n train_loss <- c(train_loss, loss)\n })\n\n cat(sprintf(\n \"\\nEpoch %d, training: loss: %3.5f \\n\",\n epoch, mean(train_loss)\n ))\n\n model$eval()\n valid_loss <- c()\n\n # disable gradient tracking to reduce memory usage\n with_no_grad({ \n coro::loop(for (b in valid_dl) {\n loss <- valid_batch(b)\n valid_loss <- c(valid_loss, loss)\n }) \n })\n\n cat(sprintf(\n \"\\nEpoch %d, validation: loss: %3.5f \\n\",\n epoch, mean(valid_loss)\n ))\n}\n```"]