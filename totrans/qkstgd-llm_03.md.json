["```py\n# Importing the necessary modules for the script to run\nimport openai\nfrom openai.embeddings_utils import get_embeddings, get_embedding\n\n# Setting the OpenAI API key using the value stored in the environment variable 'OPENAI_API_KEY'\nopenai.api_key = os.environ.get('OPENAI_API_KEY')\n\n# Setting the engine to be used for text embedding\nENGINE = 'text-embedding-ada-002'\n\n# Generating the vector representation of the given text using the specified engine.\nembedded_text = get_embedding('I love to be vectorized', engine=ENGINE)\n\n# Checking the length of the resulting vector to ensure it is the expected size (1536)\nlen(embedded_text) == '1536'\n```", "```py\n# Importing the SentenceTransformer library\nfrom sentence_transformers import SentenceTransformer\n\n# Initializing a SentenceTransformer model with the 'multi-qa-mpnet-base-cos-v1' pre-trained model\nmodel = SentenceTransformer(\n  'sentence-transformers/multi-qa-mpnet-base-cos-v1')\n\n# Defining a list of documents to generate embeddings for\ndocs = [\n          \"Around 9 Million people live in London\",\n          \"London is known for its financial district\"\n       ]\n\n# Generate vector embeddings for the documents\ndoc_emb = model.encode(\n    docs,                   # our documents (an iterable of strings)\n    batch_size=32,          # batch the embeddings by this size\n    show_progress_bar=True  # display a progress bar\n\n)\n\n# The shape of the embeddings is (2, 768), indicating a length of 768 and two embeddings generated\ndoc_emb.shape  #  == (2, 768)\n```", "```py\n# Use the PyPDF2 library to read a PDF file\nimport PyPDF2\n\n# Open the PDF file in read-binary mode\nwith open('../data/pds2.pdf', 'rb') as file:\n\n    # Create a PDF reader object\n    reader = PyPDF2.PdfReader(file)\n\n    # Initialize an empty string to hold the text\n    principles_of_ds = ''\n\n    # Loop through each page in the PDF file\n    for page in tqdm(reader.pages):\n\n        # Extract the text from the page\n        text = page.extract_text()\n\n        # Find the starting point of the text we want to extract\n        # In this case, we are extracting text starting from the string ' ]'\n        principles_of_ds += '\\n\\n' + text[text.find(' ]')+2:]\n\n# Strip any leading or trailing whitespace from the resulting string\nprinciples_of_ds = principles_of_ds.strip()\n```", "```py\n# Function to split the text into chunks of a maximum number of tokens. Inspired by OpenAI\ndef overlapping_chunks(text, max_tokens = 500, overlapping_factor = 5):\n    '''\n    max_tokens: tokens we want per chunk\n    overlapping_factor: number of sentences to start each chunk with that overlaps with the previous chunk\n    '''\n\n    # Split the text using punctuation\n    sentences = re.split(r'[.?!]', text)\n\n    # Get the number of tokens for each sentence\n    n_tokens = [len(tokenizer.encode(\" \" + sentence)) for sentence in sentences]\n\n    chunks, tokens_so_far, chunk = [], 0, []\n\n    # Loop through the sentences and tokens joined together in a tuple\n    for sentence, token in zip(sentences, n_tokens):\n\n        # If the number of tokens so far plus the number of tokens in the current sentence is greater\n        # than the max number of tokens, then add the chunk to the list of chunks and reset\n        # the chunk and tokens so far\n        if tokens_so_far + token > max_tokens:\n            chunks.append(\". \".join(chunk) + \".\")\n            if overlapping_factor > 0:\n                chunk = chunk[-overlapping_factor:]\n                tokens_so_far = sum([len(tokenizer.encode(c)) for c in chunk])\n            else:\n                chunk = []\n                tokens_so_far = 0\n\n        # If the number of tokens in the current sentence is greater than the max number of\n        # tokens, go to the next sentence\n        if token > max_tokens:\n            continue\n\n        # Otherwise, add the sentence to the chunk and add the number of tokens to the total\n        chunk.append(sentence)\n        tokens_so_far += token + 1\n\n    return chunks\n\nsplit = overlapping_chunks(principles_of_ds, overlapping_factor=0)\navg_length = sum([len(tokenizer.encode(t)) for t in split]) / len(split)\nprint(f'non-overlapping chunking approach has {len(split)} documents with average length {avg_length:.1f} tokens')\n**non-overlapping chunking approach has 286 documents with average length 474.1 tokens**\n\n# with 5 overlapping sentences per chunk\nsplit = overlapping_chunks(principles_of_ds, overlapping_factor=5)\navg_length = sum([len(tokenizer.encode(t)) for t in split]) / len(split)\nprint(f'overlapping chunking approach has {len(split)} documents with average length {avg_length:.1f} tokens')\n**overlapping chunking approach has 391 documents with average length 485.4 tokens**\n```", "```py\n# Importing the Counter and re libraries\nfrom collections import Counter\nimport re\n\n# Find all occurrences of one or more spaces in 'principles_of_ds'\nmatches = re.findall(r'[\\s]{1,}', principles_of_ds)\n\n# The 5 most frequent spaces that occur in the document\nmost_common_spaces = Counter(matches).most_common(5)\n\n# Print the most common spaces and their frequencies\nprint(most_common_spaces)\n\n[(' ', 82259),\n ('\\n', 9220),\n ('  ', 1592),\n ('\\n\\n', 333),\n ('\\n   ', 250)]\n```", "```py\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\n# Assume you have a list of text embeddings called `embeddings`\n# First, compute the cosine similarity matrix between all pairs of embeddings\ncosine_sim_matrix = cosine_similarity(embeddings)\n\n# Instantiate the AgglomerativeClustering model\nagg_clustering = AgglomerativeClustering(\n    n_clusters=None,         # the algorithm will determine the optimal number of clusters based on the data\n    distance_threshold=0.1,  # clusters will be formed until all pairwise distances between clusters are greater than 0.1\n    affinity='precomputed',  # we are providing a precomputed distance matrix (1 - similarity matrix) as input\n    linkage='complete'       # form clusters by iteratively merging the smallest clusters based on the maximum distance between their components\n)\n\n# Fit the model to the cosine distance matrix (1 - similarity matrix)\nagg_clustering.fit(1 - cosine_sim_matrix)\n\n# Get the cluster labels for each embedding\ncluster_labels = agg_clustering.labels_\n\n# Print the number of embeddings in each cluster\nunique_labels, counts = np.unique(cluster_labels, return_counts=True)\nfor label, count in zip(unique_labels, counts):\n    print(f'Cluster {label}: {count} embeddings')\n\n**Cluster 0: 2 embeddings**\n**Cluster 1: 3 embeddings**\n**Cluster 2: 4 embeddings**\n...\n```", "```py\nimport hashlib\nimport os\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\nopenai.api_key = os.environ.get('OPENAI_API_KEY', '')\npinecone_key = os.environ.get('PINECONE_KEY', '')\n\n# Create an index in Pinecone with necessary properties\n\ndef my_hash(s):\n    # Return the MD5 hash of the input string as a hexadecimal string\n    return hashlib.md5(s.encode()).hexdigest()\n\nclass DocumentInputRequest(BaseModel):\n    # define input to /document/ingest\n\nclass DocumentInputResponse(BaseModel):\n    # define output from /document/ingest\n\nclass DocumentRetrieveRequest(BaseModel):\n    # define input to /document/retrieve\n\nclass DocumentRetrieveResponse(BaseModel):\n    # define output from /document/retrieve\n\n# API route to ingest documents\n@app.post(\"/document/ingest\", response_model=DocumentInputResponse)\nasync def document_ingest(request: DocumentInputRequest):\n    # Parse request data and chunk it\n    # Create embeddings and metadata for each chunk\n    # Upsert embeddings and metadata to Pinecone\n    # Return number of upserted chunks\n    return DocumentInputResponse(chunks_count=num_chunks)\n\n# API route to retrieve documents\n@app.post(\"/document/retrieve\", response_model=DocumentRetrieveResponse)\nasync def document_retrieve(request: DocumentRetrieveRequest):\n    # Parse request data and query Pinecone for matching embeddings\n    # Sort results based on re-ranking strategy, if any\n    # Return a list of document responses\n    return DocumentRetrieveResponse(documents=documents)\n\nif __name__ == \"__main__\":\n    uvicorn.run(\"api:app\", host=\"0.0.0.0\", port=8000, reload=True)\n```"]