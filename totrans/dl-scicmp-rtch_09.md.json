["```r\nlibrary(torch)\n\nx <- torch_randn(100, 3)\nx$size()\n```", "```r\n[1] 100   3\n```", "```r\nw <- torch_randn(3, 1, requires_grad = TRUE)\n```", "```r\nb <- torch_zeros(1, 1, requires_grad = TRUE)\n```", "```r\ny <- x$matmul(w) + b\nprint(y, n = 10)\n```", "```r\ntorch_tensor\n-2.1600\n-3.3244\n 0.6046\n 0.4472\n-0.4971\n-0.0530\n 5.1259\n-1.1595\n-0.5960\n-1.4584\n... [the output was truncated (use n=-1 to disable)]\n[ CPUFloatType{100,1} ][ grad_fn = <AddBackward0> ]\n```", "```r\nw1 <- torch_randn(3, 8, requires_grad = TRUE)\n```", "```r\nb1 <- torch_zeros(1, 8, requires_grad = TRUE)\n```", "```r\nw2 <- torch_randn(8, 1, requires_grad = TRUE)\n```", "```r\nb2 <- torch_randn(1, 1, requires_grad = TRUE)\n```", "```r\nt <- torch_tensor(c(-2, 1, 5, -7))\nt$relu()\n```", "```r\ntorch_tensor\n 0\n 1\n 5\n 0\n[ CPUFloatType{4} ]\n```", "```r\nt1 <- torch_tensor(c(1, 2, 3))\nt2 <- torch_tensor(c(1, -2, 3))\n\nt1$add(t2)$relu()\n```", "```r\ntorch_tensor\n 2\n 0\n 6\n[ CPUFloatType{3} ]\n```", "```r\nt1_clamped <- t1$relu()\nt2_clamped <- t2$relu()\n\nt1_clamped$add(t2_clamped)\n```", "```r\ntorch_tensor\n 2\n 2\n 6\n[ CPUFloatType{3} ]\n```", "```r\ny <- torch_randn(5)\ny_pred <- y + 0.01\n\nloss <- (y_pred - y)$pow(2)$mean()\n\nloss\n```", "```r\ntorch_tensor\n9.99999e-05\n[ CPUFloatType{} ]\n```", "```r\nlibrary(torch)\n\n# input dimensionality (number of input features)\nd_in <- 3\n# number of observations in training set\nn <- 100\n\nx <- torch_randn(n, d_in)\ncoefs <- c(0.2, -1.3, -0.5)\ny <- x$matmul(coefs)$unsqueeze(2) + torch_randn(n, 1)\n```", "```r\n# dimensionality of hidden layer\nd_hidden <- 32\n# output dimensionality (number of predicted features)\nd_out <- 1\n\n# weights connecting input to hidden layer\nw1 <- torch_randn(d_in, d_hidden, requires_grad = TRUE)\n# weights connecting hidden to output layer\nw2 <- torch_randn(d_hidden, d_out, requires_grad = TRUE)\n\n# hidden layer bias\nb1 <- torch_zeros(1, d_hidden, requires_grad = TRUE)\n# output layer bias\nb2 <- torch_zeros(1, d_out, requires_grad = TRUE)\n```", "```r\nlearning_rate <- 1e-4\n\n### training loop ----------------------------------------\n\nfor (t in 1:200) {\n\n ### -------- Forward pass --------\n\n y_pred <- x$mm(w1)$add(b1)$relu()$mm(w2)$add(b2)\n\n ### -------- Compute loss -------- \n loss <- (y_pred - y)$pow(2)$mean()\n if (t %% 10 == 0)\n cat(\"Epoch: \", t, \"   Loss: \", loss$item(), \"\\n\")\n\n ### -------- Backpropagation --------\n\n # compute gradient of loss w.r.t. all tensors with\n # requires_grad = TRUE\n loss$backward()\n\n ### -------- Update weights -------- \n\n # Wrap in with_no_grad() because this is a part we don't \n # want to record for automatic gradient computation\n with_no_grad({\n w1 <- w1$sub_(learning_rate * w1$grad)\n w2 <- w2$sub_(learning_rate * w2$grad)\n b1 <- b1$sub_(learning_rate * b1$grad)\n b2 <- b2$sub_(learning_rate * b2$grad) \n\n # Zero gradients after every pass, as they'd\n # accumulate otherwise\n w1$grad$zero_()\n w2$grad$zero_()\n b1$grad$zero_()\n b2$grad$zero_() \n })\n\n}\n```", "```r\nEpoch: 10 Loss: 24.92771\nEpoch: 20 Loss: 23.56143\nEpoch: 30 Loss: 22.3069\nEpoch: 40 Loss: 21.14102\nEpoch: 50 Loss: 20.05027\nEpoch: 60 Loss: 19.02925\nEpoch: 70 Loss: 18.07328\nEpoch: 80 Loss: 17.16819\nEpoch: 90 Loss: 16.31367\nEpoch: 100 Loss: 15.51261\nEpoch: 110 Loss: 14.76012\nEpoch: 120 Loss: 14.05348\nEpoch: 130 Loss: 13.38944\nEpoch: 140 Loss: 12.77219\nEpoch: 150 Loss: 12.19302\nEpoch: 160 Loss: 11.64823\nEpoch: 170 Loss: 11.13535\nEpoch: 180 Loss: 10.65219\nEpoch: 190 Loss: 10.19666\nEpoch: 200 Loss: 9.766989\n```"]