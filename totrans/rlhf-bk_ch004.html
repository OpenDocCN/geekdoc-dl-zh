<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <title>ch004.xhtml</title>
  <link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css" />
</head>
<body epub:type="bodymatter">
<section id="training-overview" class="level1">
<h1>Training Overview</h1>
<p>In this chapter we provide a cursory overview of RLHF training, before getting into the specifics later in the book. RLHF, while optimizing a simple loss function, involves training multiple, different AI models in sequence and then linking them together in a complex, online optimization.</p>
<p>Here, we introduce the core objective of RLHF, which is optimizing a proxy of reward of human preferences with a distance-based regularizer (along with showing how it relates to classical RL problems). Then we showcase canonical recipes which use RLHF to create leading models to show how RLHF fits in with the rest of post-training methods. These example recipes will serve as references for later in the book, where we describe different optimization choices you have when doing RLHF, and we will point back to how different key models used different steps in training.</p>
<section id="problem-formulation" class="level2">
<h2>Problem Formulation</h2>
<p>The optimization of reinforcement learning from human feedback (RLHF) builds on top of the standard RL setup. In RL, an agent takes actions <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>a</mi><mi>t</mi></msub><annotation encoding="application/x-tex">a_t</annotation></semantics></math> sampled from a policy <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>‚à£</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\pi(a_t\mid s_t)</annotation></semantics></math> given the state of the environment <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>s</mi><mi>t</mi></msub><annotation encoding="application/x-tex">s_t</annotation></semantics></math> to maximize reward <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">r(s_t,a_t)</annotation></semantics></math> <span class="citation" data-cites="sutton2018reinforcement"><a href="ch021.xhtml#ref-sutton2018reinforcement">[55]</a></span>. Traditionally, the environment evolves according to transition (dynamics) <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>‚à£</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">p(s_{t+1}\mid s_t, a_t)</annotation></semantics></math> with an initial state distribution <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>œÅ</mi><mn>0</mn></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\rho_0(s_0)</annotation></semantics></math>. Together, the policy and dynamics induce a trajectory distribution:</p>
<p><span id="eq:rl_dynam"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>œÄ</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>œÑ</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>œÅ</mi><mn>0</mn></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo stretchy="false" form="postfix">)</mo><munderover><mo>‚àè</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>T</mi><mo>‚àí</mo><mn>1</mn></mrow></munderover><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>‚à£</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mspace width="0.167em"></mspace><mi>p</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>‚à£</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mi>.</mi><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>7</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p_{\pi}(\tau)=\rho_0(s_0)\prod_{t=0}^{T-1}\pi(a_t\mid s_t)\,p(s_{t+1}\mid s_t,a_t).\qquad{(7)}</annotation></semantics></math></span></p>
<p>Across a finite episode with horizon <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math>, the goal of an RL agent is to solve the following optimization:</p>
<p><span id="eq:rl_opt"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>œÄ</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ùîº</mi><mrow><mi>œÑ</mi><mo>‚àº</mo><msub><mi>p</mi><mi>œÄ</mi></msub></mrow></msub><mrow><mo stretchy="true" form="prefix">[</mo><munderover><mo>‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>T</mi><mo>‚àí</mo><mn>1</mn></mrow></munderover><msup><mi>Œ≥</mi><mi>t</mi></msup><mi>r</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow><mo>,</mo><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>8</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">J(\pi) = \mathbb{E}_{\tau \sim p_{\pi}} \left[ \sum_{t=0}^{T-1} \gamma^t r(s_t, a_t) \right],\qquad{(8)}</annotation></semantics></math></span></p>
<p>For continuing tasks, one often takes <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo>‚Üí</mo><mi>‚àû</mi></mrow><annotation encoding="application/x-tex">T\to\infty</annotation></semantics></math> and relies on discounting (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ≥</mi><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\gamma&lt;1</annotation></semantics></math>) to keep the objective well-defined. <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ≥</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math> is a discount factor from 0 to 1 that balances the desirability of near- versus future-rewards. Multiple methods for optimizing this expression are discussed in Chapter 11.</p>
<figure id="fig:rl">
<img src="../media/file1.png" class="center" width="320" alt="Figure 2: Standard RL loop" />
<figcaption aria-hidden="true">Figure 2: Standard RL loop</figcaption>
</figure>
<p>A standard illustration of the RL loop is shown in fig.¬†<a href="#fig:rl">2</a> and (compare this to the RLHF loop in fig.¬†<a href="#fig:rlhf">3</a>).</p>
<section id="example-rl-task-cartpole" class="level3">
<h3>Example RL Task: CartPole</h3>
<p>To make the transition function concrete, consider the classic <em>CartPole</em> (inverted pendulum) control task.</p>
<ul>
<li><p><strong>State (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>s</mi><mi>t</mi></msub><annotation encoding="application/x-tex">s_t</annotation></semantics></math>)</strong>: the cart position/velocity and pole angle/angular velocity,</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mi>t</mi></msub><mo>=</mo><mo stretchy="false" form="prefix">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>,</mo><mspace width="0.167em"></mspace><msub><mover><mi>x</mi><mo accent="true">Ãá</mo></mover><mi>t</mi></msub><mo>,</mo><mspace width="0.167em"></mspace><msub><mi>Œ∏</mi><mi>t</mi></msub><mo>,</mo><mspace width="0.167em"></mspace><msub><mover><mi>Œ∏</mi><mo accent="true">Ãá</mo></mover><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mi>.</mi></mrow><annotation encoding="application/x-tex">s_t = (x_t,\,\dot{x}_t,\,\theta_t,\,\dot{\theta}_t).</annotation></semantics></math></p></li>
<li><p><strong>Action (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>a</mi><mi>t</mi></msub><annotation encoding="application/x-tex">a_t</annotation></semantics></math>)</strong>: apply a left/right horizontal force to the cart, e.g.¬†<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub><mo>‚àà</mo><mo stretchy="false" form="prefix">{</mo><mi>‚àí</mi><mi>F</mi><mo>,</mo><mi>+</mi><mi>F</mi><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">a_t \in \{-F, +F\}</annotation></semantics></math>.</p></li>
<li><p><strong>Reward (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math>)</strong>: a simple reward is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mi>t</mi></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">r_t = 1</annotation></semantics></math> each step the pole remains balanced and the cart stays on the track (e.g.¬†<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">|</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy="false" form="prefix">|</mo><mo>‚â§</mo><mn>2.4</mn></mrow><annotation encoding="application/x-tex">|x_t| \le 2.4</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">|</mo><msub><mi>Œ∏</mi><mi>t</mi></msub><mo stretchy="false" form="prefix">|</mo><mo>‚â§</mo><msup><mn>12</mn><mo>‚àò</mo></msup></mrow><annotation encoding="application/x-tex">|\theta_t| \le 12^\circ</annotation></semantics></math>), and the episode terminates when either bound is violated.</p></li>
<li><p><strong>Dynamics / transition (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>‚à£</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">p(s_{t+1}\mid s_t,a_t)</annotation></semantics></math>)</strong>: in many environments the dynamics are deterministic (so <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> is a point mass) and can be written as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>f</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">s_{t+1} = f(s_t,a_t)</annotation></semantics></math> via Euler integration with step size <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Œî</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">\Delta t</annotation></semantics></math>. A standard simplified CartPole update uses constants cart mass <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>m</mi><mi>c</mi></msub><annotation encoding="application/x-tex">m_c</annotation></semantics></math>, pole mass <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>m</mi><mi>p</mi></msub><annotation encoding="application/x-tex">m_p</annotation></semantics></math>, pole half-length <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>, and gravity <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math>:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">temp</mtext><mo>=</mo><mfrac><mrow><msub><mi>a</mi><mi>t</mi></msub><mo>+</mo><msub><mi>m</mi><mi>p</mi></msub><mi>l</mi><mspace width="0.167em"></mspace><msubsup><mover><mi>Œ∏</mi><mo accent="true">Ãá</mo></mover><mi>t</mi><mn>2</mn></msubsup><mrow><mi mathvariant="normal">sin</mi><mo>&#8289;</mo></mrow><msub><mi>Œ∏</mi><mi>t</mi></msub></mrow><mrow><msub><mi>m</mi><mi>c</mi></msub><mo>+</mo><msub><mi>m</mi><mi>p</mi></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">\text{temp} = \frac{a_t + m_p l\,\dot{\theta}_t^2\sin\theta_t}{m_c + m_p}</annotation></semantics></math></p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>Œ∏</mi><mo accent="true">Ãà</mo></mover><mi>t</mi></msub><mo>=</mo><mfrac><mrow><mi>g</mi><mrow><mi mathvariant="normal">sin</mi><mo>&#8289;</mo></mrow><msub><mi>Œ∏</mi><mi>t</mi></msub><mo>‚àí</mo><mrow><mi mathvariant="normal">cos</mi><mo>&#8289;</mo></mrow><msub><mi>Œ∏</mi><mi>t</mi></msub><mspace width="0.167em"></mspace><mtext mathvariant="normal">temp</mtext></mrow><mrow><mi>l</mi><mrow><mo stretchy="true" form="prefix">(</mo><mstyle displaystyle="false"><mfrac><mn>4</mn><mn>3</mn></mfrac></mstyle><mo>‚àí</mo><mfrac><mrow><msub><mi>m</mi><mi>p</mi></msub><msup><mrow><mi mathvariant="normal">cos</mi><mo>&#8289;</mo></mrow><mn>2</mn></msup><msub><mi>Œ∏</mi><mi>t</mi></msub></mrow><mrow><msub><mi>m</mi><mi>c</mi></msub><mo>+</mo><msub><mi>m</mi><mi>p</mi></msub></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac></mrow><annotation encoding="application/x-tex">\ddot{\theta}_t = \frac{g\sin\theta_t - \cos\theta_t\,\text{temp}}{l\left(\tfrac{4}{3} - \frac{m_p\cos^2\theta_t}{m_c + m_p}\right)}</annotation></semantics></math></p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>x</mi><mo accent="true">Ãà</mo></mover><mi>t</mi></msub><mo>=</mo><mtext mathvariant="normal">temp</mtext><mo>‚àí</mo><mfrac><mrow><msub><mi>m</mi><mi>p</mi></msub><mi>l</mi><mspace width="0.167em"></mspace><msub><mover><mi>Œ∏</mi><mo accent="true">Ãà</mo></mover><mi>t</mi></msub><mrow><mi mathvariant="normal">cos</mi><mo>&#8289;</mo></mrow><msub><mi>Œ∏</mi><mi>t</mi></msub></mrow><mrow><msub><mi>m</mi><mi>c</mi></msub><mo>+</mo><msub><mi>m</mi><mi>p</mi></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">\ddot{x}_t = \text{temp} - \frac{m_p l\,\ddot{\theta}_t\cos\theta_t}{m_c + m_p}</annotation></semantics></math></p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>x</mi><mi>t</mi></msub><mo>+</mo><mi mathvariant="normal">Œî</mi><mi>t</mi><mspace width="0.167em"></mspace><msub><mover><mi>x</mi><mo accent="true">Ãá</mo></mover><mi>t</mi></msub><mo>,</mo><mspace width="1.0em"></mspace><msub><mover><mi>x</mi><mo accent="true">Ãá</mo></mover><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mover><mi>x</mi><mo accent="true">Ãá</mo></mover><mi>t</mi></msub><mo>+</mo><mi mathvariant="normal">Œî</mi><mi>t</mi><mspace width="0.167em"></mspace><msub><mover><mi>x</mi><mo accent="true">Ãà</mo></mover><mi>t</mi></msub><mo>,</mo></mrow><annotation encoding="application/x-tex">x_{t+1}=x_t+\Delta t\,\dot{x}_t,\quad \dot{x}_{t+1}=\dot{x}_t+\Delta t\,\ddot{x}_t,</annotation></semantics></math> <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ∏</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>Œ∏</mi><mi>t</mi></msub><mo>+</mo><mi mathvariant="normal">Œî</mi><mi>t</mi><mspace width="0.167em"></mspace><msub><mover><mi>Œ∏</mi><mo accent="true">Ãá</mo></mover><mi>t</mi></msub><mo>,</mo><mspace width="1.0em"></mspace><msub><mover><mi>Œ∏</mi><mo accent="true">Ãá</mo></mover><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mover><mi>Œ∏</mi><mo accent="true">Ãá</mo></mover><mi>t</mi></msub><mo>+</mo><mi mathvariant="normal">Œî</mi><mi>t</mi><mspace width="0.167em"></mspace><msub><mover><mi>Œ∏</mi><mo accent="true">Ãà</mo></mover><mi>t</mi></msub><mi>.</mi></mrow><annotation encoding="application/x-tex">\theta_{t+1}=\theta_t+\Delta t\,\dot{\theta}_t,\quad \dot{\theta}_{t+1}=\dot{\theta}_t+\Delta t\,\ddot{\theta}_t.</annotation></semantics></math></p></li>
</ul>
<p>This is a concrete instance of the general setup above: the policy chooses <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>a</mi><mi>t</mi></msub><annotation encoding="application/x-tex">a_t</annotation></semantics></math>, the transition function advances the state, and the reward is accumulated over the episode.</p>
</section>
<section id="manipulating-the-standard-rl-setup" class="level3">
<h3>Manipulating the Standard RL Setup</h3>
<p>The RL formulation for RLHF is seen as a less open-ended problem, where a few key pieces of RL are set to specific definitions in order to accommodate language models. There are multiple core changes from the standard RL setup to that of RLHF: Table tbl.¬†<a href="ch004.xhtml#tbl:rl-vs-rlhf">1</a> summarizes these differences between standard RL and the RLHF setup used for language models.</p>
<ol type="1">
<li><strong>Switching from a reward function to a reward model.</strong> In RLHF, a learned model of human preferences, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">r_\theta(s_t, a_t)</annotation></semantics></math> (or any other classification model) is used instead of an environmental reward function. This gives the designer a substantial increase in the flexibility of the approach and control over the final results, but at the cost of implementation complexity. In standard RL, the reward is seen as a static piece of the environment that cannot be changed or manipulated by the person designing the learning agent.</li>
<li><strong>No state transitions exist.</strong> In RLHF, the initial states for the domain are prompts sampled from a training dataset and the ‚Äúaction‚Äù is the completion to said prompt. During standard practices, this action does not impact the next state and is only scored by the reward model.</li>
<li><strong>Response level rewards.</strong> Often referred to as a bandit problem, RLHF attribution of reward is done for an entire sequence of actions, composed of multiple generated tokens, rather than in a fine-grained manner.</li>
</ol>
<div class="table-wrap">
<table id="tbl:rl-vs-rlhf">
<caption>Table 1: Key differences between standard RL and RLHF for language models.</caption>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr>
<th>Aspect</th>
<th>Standard RL</th>
<th>RLHF (language models)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Reward signal</td>
<td>Environment reward function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">r(s_t,a_t)</annotation></semantics></math></td>
<td>Learned reward / preference model <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">r_\theta(x,y)</annotation></semantics></math> (prompt <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>, completion <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>)</td>
</tr>
<tr>
<td>State transition</td>
<td>Yes: dynamics <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>‚à£</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">p(s_{t+1}\mid s_t,a_t)</annotation></semantics></math></td>
<td>Typically no: prompts <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> sampled from a dataset; the completion does not define the next prompt</td>
</tr>
<tr>
<td>Action</td>
<td>Single environment action <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>a</mi><mi>t</mi></msub><annotation encoding="application/x-tex">a_t</annotation></semantics></math></td>
<td>A completion <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math> (a sequence of tokens) sampled from <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>‚ãÖ</mi><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\pi_\theta(\cdot\mid x)</annotation></semantics></math></td>
</tr>
<tr>
<td>Reward granularity</td>
<td>Often per-step / fine-grained</td>
<td>Usually response-level (bandit-style) over the full completion</td>
</tr>
<tr>
<td>Horizon</td>
<td>Multi-step episode (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo>&gt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">T&gt;1</annotation></semantics></math>)</td>
<td>Often single-step (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">T=1</annotation></semantics></math>), though multi-turn can be modeled as longer-horizon</td>
</tr>
</tbody>
</table>
</div>
<p>Given the single-turn nature of the problem, the optimization can be re-written without the time horizon and discount factor (and the reward models): <span id="eq:rl_opt_int"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>œÄ</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ùîº</mi><mrow><mi>œÑ</mi><mo>‚àº</mo><mi>œÄ</mi></mrow></msub><mrow><mo stretchy="true" form="prefix">[</mo><msub><mi>r</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow><mi>.</mi><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>9</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">J(\pi) = \mathbb{E}_{\tau \sim \pi} \left[r_\theta(s_t, a_t) \right].\qquad{(9)}</annotation></semantics></math></span></p>
<p>In many ways, the result is that while RLHF is heavily inspired by RL optimizers and problem formulations, the actual implementation is very distinct from traditional RL.</p>
<figure id="fig:rlhf">
<img src="../media/file2.png" alt="Figure 3: Standard RLHF loop" />
<figcaption aria-hidden="true">Figure 3: Standard RLHF loop</figcaption>
</figure>
</section>
<section id="finetuning-and-regularization" class="level3">
<h3>Finetuning and Regularization</h3>
<p>In traditional RL problems, the agent must learn from a randomly initialized policy, but with RLHF, we start from a strong pretrained base model with many initial capabilities. This strong prior for RLHF induces a need to control the optimization from drifting too far from the initial policy. In order to succeed in a finetuning regime, RLHF techniques employ multiple types of regularization to control the optimization. The goal is to allow the reward maximization to still occur without the model succumbing to over-optimization, as discussed in Chapter 18. The most common change to the optimization function is to add a distance penalty on the difference between the current RLHF policy and the starting point of the optimization:</p>
<p><span id="eq:rlhf_opt_eq"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>œÄ</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ùîº</mi><mrow><mi>œÑ</mi><mo>‚àº</mo><mi>œÄ</mi></mrow></msub><mrow><mo stretchy="true" form="prefix">[</mo><msub><mi>r</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow><mo>‚àí</mo><mi>Œ≤</mi><msub><mi>ùíü</mi><mtext mathvariant="normal">KL</mtext></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>œÄ</mi><mtext mathvariant="normal">RL</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>‚ãÖ</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">‚à•</mo><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>‚ãÖ</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">)</mo><mi>.</mi><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>10</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">J(\pi) = \mathbb{E}_{\tau \sim \pi} \left[r_\theta(s_t, a_t)\right] - \beta  \mathcal{D}_{\text{KL}}(\pi_{\text{RL}}(\cdot|s_t) \| \pi_{\text{ref}}(\cdot|s_t)).\qquad{(10)}</annotation></semantics></math></span></p>
<p>Within this formulation, a lot of study into RLHF training goes into understanding how to spend a certain ‚ÄúKL budget‚Äù as measured by a distance from the initial model. For more details, see Chapter 8 on Regularization.</p>
</section>
<section id="optimization-tools-1" class="level3">
<h3>Optimization Tools</h3>
<p>In this book, we detail many popular techniques for solving this optimization problem. The popular tools of post-training include:</p>
<ul>
<li><strong>Reward modeling</strong> (Chapter 7): Where a model is trained to capture the signal from collected preference data and can then output a scalar reward indicating the quality of future text.</li>
<li><strong>Instruction finetuning</strong> (Chapter 9): A prerequisite to RLHF where models are taught the question-answer format used in the majority of language modeling interactions today by imitating preselected examples.</li>
<li><strong>Rejection sampling</strong> (Chapter 10): The most basic RLHF technique where candidate completions for instruction finetuning are filtered by a reward model imitating human preferences.</li>
<li><strong>Policy gradients</strong> (Chapter 11): The reinforcement learning algorithms used in the seminal examples of RLHF to update parameters of a language model with respect to the signal from a reward model.</li>
<li><strong>Direct alignment algorithms</strong> (Chapter 12): Algorithms that directly optimize a policy from pairwise preference data, rather than learning an intermediate reward model to then optimize later.</li>
</ul>
<p>Modern RLHF-trained models always utilize instruction finetuning followed by a mixture of the other optimization options.</p>
</section>
</section>
<section id="canonical-training-recipes" class="level2">
<h2>Canonical Training Recipes</h2>
<p>Over time various models have been identified as canonical recipes for RLHF specifically or post-training generally. These recipes reflect data practices and model abilities at the time. As the recipes age, training models with the same characteristics becomes easier and takes fewer data. There is a general trend of post-training involving more optimization steps with more training algorithms across more diverse training datasets and evaluations.</p>
<section id="instructgpt" class="level3">
<h3>InstructGPT</h3>
<p>Around the time ChatGPT first came out, the widely accepted (‚Äúcanonical‚Äù) method for post-training an LM had three major steps, with RLHF being the central piece <span class="citation" data-cites="lambert2022illustrating"><a href="ch021.xhtml#ref-lambert2022illustrating">[56]</a></span> <span class="citation" data-cites="ouyang2022training"><a href="ch021.xhtml#ref-ouyang2022training">[3]</a></span> <span class="citation" data-cites="bai2022training"><a href="ch021.xhtml#ref-bai2022training">[5]</a></span>. The three steps taken on top of a ‚Äúbase‚Äù language model (the next-token prediction model trained on large-scale web text) are summarized below in fig.¬†<a href="#fig:rlhf-basic-repeat">4</a>:</p>
<ol type="1">
<li><strong>Instruction tuning on ~10K examples</strong>: This teaches the model to follow the question-answer format and teaches some basic skills from primarily human-written data.</li>
<li><strong>Training a reward model on ~100K pairwise prompts</strong>: This model is trained from the instruction-tuned checkpoint and captures the diverse values one wishes to model in their final training. The reward model is the optimization target for RLHF.</li>
<li><strong>Training the instruction-tuned model with RLHF on another ~100K prompts</strong>: The model is optimized against the reward model with a set of prompts that the model generates over before receiving ratings.</li>
</ol>
<p>Once RLHF was done, the model was ready to be deployed to users. This recipe is the foundation of modern RLHF, but recipes have evolved substantially to include more stages and more data.</p>
<figure id="fig:rlhf-basic-repeat">
<img src="../media/file0.png" alt="Figure 4: A rendition of the early, three stage RLHF process with SFT, a reward model, and then optimization." />
<figcaption aria-hidden="true">Figure 4: A rendition of the early, three stage RLHF process with SFT, a reward model, and then optimization.</figcaption>
</figure>
</section>
<section id="t√ºlu-3" class="level3">
<h3>T√ºlu 3</h3>
<p>Modern versions of post-training involve many, many more model versions and training stages (i.e.¬†well more than the 5 RLHF steps documented for Llama 2 <span class="citation" data-cites="touvron2023llama"><a href="ch021.xhtml#ref-touvron2023llama">[44]</a></span>). An example is shown below in fig.¬†<a href="#fig:rlhf-complex">5</a> where the model undergoes numerous training iterations before convergence.</p>
<figure id="fig:rlhf-complex">
<img src="../media/file3.png" alt="Figure 5: A rendition of modern post-training with many rounds." />
<figcaption aria-hidden="true">Figure 5: A rendition of modern post-training with many rounds.</figcaption>
</figure>
<p>The most complex models trained in this era and onwards have not released full details of their training process. Leading models such as ChatGPT or Claude circa 2025 involve many, iterative rounds of training. This can even include techniques that train specialized models and then merge the weights together to get a final model capable on many subtasks <span class="citation" data-cites="li2022branch"><a href="ch021.xhtml#ref-li2022branch">[57]</a></span> (e.g.¬†Cohere‚Äôs Command A <span class="citation" data-cites="cohere2025command"><a href="ch021.xhtml#ref-cohere2025command">[58]</a></span>).</p>
<figure id="fig:tulu-3">
<img src="../media/file4.png" alt="Figure 6: A summary of the T√ºlu 3 recipe with target skills and multi-step training recipe. Lambert et al.¬†2024, License CC-BY." />
<figcaption aria-hidden="true">Figure 6: A summary of the T√ºlu 3 recipe with target skills and multi-step training recipe. Lambert et al.¬†2024, License CC-BY.</figcaption>
</figure>
<p>A fully open example version of this multi-stage version of post-training where RLHF plays a major role is T√ºlu 3. The T√ºlu 3 recipe consists of three stages:</p>
<ol type="1">
<li><strong>Instruction tuning on ~1M examples</strong>: This primarily synthetic data from a mix of frontier models such as GPT-4o and Llama 3.1 405B teaches the model general instruction following and serves as the foundation of a variety of capabilities such as mathematics or coding.</li>
<li><strong>On-policy preference data on ~1M preference pairs</strong>: This stage substantially boosts the chattiness (e.g.¬†ChatBotArena or AlpacaEval 2) of the model while also improving skills mentioned above in the instruction tuning stage.</li>
<li><strong>Reinforcement Learning with Verifiable Rewards on ~10K prompts</strong>: This stage is a small scale reinforcement learning run to boost core skills such as mathematics while maintaining overall performance (and is now seen as a precursor to modern reasoning models such as DeepSeek R1).</li>
</ol>
<p>The recipe has been successfully applied to Llama 3.1 <span class="citation" data-cites="lambert2024t"><a href="ch021.xhtml#ref-lambert2024t">[6]</a></span>, OLMo 2 <span class="citation" data-cites="olmo20242"><a href="ch021.xhtml#ref-olmo20242">[59]</a></span>, and SmolLM models <span class="citation" data-cites="alrashed2024smoltulu"><a href="ch021.xhtml#ref-alrashed2024smoltulu">[60]</a></span>.</p>
</section>
<section id="deepseek-r1" class="level3">
<h3>DeepSeek R1</h3>
<p>With the rise of reasoning language models, such as OpenAI‚Äôs o1, the best practices in post-training evolved again to re-order and redistribute compute across training stages. The clearest documentation of a reasoning model post-training recipe is DeepSeek R1 <span class="citation" data-cites="guo2025deepseek"><a href="ch021.xhtml#ref-guo2025deepseek">[61]</a></span>, which has been mirrored by Alibaba‚Äôs larger Qwen 3 models (i.e.¬†only the 32B and 225B MoE models) <span class="citation" data-cites="yang2025qwen3"><a href="ch021.xhtml#ref-yang2025qwen3">[62]</a></span> or Xiaomi‚Äôs MiMo 7B <span class="citation" data-cites="xia2025mimo"><a href="ch021.xhtml#ref-xia2025mimo">[63]</a></span>. The DeepSeek recipe follows:</p>
<ol type="1">
<li><strong>‚ÄúCold-start‚Äù of 100K+ on-policy reasoning samples</strong>: This data is sampled from an earlier RL checkpoint, R1-Zero, and heavily filtered to instill a specific reasoning process on the model. DeepSeek uses the term cold-start to describe how RL is learned from little supervised data.</li>
<li><strong>Large-scale reinforcement learning training</strong>: This stage repeatedly covers reasoning problems with the model, running RLVR ‚Äúuntil convergence‚Äù on a variety of benchmarks.</li>
<li><strong>Rejection sampling</strong> on 3/4 reasoning problems and 1/4 general queries to start the transition to a general-purpose model.</li>
<li><strong>Mixed reinforcement learning training</strong> on reasoning problems (verifiable rewards) with general preference tuning reward models to polish the model.</li>
</ol>
<p>As above, there are evolutions of the recipe, particularly with steps 3 and 4 to finalize the model before exposing it to users. Many models start with tailored instruction datasets with Chain of Thought sequences that are heavily filtered and polished from existing models, providing a fast step to strong behaviors with SFT alone before moving onto RL <span class="citation" data-cites="seed2025seed"><a href="ch021.xhtml#ref-seed2025seed">[64]</a></span>.</p>
</section>
</section>
</section>
</body>
</html>
