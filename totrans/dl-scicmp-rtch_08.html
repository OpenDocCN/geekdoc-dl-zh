<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>5  Function minimization with autograd</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>5  Function minimization with autograd</h1>
<blockquote>原文：<a href="https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/optim_1.html">https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/optim_1.html</a></blockquote>

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">

</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>In the last two chapters, we’ve learned about tensors and automatic differentiation. In the upcoming two, we take a break from studying <code>torch</code> mechanics and, instead, find out what we’re able to do with what we already have. Using nothing but tensors, and supported by nothing but <em>autograd</em>, we can already do two things:</p>
<ul>
<li><p>minimize a function (i.e., perform numerical optimization), and</p></li>
<li><p>build and train a neural network.</p></li>
</ul>
<p>In this chapter, we start with minimization, and leave the network to the next one.</p>
<section id="an-optimization-classic" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="an-optimization-classic"><span class="header-section-number">5.1</span> An optimization classic</h2>
<p>In optimization research, the <em>Rosenbrock function</em> is a classic. It is a function of two variables; its minimum is at <code>(1,1)</code>. If you take a look at its contours, you see that the minimum lies inside a stretched-out, narrow valley (<a href="#fig-optim-1-rosenbrock">fig. <span>5.1</span></a>):</p>
<div id="fig-optim-1-rosenbrock" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="../Images/8b421d397f8209f540c9229aea5161dc.png" class="quarto-discovered-preview-image img-fluid figure-img" alt="Contour plot of a function in two variables, where the small function values lie inside a stretched-out, narrow valley." data-original-src="https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/images/optim-1-rosenbrock.png"/></p>
<p/><figcaption class="figure-caption">Figure 5.1: Rosenbrock function.</figcaption><p/>
</figure>
</div>
<p>Here is the function definition. <code>a</code> and <code>b</code> are parameters that can be freely chosen; the values we use here are a frequent choice.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"/>a <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"/>b <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"/></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"/>rosenbrock <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"/>  x1 <span class="ot">&lt;-</span> x[<span class="dv">1</span>]</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"/>  x2 <span class="ot">&lt;-</span> x[<span class="dv">2</span>]</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"/>  (a <span class="sc">-</span> x1)<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> b <span class="sc">*</span> (x2 <span class="sc">-</span> x1<span class="sc">^</span><span class="dv">2</span>)<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"/>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
</section>
<section id="minimization-from-scratch" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="minimization-from-scratch"><span class="header-section-number">5.2</span> Minimization from scratch</h2>
<p>The scenario is the following. We start at some given point <code>(x1,x2)</code>, and set out to find the location where the Rosenbrock function has its minimum.</p>
<p>We follow the strategy outlined in the previous chapter: compute the function’s gradient at our current position, and use it to go the opposite way. We don’t know how far to go; if we take too big a big step we may easily overshoot. (If you look back at the contour plot, you see that if you were standing at one of the steep cliffs east or west of the minimum, this could happen very fast.)</p>
<p>Thus, it is best to proceed iteratively, taking moderate steps and re-evaluating the gradient every time.</p>
<p>In a nutshell, the optimization procedure then looks somewhat like this:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"/><span class="fu">library</span>(torch)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"/></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"/><span class="co"># attention: this is not the correct procedure yet!</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"/></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"/><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>num_iterations) {</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"/></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"/>  <span class="co"># call function, passing in current parameter value</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"/>  value <span class="ot">&lt;-</span> <span class="fu">rosenbrock</span>(x)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"/></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"/>  <span class="co"># compute gradient of value w.r.t. parameter</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"/>  value<span class="sc">$</span><span class="fu">backward</span>()</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"/></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"/>  <span class="co"># manually update parameter, subtracting a fraction</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"/>  <span class="co"># of the gradient</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"/>  <span class="co"># this is not quite correct yet!</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"/>  x<span class="sc">$</span><span class="fu">sub_</span>(lr <span class="sc">*</span> x<span class="sc">$</span>grad)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"/>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>As written, this code snippet demonstrates our intentions, but it’s not quite correct (yet). It is also missing a few prerequisites: Neither the tensor <code>x</code> nor the variables <code>lr</code> and <code>num_iterations</code> have been defined. Let’s make sure we have those ready first. <code>lr</code>, for learning rate, is the fraction of the gradient to subtract on every step, and <code>num_iterations</code> is the number of steps to take. Both are a matter of experimentation.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"/>lr <span class="ot">&lt;-</span> <span class="fl">0.01</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"/></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"/>num_iterations <span class="ot">&lt;-</span> <span class="dv">1000</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p><code>x</code> is the parameter to optimize, that is, it is the function input that hopefully, at the end of the process, will yield the minimum possible function value. This makes it the tensor <em>with respect to which</em> we want to compute the function value’s derivative. And that, in turn, means we need to create it with <code>requires_grad = TRUE</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"/>x <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(<span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>), <span class="at">requires_grad =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>The starting point, <code>(-1,1)</code>, here has been chosen arbitrarily.</p>
<p>Now, all that remains to be done is apply a small fix to the optimization loop. With <em>autograd</em> enabled on <code>x</code>, <code>torch</code> will record all operations performed on that tensor, meaning that whenever we call <code>backward()</code>, it will compute all required derivatives. However, when we subtract a fraction of the gradient, this is not something we want a derivative to be calculated for! We need to tell <code>torch</code> not to record this action, and that we can do by wrapping it in <code>with_no_grad()</code>.</p>
<p>There’s one other thing we have to tell it. By default, <code>torch</code> accumulates the gradients stored in <code>grad</code> fields. We need to zero them out for every new calculation, using <code>grad$zero_()</code>.</p>
<p>Taking into account these considerations, the parameter update should look like this:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"/><span class="fu">with_no_grad</span>({</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"/>  x<span class="sc">$</span><span class="fu">sub_</span>(lr <span class="sc">*</span> x<span class="sc">$</span>grad)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"/>  x<span class="sc">$</span>grad<span class="sc">$</span><span class="fu">zero_</span>()</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"/>})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>Here is the complete code, enhanced with logging statements that make it easier to see what is going on.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"/>num_iterations <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"/></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"/>lr <span class="ot">&lt;-</span> <span class="fl">0.01</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"/></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"/>x <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(<span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>), <span class="at">requires_grad =</span> <span class="cn">TRUE</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"/></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"/><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>num_iterations) {</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"/>  <span class="cf">if</span> (i <span class="sc">%%</span> <span class="dv">100</span> <span class="sc">==</span> <span class="dv">0</span>) <span class="fu">cat</span>(<span class="st">"Iteration: "</span>, i, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"/></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"/>  value <span class="ot">&lt;-</span> <span class="fu">rosenbrock</span>(x)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"/>  <span class="cf">if</span> (i <span class="sc">%%</span> <span class="dv">100</span> <span class="sc">==</span> <span class="dv">0</span>) {</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"/>    <span class="fu">cat</span>(<span class="st">"Value is: "</span>, <span class="fu">as.numeric</span>(value), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"/>  }</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"/></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"/>  value<span class="sc">$</span><span class="fu">backward</span>()</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"/>  <span class="cf">if</span> (i <span class="sc">%%</span> <span class="dv">100</span> <span class="sc">==</span> <span class="dv">0</span>) {</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"/>    <span class="fu">cat</span>(<span class="st">"Gradient is: "</span>, <span class="fu">as.matrix</span>(x<span class="sc">$</span>grad), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"/>  }</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"/></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"/>  <span class="fu">with_no_grad</span>({</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"/>    x<span class="sc">$</span><span class="fu">sub_</span>(lr <span class="sc">*</span> x<span class="sc">$</span>grad)</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"/>    x<span class="sc">$</span>grad<span class="sc">$</span><span class="fu">zero_</span>()</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"/>  })</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"/>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<pre><code>Iteration:  100 
Value is:  0.3502924 
Gradient is:  -0.667685 -0.5771312 

Iteration:  200 
Value is:  0.07398106 
Gradient is:  -0.1603189 -0.2532476 

Iteration:  300 
Value is:  0.02483024 
Gradient is:  -0.07679074 -0.1373911 

Iteration:  400 
Value is:  0.009619333 
Gradient is:  -0.04347242 -0.08254051 

Iteration:  500 
Value is:  0.003990697 
Gradient is:  -0.02652063 -0.05206227 

Iteration:  600 
Value is:  0.001719962 
Gradient is:  -0.01683905 -0.03373682 

Iteration:  700 
Value is:  0.0007584976 
Gradient is:  -0.01095017 -0.02221584 

Iteration:  800 
Value is:  0.0003393509 
Gradient is:  -0.007221781 -0.01477957

Iteration:  900 
Value is:  0.0001532408 
Gradient is:  -0.004811743 -0.009894371 

Iteration:  1000 
Value is:  6.962555e-05 
Gradient is:  -0.003222887 -0.006653666 </code></pre>
<p>After thousand iterations, we have reached a function value lower than 0.0001. What is the corresponding <code>(x1,x2)</code>-position?</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"/>x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<pre><code>torch_tensor
 0.9918
 0.9830
[ CPUFloatType{2} ]</code></pre>
<p>This is rather close to the true minimum of <code>(1,1)</code>. If you feel like, play around a little, and try to find out what kind of difference the learning rate makes. For example, try 0.001 and 0.1, respectively.</p>
<p>In the next chapter, we will build a neural network from scratch. There, the function we minimize will be a <em>loss function</em>, namely, the mean squared error arising from a regression problem.</p>


</section>

    
</body>
</html>