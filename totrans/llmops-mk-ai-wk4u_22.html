<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>3.2 Workflow with Kubeflow Pipelines</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>3.2 Workflow with Kubeflow Pipelines</h1>
<blockquote>原文：<a href="https://boramorka.github.io/LLM-Book/en/CHAPTER-3/3.2%20Workflow%20with%20Kubeflow%20Pipelines/">https://boramorka.github.io/LLM-Book/en/CHAPTER-3/3.2%20Workflow%20with%20Kubeflow%20Pipelines/</a></blockquote>
                
                  


  
  



<blockquote>
<p><strong>Requirements:</strong> <code>pip install kfp&gt;=2.0.0 google-cloud-aiplatform</code></p>
</blockquote>
<p>Let's look at how to orchestrate and automate ML workflows with Kubeflow Pipelines — an open framework that makes it easier for data scientists, ML engineers, and developers to build, deploy, and operate complex chains of steps. Automation saves time and ensures reproducibility and stability — the foundation of reliable ML systems. We start by setting up the SDK and the “building blocks” of pipelines.</p>
<p>First, import the required modules from the Kubeflow Pipelines SDK. These modules are the building blocks for defining pipelines.</p>
<div class="highlight"><pre><span/><code><span class="c1"># Import the DSL (domain‑specific language) and the compiler from the Kubeflow Pipelines SDK</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">kfp</span><span class="w"> </span><span class="kn">import</span> <span class="n">dsl</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">kfp</span><span class="w"> </span><span class="kn">import</span> <span class="n">compiler</span>
</code></pre></div>
<p>Here, <code>dsl</code> provides decorators and classes for describing components and structure, and <code>compiler</code> compiles a pipeline to an executable format for the Kubeflow engine.</p>
<p>The libraries evolve quickly, so warnings about upcoming changes or deprecations are common. To keep output uncluttered during learning or demos, you can selectively hide them (but it’s wise to review release notes regularly):</p>
<div class="highlight"><pre><span/><code><span class="c1"># Suppress FutureWarning originating from the Kubeflow Pipelines SDK</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">"ignore"</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">FutureWarning</span><span class="p">,</span> <span class="n">module</span><span class="o">=</span><span class="s1">'kfp.*'</span><span class="p">)</span>
</code></pre></div>
<p>This uses the standard <code>warnings</code> module to filter <code>FutureWarning</code> from <code>kfp.*</code>, helping you focus on important messages.</p>
<p>Keep in mind: follow Kubeflow Pipelines releases and suppress warnings selectively — fully silencing them can hide real problems.</p>
<p>For details, keep the Kubeflow Pipelines docs and MLOps guides handy (for example, Google Cloud materials on continuous delivery and automated pipelines). Mastering them markedly improves the efficiency and reliability of ML workflows.</p>
<p>Kubeflow structures an ML workflow into reusable components and pipelines: components are isolated steps (preprocessing, training, deployment, etc.), and a pipeline is the composition in which outputs of one step become inputs to subsequent ones, forming an end‑to‑end process.</p>
<p>As a reference point, start with a simple “greeting” component that takes a name and returns a string. This is a basic demonstration of defining a component with the Kubeflow Pipelines SDK:</p>
<div class="highlight"><pre><span/><code><span class="c1"># Import the DSL module to define components and pipelines</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">kfp</span><span class="w"> </span><span class="kn">import</span> <span class="n">dsl</span>

<span class="c1"># Define a simple component using the @dsl.component decorator</span>
<span class="nd">@dsl</span><span class="o">.</span><span class="n">component</span>
<span class="k">def</span><span class="w"> </span><span class="nf">greet_person</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="c1"># Form a greeting by combining "Hello" with the input name</span>
    <span class="n">greeting_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">'Hello, </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1">!'</span>

    <span class="c1"># Return the constructed greeting message</span>
    <span class="k">return</span> <span class="n">greeting_message</span>
</code></pre></div>
<p>The <code>@dsl.component</code> decorator marks the function as a pipeline component; <code>greet_person</code> accepts <code>name</code> and forms a greeting you can pass downstream in a real pipeline.</p>
<p>Keep input/output interfaces clear, and design components so they can be reused across pipelines.</p>
<p>When working with components, understand outputs and <code>PipelineTask</code>: a function marked with <code>@dsl.component</code>, when called inside a pipeline, doesn’t return “ready” data. It returns a <code>PipelineTask</code> object representing the step execution and acting as the link for passing data further.</p>
<div class="highlight"><pre><span/><code><span class="c1"># Assign the result of calling the component function to a variable</span>
<span class="n">hello_task</span> <span class="o">=</span> <span class="n">greet_person</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">"Erwin"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">hello_task</span><span class="p">)</span>
</code></pre></div>
<p>The component returns a <code>PipelineTask</code>, not a string.</p>
<h4 id="accessing-data-via-output">Accessing data via <code>.output</code></h4>
<p>To use a component’s output inside a pipeline, refer to the <code>.output</code> attribute of the <code>PipelineTask</code> object. It lets you feed the result of one step into the next, organizing the pipeline’s dataflow.</p>
<div class="highlight"><pre><span/><code><span class="c1"># Access the component’s output via the .output attribute</span>
<span class="nb">print</span><span class="p">(</span><span class="n">hello_task</span><span class="o">.</span><span class="n">output</span><span class="p">)</span>
</code></pre></div>
<p>The <code>.output</code> attribute has a built‑in data type (String/Integer/Float/Boolean/List/Dict) compatible across pipeline components.</p>
<h4 id="named-arguments-only">Named arguments only</h4>
<p>Important: all component parameters are passed by name (keyword arguments). This increases clarity and prevents errors, especially when a component has multiple inputs.</p>
<div class="highlight"><pre><span/><code><span class="c1"># This will raise an error because it uses a positional argument</span>
<span class="c1"># hello_task = greet_person("Erwin")</span>

<span class="c1"># Correct: call with a named argument</span>
<span class="n">hello_task</span> <span class="o">=</span> <span class="n">greet_person</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">"Erwin"</span><span class="p">)</span>
</code></pre></div>
<p>Tips
- Parameter names: always call components with named arguments only.
- Component outputs: plan data hand‑off between steps via <code>PipelineTask.output</code>.</p>
<h3 id="wiring-components-passing-outputs">Wiring components: passing outputs</h3>
<p>Building on components, let’s create a pipeline where one component’s output serves as another’s input — a core capability of Kubeflow Pipelines.</p>
<h4 id="a-dependent-component">A dependent component</h4>
<p>Define a second component that accepts the first component’s greeting and appends a follow‑up question. This shows how one pipeline step can depend on the previous step’s result.</p>
<div class="highlight"><pre><span/><code><span class="c1"># Import DSL to define components</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">kfp</span><span class="w"> </span><span class="kn">import</span> <span class="n">dsl</span>

<span class="c1"># Define a component that depends on another component’s output</span>
<span class="nd">@dsl</span><span class="o">.</span><span class="n">component</span>
<span class="k">def</span><span class="w"> </span><span class="nf">ask_about_wellbeing</span><span class="p">(</span><span class="n">greeting_message</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="c1"># Form a new message that includes the greeting and a follow‑up question</span>
    <span class="n">follow_up_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">greeting_message</span><span class="si">}</span><span class="s2">. How are you?"</span>

    <span class="c1"># Return the new message</span>
    <span class="k">return</span> <span class="n">follow_up_message</span>
</code></pre></div>
<h4 id="passing-outputs-between-components">Passing outputs between components</h4>
<p>Now pass the output of the first component (<code>greet_person</code>) as the input to the second (<code>ask_about_wellbeing</code>). This is the key step in wiring components and organizing the pipeline’s dataflow.</p>
<div class="highlight"><pre><span/><code><span class="c1"># Create a task for the first component and keep its output</span>
<span class="n">greeting_task</span> <span class="o">=</span> <span class="n">greet_person</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">"Erwin"</span><span class="p">)</span>

<span class="c1"># Feed the first component’s output into the second component</span>
<span class="n">wellbeing_task</span> <span class="o">=</span> <span class="n">ask_about_wellbeing</span><span class="p">(</span><span class="n">greeting_message</span><span class="o">=</span><span class="n">greeting_task</span><span class="o">.</span><span class="n">output</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">wellbeing_task</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">wellbeing_task</span><span class="o">.</span><span class="n">output</span><span class="p">)</span>
</code></pre></div>
<p>Here, <code>greeting_task.output</code> is passed as <code>greeting_message</code> to the second component, demonstrating how data flows between pipeline steps.</p>
<h4 id="a-common-mistake-passing-pipelinetask-instead-of-output">A common mistake: passing <code>PipelineTask</code> instead of <code>.output</code></h4>
<p>When wiring components, be sure to pass the <code>PipelineTask.output</code> attribute — not the <code>PipelineTask</code> object itself. Passing the task object will fail because the component expects a built‑in data type, not a task object.</p>
<div class="highlight"><pre><span/><code><span class="c1"># Incorrect: passing a PipelineTask instead of its output — this will error</span>
<span class="c1"># wellbeing_task = ask_about_wellbeing(greeting_message=greeting_task)</span>

<span class="c1"># Correct: pass the task’s .output attribute</span>
<span class="n">wellbeing_task</span> <span class="o">=</span> <span class="n">ask_about_wellbeing</span><span class="p">(</span><span class="n">greeting_message</span><span class="o">=</span><span class="n">greeting_task</span><span class="o">.</span><span class="n">output</span><span class="p">)</span>
</code></pre></div>
<h4 id="practical-tips">Practical tips</h4>
<ul>
<li>Always pass <code>.output</code> for dependencies: when wiring components, make sure to pass the predecessor task’s <code>.output</code>.</li>
<li>Test components individually: validate each component before integrating, to catch issues early.</li>
</ul>
<p>Mastering component wiring in Kubeflow Pipelines lets you construct modular, readable, and flexible ML workflows. It also improves collaboration and encourages reuse across projects, accelerating development.</p>
<h3 id="building-and-understanding-pipelines-in-kubeflow">Building and understanding pipelines in Kubeflow</h3>
<p>Kubeflow Pipelines orchestrate complex workflows. A pipeline links multiple components, letting data flow from one to another to form an end‑to‑end process. Here’s how to define a simple pipeline using the components above.</p>
<h4 id="defining-a-pipeline">Defining a pipeline</h4>
<p>We’ll create a pipeline that chains <code>greet_person</code> and <code>ask_about_wellbeing</code>. It accepts a name, uses it to greet the person, then asks a follow‑up. This shows how to define a pipeline and handle component outputs correctly.</p>
<div class="highlight"><pre><span/><code><span class="c1"># Import DSL to define pipelines</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">kfp</span><span class="w"> </span><span class="kn">import</span> <span class="n">dsl</span>

<span class="c1"># Define a pipeline that orchestrates the greeting and follow‑up components</span>
<span class="nd">@dsl</span><span class="o">.</span><span class="n">pipeline</span>
<span class="k">def</span><span class="w"> </span><span class="nf">hello_and_wellbeing_pipeline</span><span class="p">(</span><span class="n">recipient_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="c1"># Task for the greet_person component</span>
    <span class="n">greeting_task</span> <span class="o">=</span> <span class="n">greet_person</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">recipient_name</span><span class="p">)</span>

    <span class="c1"># Task for ask_about_wellbeing, using greeting_task’s output</span>
    <span class="n">wellbeing_task</span> <span class="o">=</span> <span class="n">ask_about_wellbeing</span><span class="p">(</span><span class="n">greeting_message</span><span class="o">=</span><span class="n">greeting_task</span><span class="o">.</span><span class="n">output</span><span class="p">)</span>

    <span class="c1"># Return the final message produced by wellbeing_task</span>
    <span class="k">return</span> <span class="n">wellbeing_task</span><span class="o">.</span><span class="n">output</span>
</code></pre></div>
<p>The <code>recipient_name</code> parameter is passed to <code>greet_person</code>. Its output (<code>greeting_task.output</code>) becomes the input to <code>ask_about_wellbeing</code>. The pipeline returns <code>wellbeing_task.output</code>, illustrating dataflow through the pipeline.</p>
<h4 id="executing-and-handling-output">Executing and handling output</h4>
<p>When you “run” the pipeline definition in code, you might expect the final string directly (for example, "Hello, Erwin. How are you?"). But because of how Kubeflow Pipelines work, the pipeline function itself returns a <code>PipelineTask</code>, not raw output data.</p>
<div class="highlight"><pre><span/><code><span class="c1"># Run the pipeline with a recipient name</span>
<span class="n">pipeline_output</span> <span class="o">=</span> <span class="n">hello_and_wellbeing_pipeline</span><span class="p">(</span><span class="n">recipient_name</span><span class="o">=</span><span class="s2">"Erwin"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pipeline_output</span><span class="p">)</span>
</code></pre></div>
<p>This highlights a key point: a pipeline function describes a workflow; actual execution happens in the Kubeflow Pipelines environment, where data is passed between components and outputs are handled according to the pipeline graph.</p>
<h4 id="error-handling-wrong-return-types">Error handling: wrong return types</h4>
<p>If you try to return a <code>PipelineTask</code> itself rather than its <code>.output</code>, the pipeline will fail. The pipeline’s return must be the data type produced by the final component, matching expected outputs.</p>
<div class="highlight"><pre><span/><code><span class="c1"># Incorrect pipeline that returns a PipelineTask object</span>
<span class="nd">@dsl</span><span class="o">.</span><span class="n">pipeline</span>
<span class="k">def</span><span class="w"> </span><span class="nf">hello_and_wellbeing_pipeline_with_error</span><span class="p">(</span><span class="n">recipient_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="n">greeting_task</span> <span class="o">=</span> <span class="n">greet_person</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">recipient_name</span><span class="p">)</span>
    <span class="n">wellbeing_task</span> <span class="o">=</span> <span class="n">ask_about_wellbeing</span><span class="p">(</span><span class="n">greeting_message</span><span class="o">=</span><span class="n">greeting_task</span><span class="o">.</span><span class="n">output</span><span class="p">)</span>

    <span class="c1"># Incorrect: returning the PipelineTask itself</span>
    <span class="k">return</span> <span class="n">wellbeing_task</span>
    <span class="c1"># This will error</span>
</code></pre></div>
<h4 id="practical-tips_1">Practical tips</h4>
<ul>
<li>Return types: ensure the pipeline’s return type matches the data type produced by its final component. This is critical for correct execution and output handling.</li>
<li>Pipeline execution: calling the pipeline definition in a script or notebook prepares the workflow. Actual execution happens in Kubeflow Pipelines, where the infrastructure runs the pipeline.</li>
</ul>
<p>This example shows how to define a simple yet effective pipeline in Kubeflow. It underscores the importance of understanding component outputs, dataflow, and Kubeflow’s orchestration features. These concepts are foundational for building scalable, reliable ML workflows.</p>
<h3 id="implementing-and-running-a-kubeflow-pipeline">Implementing and Running a Kubeflow Pipeline</h3>
<p>Implementing a Kubeflow Pipeline involves key steps: define components, orchestrate them into a pipeline, compile the pipeline to an executable format, and finally run it in a suitable environment. We illustrate these using <code>hello_and_wellbeing_pipeline</code>.</p>
<h4 id="compile-the-pipeline">Compile the pipeline</h4>
<p>Kubeflow Pipelines use YAML for the executable specification. Compilation converts the Python definition into a static configuration describing the pipeline DAG, components, and dataflow.</p>
<div class="highlight"><pre><span/><code><span class="c1"># Import the compiler from the Kubeflow Pipelines SDK</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">kfp</span><span class="w"> </span><span class="kn">import</span> <span class="n">compiler</span>

<span class="c1"># Compile the pipeline to a YAML file</span>
<span class="n">compiler</span><span class="o">.</span><span class="n">Compiler</span><span class="p">()</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">hello_and_wellbeing_pipeline</span><span class="p">,</span> <span class="s1">'pipeline.yaml'</span><span class="p">)</span>
</code></pre></div>
<p>This generates <code>pipeline.yaml</code>, a compiled representation of the pipeline. That YAML is what you deploy to the runtime.</p>
<h4 id="inspect-the-compiled-pipeline">Inspect the compiled pipeline</h4>
<p>Viewing the YAML helps understand how the structure is captured. Optional but useful for learning and debugging.</p>
<div class="highlight"><pre><span/><code><span class="c1"># Inspect the compiled pipeline YAML (cross-platform Python approach)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="s1">'pipeline.yaml'</span><span class="p">)</span><span class="o">.</span><span class="n">read_text</span><span class="p">())</span>
</code></pre></div>
<p>Alternatively, from the command line:
</p><div class="highlight"><pre><span/><code>cat<span class="w"> </span>pipeline.yaml<span class="w">      </span><span class="c1"># Linux/macOS</span>
<span class="nb">type</span><span class="w"> </span>pipeline.yaml<span class="w">     </span><span class="c1"># Windows</span>
</code></pre></div>
<h4 id="run-the-pipeline">Run the pipeline</h4>
<p>Use Vertex AI Pipelines (a managed, serverless environment on Google Cloud) to run the compiled pipeline without managing infrastructure.</p>
<p>First, define pipeline arguments — inputs that parameterize runs:</p>
<div class="highlight"><pre><span/><code><span class="c1"># Define pipeline arguments</span>
<span class="n">pipeline_arguments</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"recipient_name"</span><span class="p">:</span> <span class="s2">"World!"</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
<p>Then use <code>google.cloud.aiplatform.PipelineJob</code> to configure and submit the run:</p>
<div class="highlight"><pre><span/><code><span class="kn">from</span><span class="w"> </span><span class="nn">google.cloud.aiplatform</span><span class="w"> </span><span class="kn">import</span> <span class="n">PipelineJob</span>

<span class="n">job</span> <span class="o">=</span> <span class="n">PipelineJob</span><span class="p">(</span>
    <span class="n">template_path</span><span class="o">=</span><span class="s2">"pipeline.yaml"</span><span class="p">,</span>
    <span class="n">display_name</span><span class="o">=</span><span class="s2">"hello_and_wellbeing_ai_pipeline"</span><span class="p">,</span>
    <span class="n">parameter_values</span><span class="o">=</span><span class="n">pipeline_arguments</span><span class="p">,</span>
    <span class="n">location</span><span class="o">=</span><span class="s2">"us-central1"</span><span class="p">,</span>
    <span class="n">pipeline_root</span><span class="o">=</span><span class="s2">"./"</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">job</span><span class="o">.</span><span class="n">submit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">job</span><span class="o">.</span><span class="n">state</span><span class="p">)</span>
</code></pre></div>
<p>Note: due to class/notebook constraints, we don’t execute this here. Run it in your own Google Cloud project.</p>
<h3 id="summary">Summary</h3>
<p>We covered implementing a Kubeflow Pipeline: defining components and a pipeline, compiling it to a deployable format, and running it in a managed environment. With these steps, you can automate and scale ML workflows effectively.</p>
<h3 id="automating-and-orchestrating-a-finetuning-pipeline-with-kubeflow">Automating and Orchestrating a Fine‑Tuning Pipeline with Kubeflow</h3>
<p>As a practical example, automate and orchestrate a parameter‑efficient fine‑tuning (PEFT) pipeline for Google’s PaLM 2 using Kubeflow Pipelines. Reusing existing pipelines significantly reduces development time and preserves best practices.</p>
<h4 id="reusing-existing-pipelines-for-efficiency">Reusing existing pipelines for efficiency</h4>
<p>Reusing a provided pipeline accelerates experimentation and deployment, especially with large models. Here we focus on Google’s PEFT pipeline for PaLM 2, which lets us fine‑tune a base model on our dataset without starting from scratch.</p>
<h4 id="data-preparation-and-model-versioning">Data preparation and model versioning</h4>
<p>Use two JSONL files for training and evaluation. Removing timestamps ensures consistency across collaborators.</p>
<div class="highlight"><pre><span/><code><span class="n">TRAINING_DATA_URI</span> <span class="o">=</span> <span class="s2">"./tune_data_stack_overflow_python_qa.jsonl"</span>
<span class="n">EVALUATION_DATA_URI</span> <span class="o">=</span> <span class="s2">"./tune_eval_data_stack_overflow_python_qa.jsonl"</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">datetime</span>
<span class="n">date</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s2">"%H:</span><span class="si">%d</span><span class="s2">:%m:%Y"</span><span class="p">)</span>
<span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"deep-learning-ai-model-</span><span class="si">{</span><span class="n">date</span><span class="si">}</span><span class="s2">"</span>
</code></pre></div>
<p>Set core hyperparameters:</p>
<div class="highlight"><pre><span/><code><span class="n">TRAINING_STEPS</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">EVALUATION_INTERVAL</span> <span class="o">=</span> <span class="mi">20</span>
</code></pre></div>
<p>Authenticate and set project context (example helper):</p>
<div class="highlight"><pre><span/><code><span class="kn">from</span><span class="w"> </span><span class="nn">utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">authenticate</span>
<span class="n">credentials</span><span class="p">,</span> <span class="n">PROJECT_ID</span> <span class="o">=</span> <span class="n">authenticate</span><span class="p">()</span>
<span class="n">REGION</span> <span class="o">=</span> <span class="s2">"us-central1"</span>
</code></pre></div>
<p>Define pipeline arguments:</p>
<div class="highlight"><pre><span/><code><span class="n">pipeline_arguments</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"model_display_name"</span><span class="p">:</span> <span class="n">MODEL_NAME</span><span class="p">,</span>
    <span class="s2">"location"</span><span class="p">:</span> <span class="n">REGION</span><span class="p">,</span>
    <span class="s2">"large_model_reference"</span><span class="p">:</span> <span class="s2">"text-bison@001"</span><span class="p">,</span>
    <span class="s2">"project"</span><span class="p">:</span> <span class="n">PROJECT_ID</span><span class="p">,</span>
    <span class="s2">"train_steps"</span><span class="p">:</span> <span class="n">TRAINING_STEPS</span><span class="p">,</span>
    <span class="s2">"dataset_uri"</span><span class="p">:</span> <span class="n">TRAINING_DATA_URI</span><span class="p">,</span>
    <span class="s2">"evaluation_interval"</span><span class="p">:</span> <span class="n">EVALUATION_INTERVAL</span><span class="p">,</span>
    <span class="s2">"evaluation_data_uri"</span><span class="p">:</span> <span class="n">EVALUATION_DATA_URI</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
<p>Submit the job via <code>PipelineJob</code> (enable caching to reuse unchanged step outputs):</p>
<div class="highlight"><pre><span/><code><span class="kn">from</span><span class="w"> </span><span class="nn">google.cloud.aiplatform</span><span class="w"> </span><span class="kn">import</span> <span class="n">PipelineJob</span>

<span class="n">pipeline_root</span> <span class="o">=</span> <span class="s2">"./"</span>

<span class="n">job</span> <span class="o">=</span> <span class="n">PipelineJob</span><span class="p">(</span>
    <span class="n">template_path</span><span class="o">=</span><span class="n">template_path</span><span class="p">,</span>
    <span class="n">display_name</span><span class="o">=</span><span class="sa">f</span><span class="s2">"deep_learning_ai_pipeline-</span><span class="si">{</span><span class="n">date</span><span class="si">}</span><span class="s2">"</span><span class="p">,</span>
    <span class="n">parameter_values</span><span class="o">=</span><span class="n">pipeline_arguments</span><span class="p">,</span>
    <span class="n">location</span><span class="o">=</span><span class="n">REGION</span><span class="p">,</span>
    <span class="n">pipeline_root</span><span class="o">=</span><span class="n">pipeline_root</span><span class="p">,</span>
    <span class="n">enable_caching</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">job</span><span class="o">.</span><span class="n">submit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">job</span><span class="o">.</span><span class="n">state</span><span class="p">)</span>
</code></pre></div>
<h4 id="conclusion">Conclusion</h4>
<p>This example illustrates automating and orchestrating a fine‑tuning pipeline for a base model with Kubeflow Pipelines. By reusing an existing pipeline, specifying key parameters, and executing in a managed environment, you can efficiently fine‑tune large models like PaLM 2 on specific datasets. This approach accelerates development and embeds MLOps best practices such as versioning, reproducibility, and efficient resource use.</p>
<h2 id="theory-questions">Theory Questions</h2>
<ol>
<li>The role of Kubeflow Pipelines in automating ML workflows and ensuring reproducibility.</li>
<li>The functions of the <code>dsl</code> and <code>compiler</code> modules in the SDK.</li>
<li>How to manage <code>FutureWarning</code> while keeping logs readable without missing important changes.</li>
<li>Why clear interfaces and reuse improve modularity and efficiency.</li>
<li>The purpose of the <code>@dsl.component</code> decorator.</li>
<li>What the <code>PipelineTask</code> object represents when calling a component and why it’s useful.</li>
<li>How to pass one component’s output as another’s input.</li>
<li>Why components accept only named arguments.</li>
<li>How to wire components and the role of the <code>.output</code> attribute.</li>
<li>How a pipeline is defined and what to watch for to return the correct value.</li>
<li>Steps for compiling, inspecting, and running a pipeline, and the role of YAML.</li>
<li>How reusing pipelines (e.g., PEFT for PaLM 2) speeds work and preserves best practices.</li>
<li>Why to version data and models in MLOps; give an example of a version identifier.</li>
<li>How to specify pipeline arguments for model fine‑tuning.</li>
<li>Pros and cons of automating and orchestrating complex workflows in Kubeflow for large models.</li>
</ol>
<h2 id="practical-tasks">Practical Tasks</h2>
<ol>
<li>Import <code>dsl</code> and <code>compiler</code> from the Kubeflow SDK and suppress <code>FutureWarning</code> from <code>kfp.*</code>.</li>
<li>Define a component <code>add_numbers(a: int, b: int) -&gt; int</code> with <code>@dsl.component</code>.</li>
<li>Suppress <code>DeprecationWarning</code> from any modules (via <code>warnings</code>).</li>
<li>Create two components: one returns a number, the other doubles it; wire them in a pipeline.</li>
<li>Compile a simple pipeline to YAML using <code>compiler</code>.</li>
<li>Show how calling a component returns a <code>PipelineTask</code> and how to access <code>.output</code>.</li>
<li>Demonstrate the error from returning a <code>PipelineTask</code> from a pipeline function, then fix it with comments.</li>
<li>Write a JSON‑to‑JSON preprocessing script (filter/map) that mimics a preprocessing component.</li>
<li>Add a function for versioning: append current date/time to a base model name.</li>
<li>Provide arguments and submit the compiled YAML to a runtime (pseudo‑API).</li>
</ol>












                
                  
</body>
</html>