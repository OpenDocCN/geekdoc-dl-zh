<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <title>ch014.xhtml</title>
  <link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css" />
</head>
<body epub:type="bodymatter">
<section id="reasoning-training-inference-time-scaling" class="level1">
<h1>Reasoning Training &amp; Inference-Time Scaling</h1>
<p>Reasoning models and inference-time scaling – the underlying property of language models that they exploit so well – enabled a massive step in language model performance in the end of 2024, through 2025, and into the future. These models, trained with a large amount of reinforcement learning with verifiable rewards (RLVR) <span class="citation" data-cites="lambert2024t"><a href="ch021.xhtml#ref-lambert2024t">[6]</a></span>, still utilize large amounts of RLHF. In this chapter we review the path that led the AI community to a transformed appreciation for RL’s potential in language models, review the fundamentals of RLVR, highlight key works, and point to the future debates that will define the area in the next few years.</p>
<p>To start, at the 2016 edition of the Neural Information Processing Systems (NeurIPS) conference, Yann LeCun first introduced his now-famous cake metaphor for where learning happens in modern machine learning systems:</p>
<blockquote>
<p>If intelligence is a cake, the bulk of the cake is unsupervised learning, the icing on the cake is supervised learning, and the cherry on the cake is reinforcement learning (RL).</p>
</blockquote>
<p>This analogy is now largely complete with modern language models and recent changes to the post-training stack. In this analogy:</p>
<ul>
<li>Self-supervised learning on vast swaths of internet data makes up the majority of the cake (especially when viewed in compute spent in FLOPs),</li>
<li>The beginning of post-training in supervised finetuning (SFT) for instructions tunes the model to a narrower distribution (along with the help of chosen examples for RLHF), and</li>
<li>Finally “pure” reinforcement learning (RL) is the cherry on top.</li>
</ul>
<p>We learn just “a few bits” of information with RL in just a few training samples. This little bit of reasoning training emerged with <strong>reasoning models</strong> that use a combination of the post-training techniques discussed in this book to align preferences along with RL training on verifiable domains to dramatically increase capabilities such as reasoning, coding, and mathematics problem solving.</p>
<p>The training method for these models, Reinforcement Learning with Verifiable Rewards (RLVR) <span class="citation" data-cites="lambert2024t"><a href="ch021.xhtml#ref-lambert2024t">[6]</a></span>, proceeds very similarly to RLHF, but it makes the reward model optional in lieu of a scoring function that returns a positive reward when the answer is correct and 0 otherwise. The ideas behind RLVR are not new to the RL literature and there are many related ideas in the language modeling literature where the model learns from feedback on if the answer is correct.</p>
<p>Originally, RL with Verifiable Rewards (RLVR) was to be named RL with Ground Truth rewards (RLGT). However, RLVR is subtly different from learning solely from ground truth answers. In domains like mathematics, a single ground truth answer is available to verify solutions. In other domains, such as code generation or precise instruction following, answers can be verified with a checking function (e.g., a unit test), even when there are multiple correct solutions rather than just a single ground truth answer. The core to progress on RLVR is having a variety and depth of these verifiable problems, even if the exact solution isn’t known a priori.</p>
<figure id="fig:rlvr">
<img src="../media/file18.png" alt="Figure 20: RLVR in the form of an RL feedback loop. Instead of a reward model, we use a verification function." />
<figcaption aria-hidden="true">Figure 20: RLVR in the form of an RL feedback loop. Instead of a reward model, we use a verification function.</figcaption>
</figure>
<p>The first models to successfully deploy this type of training were OpenAI’s o1 <span class="citation" data-cites="openai2024o1"><a href="ch021.xhtml#ref-openai2024o1">[48]</a></span> and the open-weight model DeepSeek R1 <span class="citation" data-cites="guo2025deepseek"><a href="ch021.xhtml#ref-guo2025deepseek">[61]</a></span>. Soon after, the entire AI industry prioritized this training process and model style. The core change here is more of a reallocation of the stages of training and the priority of different behaviors rather than this type of RL setup being entirely new. Reasoning models brought an era where scaling RL training is expected.</p>
<p>As for the type of behavior these models accrue, consider the following example with DeepSeek V3 0325 versus their reasoning model, DeepSeek R1, on the query <code>Write me a short poem about a goldfish</code>. DeepSeek V3 is very to the point:</p>
<blockquote>
<p>Goldfish Dreams</p>
<p>In a bowl of glass so clear and round, A tiny goldfish swims without a sound. With scales that gleam like sunlight’s kiss, He drifts along in watery bliss.</p>
<p>A flick of fin, a flash so bright, Chasing bubbles in the light. Does he dream of oceans wide? Or is his little world enough—his pride?</p>
</blockquote>
<p>Meanwhile, DeepSeek R1’s experience is different. It first generates numerous thinking tokens to craft a response before responding to the user. The answer appears as:</p>
<blockquote>
<p><code>&lt;thinking&gt;</code> Okay, the user wants a short poem about a goldfish. Let me start by brainstorming some goldfish-related imagery. Goldfish are often in bowls, right? So maybe start with the setting. Words like “gilded bowl” or “crystal globe” could work.</p>
<p>Next, think about the movement of the goldfish. They swim smoothly, so words like “dart”, “glide”, or “swirl” come to mind. Maybe describe their fins as “silken” to emphasize delicacy.</p>
<p>Then, consider the environment around the goldfish. Sunlight through water creates shimmering effects. Words like “sunbeams dance” or “ripples spin” might capture that. Including elements like castles or pebbles adds a playful touch.</p>
<p>Themes? Maybe the contrast between the goldfish’s small world and its perception of freedom. The bowl as a universe, tiny but complete. Ending with a sense of mystery or silent observation could give depth. Need to keep it concise, maybe four stanzas with a rhyme scheme. Let me check the rhythm and flow. Avoid clichés but keep it accessible. Alright, let’s piece it together. <code>&lt;/thinking&gt;</code></p>
<p>Gilded Dreams in Liquid Glass</p>
<p>A gilded bowl, a crystal globe, Where tiny tides in silence flow— A flash of orange, a whispered robe, You dart through sunlit indigo.</p>
<p>Your world is small, yet boundless too— Each pebble, castle, weed unfurled. The surface bends the sky to blue, And ripples spin your liquid world.</p>
<p>You glide through myths we’ll never know, A silent monarch, veiled in mist. Do dreams drift where the bubbles go? Or memories in amethyst?</p>
<p>We watch you trace your endless sphere— A universe within a tear.</p>
</blockquote>
<p>The tokens in the <code>&lt;thinking&gt;</code> tags is the model’s reasoning. For more complex problems the reasoning stage can take thousands of tokens before producing an answer. So, long-context language models were a prerequisite to advanced reasoning behavior, but that is not the focus of this chapter.</p>
<p>The core intuition for <em>how this training works</em> is that for a given model, we repeat the following cycle:</p>
<ol type="1">
<li>Sample multiple answers to multiple questions,</li>
<li>Take gradient steps towards the answers that are correct, and</li>
<li>Repeat, revisiting the same data.</li>
</ol>
<p>Remarkably, this extremely simple approach (when done with a careful distribution of data and stable training infrastructure) helps the models learn by revisiting the same questions again and again. Even more remarkable is that the improvements on these training questions generalize to questions and (some) domains the models have never seen!</p>
<p>This simple approach allows the models to lightly search over behavior space and the RL algorithm increases the likelihood of behaviors that are correlated with correct answers.</p>
<section id="the-origins-of-new-reasoning-models" class="level2">
<h2>The Origins of New Reasoning Models</h2>
<p>Here we detail the high-level trends that led to the explosion of reasoning models in 2025.</p>
<section id="why-does-rl-work-now" class="level3">
<h3>Why Does RL Work Now?</h3>
<p>Despite many, many takes that “RL doesn’t work yet” <span class="citation" data-cites="irpan2018deep"><a href="ch021.xhtml#ref-irpan2018deep">[253]</a></span> or paper’s detailing deep reproducibility issues with RL <span class="citation" data-cites="henderson2018deep"><a href="ch021.xhtml#ref-henderson2018deep">[254]</a></span>, the field overcame it to find high-impact applications. The takeoff of RL-focused training on language models indicates steps in many fundamental issues for the research area, including:</p>
<ul>
<li><p><strong>Stability of RL can be solved</strong>: For its entire existence, the limiting factor on RL’s adoption has been stability. This manifests in two ways. First, the learning itself can be fickle and not always work. Second, the training itself is known to be more brittle than standard language model training and more prone to loss spikes, crashes, etc. Countless releases are using this style of RL training and substantial academic uptake has occurred. The technical barriers to entry on RL are at an all time low.</p></li>
<li><p><strong>Open-source versions already “exist”</strong>: Many tools already exist for training language models with RLVR and related techniques. Examples include TRL <span class="citation" data-cites="vonwerra2022trl"><a href="ch021.xhtml#ref-vonwerra2022trl">[42]</a></span>, Open Instruct <span class="citation" data-cites="lambert2024t"><a href="ch021.xhtml#ref-lambert2024t">[6]</a></span>, veRL <span class="citation" data-cites="sheng2024hybridflow"><a href="ch021.xhtml#ref-sheng2024hybridflow">[255]</a></span>, and OpenRLHF <span class="citation" data-cites="hu2024openrlhf"><a href="ch021.xhtml#ref-hu2024openrlhf">[256]</a></span>, where many of these are building on optimizations from earlier in the arc of RLHF and post-training. The accessibility of tooling is enabling a large uptake of research that’ll likely soon render this chapter out of date.</p></li>
</ul>
<p>Multiple resources point to RL training for reasoning only being viable on leading models coming out from about 2024 onwards, indicating that a certain level of underlying capability was needed in the models before reasoning training was possible.</p>
</section>
<section id="rl-training-vs.-inference-time-scaling" class="level3">
<h3>RL Training vs. Inference Time Scaling</h3>
<p>Training with Reinforcement Learning to elicit reasoning behaviors and performance on verifiable domains is closely linked to the ideas of inference time scaling. Inference-time scaling, also called test-time scaling, is the general class of methods that use more computational power at inference in order to perform better at downstream tasks. Methods for inference-time scaling were studied before the release of DeepSeek R1 and OpenAI’s o1, which both massively popularized investment in RL training specifically. Examples include value-guided sampling <span class="citation" data-cites="liu2023don"><a href="ch021.xhtml#ref-liu2023don">[257]</a></span> or repeated random sampling with answer extraction <span class="citation" data-cites="brown2024large"><a href="ch021.xhtml#ref-brown2024large">[238]</a></span>. Beyond this, inference-time scaling can be used to improve more methods of AI training beyond chain of thought reasoning to solve problems, such as with reward models that consider the options deeply <span class="citation" data-cites="ankner2024critique"><a href="ch021.xhtml#ref-ankner2024critique">[137]</a></span> <span class="citation" data-cites="liu2025inference"><a href="ch021.xhtml#ref-liu2025inference">[251]</a></span>.</p>
<p>RL training is a short path to inference time scaling laws being used, but in the long-term we will have more methods for eliciting the inference-time tradeoffs we need for best performance. Training models heavily with RL changes them so that they generate more tokens per response in a way that is strongly correlated with downstream performance. This is a substantial shift from the length-bias seen in early RLHF systems <span class="citation" data-cites="singhal2023long"><a href="ch021.xhtml#ref-singhal2023long">[9]</a></span>, where the human preference training had a side effect of increasing response rate for marginal gains on preference rankings.</p>
<p>Downstream of the RL trained models there are many methods being explored to continue to push the limits of reasoning and inference-time compute. These are largely out of the scope of this book due to their rapidly evolving nature, but they include distilling reasoning behavior from a larger RL trained model to a smaller model via instruction tuning <span class="citation" data-cites="muennighoff2025s1"><a href="ch021.xhtml#ref-muennighoff2025s1">[258]</a></span>, composing more inference calls <span class="citation" data-cites="chen2024more"><a href="ch021.xhtml#ref-chen2024more">[259]</a></span>, and more. What is important here is the correlation between downstream performance and an increase in the number of tokens generated – otherwise it is just wasted energy.</p>
</section>
<section id="the-future-beyond-reasoning-of-rlvr" class="level3">
<h3>The Future (Beyond Reasoning) of RLVR</h3>
<p>In many domains, these new flavors of RLVR are much more aligned with the goals of developers by being focused on performance rather than behavior. Standard finetuning APIs generally use a parameter-efficient finetuning method such as LoRA with supervised finetuning on instructions. Developers pass in prompts and completions and the model is tuned to match that by updating model parameters to match the completions, which increases the prevalence of features from your data in the model’s generations.</p>
<p>RLVR is focused on matching answers. Given queries and correct answers, RLVR helps the model learn to get the correct answers. While standard instruction tuning is done with 1 or 2 epochs of loss updates over the data, RLVR gets its name by doing hundreds or thousands of epochs over the same few data points to give the model time to learn new behaviors. This can be viewed as reinforcing positive behaviors that would work sparingly in the base model version into robust behaviors after RLVR.</p>
<p><strong>The scope of RL training for language models continues to grow</strong>: The biggest takeaway from o1 and R1 on a fundamental scientific level was that we have even more ways to train language models to potentially valuable behaviors. The more open doors that are available to researchers and engineers, the more optimism we should have about AI’s general trajectory.</p>
</section>
</section>
<section id="understanding-reasoning-training-methods" class="level2">
<h2>Understanding Reasoning Training Methods</h2>
<p>The investment in reasoning has instigated a major evolution in the art of how models are trained to follow human instructions. These recipes still use the common pieces discussed in earlier chapters, including instruction finetuning, reinforcement learning from human feedback, and reinforcement learning with verifiable rewards (RLVR). The core change is using far more RLVR and applying the other training techniques in different orders – traditionally for a reasoning model the core training step is either a large-scale RL run or a large-scale instruction tuning run on <em>outputs</em> of another model that had undergone a substantial portion of RLVR training (referred to as distillation).</p>
<section id="reasoning-research-pre-openais-o1-or-deepseek-r1" class="level3">
<h3>Reasoning Research Pre OpenAI’s o1 or DeepSeek R1</h3>
<p>Before the takeoff of reasoning models, a substantial effort was made understanding how to train language models to be better at verifiable domains. The main difference between these works below is that their methodologies did not scale up to the same factor as those used in DeepSeek R1 and subsequent models, or they resulted in models that made sacrifices in overall performance in exchange for higher mathematics or coding abilities. The underlying ideas and motivations are included to paint a broader picture for how reasoning models emerged within the landscape.</p>
<p>Some of the earliest efforts training language models on verifiable domains include self-taught reasoner (STaR) line of work<span class="citation" data-cites="zelikman2022star"><a href="ch021.xhtml#ref-zelikman2022star">[260]</a></span> <span class="citation" data-cites="Zelikman2024QuietSTaRLM"><a href="ch021.xhtml#ref-Zelikman2024QuietSTaRLM">[261]</a></span> and TRICE <span class="citation" data-cites="hoffman2023training"><a href="ch021.xhtml#ref-hoffman2023training">[262]</a></span>, which both used ground-truth reward signals to encourage chain of thought reasoning in models throughout 2022 and 2023. STaR effectively approximates the policy gradient algorithm, but in practice filters samples differently and uses a cross-entropy measure instead of a log-probability, and Quiet-STaR expands on this with very related ideas of recent reasoning models by having the model generate tokens before trying to answer the verifiable question (which helps with training performance). TRICE <span class="citation" data-cites="hoffman2023training"><a href="ch021.xhtml#ref-hoffman2023training">[262]</a></span> also improves upon reasoning by generating traces and then optimizing with a custom Markov chain Monte Carlo inspired expectation maximization algorithm. VinePPO <span class="citation" data-cites="VinePPO"><a href="ch021.xhtml#ref-VinePPO">[263]</a></span> followed these and used a setup that shifted closer to modern reasoning models. VinePPO uses binary rewards math questions (GSM8K and MATH training sets in the paper) correctness with a PPO-based algorithm. Other work before OpenAI’s o1 and DeepSeek R1 used code execution as a feedback signal for training <span class="citation" data-cites="gehring2024rlefgroundingcodellms"><a href="ch021.xhtml#ref-gehring2024rlefgroundingcodellms">[264]</a></span>, <span class="citation" data-cites="xudpoppo"><a href="ch021.xhtml#ref-xudpoppo">[265]</a></span> or verification for theorem proving (called Reinforcement Learning from Verifier Feedback, RLVF, here) <span class="citation" data-cites="amit2024models"><a href="ch021.xhtml#ref-amit2024models">[266]</a></span>. Tülu 3 expanded upon these methods by using a simple PPO trainer to reward completions with correct answers – most importantly while maintaining the model’s overall performance on a broad suite of evaluations. The binary rewards of Tülu 3 and modern reasoning training techniques can be contrasted to the iterative approach of STaR or the log-likelihood rewards of Quiet-STaR.</p>
</section>
<section id="early-reasoning-models" class="level3">
<h3>Early Reasoning Models</h3>
<p>A summary of the foundational reasoning research reports, some of which are accompanied by open data and model weights, following DeepSeek R1 is below.</p>
<div class="table-wrap">
<table id="tbl:reasoning_list">
<caption>Table 6: A summary of the notable reasoning model technical reports in 2025, the first year of substantial inference-time scaling with RLHF.</caption>
<colgroup>
<col style="width: 9%" />
<col style="width: 20%" />
<col style="width: 51%" />
<col style="width: 10%" />
<col style="width: 8%" />
</colgroup>
<thead>
<tr>
<th>Date</th>
<th>Name</th>
<th>TLDR</th>
<th>Open weights</th>
<th>Open data</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025-01-22</td>
<td>DeepSeek R1 <span class="citation" data-cites="guo2025deepseek"><a href="ch021.xhtml#ref-guo2025deepseek">[61]</a></span></td>
<td>RL-based upgrade to DeepSeek, big gains on math &amp; code reasoning</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>2025-01-22</td>
<td>Kimi 1.5 <span class="citation" data-cites="team2025kimi"><a href="ch021.xhtml#ref-team2025kimi">[205]</a></span></td>
<td>Scales PPO/GRPO on Chinese/English data; strong AIME maths</td>
<td>No</td>
<td>No</td>
</tr>
<tr>
<td>2025-03-31</td>
<td>Open-Reasoner-Zero <span class="citation" data-cites="hu2025openreasonerzero"><a href="ch021.xhtml#ref-hu2025openreasonerzero">[267]</a></span></td>
<td>Fully open replication of base model RL</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>2025-04-10</td>
<td>Seed-Thinking 1.5 <span class="citation" data-cites="seed2025seed"><a href="ch021.xhtml#ref-seed2025seed">[64]</a></span></td>
<td>ByteDance RL pipeline with dynamic CoT gating</td>
<td>Yes (7B)</td>
<td>No</td>
</tr>
<tr>
<td>2025-04-30</td>
<td>Phi-4 Reasoning <span class="citation" data-cites="abdin2025phi4"><a href="ch021.xhtml#ref-abdin2025phi4">[268]</a></span></td>
<td>14B model; careful SFT→RL; excels at STEM reasoning</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>2025-05-02</td>
<td>Llama-Nemotron <span class="citation" data-cites="bercovich2025llamanemotron"><a href="ch021.xhtml#ref-bercovich2025llamanemotron">[269]</a></span></td>
<td>Multi-size “reasoning-toggle” models</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>2025-05-12</td>
<td>INTELLECT-2 <span class="citation" data-cites="primeintellectteam2025intellect2reasoningmodeltrained"><a href="ch021.xhtml#ref-primeintellectteam2025intellect2reasoningmodeltrained">[198]</a></span></td>
<td>First globally-decentralized RL training run (32B)</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>2025-05-12</td>
<td>Xiaomi MiMo <span class="citation" data-cites="xia2025mimo"><a href="ch021.xhtml#ref-xia2025mimo">[63]</a></span></td>
<td>End-to-end reasoning pipeline from pre- to post-training</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>2025-05-14</td>
<td>Qwen 3 <span class="citation" data-cites="yang2025qwen3"><a href="ch021.xhtml#ref-yang2025qwen3">[62]</a></span></td>
<td>Similar to R1 recipe applied to new models</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>2025-05-21</td>
<td>Hunyuan-TurboS <span class="citation" data-cites="liu2025hunyuan"><a href="ch021.xhtml#ref-liu2025hunyuan">[270]</a></span></td>
<td>Mamba-Transformer MoE, adaptive long/short CoT</td>
<td>No</td>
<td>No</td>
</tr>
<tr>
<td>2025-05-28</td>
<td>Skywork OR-1 <span class="citation" data-cites="he2025skyworkor1"><a href="ch021.xhtml#ref-he2025skyworkor1">[271]</a></span></td>
<td>RL recipe avoiding entropy collapse; beats DeepSeek on AIME</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>2025-06-04</td>
<td>Xiaomi MiMo VL <span class="citation" data-cites="coreteam2025mimovltechnicalreport"><a href="ch021.xhtml#ref-coreteam2025mimovltechnicalreport">[272]</a></span></td>
<td>Adapting reasoning pipeline end-to-end to include multi-modal tasks</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>2025-06-04</td>
<td>OpenThoughts <span class="citation" data-cites="guha2025openthoughts"><a href="ch021.xhtml#ref-guha2025openthoughts">[273]</a></span></td>
<td>Public 1.2M-example instruction dataset distilled from QwQ-32B</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>2025-06-10</td>
<td>Magistral <span class="citation" data-cites="mistral2025magistral"><a href="ch021.xhtml#ref-mistral2025magistral">[274]</a></span></td>
<td>Pure RL on Mistral 3; multilingual CoT; small model open-sourced</td>
<td>Yes (24B)</td>
<td>No</td>
</tr>
<tr>
<td>2025-06-16</td>
<td>MiniMax-M1 <span class="citation" data-cites="minimax2025minimaxm1scalingtesttimecompute"><a href="ch021.xhtml#ref-minimax2025minimaxm1scalingtesttimecompute">[275]</a></span></td>
<td>Open-weight 456B MoE hybrid/Lightning Attention reasoning model; 1M context; RL w/CISPO; releases 40K/80K thinking-budget checkpoints</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>2025-07-10</td>
<td>Kimi K2 <span class="citation" data-cites="kimiteam2025kimik2"><a href="ch021.xhtml#ref-kimiteam2025kimik2">[276]</a></span></td>
<td>1T MoE (32B active) with MuonClip (QK-clip) for stability; 15.5T token pretrain without loss spikes; multi-stage post-train with agentic data synthesis + joint RL; releases base + post-trained checkpoints.</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>2025-07-28</td>
<td>GLM-4.5 <span class="citation" data-cites="zeng2025glm45"><a href="ch021.xhtml#ref-zeng2025glm45">[277]</a></span></td>
<td>Open-weight 355B-A32B MoE “ARC” model with thinking/non-thinking modes; 23T-token multi-stage training + post-train w/ expert iteration and RL; releases GLM-4.5 + GLM-4.5-Air (MIT).</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>2025-08-20</td>
<td>Nemotron Nano 2 <span class="citation" data-cites="nvidia2025nemotronnano2"><a href="ch021.xhtml#ref-nvidia2025nemotronnano2">[278]</a></span></td>
<td>Hybrid Mamba-Transformer for long “thinking traces”; FP8 pretraining at 20T tokens then compression/distillation; explicitly releases multiple checkpoints plus “majority” of pre/post-training datasets.</td>
<td>Yes</td>
<td>Yes (most)</td>
</tr>
<tr>
<td>2025-09-09</td>
<td>K2-Think <span class="citation" data-cites="llm3602025k2think"><a href="ch021.xhtml#ref-llm3602025k2think">[279]</a></span></td>
<td>Parameter-efficient math reasoning system: a 32B open-weights model with test-time scaling recipe; positioned as fully open incl. training data/code (per release materials).</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>2025-09-23</td>
<td>LongCat-Flash-Thinking <span class="citation" data-cites="mlcteam2025longcat"><a href="ch021.xhtml#ref-mlcteam2025longcat">[280]</a></span></td>
<td>560B MoE reasoning model; report is explicit about a staged recipe from long-CoT cold start to large-scale RL; open-source release.</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>2025-10-21</td>
<td>Ring-1T <span class="citation" data-cites="ringteam2025everystepevolves"><a href="ch021.xhtml#ref-ringteam2025everystepevolves">[281]</a></span></td>
<td>Trillion-scale “thinking model” with RL scaling focus; report frames bottlenecks/solutions for scaling RL at 1T and releases an open model.</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>2025-11-20</td>
<td>OLMo 3 Think <span class="citation" data-cites="teamolmo2025olmo3"><a href="ch021.xhtml#ref-teamolmo2025olmo3">[282]</a></span></td>
<td>Fully open “model flow” release: reports the entire lifecycle (stages, checkpoints, and data points) and positions OLMo 3 Think 32B as a flagship open thinking model.</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>2025-12-02</td>
<td>DeepSeek V3.2 <span class="citation" data-cites="deepseekai2025v32"><a href="ch021.xhtml#ref-deepseekai2025v32">[283]</a></span></td>
<td>Open-weight MoE frontier push with a report that foregrounds attention efficiency changes, RL framework upgrades, and data synthesis for agentic/reasoning performance.</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>2025-12-15</td>
<td>Nemotron 3 Nano <span class="citation" data-cites="nvidia2025nemotron3nano"><a href="ch021.xhtml#ref-nvidia2025nemotron3nano">[284]</a></span></td>
<td>30B-A3B MoE hybrid Mamba-Transformer; pretrain on 25T tokens and includes SFT + large-scale RL; explicitly states it ships weights + recipe/code + most training data.</td>
<td>Yes</td>
<td>Yes (most)</td>
</tr>
<tr>
<td>2025-12-16</td>
<td>MiMo-V2-Flash <span class="citation" data-cites="mimo2025flash"><a href="ch021.xhtml#ref-mimo2025flash">[285]</a></span></td>
<td>309B MoE (15B active) optimized for speed: hybrid SWA/GA attention (5:1, 128-token window) + lightweight MTP; FP8 pretrain on 27T tokens; post-train with MOPD + large-scale agentic RL for reasoning/coding.</td>
<td>Yes</td>
<td>No</td>
</tr>
</tbody>
</table>
</div>
<!-- | 2025-08-15 | Ovis2.5 [@lu2025ovis25]                                  | Multimodal successor emphasizing native-resolution vision + optional thinking mode; trains reflection/self-checking; five-phase curriculum ending with DPO + GRPO; releases Ovis2.5-2B and -9B.                                         | Yes          | No         | -->
</section>
<section id="common-practices-in-training-reasoning-models" class="level3">
<h3>Common Practices in Training Reasoning Models</h3>
<p>In this section we detail common methods used to sequence training stages and modify data to maximize performance when training a reasoning model.</p>
<p>Note that these papers could have used a listed technique and not mentioned it while their peers do, so these examples are a subset of known implementations and should be used as reference, but not a final proclamation on what is an optimal recipe.</p>
<ul>
<li><strong>Offline difficulty filtering</strong>: A core intuition of RLVR is that models can only learn from examples where there is a gradient. If the starting model for RLVR can solve a problem either 100% of the time or 0% of the time, there will be no gradient between different completions to the prompt (i.e., all strategies appear the same to the policy gradient algorithm). Many models have used difficulty filtering before starting a large-scale RL to restrict the training problems to those that the starting point model solves only 20-80% of the time. This data is collected by sampling N, e.g. 16, completions to each prompt in the training set and verifying which percentage are correct. Forms of this were used by Seed-Thinking 1.5, Open Reasoner Zero, Phi 4, INTELLECT-2, MiMo RL, Skywork OR-1, and others.</li>
<li><strong>Per-batch online filtering</strong> (or difficulty curriculums throughout training): To compliment the offline filtering to find the right problems to train on, another major question is “what order should we present the problems to the model during learning.” In order to address this, many models use online filtering of questions in the batch, prebuilt curriculums/data schedulers, saving harder problems for later in training, or other ideas to improve long-term stability. Related ideas are used by Kimi 1.5, Magistral, Llama-Nemotron, INTELLECT-2, MiMo-RL, Hunyuan-TurboS, and others.</li>
<li><strong>Remove KL penalty</strong>: As the length of RL runs increased for reasoning models relative to RLHF training, and the reward function became less prone to over-optimization, many models removed the KL penalty constraining the RL-learned policy to be similar to the base model of training. This allows the model to further explore during its training. This was used by RAGEN<span class="citation" data-cites="wang2025ragenunderstandingselfevolutionllm"><a href="ch021.xhtml#ref-wang2025ragenunderstandingselfevolutionllm">[286]</a></span>, Magistral, OpenReasonerZero, Skywork OR-1, and others.</li>
<li><strong>Relaxed policy-gradient clipping</strong>: New variations of the algorithm GRPO, such as DAPO <span class="citation" data-cites="yu2025dapo"><a href="ch021.xhtml#ref-yu2025dapo">[193]</a></span>, proposed modifications to the two sided clipping objective used in GRPO (or PPO) in order to enable better exploration. Clipping has also been shown to cause potentially spurious learning signals when rewards are imperfect <span class="citation" data-cites="shao2025spurious"><a href="ch021.xhtml#ref-shao2025spurious">[287]</a></span>. This two-sided clipping with different ranges per gradient direction is used by RAGEN, Magistral, INTELLECT-2, and others.</li>
<li><strong>Off-policy data (or fully asynchronous updates)</strong>: As the length of completions needed to solve tasks with RL increases dramatically with harder problems (particularly in the <em>variance</em> of the response length), compute in RL runs can sit idle. To solve this, training is moving to asynchronous updates or changing how problems are arranged into batches to improve overall throughput. Partial-to-full asynchronous (off-policy) data is used by Seed-Thinking 1.5, INTELLECT-2, and others.</li>
<li><strong>Additional format rewards</strong>: In order to make the reasoning process predictable, many models add minor rewards to make sure the model follows the correct format of e.g. <code>&lt;think&gt;...&lt;/think&gt;</code> before an answer. This is used by DeepSeek R1, OpenReasonerZero, Magistral, Skywork OR-1, and others.</li>
<li><strong>Language consistency rewards</strong>: Similar to format rewards, some multilingual reasoning models use language consistency rewards to prioritize models that do not change languages while reasoning (for a better and more predictable user experience). These include DeepSeek R1, Magistral, and others.</li>
<li><strong>Length penalties</strong>: Many models use different forms of length penalties during RL training to either stabilize the learning process over time or to mitigate overthinking on hard problems. Some examples include Kimi 1.5 progressively extend target length to combat overthinking (while training accuracy is high across difficulty curriculum) or INTELLECT-2 running a small length penalty throughout. Others use overlong filtering and other related implementations to improve throughput.</li>
<li><strong>Loss normalization</strong>: There has been some discussion (see the chapter on Policy Gradients or <span class="citation" data-cites="liu2025understanding"><a href="ch021.xhtml#ref-liu2025understanding">[185]</a></span>) around potential length or difficulty biases introduced by the per-group normalization terms of the original GRPO algorithm. As such, some models, such as Magistral or MiMo, chose to normalize either losses or advantages at the batch level instead of the group level.</li>
<li><strong>Parallel test-time compute scaling</strong>: Combining answers from multiple parallel, independently-sampled rollouts can lead to substantial improvements over using the answer from a single rollout. The most naive form of parallel test-time compute scaling, as done in DeepSeek-R1, Phi-4, and others, involves using the answer returned by a majority of rollouts as the final answer. A more advanced technique is to use a scoring model trained to select the best answer out of the answers from the parallel rollouts. This technique has yet to be adopted by open reasoning model recipes (as of June 2025) but was mentioned in the Claude 4 announcement <span class="citation" data-cites="anthropic2025claude4"><a href="ch021.xhtml#ref-anthropic2025claude4">[288]</a></span> and used in DeepSeek-GRM <span class="citation" data-cites="liu2025inference"><a href="ch021.xhtml#ref-liu2025inference">[251]</a></span>.</li>
</ul>
<p>In complement to the common techniques, there are also many common findings on how reasoning training can create useful models without sacrificing ancillary capabilities:</p>
<ul>
<li><strong>Text-only reasoning boosts multimodal performance</strong>: Magistral, MiMo-VL, and others find that training a multimodal model and then performing text-only reasoning training after it can <em>improve</em> multimodal performance in the final model.</li>
<li><strong>Toggleable reasoning with system prompt</strong> (or length control): Llama-Nemotron, Qwen 3, and others use specific system prompts (possibly in combination with length-controlled RL training <span class="citation" data-cites="aggarwal2025l1"><a href="ch021.xhtml#ref-aggarwal2025l1">[289]</a></span>) to enable a toggle-able thinking length for the user.</li>
</ul>
</section>
</section>
</section>
</body>
</html>
