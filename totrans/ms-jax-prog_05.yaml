- en: 'Chapter 4: Neural Networks and Deep Learning Foundations'
  id: totrans-0
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: In this chapter, we unravel the magic behind these computational symphonies,
    inspired by the human brain. As we explore their fundamental concepts and diverse
    architectures, you're on the path to mastering the building blocks of modern AI.
  id: totrans-2
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 4.1 Introduction to Neural Networks and Their Components
  id: totrans-3
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Neural networks, the backbone of contemporary artificial intelligence, are inspired
    by the intricate workings of the human brain. Understanding their fundamental
    components is the key to unlocking the potential of these powerful computational
    models.
  id: totrans-4
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: What Are Neural Networks?
  id: totrans-5
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: At their core, neural networks are a type of artificial intelligence designed
    to mimic the functionality of the human brain. They consist of interconnected
    nodes called neurons, organized into layers. Each neuron receives input, processes
    it through a simple calculation, and transmits the output to other neurons.
  id: totrans-6
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Components of a Neural Network
  id: totrans-7
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '1\. Neurons: These are the basic units of computation within a neural network.
    They receive input from other neurons, perform calculations, and produce an output.'
  id: totrans-8
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '2\. Weights: Connections between neurons are determined by weights. The weight
    of a connection dictates the influence one neuron''s output has on another''s
    input.'
  id: totrans-9
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '3\. Biases: Numbers added to a neuron''s input before passing through the activation
    function, contributing to the overall flexibility of the network.'
  id: totrans-10
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '4\. Activation Functions: Responsible for deciding whether a neuron should
    "fire" or not. These functions introduce nonlinearity, allowing the network to
    learn complex patterns.'
  id: totrans-11
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '5\. Layers: Neurons are organized into layers. Neural networks typically comprise
    input, hidden, and output layers, each serving a specific purpose in information
    processing.'
  id: totrans-12
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: How Neural Networks Work
  id: totrans-13
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Neural networks operate by learning to map inputs to outputs. Through iterative
    adjustments of weights and biases, a process known as training, the network aims
    to minimize the error between its predictions and the desired outcomes.
  id: totrans-14
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Types of Neural Networks
  id: totrans-15
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '1\. Perceptrons: The simplest form, consisting of a single layer of neurons.'
  id: totrans-16
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '2\. Multilayer Perceptrons (MLPs): Expanded versions of perceptrons with multiple
    layers, enhancing their ability to learn intricate patterns.'
  id: totrans-17
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '3\. Convolutional Neural Networks (CNNs): Specialized for image recognition,
    utilizing filters for feature extraction.'
  id: totrans-18
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '4\. Recurrent Neural Networks (RNNs): Tailored for sequential data processing,
    such as text or speech.'
  id: totrans-19
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Applications of Neural Networks
  id: totrans-20
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'Neural networks find applications across diverse domains:'
  id: totrans-21
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'Image Recognition: Identifying objects in images, from faces to traffic signs.'
  id: totrans-22
  prefs:
  - PREF_UL
  stylish: true
  type: TYPE_NORMAL
- en: 'Natural Language Processing (NLP): Processing and understanding human language
    for translation and text classification.'
  id: totrans-23
  prefs:
  - PREF_UL
  stylish: true
  type: TYPE_NORMAL
- en: 'Speech Recognition: Transcribing spoken language into text.'
  id: totrans-24
  prefs:
  - PREF_UL
  stylish: true
  type: TYPE_NORMAL
- en: 'Recommender Systems: Recommending products, movies, books, or music to users.'
  id: totrans-25
  prefs:
  - PREF_UL
  stylish: true
  type: TYPE_NORMAL
- en: '`Anomaly Detection: Detecting anomalies in data, such as fraud or network intrusions.`'
  id: totrans-26
  prefs:
  - PREF_UL
  stylish: true
  type: TYPE_NORMAL
- en: '`As powerful tools with a broad range of applications, neural networks continue
    to evolve, promising innovative solutions and reshaping the landscape of artificial
    intelligence.`'
  id: totrans-27
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`4.2 Activation Functions`'
  id: totrans-28
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '`Activation functions are the powerhouses of neural networks, injecting crucial
    nonlinearity into the network''s computations. This introduces decision-making
    capabilities, allowing the network to grasp intricate patterns in data. The choice
    of activation function shapes the network''s behavior, making it a critical factor
    in achieving optimal performance.`'
  id: totrans-29
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: '`Sigmoid Function`'
  id: totrans-30
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import numpy as np`'
  id: totrans-31
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def sigmoid(x):`'
  id: totrans-32
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`return 1 / (1 + np.exp(-x))`'
  id: totrans-33
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`Sigmoid squashes input to a range between 0 and 1, fitting well for binary
    classification tasks.`'
  id: totrans-34
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`Rectified Linear Unit (ReLU)`'
  id: totrans-35
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def relu(x):`'
  id: totrans-36
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`return np.maximum(0, x)`'
  id: totrans-37
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`ReLU outputs the input directly if positive, enhancing efficiency compared
    to sigmoid.`'
  id: totrans-38
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`Tanh Function`'
  id: totrans-39
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def tanh(x):`'
  id: totrans-40
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`return np.tanh(x)`'
  id: totrans-41
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`Similar to sigmoid but with an output range of -1 to 1.`'
  id: totrans-42
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`Softmax Function`'
  id: totrans-43
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def softmax(x):`'
  id: totrans-44
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`exp_values = np.exp(x - np.max(x, axis=1, keepdims=True))`'
  id: totrans-45
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`return exp_values / np.sum(exp_values, axis=1, keepdims=True)`'
  id: totrans-46
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`Used in the output layer for multi-class classification, converting outputs
    into probabilities.`'
  id: totrans-47
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`Backpropagation: Unraveling the Gradient Descent Algorithm`'
  id: totrans-48
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`Backpropagation, the engine behind neural network training, iteratively adjusts
    weights and biases to minimize the error between predicted and actual outputs.`'
  id: totrans-49
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`Assuming a simple neural network with one hidden layer`'
  id: totrans-50
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`def backpropagation(inputs, targets, weights_input_hidden, weights_hidden_output):`'
  id: totrans-51
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`# Forward pass`'
  id: totrans-52
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`hidden_inputs = np.dot(inputs, weights_input_hidden)`'
  id: totrans-53
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`hidden_outputs = sigmoid(hidden_inputs)`'
  id: totrans-54
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`final_inputs = np.dot(hidden_outputs, weights_hidden_output)`'
  id: totrans-55
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`final_outputs = sigmoid(final_inputs)`'
  id: totrans-56
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`# Calculate error`'
  id: totrans-57
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`output_errors = targets - final_outputs`'
  id: totrans-58
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`# Backward pass`'
  id: totrans-59
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`output_grad = final_outputs * (1 - final_outputs) * output_errors`'
  id: totrans-60
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`hidden_errors = np.dot(output_grad, weights_hidden_output.T)`'
  id: totrans-61
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`hidden_grad = hidden_outputs * (1 - hidden_outputs) * hidden_errors`'
  id: totrans-62
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`# Update weights and biases`'
  id: totrans-63
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`weights_hidden_output += np.dot(hidden_outputs.T, output_grad)`'
  id: totrans-64
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`weights_input_hidden += np.dot(inputs.T, hidden_grad)`'
  id: totrans-65
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`This simple example illustrates the essence of backpropagation, where errors
    are propagated backward through the network to adjust parameters.`'
  id: totrans-66
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`Optimization Techniques: Elevating Neural Network Performance`'
  id: totrans-67
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`Optimization techniques enhance the efficiency of the training process, ensuring
    convergence and preventing overfitting.`'
  id: totrans-68
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`Stochastic Gradient Descent (SGD)`'
  id: totrans-69
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def stochastic_gradient_descent(inputs, targets, learning_rate=0.01, epochs=100):`'
  id: totrans-70
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`for epoch in range(epochs):`'
  id: totrans-71
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`for i in range(len(inputs))`'
  id: totrans-72
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`# Forward pass`'
  id: totrans-73
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`# Backward pass and weight updates`'
  id: totrans-74
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`Momentum`'
  id: totrans-75
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def momentum_optimizer(inputs, targets, learning_rate=0.01, momentum=0.9,
    epochs=100)`'
  id: totrans-76
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`velocity = 0`'
  id: totrans-77
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`for epoch in range(epochs):`'
  id: totrans-78
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`for i in range(len(inputs))`'
  id: totrans-79
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`# Forward pass`'
  id: totrans-80
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`# Backward pass and weight updates`'
  id: totrans-81
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`velocity = momentum * velocity + learning_rate * gradient`'
  id: totrans-82
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`Adaptive Learning Rate`'
  id: totrans-83
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def adaptive_learning_rate_optimizer(inputs, targets, learning_rate=0.01,
    epochs=100):`'
  id: totrans-84
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`for epoch in range(epochs):`'
  id: totrans-85
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`for i in range(len(inputs))`'
  id: totrans-86
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`# Forward pass`'
  id: totrans-87
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`# Backward pass and weight updates`'
  id: totrans-88
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`learning_rate *= 1.0 / (1.0 + decay * epoch)`'
  id: totrans-89
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Regularization Techniques
  id: totrans-90
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def dropout(inputs, dropout_rate=0.2):`'
  id: totrans-91
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`mask = (np.random.rand(*inputs.shape) < 1.0 - dropout_rate) / (1.0 - dropout_rate)`'
  id: totrans-92
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`return inputs * mask`'
  id: totrans-93
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def weight_decay(weights, decay_rate=0.001):`'
  id: totrans-94
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`return weights - decay_rate * weights`'
  id: totrans-95
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: These techniques, when applied judiciously, contribute to the robustness and
    generalization ability of neural networks.
  id: totrans-96
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: In summary, activation functions, backpropagation, and optimization techniques
    are pivotal in the neural network landscape. Grasping these concepts equips you
    to wield the power of neural networks effectively, paving the way for impactful
    solutions to real-world challenges.
  id: totrans-97
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`4.3 Unveiling Neural Network Diversity`'
  id: totrans-98
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Neural networks, the powerhouses of artificial intelligence, have reshaped how
    machines learn from data. Among their diverse structures, perceptrons, multilayer
    perceptrons (MLPs), and convolutional neural networks (CNNs) emerge as key players,
    each contributing uniquely to solving a variety of real-world challenges.
  id: totrans-99
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'Perceptrons: The Essence of Simplicity'
  id: totrans-100
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Originating in 1958, perceptrons are the foundational bricks of neural networks.
    With a single layer of neurons processing binary inputs and producing binary outputs,
    perceptrons excel in straightforward binary classification tasks. Imagine determining
    whether an email is spam or not – perceptrons handle such decisions with ease.
  id: totrans-101
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'Multilayer Perceptrons (MLPs): Expanding Horizons'
  id: totrans-102
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: MLPs take the simplicity of perceptrons and amplify it. By stacking multiple
    layers of neurons, MLPs navigate intricate patterns in data. This versatility
    makes them ideal for diverse tasks like multi-class classification and regression,
    where relationships between features and outputs are more nuanced.
  id: totrans-103
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'Convolutional Neural Networks (CNNs): Visual Prowess'
  id: totrans-104
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Enter CNNs, the maestros of image recognition. Inspired by the human visual
    cortex, CNNs utilize filters to navigate through input images, extracting essential
    features for object recognition. Whether it's classifying images, detecting objects,
    or segmenting visual data, CNNs demonstrate unparalleled proficiency in visual
    tasks.
  id: totrans-105
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Comparing Strengths and Applications
  id: totrans-106
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Perceptrons shine in simplicity and computational efficiency, suited for straightforward
    relationships between features and outputs. MLPs, with their knack for unraveling
    complex patterns, prove versatile across a spectrum of classification and regression
    challenges. CNNs, masters of visual data, excel in tasks where images and patterns
    demand intricate analysis.
  id: totrans-107
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: As we step into the future, neural networks continue to evolve. New architectures,
    training methods, and applications emerge at a swift pace. The journey ahead promises
    more sophistication and power, with neural networks poised to conquer increasingly
    intricate challenges and redefine the landscape of artificial intelligence.
  id: totrans-108
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'Coding Challenge: Implementing a Multilayer Perceptron (MLP)'
  id: totrans-109
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Your task is to implement a simple multilayer perceptron (MLP) for a binary
    classification problem. Use NumPy for matrix operations and implement both forward
    and backward passes. Additionally, include a training loop to update the weights
    and biases using gradient descent.
  id: totrans-110
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'Requirements:'
  id: totrans-111
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '1\. Design a multilayer perceptron with:'
  id: totrans-112
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '- Input layer with 5 neurons.'
  id: totrans-113
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '- Hidden layer with 10 neurons, using a ReLU activation function.'
  id: totrans-114
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '- Output layer with 1 neuron and a sigmoid activation function.'
  id: totrans-115
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 2\. Implement forward pass logic to compute the predicted output.
  id: totrans-116
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 3\. Implement backward pass logic to calculate gradients and update weights
    and biases using gradient descent.
  id: totrans-117
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 4\. Create a simple dataset for binary classification (e.g., use NumPy to generate
    random data).
  id: totrans-118
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 5\. Train your MLP on the dataset for a specified number of epochs.
  id: totrans-119
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'Solution:'
  id: totrans-120
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'Here''s a simplified solution in Python using NumPy:'
  id: totrans-121
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`import numpy as np`'
  id: totrans-122
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Define the MLP architecture
  id: totrans-123
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`input_size = 5`'
  id: totrans-124
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`hidden_size = 10`'
  id: totrans-125
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`output_size = 1`'
  id: totrans-126
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`learning_rate = 0.01`'
  id: totrans-127
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: epochs = 1000
  id: totrans-128
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Initialize weights and biases
  id: totrans-129
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`weights_input_hidden = np.random.randn(input_size, hidden_size)`'
  id: totrans-130
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`biases_hidden = np.zeros((1, hidden_size))`'
  id: totrans-131
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`weights_hidden_output = np.random.randn(hidden_size, output_size)`'
  id: totrans-132
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`biases_output = np.zeros((1, output_size))`'
  id: totrans-133
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Activation functions
  id: totrans-134
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`def relu(x):`'
  id: totrans-135
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: return np.maximum(0, x)
  id: totrans-136
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`def sigmoid(x):`'
  id: totrans-137
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: return 1 / (1 + np.exp(-x))
  id: totrans-138
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Forward pass
  id: totrans-139
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`def forward_pass(inputs):`'
  id: totrans-140
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`hidden_layer_input = np.dot(inputs, weights_input_hidden) + biases_hidden`'
  id: totrans-141
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`hidden_layer_output = relu(hidden_layer_input)`'
  id: totrans-142
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) +
    biases_output`'
  id: totrans-143
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`predicted_output = sigmoid(output_layer_input)`'
  id: totrans-144
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: return predicted_output, hidden_layer_output
  id: totrans-145
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Backward pass
  id: totrans-146
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`def backward_pass(inputs, predicted_output, hidden_layer_output, labels):`'
  id: totrans-147
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`output_error = predicted_output - labels`'
  id: totrans-148
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`output_delta = output_error * (predicted_output * (1 - predicted_output))`'
  id: totrans-149
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`hidden_layer_error = output_delta.dot(weights_hidden_output.T)`'
  id: totrans-150
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`hidden_layer_delta = hidden_layer_error * (hidden_layer_output > 0)`'
  id: totrans-151
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`# Update weights and biases`'
  id: totrans-152
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`weights_hidden_output -= learning_rate * hidden_layer_output.T.dot(output_delta)`'
  id: totrans-153
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`biases_output -= learning_rate * np.sum(output_delta, axis=0, keepdims=True)`'
  id: totrans-154
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`weights_input_hidden -= learning_rate * inputs.T.dot(hidden_layer_delta)`'
  id: totrans-155
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`biases_hidden -= learning_rate * np.sum(hidden_layer_delta, axis=0, keepdims=True)`'
  id: totrans-156
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Generate a simple dataset
  id: totrans-157
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`np.random.seed(42)`'
  id: totrans-158
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`X = np.random.rand(100, input_size)`'
  id: totrans-159
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`y = (X[:, 0] + X[:, 1] > 1).astype(int).reshape(-1, 1)`'
  id: totrans-160
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Training loop
  id: totrans-161
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: for epoch in range(epochs)
  id: totrans-162
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`# Forward pass`'
  id: totrans-163
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`predicted_output, hidden_layer_output = forward_pass(X)`'
  id: totrans-164
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`# Backward pass`'
  id: totrans-165
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`backward_pass(X, predicted_output, hidden_layer_output, y)`'
  id: totrans-166
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`# Print loss every 100 epochs`'
  id: totrans-167
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`if epoch % 100 == 0:`'
  id: totrans-168
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`loss = -np.mean(y * np.log(predicted_output) + (1 - y) * np.log(1 - predicted_output))`'
  id: totrans-169
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`print(f"Epoch {epoch}, Loss: {loss}")`'
  id: totrans-170
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Test the trained model on a new data point
  id: totrans-171
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`new_data_point = np.array([[0.6, 0.7, 0.8, 0.9, 1.0]])`'
  id: totrans-172
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`prediction, _ = forward_pass(new_data_point)`'
  id: totrans-173
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`print(f"Predicted Output for New Data Point: {prediction}")`'
  id: totrans-174
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'Note: This is a simplified example for educational purposes. In practice, deep
    learning frameworks like TensorFlow or PyTorch are commonly used for building
    and training neural networks.'
  id: totrans-175
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: With neural networks as our compass, we've uncovered the essence of deep learning.
    Activation functions, backpropagation, and varied architectures are now tools
    in your arsenal. This foundation propels you toward practical applications, where
    the transformative power of deep learning unfolds.
  id: totrans-176
  prefs: []
  stylish: true
  type: TYPE_NORMAL
