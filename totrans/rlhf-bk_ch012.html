<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <title>ch012.xhtml</title>
  <style>
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
/*
 * Custom CSS file. Override it as you like.
 *
 * Credits to @killercup (https://gist.github.com/killercup); Extracted from this Gist:
 *   https://gist.github.com/killercup/5917178
 * Substantial modifications made by natolambert
 */

html {
    font-size: 100%;
    overflow-y: scroll;
    -webkit-text-size-adjust: 100%;
    -ms-text-size-adjust: 100%;
}

body {
    color: #444;
    font-family: Georgia, Palatino, 'Palatino Linotype', Times, 'Times New Roman', serif;
    font-size: 12px;
    line-height: 1.7;
    padding: 1em;
    margin: auto;
    max-width: 42em;
    background: #fefefe;
}

a {
    color: #0645ad;
    text-decoration: none;
}

a:visited {
    color: #0b0080;
}

a:hover {
    color: #06e;
}

a:active {
    color: #faa700;
}

a:focus {
    outline: thin dotted;
}

*::-moz-selection {
    background: rgba(255, 255, 0, 0.3);
    color: #000;
}

*::selection {
    background: rgba(255, 255, 0, 0.3);
    color: #000;
}

a::-moz-selection {
    background: rgba(255, 255, 0, 0.3);
    color: #0645ad;
}

a::selection {
    background: rgba(255, 255, 0, 0.3);
    color: #0645ad;
}

p {
    margin: 1em 0;
}

img {
    max-width: 100%;
}

h1,
h2,
h3,
h4,
h5,
h6 {
    color: #111;
    line-height: 125%;
    margin-top: 2em;
    font-weight: normal;
    position: relative;
}

/* Heading anchor link styles */
.header-anchor {
    opacity: 0;
    font-size: 0.8em;
    vertical-align: middle;
    position: absolute;
    margin-left: 0.3em;
    transition: opacity 0.2s ease-in-out;
}

h2:hover .header-anchor,
h3:hover .header-anchor,
h4:hover .header-anchor,
h5:hover .header-anchor,
h6:hover .header-anchor {
    opacity: 1;
}

h4,
h5,
h6 {
    font-weight: bold;
}

h1 {
    font-size: 2.5em;
}

h1.title {
    hyphens: none;
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    word-break: keep-all;
}

h2 {
    font-size: 2em;
}

h3 {
    font-size: 1.5em;
}

h4 {
    font-size: 1.2em;
}

h5 {
    font-size: 1em;
}

h6 {
    font-size: 0.9em;
}

blockquote {
    color: #666666;
    margin: 0;
    padding-left: 3em;
    border-left: 0.5em #EEE solid;
}

hr {
    display: block;
    height: 2px;
    border: 0;
    border-top: 1px solid #aaa;
    border-bottom: 1px solid #eee;
    margin: 1em 0;
    padding: 0;
}

pre,
code,
kbd,
samp {
    color: #000;
    font-family: monospace, monospace;
    _font-family: 'courier new', monospace;
    font-size: 0.98em;
}

pre {
    white-space: pre;
    white-space: pre-wrap;
    word-wrap: break-word;
}


b,
strong {
    font-weight: bold;
}

dfn {
    font-style: italic;
}

ins {
    background: #ff9;
    color: #000;
    text-decoration: none;
}

mark {
    background: #ff0;
    color: #000;
    font-style: italic;
    font-weight: bold;
}

sub,
sup {
    font-size: 75%;
    line-height: 0;
    position: relative;
    vertical-align: baseline;
}

sup {
    top: -0.5em;
}

sub {
    bottom: -0.25em;
}

ul,
ol {
    margin: 1em 0;
    padding: 0 0 0 2em;
}

li p:last-child {
    margin-bottom: 0;
}

ul ul,
ol ol {
    margin: .3em 0;
}

dl {
    margin-bottom: 1em;
}

dt {
    font-weight: bold;
    margin-bottom: .8em;
}

dd {
    margin: 0 0 .8em 2em;
}

dd:last-child {
    margin-bottom: 0;
}

img {
    border: 0;
    -ms-interpolation-mode: bicubic;
    vertical-align: middle;
}

figure {
    display: block;
    text-align: center;
    margin: 1em 0;
}

figure img {
    border: none;
    margin: 0 auto;
}

figcaption {
    font-size: 0.8em;
    font-style: italic;
    margin: 0 0 .8em;
}

/* for html tables */
table {
    margin-bottom: 2em;
    border-bottom: 1px solid #ddd;
    border-right: 1px solid #ddd;
    border-spacing: 0;
    box-shadow: none;
    border: none;
    border-collapse: collapse;
    margin-left: auto;
    margin-right: auto;
    width: auto; /* Keeps natural width; wrapper handles overflow */
    display: table;
}

th {
    padding: 12px;
    text-align: center;
    background-color: #eee;
    border: 1px solid #ddd;
}

td {
    padding: 12px;
    text-align: left; /* Keeps data cells left-aligned */
    border: 1px solid #ddd;
    vertical-align: top;
}

.table-scroll {
    max-width: 100%;
    overflow-x: auto;
    -webkit-overflow-scrolling: touch;
    margin: 1.5em auto 2em;
}

.table-scroll table {
    margin: 0 auto;
    display: table;
    width: auto;
    width: fit-content;
    width: max-content;
}

.table-wrap {
    margin: 1.5em auto 2em;
}

.table-wrap table {
    margin: 0 auto;
}

.table-scroll::-webkit-scrollbar {
    height: 8px;
}

.table-scroll::-webkit-scrollbar-thumb {
    background-color: rgba(0, 0, 0, 0.2);
    border-radius: 4px;
}

.table-scroll::-webkit-scrollbar-track {
    background: rgba(0, 0, 0, 0.05);
}

.author {
    font-size: 1.2em;
    text-align: center;
}

/* Target only mobile screens */
@media only screen and (max-width: 479px) {
    body {
        font-size: 14px;
    }
}

@media only screen and (min-width: 480px) {
    body {
        font-size: 15px;
    }
}

@media only screen and (min-width: 768px) {
    body {
        font-size: 16px;
    }
}

@media print {
    * {
        background: transparent !important;
        color: black !important;
        filter: none !important;
        -ms-filter: none !important;
    }
    body {
        font-size: 12pt;
        max-width: 100%;
    }
    a,
    a:visited {
        text-decoration: underline;
    }
    hr {
        height: 1px;
        border: 0;
        border-bottom: 1px solid black;
    }
    a[href]:after {
        content: " (" attr(href) ")";
    }
    abbr[title]:after {
        content: " (" attr(title) ")";
    }
    .ir a:after,
    a[href^="javascript:"]:after,
    a[href^="#"]:after {
        content: "";
    }
    pre,
    blockquote {
        border: 1px solid #999;
        padding-right: 1em;
        page-break-inside: avoid;
    }
    tr,
    img {
        page-break-inside: avoid;
    }
    img {
        max-width: 100% !important;
    }
    @page :left {
        margin: 15mm 20mm 15mm 10mm;
    }
    @page :right {
        margin: 15mm 10mm 15mm 20mm;
    }
    p,
    h2,
    h3 {
        orphans: 3;
        widows: 3;
    }
    h2,
    h3 {
        page-break-after: avoid;
    }
}

.dropdown-content {
  display: none;
}



thead {
    background-color: #f5f5f5;
}


.dropdown-content.open {
  display: block;
  max-height: 2000px;
}
/* Header Nav Block */
    .chapter-nav {
        display: grid;
        grid-template-columns: repeat(3, 1fr);
        /* grid-template-rows: auto auto;  */
        gap: 0.5rem;
        padding: 0.5rem;
        max-width: 1200px;
        text-align: left;
    }  
    .section {
        background-color: #ffffff;
        padding-top: 5px;
        padding-right: 12px;
        padding-bottom: 8px;
        padding-left: 12px;
        border-radius: 5px;
        text-align: left;
    }
  .dropdown-content {
    max-height: 0;
    overflow: hidden;
    transition: max-height 0.3s ease-out;
    background: #f8f8f8;  /* Unified with section background */
  }
  /* dropdown row */
  .dropdown-button {
    width: 100%;
    text-align: left;
    padding: 0.5rem;
    background: #f8f8f8;
    display: flex;
    align-items: center;
    gap: 0.5rem;
    cursor: pointer;
    border: none;
    font-size: 0.9rem;
  }
  /* carrot button */
  .dropdown-button .chevron {
    width: 14px;
    height: 14px;
    transition: transform 0.2s;
  }
  /* dropdown animation */
  .dropdown-button[aria-expanded="true"] .chevron {
    transform: rotate(180deg);
  }
  .section h3 {
    font-weight: bold;
    font-size: 0.8rem;
    margin-top: 5px;
    margin-bottom: 5px;  /* or whatever bottom spacing you prefer */
  }
  .section ol, .section ul {
    margin: 0;
    padding-left: 20px;
    text-align: left;
  }
  .section li {
    font-size: 12px;
    line-height: 1.3;
    margin-bottom: 3px;
    text-align: left;
  }
  .section a {
    color: #0066cc;
    text-decoration: none;
  }
  .section a:hover {
    text-decoration: underline;
  }

  /* Mobile Responsiveness */
  @media screen and (max-width: 768px) {
    .chapter-nav {
      grid-template-columns: 1fr;
    }
    .section {
      margin-bottom: 10px;
    }
    .section p {
      font-size: 16px;
    }
    .section li {
      font-size: 14px;
    }
  }  </style>
  <link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css" />
</head>
<body epub:type="bodymatter">
<section id="direct-alignment-algorithms" class="level1">
<h1>Direct Alignment Algorithms</h1>
<p>Direct Alignment Algorithms (DAAs) allow one to update models to solve the same RLHF objective, shown again in eq.¬†<a href="ch012.xhtml#eq:review_rlhf">68</a>, without ever training an intermediate reward model or using reinforcement learning optimizers. It solves the same preference learning problem we‚Äôve been studying (with literally the same data!), in order to make language models more aligned, smarter, and easier to use. The lack of a reward model and online optimization makes DAAs far simpler to implement, reducing compute spent during training and making experimentation easier. This chapter details the complex mathematics that were done to derive these algorithms, and then shows that the sometimes tedious derivations result in simple implementations.</p>
<p>The most prominent DAA and one that catalyzed an entire academic movement of aligning language models is Direct Preference Optimization (DPO) <span class="citation" data-cites="rafailov2024direct"><a href="ch021.xhtml#ref-rafailov2024direct">[20]</a></span>. At its core, DPO is using gradient ascent to solve the same constrained RLHF objective (see Chapter 4):</p>
<p><span id="eq:review_rlhf"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><munder><mi mathvariant="normal">max</mi><mi>œÄ</mi></munder><msub><mi>ùîº</mi><mrow><mi>x</mi><mo>‚àº</mo><mi>ùíü</mi></mrow></msub><msub><mi>ùîº</mi><mrow><mi>y</mi><mo>‚àº</mo><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></msub><mrow><mo stretchy="true" form="prefix">[</mo><msub><mi>r</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow><mo>‚àí</mo><mi>Œ≤</mi><msub><mi>ùíü</mi><mtext mathvariant="normal">KL</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">‚à•</mo><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>68</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex"> \max_{\pi} \mathbb{E}_{x \sim \mathcal{D}}\mathbb{E}_{y \sim \pi(y|x)} \left[r_\theta(x, y)\right] - \beta \mathcal{D}_{\text{KL}}\left(\pi(y|x) \| \pi_{\text{ref}}(y|x)\right)\qquad{(68)}</annotation></semantics></math></span></p>
<p>Since its release in May of 2023, after a brief delay where the community figured out the right data and hyperparameters to use DPO with (specifically, surprisingly low learning rates), many popular models have used DPO or its variants, from Zephyr-<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ≤</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math> kickstarting it in October of 2023 <span class="citation" data-cites="tunstall2023zephyr"><a href="ch021.xhtml#ref-tunstall2023zephyr">[21]</a></span>, Llama 3 Instruct <span class="citation" data-cites="dubey2024llama"><a href="ch021.xhtml#ref-dubey2024llama">[24]</a></span>, T√ºlu 2 <span class="citation" data-cites="ivison2023camels"><a href="ch021.xhtml#ref-ivison2023camels">[22]</a></span> and 3 <span class="citation" data-cites="lambert2024t"><a href="ch021.xhtml#ref-lambert2024t">[6]</a></span>, Nemotron 4 340B <span class="citation" data-cites="adler2024nemotron"><a href="ch021.xhtml#ref-adler2024nemotron">[25]</a></span>, and others. Technically, Sequence Likelihood Calibration (SLiC-HF) was the first, modern direct alignment algorithm released <span class="citation" data-cites="zhao2023slic"><a href="ch021.xhtml#ref-zhao2023slic">[210]</a></span>, but it did not catch on due to a combination of factors (unwinding the adoption of research methods is always a tricky task).</p>
<p>The most impactful part of DPO and DAAs is lowering the barrier of entry to experimenting with language model post-training ‚Äì it uses less compute, is easier to implement from scratch, and is easier to get working on both toy and production examples.</p>
<p><em>Throughout this chapter, we use <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> to denote prompts and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math> to denote completions. This notation is common in the language model literature, where methods operate on full prompt-completion pairs rather than individual tokens.</em></p>
<section id="direct-preference-optimization-dpo" class="level2">
<h2>Direct Preference Optimization (DPO)</h2>
<p>Here we explain intuitions for how DPO works and re-derive the core equations fully.</p>
<section id="how-dpo-works" class="level3">
<h3>How DPO Works</h3>
<p>DPO at a surface level is directly optimizing a policy to solve the RLHF objective. The loss function for this, which we will revisit below in the derivations, is a pairwise relationship of log-probabilities. The loss function derived from a Bradley-Terry reward model follows:</p>
<p><span id="eq:dpo_core"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>‚Ñí</mi><mtext mathvariant="normal">DPO</mtext></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo>;</mo><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>‚àí</mi><msub><mi>ùîº</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><msub><mi>y</mi><mi>c</mi></msub><mo>,</mo><msub><mi>y</mi><mi>r</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>‚àº</mo><mi>ùíü</mi></mrow></msub><mrow><mo stretchy="true" form="prefix">[</mo><mi mathvariant="normal">log</mi><mi>œÉ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ≤</mi><mi mathvariant="normal">log</mi><mfrac><mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo>‚àí</mo><mi>Œ≤</mi><mi mathvariant="normal">log</mi><mfrac><mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>r</mi></msub><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>r</mi></msub><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>69</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex"> \mathcal{L}_{\text{DPO}}(\pi_\theta; \pi_{\text{ref}}) = -\mathbb{E}_{(x, y_c, y_r) \sim \mathcal{D}}\left[ \log \sigma\left( \beta \log \frac{\pi_{\theta}(y_c \mid x)}{\pi_{\text{ref}}(y_c \mid x)} - \beta \log \frac{\pi_{\theta}(y_r \mid x)}{\pi_{\text{ref}}(y_r \mid x)} \right) \right] \qquad{(69)}</annotation></semantics></math></span></p>
<p>Throughout, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ≤</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math> is a hyperparameter balancing the reward optimization to the KL distance between the final model and the initial reference (i.e.¬†balancing over-optimization, as a crucial hyperparameter to using DPO correction). This relies on the implicit reward for DPO training that replaces using an external reward model, which is a log-ratio of probabilities:</p>
<p><span id="eq:dpo_reward"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>Œ≤</mi><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><mfrac><mrow><msub><mi>œÄ</mi><mi>r</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>70</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">r(x, y) = \beta  \log \frac{\pi_r(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\qquad{(70)}</annotation></semantics></math></span></p>
<p>where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>œÄ</mi><mi>r</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\pi_r(y \mid x)</annotation></semantics></math> is the exact, optimal reward policy that we are solving for. This comes from deriving the Bradley-Terry reward with respect to an optimal policy (shown in eq.¬†<a href="ch012.xhtml#eq:dpo_opt_policy">84</a>), as shown in the Bradley-Terry model section of Chapter 7. Essentially, the implicit reward model shows ‚Äúthe probability of human preference data in terms of the optimal policy rather than the reward model.‚Äù</p>
<p>Let us consider the loss shown in eq.¬†<a href="ch012.xhtml#eq:dpo_core">69</a> that the optimizer must decrease. Here, the loss will be lower when the log-ratio of the chosen response is bigger than the log-ratio of the rejected response (normalized by the reference model). In practice, this is a sum of log-probabilities of the model across the sequence of tokens in the data presented. Hence, DPO is increasing the delta in probabilities between the chosen and rejected responses.</p>
<p>With the reward in eq.¬†<a href="ch012.xhtml#eq:dpo_reward">70</a>, we can write the gradient of the loss to further interpret what is going on:</p>
<p><span id="eq:dpo_gradient"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><msub><mi>‚Ñí</mi><mtext mathvariant="normal">DPO</mtext></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo>;</mo><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>‚àí</mi><mi>Œ≤</mi><msub><mi>ùîº</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><msub><mi>y</mi><mi>c</mi></msub><mo>,</mo><msub><mi>y</mi><mi>r</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>‚àº</mo><mi>ùíü</mi></mrow></msub><mrow><mo stretchy="true" form="prefix">[</mo><mi>œÉ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>r</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><msub><mi>y</mi><mi>r</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>‚àí</mo><msub><mi>r</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><msub><mi>y</mi><mi>c</mi></msub><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mi mathvariant="normal">log</mi><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>‚àí</mo><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mi mathvariant="normal">log</mi><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>r</mi></msub><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>71</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\nabla_{\theta}\mathcal{L}_{\text{DPO}}(\pi_{\theta}; \pi_{\text{ref}}) = -\beta \mathbb{E}_{(x, y_c, y_r)\sim \mathcal{D}}\left[ \sigma\left(r_{\theta}(x, y_r) - r_{\theta}(x, y_c)\right) \left(\nabla_{\theta}\log \pi(y_c \mid x) - \nabla_{\theta}\log \pi(y_r \mid x)\right) \right] \qquad{(71)}</annotation></semantics></math></span></p>
<p>Here, the gradient solves the above objective by doing the following:</p>
<ul>
<li>The first term within the sigmoid function, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>œÉ</mi><mo stretchy="false" form="prefix">(</mo><mi>‚ãÖ</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\sigma(\cdot)</annotation></semantics></math>, creates a weight of the parameter update from 0 to 1 that is higher when the reward estimate is incorrect. When the rejected sample is preferred over the chosen, the weight update should be larger!</li>
<li>Second, the terms in the inner brackets <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">[</mo><mi>‚ãÖ</mi><mo stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[\cdot]</annotation></semantics></math> increase the likelihood of the chosen response <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mi>c</mi></msub><annotation encoding="application/x-tex">y_c</annotation></semantics></math> and decrease the likelihood of the rejected <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mi>r</mi></msub><annotation encoding="application/x-tex">y_r</annotation></semantics></math>.</li>
<li>These terms are weighted by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ≤</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>, which controls how the update balances ordering the completions correctly relative to the KL distance.</li>
</ul>
<p>The core intuition is that DPO is fitting an implicit reward model whose corresponding optimal policy can be extracted in a closed form (thanks to gradient descent and our ML tools). The closed form of the equation means that it is straightforward to implement the exact gradient, rather than needing to reach it by proxy of training a reward model and sampling completions to score. What is often misunderstood is that DPO is learning a reward model at its core, hence the subtitle of the paper <em>Your Language Model is Secretly a Reward Model.</em> It is easy to confuse this with the DPO objective training a policy directly, hence studying the derivations below is good for a complete understanding.</p>
<p>With the implicit reward model learning, DPO is generating an optimal solution to the RLHF objective given the data in the dataset and the specific KL constraint in the objective <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ≤</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>. Here, DPO solves for the exact policy given a specific KL distance because the generations are not online as in policy gradient algorithms ‚Äì a core difference from the RL methods for preference tuning. In many ways, this makes the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ≤</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math> value easier to tune with DPO relative to online RL methods, but crucially and intuitively the optimal value depends on the model being trained and the data training it.</p>
<p>At each batch of preference data, composed of many pairs of completions <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mrow><mi>c</mi><mi>h</mi><mi>o</mi><mi>s</mi><mi>e</mi><mi>n</mi></mrow></msub><mo>‚âª</mo><msub><mi>y</mi><mrow><mi>r</mi><mi>e</mi><mi>j</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>e</mi><mi>d</mi></mrow></msub></mrow><annotation encoding="application/x-tex">y_{chosen} \succ y_{rejected}</annotation></semantics></math>, DPO takes gradient steps directly towards the optimal solution. It is far simpler than policy gradient methods.</p>
<figure id="fig:dpo-meme">
<img src="../media/file16.jpg" alt="Figure 18: When DPO first released it sparked a fierce debate in the research community about how to best do RLHF and preference learning. This meme is a great job capturing the sentiment, where the debate often felt forced and over the top, but many people both getting started and in top labs were getting immense benefit out of DPO. DPO simplicity meme, credit Tom Goldstein." />
<figcaption aria-hidden="true">Figure 18: When DPO first released it sparked a fierce debate in the research community about how to best do RLHF and preference learning. This meme is a great job capturing the sentiment, where the debate often felt forced and over the top, but many people both getting started and in top labs were getting immense benefit out of DPO. DPO simplicity meme, credit Tom Goldstein.</figcaption>
</figure>
</section>
<section id="dpo-derivation" class="level3">
<h3>DPO Derivation</h3>
<p>The DPO derivation takes two primary parts. First, the authors show the form of the policy that optimally solved the RLHF objective used throughout this book. Next, they show how to arrive at that solution from pairwise preference data (i.e.¬†a Bradley Terry model).</p>
<section id="deriving-the-optimal-rlhf-solution" class="level4">
<h4>1. Deriving the Optimal RLHF Solution</h4>
<p>To start, we should consider the RLHF optimization objective once again, here indicating we wish to maximize this quantity:</p>
<p><span id="eq:rlhf_opt_eq_repeat"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><munder><mi mathvariant="normal">max</mi><mi>œÄ</mi></munder><msub><mi>ùîº</mi><mrow><mi>x</mi><mo>‚àº</mo><mi>ùíü</mi></mrow></msub><msub><mi>ùîº</mi><mrow><mi>y</mi><mo>‚àº</mo><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></msub><mrow><mo stretchy="true" form="prefix">[</mo><msub><mi>r</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow><mo>‚àí</mo><mi>Œ≤</mi><msub><mi>ùíü</mi><mtext mathvariant="normal">KL</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">‚à•</mo><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>72</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex"> \max_{\pi} \mathbb{E}_{x \sim \mathcal{D}}\mathbb{E}_{y \sim \pi(y|x)} \left[r_\theta(x, y)\right] - \beta \mathcal{D}_{\text{KL}}\left(\pi(y|x) \| \pi_{\text{ref}}(y|x)\right)\qquad{(72)}</annotation></semantics></math></span></p>
<p>Here, the dual expectation only applies to the sampling to compute the expected reward, as the KL term is still an analytical expression. First, let us expand the definition of KL-divergence. Recall that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ùíü</mi><mtext mathvariant="normal">KL</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>œÄ</mi><mo stretchy="false" form="postfix">‚à•</mo><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ùîº</mi><mrow><mi>y</mi><mo>‚àº</mo><mi>œÄ</mi></mrow></msub><mrow><mo stretchy="true" form="prefix">[</mo><mi mathvariant="normal">log</mi><mfrac><mrow><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{D}_{\text{KL}}(\pi \| \pi_{\text{ref}}) = \mathbb{E}_{y \sim \pi}\left[\log \frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)}\right]</annotation></semantics></math>, where the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\pi(y|x)</annotation></semantics></math> weighting in the sum becomes the sampling distribution. Since both terms now share the same expectation over <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>‚àº</mo><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">y \sim \pi(y|x)</annotation></semantics></math>, we can combine them:</p>
<p><span id="eq:dpo_deriv_1"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><munder><mi mathvariant="normal">max</mi><mi>œÄ</mi></munder><msub><mi>ùîº</mi><mrow><mi>x</mi><mo>‚àº</mo><mi>ùíü</mi></mrow></msub><msub><mi>ùîº</mi><mrow><mi>y</mi><mo>‚àº</mo><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></msub><mrow><mo stretchy="true" form="prefix">[</mo><mi>r</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false" form="postfix">)</mo><mo>‚àí</mo><mi>Œ≤</mi><mi mathvariant="normal">log</mi><mfrac><mrow><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo stretchy="true" form="postfix">]</mo></mrow><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>73</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\max_{\pi} \mathbb{E}_{x \sim \mathcal{D}}\mathbb{E}_{y \sim \pi(y|x)}\left[r(x,y)-\beta\log\frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)}\right] \qquad{(73)}</annotation></semantics></math></span></p>
<p>Next, pull the negative sign out of the difference in brackets. To do this, split it into two terms:</p>
<p><span id="eq:dpo_deriv_2"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo><munder><mi mathvariant="normal">max</mi><mi>œÄ</mi></munder><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>ùîº</mi><mrow><mi>x</mi><mo>‚àº</mo><mi>ùíü</mi></mrow></msub><msub><mi>ùîº</mi><mrow><mi>y</mi><mo>‚àº</mo><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></msub><mo stretchy="false" form="prefix">[</mo><mi>r</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">]</mo><mo>‚àí</mo><mi>Œ≤</mi><mspace width="0.167em"></mspace><msub><mi>ùîº</mi><mrow><mi>x</mi><mo>‚àº</mo><mi>ùíü</mi></mrow></msub><msub><mi>ùîº</mi><mrow><mi>y</mi><mo>‚àº</mo><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></msub><mrow><mo stretchy="true" form="prefix">[</mo><mi mathvariant="normal">log</mi><mfrac><mrow><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo stretchy="true" form="postfix">]</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>74</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex"> = \max_{\pi}\left(\mathbb{E}_{x \sim \mathcal{D}}\mathbb{E}_{y \sim \pi(y|x)}[r(x,y)] - \beta\,\mathbb{E}_{x \sim \mathcal{D}}\mathbb{E}_{y \sim \pi(y|x)}\left[\log\frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)}\right]\right) \qquad{(74)}</annotation></semantics></math></span></p>
<p>Then, remove the factor of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>‚àí</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">-1</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ≤</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>,</p>
<p><span id="eq:dpo_deriv_3"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo><munder><mi mathvariant="normal">min</mi><mi>œÄ</mi></munder><mrow><mo stretchy="true" form="prefix">(</mo><mi>‚àí</mi><msub><mi>ùîº</mi><mrow><mi>x</mi><mo>‚àº</mo><mi>ùíü</mi></mrow></msub><msub><mi>ùîº</mi><mrow><mi>y</mi><mo>‚àº</mo><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></msub><mo stretchy="false" form="prefix">[</mo><mi>r</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">]</mo><mo>+</mo><mi>Œ≤</mi><mspace width="0.167em"></mspace><msub><mi>ùîº</mi><mrow><mi>x</mi><mo>‚àº</mo><mi>ùíü</mi></mrow></msub><msub><mi>ùîº</mi><mrow><mi>y</mi><mo>‚àº</mo><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></msub><mrow><mo stretchy="true" form="prefix">[</mo><mi mathvariant="normal">log</mi><mfrac><mrow><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mrow><mi mathvariant="normal">r</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">f</mi></mrow></msub><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo stretchy="true" form="postfix">]</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>75</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex"> = \min_{\pi}\left(-\mathbb{E}_{x \sim \mathcal{D}}\mathbb{E}_{y \sim \pi(y|x)}[r(x,y)] + \beta\,\mathbb{E}_{x \sim \mathcal{D}}\mathbb{E}_{y \sim \pi(y|x)}\left[\log\frac{\pi(y|x)}{\pi_{\mathrm{ref}}(y|x)}\right]\right) \qquad{(75)}</annotation></semantics></math></span></p>
<p>Divide by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ≤</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math> and recombine:</p>
<p><span id="eq:dpo_deriv_4"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo><munder><mi mathvariant="normal">min</mi><mi>œÄ</mi></munder><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>ùîº</mi><mrow><mi>x</mi><mo>‚àº</mo><mi>ùíü</mi></mrow></msub><msub><mi>ùîº</mi><mrow><mi>y</mi><mo>‚àº</mo><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></msub><mrow><mo stretchy="true" form="prefix">[</mo><mi mathvariant="normal">log</mi><mfrac><mrow><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo>‚àí</mo><mfrac><mn>1</mn><mi>Œ≤</mi></mfrac><mi>r</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>76</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex"> = \min_{\pi}\left(\mathbb{E}_{x \sim \mathcal{D}}\mathbb{E}_{y \sim \pi(y|x)}\left[ \log\frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)} - \frac{1}{\beta}r(x,y) \right]\right) \qquad{(76)}</annotation></semantics></math></span></p>
<p>Next, we must introduce a partition function, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Z</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">Z(x)</annotation></semantics></math>:</p>
<p><span id="eq:dpo_partition"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Z</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><munder><mo>‚àë</mo><mi>y</mi></munder><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mrow><mi mathvariant="normal">exp</mi><mo>&#8289;</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mn>1</mn><mi>Œ≤</mi></mfrac><mi>r</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>77</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex"> Z(x) = \sum_y \pi_{\text{ref}}(y|x)\exp\left(\frac{1}{\beta}r(x,y)\right) \qquad{(77)}</annotation></semantics></math></span></p>
<p>The partition function acts as a normalization factor over the reference policy, summing over all possible responses <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math> to a prompt <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>. With this substituted in, we obtain our intermediate transformation:</p>
<p><span id="eq:dpo_deriv_5"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><munder><mi mathvariant="normal">min</mi><mi>œÄ</mi></munder><msub><mi>ùîº</mi><mrow><mi>x</mi><mo>‚àº</mo><mi>ùíü</mi></mrow></msub><msub><mi>ùîº</mi><mrow><mi>y</mi><mo>‚àº</mo><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></msub><mrow><mo stretchy="true" form="prefix">[</mo><mi mathvariant="normal">log</mi><mfrac><mrow><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><mfrac><mn>1</mn><mrow><mi>Z</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mrow><mi mathvariant="normal">exp</mi><mo>&#8289;</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mn>1</mn><mi>Œ≤</mi></mfrac><mi>r</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo>‚àí</mo><mi mathvariant="normal">log</mi><mi>Z</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>78</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex"> \min_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\mathbb{E}_{y\sim\pi(y|x)}\left[\log\frac{\pi(y|x)}{\frac{1}{Z(x)}\pi_{\text{ref}}(y|x)\exp\left(\frac{1}{\beta}r(x,y)\right)} - \log Z(x)\right] \qquad{(78)}</annotation></semantics></math></span></p>
<p>To see how this is obtained, consider the internal part of the optimization in brackets of eq.¬†<a href="ch012.xhtml#eq:dpo_deriv_4">76</a>:</p>
<p><span id="eq:dpo_deriv_6"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><mfrac><mrow><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo>‚àí</mo><mfrac><mn>1</mn><mi>Œ≤</mi></mfrac><mi>r</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false" form="postfix">)</mo><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>79</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex"> \log\frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)} - \frac{1}{\beta}r(x,y) \qquad{(79)}</annotation></semantics></math></span></p>
<p>Then, add <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><mi>Z</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>‚àí</mo><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><mi>Z</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\log Z(x) - \log Z(x)</annotation></semantics></math> to both sides:</p>
<p><span id="eq:dpo_deriv_7"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><mfrac><mrow><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo>‚àí</mo><mfrac><mn>1</mn><mi>Œ≤</mi></mfrac><mi>r</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false" form="postfix">)</mo><mo>+</mo><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><mi>Z</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>‚àí</mo><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><mi>Z</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>80</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex"> = \log\frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)} - \frac{1}{\beta}r(x,y) + \log Z(x) - \log Z(x) \qquad{(80)}</annotation></semantics></math></span></p>
<p>Then, we group the terms:</p>
<p><span id="eq:dpo_deriv_8"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi mathvariant="normal">log</mi><mfrac><mrow><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo>+</mo><mi mathvariant="normal">log</mi><mi>Z</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow><mo>‚àí</mo><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><mi>Z</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>‚àí</mo><mfrac><mn>1</mn><mi>Œ≤</mi></mfrac><mi>r</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false" form="postfix">)</mo><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>81</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex"> = \left( \log \frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)} + \log Z(x) \right) - \log Z(x) - \frac{1}{\beta}r(x,y) \qquad{(81)}</annotation></semantics></math></span></p>
<p>With <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>+</mo><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>‚ãÖ</mo><mi>y</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\log(x) + \log(y) = \log(x\cdot y)</annotation></semantics></math> (and moving <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Z</mi><annotation encoding="application/x-tex">Z</annotation></semantics></math> to the denominator), we get:</p>
<p><span id="eq:dpo_deriv_9"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><mfrac><mrow><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><mfrac><mn>1</mn><mrow><mi>Z</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo>‚àí</mo><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><mi>Z</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>‚àí</mo><mfrac><mn>1</mn><mi>Œ≤</mi></mfrac><mi>r</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false" form="postfix">)</mo><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>82</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex"> = \log \frac{\pi(y|x)}{\frac{1}{Z(x)}\pi_{\text{ref}}(y|x)}- \log Z(x) - \frac{1}{\beta}r(x,y) \qquad{(82)}</annotation></semantics></math></span></p>
<p>Next, we expand <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mi>Œ≤</mi></mfrac><mi>r</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\frac{1}{\beta}r(x,y)</annotation></semantics></math> to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><mrow><mi mathvariant="normal">exp</mi><mo>&#8289;</mo></mrow><mfrac><mn>1</mn><mi>Œ≤</mi></mfrac><mi>r</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\log \exp \frac{1}{\beta}r(x,y)</annotation></semantics></math> and do the same operation to get eq.¬†<a href="ch012.xhtml#eq:dpo_deriv_5">78</a>. With this optimization form, we need to actually solve for the optimal policy <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>œÄ</mi><mo>*</mo></msup><annotation encoding="application/x-tex">\pi^*</annotation></semantics></math>. To do so, let us consider the above optimization as a KL distance:</p>
<p><span id="eq:dpo_deriv_10"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><munder><mi mathvariant="normal">min</mi><mi>œÄ</mi></munder><msub><mi>ùîº</mi><mrow><mi>x</mi><mo>‚àº</mo><mi>ùíü</mi></mrow></msub><mrow><mo stretchy="true" form="prefix">[</mo><msub><mi>ùíü</mi><mtext mathvariant="normal">KL</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="prefix">|</mo><mo stretchy="false" form="prefix">|</mo><mfrac><mn>1</mn><mrow><mi>Z</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mi mathvariant="normal">exp</mi><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mn>1</mn><mi>Œ≤</mi></mfrac><mi>r</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>‚àí</mo><mi mathvariant="normal">log</mi><mi>Z</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>83</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex"> \min_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathcal{D}_{\text{KL}} \left(\pi(y|x)||\frac{1}{Z(x)}\pi_{\text{ref}}(y|x)\exp\left(\frac{1}{\beta}r(x,y)\right) \right) - \log Z(x)\right] \qquad{(83)}</annotation></semantics></math></span></p>
<p>Since the partition function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Z</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">Z(x)</annotation></semantics></math> does not depend on the final answer, we can ignore it. This leaves us with just the KL distance between our policy we are learning and a form relating the partition, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ≤</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>, reward, and reference policy. The Gibb‚Äôs inequality tells this is minimized at a distance of 0, only when the two quantities are equal! Hence, we get an optimal policy:</p>
<p><span id="eq:dpo_opt_policy"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>œÄ</mi><mo>*</mo></msup><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Z</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mrow><mi mathvariant="normal">exp</mi><mo>&#8289;</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mn>1</mn><mi>Œ≤</mi></mfrac><mi>r</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>84</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex"> \pi^*(y|x) = \pi(y|x) = \frac{1}{Z(x)}\pi_{\text{ref}}(y|x)\exp\left(\frac{1}{\beta}r(x,y)\right) \qquad{(84)}</annotation></semantics></math></span></p>
</section>
<section id="deriving-dpo-objective-for-bradley-terry-models" class="level4">
<h4>2. Deriving DPO Objective for Bradley Terry Models</h4>
<p>To start, recall from Chapter 7 on Reward Modeling and Chapter 6 on Preference Data that a Bradley-Terry model of human preferences is formed as:</p>
<p><span id="eq:bradley_terry_dpo"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>p</mi><mo>*</mo></msup><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>‚âª</mo><msub><mi>y</mi><mn>2</mn></msub><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mrow><mrow><mi mathvariant="normal">exp</mi><mo>&#8289;</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>r</mi><mo>*</mo></msup><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><msub><mi>y</mi><mn>1</mn></msub><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mrow><mi mathvariant="normal">exp</mi><mo>&#8289;</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>r</mi><mo>*</mo></msup><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><msub><mi>y</mi><mn>1</mn></msub><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mi mathvariant="normal">exp</mi><mo>&#8289;</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>r</mi><mo>*</mo></msup><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><msub><mi>y</mi><mn>2</mn></msub><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>85</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p^*(y_1 \succ y_2 \mid x) = \frac{\exp\left(r^*(x,y_1)\right)}{\exp\left(r^*(x,y_1)\right) + \exp\left(r^*(x, y_2)\right)} \qquad{(85)}</annotation></semantics></math></span></p>
<p>By manipulating eq.¬†<a href="ch012.xhtml#eq:dpo_opt_policy">84</a>, we can solve for the optimal reward. First, take the logarithm of both sides:</p>
<p><span id="eq:dpo_reward_deriv1"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><msup><mi>œÄ</mi><mo>*</mo></msup><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mn>1</mn><mrow><mi>Z</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mi mathvariant="normal">exp</mi><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mn>1</mn><mi>Œ≤</mi></mfrac><msup><mi>r</mi><mo>*</mo></msup><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>86</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\log \pi^*(y|x) = \log \left( \frac{1}{Z(x)}\pi_{\text{ref}}(y|x)\exp\left(\frac{1}{\beta}r^*(x,y)\right) \right)\qquad{(86)}</annotation></semantics></math></span></p>
<p>Expanding the right-hand side using <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mi>b</mi><mi>c</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><mi>a</mi><mo>+</mo><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><mi>b</mi><mo>+</mo><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">\log(abc) = \log a + \log b + \log c</annotation></semantics></math>:</p>
<p><span id="eq:dpo_reward_deriv2"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><msup><mi>œÄ</mi><mo>*</mo></msup><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>‚àí</mi><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><mi>Z</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>+</mo><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>+</mo><mfrac><mn>1</mn><mi>Œ≤</mi></mfrac><msup><mi>r</mi><mo>*</mo></msup><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false" form="postfix">)</mo><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>87</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\log \pi^*(y|x) = -\log Z(x) + \log \pi_{\text{ref}}(y|x) + \frac{1}{\beta}r^*(x,y)\qquad{(87)}</annotation></semantics></math></span></p>
<p>Rearranging to solve for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>r</mi><mo>*</mo></msup><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">r^*(x,y)</annotation></semantics></math>:</p>
<p><span id="eq:dpo_reward_deriv3"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mi>Œ≤</mi></mfrac><msup><mi>r</mi><mo>*</mo></msup><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><msup><mi>œÄ</mi><mo>*</mo></msup><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>‚àí</mo><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>+</mo><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><mi>Z</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>88</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\frac{1}{\beta}r^*(x,y) = \log \pi^*(y|x) - \log \pi_{\text{ref}}(y|x) + \log Z(x)\qquad{(88)}</annotation></semantics></math></span></p>
<p>Multiplying both sides by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ≤</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>:</p>
<p><span id="eq:dpo_reward_full"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>r</mi><mo>*</mo></msup><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>Œ≤</mi><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><mfrac><mrow><msup><mi>œÄ</mi><mo>*</mo></msup><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo>+</mo><mi>Œ≤</mi><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><mi>Z</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>89</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">r^*(x, y) = \beta \log \frac{\pi^*(y \mid x)}{\pi_{\text{ref}}(y \mid x)} + \beta \log Z(x)\qquad{(89)}</annotation></semantics></math></span></p>
<p>We then can substitute the reward into the Bradley-Terry equation shown in eq.¬†<a href="ch012.xhtml#eq:bradley_terry_dpo">85</a> to obtain:</p>
<p><span id="eq:dpo_loss_deriv0"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>p</mi><mo>*</mo></msup><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>‚âª</mo><msub><mi>y</mi><mn>2</mn></msub><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mrow><mrow><mi mathvariant="normal">exp</mi><mo>&#8289;</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ≤</mi><mi mathvariant="normal">log</mi><mfrac><mrow><msup><mi>œÄ</mi><mo>*</mo></msup><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo>+</mo><mi>Œ≤</mi><mi mathvariant="normal">log</mi><mi>Z</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mrow><mi mathvariant="normal">exp</mi><mo>&#8289;</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ≤</mi><mi mathvariant="normal">log</mi><mfrac><mrow><msup><mi>œÄ</mi><mo>*</mo></msup><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo>+</mo><mi>Œ≤</mi><mi mathvariant="normal">log</mi><mi>Z</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mi mathvariant="normal">exp</mi><mo>&#8289;</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ≤</mi><mi mathvariant="normal">log</mi><mfrac><mrow><msup><mi>œÄ</mi><mo>*</mo></msup><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>2</mn></msub><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>2</mn></msub><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo>+</mo><mi>Œ≤</mi><mi mathvariant="normal">log</mi><mi>Z</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>90</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p^*(y_1 \succ y_2 \mid x) = \frac{\exp\left(\beta \log \frac{\pi^*(y_1 \mid x)}{\pi_{\text{ref}}(y_1 \mid x)} + \beta \log Z(x)\right)}
{\exp\left(\beta \log \frac{\pi^*(y_1 \mid x)}{\pi_{\text{ref}}(y_1 \mid x)} + \beta \log Z(x)\right) + \exp\left(\beta \log \frac{\pi^*(y_2 \mid x)}{\pi_{\text{ref}}(y_2 \mid x)} + \beta \log Z(x)\right)} \qquad{(90)}</annotation></semantics></math></span></p>
<p>By decomposing the exponential expressions from <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>e</mi><mrow><mi>a</mi><mo>+</mo><mi>b</mi></mrow></msup><annotation encoding="application/x-tex">e^{a+b}</annotation></semantics></math> to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>e</mi><mi>a</mi></msup><msup><mi>e</mi><mi>b</mi></msup></mrow><annotation encoding="application/x-tex">e^a e^b</annotation></semantics></math> and then cancelling out the terms <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>e</mi><mrow><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><mo stretchy="false" form="prefix">(</mo><mi>Z</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">e^{\log(Z(x))}</annotation></semantics></math>, this simplifies to:</p>
<p><span id="eq:dpo_loss_deriv1"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>p</mi><mo>*</mo></msup><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>‚âª</mo><msub><mi>y</mi><mn>2</mn></msub><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mrow><mrow><mi mathvariant="normal">exp</mi><mo>&#8289;</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ≤</mi><mi mathvariant="normal">log</mi><mfrac><mrow><msup><mi>œÄ</mi><mo>*</mo></msup><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mrow><mi mathvariant="normal">exp</mi><mo>&#8289;</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ≤</mi><mi mathvariant="normal">log</mi><mfrac><mrow><msup><mi>œÄ</mi><mo>*</mo></msup><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mi mathvariant="normal">exp</mi><mo>&#8289;</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ≤</mi><mi mathvariant="normal">log</mi><mfrac><mrow><msup><mi>œÄ</mi><mo>*</mo></msup><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>2</mn></msub><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>2</mn></msub><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>91</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p^*(y_1 \succ y_2 \mid x) = \frac{\exp\left(\beta \log \frac{\pi^*(y_1 \mid x)}{\pi_{\text{ref}}(y_1 \mid x)}\right)}
{\exp\left(\beta \log \frac{\pi^*(y_1 \mid x)}{\pi_{\text{ref}}(y_1 \mid x)}\right) + \exp\left(\beta \log \frac{\pi^*(y_2 \mid x)}{\pi_{\text{ref}}(y_2 \mid x)}\right)} \qquad{(91)}</annotation></semantics></math></span></p>
<p>Then, multiply the numerator and denominator by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">exp</mi><mo>&#8289;</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>‚àí</mi><mi>Œ≤</mi><mi mathvariant="normal">log</mi><mfrac><mrow><msup><mi>œÄ</mi><mo>*</mo></msup><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\exp\left(-\beta \log \frac{\pi^*(y_1 \mid x)}{\pi_{\text{ref}}(y_1 \mid x)}\right)</annotation></semantics></math> to obtain:</p>
<p><span id="eq:dpo_loss_deriv2"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>p</mi><mo>*</mo></msup><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>‚âª</mo><msub><mi>y</mi><mn>2</mn></msub><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mrow><mi mathvariant="normal">exp</mi><mo>&#8289;</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ≤</mi><mi mathvariant="normal">log</mi><mfrac><mrow><msup><mi>œÄ</mi><mo>*</mo></msup><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>2</mn></msub><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>2</mn></msub><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo>‚àí</mo><mi>Œ≤</mi><mi mathvariant="normal">log</mi><mfrac><mrow><msup><mi>œÄ</mi><mo>*</mo></msup><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>92</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p^*(y_1 \succ y_2 \mid x) = \frac{1}{1 + \exp\left(\beta \log \frac{\pi^*(y_2 \mid x)}{\pi_{\text{ref}}(y_2 \mid x)} - \beta \log \frac{\pi^*(y_1 \mid x)}{\pi_{\text{ref}}(y_1 \mid x)}\right)} \qquad{(92)}</annotation></semantics></math></span></p>
<p>Finally, with the definition of a sigmoid function as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>œÉ</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mi>‚àí</mi><mi>x</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">\sigma(x) = \frac{1}{1+e^{-x}}</annotation></semantics></math>, we obtain:</p>
<p><span id="eq:dpo_loss_deriv3"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>p</mi><mo>*</mo></msup><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>‚âª</mo><msub><mi>y</mi><mn>2</mn></msub><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>œÉ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ≤</mi><mi mathvariant="normal">log</mi><mfrac><mrow><msup><mi>œÄ</mi><mo>*</mo></msup><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo>‚àí</mo><mi>Œ≤</mi><mi mathvariant="normal">log</mi><mfrac><mrow><msup><mi>œÄ</mi><mo>*</mo></msup><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>2</mn></msub><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>2</mn></msub><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>93</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p^*(y_1 \succ y_2 \mid x) = \sigma\left(\beta \log \frac{\pi^*(y_1 \mid x)}{\pi_{\text{ref}}(y_1 \mid x)} - \beta \log \frac{\pi^*(y_2 \mid x)}{\pi_{\text{ref}}(y_2 \mid x)}\right) \qquad{(93)}</annotation></semantics></math></span></p>
<p>This is the loss function for DPO, as shown in eq.¬†<a href="ch012.xhtml#eq:dpo_core">69</a>. The DPO paper has an additional derivation for the objective under a Plackett-Luce Model, which is far less used in practice <span class="citation" data-cites="rafailov2024direct"><a href="ch021.xhtml#ref-rafailov2024direct">[20]</a></span>.</p>
</section>
<section id="deriving-the-bradley-terry-dpo-gradient" class="level4">
<h4>3. Deriving the Bradley Terry DPO Gradient</h4>
<p>We used the DPO gradient shown in eq.¬†<a href="ch012.xhtml#eq:dpo_gradient">71</a> to explain intuitions for how the model learns. To derive this, we must take the gradient of eq.¬†<a href="ch012.xhtml#eq:dpo_loss_deriv3">93</a> with respect to the model parameters.</p>
<p><span id="eq:dpo_grad_0"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><msub><mi>‚Ñí</mi><mtext mathvariant="normal">DPO</mtext></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo>;</mo><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>‚àí</mi><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><msub><mi>ùîº</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><msub><mi>y</mi><mi>c</mi></msub><mo>,</mo><msub><mi>y</mi><mi>r</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>‚àº</mo><mi>ùíü</mi></mrow></msub><mrow><mo stretchy="true" form="prefix">[</mo><mi mathvariant="normal">log</mi><mi>œÉ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ≤</mi><mi mathvariant="normal">log</mi><mfrac><mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo>‚àí</mo><mi>Œ≤</mi><mi mathvariant="normal">log</mi><mfrac><mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>r</mi></msub><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>r</mi></msub><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>94</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\nabla_{\theta}\mathcal{L}_{\text{DPO}}(\pi_{\theta}; \pi_{\text{ref}}) = -\nabla_{\theta}\mathbb{E}_{(x,y_c,y_r)\sim\mathcal{D}}\left[ \log \sigma\left(\beta \log \frac{\pi_{\theta}(y_c|x)}{\pi_{\text{ref}}(y_c|x)} - \beta \log \frac{\pi_{\theta}(y_r|x)}{\pi_{\text{ref}}(y_r|x)}\right)\right] \qquad{(94)}</annotation></semantics></math></span></p>
<p>To start, this can be rewritten. We know that the derivative of a sigmoid function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>d</mi><mrow><mi>d</mi><mi>x</mi></mrow></mfrac><mi>œÉ</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>œÉ</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>‚àí</mo><mi>œÉ</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\frac{d}{dx} \sigma(x) = \sigma(x)(1-\sigma(x))</annotation></semantics></math>, the derivative of logarithm <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>d</mi><mrow><mi>d</mi><mi>x</mi></mrow></mfrac><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><mi>x</mi><mo>=</mo><mfrac><mn>1</mn><mi>x</mi></mfrac></mrow><annotation encoding="application/x-tex">\frac{d}{dx} \log x = \frac{1}{x}</annotation></semantics></math>, and properties of sigmoid <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>œÉ</mi><mo stretchy="false" form="prefix">(</mo><mi>‚àí</mi><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mn>1</mn><mo>‚àí</mo><mi>œÉ</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\sigma(-x)=1-\sigma(x)</annotation></semantics></math>, so we can reformat the above equation.</p>
<p>First, let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mo>=</mo><mi>Œ≤</mi><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><mfrac><mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo>‚àí</mo><mi>Œ≤</mi><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><mfrac><mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>r</mi></msub><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>r</mi></msub><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">u=\beta \log \frac{\pi_{\theta}(y_c|x)}{\pi_{\text{ref}}(y_c|x)} - \beta \log \frac{\pi_{\theta}(y_r|x)}{\pi_{\text{ref}}(y_r|x)}</annotation></semantics></math> (the expression inside the sigmoid). Then, we have</p>
<p><span id="eq:dpo_grad_2"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><msub><mi>‚Ñí</mi><mtext mathvariant="normal">DPO</mtext></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo>;</mo><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>‚àí</mi><msub><mi>ùîº</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><msub><mi>y</mi><mi>c</mi></msub><mo>,</mo><msub><mi>y</mi><mi>r</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>‚àº</mo><mi>ùíü</mi></mrow></msub><mrow><mo stretchy="true" form="prefix">[</mo><mfrac><mrow><msup><mi>œÉ</mi><mo>‚Ä≤</mo></msup><mo stretchy="false" form="prefix">(</mo><mi>u</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><mi>œÉ</mi><mo stretchy="false" form="prefix">(</mo><mi>u</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mi>u</mi><mo stretchy="true" form="postfix">]</mo></mrow><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>95</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\nabla_{\theta}\mathcal{L}_{\text{DPO}}(\pi_{\theta};\pi_{\text{ref}}) = -\mathbb{E}_{(x, y_c, y_r)\sim \mathcal{D}}\left[\frac{\sigma&#39;(u)}{\sigma(u)}\nabla_{\theta}u\right] \qquad{(95)}</annotation></semantics></math></span></p>
<p>Expanding this and using the above expressions for sigmoid and logarithms results in the gradient introduced earlier:</p>
<p><span id="eq:dpo_grad_3"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>‚àí</mi><msub><mi>ùîº</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><msub><mi>y</mi><mi>c</mi></msub><mo>,</mo><msub><mi>y</mi><mi>r</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>‚àº</mo><mi>ùíü</mi></mrow></msub><mrow><mo stretchy="true" form="prefix">[</mo><mi>Œ≤</mi><mi>œÉ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ≤</mi><mi mathvariant="normal">log</mi><mfrac><mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>r</mi></msub><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>r</mi></msub><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo>‚àí</mo><mi>Œ≤</mi><mi mathvariant="normal">log</mi><mfrac><mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mi mathvariant="normal">log</mi><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>‚àí</mo><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mi mathvariant="normal">log</mi><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>r</mi></msub><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>96</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex"> -\mathbb{E}_{(x,y_c,y_r)\sim\mathcal{D}}\left[\beta\sigma\left(\beta\log\frac{\pi_{\theta}(y_r|x)}{\pi_{\text{ref}}(y_r|x)} - \beta\log\frac{\pi_{\theta}(y_c|x)}{\pi_{\text{ref}}(y_c|x)}\right)\left[\nabla_{\theta}\log\pi(y_c|x)-\nabla_{\theta}\log\pi(y_r|x)\right]\right] \qquad{(96)}</annotation></semantics></math></span></p>
</section>
</section>
</section>
<section id="numerical-concerns-weaknesses-and-alternatives" class="level2">
<h2>Numerical Concerns, Weaknesses, and Alternatives</h2>
<p>Many variants of the DPO algorithm have been proposed to address weaknesses of DPO. For example, without rollouts where a reward model can rate generations, DPO treats every pair of preference data with equal weight. In reality, as seen in Chapter 6 on Preference Data, there are many ways of capturing preference data with a richer label than binary. Multiple algorithms have been proposed to re-balance the optimization away from treating each pair equally.</p>
<ul>
<li><strong>REgression to RElative REward Based RL (REBEL)</strong> adds signal from a reward model, as a margin between chosen and rejected responses, rather than solely the pairwise preference data to more accurately solve the RLHF problem <span class="citation" data-cites="gao2024rebel"><a href="ch021.xhtml#ref-gao2024rebel">[166]</a></span>.</li>
<li><strong>Conservative DPO (cDPO) and Identity Preference Optimization (IPO)</strong> address the overfitting by assuming noise in the preference data. cDPO assumes N percent of the data is incorrectly labelled <span class="citation" data-cites="rafailov2024direct"><a href="ch021.xhtml#ref-rafailov2024direct">[20]</a></span> and IPO changes the optimization to soften probability of preference rather than optimize directly from a label <span class="citation" data-cites="azar2024general"><a href="ch021.xhtml#ref-azar2024general">[211]</a></span>. Practically, IPO changes the preference probability to a nonlinear function, moving away from the Bradley-Terry assumption, with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Œ®</mi><mo stretchy="false" form="prefix">(</mo><mi>q</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mi>q</mi><mrow><mn>1</mn><mo>‚àí</mo><mi>q</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Psi(q) = \log\left(\frac{q}{1-q}\right)</annotation></semantics></math>.</li>
<li><strong>DPO with an offset (ODPO)</strong> ‚Äúrequires the difference between the likelihood of the preferred and dispreferred response to be greater than an offset value‚Äù <span class="citation" data-cites="amini2024direct"><a href="ch021.xhtml#ref-amini2024direct">[212]</a></span> ‚Äì do not treat every data pair equally, but this can come at the cost of a more difficult labeling environment.</li>
</ul>
<p>Some variants to DPO attempt to either improve the learning signal by making small changes to the loss or make the application more efficient by reducing memory usage.</p>
<ul>
<li><strong>Odds Ratio Policy Optimization (ORPO)</strong> directly updates the policy model with a pull towards the chosen response, similar to the instruction finetuning loss, with a small penalty on the chosen response <span class="citation" data-cites="hong2024reference"><a href="ch021.xhtml#ref-hong2024reference">[213]</a></span>. This change of loss function removes the need for a reference model, simplifying the setup. The best way to view ORPO is DPO inspired, rather than a DPO derivative.</li>
<li><strong>Simple Preference Optimization SimPO</strong> makes a minor change to the DPO optimization, by averaging the log-probabilities rather than summing them (SimPO) or adding length normalization, to improve performance <span class="citation" data-cites="meng2025simpo"><a href="ch021.xhtml#ref-meng2025simpo">[214]</a></span>.</li>
</ul>
<figure id="fig:dpo_issue">
<img src="../media/file17.png" class="center" alt="Figure 19: Sketch of preference displacement in DPO." />
<figcaption aria-hidden="true">Figure 19: Sketch of preference displacement in DPO.</figcaption>
</figure>
<p>One of the core issues <em>apparent</em> in DPO is that the optimization drives only to increase the margin between the probability of the chosen and rejected responses. Numerically, the model reduces the probability of both the chosen and rejected responses, but the <em>rejected response is reduced by a greater extent</em> as shown in fig.¬†<a href="#fig:dpo_issue">19</a>. Intuitively, it is not clear how this generalizes, but work has posited that it increases the probability of unaddressed for behaviors ‚Äì i.e.¬†tokens that language model could generate, but are not in the distribution of the post-training datasets <span class="citation" data-cites="razin2024unintentional"><a href="ch021.xhtml#ref-razin2024unintentional">[215]</a></span> <span class="citation" data-cites="ren2024learning"><a href="ch021.xhtml#ref-ren2024learning">[216]</a></span>. Simple methods‚Äîsuch as Cal-DPO <span class="citation" data-cites="xiao2024cal"><a href="ch021.xhtml#ref-xiao2024cal">[217]</a></span>, which adjusts the optimization process, and AlphaPO <span class="citation" data-cites="gupta2025alphapo"><a href="ch021.xhtml#ref-gupta2025alphapo">[218]</a></span>, which modifies the reward shape‚Äîmitigate this <strong>preference displacement</strong>. In practice, the exact impact of this is not well known, but points to a potential reason why online methods can outperform vanilla DPO.</p>
<p>The largest other reason that is posited for DPO-like methods to have a lower ceiling on performance than online (RL based) RLHF methods is that the training signal comes from completions from previous or other models. Online variants of DPO alleviate these limitations by generating new completions and incorporating a preference signal at training time. <strong>Online DPO</strong> <span class="citation" data-cites="guo2024direct"><a href="ch021.xhtml#ref-guo2024direct">[219]</a></span> samples generations from the current model, while <strong>Discriminator-Guided DPO</strong> (D2PO) <span class="citation" data-cites="singhal2024d2po"><a href="ch021.xhtml#ref-singhal2024d2po">[220]</a></span> uses reward model relabelling to create new preference data on the fly, and many more variants exist.</p>
<p>There is a long list of other DAA variants, such as Direct Nash Optimization (DNO) <span class="citation" data-cites="rosset2024direct"><a href="ch021.xhtml#ref-rosset2024direct">[221]</a></span> or Binary Classifier Optimization (BCO) <span class="citation" data-cites="jung2024binary"><a href="ch021.xhtml#ref-jung2024binary">[222]</a></span>, but the choice of algorithm is far less important than the initial model and the data used <span class="citation" data-cites="lambert2024t"><a href="ch021.xhtml#ref-lambert2024t">[6]</a></span> <span class="citation" data-cites="zhao2024rainbowpo"><a href="ch021.xhtml#ref-zhao2024rainbowpo">[223]</a></span> <span class="citation" data-cites="gorbatovski2025differences"><a href="ch021.xhtml#ref-gorbatovski2025differences">[224]</a></span>.</p>
</section>
<section id="implementation-considerations" class="level2">
<h2>Implementation Considerations</h2>
<p>DAAs such as DPO are implemented very differently than policy gradient optimizers. The DPO loss, taken from the original implementation, largely can be summarized as follows <span class="citation" data-cites="rafailov2024direct"><a href="ch021.xhtml#ref-rafailov2024direct">[20]</a></span>:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>pi_logratios <span class="op">=</span> policy_chosen_logps <span class="op">-</span> policy_rejected_logps</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>ref_logratios <span class="op">=</span> reference_chosen_logps <span class="op">-</span> reference_rejected_logps</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> pi_logratios <span class="op">-</span> ref_logratios  <span class="co"># also known as h_{\pi_\theta}^{y_w,y_l}</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> <span class="op">-</span>F.logsigmoid(beta <span class="op">*</span> logits)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>chosen_rewards <span class="op">=</span> beta <span class="op">*</span> (policy_chosen_logps <span class="op">-</span> reference_chosen_logps).detach()</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>rejected_rewards <span class="op">=</span> beta <span class="op">*</span> (policy_rejected_logps <span class="op">-</span> reference_rejected_logps).detach()</span></code></pre></div>
<p>This can be used in standard language model training stacks as this information is already collated during the forward pass of a model (with the addition of a reference model).</p>
<p>In most ways, this is simpler and a quality of life improvement, but also they offer a different set of considerations.</p>
<ol type="1">
<li><strong>KL distance is static</strong>: In DPO and other algorithms, the KL distance is set explicitly by the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ≤</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math> parameter that balances the distance penalty to the optimization. This is due to the fact that DPO takes gradient steps towards the <em>optimal</em> solution to the RLHF objective given the data ‚Äì it steps exactly to the solution set by the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ≤</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math> term. On the other hand, RL based optimizers take steps based on the batch and recent data.</li>
<li><strong>Caching log-probabilities</strong>: Simple implementations of DPO do the forward passes for the policy model and reference models at the same time for conveniences with respect to the loss function. Though, this doubles the memory used and results in increased GPU usage. To avoid this, one can compute the log-probabilities of the reference model over the training dataset first, then reference it when computing the loss and updating the parameters per batch, reducing the peak memory usage by 50%.</li>
</ol>
</section>
<section id="daas-vs.-rl-online-vs.-offline-data" class="level2">
<h2>DAAs vs.¬†RL: Online vs.¬†Offline Data</h2>
<p>Broadly, the argument boils down to one question: Do we need the inner workings of reinforcement learning, with value functions, policy gradients, and all, to align language models with RLHF? This, like most questions phrased this way, is overly simplistic. Of course, both methods are well-established, but it is important to illustrate where the fundamental differences and performance manifolds lie.</p>
<p>Multiple reports have concluded that policy-gradient based and RL methods outperform DPO and its variants. The arguments take different forms, from training models with different algorithms but controlled data<span class="citation" data-cites="ivison2024unpacking"><a href="ch021.xhtml#ref-ivison2024unpacking">[190]</a></span> <span class="citation" data-cites="xu2024dpo"><a href="ch021.xhtml#ref-xu2024dpo">[225]</a></span> or studying the role of on-policy data within the RL optimization loop <span class="citation" data-cites="tajwar2024preference"><a href="ch021.xhtml#ref-tajwar2024preference">[226]</a></span>. In all of these cases, DPO algorithms are a hair behind.</p>
<p>Even with this performance delta, DAA are still used extensively in leading models due to its simplicity. DAAs provide a controlled environment where iterations on training data and other configurations can be made rapidly, and given that data is often far more important than algorithms, using DPO can be fine.</p>
<p>With the emergence of reasoning models that are primarily trained with RL, further investment will return to using RL for preference-tuning, which in the long-term will improve the robustness of RL infrastructure and cement this margin between DAAs and RL for optimizing from human feedback.</p>
</section>
</section>
</body>
</html>
