- en: 'Chapter 5: Building Deep Learning Models in Jax'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Welcome to the hands-on side of deep learning with Jax! This chapter is your
    practical gateway to crafting neural networks using Jax's functional programming
    prowess. Say farewell to theory; it's time to roll up your sleeves and turn neural
    network concepts into tangible models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Jax's functional programming paradigm
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Jax's functional programming paradigm presents an elegant and expressive approach
    to construct neural network models. This methodology treats neural networks as
    functions, offering a streamlined path for modeling different network architectures
    and experimenting with ease.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Key Steps in Implementing Neural Networks with Jax's Functional Programming
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Network Architecture Definition: Define the neural network structure, specifying
    layers, activation functions, and connections between layers. This sets the groundwork
    for the network's computational flow.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: import jax.numpy as jnp
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 'def neural_network(x):'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: layer1 = jnp.tanh(jnp.dot(x, W1) + b1)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: output = jnp.dot(layer1, W2) + b2
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: return output
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Parameter Initialization: Initialize network parameters, like weights and
    biases, using appropriate random distributions to set the starting points for
    training.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: W1 = jnp.random.normal(key, (input_dim, hidden_dim))
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: b1 = jnp.random.normal(key, (hidden_dim,))
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: W2 = jnp.random.normal(key, (hidden_dim, output_dim))
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: b2 = jnp.random.normal(key, (output_dim,))
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Forward Propagation Implementation: Construct forward propagation to pass
    input data through the network, involving activation functions and layer computations.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'def forward_pass(x, W1, b1, W2, b2):'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: layer1 = jnp.tanh(jnp.dot(x, W1) + b1)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: output = jnp.dot(layer1, W2) + b2
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: return output
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Loss Function Definition: Define a suitable loss function that measures
    the error between the network's output and the desired output.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'def mse_loss(predicted, target):'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: return jnp.mean((predicted - target)2)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Gradient Computation: Utilize Jax's automatic differentiation to compute
    gradients of the loss function concerning the network parameters.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: gradients = jax.grad(mse_loss)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Parameter Updates: Employ an optimizer like SGD or Adam to iteratively update
    network parameters using the computed gradients.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'def update_parameters(parameters, gradients, learning_rate):'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: new_parameters = parameters - learning_rate * gradients
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: return new_parameters
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Benefits of Jax's Functional Programming Paradigm
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Conciseness: Promotes clear, concise, and maintainable code for complex
    neural network models.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Expressiveness: Allows expressive code to convey network structure and computations
    with clarity.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Modular Design: Facilitates a modular approach for creating reusable components
    and organizing code efficiently.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '4\. Error Proneness: Reduces error risks by isolating code and avoiding mutable
    state.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Experimentation Efficiency: Enables rapid prototyping and experimentation
    with various architectures and configurations.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 'Jax''s Automatic Differentiation: A Powerful Tool for Neural Network Training'
  id: totrans-37
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Jax's automatic differentiation capabilities streamline the process of defining,
    training, and optimizing neural networks. By automating gradient computation,
    Jax empowers you to focus on the core aspects of neural network design and optimization,
    enabling you to build and train sophisticated models with remarkable efficiency.
  id: totrans-38
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Defining Neural Networks with Automatic Differentiation
  id: totrans-39
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 1\. Functional Programming Paradigm: Leverage Jax's functional programming paradigm
    to define neural networks concisely and expressively. This approach treats neural
    networks as functions, making them modular and easy to manipulate.
  id: totrans-40
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 2\. Layer Definition: Define individual layers of the neural network, specifying
    the number of neurons, activation functions, and connections between neurons.
    Jax's vectorization capabilities allow for efficient computation of operations
    across batches of data.
  id: totrans-41
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 3\. Automatic Gradient Computation: Utilize Jax's automatic differentiation
    to compute the gradients of the network's output with respect to its parameters.
    This eliminates the need for explicit gradient calculation, reducing the complexity
    of training.
  id: totrans-42
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Training Neural Networks with Automatic Differentiation
  id: totrans-43
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 1\. Loss Function Definition: Define a loss function that measures the error
    between the network's output and the desired output. Jax provides a variety of
    loss functions, such as mean squared error (MSE) for regression tasks and cross-entropy
    loss for classification tasks.
  id: totrans-44
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 2\. Gradient-Based Optimization: Employ gradient-based optimization algorithms
    to iteratively adjust the network's parameters to minimize the loss function.
    Jax offers a suite of optimizers, including stochastic gradient descent (SGD),
    Adam, and RMSProp, each with its strengths and weaknesses.
  id: totrans-45
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 3\. Optimizers and Learning Rate: Choose an appropriate optimizer and learning
    rate based on the specific task and network architecture. Optimizers like Adam
    often perform well with complex networks and large datasets, while SGD may be
    suitable for simpler models.
  id: totrans-46
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '4\. Training Loop Implementation: Implement a training loop that iteratively
    feeds batches of data to the network, computes the loss, calculates gradients,
    and updates parameters using the chosen optimizer. Monitor the loss over time
    to assess the network''s progress.'
  id: totrans-47
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Key Benefits of Using Jax's Automatic Differentiation
  id: totrans-48
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 1\. Efficient Gradient Computation: Jax's automatic differentiation automatically
    computes gradients, saving time and reducing the risk of errors compared to manual
    gradient calculations.
  id: totrans-49
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 2\. Simplified Training Process: By handling gradient computation, Jax simplifies
    the training process, allowing you to focus on network design and optimization
    strategies.
  id: totrans-50
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '3\. Flexibility and Expressiveness: Jax''s functional programming paradigm
    supports a wide range of network architectures and activation functions, providing
    flexibility in model design.'
  id: totrans-51
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 4\. Reduced Coding Effort: Automatic differentiation reduces the amount of coding
    required for training neural networks, making the process more streamlined.
  id: totrans-52
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 5\. Accelerated Model Development: Automatic differentiation accelerates model
    development by simplifying training and enabling rapid experimentation with different
    network architectures.
  id: totrans-53
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Jax's automatic differentiation capabilities play a crucial role in the development
    and training of neural networks. By automating gradient computation, Jax empowers
    you to focus on the core aspects of neural network design and optimization, enabling
    you to build and train sophisticated models with remarkable efficiency.
  id: totrans-54
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 5.2 Jax's Optimizers
  id: totrans-55
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
- en: Jax provides a suite of powerful optimizers, algorithms designed to iteratively
    adjust the weights and biases of a neural network to minimize the error between
    the network's output and the desired output. These optimizers, such as stochastic
    gradient descent (SGD) and Adam, work in conjunction with automatic differentiation
    to efficiently train neural network models.
  id: totrans-56
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Understanding Optimizer Operation
  id: totrans-57
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 1\. Loss Function: The loss function measures the error between the network's
    output and the desired output. Optimizers strive to minimize this loss by adjusting
    the network's parameters.
  id: totrans-58
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: def `mse_loss(predicted, target):`
  id: totrans-59
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: return `jnp.mean((predicted - target)**2)`
  id: totrans-60
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 2\. Optimizer Selection: Choose an appropriate optimizer based on the network
    architecture, dataset, and task.
  id: totrans-61
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`optimizer = jax.optimizers.adam(0.001)`'
  id: totrans-62
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 3\. Training Loop: Implement the training loop to iteratively feed data, compute
    loss, calculate gradients, and update parameters.
  id: totrans-63
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'for epoch in range(num_epochs):'
  id: totrans-64
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'for batch_x, batch_y in training_data:'
  id: totrans-65
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`predicted = neural_network(batch_x)`'
  id: totrans-66
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`loss = mse_loss(predicted, batch_y)`'
  id: totrans-67
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`grads = jax.grad(mse_loss)(batch_x, predicted)`'
  id: totrans-68
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`opt_state = optimizer.update(grads, opt_state)`'
  id: totrans-69
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`params = optimizer.get_params(opt_state)`'
  id: totrans-70
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Common Jax Optimizers
  id: totrans-71
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 1\. Stochastic Gradient Descent (SGD): A fundamental optimizer that updates
    parameters based on the gradients of a single training example at each iteration.
  id: totrans-72
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: def `sgd_optimizer(params, gradients, learning_rate):`
  id: totrans-73
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: return `[param - learning_rate * grad for param, grad in zip(params, gradients)]`
  id: totrans-74
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 2\. Mini-Batch Gradient Descent: An extension of SGD that updates parameters
    based on the average gradients of a small batch of training examples at each iteration.
  id: totrans-75
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: def `mini_batch_sgd_optimizer(params, gradients, learning_rate):`
  id: totrans-76
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`batch_size = len(gradients)`'
  id: totrans-77
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: return `[param - learning_rate * (sum(grad) / batch_size) for param, grad in
    zip(params, gradients)]`
  id: totrans-78
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 3\. Momentum: Incorporates past gradient updates to accelerate the movement
    in the direction of decreasing error, enhancing convergence.
  id: totrans-79
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: def `momentum_optimizer(params, gradients, learning_rate, momentum_factor, velocities):`
  id: totrans-80
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`updated_velocities = [momentum_factor * vel + learning_rate * grad for vel,
    grad in zip(velocities, gradients)]`'
  id: totrans-81
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`updated_params = [param - vel for param, vel in zip(params, updated_velocities)]`'
  id: totrans-82
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`return updated_params, updated_velocities`'
  id: totrans-83
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 4\. Adaptive Learning Rate: Adjusts the learning rate dynamically based on the
    network's progress, preventing oscillations and overshooting.
  id: totrans-84
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '```def adaptive_learning_rate_optimizer(params, gradients, learning_rate, epsilon):```'
  id: totrans-85
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`squared_gradients = [grad  2 for grad in gradients]`'
  id: totrans-86
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`adjusted_learning_rate = [learning_rate / (jnp.sqrt(squared_grad) + epsilon)
    for squared_grad in squared_gradients]`'
  id: totrans-87
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`updated_params = [param - adj_lr * grad for param, adj_lr, grad in zip(params,
    adjusted_learning_rate, gradients)]`'
  id: totrans-88
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`return updated_params`'
  id: totrans-89
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 5\. Adam: A sophisticated optimizer that combines momentum, adaptive learning
    rate, and bias correction to achieve efficient and stable training.
  id: totrans-90
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '```def adam_optimizer(params, gradients, learning_rate, beta1, beta2, epsilon,
    m, v, t):```'
  id: totrans-91
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`m = [beta1 * m_ + (1 - beta1) * grad for m_, grad in zip(m, gradients)]`'
  id: totrans-92
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`v = [beta2 * v_ + (1 - beta2) * grad2 for v_, grad in zip(v, gradients)]`'
  id: totrans-93
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`m_hat = [m_ / (1 - beta1t) for m_ in m]`'
  id: totrans-94
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`v_hat = [v_ / (1 - beta2t) for v_ in v]`'
  id: totrans-95
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`updated_params = [param - learning_rate * m_h / (jnp.sqrt(v_h) + epsilon)
    for param, m_h, v_h in zip(params, m_hat, v_hat)]`'
  id: totrans-96
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`return updated_params, m, v`'
  id: totrans-97
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Selecting the Optimal Optimizer
  id: totrans-98
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: The choice of optimizer depends on the specific neural network architecture,
    dataset, and task at hand. Experimentation and evaluation are essential to determine
    the optimal optimizer for a given problem.
  id: totrans-99
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Tips for Efficient Training with Jax Optimizers
  id: totrans-100
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 1\. Appropriate Learning Rate: Choose an appropriate learning rate that balances
    speed and stability. Too high a learning rate can lead to oscillations and divergence,
    while too low a learning rate can slow down training.
  id: totrans-101
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 2\. Batch Size Selection: Select an appropriate batch size that balances efficiency
    with gradient estimation accuracy. Larger batches can accelerate training but
    may introduce more noise into gradient estimates.
  id: totrans-102
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 3\. Regularization Techniques: Employ regularization techniques, such as L1
    or L2 regularization, to prevent overfitting and improve generalization.
  id: totrans-103
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 4\. Early Stopping: Utilize early stopping to prevent overfitting and stop training
    when the network's performance on a validation dataset starts to deteriorate.
  id: totrans-104
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 5\. Hyperparameter Optimization: Consider using hyperparameter optimization
    techniques to automatically find the optimal combination of hyperparameters, including
    optimizer parameters and regularization strengths.
  id: totrans-105
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Jax optimizers play a crucial role in training neural network models effectively.
    By leveraging these powerful algorithms, you can efficiently minimize training
    error, improve model generalization, and achieve superior performance for a wide
    range of tasks.
  id: totrans-106
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'Coding Challenge: Implementing an Optimizer in Jax'
  id: totrans-107
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Create a simple optimizer function in Jax for updating the parameters of a neural
    network based on the gradients. Implement a basic version of stochastic gradient
    descent (SGD).
  id: totrans-108
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Requirements
  id: totrans-109
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 1\. Use Jax for tensor operations and automatic differentiation.
  id: totrans-110
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 2\. The optimizer function should take network parameters, gradients, and a
    learning rate as input and return the updated parameters.
  id: totrans-111
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 3\. Implement the optimizer function as a pure function without side effects.
  id: totrans-112
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 4\. Provide a simple example of using your optimizer to update parameters in
    a linear regression setting.
  id: totrans-113
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-114
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: import`jax`
  id: totrans-115
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: import`jax.numpy as jnp`
  id: totrans-116
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: def`sgd_optimizer(params, gradients, learning_rate):`
  id: totrans-117
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`# Your implementation here`'
  id: totrans-118
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`updated_params = [param - learning_rate * grad for param, grad in zip(params,
    gradients)]`'
  id: totrans-119
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`return updated_params`'
  id: totrans-120
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'Example usage:'
  id: totrans-121
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`params = [jnp.array([1.0, 2.0]), jnp.array([3.0])]`'
  id: totrans-122
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`gradients = [jnp.array([0.5, 1.0]), jnp.array([2.0])]`'
  id: totrans-123
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`learning_rate = 0.01`'
  id: totrans-124
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`updated_params = sgd_optimizer(params, gradients, learning_rate)`'
  id: totrans-125
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`print("Updated Parameters:", updated_params)`'
  id: totrans-126
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: Solution
  id: totrans-127
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: import`jax`
  id: totrans-128
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: import`jax.numpy as jnp`
  id: totrans-129
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: def`sgd_optimizer(params, gradients, learning_rate):`
  id: totrans-130
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`updated_params = [param - learning_rate * grad for param, grad in zip(params,
    gradients)]`'
  id: totrans-131
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`return updated_params`'
  id: totrans-132
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'Example usage:'
  id: totrans-133
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
- en: '`params = [jnp.array([1.0, 2.0]), jnp.array([3.0])]`'
  id: totrans-134
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`gradients = [jnp.array([0.5, 1.0]), jnp.array([2.0])]`'
  id: totrans-135
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`learning_rate = 0.01`'
  id: totrans-136
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`updated_params = sgd_optimizer(params, gradients, learning_rate)`'
  id: totrans-137
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: '`print("Updated Parameters:", updated_params)`'
  id: totrans-138
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: This coding challenge aims to test your understanding of implementing a basic
    optimizer in Jax.
  id: totrans-139
  prefs: []
  stylish: true
  type: TYPE_NORMAL
- en: 'As you wrap up this chapter, remember: Jax isn''t just a tool; it''s your ally
    in the realm of deep learning. Armed with functional programming, automatic differentiation,
    and dynamic optimizers, you''re now equipped to transform neural network ideas
    into real-world solutions.'
  id: totrans-140
  prefs: []
  stylish: true
  type: TYPE_NORMAL
