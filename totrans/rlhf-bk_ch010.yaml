- en: Rejection Sampling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Rejection Sampling (RS) is a popular and simple baseline for performing preference
    fine-tuning. This makes it one of a handful of methods that are used after a first
    round of instruction tuning in order to further refine the model to human preferences.
    Rejection sampling operates by curating new candidate completions, filtering them
    based on a trained reward model, and then instruction finetuning the original
    model only on the top completions (same loss function as when doing a dedicated
    training stage for learning to follow instructions).
  prefs: []
  type: TYPE_NORMAL
- en: The name originates from computational statistics [[175]](ch021.xhtml#ref-gilks1992adaptive),
    where one wishes to sample from a complex distribution, but does not have a direct
    method to do so. To alleviate this, one samples from a simpler distribution to
    model and uses a heuristic to check if the sample is permissible. With language
    models, the target distribution is high-quality completions to prompts, the filter
    is a reward model, and the sampling distribution is the current model.
  prefs: []
  type: TYPE_NORMAL
- en: Many prominent RLHF and preference fine-tuning papers have used rejection sampling
    as a baseline, but a canonical implementation and documentation does not exist.
  prefs: []
  type: TYPE_NORMAL
- en: WebGPT [[4]](ch021.xhtml#ref-nakano2021webgpt), Anthropic‚Äôs Helpful and Harmless
    agent [[5]](ch021.xhtml#ref-bai2022training), OpenAI‚Äôs popular paper on process
    reward models [[45]](ch021.xhtml#ref-lightman2023let), Llama 2 Chat models [[44]](ch021.xhtml#ref-touvron2023llama),
    and other seminal works all use this baseline; more recent work has formalized
    it directly (e.g., RAFT [[176]](ch021.xhtml#ref-dong2023raft) for applying it
    to alignment in multiple modalities and Statistical Rejection Sampling Optimization
    (RSO) [[177]](ch021.xhtml#ref-liu2023statistical) that gives a principled overview
    on how rejection sampling relates to other preference learning objectives).
  prefs: []
  type: TYPE_NORMAL
- en: '*Throughout this chapter, we use <semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics>
    to denote prompts and <semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics>
    to denote completions. This notation is common in the language model literature,
    where methods operate on full prompt-completion pairs rather than individual tokens.*'
  prefs: []
  type: TYPE_NORMAL
- en: Training Process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Rejection sampling overall follows a few stages.
  prefs: []
  type: TYPE_NORMAL
- en: '**Prompt and reward model selection:** First, you must select the prompts you
    want to train on, relative to other stages of training. The simplest method is
    to re-use every prompt from the first SFT/IFT stage, but this can cause some overfitting.
    Before doing rejection sampling, you must also have trained a reward model (see
    Chapter 7 for more information).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Generate completions from the starting checkpoint:** Next, one must generate
    completions to the selected prompts with the model they want to optimize. This
    can involve tweaking many settings, such as sampling temperature, top-p, max sequence
    length, number of completions per prompt, etc.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Select top completions with a reward model**: All completions are ranked
    by a reward model. This can include deduplication to only have one prompt per
    completion after this stage, or not, as a lot of the decisions become based on
    empirical ablation studies.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**SFT on top completions:** To finish rejection sampling, one instruction finetunes
    the starting checkpoint on the selected completions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A visual overview of the rejection sampling process is included below in fig.¬†[14](#fig:rs-overview).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14: Rejection sampling overview.](../media/file12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: Rejection sampling overview.'
  prefs: []
  type: TYPE_NORMAL
- en: The actual details on which prompts to use, how to select a reward model, how
    to sequence rejection sampling, etc. are not well documented in the literature.
    This chapter provides an overview of the methods and leaves further experimentation
    to the reader.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Generating Completions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To generate a set of multiple candidate completions per prompt, let‚Äôs define
    a set of <semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics>
    prompts as a vector:'
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><mi>X</mi><mo>=</mo><mo stretchy="false" form="prefix">[</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi><mo>,</mo><msub><mi>x</mi><mi>M</mi></msub><mo
    stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">X
    = [x_1, x_2, ..., x_M]</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: These prompts can come from many sources, but most commonly they come from the
    instruction training set.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each prompt <semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding="application/x-tex">x_i</annotation></semantics>,
    we generate <semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics>
    completions. We can represent this as a matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '<semantics><mrow><mi>Y</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd
    columnalign="center" style="text-align: center"><msub><mi>y</mi><mrow><mn>1</mn><mo>,</mo><mn>1</mn></mrow></msub></mtd><mtd
    columnalign="center" style="text-align: center"><msub><mi>y</mi><mrow><mn>1</mn><mo>,</mo><mn>2</mn></mrow></msub></mtd><mtd
    columnalign="center" style="text-align: center"><mi>‚ãØ</mi></mtd><mtd columnalign="center"
    style="text-align: center"><msub><mi>y</mi><mrow><mn>1</mn><mo>,</mo><mi>N</mi></mrow></msub></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><msub><mi>y</mi><mrow><mn>2</mn><mo>,</mo><mn>1</mn></mrow></msub></mtd><mtd
    columnalign="center" style="text-align: center"><msub><mi>y</mi><mrow><mn>2</mn><mo>,</mo><mn>2</mn></mrow></msub></mtd><mtd
    columnalign="center" style="text-align: center"><mi>‚ãØ</mi></mtd><mtd columnalign="center"
    style="text-align: center"><msub><mi>y</mi><mrow><mn>2</mn><mo>,</mo><mi>N</mi></mrow></msub></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><mi>‚ãÆ</mi></mtd><mtd columnalign="center"
    style="text-align: center"><mi>‚ãÆ</mi></mtd><mtd columnalign="center" style="text-align:
    center"><mo>‚ã±</mo></mtd><mtd columnalign="center" style="text-align: center"><mi>‚ãÆ</mi></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><msub><mi>y</mi><mrow><mi>M</mi><mo>,</mo><mn>1</mn></mrow></msub></mtd><mtd
    columnalign="center" style="text-align: center"><msub><mi>y</mi><mrow><mi>M</mi><mo>,</mo><mn>2</mn></mrow></msub></mtd><mtd
    columnalign="center" style="text-align: center"><mi>‚ãØ</mi></mtd><mtd columnalign="center"
    style="text-align: center"><msub><mi>y</mi><mrow><mi>M</mi><mo>,</mo><mi>N</mi></mrow></msub></mtd></mtr></mtable><mo
    stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">Y
    = \begin{bmatrix} y_{1,1} & y_{1,2} & \cdots & y_{1,N} \\ y_{2,1} & y_{2,2} &
    \cdots & y_{2,N} \\ \vdots & \vdots & \ddots & \vdots \\ y_{M,1} & y_{M,2} & \cdots
    & y_{M,N} \end{bmatrix}</annotation></semantics>'
  prefs: []
  type: TYPE_NORMAL
- en: where <semantics><msub><mi>y</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><annotation
    encoding="application/x-tex">y_{i,j}</annotation></semantics> represents the <semantics><mi>j</mi><annotation
    encoding="application/x-tex">j</annotation></semantics>-th completion for the
    <semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics>-th
    prompt. Each row <semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics>
    corresponds to a single prompt <semantics><msub><mi>x</mi><mi>i</mi></msub><annotation
    encoding="application/x-tex">x_i</annotation></semantics> and contains its <semantics><mi>N</mi><annotation
    encoding="application/x-tex">N</annotation></semantics> candidate completions;
    each column <semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics>
    corresponds to the <semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics>-th
    sampled completion across all prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Scoring Completions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now, we pass all of these prompt-completion pairs through a reward model, to
    get a matrix of rewards. We‚Äôll represent the rewards as a matrix <semantics><mi>R</mi><annotation
    encoding="application/x-tex">R</annotation></semantics>:'
  prefs: []
  type: TYPE_NORMAL
- en: '<semantics><mrow><mi>R</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd
    columnalign="center" style="text-align: center"><msub><mi>r</mi><mrow><mn>1</mn><mo>,</mo><mn>1</mn></mrow></msub></mtd><mtd
    columnalign="center" style="text-align: center"><msub><mi>r</mi><mrow><mn>1</mn><mo>,</mo><mn>2</mn></mrow></msub></mtd><mtd
    columnalign="center" style="text-align: center"><mi>‚ãØ</mi></mtd><mtd columnalign="center"
    style="text-align: center"><msub><mi>r</mi><mrow><mn>1</mn><mo>,</mo><mi>N</mi></mrow></msub></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><msub><mi>r</mi><mrow><mn>2</mn><mo>,</mo><mn>1</mn></mrow></msub></mtd><mtd
    columnalign="center" style="text-align: center"><msub><mi>r</mi><mrow><mn>2</mn><mo>,</mo><mn>2</mn></mrow></msub></mtd><mtd
    columnalign="center" style="text-align: center"><mi>‚ãØ</mi></mtd><mtd columnalign="center"
    style="text-align: center"><msub><mi>r</mi><mrow><mn>2</mn><mo>,</mo><mi>N</mi></mrow></msub></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><mi>‚ãÆ</mi></mtd><mtd columnalign="center"
    style="text-align: center"><mi>‚ãÆ</mi></mtd><mtd columnalign="center" style="text-align:
    center"><mo>‚ã±</mo></mtd><mtd columnalign="center" style="text-align: center"><mi>‚ãÆ</mi></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><msub><mi>r</mi><mrow><mi>M</mi><mo>,</mo><mn>1</mn></mrow></msub></mtd><mtd
    columnalign="center" style="text-align: center"><msub><mi>r</mi><mrow><mi>M</mi><mo>,</mo><mn>2</mn></mrow></msub></mtd><mtd
    columnalign="center" style="text-align: center"><mi>‚ãØ</mi></mtd><mtd columnalign="center"
    style="text-align: center"><msub><mi>r</mi><mrow><mi>M</mi><mo>,</mo><mi>N</mi></mrow></msub></mtd></mtr></mtable><mo
    stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">R
    = \begin{bmatrix} r_{1,1} & r_{1,2} & \cdots & r_{1,N} \\ r_{2,1} & r_{2,2} &
    \cdots & r_{2,N} \\ \vdots & \vdots & \ddots & \vdots \\ r_{M,1} & r_{M,2} & \cdots
    & r_{M,N} \end{bmatrix}</annotation></semantics>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each reward <semantics><msub><mi>r</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><annotation
    encoding="application/x-tex">r_{i,j}</annotation></semantics> is computed by passing
    the completion <semantics><msub><mi>y</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><annotation
    encoding="application/x-tex">y_{i,j}</annotation></semantics> and its corresponding
    prompt <semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding="application/x-tex">x_i</annotation></semantics>
    through a reward model <semantics><mi>‚Ñõ</mi><annotation encoding="application/x-tex">\mathcal{R}</annotation></semantics>:'
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><msub><mi>r</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><mi>‚Ñõ</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>‚à£</mo><msub><mi>x</mi><mi>i</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">r_{i,j}
    = \mathcal{R}(y_{i,j} \mid x_i)</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple methods to select the top completions to train on.
  prefs: []
  type: TYPE_NORMAL
- en: To formalize the process of selecting the best completions based on our reward
    matrix, we can define a selection function <semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics>
    that operates on the reward matrix <semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics>.
  prefs: []
  type: TYPE_NORMAL
- en: Top Per Prompt
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The first potential selection function takes the max reward per prompt.
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><mi>S</mi><mo stretchy="false" form="prefix">(</mo><mi>R</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mo stretchy="false" form="prefix">[</mo><mrow><mi
    mathvariant="normal">arg</mi><mo>‚Å°</mo></mrow><munder><mi mathvariant="normal">max</mi><mi>j</mi></munder><msub><mi>r</mi><mrow><mn>1</mn><mo>,</mo><mi>j</mi></mrow></msub><mo>,</mo><mrow><mi
    mathvariant="normal">arg</mi><mo>‚Å°</mo></mrow><munder><mi mathvariant="normal">max</mi><mi>j</mi></munder><msub><mi>r</mi><mrow><mn>2</mn><mo>,</mo><mi>j</mi></mrow></msub><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi><mo>,</mo><mrow><mi
    mathvariant="normal">arg</mi><mo>‚Å°</mo></mrow><munder><mi mathvariant="normal">max</mi><mi>j</mi></munder><msub><mi>r</mi><mrow><mi>M</mi><mo>,</mo><mi>j</mi></mrow></msub><mo
    stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">S(R)
    = [\arg\max_{j} r_{1,j}, \arg\max_{j} r_{2,j}, ..., \arg\max_{j} r_{M,j}]</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: 'This function <semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics>
    returns a vector of indices, where each index corresponds to the column with the
    maximum reward for each row in <semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics>.
    We can then use these indices to select our chosen completions:'
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><msub><mi>Y</mi><mrow><mi>c</mi><mi>h</mi><mi>o</mi><mi>s</mi><mi>e</mi><mi>n</mi></mrow></msub><mo>=</mo><mo
    stretchy="false" form="prefix">[</mo><msub><mi>y</mi><mrow><mn>1</mn><mo>,</mo><mi>S</mi><mo
    stretchy="false" form="prefix">(</mo><mi>R</mi><msub><mo stretchy="false" form="postfix">)</mo><mn>1</mn></msub></mrow></msub><mo>,</mo><msub><mi>y</mi><mrow><mn>2</mn><mo>,</mo><mi>S</mi><mo
    stretchy="false" form="prefix">(</mo><mi>R</mi><msub><mo stretchy="false" form="postfix">)</mo><mn>2</mn></msub></mrow></msub><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi><mo>,</mo><msub><mi>y</mi><mrow><mi>M</mi><mo>,</mo><mi>S</mi><mo
    stretchy="false" form="prefix">(</mo><mi>R</mi><msub><mo stretchy="false" form="postfix">)</mo><mi>M</mi></msub></mrow></msub><mo
    stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">Y_{chosen}
    = [y_{1,S(R)_1}, y_{2,S(R)_2}, ..., y_{M,S(R)_M}]</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: Top Overall Pairs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Alternatively, we can select the top K prompt-completion pairs from the entire
    set. First, let‚Äôs flatten our reward matrix R into a single vector:'
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><msub><mi>R</mi><mrow><mi>f</mi><mi>l</mi><mi>a</mi><mi>t</mi></mrow></msub><mo>=</mo><mo
    stretchy="false" form="prefix">[</mo><msub><mi>r</mi><mrow><mn>1</mn><mo>,</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>r</mi><mrow><mn>1</mn><mo>,</mo><mn>2</mn></mrow></msub><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi><mo>,</mo><msub><mi>r</mi><mrow><mn>1</mn><mo>,</mo><mi>N</mi></mrow></msub><mo>,</mo><msub><mi>r</mi><mrow><mn>2</mn><mo>,</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>r</mi><mrow><mn>2</mn><mo>,</mo><mn>2</mn></mrow></msub><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi><mo>,</mo><msub><mi>r</mi><mrow><mn>2</mn><mo>,</mo><mi>N</mi></mrow></msub><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi><mo>,</mo><msub><mi>r</mi><mrow><mi>M</mi><mo>,</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>r</mi><mrow><mi>M</mi><mo>,</mo><mn>2</mn></mrow></msub><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi><mo>,</mo><msub><mi>r</mi><mrow><mi>M</mi><mo>,</mo><mi>N</mi></mrow></msub><mo
    stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">R_{flat}
    = [r_{1,1}, r_{1,2}, ..., r_{1,N}, r_{2,1}, r_{2,2}, ..., r_{2,N}, ..., r_{M,1},
    r_{M,2}, ..., r_{M,N}]</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: This <semantics><msub><mi>R</mi><mrow><mi>f</mi><mi>l</mi><mi>a</mi><mi>t</mi></mrow></msub><annotation
    encoding="application/x-tex">R_{flat}</annotation></semantics> vector has length
    <semantics><mrow><mi>M</mi><mo>√ó</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">M
    \times N</annotation></semantics>, where <semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics>
    is the number of prompts and <semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics>
    is the number of completions per prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can define a selection function <semantics><msub><mi>S</mi><mi>K</mi></msub><annotation
    encoding="application/x-tex">S_K</annotation></semantics> that selects the indices
    of the K highest values in <semantics><msub><mi>R</mi><mrow><mi>f</mi><mi>l</mi><mi>a</mi><mi>t</mi></mrow></msub><annotation
    encoding="application/x-tex">R_{flat}</annotation></semantics>:'
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><msub><mi>S</mi><mi>K</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>R</mi><mrow><mi>f</mi><mi>l</mi><mi>a</mi><mi>t</mi></mrow></msub><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mtext mathvariant="normal">argsort</mtext><mo
    stretchy="false" form="prefix">(</mo><msub><mi>R</mi><mrow><mi>f</mi><mi>l</mi><mi>a</mi><mi>t</mi></mrow></msub><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="false" form="prefix">[</mo><mi>‚àí</mi><mi>K</mi><mo>:</mo><mo
    stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">S_K(R_{flat})
    = \text{argsort}(R_{flat})[-K:]</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: where <semantics><mtext mathvariant="normal">argsort</mtext><annotation encoding="application/x-tex">\text{argsort}</annotation></semantics>
    returns the indices that would sort the array in ascending order, and we take
    the last K indices to get the K highest values.
  prefs: []
  type: TYPE_NORMAL
- en: To get our selected completions, we need to map these flattened indices back
    to our original completion matrix <semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics>.
    To recover the corresponding (prompt, completion) pair, you can map a zero-indexed
    flattened index <semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>
    to <semantics><mrow><mo stretchy="false" form="prefix">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(i,j)</annotation></semantics>
    via <semantics><mrow><mi>i</mi><mo>=</mo><mo stretchy="false" form="prefix">‚åä</mo><mi>k</mi><mi>/</mi><mi>N</mi><mo
    stretchy="false" form="postfix">‚åã</mo><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">i
    = \lfloor k / N \rfloor + 1</annotation></semantics> and <semantics><mrow><mi>j</mi><mo>=</mo><mo
    stretchy="false" form="prefix">(</mo><mi>k</mi><mrow><mrow><mi mathvariant="normal">mod</mi><mo>‚Å°</mo></mrow><mi>N</mi></mrow><mo
    stretchy="false" form="postfix">)</mo><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">j
    = (k \bmod N) + 1</annotation></semantics>.
  prefs: []
  type: TYPE_NORMAL
- en: Selection Example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Consider the case where we have the following situation, with 5 prompts and
    4 completions. We will show two ways of selecting the completions based on reward.
  prefs: []
  type: TYPE_NORMAL
- en: '<semantics><mrow><mi>R</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd
    columnalign="center" style="text-align: center"><mn>0.7</mn></mtd><mtd columnalign="center"
    style="text-align: center"><mn>0.3</mn></mtd><mtd columnalign="center" style="text-align:
    center"><mn>0.5</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.2</mn></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><mn>0.4</mn></mtd><mtd columnalign="center"
    style="text-align: center"><mn>0.8</mn></mtd><mtd columnalign="center" style="text-align:
    center"><mn>0.6</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.5</mn></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><mn>0.9</mn></mtd><mtd columnalign="center"
    style="text-align: center"><mn>0.3</mn></mtd><mtd columnalign="center" style="text-align:
    center"><mn>0.4</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.7</mn></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><mn>0.2</mn></mtd><mtd columnalign="center"
    style="text-align: center"><mn>0.5</mn></mtd><mtd columnalign="center" style="text-align:
    center"><mn>0.8</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.6</mn></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><mn>0.5</mn></mtd><mtd columnalign="center"
    style="text-align: center"><mn>0.4</mn></mtd><mtd columnalign="center" style="text-align:
    center"><mn>0.3</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.6</mn></mtd></mtr></mtable><mo
    stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">R
    = \begin{bmatrix} 0.7 & 0.3 & 0.5 & 0.2 \\ 0.4 & 0.8 & 0.6 & 0.5 \\ 0.9 & 0.3
    & 0.4 & 0.7 \\ 0.2 & 0.5 & 0.8 & 0.6 \\ 0.5 & 0.4 & 0.3 & 0.6 \end{bmatrix}</annotation></semantics>'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, **per prompt**. Intuitively, we can highlight the reward matrix as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '<semantics><mrow><mi>R</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd
    columnalign="center" style="text-align: center"><mtext mathvariant="bold">ùüé.ùüï</mtext></mtd><mtd
    columnalign="center" style="text-align: center"><mn>0.3</mn></mtd><mtd columnalign="center"
    style="text-align: center"><mn>0.5</mn></mtd><mtd columnalign="center" style="text-align:
    center"><mn>0.2</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align:
    center"><mn>0.4</mn></mtd><mtd columnalign="center" style="text-align: center"><mtext
    mathvariant="bold">ùüé.ùüñ</mtext></mtd><mtd columnalign="center" style="text-align:
    center"><mn>0.6</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.5</mn></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><mtext mathvariant="bold">ùüé.ùüó</mtext></mtd><mtd
    columnalign="center" style="text-align: center"><mn>0.3</mn></mtd><mtd columnalign="center"
    style="text-align: center"><mn>0.4</mn></mtd><mtd columnalign="center" style="text-align:
    center"><mn>0.7</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align:
    center"><mn>0.2</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.5</mn></mtd><mtd
    columnalign="center" style="text-align: center"><mtext mathvariant="bold">ùüé.ùüñ</mtext></mtd><mtd
    columnalign="center" style="text-align: center"><mn>0.6</mn></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><mn>0.5</mn></mtd><mtd columnalign="center"
    style="text-align: center"><mn>0.4</mn></mtd><mtd columnalign="center" style="text-align:
    center"><mn>0.3</mn></mtd><mtd columnalign="center" style="text-align: center"><mtext
    mathvariant="bold">ùüé.ùüî</mtext></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation
    encoding="application/x-tex">R = \begin{bmatrix} \textbf{0.7} & 0.3 & 0.5 & 0.2
    \\ 0.4 & \textbf{0.8} & 0.6 & 0.5 \\ \textbf{0.9} & 0.3 & 0.4 & 0.7 \\ 0.2 & 0.5
    & \textbf{0.8} & 0.6 \\ 0.5 & 0.4 & 0.3 & \textbf{0.6} \end{bmatrix}</annotation></semantics>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the argmax method, we select the best completion for each prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><mi>S</mi><mo stretchy="false" form="prefix">(</mo><mi>R</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mo stretchy="false" form="prefix">[</mo><mrow><mi
    mathvariant="normal">arg</mi><mo>‚Å°</mo></mrow><munder><mi mathvariant="normal">max</mi><mi>j</mi></munder><msub><mi>r</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mrow><mtext mathvariant="normal">for</mtext></mrow> <mi>i</mi><mo>‚àà</mo><mo stretchy="false"
    form="prefix">[</mo><mn>1</mn><mo>,</mo><mn>5</mn><mo stretchy="false" form="postfix">]</mo><mo
    stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">S(R)
    = [\arg\max_{j} r_{i,j} \text{ for } i \in [1,5]]</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><mi>S</mi><mo stretchy="false" form="prefix">(</mo><mi>R</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mo stretchy="false" form="prefix">[</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>3</mn><mo>,</mo><mn>4</mn><mo
    stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">S(R)
    = [1, 2, 1, 3, 4]</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: 'This means we would select:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For prompt 1: completion 1 (reward 0.7)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For prompt 2: completion 2 (reward 0.8)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For prompt 3: completion 1 (reward 0.9)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For prompt 4: completion 3 (reward 0.8)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For prompt 5: completion 4 (reward 0.6)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, **best overall**. Let‚Äôs highlight the top 5 overall completion pairs.
  prefs: []
  type: TYPE_NORMAL
- en: '<semantics><mrow><mi>R</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd
    columnalign="center" style="text-align: center"><mtext mathvariant="bold">ùüé.ùüï</mtext></mtd><mtd
    columnalign="center" style="text-align: center"><mn>0.3</mn></mtd><mtd columnalign="center"
    style="text-align: center"><mn>0.5</mn></mtd><mtd columnalign="center" style="text-align:
    center"><mn>0.2</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align:
    center"><mn>0.4</mn></mtd><mtd columnalign="center" style="text-align: center"><mtext
    mathvariant="bold">ùüé.ùüñ</mtext></mtd><mtd columnalign="center" style="text-align:
    center"><mn>0.6</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.5</mn></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><mtext mathvariant="bold">ùüé.ùüó</mtext></mtd><mtd
    columnalign="center" style="text-align: center"><mn>0.3</mn></mtd><mtd columnalign="center"
    style="text-align: center"><mn>0.4</mn></mtd><mtd columnalign="center" style="text-align:
    center"><mtext mathvariant="bold">ùüé.ùüï</mtext></mtd></mtr><mtr><mtd columnalign="center"
    style="text-align: center"><mn>0.2</mn></mtd><mtd columnalign="center" style="text-align:
    center"><mn>0.5</mn></mtd><mtd columnalign="center" style="text-align: center"><mtext
    mathvariant="bold">ùüé.ùüñ</mtext></mtd><mtd columnalign="center" style="text-align:
    center"><mn>0.6</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align:
    center"><mn>0.5</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.4</mn></mtd><mtd
    columnalign="center" style="text-align: center"><mn>0.3</mn></mtd><mtd columnalign="center"
    style="text-align: center"><mn>0.6</mn></mtd></mtr></mtable><mo stretchy="true"
    form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">R
    = \begin{bmatrix} \textbf{0.7} & 0.3 & 0.5 & 0.2 \\ 0.4 & \textbf{0.8} & 0.6 &
    0.5 \\ \textbf{0.9} & 0.3 & 0.4 & \textbf{0.7} \\ 0.2 & 0.5 & \textbf{0.8} & 0.6
    \\ 0.5 & 0.4 & 0.3 & 0.6 \end{bmatrix}</annotation></semantics>'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we flatten the reward matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><msub><mi>R</mi><mrow><mi>f</mi><mi>l</mi><mi>a</mi><mi>t</mi></mrow></msub><mo>=</mo><mo
    stretchy="false" form="prefix">[</mo><mn>0.7</mn><mo>,</mo><mn>0.3</mn><mo>,</mo><mn>0.5</mn><mo>,</mo><mn>0.2</mn><mo>,</mo><mn>0.4</mn><mo>,</mo><mn>0.8</mn><mo>,</mo><mn>0.6</mn><mo>,</mo><mn>0.5</mn><mo>,</mo><mn>0.9</mn><mo>,</mo><mn>0.3</mn><mo>,</mo><mn>0.4</mn><mo>,</mo><mn>0.7</mn><mo>,</mo><mn>0.2</mn><mo>,</mo><mn>0.5</mn><mo>,</mo><mn>0.8</mn><mo>,</mo><mn>0.6</mn><mo>,</mo><mn>0.5</mn><mo>,</mo><mn>0.4</mn><mo>,</mo><mn>0.3</mn><mo>,</mo><mn>0.6</mn><mo
    stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">R_{flat}
    = [0.7, 0.3, 0.5, 0.2, 0.4, 0.8, 0.6, 0.5, 0.9, 0.3, 0.4, 0.7, 0.2, 0.5, 0.8,
    0.6, 0.5, 0.4, 0.3, 0.6]</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we select the indices of the 5 highest values: <semantics><mrow><msub><mi>S</mi><mn>5</mn></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>R</mi><mrow><mi>f</mi><mi>l</mi><mi>a</mi><mi>t</mi></mrow></msub><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mo stretchy="false" form="prefix">[</mo><mn>8</mn><mo>,</mo><mn>5</mn><mo>,</mo><mn>14</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>11</mn><mo
    stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">S_5(R_{flat})
    = [8, 5, 14, 0, 11]</annotation></semantics>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mapping these back to our original matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: Index 8 ‚Üí prompt 3, completion 1 (reward 0.9)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Index 5 ‚Üí prompt 2, completion 2 (reward 0.8)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Index 14 ‚Üí prompt 4, completion 3 (reward 0.8)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Index 0 ‚Üí prompt 1, completion 1 (reward 0.7)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Index 11 ‚Üí prompt 3, completion 4 (reward 0.7)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation Example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Here is a code snippet showing how the selection methods could be implemented.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 3\. Fine-tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the selected completions, you then perform standard instruction fine-tuning
    on the current rendition of the model. More details can be found in the [chapter
    on instruction tuning](https://rlhfbook.com/c/instructions).
  prefs: []
  type: TYPE_NORMAL
- en: Implementation Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The core hyperparameters for performing this training are very intuitive:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sampling parameters**: Rejection sampling is directly dependent on the completions
    received from the model. Common settings for rejection sampling include temperatures
    above zero, e.g.¬†between 0.7 and 1.0, with other modifications to parameters such
    as top-p or top-k sampling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Completions per prompt**: Successful implementations of rejection sampling
    have included 10 to 30 or more completions for each prompt. Using too few completions
    will make training biased and/or noisy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Instruction tuning details**: No clear training details for the instruction
    tuning during rejection sampling have been released. It is likely that they use
    slightly different settings than the initial instruction tuning phase of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Heterogeneous model generations**: Some implementations of rejection sampling
    include generations from multiple models rather than just the current model that
    is going to be trained. Best practices on how to do this are not established.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reward model training**: The reward model used will heavily impact the final
    result. For more resources on reward model training, see the [relevant chapter](https://rlhfbook.com/c/07-reward-models).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When doing batch reward model inference, you can sort the tokenized completions
    by length so that the batches are of similar lengths. This eliminates the need
    to run inference on as many padding tokens and will improve throughput in exchange
    for minor implementation complexity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Related: Best-of-N Sampling'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Best-of-N (BoN) is a close relative of rejection sampling, where the same generate-and-score
    procedure is followed, but you do **not** fine-tune the model on the selected
    completions. Instead, BoN is a way of computing a best possible completion to
    a static prompt (or set of prompts) at inference time, and related techniques
    are often used in ‚ÄúPro‚Äù tiers of chat models that spend extra compute to get an
    answer to your query.
  prefs: []
  type: TYPE_NORMAL
- en: Best-of-N sampling is often included as a baseline relative to RLHF training
    methods. It is important to remember that BoN *does not* modify the underlying
    model, but is a sampling technique. For this reason, comparisons for BoN sampling
    to online training methods, such as PPO, are still valid in some contexts. For
    example, you can still measure the KL distance when running BoN sampling relative
    to any other policy.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we will show that when using simple BoN sampling over one prompt, both
    selection criteria shown above are equivalent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let R be a reward vector for our single prompt with N completions:'
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><mi>R</mi><mo>=</mo><mo stretchy="false" form="prefix">[</mo><msub><mi>r</mi><mn>1</mn></msub><mo>,</mo><msub><mi>r</mi><mn>2</mn></msub><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi><mo>,</mo><msub><mi>r</mi><mi>N</mi></msub><mo
    stretchy="false" form="postfix">]</mo><mrow><mo stretchy="false" form="prefix">(</mo><mn>31</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">R
    = [r_1, r_2, ..., r_N]\qquad{(31)}</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: Where <semantics><msub><mi>r</mi><mi>j</mi></msub><annotation encoding="application/x-tex">r_j</annotation></semantics>
    represents the reward for the j-th completion.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the argmax method, we select the best completion for the prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><mi>S</mi><mo stretchy="false" form="prefix">(</mo><mi>R</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mrow><mi mathvariant="normal">arg</mi><mo>‚Å°</mo></mrow><munder><mi
    mathvariant="normal">max</mi><mrow><mi>j</mi><mo>‚àà</mo><mo stretchy="false" form="prefix">[</mo><mn>1</mn><mo>,</mo><mi>N</mi><mo
    stretchy="false" form="postfix">]</mo></mrow></munder><msub><mi>r</mi><mi>j</mi></msub><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>32</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">S(R) = \arg\max_{j \in [1,N]} r_j\qquad{(32)}</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: Using the Top-K method with <semantics><mrow><mi>K</mi><mo>=</mo><mn>1</mn></mrow><annotation
    encoding="application/x-tex">K=1</annotation></semantics> reduces to the same
    method, which is common practice.
  prefs: []
  type: TYPE_NORMAL
