<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>1.4 Advanced Machine Reasoning: Strategies</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>1.4 Advanced Machine Reasoning: Strategies</h1>
<blockquote>原文：<a href="https://boramorka.github.io/LLM-Book/en/CHAPTER-1/1.4%20Advanced%20Machine%20Reasoning/">https://boramorka.github.io/LLM-Book/en/CHAPTER-1/1.4%20Advanced%20Machine%20Reasoning/</a></blockquote>
                
                  


  
  



<p>Advanced machine reasoning brings together a set of practices that help language models solve complex tasks more reliably and transparently. Chain of Thought (CoT) encourages step‑by‑step solutions, breaking the problem into logical stages. This approach improves accuracy and makes the reasoning auditable: the user can see how the model arrived at the answer, which is especially helpful for multi‑constraint tasks, comparative analysis, and calculations. In education, CoT mimics a tutor who guides you through each step rather than giving a finished answer. In customer support, it helps unpack complicated requests: clarify details, check assumptions, fix misunderstandings, and provide a correct conclusion.</p>
<p>In parallel with CoT, teams often use Inner Monologue, where intermediate reasoning is hidden and only the result (or a minimal slice of logic) is shown. This is appropriate when exposing internal steps could harm learning (avoiding “spoilers”), when sensitive information is involved, or when extra details would degrade the user experience.</p>
<p>To make the examples reproducible, start by preparing the environment and API client.</p>
<div class="highlight"><pre><span/><code><span class="c1"># Import libraries and load keys</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">dotenv</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dotenv</span><span class="p">,</span> <span class="n">find_dotenv</span>

<span class="n">load_dotenv</span><span class="p">(</span><span class="n">find_dotenv</span><span class="p">())</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">()</span>
</code></pre></div>
<div class="highlight"><pre><span/><code><span class="k">def</span><span class="w"> </span><span class="nf">get_response_for_queries</span><span class="p">(</span><span class="n">query_prompts</span><span class="p">,</span>
                             <span class="n">model_name</span><span class="o">=</span><span class="s2">"gpt-4o-mini"</span><span class="p">,</span>
                             <span class="n">response_temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                             <span class="n">max_response_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Returns the model response based on a list of messages (system/user...).</span>
<span class="sd">    """</span>
    <span class="n">model_response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="n">query_prompts</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">response_temperature</span><span class="p">,</span>
        <span class="n">max_tokens</span><span class="o">=</span><span class="n">max_response_tokens</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">model_response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span>
</code></pre></div>
<p>Next, we’ll set up a wrapper for requests and move to CoT prompting, where the reasoning is structured into steps under a special delimiter. The system message describes the analysis stages, and the user input is wrapped in delimiters, simplifying parsing and later post‑processing.</p>
<div class="highlight"><pre><span/><code><span class="n">step_delimiter</span> <span class="o">=</span> <span class="s2">"####"</span>

<span class="n">system_prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"""</span>
<span class="s2">Follow the steps, separating them with the '</span><span class="si">{</span><span class="n">step_delimiter</span><span class="si">}</span><span class="s2">' marker.</span>

<span class="s2">Step 1:</span><span class="si">{</span><span class="n">step_delimiter</span><span class="si">}</span><span class="s2"> Check whether the question is about a specific product (not a category).</span>

<span class="s2">Step 2:</span><span class="si">{</span><span class="n">step_delimiter</span><span class="si">}</span><span class="s2"> If yes, match it to the product list (brand, specs, price).</span>

<span class="s2">[Insert your product list here]</span>

<span class="s2">Step 3:</span><span class="si">{</span><span class="n">step_delimiter</span><span class="si">}</span><span class="s2"> Identify the user’s assumptions (comparisons/specifications).</span>

<span class="s2">Step 4:</span><span class="si">{</span><span class="n">step_delimiter</span><span class="si">}</span><span class="s2"> Verify those assumptions against the product data.</span>

<span class="s2">Step 5:</span><span class="si">{</span><span class="n">step_delimiter</span><span class="si">}</span><span class="s2"> Correct inaccuracies using only the list and respond politely.</span>
<span class="s2">"""</span>

<span class="n">example_query_1</span> <span class="o">=</span> <span class="s2">"How does the BlueWave Chromebook compare to the TechPro Desktop in terms of cost?"</span>
<span class="n">example_query_2</span> <span class="o">=</span> <span class="s2">"Are televisions available for sale?"</span>

<span class="n">query_prompts_1</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s1">'role'</span><span class="p">:</span> <span class="s1">'system'</span><span class="p">,</span> <span class="s1">'content'</span><span class="p">:</span> <span class="n">system_prompt</span><span class="p">},</span>
    <span class="p">{</span><span class="s1">'role'</span><span class="p">:</span> <span class="s1">'user'</span><span class="p">,</span> <span class="s1">'content'</span><span class="p">:</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">step_delimiter</span><span class="si">}{</span><span class="n">example_query_1</span><span class="si">}{</span><span class="n">step_delimiter</span><span class="si">}</span><span class="s2">"</span><span class="p">},</span>
<span class="p">]</span>

<span class="n">query_prompts_2</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s1">'role'</span><span class="p">:</span> <span class="s1">'system'</span><span class="p">,</span> <span class="s1">'content'</span><span class="p">:</span> <span class="n">system_prompt</span><span class="p">},</span>
    <span class="p">{</span><span class="s1">'role'</span><span class="p">:</span> <span class="s1">'user'</span><span class="p">,</span> <span class="s1">'content'</span><span class="p">:</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">step_delimiter</span><span class="si">}{</span><span class="n">example_query_2</span><span class="si">}{</span><span class="n">step_delimiter</span><span class="si">}</span><span class="s2">"</span><span class="p">},</span>
<span class="p">]</span>
</code></pre></div>
<div class="highlight"><pre><span/><code><span class="n">response_to_query_1</span> <span class="o">=</span> <span class="n">get_response_for_queries</span><span class="p">(</span><span class="n">query_prompts_1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response_to_query_1</span><span class="p">)</span>

<span class="n">response_to_query_2</span> <span class="o">=</span> <span class="n">get_response_for_queries</span><span class="p">(</span><span class="n">query_prompts_2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response_to_query_2</span><span class="p">)</span>
</code></pre></div>
<p>To compare approaches, first print the full answer with intermediate CoT steps, then apply an Inner Monologue variant where only the final portion is shown to the user. If the model returns text with steps separated by <code>step_delimiter</code>, you can keep just the final segment — keeping the interface succinct where the “inner workings” aren’t needed.</p>
<div class="highlight"><pre><span/><code><span class="k">try</span><span class="p">:</span>
    <span class="n">final_response</span> <span class="o">=</span> <span class="n">response_to_query_2</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">step_delimiter</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">final_response</span> <span class="o">=</span> <span class="s2">"Sorry, there was a problem. Please try another question."</span>

<span class="nb">print</span><span class="p">(</span><span class="n">final_response</span><span class="p">)</span>
</code></pre></div>
<p>The result is two modes of the same solution: a detailed one, with a visible chain of steps, and a concise one showing only the outcome. Clear prompt design helps in both cases; keep refining your prompts based on observed behavior. When UI clarity matters and extra details are undesirable, prefer Inner Monologue for display while still leveraging internal step‑by‑step analysis for quality control.</p>
<h2 id="theory-questions">Theory Questions</h2>
<ol>
<li>What is Chain of Thought (CoT) and why is it useful for multi‑step tasks?</li>
<li>How does CoT transparency increase user trust in model answers?</li>
<li>How does CoT help in educational scenarios?</li>
<li>How does a reasoning chain improve the quality of support chatbot answers?</li>
<li>What is Inner Monologue, and how does it differ from CoT in terms of what the user sees?</li>
<li>Why is Inner Monologue important when dealing with sensitive information?</li>
<li>How can Inner Monologue help in learning scenarios without revealing “spoilers”?</li>
<li>What steps are needed to prepare the environment for OpenAI API examples?</li>
<li>How is the <code>get_response_for_queries</code> function structured?</li>
<li>How does CoT prompting simplify handling complex queries?</li>
<li>How does the system/user prompt structure help answer product questions?</li>
<li>Why is extracting only the final part of the answer useful when using Inner Monologue?</li>
</ol>
<h2 id="practical-tasks">Practical Tasks</h2>
<ol>
<li>Implement <code>chain_of_thought_prompting(query)</code>, which generates a system prompt with step structure and wraps the user query in a delimiter.</li>
<li>Write <code>get_final_response(output, delimiter)</code> to extract the last part of the answer and handle possible errors.</li>
<li>Create a script that sends two queries — one with CoT and one with Inner Monologue — and prints both responses.</li>
<li>Implement <code>validate_response_structure(resp, delimiter)</code> to check that the answer contains the required number of steps.</li>
<li>Build a <code>QueryProcessor</code> class that encapsulates CoT and Inner Monologue logic (key loading, prompt assembly, request sending, post‑processing, and error handling).</li>
</ol>












                
                  
</body>
</html>