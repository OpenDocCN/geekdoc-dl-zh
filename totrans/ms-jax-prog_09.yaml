- en: 'Chapter 7: Deep Learning Applications with Jax'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Welcome to the practical side of Jax! In this chapter, we'll explore how Jax
    becomes a powerhouse for real-world applications in deep learning. From image
    classification to natural language processing and generative modeling, Jax flexes
    its muscles to tackle diverse tasks, proving its versatility in the hands of developers.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Image Classification Models with Jax
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Image classification is a fundamental task in deep learning, and with Jax's
    capabilities, constructing powerful models becomes both efficient and effective.
    In this section, we'll walk through the process of building an image classification
    model using Convolutional Neural Networks (CNNs) in Jax.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Importing Necessary Libraries
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Begin by importing the required libraries. Jax, together with libraries like
    NumPy and Jax's neural network module `flax`, provides a solid foundation for
    creating sophisticated models.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: import jax
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: import jax.numpy as jnp
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: from flax import linen as nn
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Defining the CNN Model
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Constructing the CNN architecture is straightforward with Jax. Here, we define
    a simple CNN using the `nn.Conv` and `nn.Dense` layers.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'class CNNModel(nn.Module):'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'features: int'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 'def setup(self):'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: self.conv1 = nn.Conv(features=self.features, kernel_size=(3, 3))
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: self.conv2 = nn.Conv(features=self.features * 2, kernel_size=(3, 3))
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: self.flatten = nn.Flatten()
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: self.dense = nn.Dense(features=10)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 'def __call__(self, x):'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: x = self.conv1(x)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: x = self.conv2(x)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: x = self.flatten(x)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: return self.dense(x)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Initializing the Model
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the model with random parameters. Jax allows for easy parameter initialization
    using its PRNG key.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: key = jax.random.PRNGKey(42)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: input_shape = (1, 28, 28, 1)  # Assuming grayscale images of size 28x28
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: model = CNNModel(features=32)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: params = model.init(key, jnp.ones(input_shape))
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Forward Pass
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Perform a forward pass to check if the model processes inputs correctly.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: input_data = jnp.ones(input_shape)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: output = model.apply(params, input_data)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: print("Model Output Shape:", output.shape)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Training Loop
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: To train the model, set up a training loop using Jax's automatic differentiation
    and an optimizer like SGD.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 'def loss_fn(params, input_data, targets):'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: predictions = model.apply(params, input_data)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: loss = jnp.mean(jax.nn.softmax_cross_entropy_with_logits(targets, predictions))
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: return loss
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: grad_fn = jax.grad(loss_fn)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: learning_rate = 0.01
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: optimizer = jax.optimizers.sgd(learning_rate)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Inside the training loop, update parameters using optimizer and gradients.
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'for epoch in range(num_epochs):'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: grad = grad_fn(params, input_data, targets)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: optimizer = optimizer.apply_gradient(grad)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Building image classification models with Jax is a seamless process. The modular
    design and concise syntax allow for quick experimentation and efficient development.
    The combination of Jax's flexibility and neural network modules facilitates the
    creation of models tailored to specific tasks, ensuring that you can effortlessly
    implement and train image classification models for various applications.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 NLP Models for Text Classification and Sentiment Analysis
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Natural Language Processing (NLP) tasks, such as text classification and sentiment
    analysis, are at the core of language understanding. Let's explore how to implement
    NLP models for these tasks using Jax.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Importing Necessary Libraries
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Begin by importing the required libraries, including Jax, NumPy, and the relevant
    modules from `flax` for neural network building.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: import jax
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: import jax.numpy as jnp
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: from flax import linen as nn
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Defining an RNN Model for Text Classification
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: For text classification, Recurrent Neural Networks (RNNs) are effective. Define
    an RNN model using Jax's `flax.linen` module.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: 'class RNNModel(nn.Module):'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 'vocab_size: int'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 'hidden_size: int'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 'def setup(self):'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: self.embedding = nn.Embed(vocab_size=self.vocab_size, features=self.hidden_size)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: self.rnn = nn.LSTMCell(name="lstm")
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: self.dense = nn.Dense(features=2)  # Assuming binary classification
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'def __call__(self, x):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: x = self.embedding(x)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: h = None
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: 'for _ in range(x.shape[1]):'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: h = self.rnn(h, x[:, _])
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: return self.dense(h)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Initializing and Forward Pass
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the model and perform a forward pass to check its output shape.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: key = jax.random.PRNGKey(42)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: seq_len, batch_size = 20, 64
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: model = RNNModel(vocab_size=10000, hidden_size=64)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: params = model.init(key, jnp.ones((batch_size, seq_len)))
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: input_data = jnp.ones((batch_size, seq_len), dtype=jnp.int32)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: output = model.apply(params, input_data)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: print("Model Output Shape:", output.shape)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Training Loop for Sentiment Analysis
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: For sentiment analysis, define a simple training loop using Jax's automatic
    differentiation and an optimizer like SGD.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: 'def loss_fn(params, input_data, targets):'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: predictions = model.apply(params, input_data)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: loss = jnp.mean(jax.nn.softmax_cross_entropy_with_logits(targets, predictions))
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: return loss
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: grad_fn = jax.grad(loss_fn)
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: learning_rate = 0.01
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: optimizer = jax.optimizers.sgd(learning_rate)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Inside the training loop, update parameters using optimizer and gradients.
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'for epoch in range(num_epochs):'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: grad = grad_fn(params, input_data, targets)
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: optimizer = optimizer.apply_gradient(grad)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Implementing NLP models for text classification and sentiment analysis with
    Jax is straightforward. The modular structure of Jax's `flax` allows you to define
    and train models with ease. The provided code snippets offer a starting point
    for building and experimenting with NLP models tailored to your specific applications.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 Developing Generative Models for Realistic Images and Text
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generative models are the artists of the AI world, creating new content that
    mirrors the patterns learned during training. Let's develop generative models
    using Jax for both images and text!
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Building a Variational Autoencoder (VAE) for Image Generation
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Variational Autoencoders (VAEs) are excellent for generating realistic images.
    Define a VAE model using Jax and Flax.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: 'class VAE(nn.Module):'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: 'latent_dim: int = 50'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: 'def setup(self):'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: self.encoder = nn.Sequential([nn.Dense(256), nn.relu, nn.Dense(2 * self.latent_dim)])
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: self.decoder = nn.Sequential([nn.Dense(256), nn.relu, nn.Dense(784), nn.sigmoid])
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'def __call__(self, x):'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: mean, log_std = jnp.split(self.encoder(x), 2, axis=-1)
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: std = jnp.exp(log_std)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: eps = jax.random.normal(self.make_rng(), mean.shape)
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: z = mean + std * eps
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: reconstruction = self.decoder(z)
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: return reconstruction, mean, log_std
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Training the VAE for Image Generation
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Define a training loop for the VAE model using a dataset like MNIST.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Assume `train_images` is the training dataset.
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: vae = VAE()
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: params = vae.init(jax.random.PRNGKey(42), jnp.ones((28 * 28,)))
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: optimizer = jax.optimizers.adam(learning_rate=0.001)
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'def loss_fn(params, images):'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: reconstructions, mean, log_std = vae.apply(params, images)
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '# Define the reconstruction loss and KL divergence term here.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: reconstruction_loss = ...
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: kl_divergence = ...
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: return reconstruction_loss + kl_divergence
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: 'for epoch in range(num_epochs):'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: grad = jax.grad(loss_fn)(params, train_images)
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: optimizer = optimizer.apply_gradient(grad)
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Creating Text with a Generative Adversarial Network (GAN)
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Generative Adversarial Networks (GANs) excel at generating realistic text. Define
    a simple GAN model for text generation.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'class GAN(nn.Module):'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 'latent_dim: int = 100'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 'def setup(self):'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: self.generator = nn.Sequential([nn.Dense(256), nn.relu, nn.Dense(784)])
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: self.discriminator = nn.Sequential([nn.Dense(256), nn.relu, nn.Dense(1), nn.sigmoid])
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'def __call__(self, z):'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: fake_data = self.generator(z)
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: return fake_data
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 'def discriminate(self, x):'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: return self.discriminator(x)
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Training the GAN for Text Generation
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Train the GAN model using a suitable loss function, often involving adversarial
    and reconstruction components.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: gan = GAN()
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: params = gan.init(jax.random.PRNGKey(42), jnp.ones((gan.latent_dim,)))
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: optimizer = jax.optimizers.adam(learning_rate=0.0002, beta1=0.5, beta2=0.999)
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'def loss_fn(params, real_data):'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: fake_data = gan.apply(params, jax.random.normal(jax.random.PRNGKey(42), (batch_size,
    gan.latent_dim)))
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '# Define the adversarial and reconstruction loss components here.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: adversarial_loss = ...
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: reconstruction_loss = ...
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: return adversarial_loss + reconstruction_loss
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'for epoch in range(num_epochs):'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: grad = jax.grad(loss_fn)(params, real_data)
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: optimizer = optimizer.apply_gradient(grad)
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Developing generative models with Jax empowers you to create diverse and realistic
    content, whether it's images or text. The flexibility and expressiveness of Jax's
    programming model make it an ideal choice for experimenting with and refining
    your generative models.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'Coding Challenge 7: Image Generation with Variational Autoencoder (VAE)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Implement a Variational Autoencoder (VAE) using Jax and Flax for image generation.
    Use a dataset of your choice, such as MNIST, for training the VAE. After training,
    generate new images with the trained VAE.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Here's a basic implementation of a VAE in Jax and Flax. Please note that this
    is a simplified example, and you may need to adapt it based on your specific dataset
    and requirements.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: import jax
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: import jax.numpy as jnp
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: import flax
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: from flax import linen as nn
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: from flax.training import train_state
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Define the VAE model
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'class VAE(nn.Module):'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'latent_dim: int = 20'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'def setup(self):'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: self.encoder = nn.Sequential([nn.Conv(32, (3, 3)), nn.relu, nn.Conv(64, (3,
    3)), nn.relu, nn.flatten,
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: nn.Dense(2 * self.latent_dim)])
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: self.decoder = nn.Sequential([nn.Dense(7 * 7 * 64), nn.relu, nn.reshape((7,
    7, 64)),
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: nn.ConvTranspose(32, (3, 3)), nn.relu, nn.ConvTranspose(1, (3, 3)), nn.sigmoid])
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'def __call__(self, x):'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: mean, log_std = jnp.split(self.encoder(x), 2, axis=-1)
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: std = jnp.exp(log_std)
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: eps = jax.random.normal(self.make_rng(), mean.shape)
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: z = mean + std * eps
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: reconstruction = self.decoder(z)
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: return reconstruction, mean, log_std
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Define training and evaluation steps
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'def train_step(state, batch):'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'def loss_fn(params):'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'reconstructions, mean, log_std = vae.apply({''params'': params}, batch[''image''])'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '# Define the reconstruction loss and KL divergence term here.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: reconstruction_loss = jnp.mean((reconstructions - batch['image'])  2)
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: kl_divergence = -0.5 * jnp.mean(1 + 2 * log_std - mean2 - jnp.exp(2 * log_std))
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: return reconstruction_loss + kl_divergence
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: grad = jax.grad(loss_fn)(state.params)
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: new_state = state.apply_gradient(grad)
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: return new_state
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'def eval_step(params, batch):'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 'reconstructions, _, _ = vae.apply({''params'': params}, batch[''image''])'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: reconstruction_loss = jnp.mean((reconstructions - batch['image'])  2)
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: return reconstruction_loss
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Load your dataset (e.g., MNIST)
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '...'
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Initialize the VAE and optimizer
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: vae = VAE()
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: params = vae.init(jax.random.PRNGKey(42), jnp.ones((28, 28, 1)))
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: optimizer = flax.optim.Adam(learning_rate=0.001).create(params)
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Training loop
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'for epoch in range(num_epochs):'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '# Iterate over batches'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'for batch in batches:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: state = train_step(state, batch)
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '# Evaluate on a validation set (optional)'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: validation_loss = jnp.mean([eval_step(state.params, val_batch) for val_batch
    in validation_batches])
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Generate new images after training
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: new_images, _, _ = vae.apply(state.params, jax.random.normal(jax.random.PRNGKey(42),
    (num_generated_images, vae.latent_dim)))
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Challenge Extension
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Experiment with different hyperparameters, architectures, or datasets to improve
    the VAE's performance and generate more diverse and realistic images.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Jax empowers you to turn deep learning concepts into reality. As we've seen
    in this chapter, you can leverage Jax's capabilities to build robust models for
    image classification, handle natural language intricacies, and even delve into
    the creative realm of generative models. Armed with Jax, you're equipped to bring
    your deep learning ideas to life, crafting solutions that go beyond theory to
    make a tangible impact.
  id: totrans-208
  prefs: []
  stylish: true
  type: TYPE_NORMAL
