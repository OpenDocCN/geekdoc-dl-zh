- en: Training LLaMA 3 on TPUs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åœ¨TPUä¸Šè®­ç»ƒLLaMA 3
- en: åŸæ–‡ï¼š[https://jax-ml.github.io/scaling-book/applied-training](https://jax-ml.github.io/scaling-book/applied-training)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://jax-ml.github.io/scaling-book/applied-training](https://jax-ml.github.io/scaling-book/applied-training)
- en: '<d-title>Part 6 of [How To Scale Your Model](/scaling-book) ([Part 5: Training](../training)
    | [Part 7: Inference](../inference))'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: <d-title>ç¬¬6éƒ¨åˆ†[å¦‚ä½•æ‰©å±•ä½ çš„æ¨¡å‹](/scaling-book) ([ç¬¬5éƒ¨åˆ†ï¼šè®­ç»ƒ](../training) | [ç¬¬7éƒ¨åˆ†ï¼šæ¨ç†](../inference))
- en: Let's take a close look at how we'd train LLaMA 3 models on TPU v5p using what
    we've learned in the previous section. How big are they? How expensive is training
    in different configurations? How are they sharded? Let's work through some back-of-the-envelope
    estimates for how the previous sections map onto real models.</d-title>  <d-byline><d-article><d-contents>###
    Contents
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»”ç»†çœ‹çœ‹å¦‚ä½•ä½¿ç”¨ä¸Šä¸€èŠ‚å­¦åˆ°çš„çŸ¥è¯†åœ¨TPU v5pä¸Šè®­ç»ƒLLaMA 3æ¨¡å‹ã€‚å®ƒä»¬æœ‰å¤šå¤§ï¼Ÿä¸åŒé…ç½®çš„è®­ç»ƒæˆæœ¬å¦‚ä½•ï¼Ÿå®ƒä»¬æ˜¯å¦‚ä½•åˆ†ç‰‡çš„ï¼Ÿè®©æˆ‘ä»¬é€šè¿‡ä¸€äº›ç²—ç•¥ä¼°è®¡æ¥çœ‹çœ‹å‰å‡ èŠ‚æ˜¯å¦‚ä½•æ˜ å°„åˆ°å®é™…æ¨¡å‹ä¸Šçš„ã€‚
- en: '[What does LLaMA 3 look like?](#what-does-llama-3-look-like)[Counting parameters
    and FLOPs](#counting-parameters-and-flops)[How to shard LLaMA 3-70B for training](#how-to-shard-llama-3-70b-for-training)[Worked
    Problems](#worked-problems)</d-contents>'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[LLaMA 3é•¿ä»€ä¹ˆæ ·ï¼Ÿ](#what-does-llama-3-look-like)[è®¡ç®—å‚æ•°å’ŒFLOPs](#counting-parameters-and-flops)[å¦‚ä½•åˆ†ç‰‡è®­ç»ƒLLaMA
    3-70B](#how-to-shard-llama-3-70b-for-training)[ç»ƒä¹ é—®é¢˜](#worked-problems)'
- en: '*Our goal in this section is to apply results from the previous section to
    a very practical problem: training the LLaMA 3 family (herd) of models. Unlike
    the previous sections we want you to do a lot of this work yourself. For this
    reason, weâ€™ve hidden the answers to each section so you can try to answer it first.
    Try grabbing a pen and doing by hand!*'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*æœ¬èŠ‚çš„ç›®æ ‡æ˜¯å°†å‰èŠ‚çš„ç»“æœåº”ç”¨äºä¸€ä¸ªéå¸¸å®é™…çš„é—®é¢˜ï¼šè®­ç»ƒLLaMA 3ç³»åˆ—ï¼ˆç¾¤ï¼‰æ¨¡å‹ã€‚ä¸å‰é¢çš„ç« èŠ‚ä¸åŒï¼Œæˆ‘ä»¬å¸Œæœ›ä½ è‡ªå·±åšå¾ˆå¤šå·¥ä½œã€‚å› æ­¤ï¼Œæˆ‘ä»¬éšè—äº†æ¯ä¸ªéƒ¨åˆ†çš„ç­”æ¡ˆï¼Œè¿™æ ·ä½ å¯ä»¥å…ˆå°è¯•å›ç­”å®ƒã€‚è¯•ç€æ‹¿ä¸€æ”¯ç¬”ï¼Œæ‰‹åŠ¨å°è¯•ï¼*'
- en: What does LLaMA 3 look like?
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LLaMA 3é•¿ä»€ä¹ˆæ ·ï¼Ÿ
- en: 'The LLaMA-3 model family <d-cite key="llama3">includes 3 main models: LLaMA
    3 8B, 70B, and 405B. Weâ€™ll mostly focus on 70B, and leave 8B and 405B for you
    to explore in the problem section at the end. Hereâ€™s the architecture for LLaMA
    3-70B, taken from the LLaMA [HuggingFace page](https://huggingface.co/meta-llama/Meta-Llama-3-70B/blob/main/config.json).</d-cite>'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA-3æ¨¡å‹å®¶æ—åŒ…æ‹¬3ä¸ªä¸»è¦æ¨¡å‹ï¼šLLaMA 3 8Bã€70Bå’Œ405Bã€‚æˆ‘ä»¬å°†ä¸»è¦å…³æ³¨70Bï¼Œå¹¶å°†8Bå’Œ405Bç•™ç»™ä½ åœ¨æ–‡ç« æœ«å°¾çš„é—®é¢˜éƒ¨åˆ†æ¢ç´¢ã€‚ä»¥ä¸‹æ˜¯LLaMA
    3-70Bçš„æ¶æ„ï¼Œæ‘˜è‡ªLLaMA [HuggingFaceé¡µé¢](https://huggingface.co/meta-llama/Meta-Llama-3-70B/blob/main/config.json)ã€‚
- en: '| **hyperparam** | **value** |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '| **è¶…å‚æ•°** | **å€¼** |'
- en: '| --- | --- |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| \(n_\text{layers}\) (L) | 80 |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| \(n_\text{layers}\) (L) | 80 |'
- en: '| \(d_\text{model}\) (D) | 8,192 |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| \(d_\text{model}\) (D) | 8,192 |'
- en: '| \(d_{ff}\) (F) | 28,672 |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| \(d_{ff}\) (F) | 28,672 |'
- en: '| \(n_\text{heads}\) (N) | 64 |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| \(n_\text{heads}\) (N) | 64 |'
- en: '| \(n_\text{kv_heads}\) (K) | 8 |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| \(n_\text{kv_heads}\) (K) | 8 |'
- en: '| \(d_\text{qkv}\) (H) | 128 |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| \(d_\text{qkv}\) (H) | 128 |'
- en: '| \(n_\text{embeddings}\) (V) | 128,256 |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| \(n_\text{embeddings}\) (V) | 128,256 |'
- en: 'To highlight how easy this is to find, hereâ€™s the config itself, along with
    a mapping:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†çªå‡ºè¿™ä¸€ç‚¹æœ‰å¤šå®¹æ˜“æ‰¾åˆ°ï¼Œè¿™é‡Œæä¾›äº†é…ç½®æœ¬èº«ï¼Œä»¥åŠä¸€ä¸ªæ˜ å°„ï¼š
- en: <picture>![](../Images/08b4ca2bb72deccd30a73b1a0b07b724.png)</picture>
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: <picture>![](../Images/08b4ca2bb72deccd30a73b1a0b07b724.png)</picture>
- en: '*Itâ€™s useful to make a big table with these numbers for many different open-source
    LLMs, so you can quickly compare the design decisions theyâ€™ve made.*'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*åˆ¶ä½œä¸€ä¸ªåŒ…å«è¿™äº›æ•°å­—çš„å¤§è¡¨æ ¼ï¼Œç”¨äºæ¯”è¾ƒè®¸å¤šä¸åŒçš„å¼€æºLLMçš„è®¾è®¡å†³ç­–ï¼Œè¿™æ ·ä½ å¯ä»¥å¿«é€Ÿæ¯”è¾ƒå®ƒä»¬ã€‚*'
- en: Counting parameters and FLOPs
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®¡ç®—å‚æ•°å’ŒFLOPs
- en: '**Question:** From this table, can we calculate the LLaMA 3-70B parameter count?
    ğŸ¤« Letâ€™s apply the content of [Section 4](../transformers) and see if we can get
    70B!'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**é—®é¢˜ï¼š** ä»è¿™ä¸ªè¡¨æ ¼ä¸­ï¼Œæˆ‘ä»¬èƒ½è®¡ç®—å‡ºLLaMA 3-70Bçš„å‚æ•°æ•°é‡å—ï¼ŸğŸ¤« è®©æˆ‘ä»¬åº”ç”¨ç¬¬4èŠ‚çš„å†…å®¹ï¼Œçœ‹çœ‹æˆ‘ä»¬æ˜¯å¦èƒ½å¾—åˆ°70Bï¼'
- en: '| param | formula | count |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| å‚æ•° | å…¬å¼ | æ•°é‡ |'
- en: '| --- | --- | --- |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| FFW params | d_model * d_ff * 3 (for gelu + out-projection) * n_layers |
    8,192 * 8,192 * 3.5 * 3 * 80 = **56.3e9** |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| FFWå‚æ•° | d_model * d_ff * 3 (for gelu + out-projection) * n_layers | 8,192
    * 8,192 * 3.5 * 3 * 80 = **56.3e9** |'
- en: '| Vocab params | 2 (input and output embeddings) * n_embeddings * d_model |
    2 * 128,256 * 8,192 = **2.1e9** |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| Vocabå‚æ•° | 2 (input and output embeddings) * n_embeddings * d_model | 2 *
    128,256 * 8,192 = **2.1e9** |'
- en: '| Attention params | n_layers * [ 2 (for q embedding and concatenated output
    projection) * d_model * n_heads * d_qkv + 2 (for k and v) * d_model * n_kv_heads
    * d_qkv] | 80 * (2 * 8,192 * 64 * 128 + 2 * 8,192 * 8 * 128) = **12e9** |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| Attentionå‚æ•° | n_layers * [ 2 (for q embedding and concatenated output projection)
    * d_model * n_heads * d_qkv + 2 (for k and v) * d_model * n_kv_heads * d_qkv]
    | 80 * (2 * 8,192 * 64 * 128 + 2 * 8,192 * 8 * 128) = **12e9** |'
- en: '| Â  | Â  | 56.3e9 + 2.1e9 + 12e9 = **70.4e9** |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| Â  | Â  | 56.3e9 + 2.1e9 + 12e9 = **70.4e9** |'
- en: Thatâ€™s great! We get the number we expect. Youâ€™ll notice as expected that the
    FFW parameters totally dominate the overall parameter count, although attention
    is non-trivial.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¤ªæ£’äº†ï¼æˆ‘ä»¬å¾—åˆ°äº†é¢„æœŸçš„æ•°å­—ã€‚æ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼Œä½ ä¼šæ³¨æ„åˆ° FFW å‚æ•°åœ¨æ•´ä½“å‚æ•°æ•°é‡ä¸­å®Œå…¨å ä¸»å¯¼åœ°ä½ï¼Œå°½ç®¡æ³¨æ„åŠ›æœºåˆ¶å¹¶ä¸ç®€å•ã€‚
- en: '**Takeaway**: The 3 big weight matrices in the MLP block are so much larger
    than all the other arrays in the Transformer that we can typically almost ignore
    all other parameters when reasoning about model memory or FLOPs. For LLaMA 3-70B,
    they represent 56B of 70B parameters.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ€»ç»“ï¼š** MLP å—ä¸­çš„ 3 ä¸ªå¤§æƒé‡çŸ©é˜µæ¯” Transformer ä¸­çš„æ‰€æœ‰å…¶ä»–æ•°ç»„éƒ½è¦å¤§å¾—å¤šï¼Œå› æ­¤å½“æˆ‘ä»¬è€ƒè™‘æ¨¡å‹å†…å­˜æˆ– FLOPs æ—¶ï¼Œé€šå¸¸å¯ä»¥å‡ ä¹å¿½ç•¥æ‰€æœ‰å…¶ä»–å‚æ•°ã€‚å¯¹äº
    LLaMA 3-70B æ¥è¯´ï¼Œå®ƒä»¬ä»£è¡¨äº† 70B ä¸ªå‚æ•°ä¸­çš„ 56Bã€‚'
- en: Letâ€™s look at FLOPs now! *Remember the general rules for training from [Section
    4](../transformers).*
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¥çœ‹çœ‹ FLOPsï¼*è®°ä½ [ç¬¬ 4 èŠ‚](../transformers) ä¸­è®­ç»ƒçš„ä¸€èˆ¬è§„åˆ™ã€‚*
- en: '**Question:** How many FLOPs does LLaMA-3 perform per token per training step?
    *This helps us determine how expensive the whole training process will be.*'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**é—®é¢˜ï¼š** LLaMA-3 æ¯ä¸ª token æ¯ä¸ªè®­ç»ƒæ­¥éª¤æ‰§è¡Œå¤šå°‘ FLOPsï¼Ÿ*è¿™æœ‰åŠ©äºæˆ‘ä»¬ç¡®å®šæ•´ä¸ªè®­ç»ƒè¿‡ç¨‹å°†æœ‰å¤šæ˜‚è´µã€‚*'
- en: <details><summary>Click here for the answer, once youâ€™ve thought about it!</summary>
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>ç‚¹å‡»æ­¤å¤„æŸ¥çœ‹ç­”æ¡ˆï¼Œåœ¨ä½ æ€è€ƒè¿‡åï¼</summary>
- en: '**Answer**: As shown in [Section 4](../transformers), we do roughly \(6 \cdot
    \text{param count}\) FLOPs per token, so here thatâ€™s roughly `6 * 70e9 = 4.2e11`
    FLOPs / token. Thatâ€™s about half a TFLOP per token per step. Assuming weâ€™re compute-bound,
    this should take roughly `4.2e11 / 4.59E+14 = 1ms` on a single TPU v5p chip, assuming
    perfect FLOPs utilization.</details>'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç­”æ¡ˆï¼š** å¦‚ [ç¬¬ 4 èŠ‚](../transformers) æ‰€ç¤ºï¼Œæˆ‘ä»¬æ¯ token å¤§çº¦éœ€è¦ \(6 \cdot \text{å‚æ•°æ•°é‡}\)
    ä¸ª FLOPsï¼Œæ‰€ä»¥è¿™é‡Œå¤§çº¦æ˜¯ `6 * 70e9 = 4.2e11` FLOPs / tokenã€‚è¿™æ„å‘³ç€æ¯æ­¥å¤§çº¦æœ‰ 0.5 TFLOPsã€‚å‡è®¾æˆ‘ä»¬å—é™äºè®¡ç®—èƒ½åŠ›ï¼Œè¿™åº”è¯¥åœ¨å•ä¸ª
    TPU v5p èŠ¯ç‰‡ä¸Šå¤§çº¦éœ€è¦ `4.2e11 / 4.59E+14 = 1ms`ï¼Œå‡è®¾ FLOPs åˆ©ç”¨ç‡å®Œç¾ã€‚</details>'
- en: '**Question:** LLaMA 3 was trained for about 15 trillion tokens. How many FLOPs
    is that total?'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**é—®é¢˜ï¼š** LLaMA 3 å¤§çº¦è®­ç»ƒäº† 1500 ä¸‡äº¿ä¸ª tokenã€‚è¿™æ€»å…±éœ€è¦å¤šå°‘ FLOPsï¼Ÿ'
- en: <details><summary>Click here for the answer, once youâ€™ve thought about it!</summary>
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>ç‚¹å‡»æ­¤å¤„æŸ¥çœ‹ç­”æ¡ˆï¼Œåœ¨ä½ æ€è€ƒè¿‡åï¼</summary>
- en: '**Answer**: Thatâ€™s easy, itâ€™s just `4.2e11 * 15e12 = 6.3e24 FLOPs` total. 6.3
    yottaFLOPs. Thatâ€™s a lot! On a single TPU this would take `6.3e24 / 4.59E+14 =
    435 years`. Thatâ€™s also a lot!</details>'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç­”æ¡ˆï¼š** è¿™å¾ˆç®€å•ï¼Œæ€»å…±åªéœ€è¦ `4.2e11 * 15e12 = 6.3e24 FLOPs`ã€‚6.3 yottaFLOPsã€‚è¿™å¤ªå¤šäº†ï¼åœ¨ä¸€ä¸ªå•ç‹¬çš„
    TPU ä¸Šè¿™å°†éœ€è¦ `6.3e24 / 4.59E+14 = 435 years`ã€‚è¿™ä¹Ÿå¤ªå¤šäº†ï¼</details>'
- en: '**Question:** Letâ€™s say we wanted to train on a full TPU v5p pod with 16x20x28
    = 8960 chips. How long would this take to train at 40% MFU in bfloat16, assuming
    we are compute-bound?'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**é—®é¢˜ï¼š** å‡è®¾æˆ‘ä»¬æƒ³åœ¨å®Œæ•´çš„ TPU v5p pod ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¯¥ pod æœ‰ 16x20x28 = 8960 ä¸ªèŠ¯ç‰‡ã€‚åœ¨ 40% MFU ä¸‹ä»¥
    bfloat16 æ ¼å¼è®­ç»ƒéœ€è¦å¤šé•¿æ—¶é—´ï¼Œå‡è®¾æˆ‘ä»¬å—é™äºè®¡ç®—èƒ½åŠ›ï¼Ÿ'
- en: <details><summary>Click here for the answer, once youâ€™ve thought about it!</summary>
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>ç‚¹å‡»æ­¤å¤„æŸ¥çœ‹ç­”æ¡ˆï¼Œåœ¨ä½ æ€è€ƒè¿‡åï¼</summary>
- en: '**Answer**: We know that each TPU v5p can perform 4.59e14 FLOPs / second. At
    40% MFU, this will take about `T = 6.3e24 / (8960 * 4.59e14 * 0.4) = 3.8e6 seconds`.
    **This is about 44 days!** Thatâ€™s fairly reasonable, assuming we can actually
    achieve 40% MFU.</details>'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç­”æ¡ˆï¼š** æˆ‘ä»¬çŸ¥é“æ¯ä¸ª TPU v5p æ¯ç§’å¯ä»¥æ‰§è¡Œ 4.59e14 ä¸ª FLOPsã€‚åœ¨ 40% MFU ä¸‹ï¼Œè¿™å°†éœ€è¦å¤§çº¦ `T = 6.3e24
    / (8960 * 4.59e14 * 0.4) = 3.8e6 seconds`ã€‚**è¿™å¤§çº¦æ˜¯ 44 å¤©ï¼**è¿™ç›¸å½“åˆç†ï¼Œå‡è®¾æˆ‘ä»¬å®é™…ä¸Šèƒ½å¤Ÿè¾¾åˆ° 40%
    MFUã€‚</details>'
- en: '**Question:** LLaMA 3-70B was pretrained with a batch size of about 4M tokens.
    How many TPUs do we need at minimum to train with this batch size? *You can assume
    bfloat16 parameters and float32 optimizer state, and that you checkpoint gradients
    4 times per layer.*'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**é—®é¢˜ï¼š** LLaMA 3-70B ä½¿ç”¨å¤§çº¦ 4M ä¸ª token çš„æ‰¹å¤§å°è¿›è¡Œé¢„è®­ç»ƒã€‚ä¸ºäº†ä½¿ç”¨è¿™ä¸ªæ‰¹å¤§å°è¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬è‡³å°‘éœ€è¦å¤šå°‘ä¸ª TPUï¼Ÿ*ä½ å¯ä»¥å‡è®¾
    bfloat16 å‚æ•°å’Œ float32 ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œå¹¶ä¸”æ¯å±‚æ£€æŸ¥æ¢¯åº¦ 4 æ¬¡ã€‚*'
- en: <details><summary>Click here for the answer, once youâ€™ve thought about it!</summary>
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>ç‚¹å‡»æ­¤å¤„æŸ¥çœ‹ç­”æ¡ˆï¼Œåœ¨ä½ æ€è€ƒè¿‡åï¼</summary>
- en: '**Answer**: This question is primarily asking about memory usage, since thatâ€™s
    the only strict constraint on available compute. During training, we have three
    primary uses of HBM: model parameters, optimizer state, and gradient checkpoints.
    If we assume bfloat16 weights, float32 optimizer state, and a *very* conservative
    gradient checkpointing scheme (4 times per layer), we have:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç­”æ¡ˆï¼š** è¿™ä¸ªé—®é¢˜ä¸»è¦è¯¢é—®çš„æ˜¯å†…å­˜ä½¿ç”¨æƒ…å†µï¼Œå› ä¸ºè¿™æ˜¯å”¯ä¸€ä¸¥æ ¼çš„è®¡ç®—èƒ½åŠ›çº¦æŸã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸‰ä¸ªä¸»è¦çš„ HBM ä½¿ç”¨åœºæ™¯ï¼šæ¨¡å‹å‚æ•°ã€ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦æ£€æŸ¥ç‚¹ã€‚å¦‚æœæˆ‘ä»¬å‡è®¾
    bfloat16 æƒé‡ã€float32 ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œä»¥åŠä¸€ä¸ªéå¸¸ä¿å®ˆçš„æ¢¯åº¦æ£€æŸ¥ç‚¹æ–¹æ¡ˆï¼ˆæ¯å±‚ 4 æ¬¡ï¼‰ï¼Œæˆ‘ä»¬æœ‰ï¼š'
- en: '| **Params** | 2 * 70GB | ~140GB |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| **å‚æ•°** | 2 * 70GB | ~140GB |'
- en: '| **Optimizer State** | 8 * 70GB | ~560GB |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| **ä¼˜åŒ–å™¨çŠ¶æ€** | 8 * 70GB | ~560GB |'
- en: '| **Gradient Checkpoints** | 2 * 8192 * 4e6 * 4 * 80 | ~20.9TB |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| **æ¢¯åº¦æ£€æŸ¥ç‚¹** | 2 * 8192 * 4e6 * 4 * 80 | ~20.9TB |'
- en: '| **Total** | Â  | ~21.6TB |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| **æ€»è®¡** | Â  | ~21.6TB |'
- en: The total here is about 21.6TB. You notice that gradient checkpointing strongly
    dominates the memory picture, even with a very conservative checkpointing scheme.
    We could technically go to 1 checkpoint per layer, or do microbatching, but this
    is a reasonable picture. With these assumptions, since each TPU v5p has 96GB of
    HBM, we need `21.6e12 / 96e9 = 225` TPUs. Thatâ€™s not very much actually!
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»è®¡å¤§çº¦æ˜¯ 21.6TBã€‚ä½ æ³¨æ„åˆ°ï¼Œå³ä½¿æ˜¯éå¸¸ä¿å®ˆçš„æ£€æŸ¥ç‚¹æ–¹æ¡ˆï¼Œæ¢¯åº¦æ£€æŸ¥ç‚¹ä¹Ÿå¼ºçƒˆä¸»å¯¼äº†å†…å­˜å›¾æ™¯ã€‚æŠ€æœ¯ä¸Šæˆ‘ä»¬å¯ä»¥åšåˆ°æ¯å±‚ä¸€ä¸ªæ£€æŸ¥ç‚¹ï¼Œæˆ–è€…è¿›è¡Œå¾®æ‰¹å¤„ç†ï¼Œä½†è¿™æ˜¯ä¸€ä¸ªåˆç†çš„å›¾æ™¯ã€‚åœ¨è¿™äº›å‡è®¾ä¸‹ï¼Œç”±äºæ¯ä¸ª
    TPU v5p æœ‰ 96GB çš„ HBMï¼Œæˆ‘ä»¬éœ€è¦ `21.6e12 / 96e9 = 225` ä¸ª TPUã€‚å®é™…ä¸Šå¹¶ä¸å¤šï¼
- en: '*Why wouldnâ€™t we do this?* Well, because it would take us `44 days * 8960 /
    225 = 1752 days` to train. Thatâ€™s nearly four years. **Thatâ€™s a lot.** Still,
    this makes it clear that weâ€™re using these large clusters not because weâ€™re bound
    by memory but rather because we need the extra FLOPs.</details>'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*ä¸ºä»€ä¹ˆä¸è¿™æ ·åšå‘¢ï¼Ÿ* å¥½å§ï¼Œå› ä¸ºè¿™éœ€è¦æˆ‘ä»¬ `44 å¤© * 8960 / 225 = 1752 å¤©` æ¥è®­ç»ƒã€‚è¿™å‡ ä¹å››å¹´çš„æ—¶é—´ã€‚**è¿™å¤ªå¤šäº†ã€‚**å°½ç®¡å¦‚æ­¤ï¼Œè¿™æ¸…æ¥šåœ°è¡¨æ˜æˆ‘ä»¬ä½¿ç”¨è¿™äº›å¤§å‹é›†ç¾¤å¹¶ä¸æ˜¯å› ä¸ºæˆ‘ä»¬å—é™äºå†…å­˜ï¼Œè€Œæ˜¯å› ä¸ºæˆ‘ä»¬éœ€è¦é¢å¤–çš„
    FLOPsã€‚</details>'
- en: '**Question:** Under the same assumptions as the question above, if we use 8960
    TPU v5p chips, how much memory will we use per-chip?'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**é—®é¢˜**ï¼šåœ¨ä¸Šè¿°é—®é¢˜çš„ç›¸åŒå‡è®¾ä¸‹ï¼Œå¦‚æœæˆ‘ä»¬ä½¿ç”¨ 8960 ä¸ª TPU v5p èŠ¯ç‰‡ï¼Œæ¯ç‰‡èŠ¯ç‰‡æˆ‘ä»¬å°†ä½¿ç”¨å¤šå°‘å†…å­˜ï¼Ÿ'
- en: <details><summary>Click here for the answer, once youâ€™ve thought about it!</summary>
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>ç‚¹å‡»æ­¤å¤„æŸ¥çœ‹ç­”æ¡ˆï¼Œåœ¨ä½ æ€è€ƒè¿‡åï¼</summary>
- en: '**Answer**: Our total memory is still about 21.6TB, so per-chip weâ€™ll be using
    about 2.4GB per chip, which is basically nothing. If we did much more aggressive
    checkpointing, e.g. 12 checkpoints per layer, weâ€™d still only be at 8GB per chip.
    Weâ€™re nowhere near being memory bound during training at these scales.</details>'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç­”æ¡ˆ**ï¼šæˆ‘ä»¬çš„æ€»å†…å­˜ä»ç„¶æ˜¯å¤§çº¦ 21.6TBï¼Œæ‰€ä»¥æ¯ç‰‡èŠ¯ç‰‡æˆ‘ä»¬å°†ä½¿ç”¨å¤§çº¦ 2.4GBï¼Œè¿™åŸºæœ¬ä¸Šä¸ç®—ä»€ä¹ˆã€‚å¦‚æœæˆ‘ä»¬è¿›è¡Œæ›´æ¿€è¿›çš„æ£€æŸ¥ç‚¹ï¼Œä¾‹å¦‚æ¯å±‚ 12
    ä¸ªæ£€æŸ¥ç‚¹ï¼Œæˆ‘ä»¬æ¯ç‰‡èŠ¯ç‰‡ä¹Ÿåªä¼šè¾¾åˆ° 8GBã€‚åœ¨è¿™äº›è§„æ¨¡ä¸‹ï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ ¹æœ¬ä¸ä¼šæ¥è¿‘å†…å­˜é™åˆ¶ã€‚</details>'
- en: '**Takeaways**: It is technically possible to train even very large models on
    very small topologies, with the caveat that they will likely take a long time.
    Being able to calculate the total FLOPs of a training run allows us to ballpark
    its training time by assuming a modest MFU and a known topology.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¦ç‚¹**ï¼šåœ¨æŠ€æœ¯ä¸Šï¼Œå³ä½¿åœ¨éå¸¸å°çš„æ‹“æ‰‘ç»“æ„ä¸Šè®­ç»ƒéå¸¸å¤§çš„æ¨¡å‹ä¹Ÿæ˜¯å¯èƒ½çš„ï¼Œä½†å‰ææ˜¯å®ƒä»¬å¯èƒ½éœ€è¦å¾ˆé•¿æ—¶é—´ã€‚èƒ½å¤Ÿè®¡ç®—è®­ç»ƒè¿è¡Œçš„æ€»ä½“ FLOPs å…è®¸æˆ‘ä»¬é€šè¿‡å‡è®¾ä¸€ä¸ªé€‚åº¦çš„
    MFU å’Œå·²çŸ¥çš„æ‹“æ‰‘æ¥ä¼°ç®—å…¶è®­ç»ƒæ—¶é—´ã€‚'
- en: How to shard LLaMA 3-70B for training
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¦‚ä½•ä¸ºè®­ç»ƒåˆ†ç‰‡ LLaMA 3-70B
- en: Letâ€™s stick to our setting from above and say we want to train LLaMA 3-70B with
    4M token batch size (1024 sequences of length 4096 per batch) on a TPU v5p pod
    of 8960 chips. Letâ€™s discuss what the best sharding strategy is for this model.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åšæŒä¸Šé¢çš„è®¾ç½®ï¼Œå¹¶è¯´æˆ‘ä»¬æƒ³è¦ä½¿ç”¨ 4M ä»¤ç‰Œæ‰¹å¤§å°ï¼ˆæ¯ä¸ªæ‰¹æ¬¡ 1024 ä¸ªé•¿åº¦ä¸º 4096 çš„åºåˆ—ï¼‰åœ¨ 8960 ä¸ªèŠ¯ç‰‡çš„ TPU v5p pod
    ä¸Šè®­ç»ƒ LLaMA 3-70Bã€‚è®©æˆ‘ä»¬è®¨è®ºè¿™ä¸ªæ¨¡å‹çš„æœ€ä½³åˆ†ç‰‡ç­–ç•¥ã€‚
- en: '**Question:** Under the assumptions above, can we train our model with FSDP
    alone? To start, letâ€™s say we canâ€™t do any sequence/context parallelism. *This
    should be the first idea you have, since itâ€™s simple and will introduce no extra
    communication if it works.*'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**é—®é¢˜**ï¼šåœ¨ä¸Šè¿°å‡è®¾ä¸‹ï¼Œæˆ‘ä»¬èƒ½å¦ä»…ä½¿ç”¨ FSDP è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ï¼Ÿé¦–å…ˆï¼Œè®©æˆ‘ä»¬å‡è®¾æˆ‘ä»¬æ— æ³•è¿›è¡Œä»»ä½•åºåˆ—/ä¸Šä¸‹æ–‡å¹¶è¡Œã€‚*è¿™åº”è¯¥æ˜¯ä½ é¦–å…ˆæƒ³åˆ°çš„æƒ³æ³•ï¼Œå› ä¸ºå®ƒç®€å•ï¼Œå¦‚æœå¯è¡Œçš„è¯ï¼Œå°†ä¸ä¼šå¼•å…¥é¢å¤–çš„é€šä¿¡ã€‚*'
- en: <details><summary>Click here for the answer, once youâ€™ve thought about it!</summary>
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>ç‚¹å‡»æ­¤å¤„æŸ¥çœ‹ç­”æ¡ˆï¼Œåœ¨ä½ æ€è€ƒè¿‡åï¼</summary>
- en: '**Answer**: This answer will be a little pedantic. As noted above, LLaMA 3-70B
    is initially trained with sequences of length 4K, so the batch size of 4M tokens
    gives us a *sequence batch size* of 1024\. That means we can only really do pure
    data parallelism/FSDP up to 1024 chips *because thatâ€™s how many sequences we have
    to do data parallelism over*. So the answer in the simple sense of â€œfully data
    parallelism with no extra communicationâ€ is no. The next question will answer
    a slightly less pedantic version of this.</details>'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç­”æ¡ˆ**ï¼šè¿™ä¸ªç­”æ¡ˆå¯èƒ½ä¼šæœ‰äº›ç¹çã€‚å¦‚ä¸Šæ‰€è¿°ï¼ŒLLaMA 3-70B åˆå§‹è®­ç»ƒæ—¶ä½¿ç”¨é•¿åº¦ä¸º 4K çš„åºåˆ—ï¼Œå› æ­¤ 4M ä»¤ç‰Œçš„æ‰¹å¤§å°ç»™æˆ‘ä»¬å¸¦æ¥äº† *åºåˆ—æ‰¹å¤§å°*
    ä¸º 1024ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å®é™…ä¸Šåªèƒ½åšåˆ° 1024 ä¸ªèŠ¯ç‰‡çš„çº¯æ•°æ®å¹¶è¡Œ/FSDPï¼Œå› ä¸ºè¿™å°±æ˜¯æˆ‘ä»¬å¿…é¡»è¿›è¡Œæ•°æ®å¹¶è¡Œå¤„ç†çš„åºåˆ—æ•°é‡ã€‚æ‰€ä»¥åœ¨ç®€å•æ„ä¹‰ä¸Šï¼Œâ€œæ— é¢å¤–é€šä¿¡çš„å…¨æ•°æ®å¹¶è¡Œâ€æ˜¯ä¸å¯èƒ½çš„ã€‚ä¸‹ä¸€ä¸ªé—®é¢˜å°†å›ç­”è¿™ä¸ªé—®é¢˜çš„ç¨å¾®ä¸é‚£ä¹ˆç¹ççš„ç‰ˆæœ¬ã€‚</details>'
- en: '**Question:** Letâ€™s relax the requirement of not doing any sequence sharding.
    If we allow ourselves to do FSDP over both the batch *and* sequence axes, can
    we train LLaMA 3-70B with only FSDP on 8960 chips?'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**é—®é¢˜**ï¼šå¦‚æœæˆ‘ä»¬æ”¾å®½ä¸è¿›è¡Œä»»ä½•åºåˆ—åˆ†ç‰‡çš„è¦æ±‚ã€‚å¦‚æœæˆ‘ä»¬å…è®¸è‡ªå·±åœ¨æ‰¹å’Œåºåˆ—è½´ä¸Šè¿›è¡Œ FSDPï¼Œæˆ‘ä»¬èƒ½å¦åœ¨ 8960 ä¸ªèŠ¯ç‰‡ä¸Šä»…ä½¿ç”¨ FSDP è®­ç»ƒ
    LLaMA 3-70Bï¼Ÿ'
- en: <details><summary>Click here for the answer, once youâ€™ve thought about it!</summary>
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>ç‚¹å‡»æ­¤å¤„æŸ¥çœ‹ç­”æ¡ˆï¼Œåœ¨ä½ æ€è€ƒè¿‡åï¼</summary>
- en: '**Answer**: Now that weâ€™re allowing ourselves to do sequence/context parallelism
    as well, we can scale up way more. First letâ€™s calculate our per-device batch
    size. If we do 8960-way FSDP, we end with a per-TPU batch size of `4 * 1024 *
    1024 / 8960 = 468 tokens`. We know from the previous section that we become ICI-bound
    by FSDP when \(\text{per device batch size} < 2550 / M_X\). Since we can dedicate
    3 axes here with a full 3D pod, this would give us a lower bound of 850, which
    weâ€™re well below. **So the answer is no, even with 3 axes. We would be solidly
    communication-bound.**</details>'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç­”æ¡ˆ**ï¼šç°åœ¨æˆ‘ä»¬å…è®¸è‡ªå·±è¿›è¡Œåºåˆ—/ä¸Šä¸‹æ–‡å¹¶è¡Œï¼Œæˆ‘ä»¬å¯ä»¥æ‰©å±•å¾—æ›´å¤šã€‚é¦–å…ˆè®©æˆ‘ä»¬è®¡ç®—æˆ‘ä»¬çš„æ¯è®¾å¤‡æ‰¹å¤„ç†å¤§å°ã€‚å¦‚æœæˆ‘ä»¬è¿›è¡Œ8960è·¯FSDPï¼Œæˆ‘ä»¬å°†å¾—åˆ°æ¯ä¸ªTPUçš„æ‰¹å¤„ç†å¤§å°ä¸º`4
    * 1024 * 1024 / 8960 = 468 tokens`ã€‚ä»ä¸Šä¸€èŠ‚æˆ‘ä»¬çŸ¥é“ï¼Œå½“ \(\text{per device batch size} <
    2550 / M_X\) æ—¶ï¼Œæˆ‘ä»¬é€šè¿‡FSDPä¼šå˜æˆICIå—é™ã€‚ç”±äºæˆ‘ä»¬å¯ä»¥åœ¨è¿™é‡Œä½¿ç”¨å®Œæ•´çš„3D podæ¥åˆ†é…3ä¸ªè½´ï¼Œè¿™å°†ç»™æˆ‘ä»¬ä¸€ä¸ªä¸‹é™ä¸º850ï¼Œæˆ‘ä»¬è¿œè¿œä½äºè¿™ä¸ªå€¼ã€‚**æ‰€ä»¥ç­”æ¡ˆæ˜¯ï¼Œå³ä½¿æœ‰3ä¸ªè½´ï¼Œæˆ‘ä»¬ä¹Ÿä¼šæ˜¯é€šä¿¡å—é™çš„ã€‚**'
- en: '**Question:** Now letâ€™s look at mixed tensor parallelism and FSDP. Does there
    exist some combination that lets us remain compute-bound? What amount of FSDP
    and tensor parallelism should we do if so?'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**é—®é¢˜**ï¼šç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹æ··åˆå¼ é‡å¹¶è¡Œå’ŒFSDPã€‚æ˜¯å¦å­˜åœ¨æŸç§ç»„åˆä½¿æˆ‘ä»¬ä¿æŒè®¡ç®—å—é™ï¼Ÿå¦‚æœå­˜åœ¨ï¼Œæˆ‘ä»¬åº”è¯¥è¿›è¡Œå¤šå°‘FSDPå’Œå¼ é‡å¹¶è¡Œï¼Ÿ'
- en: <details><summary>Click here for the answer, once youâ€™ve thought about it!</summary>
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>ç‚¹å‡»æ­¤å¤„æŸ¥çœ‹ç­”æ¡ˆï¼Œåœ¨ä½ æ€è€ƒè¿‡åï¼</summary>
- en: '**Answer**: First letâ€™s check to see if this will even fit. We know that weâ€™ll
    be comms-bound if our per-chip batch size is less than $2550^2 / 2F = 113$. As
    we saw above, weâ€™re slightly above this. So thatâ€™s great! Now to pick the optimal
    amount of FSDP, we can use the formula'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç­”æ¡ˆ**ï¼šé¦–å…ˆè®©æˆ‘ä»¬çœ‹çœ‹è¿™æ˜¯å¦ç”šè‡³èƒ½é€‚åº”ã€‚æˆ‘ä»¬çŸ¥é“å¦‚æœæˆ‘ä»¬çš„æ¯ç‰‡èŠ¯ç‰‡æ‰¹å¤„ç†å¤§å°å°äº $2550^2 / 2F = 113$ï¼Œæˆ‘ä»¬å°†ä¼šé€šä¿¡å—é™ã€‚å¦‚ä¸Šæ‰€è¿°ï¼Œæˆ‘ä»¬ç•¥é«˜äºè¿™ä¸ªå€¼ã€‚æ‰€ä»¥è¿™å¾ˆå¥½ï¼ç°åœ¨ä¸ºäº†é€‰æ‹©æœ€ä¼˜çš„FSDPæ•°é‡ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å…¬å¼'
- en: \[X_{opt} = \sqrt{\frac{2BN}{F}} = \sqrt{\frac{2 \cdot 4.19e6 \cdot 8960}{28672}}
    = 1618\]
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: \[X_{opt} = \sqrt{\frac{2BN}{F}} = \sqrt{\frac{2 \cdot 4.19e6 \cdot 8960}{28672}}
    = 1618\]
- en: Rounding to a reasonable multiple of 2, that gives us roughly 2048-way FSDP
    and 4-way tensor parallelism parallelism. That should work well!</details>
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: å››èˆäº”å…¥åˆ°åˆç†çš„2çš„å€æ•°ï¼Œè¿™ç»™æˆ‘ä»¬å¤§çº¦2048è·¯FSDPå’Œ4è·¯å¼ é‡å¹¶è¡Œã€‚è¿™åº”è¯¥ä¼šå¾ˆå¥½ï¼
- en: '**Takeaways**: We can train LLaMA-3 with a 4M token batch size on a full TPU
    v5p pod with a mixture of data parallelism (1024-way), sequence parallelism (2-way),
    and tensor parallelism (4-way) without being communication-bound. We will be comms-bound
    if we try to do pure FSDP or FSDP + sequence parallelism. The equations weâ€™ve
    cooked up in the previous section are very practical.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¦ç‚¹**ï¼šæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ•°æ®å¹¶è¡Œï¼ˆ1024è·¯ï¼‰ã€åºåˆ—å¹¶è¡Œï¼ˆ2è·¯ï¼‰å’Œå¼ é‡å¹¶è¡Œï¼ˆ4è·¯ï¼‰çš„ç»„åˆï¼Œåœ¨å®Œæ•´çš„TPU v5p podä¸Šä»¥4M tokenæ‰¹å¤„ç†å¤§å°è®­ç»ƒLLaMA-3ï¼Œè€Œä¸ä¼šå—åˆ°é€šä¿¡é™åˆ¶ã€‚å¦‚æœæˆ‘ä»¬å°è¯•çº¯FSDPæˆ–FSDP
    + åºåˆ—å¹¶è¡Œï¼Œæˆ‘ä»¬ä¼šå—åˆ°é€šä¿¡é™åˆ¶ã€‚æˆ‘ä»¬åœ¨ä¸Šä¸€èŠ‚ä¸­æå‡ºçš„æ–¹ç¨‹éå¸¸å®ç”¨ã€‚'
- en: Worked Problems
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å·¥ä½œé—®é¢˜
- en: '**Question 1 [Scaling LLaMA 70B to more chips]:** say we want to train LLaMA
    3-70B on 4 pods with the same batch size. What parallelism scheme would we use?
    Would we be compute or communication bound? Roughly how long would it take to
    train? *Make sure to use the correct roofline bound.*'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**é—®é¢˜1 [å°†LLaMA 70Bæ‰©å±•åˆ°æ›´å¤šèŠ¯ç‰‡]ï¼š**å¦‚æœæˆ‘ä»¬æƒ³åœ¨4ä¸ªpodä¸Šä»¥ç›¸åŒçš„æ‰¹å¤„ç†å¤§å°è®­ç»ƒLLaMA 3-70Bï¼Œæˆ‘ä»¬ä¼šä½¿ç”¨å“ªç§å¹¶è¡Œæ–¹æ¡ˆï¼Ÿæˆ‘ä»¬ä¼šæ˜¯è®¡ç®—å—é™è¿˜æ˜¯é€šä¿¡å—é™ï¼Ÿå¤§è‡´éœ€è¦å¤šé•¿æ—¶é—´æ¥è®­ç»ƒï¼Ÿ*ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„rooflineç•Œé™*ã€‚'
- en: '**Question 2 [LLaMA 405B]:**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**é—®é¢˜2 [LLaMA 405B]ï¼š**'
- en: (a) Using the LLaMA 3-405B [config](https://huggingface.co/meta-llama/Llama-3.1-405B/blob/main/config.json),
    write a table with all the key hyperparameters as above. How many total parameters
    does this model have? How many FLOPs per training step? How many FLOPs do we perform
    if we train for 15T tokens?
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: (a) ä½¿ç”¨ LLaMA 3-405B [é…ç½®](https://huggingface.co/meta-llama/Llama-3.1-405B/blob/main/config.json)ï¼Œç¼–å†™ä¸€ä¸ªåŒ…å«æ‰€æœ‰ä¸Šè¿°å…³é”®è¶…å‚æ•°çš„è¡¨æ ¼ã€‚è¿™ä¸ªæ¨¡å‹æ€»å…±æœ‰å¤šå°‘ä¸ªå‚æ•°ï¼Ÿæ¯ä¸€æ­¥è®­ç»ƒæœ‰å¤šå°‘ä¸ªFLOPsï¼Ÿå¦‚æœæˆ‘ä»¬è®­ç»ƒ15Tä¸ªtokenï¼Œä¼šæœ‰å¤šå°‘FLOPsï¼Ÿ
- en: (b) Assume we want to train on 8 TPU v5p pods. What parallelism scheme would
    we use? How long would training take? Would be compute or comms bound?
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: (b) å‡è®¾æˆ‘ä»¬æƒ³åœ¨8ä¸ªTPU v5p podä¸Šè¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬ä¼šä½¿ç”¨å“ªç§å¹¶è¡Œæ–¹æ¡ˆï¼Ÿè®­ç»ƒéœ€è¦å¤šé•¿æ—¶é—´ï¼Ÿä¼šæ˜¯è®¡ç®—å—é™è¿˜æ˜¯é€šä¿¡å—é™ï¼Ÿ
- en: Thatâ€™s all for Section 6\. For Section 7, about Transformer inference, click
    [here](../inference).</d-article>  <d-appendix><d-footnote-list><d-citation-list>###
    Miscellaneous
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯ç¬¬6èŠ‚çš„æ‰€æœ‰å†…å®¹ã€‚å…³äºç¬¬7èŠ‚ï¼Œå…³äºTransformeræ¨ç†ï¼Œè¯·ç‚¹å‡»[è¿™é‡Œ](../inference)ã€‚</d-article>  <d-appendix><d-footnote-list><d-citation-list>###
    æ‚é¡¹
- en: ^*Work done at Google DeepMind, now at MatX.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ^*åœ¨Google DeepMindå®Œæˆçš„å·¥ä½œï¼Œç°åœ¨åœ¨MatXã€‚
- en: Citation
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¼•ç”¨
- en: 'For attribution in academic contexts, please cite this work as:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å­¦æœ¯ç¯å¢ƒä¸­è¿›è¡Œå½’å±æ—¶ï¼Œè¯·å¼•ç”¨æ­¤å·¥ä½œå¦‚ä¸‹ï¼š
- en: '[PRE0]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'or as a BibTeX entry:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…ä½œä¸ºä¸€ä¸ªBibTeXæ¡ç›®ï¼š
- en: '[PRE1]</d-citation-list></d-footnote-list></d-appendix> <d-bibliography src="/scaling-book/assets/bibliography/main.bib"></d-bibliography></d-byline>'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE1] <d-citation-list></d-citation-list> <d-footnote-list></d-footnote-list>
    <d-appendix> <d-bibliography src="/scaling-book/assets/bibliography/main.bib"></d-bibliography></d-byline>'
