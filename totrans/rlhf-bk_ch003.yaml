- en: Definitions & Background
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter includes all the definitions, symbols, and operations frequently
    used in the RLHF process and with a quick overview of language models, which is
    the guiding application of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Language Modeling Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The majority of modern language models are trained to learn the joint probability
    distribution of sequences of tokens (words, subwords, or characters) in an autoregressive
    manner. Autoregression simply means that each next prediction depends on the previous
    entities in the sequence. Given a sequence of tokens <semantics><mrow><mi>x</mi><mo>=</mo><mo
    stretchy="false" form="prefix">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><msub><mi>x</mi><mi>T</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">x
    = (x_1, x_2, \ldots, x_T)</annotation></semantics>, the model factorizes the probability
    of the entire sequence into a product of conditional distributions:'
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><msub><mi>P</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><munderover><mo>‚àè</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><msub><mi>P</mi><mi>Œ∏</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>‚à£</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></msub><mo
    stretchy="false" form="postfix">)</mo><mi>.</mi><mrow><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P_{\theta}(x)
    = \prod_{t=1}^{T} P_{\theta}(x_{t} \mid x_{1}, \ldots, x_{t-1}).\qquad{(1)}</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to fit a model that accurately predicts this, the goal is often to
    maximize the likelihood of the training data as predicted by the current model.
    To do so, we can minimize a negative log-likelihood (NLL) loss:'
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><msub><mi>‚Ñí</mi><mtext mathvariant="normal">LM</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>‚àí</mi><msub><mi>ùîº</mi><mrow><mi>x</mi><mo>‚àº</mo><mi>ùíü</mi></mrow></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><munderover><mo>‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mi
    mathvariant="normal">log</mi><msub><mi>P</mi><mi>Œ∏</mi></msub><mrow><mo stretchy="true"
    form="prefix">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>‚à£</mo><msub><mi>x</mi><mrow><mo><</mo><mi>t</mi></mrow></msub><mo
    stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mi>.</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>2</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">\mathcal{L}_{\text{LM}}(\theta)=-\,\mathbb{E}_{x
    \sim \mathcal{D}}\left[\sum_{t=1}^{T}\log P_{\theta}\left(x_t \mid x_{<t}\right)\right].
    \qquad{(2)}</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: In practice, one uses a cross-entropy loss with respect to each next-token prediction,
    computed by comparing the true token in a sequence to what was predicted by the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Language models come in many architectures with different trade-offs in terms
    of knowledge, speed, and other performance characteristics. Modern LMs, including
    ChatGPT, Claude, Gemini, etc., most often use **decoder-only Transformers** [[49]](ch021.xhtml#ref-Vaswani2017AttentionIA).
    The core innovation of the Transformer was heavily utilizing the **self-attention**
    [[50]](ch021.xhtml#ref-Bahdanau2014NeuralMT) mechanism to allow the model to directly
    attend to concepts in context and learn complex mappings. Throughout this book,
    particularly when covering reward models in Chapter 7, we will discuss adding
    new heads or modifying a language modeling (LM) head of the transformer. The LM
    head is a final linear projection layer that maps from the model‚Äôs internal embedding
    space to the tokenizer space (a.k.a. vocabulary). We‚Äôll see in this book that
    different ‚Äúheads‚Äù of a language model can be applied to finetune the model to
    different purposes ‚Äì in RLHF this is most often done when training a reward model,
    which is highlighted in Chapter 7.
  prefs: []
  type: TYPE_NORMAL
- en: ML Definitions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Kullback-Leibler (KL) divergence (<semantics><mrow><msub><mi>ùíü</mi><mtext
    mathvariant="normal">KL</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>P</mi><mo
    stretchy="false" form="prefix">|</mo><mo stretchy="false" form="prefix">|</mo><mi>Q</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{D}_{\text{KL}}(P
    || Q)</annotation></semantics>)**, also known as KL divergence, is a measure of
    the difference between two probability distributions. For discrete probability
    distributions <semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics>
    and <semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics>
    defined on the same probability space <semantics><mi>ùí≥</mi><annotation encoding="application/x-tex">\mathcal{X}</annotation></semantics>,
    the KL distance from <semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics>
    to <semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics>
    is defined as:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <semantics><mrow><msub><mi>ùíü</mi><mtext mathvariant="normal">KL</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>P</mi><mo stretchy="false" form="prefix">|</mo><mo
    stretchy="false" form="prefix">|</mo><mi>Q</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><munder><mo>‚àë</mo><mrow><mi>x</mi><mo>‚àà</mo><mi>ùí≥</mi></mrow></munder><mi>P</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mrow><mi
    mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>P</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><mi>Q</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo
    stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="false" form="prefix">(</mo><mn>3</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\mathcal{D}_{\text{KL}}(P
    || Q) = \sum_{x \in \mathcal{X}} P(x) \log \left(\frac{P(x)}{Q(x)}\right) \qquad{(3)}</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: NLP Definitions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Prompt (<semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics>)**:
    The input text given to a language model to generate a response or completion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Completion (<semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics>)**:
    The output text generated by a language model in response to a prompt. Often the
    completion is denoted as <semantics><mrow><mi>y</mi><mo>‚à£</mo><mi>x</mi></mrow><annotation
    encoding="application/x-tex">y\mid x</annotation></semantics>. Rewards and other
    values are often computed as <semantics><mrow><mi>r</mi><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo>‚à£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">r(y\mid
    x)</annotation></semantics> or <semantics><mrow><mi>P</mi><mo stretchy="false"
    form="prefix">(</mo><mi>y</mi><mo>‚à£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">P(y\mid x)</annotation></semantics>.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Chosen Completion (<semantics><msub><mi>y</mi><mi>c</mi></msub><annotation
    encoding="application/x-tex">y_c</annotation></semantics>)**: The completion that
    is selected or preferred over other alternatives, often denoted as <semantics><msub><mi>y</mi><mrow><mi>c</mi><mi>h</mi><mi>o</mi><mi>s</mi><mi>e</mi><mi>n</mi></mrow></msub><annotation
    encoding="application/x-tex">y_{chosen}</annotation></semantics>.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rejected Completion (<semantics><msub><mi>y</mi><mi>r</mi></msub><annotation
    encoding="application/x-tex">y_r</annotation></semantics>)**: The disfavored completion
    in a pairwise setting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Preference Relation (<semantics><mo>‚âª</mo><annotation encoding="application/x-tex">\succ</annotation></semantics>)**:
    A symbol indicating that one completion is preferred over another, e.g., <semantics><mrow><msub><mi>y</mi><mrow><mi>c</mi><mi>h</mi><mi>o</mi><mi>s</mi><mi>e</mi><mi>n</mi></mrow></msub><mo>‚âª</mo><msub><mi>y</mi><mrow><mi>r</mi><mi>e</mi><mi>j</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>e</mi><mi>d</mi></mrow></msub></mrow><annotation
    encoding="application/x-tex">y_{chosen} \succ y_{rejected}</annotation></semantics>.
    E.g. a reward model predicts the probability of a preference relation, <semantics><mrow><mi>P</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>‚âª</mo><msub><mi>y</mi><mi>r</mi></msub><mo>‚à£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">P(y_c
    \succ y_r \mid x)</annotation></semantics>.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Policy (<semantics><mi>œÄ</mi><annotation encoding="application/x-tex">\pi</annotation></semantics>)**:
    A probability distribution over possible completions, parameterized by <semantics><mi>Œ∏</mi><annotation
    encoding="application/x-tex">\theta</annotation></semantics>: <semantics><mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>y</mi><mo>‚à£</mo><mi>x</mi><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\pi_\theta(y\mid
    x)</annotation></semantics>.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RL Definitions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Reward (<semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics>)**:
    A scalar value indicating the desirability of an action or state, typically denoted
    as <semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics>.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Action (<semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics>)**:
    A decision or move made by an agent in an environment, often represented as <semantics><mrow><mi>a</mi><mo>‚àà</mo><mi>A</mi></mrow><annotation
    encoding="application/x-tex">a \in A</annotation></semantics>, where <semantics><mi>A</mi><annotation
    encoding="application/x-tex">A</annotation></semantics> is the set of possible
    actions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**State (<semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics>)**:
    The current configuration or situation of the environment, usually denoted as
    <semantics><mrow><mi>s</mi><mo>‚àà</mo><mi>S</mi></mrow><annotation encoding="application/x-tex">s
    \in S</annotation></semantics>, where <semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics>
    is the state space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trajectory (<semantics><mi>œÑ</mi><annotation encoding="application/x-tex">\tau</annotation></semantics>)**:
    A trajectory <semantics><mi>œÑ</mi><annotation encoding="application/x-tex">\tau</annotation></semantics>
    is a sequence of states, actions, and rewards experienced by an agent: <semantics><mrow><mi>œÑ</mi><mo>=</mo><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo>,</mo><msub><mi>a</mi><mn>0</mn></msub><mo>,</mo><msub><mi>r</mi><mn>0</mn></msub><mo>,</mo><msub><mi>s</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>r</mi><mn>1</mn></msub><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi><mo>,</mo><msub><mi>s</mi><mi>T</mi></msub><mo>,</mo><msub><mi>a</mi><mi>T</mi></msub><mo>,</mo><msub><mi>r</mi><mi>T</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\tau
    = (s_0, a_0, r_0, s_1, a_1, r_1, ..., s_T, a_T, r_T)</annotation></semantics>.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trajectory Distribution (<semantics><mrow><mo stretchy="false" form="prefix">(</mo><mi>œÑ</mi><mo>‚à£</mo><mi>œÄ</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\tau\mid\pi)</annotation></semantics>)**:
    The probability of a trajectory under policy <semantics><mi>œÄ</mi><annotation
    encoding="application/x-tex">\pi</annotation></semantics> is <semantics><mrow><mi>P</mi><mo
    stretchy="false" form="prefix">(</mo><mi>œÑ</mi><mo>‚à£</mo><mi>œÄ</mi><mo stretchy="false"
    form="postfix">)</mo><mo>=</mo><mi>p</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo
    stretchy="false" form="postfix">)</mo><msubsup><mo>‚àè</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>T</mi></msubsup><mi>œÄ</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>‚à£</mo><msub><mi>s</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mi>p</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>‚à£</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">P(\tau\mid\pi)
    = p(s_0)\prod_{t=0}^T \pi(a_t\mid s_t)p(s_{t+1}\mid s_t,a_t)</annotation></semantics>,
    where <semantics><mrow><mi>p</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">p(s_0)</annotation></semantics>
    is the prior state distribution and <semantics><mrow><mi>p</mi><mo stretchy="false"
    form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>‚à£</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">p(s_{t+1}\mid
    s_t,a_t)</annotation></semantics> is the transition probability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Policy (<semantics><mi>œÄ</mi><annotation encoding="application/x-tex">\pi</annotation></semantics>)**,
    also called the **policy model** in RLHF: In RL, a policy is a strategy or rule
    that the agent follows to decide which action to take in a given state: <semantics><mrow><mi>œÄ</mi><mo
    stretchy="false" form="prefix">(</mo><mi>a</mi><mo>‚à£</mo><mi>s</mi><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\pi(a\mid
    s)</annotation></semantics>.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Discount Factor (<semantics><mi>Œ≥</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics>)**:
    A scalar <semantics><mrow><mn>0</mn><mo>‚â§</mo><mi>Œ≥</mi><mo><</mo><mn>1</mn></mrow><annotation
    encoding="application/x-tex">0 \le \gamma < 1</annotation></semantics> that exponentially
    down-weights future rewards in the return, trading off immediacy versus long-term
    gain and guaranteeing convergence for infinite-horizon sums. Sometimes discounting
    is not used, which is equivalent to <semantics><mrow><mi>Œ≥</mi><mo>=</mo><mn>1</mn></mrow><annotation
    encoding="application/x-tex">\gamma=1</annotation></semantics>.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Value Function (<semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation></semantics>)**:
    A function that estimates the expected cumulative reward from a given state: <semantics><mrow><mi>V</mi><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>ùîº</mi><mo
    stretchy="false" form="prefix">[</mo><msubsup><mo>‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>‚àû</mi></msubsup><msup><mi>Œ≥</mi><mi>t</mi></msup><msub><mi>r</mi><mi>t</mi></msub><mo>‚à£</mo><msub><mi>s</mi><mn>0</mn></msub><mo>=</mo><mi>s</mi><mo
    stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">V(s)
    = \mathbb{E}[\sum_{t=0}^{\infty} \gamma^t r_t \mid s_0 = s]</annotation></semantics>.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Q-Function (<semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics>)**:
    A function that estimates the expected cumulative reward from taking a specific
    action in a given state: <semantics><mrow><mi>Q</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mi>ùîº</mi><mo stretchy="false"
    form="prefix">[</mo><msubsup><mo>‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>‚àû</mi></msubsup><msup><mi>Œ≥</mi><mi>t</mi></msup><msub><mi>r</mi><mi>t</mi></msub><mo>‚à£</mo><msub><mi>s</mi><mn>0</mn></msub><mo>=</mo><mi>s</mi><mo>,</mo><msub><mi>a</mi><mn>0</mn></msub><mo>=</mo><mi>a</mi><mo
    stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">Q(s,a)
    = \mathbb{E}[\sum_{t=0}^{\infty} \gamma^t r_t \mid s_0 = s, a_0 = a]</annotation></semantics>.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Advantage Function (<semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics>)**:
    The advantage function <semantics><mrow><mi>A</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">A(s,a)</annotation></semantics>
    quantifies the relative benefit of taking action <semantics><mi>a</mi><annotation
    encoding="application/x-tex">a</annotation></semantics> in state <semantics><mi>s</mi><annotation
    encoding="application/x-tex">s</annotation></semantics> compared to the average
    action. It‚Äôs defined as <semantics><mrow><mi>A</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mi>Q</mi><mo stretchy="false"
    form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo><mo>‚àí</mo><mi>V</mi><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">A(s,a) = Q(s,a) - V(s)</annotation></semantics>.
    Advantage functions (and value functions) can depend on a specific policy, <semantics><mrow><msup><mi>A</mi><mi>œÄ</mi></msup><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">A^\pi(s,a)</annotation></semantics>.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Policy-conditioned Values (<semantics><mrow><mo stretchy="false" form="prefix">[</mo><msup><mo
    stretchy="false" form="postfix">]</mo><mrow><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><mi>‚ãÖ</mi><mo
    stretchy="false" form="postfix">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">[]^{\pi(\cdot)}</annotation></semantics>)**:
    Across RL derivations and implementations, a crucial component of the theory and
    practice is collecting data or values conditioned on a specific policy. Throughout
    this book we will switch between the simpler notation of value functions (<semantics><mrow><mi>V</mi><mo>,</mo><mi>A</mi><mo>,</mo><mi>Q</mi><mo>,</mo><mi>G</mi></mrow><annotation
    encoding="application/x-tex">V,A,Q,G</annotation></semantics>) and their specific
    policy-conditioned values (<semantics><mrow><msup><mi>V</mi><mi>œÄ</mi></msup><mo>,</mo><msup><mi>A</mi><mi>œÄ</mi></msup><mo>,</mo><msup><mi>Q</mi><mi>œÄ</mi></msup></mrow><annotation
    encoding="application/x-tex">V^\pi,A^\pi,Q^\pi</annotation></semantics>). Also
    crucial in the expected value computation is sampling from data <semantics><mi>d</mi><annotation
    encoding="application/x-tex">d</annotation></semantics>, that is conditioned on
    a specific policy, <semantics><msub><mi>d</mi><mi>œÄ</mi></msub><annotation encoding="application/x-tex">d_\pi</annotation></semantics>
    (e.g., <semantics><mrow><mi>s</mi><mo>‚àº</mo><msub><mi>d</mi><mi>œÄ</mi></msub></mrow><annotation
    encoding="application/x-tex">s \sim d_\pi</annotation></semantics> and <semantics><mrow><mi>a</mi><mo>‚àº</mo><mi>œÄ</mi><mo
    stretchy="false" form="prefix">(</mo><mi>‚ãÖ</mi><mo>‚à£</mo><mi>s</mi><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">a \sim \pi(\cdot\mid
    s)</annotation></semantics> when estimating <semantics><mrow><msub><mi>ùîº</mi><mrow><mi>s</mi><mo>‚àº</mo><msub><mi>d</mi><mi>œÄ</mi></msub><mo>,</mo><mi>a</mi><mo>‚àº</mo><mi>œÄ</mi><mo
    stretchy="false" form="prefix">(</mo><mi>‚ãÖ</mi><mo>‚à£</mo><mi>s</mi><mo stretchy="false"
    form="postfix">)</mo></mrow></msub><mo stretchy="false" form="prefix">[</mo><msup><mi>A</mi><mi>œÄ</mi></msup><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false"
    form="postfix">)</mo><mo stretchy="false" form="postfix">]</mo></mrow><annotation
    encoding="application/x-tex">\mathbb{E}_{s\sim d_\pi,\,a\sim\pi(\cdot\mid s)}[A^\pi(s,a)]</annotation></semantics>).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Expectation of Reward Optimization**: The primary goal in RL, which involves
    maximizing the expected cumulative reward:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <semantics><mrow><munder><mi mathvariant="normal">max</mi><mi>Œ∏</mi></munder><msub><mi>ùîº</mi><mrow><mi>s</mi><mo>‚àº</mo><msub><mi>œÅ</mi><mi>œÄ</mi></msub><mo>,</mo><mi>a</mi><mo>‚àº</mo><msub><mi>œÄ</mi><mi>Œ∏</mi></msub></mrow></msub><mo
    stretchy="false" form="prefix">[</mo><munderover><mo>‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mo
    accent="false">‚àû</mo></munderover><msup><mi>Œ≥</mi><mi>t</mi></msup><msub><mi>r</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">]</mo><mrow><mo stretchy="false" form="prefix">(</mo><mn>4</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\max_{\theta}
    \mathbb{E}_{s \sim \rho_\pi, a \sim \pi_\theta}[\sum_{t=0}^{\infty} \gamma^t r_t]\qquad{(4)}</annotation></semantics>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: where <semantics><msub><mi>œÅ</mi><mi>œÄ</mi></msub><annotation encoding="application/x-tex">\rho_\pi</annotation></semantics>
    is the state distribution under policy <semantics><mi>œÄ</mi><annotation encoding="application/x-tex">\pi</annotation></semantics>,
    and <semantics><mi>Œ≥</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics>
    is the discount factor.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Finite Horizon Reward (<semantics><mrow><mi>J</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">J(\pi_\theta)</annotation></semantics>)**:
    The expected finite-horizon discounted return of the policy <semantics><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><annotation
    encoding="application/x-tex">\pi_\theta</annotation></semantics>, parameterized
    by <semantics><mi>Œ∏</mi><annotation encoding="application/x-tex">\theta</annotation></semantics>
    is defined as:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <semantics><mrow><mi>J</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ùîº</mi><mrow><mi>œÑ</mi><mo>‚àº</mo><msub><mi>œÄ</mi><mi>Œ∏</mi></msub></mrow></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><munderover><mo>‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>T</mi></munderover><msup><mi>Œ≥</mi><mi>t</mi></msup><msub><mi>r</mi><mi>t</mi></msub><mo
    stretchy="true" form="postfix">]</mo></mrow><mrow><mo stretchy="false" form="prefix">(</mo><mn>5</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">J(\pi_\theta)
    = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \gamma^t r_t \right]\qquad{(5)}</annotation></semantics>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: where <semantics><mrow><mi>œÑ</mi><mo>‚àº</mo><msub><mi>œÄ</mi><mi>Œ∏</mi></msub></mrow><annotation
    encoding="application/x-tex">\tau \sim \pi_\theta</annotation></semantics> denotes
    trajectories sampled by following policy <semantics><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><annotation
    encoding="application/x-tex">\pi_\theta</annotation></semantics> and <semantics><mi>T</mi><annotation
    encoding="application/x-tex">T</annotation></semantics> is the finite horizon.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**On-policy**: In RLHF, particularly in the debate between RL and Direct Alignment
    Algorithms, the discussion of **on-policy** data is common. In the RL literature,
    on-policy means that the data is generated *exactly* by the current form of the
    agent, but in the general preference-tuning literature, on-policy is expanded
    to mean generations from that edition of model ‚Äì e.g.¬†a instruction-tuned checkpoint
    before running any preference fine-tuning. In this context, off-policy could be
    data generated by any other language model being used in post-training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RLHF Only Definitions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Reference Model (<semantics><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><annotation
    encoding="application/x-tex">\pi_{\text{ref}}</annotation></semantics>)**: This
    is a saved set of parameters used in RLHF where outputs of it are used to regularize
    the optimization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extended Glossary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Synthetic Data**: This is any training data for an AI model that is the output
    from another AI system. This could be anything from text generated from an open-ended
    prompt of a model to a model rewriting existing content.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distillation**: Distillation is a general set of practices in training AI
    models where a model is trained on the outputs of a stronger model. This is a
    type of synthetic data known to make strong, smaller models. Most models make
    the rules around distillation clear through either the license, for open weight
    models, or the terms of service, for models accessible only via API. The term
    distillation is now overloaded with a specific technical definition from the ML
    literature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**(Teacher-student) Knowledge Distillation**: Knowledge distillation from a
    specific teacher to a student model is a specific type of distillation above and
    where the term originated. It is a specific deep learning method where a neural
    network loss is modified to learn from the log-probabilities of the teacher model
    over multiple potential tokens/logits, instead of learning directly from a chosen
    output [[51]](ch021.xhtml#ref-hinton2015distilling). An example of a modern series
    of models trained with Knowledge Distillation is Gemma 2 [[52]](ch021.xhtml#ref-team2024gemma)
    or Gemma 3\. For a language modeling setup, the next-token loss function can be
    modified as follows [[53]](ch021.xhtml#ref-agarwal2024policy), where the student
    model <semantics><msub><mi>P</mi><mi>Œ∏</mi></msub><annotation encoding="application/x-tex">P_\theta</annotation></semantics>
    learns from the teacher distribution <semantics><msub><mi>P</mi><mi>œï</mi></msub><annotation
    encoding="application/x-tex">P_\phi</annotation></semantics>:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <semantics><mrow><msub><mi>‚Ñí</mi><mtext mathvariant="normal">KD</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>‚àí</mi><msub><mi>ùîº</mi><mrow><mi>x</mi><mo>‚àº</mo><mi>ùíü</mi></mrow></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><munderover><mo>‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><msub><mi>P</mi><mi>œï</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>‚à£</mo><msub><mi>x</mi><mrow><mo><</mo><mi>t</mi></mrow></msub><mo
    stretchy="false" form="postfix">)</mo><mi mathvariant="normal">log</mi><msub><mi>P</mi><mi>Œ∏</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>‚à£</mo><msub><mi>x</mi><mrow><mo><</mo><mi>t</mi></mrow></msub><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow><mi>.</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>6</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">\mathcal{L}_{\text{KD}}(\theta) = -\,\mathbb{E}_{x
    \sim \mathcal{D}}\left[\sum_{t=1}^{T} P_{\phi}(x_t \mid x_{<t}) \log P_{\theta}(x_t
    \mid x_{<t})\right]. \qquad{(6)}</annotation></semantics>
  prefs: []
  type: TYPE_NORMAL
- en: '**In-context Learning (ICL)**: In-context here refers to any information within
    the context window of the language model. Usually, this is information added to
    the prompt. The simplest form of in-context learning is adding examples of a similar
    form before the prompt. Advanced versions can learn which information to include
    for a specific use-case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Chain of Thought (CoT)**: Chain of thought is a specific behavior of language
    models where they are steered towards a behavior that breaks down a problem in
    a step-by-step form. The original version of this was through the prompt ‚ÄúLet‚Äôs
    think step-by-step‚Äù [[54]](ch021.xhtml#ref-wei2022chain).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
