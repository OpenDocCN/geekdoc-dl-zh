- en: All About Rooflines
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 所有关于屋顶线的内容
- en: 原文：[https://jax-ml.github.io/scaling-book/roofline](https://jax-ml.github.io/scaling-book/roofline)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://jax-ml.github.io/scaling-book/roofline](https://jax-ml.github.io/scaling-book/roofline)
- en: '<d-title>Part 1 of [How To Scale Your Model](/scaling-book) ([Part 0: Introduction](..)
    | [Part 2: TPUs](../tpus))'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: <d-title>《如何扩展你的模型》的第一部分[How To Scale Your Model](/scaling-book) ([第0部分：介绍](..)
    | [第2部分：TPUs](../tpus))
- en: 'When we run algorithms on hardware, we''re bounded by three things: how fast
    our computer can do math (OPs/second), the bandwidth available for moving data
    around (bytes/second), and the total memory available to store data (bytes). These
    “roofline” constraints let us upper and lower bound the time of a given computation.</d-title>  <d-byline><d-article><d-contents>###
    Contents'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在硬件上运行算法时，我们受限于三个方面：我们的计算机进行数学运算的速度（每秒OPs），用于移动数据的可用带宽（每秒字节），以及存储数据的总内存（字节）。这些“屋顶线”约束让我们能够上下界一个给定计算的时间。</d-title>  <d-byline><d-article><d-contents>###
    内容
- en: '[Where Does the Time Go?](#where-does-the-time-go)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[时间都去哪儿了？](#where-does-the-time-go)'
- en: '[Visualizing rooflines](#visualizing-rooflines)'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[可视化屋顶线](#visualizing-rooflines)'
- en: '[Matrix multiplication](#matrix-multiplication)'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[矩阵乘法](#matrix-multiplication)'
- en: '[Network communication rooflines](#network-communication-rooflines)'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[网络通信屋顶线](#network-communication-rooflines)'
- en: '[A Few Problems to Work](#a-few-problems-to-work)</d-contents>'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[一些需要解决的问题](#a-few-problems-to-work)</d-contents>'
- en: Where Does the Time Go?
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间都去哪儿了？
- en: 'Let’s start with an extremely simple question: *why does an algorithm take
    50ms instead of 50s or 5ms*? What is actually happening within the model that
    takes substantial time and how long should we expect it to take?'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个非常简单的问题开始：*为什么算法需要50ms而不是50s或5ms*？模型内部实际上发生了什么，需要大量时间，我们应该期望它需要多长时间？
- en: '**Computation:** A deep learning model is effectively a bunch of matrix multiplications,
    each composed of floating-point multiplication and addition ‘operations’ (FLOPs).
    Our accelerator speed determines how long these take to compute:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**计算：** 深度学习模型实际上是一系列矩阵乘法，每个乘法由浮点数乘法和加法‘操作’（FLOPs）组成。我们的加速器速度决定了这些计算所需的时间：'
- en: \[\begin{equation} T_\text{math} = \frac{\text{Computation FLOPs}}{\text{Accelerator
    FLOPs/s}} \end{equation}\]
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation} T_\text{math} = \frac{\text{计算FLOPs}}{\text{加速器FLOPs/s}}
    \end{equation}\]
- en: For instance, an NVIDIA H100 can perform about 9.89e14 bfloat16<d-footnote>bf16
    is short for [bfloat16](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format),
    a 16-bit floating point format often used in ML.</d-footnote> FLOPs/s while a
    TPU v6e can perform 9.1e14 FLOPs/s.<d-footnote>H100s and B200s can usually only
    achieve around 80-85% of the claimed peak FLOPs, while TPUs can get closer to
    95% in normal use.</d-footnote> That means doing 1e12 FLOPs on an H100 will take
    (roughly) `1e12 / 9.89e14 = 1.01ms` and `1e12 / 9.1e14 = 1.1ms` on a TPU v6e.<d-footnote>Note
    that these chips are priced differently, and this comparison does not normalize
    to cost.</d-footnote>
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，NVIDIA H100可以每秒执行大约9.89e14 bfloat16 FLOPs，而TPU v6e可以执行9.1e14 FLOPs/s。<d-footnote>bf16是[bfloat16](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format)的缩写，这是一种常用于机器学习的16位浮点格式。</d-footnote><d-footnote>H100s和B200s通常只能达到声称的峰值FLOPs的80-85%，而TPUs在正常使用中可以接近95%。</d-footnote>这意味着在H100上执行1e12
    FLOPs将需要（大约）`1e12 / 9.89e14 = 1.01ms`，而在TPU v6e上需要`1e12 / 9.1e14 = 1.1ms`。</d-footnote><d-footnote>请注意，这些芯片的价格不同，此比较并未按成本进行归一化。</d-footnote>
- en: '**Communication within a chip:** *Within an accelerator*, tensors need to be
    transferred between on-chip memory (HBM) and the compute cores. You’ll see the
    bandwidth of this link referred to as “HBM bandwidth”<d-footnote>NVIDIA also calls
    this "memory bandwidth."</d-footnote> On an H100, [this is about 3.35TB/s](https://www.nvidia.com/en-us/data-center/h100/)
    and on TPU v6e [this is about 1.6TB/s](https://cloud.google.com/tpu/docs/v6e).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**芯片内部的通信：** 在一个加速器内部，张量需要在片上内存（HBM）和计算核心之间传输。你将看到这个链路的带宽被称为“HBM带宽”<d-footnote>NVIDIA也称之为"内存带宽"。</d-footnote>在H100上，[这个带宽大约是3.35TB/s](https://www.nvidia.com/en-us/data-center/h100/)，在TPU
    v6e上[这个带宽大约是1.6TB/s](https://cloud.google.com/tpu/docs/v6e)。'
- en: '**Communication between chips:** When we distribute a model *across multiple
    accelerators*, tensors frequently need to be transferred between them. There are
    often a few options for this on our hardware (ICI, DCN, and PCIe), each with different
    bandwidths.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**芯片间的通信：** 当我们将模型*分布到多个加速器上*时，张量经常需要在它们之间传输。在我们的硬件上通常有几个选项（ICI、DCN和PCIe），每个选项都有不同的带宽。'
- en: 'Whether the communication is within a chip or between chips, we measure this
    in bytes/s and estimate the total communication time with:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 不论通信是在芯片内部还是芯片之间，我们用字节/s来衡量，并用以下公式估计总的通信时间：
- en: \[\begin{equation} T_\text{comms} = \frac{\text{Communication Bytes}}{\text{Network/Memory
    Bandwidth Bytes/s}} \end{equation}\]
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation} T_\text{comms} = \frac{\text{Communication Bytes}}{\text{Network/Memory
    Bandwidth Bytes/s}} \end{equation}\]
- en: Typically (but not always), computation within a single chip can be overlapped
    with communication within a chip and between chips. This means **we can lower-bound
    training and inference time by using the maximum of computation and communication
    time**. We can also **upper-bound with their sum**. In practice, we optimize against
    the maximum as the algebra is simpler and we can usually come close to this bound
    by overlapping our communication and computation. If we optimize with the maximum
    in mind then the lower and upper bounds differ by at most a factor of 2 since
    $T_\text{math} + T_\text{comms} \leq 2 * \max(T_\text{math}, T_\text{comms})$.
    We then increase accuracy beyond this by modeling ‘overlap regions’ and overheads,
    which can be informed by profiling your specific model and target system.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 通常（但不总是），单个芯片内的计算可以与芯片内和芯片间的通信重叠。这意味着**我们可以通过使用计算和通信时间的最大值来降低训练和推理时间的下限**。我们也可以**通过它们的和来设定上限**。在实践中，我们针对最大值进行优化，因为代数更简单，我们通常可以通过重叠我们的通信和计算来接近这个界限。如果我们考虑到最大值进行优化，那么下限和上限的差异最多是2倍，因为$T_\text{math}
    + T_\text{comms} \leq 2 * \max(T_\text{math}, T_\text{comms})$。然后我们通过建模“重叠区域”和开销来提高准确性，这些可以由对特定模型和目标系统的分析来指导。
- en: \[\begin{equation} T_\text{lower}=\max(T_\text{math}, T_\text{comms}) \end{equation}\]
    \[\begin{equation} T_\text{upper} = T_\text{math} + T_\text{comms} \end{equation}\]
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation} T_\text{lower}=\max(T_\text{math}, T_\text{comms}) \end{equation}\]
    \[\begin{equation} T_\text{upper} = T_\text{math} + T_\text{comms} \end{equation}\]
- en: If we assume we can perfectly overlap communication and computation, when $T_\text{math}
    > T_\text{comms}$, we see full utilization from our hardware. We call this being
    “compute-bound”. When $T_\text{comms} > T_\text{math}$, we tend to be “communication-bound”
    and at least some fraction of our accelerator FLOPs/s is wasted waiting for data
    to be passed around. One way to tell if an operation will be compute or communication-bound
    is to look at its “*arithmetic intensity*” or “*operational intensity*”.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们假设可以完美地重叠通信和计算，当$T_\text{math} > T_\text{comms}$时，我们可以从硬件中获得完全的利用率。我们称这为“计算限制”。当$T_\text{comms}
    > T_\text{math}$时，我们往往处于“通信限制”，并且至少有一部分加速器FLOPs/s被浪费在等待数据传输上。判断一个操作是计算限制还是通信限制的一种方法是通过查看其“*算术强度*”或“*操作强度*”。
- en: '**Definition:** the arithmetic intensity of an algorithm is given by the ratio
    of the total FLOPs it performs to the number of bytes it needs to communicate
    — either within a chip or between chips.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**：算法的算术强度由其执行的总FLOPs与需要通信的字节数之比给出——无论是芯片内部还是芯片之间。'
- en: \[\begin{equation} \text{Arithmetic Intensity} = \frac{\text{Computation FLOPs}}{\text{Communication
    Bytes}} \end{equation}\]
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation} \text{Arithmetic Intensity} = \frac{\text{Computation FLOPs}}{\text{Communication
    Bytes}} \end{equation}\]
- en: Arithmetic intensity measures the “FLOPs per byte” of a given operation. To
    a first order, when our arithmetic intensity is high, $T_\text{math}$ is large
    compared to $T_\text{comms}$ and we typically use most of the available FLOPs.
    When the opposite is true, we spent more time on comms and waste FLOPs. The point
    where this crossover happens is the “peak arithmetic intensity” of our hardware,
    the ratio of peak accelerator FLOPs/s to accelerator bandwidth.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 算术强度衡量的是给定操作的“每字节FLOPs”。在第一阶近似中，当我们的算术强度高时，与$T_\text{comms}$相比，$T_\text{math}$较大，我们通常使用大部分可用的FLOPs。当情况相反时，我们在通信上花费更多时间，浪费FLOPs。这种交叉点发生的地方是硬件的“峰值算术强度”，即峰值加速器FLOPs/s与加速器带宽的比率。
- en: \[\begin{align*} T_\text{math} > T_\text{comms} \Leftrightarrow \frac{\text{Computation
    FLOPs}} {\text{Accelerator FLOPs/s}} > \frac{\text{Communication Bytes}}{\text{Bandwidth
    Bytes/s}} & \\[0.5em] \Leftrightarrow \frac{\text{Computation FLOPs}}{\text{Communication
    Bytes}} > \frac{\text{Accelerator FLOPs/s}}{\text{Bandwidth Bytes/s}} & \\[0.5em]
    \Leftrightarrow \text{Intensity}(\text{Computation}) > \text{Intensity}(\text{Accelerator})
    & \\ \end{align*}\]
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} T_\text{math} > T_\text{comms} \Leftrightarrow \frac{\text{Computation
    FLOPs}} {\text{Accelerator FLOPs/s}} > \frac{\text{Communication Bytes}}{\text{Bandwidth
    Bytes/s}} & \\[0.5em] \Leftrightarrow \frac{\text{Computation FLOPs}}{\text{Communication
    Bytes}} > \frac{\text{Accelerator FLOPs/s}}{\text{Bandwidth Bytes/s}} & \\[0.5em]
    \Leftrightarrow \text{Intensity}(\text{Computation}) > \text{Intensity}(\text{Accelerator})
    & \\ \end{align*}\]
- en: 'The quantity $\text{Intensity}(\text{Accelerator})$ is the arithmetic intensity
    at which our accelerator achieves its peak FLOPs/s. **For the TPU v5e MXU, this
    is about 240 FLOPs/byte**, since the TPU can perform `1.97e14` FLOPs/s and load
    `8.2e11` bytes/s from HBM.<d-footnote>The MXU is the matrix multiply unit on the
    TPU. We specify this here because the TPU has other accelerators like the VPU
    that are responsible for elementwise operations that have a different peak FLOPs/s.</d-footnote>
    That means if an algorithm has a lower arithmetic intensity than 240 FLOPs/byte,
    it will be bound by byte loading and thus we won’t make good use of our hardware.<d-footnote>This
    is only true if the algorithm loads its weights from HBM and runs in the MXU.
    As we''ll discuss in the next section, we can sometimes store parameters in VMEM
    which has a much higher bandwidth. Many algorithms also run in the VPU, which
    has different performance characteristics.</d-footnote> Let’s look at one such
    example:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 量$\text{Intensity}(\text{Accelerator})$是我们加速器达到其峰值FLOPs/s的算术强度。**对于TPU v5e MXU，这大约是240
    FLOPs/byte**，因为TPU可以每秒执行`1.97e14` FLOPs，并从HBM以每秒`8.2e11` bytes的速度加载。<d-footnote>MXU是TPU上的矩阵乘法单元。我们在这里指定这一点，因为TPU还有其他加速器，如VPU，它们负责具有不同峰值FLOPs/s的逐元素操作。</d-footnote>这意味着如果一个算法的算术强度低于240
    FLOPs/byte，它将受到字节加载的限制，因此我们无法充分利用我们的硬件。<d-footnote>这仅适用于算法从HBM加载权重并在MXU上运行的情况。正如我们将在下一节讨论的，我们有时可以将参数存储在具有更高带宽的VMEM中。许多算法也在VPU上运行，它具有不同的性能特征。</d-footnote>让我们看看这样一个例子：
- en: '**Example (dot product):** to compute the dot product of two vectors in bfloat16
    precision, `x • y: bf16[N], bf16[N] → bf16[1]`, we need to load $x$ and $y$ from
    memory, each of which has $2 * N = 2N$ bytes, perform $N$ multiplications and
    $N-1$ additions, and write $2$ bytes back into HBM \(\begin{equation} \text{Intensity}(\text{dot
    product}) = \frac{\text{Total FLOPs}}{\text{Total Bytes}} = \frac{N + N - 1}{2N
    + 2N + 2} = \frac{2N - 1}{4N + 2} \rightarrow \frac{1}{2} \end{equation}\)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例（点积）**：为了以bfloat16精度计算两个向量的点积，`x • y: bf16[N], bf16[N] → bf16[1]`，我们需要从内存中加载$x$和$y$，每个都有$2
    * N = 2N$字节，执行$N$次乘法和$N-1$次加法，并将$2$字节写回HBM \(\begin{equation} \text{Intensity}(\text{dot
    product}) = \frac{\text{Total FLOPs}}{\text{Total Bytes}} = \frac{N + N - 1}{2N
    + 2N + 2} = \frac{2N - 1}{4N + 2} \rightarrow \frac{1}{2} \end{equation}\)'
- en: as $N\rightarrow\infty$. So the dot product has an arithmetic intensity of $\frac{1}{2}$
    or, put another way, the dot product does 0.5 floating point operations per byte
    loaded. This means our arithmetic intensity is lower than that of our hardware
    and we will be communication-bound.<d-footnote>The 240 number above is not the
    correct comparison here since, as you will see in the next section, a dot-product
    is performed on the VPU and not the MXU. The TPU v5p VPU can do roughly 7e12 FLOPs
    / second, so its critical intensity is around 3, which means we are still somewhat
    comms-bound here. Either way, the fact that our intensity is low and constant
    means it is difficult to be compute-bound on most hardware.</d-footnote>
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当$N\rightarrow\infty$时。因此，点积的算术强度为$\frac{1}{2}$，或者换句话说，点积每加载一个字节执行0.5次浮点运算。这意味着我们的算术强度低于硬件的算术强度，我们将受到通信限制。<d-footnote>上面的240数字在这里不是正确的比较，因为正如你将在下一节看到的，点积是在VPU上而不是在MXU上执行的。TPU
    v5p VPU每秒可以执行大约7e12 FLOPs，因此其关键强度约为3，这意味着我们在这里仍然有一定的通信限制。无论如何，我们的强度低且恒定的事实意味着在大多数硬件上很难成为计算限制。</d-footnote>
- en: Visualizing rooflines
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可视化屋顶线
- en: 'We can visualize the tradeoff between memory and compute using a **roofline
    plot**, which plots the peak achievable FLOPs/s (throughput) of an algorithm on
    our hardware (the y-axis) against the arithmetic intensity of that algorithm (the
    x-axis). Here’s an example log-log plot:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用**屋顶线图**来可视化内存和计算之间的权衡，该图显示了算法在硬件上（y轴）能达到的最大FLOPs/s（吞吐量）与该算法的算术强度（x轴）之间的关系。以下是一个示例对数-对数图：
- en: <picture>![](../Images/58df5db16fda7288d7bb98d4be55f64a.png)</picture>
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: <picture>![](../Images/58df5db16fda7288d7bb98d4be55f64a.png)</picture>
- en: '**Figure:** an example roofline plot showing two algorithms with different
    arithmetic intensities (Algo 1 and Algo 2) and their corresponding theoretical
    peak throughput under different bandwidths (BW1 and BW2). In the red area, an
    algorithm is bandwidth bound at both bandwidths and is wasting some fraction of
    the hardware''s peak FLOPs/s. The yellow area is bandwidth-bound only at the lower
    bandwidth (BW1). The green area is compute-bound at all bandwidths. Here, we are
    using the peak FLOPs/s of the accelerator and increasing bandwidth or improving
    intensity yield no benefit.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**图示：**一个示例屋顶线图，显示了具有不同算术强度（Algo 1和Algo 2）的两种算法及其在不同带宽（BW1和BW2）下的对应理论峰值吞吐量。在红色区域，算法在两种带宽下都受带宽限制，并且浪费了硬件峰值FLOPs/s的一部分。黄色区域仅在较低带宽（BW1）下受带宽限制。绿色区域在所有带宽下都受计算限制。在这里，我们使用加速器的峰值FLOPs/s，增加带宽或提高强度不会带来任何好处。'
- en: Above, as the intensity increases (moving left to right), we initially see a
    linear increase in the performance of our algorithm (in FLOPs/s) until we hit
    the critical arithmetic intensity of the hardware, 240 in the case of the TPU
    v5e. Any algorithm with a lower intensity will be bandwidth (BW) bound and limited
    by the peak memory bandwidth (shown in red). Any algorithm to the right will fully
    utilize our FLOPs (shown in green). Here, Algo 1 is comms-bound and uses only
    a fraction of the total hardware FLOPs/s. Algo 2 is compute-bound. We can generally
    improve the performance of an algorithm either by increasing its arithmetic intensity
    or by increasing the memory bandwidth available (moving from BW1 to BW2).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在上方，随着强度的增加（从左到右），我们的算法性能（以FLOPs/s计）最初呈线性增长，直到我们达到硬件的关键算术强度，TPU v5e的情况为240。任何强度较低的算法都将受带宽（BW）限制，并受限于峰值内存带宽（以红色显示）。任何位于右侧的算法将充分利用我们的FLOPs（以绿色显示）。在这里，Algo
    1受通信限制，只使用了总硬件FLOPs/s的一部分。Algo 2受计算限制。我们可以通过增加算法的算术强度或增加可用的内存带宽（从BW1移动到BW2）来一般地提高算法的性能。
- en: Matrix multiplication
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 矩阵乘法
- en: 'Let’s look at our soon-to-be favorite algorithm: matrix multiplication (aka
    matmul). We write $X * Y \rightarrow Z$ where $X$ has shape $\text{bf16}[B, D]$,
    $Y$ has shape $\text{bf16}[D, F]$, and $Z$ has shape $\text{bf16}[B, F]$. To do
    the matmul we need to load $2DF + 2BD$ bytes, perform $2BDF$ FLOPs, and write
    $2BF$ bytes back.<d-footnote>Technically we perform $BF \times (2D - 1)$ FLOPs
    but this is close enough. This comes from $BDF$ multiplications and $BF * (D-1)$
    additions. Section 4 has more details.</d-footnote> <d-footnote>Although the output
    of a matmul is technically float32 we usually cast down to bfloat16 before copying
    back to HBM.</d-footnote> Thus:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们即将喜欢的算法：矩阵乘法（也称为matmul）。我们写成$X * Y \rightarrow Z$，其中$X$的形状为$\text{bf16}[B,
    D]$，$Y$的形状为$\text{bf16}[D, F]$，$Z$的形状为$\text{bf16}[B, F]$。要进行matmul，我们需要加载$2DF
    + 2BD$字节，执行$2BDF$次浮点运算（FLOPs），并将$2BF$字节写回。<d-footnote>技术上我们执行$BF \times (2D -
    1)$次FLOPs，但这已经足够接近了。这来自于$BDF$次乘法和$BF * (D-1)$次加法。第4节有更多细节。</d-footnote> <d-footnote>尽管矩阵乘法的输出在技术上为float32，我们通常在将其复制回HBM之前将其转换为bfloat16。</d-footnote>
    因此：
- en: \[\begin{equation} \text{Intensity}(\text{matmul}) = \frac{2BDF}{2BD + 2DF +
    2BF} = \frac{BDF}{BD + DF + BF} \end{equation}\]
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation} \text{Intensity}(\text{matmul}) = \frac{2BDF}{2BD + 2DF +
    2BF} = \frac{BDF}{BD + DF + BF} \end{equation}\]
- en: We can get a nice simplification if we assume our “batch size” $B$ is small
    relative to $D$ and $F$. Then we get
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们假设我们的“批量大小”$B$相对于$D$和$F$较小，那么我们可以得到一个很好的简化。然后我们得到
- en: \[\begin{equation} \frac{BDF}{BD + DF + BF} \approxeq \frac{BDF}{DF} = B \end{equation}\]
    \[\begin{equation} \text{Intensity}(\text{matmul}) > \text{Intensity}(\text{TPU})
    \implies B > \frac{1.97e14}{8.20e11} = 240 \end{equation}\]
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation} \frac{BDF}{BD + DF + BF} \approxeq \frac{BDF}{DF} = B \end{equation}\]
    \[\begin{equation} \text{Intensity}(\text{matmul}) > \text{Intensity}(\text{TPU})
    \implies B > \frac{1.97e14}{8.20e11} = 240 \end{equation}\]
- en: This is a reasonable assumption for Transformer matmuls since we typically have
    a local (per-replica) batch size $B < 1024$ tokens (*not sequences*) but $D$ and
    $F > 8000$. Thus we generally become compute-bound when our per-replica<d-footnote>We
    say per-replica because, if we do some kind of model sharding to increase the
    number of chips used in the matmul, we scale both our available compute and memory
    bandwidth by the same amount. Thus the critical batch size is true per independent
    copy of the model weights.</d-footnote> batch size is greater than 240 tokens,
    a very simple rule!
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Transformer 矩阵乘法来说，这是一个合理的假设，因为我们通常有一个局部（每个副本）的批量大小 $B < 1024$ 个标记（*不是序列*），但
    $D$ 和 $F$ 大于 8000。因此，当我们的每个副本的批量大小超过 240 个标记时，我们通常会成为计算瓶颈！<d-footnote>我们说每个副本，因为如果我们进行某种模型分片以增加用于矩阵乘法的芯片数量，我们将以相同的数量扩展我们的可用计算和内存带宽。因此，关键的批量大小是针对模型权重的独立副本而言的。</d-footnote>
- en: '**Takeaway:** for a bfloat16 matmul to be compute-bound on most TPUs, we need
    our per-replica token batch size to be greater than 240.<d-footnote>Note that
    this is _not_ the batch size in the usual sense, where it means the batch size
    in sequences. It turns out most rooflines depend purely on the number of tokens,
    whether they belong to the same or different sequences. For instance if you have
    a batch size of 512 sequences of 4096 tokens on 128 GPUs, you have a total batch
    size of `512 * 4096 = 2M` tokens, and a local batch size of 16k tokens.</d-footnote>'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**要点总结：**对于一个bfloat16矩阵乘法要在大多数TPU上成为计算瓶颈，我们需要我们的每个副本标记批量大小超过240。<d-footnote>请注意，这*不是*通常意义上的批量大小，其中它意味着序列中的批量大小。事实证明，大多数屋顶线纯粹取决于标记的数量，无论它们是否属于同一序列或不同序列。例如，如果你有512个序列，每个序列有4096个标记，分布在128个GPU上，你将有总批量大小
    `512 * 4096 = 2M` 个标记，局部批量大小为16k个标记。</d-footnote>'
- en: This comes with a few notable caveats we’ll explore in the problems below, particularly
    with respect to quantization (e.g., if we quantize our activations but still do
    full-precision FLOPs), but it’s a good rule to remember. For GPUs, this number
    is slightly higher (closer to 300), but the same conclusion generally holds. When
    we [decompose a big matmul into smaller matmuls](https://docs.jax.dev/en/latest/pallas/tpu/matmul.html#your-first-matrix-multiplication-kernel),
    the tile sizes also matter.<d-footnote>When we do a large matrix multiplication,
    we need to break it down into smaller tiles which fit into VMEM/SMEM/TMEM, the
    higher-bandwidth on-chip memory. This causes us to load chunks multiple times,
    so it's no longer quite true that we only load $O(N^2)$ bytes. Consider an $(m,
    k) \cdot (k, n)$ matmul with tile sizes $bm$, $bk$, $bm$. Let $tm = m / bm$, etc.
    Then the total FLOPs is $2 \cdot tm \cdot tn \cdot tk \cdot bm \cdot bn \cdot
    bk$ and the total bytes are $2 \cdot tm \cdot tn \cdot (tk \cdot (bm \cdot bk
    + bk \cdot bn) + 2 \cdot bm \cdot bn)$. Ignoring the last term, we have an intensity
    of $bm \cdot bn / (bm + bn)$, which is similar to the above.</d-footnote> We’ll
    discuss the lower-level GPU and TPU details in the [next section](../tpus).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这在下文中的一些注意事项中会有所体现，特别是关于量化（例如，如果我们量化我们的激活但仍然进行全精度FLOPs），但这是一个值得记住的好规则。对于GPU来说，这个数字略高（接近300），但通常得出相同的结论。当我们[将大矩阵乘法分解成较小的矩阵乘法](https://docs.jax.dev/en/latest/pallas/tpu/matmul.html#your-first-matrix-multiplication-kernel)时，瓦片大小也很重要。<d-footnote>当我们进行大矩阵乘法时，我们需要将其分解成适合VMEM/SMEM/TMEM（片上高带宽内存）的小瓦片。这导致我们多次加载块，因此我们不再只加载
    $O(N^2)$ 字节。考虑一个 $(m, k) \cdot (k, n)$ 矩阵乘法，瓦片大小为 $bm$，$bk$，$bm$。设 $tm = m / bm$，等等。总的FLOPs是
    $2 \cdot tm \cdot tn \cdot tk \cdot bm \cdot bn \cdot bk$，总的字节数是 $2 \cdot tm \cdot
    tn \cdot (tk \cdot (bm \cdot bk + bk \cdot bn) + 2 \cdot bm \cdot bn)$。忽略最后一个项，我们有强度
    $bm \cdot bn / (bm + bn)$，这与上面类似。</d-footnote>我们将在下一节中讨论更底层的GPU和TPU细节。[下一节](../tpus)。
- en: Network communication rooflines
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网络通信屋顶线
- en: 'All the rooflines we’ve discussed so far have been memory-bandwidth rooflines,
    *all within a single chip*. This shouldn’t be taken as a rule. In fact, most of
    the rooflines we’ll care about in this book involve communication between chips:
    usually matrix multiplications that involve matrices sharded across multiple TPUs.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前讨论的所有屋顶线都是内存带宽屋顶线，*都在单个芯片内*。这不应被视为规则。事实上，本书中我们将关注的许多屋顶线都涉及芯片之间的通信：通常是涉及跨多个TPU分片矩阵的矩阵乘法。
- en: To pick a somewhat contrived example, say we want to multiply two big matrices
    $X\sim \text{bfloat16[B, D]}$ and $Y \sim \text{bfloat16[D, F]}$ which are split
    evenly across 2 TPUs/GPUs (along the $D$ dimension). To do this multiplication
    (as we’ll see in [Section 3](../sharding)), we can multiply half of each matrix
    on each TPU (`A = X[:, :D // 2] @ Y[:D // 2, :]` on TPU 0 and `B = X[:, D // 2:]
    @ Y[D // 2:, :]` on TPU 1) and then copy the resulting “partial sums” to the other
    TPU and add them together. Say we can copy `4.5e10` bytes in each direction and
    perform `1.97e14` FLOPs/s on each chip. What are $T_\text{math}$ and $T_\text{comms}$?
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了举一个稍微有些牵强的例子，假设我们想乘以两个大矩阵\(X\sim \text{bfloat16[B, D]}\)和\(Y \sim \text{bfloat16[D,
    F]}\)，它们在2个TPU/GPU上均匀分割（沿D维度）。为了进行这个乘法（正如我们将在[第3节](../sharding)中看到的那样），我们可以在每个TPU上乘以每个矩阵的一半（`A
    = X[:, :D // 2] @ Y[:D // 2, :]`在TPU 0上，`B = X[:, D // 2:] @ Y[D // 2:, :]`在TPU
    1上）然后将结果“部分和”复制到另一个TPU并相加。假设我们可以在每个方向上复制`4.5e10`字节，并在每个芯片上以`1.97e14` FLOPs/s的速度执行操作。\(T_\text{math}\)和\(T_\text{comms}\)是多少？
- en: $T_\text{math}$ is clearly half of what it was before, since each TPU is doing
    half the work, i.e.<d-footnote>We're ignoring the FLOPs required to add the two
    partial sums together (another BF additions), but this is basically negligible.</d-footnote>
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: \(T_\text{math}\)显然是之前的一半，因为每个TPU只做一半的工作，即<d-footnote>我们忽略了将两个部分和相加所需的FLOPs（另一个BF加法），但这基本上可以忽略不计。</d-footnote>
- en: \[T_\text{math} = \frac{2BDF}{2 \cdot \text{Accelerator FLOPs/s}} = \frac{BDF}{1.97e14}\]
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: \[T_\text{math} = \frac{2BDF}{2 \cdot \text{加速器FLOPs/s}} = \frac{BDF}{1.97e14}\]
- en: Now what about $T_\text{comms}$? This now refers to the communication time between
    chips! This is just the total bytes sent divided by the network bandwidth, i.e.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，\(T_\text{comms}\)又是怎么回事呢？现在它指的是芯片之间的通信时间！这仅仅是发送的总字节数除以网络带宽，即
- en: \[T_\text{comms} = \frac{2BF}{\text{Network Bandwidth}} = \frac{2BF}{4.5e10}\]
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: \[T_\text{comms} = \frac{2BF}{\text{网络带宽}} = \frac{2BF}{4.5e10}\]
- en: Therefore we become compute-bound (now with respect to the inter-chip network)
    when \(\text{Intensity}(\text{matmul (2-chips)}) > \text{Intensity}(\text{TPU
    w.r.t. inter-chip network})\) or equivalently when $\frac{BDF}{2BF} = \frac{D}{2}
    > \frac{1.97e14}{4.5e10} = 4377$ or $D > 8755$. Note that, unlike before, the
    critical threshhold now depends on $D$ and not $B$! Try to think why that is.
    This is just one such example, but we highlight that this kind of roofline is
    critical to knowing when we can parallelize an operation across multiple TPUs.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当\(\text{Intensity}(\text{matmul (2-chips)}) > \text{Intensity}(\text{TPU
    w.r.t. inter-chip network})\)或等价地，当\(\frac{BDF}{2BF} = \frac{D}{2} > \frac{1.97e14}{4.5e10}
    = 4377\)或\(D > 8755\)时，我们变得计算受限（现在与芯片间网络相关）。注意，与之前不同，关键阈值现在取决于\(D\)而不是\(B\)！试着想想为什么是这样。这只是这样一个例子，但我们强调这种roofline对于知道何时可以在多个TPU上并行化操作至关重要。
- en: A Few Problems to Work
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一些需要解决的问题
- en: '**Question 1 [int8 matmul]:** Say we want to do the matmul $X[B, D] \cdot_D
    Y[D, F] \rightarrow Z[B, F]$ in int8 precision (1 byte per parameter) instead
    of bfloat16.<d-footnote>Here and throughout we''ll use the notation $A \cdot_D
    B$ to indicate that the multiplication is performing a contraction over the D
    dimension. This is an abuse of einsum notation.</d-footnote>'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题1 [int8矩阵乘法]**：假设我们想在int8精度（每个参数1字节）下执行矩阵乘法\(X[B, D] \cdot_D Y[D, F] \rightarrow
    Z[B, F]\)，而不是bfloat16。<d-footnote>在此以及整个过程中，我们将使用\(A \cdot_D B\)的记法来表示乘法在D维度上执行收缩。这是对einsum记法的滥用。</d-footnote>'
- en: How many bytes need to be loaded from memory? How many need to be written back
    to memory?
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 需要从内存中加载多少字节？需要写回多少字节到内存？
- en: How many total OPs are performed?
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行了多少总OP操作？
- en: What is the arithmetic intensity?
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是算术强度？
- en: What is a roofline estimate for $T_\text{math}$ and $T_\text{comms}$? What are
    reasonable upper and lower bounds for the runtime of the whole operation?
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于\(T_\text{math}\)和\(T_\text{comms}\)的roofline估计是什么？整个操作的运行时间有什么合理的上下限？
- en: Assume our HBM bandwidth is `8.1e11` bytes/s and our int8 peak OPs/s is `3.94e14`
    (about 2x bfloat16).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的HBM带宽为`8.1e11`字节/秒，我们的int8峰值OPs/s为`3.94e14`（大约是bfloat16的两倍）。
- en: <details><summary>Click here for the answer.</summary>
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>点击此处查看答案。</summary>
- en: Because we’re storing our parameters in int8, we have 1 byte per parameter,
    so we have \(BD + DF\) bytes loaded from HBM and \(BF\) written back.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因为我们将参数存储在int8格式中，每个参数占用1个字节，所以从HBM加载了\(BD + DF\)个字节，并写回了\(BF\)个字节。
- en: This is the same as in bfloat16, but in theory int8 OPs/s should be faster.
    So this is still $2BDF$ FLOPs.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这与bfloat16相同，但在理论上int8 OPs/s应该更快。所以这仍然是\(2BDF\) FLOPs。
- en: Arithmetic intensity is \(2BDF / (BD + DF + BF)\). If we make the same assumption
    as above about \(B \ll D\) and \(B \ll F\), we get an arithmetic intensity of
    \(2B\), meaning our rule becomes $B > \text{HBM int8 arithmetic intensity} / 2$.
    Using the numbers given, this int8 intensity is `3.94e14 / 8.1e11 = 486`, so the
    rule is $B > 486 / 2 = 243$. Note that this is basically unchanged!
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 算术强度是 \(2BDF / (BD + DF + BF)\)。如果我们对\(B \ll D\)和\(B \ll F\)做出相同的假设，我们得到算术强度为\(2B\)，这意味着我们的规则变为$B
    > \text{HBM int8 arithmetic intensity} / 2$。使用给定的数字，这个int8强度是`3.94e14 / 8.1e11
    = 486`，所以规则是$B > 486 / 2 = 243$。请注意，这基本上没有变化！
- en: \(T_\text{math} = 2BDF / 3.94e14\) and \(T_\text{comms} = (BD + DF + BF) / 8.1e11\),
    so a reasonable lower bound is \(\max(T_\text{math}, T_\text{comms})\) and an
    upper bound is \(T_\text{math} + T_\text{comms}\).</details>
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: \(T_\text{math} = 2BDF / 3.94e14\) 和 \(T_\text{comms} = (BD + DF + BF) / 8.1e11\)，因此一个合理的下限是
    \(\max(T_\text{math}, T_\text{comms})\)，一个上限是 \(T_\text{math} + T_\text{comms}\)。</details>
- en: '**Question 2 [int8 + bf16 matmul]:** In practice we often do different weight
    vs. activation quantization, so we might store our weights in very low precision
    but keep activations (and compute) in a higher precision. Say we want to quantize
    our weights in int8 but keep activations (and compute) in bfloat16\. At what batch
    size do we become compute bound? Assume `1.97e14` bfloat16 FLOPs/s.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题2 [int8 + bf16 矩阵乘法]**：在实际操作中，我们经常对权重和激活进行不同的量化，因此我们可能会以非常低的精度存储权重，但保持激活（和计算）以更高的精度。假设我们想以int8量化权重，但保持激活（和计算）以bfloat16精度。在什么批量大小下我们成为计算限制？假设bfloat16
    FLOPs/s为`1.97e14`。'
- en: '*Hint: this means specifically `bfloat16[B, D] * int8[D, F] -> bfloat16[B,
    F]` where $B$ is the “batch size”.*'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*提示：这特别意味着 `bfloat16[B, D] * int8[D, F] -> bfloat16[B, F]` 其中 $B$ 是“批量大小”.*'
- en: <details><summary>Click here for the answer.</summary>
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>点击此处查看答案。</summary>
- en: Again assuming B is small, we have 2BDF bfloat16 FLOPs but only DF weights (instead
    of 2DF in bfloat16). This means we become compute-bound when \(2B > 240\) or \(B
    > 120\). This is a lot lower, meaning if we can do int8 weight quantization (which
    is fairly easy to do) but still do bfloat16 FLOPs, we get a meaningful win in
    efficiency (although int8 OPs would be better).</details>
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 再次假设B很小，我们有2BDF bfloat16 FLOPs，但只有DF权重（而不是bfloat16中的2DF）。这意味着当\(2B > 240\)或\(B
    > 120\)时，我们成为计算限制。这要低得多，这意味着如果我们能够进行int8权重量化（这相对容易做到），但仍然进行bfloat16 FLOPs，我们将在效率上获得有意义的提升（尽管int8操作会更好）。</details>
- en: '**Question 3:** Taking the setup from Question 2, make a roofline plot of peak
    FLOPs/s vs. $B$ for $F = D = 4096$ and $F = D = 1024$. *Use the exact number of
    bytes loaded, not an approximation.*'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题3**：从问题2的设置出发，为\(F = D = 4096\)和\(F = D = 1024\)绘制峰值FLOPs/s与$B$的关系图。*使用确切的字节数加载，而不是近似值。*'
- en: <details><summary>Click here for the answer.</summary>
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>点击此处查看答案。</summary>
- en: 'Here is the plot in question:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是相关的问题图：
- en: <picture>![](../Images/183a7a5b6f767193725b26b73105376f.png)</picture>
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: <picture>![](../Images/183a7a5b6f767193725b26b73105376f.png)</picture>
- en: 'Note that both models eventually acheive the peak hardware FLOPs/s, but the
    larger D/F achieve it sooner. D=F=1024 almost doubles the critical batch size.
    The code to generate this figure is here:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这两个模型最终都达到了峰值硬件FLOPs/s，但更大的D/F模型更早实现。D=F=1024几乎将关键批量大小翻倍。生成此图的代码在此：
- en: '[PRE0]</details>'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE0]</details>'
- en: '**Question 4:** What if we wanted to perform $\text{int8[B, D]} *_D \text{int8[B,
    D, F]} \rightarrow \text{int8[B, F]}$ where we imagine having a different matrix
    for each batch element. What is the arithmetic intensity of this operation?'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题4**：如果我们想执行$\text{int8[B, D]} *_D \text{int8[B, D, F]} \rightarrow \text{int8[B,
    F]}$，其中我们想象为每个批量元素有一个不同的矩阵。这个操作的算术强度是多少？'
- en: <details><summary>Click here for the answer.</summary>
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>点击此处查看答案。</summary></details>
- en: Let’s start by looking at the total FLOPs and comms.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看总的FLOPs和通信量。
- en: 'Total FLOPs: the FLOPs is basically the same, since we’re doing the same number
    of \(BD \times DF\) matmuls (this is discussed more in section 4). So this is
    just \(2BDF\).'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 总FLOPs：FLOPs基本上是相同的，因为我们执行相同数量的\(BD \times DF\)矩阵乘法（这将在第4节中进一步讨论）。所以这仅仅是\(2BDF\)。
- en: 'Total comms: we have a lot more comms here: \(BD + BDF + BF\).'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 总通信量：这里有很多通信：\(BD + BDF + BF\)。
- en: Therefore, our arithmetic intensity is now actually \(2BDF / (BD + BDF + BF)\).
    Since \(BDF\) dominates the denominator, this is roughly \(2\). So instead of
    it depending on the batch size, this is essentially constant. This is bad because
    it means we’ll basically always be comms bound no matter what.</details>
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，我们的算术强度现在是实际上 \(2BDF / (BD + BDF + BF)\)。由于 \(BDF\) 在分母中占主导地位，这大约是 \(2\)。所以它不再依赖于批量大小，这实际上是常数。这是不好的，因为它意味着我们基本上总是通信限制，无论什么情况。</details>
- en: '**Problem 5 [Memory Rooflines for GPUs]:** Using the [spec sheet provided by
    NVIDIA for the H100](https://www.nvidia.com/en-us/data-center/h100/), calculate
    the batch size at which a matrix multiplication will become compute-bound. *Note
    that the Tensor Core FLOPs numbers are twice the true value since they’re only
    achievable with structured sparsity.*'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 5 [GPU 的内存屋顶线]:** 使用 NVIDIA 为 H100 提供的 [规格表](https://www.nvidia.com/en-us/data-center/h100/)，计算矩阵乘法成为计算限制时的批量大小。*注意，由于只有通过结构化稀疏性才能实现，Tensor
    Core FLOPs 数值是真实值的两倍。*'
- en: <details><summary>Click here for the answer.</summary>
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>点击此处查看答案。</summary>
- en: From the spec sheet, we see that the reported bfloat16 FLOPs value is `1.979e15`
    FLOPs/s with an asterisk noting “with sparsity”. The true value is half this without
    sparsity, meaning close to `1e15` FLOPs/s. The memory bandwidth is 3.35TB/s, or
    `3.35e12` bytes / second. Thus $B_\text{crit}$ is `1e15 / 3.35e12 = 298`, rather
    similar to the TPU.</details>
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 从规格表中，我们看到报告的 bfloat16 FLOPs 值为 `1.979e15` FLOPs/s，旁边有一个星号注明“具有稀疏性”。真实值在没有稀疏性的情况下是这一数值的一半，即接近
    `1e15` FLOPs/s。内存带宽为 3.35TB/s，或 `3.35e12` 字节/秒。因此 $B_\text{crit}$ 是 `1e15 / 3.35e12
    = 298`，与 TPU 非常相似。</details>
- en: That’s it for Part 1! For Part 2, looking at how real TPUs handle FLOPs and
    communication, [click here](../tpus).</d-article>  <d-appendix><d-footnote-list><d-citation-list>###
    Miscellaneous
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第一部分到此结束！对于第二部分，我们将探讨真实 TPUs 如何处理 FLOPs 和通信，[点击此处](../tpus)。</d-article>  <d-appendix><d-footnote-list><d-citation-list>###
    杂项
- en: ^*Work done at Google DeepMind, now at MatX.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ^*在 Google DeepMind 完成的工作，现在在 MatX。
- en: Citation
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 引用
- en: 'For attribution in academic contexts, please cite this work as:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在学术环境中进行归属时，请引用此工作如下：
- en: '[PRE1]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'or as a BibTeX entry:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 或者作为一个 BibTeX 条目：
- en: '[PRE2]</d-citation-list></d-footnote-list></d-appendix> <d-bibliography src="/scaling-book/assets/bibliography/main.bib"></d-bibliography></d-byline>'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE2]</d-citation-list></d-footnote-list></d-appendix> <d-bibliography src="/scaling-book/assets/bibliography/main.bib"></d-bibliography></d-byline>'
