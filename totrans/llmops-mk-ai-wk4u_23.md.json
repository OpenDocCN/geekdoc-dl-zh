["```py\n`# Use the warnings library to control warning messages import  warnings  # Ignore all warnings to ensure clean runtime output warnings.filterwarnings('ignore')  # Load API keys for third‑party services used in the project from  utils  import get_circle_ci_api_key, get_github_api_key, get_openai_api_key  # Obtain individual API keys for CircleCI, GitHub, and OpenAI circle_ci_api_key = get_circle_ci_api_key() github_api_key = get_github_api_key() openai_api_key = get_openai_api_key()` \n```", "```py\n`# Define a template for structuring quiz questions quiz_question_template = \"{question}\"  # Initialize a quiz bank with subjects, categories, and facts quiz_bank = \"\"\" Here are three new quiz questions following the given format:  1\\. Subject: A Historical Conflict   Categories: History, Politics   Facts:   - Began in 1914 and ended in 1918   - Involved two major alliances: the Allies and the Central Powers   - Known for extensive trench warfare on the Western Front   2\\. Subject: A Revolutionary Communication Technology   Categories: Technology, History   Facts:   - Invented by Alexander Graham Bell in 1876   - Revolutionized long‑distance communication   - The first words transmitted were \"Mr. Watson, come here, I want to see you\"   3\\. Subject: An Iconic American Landmark   Categories: Geography, History   Facts:   - Gifted to the United States by France in 1886   - Symbolizes freedom and democracy   - Located on Liberty Island in New York Harbor  \"\"\"` \n```", "```py\n`# Define a delimiter to separate different parts of the quiz prompt section_delimiter = \"####\"  # Create a detailed prompt template guiding the AI to generate user‑customized quizzes quiz_generation_prompt_template = f\"\"\" Instructions for generating a customized quiz: Each question is separated by four hashes, i.e. {section_delimiter}  The user chooses a category for the quiz. Ensure the questions are relevant to the chosen category.  Step 1:{section_delimiter} Identify the user‑selected category from the list below: * History * Technology * Geography * Politics  Step 2:{section_delimiter} Choose up to two subjects that match the selected category from the quiz bank:  {quiz_bank}  Step 3:{section_delimiter} Create a quiz based on the selected subjects by formulating three questions per subject.  Quiz format: Question 1:{section_delimiter} <Insert Question 1> Question 2:{section_delimiter} <Insert Question 2> Question 3:{section_delimiter} <Insert Question 3> \"\"\"` \n```", "```py\n`# Import required components from LangChain for prompt structuring and LLM interaction from  langchain.prompts  import ChatPromptTemplate from  langchain_openai  import ChatOpenAI from  langchain.schema.output_parser  import StrOutputParser  # Convert the detailed quiz generation prompt into a structured format for the LLM structured_chat_prompt = ChatPromptTemplate.from_messages([(\"user\", quiz_generation_prompt_template)])  # Select the language model for quiz question generation language_model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)  # Configure an output parser to convert the LLM response into a readable format response_parser = StrOutputParser()` \n```", "```py\n`# Compose the structured prompt, language model, and output parser into a quiz generation pipeline quiz_generation_pipeline = structured_chat_prompt | language_model | response_parser  # Execute the pipeline to generate a quiz (example invocation not shown)` \n```", "```py\n`from  langchain.prompts  import ChatPromptTemplate from  langchain_openai  import ChatOpenAI from  langchain.schema.output_parser  import StrOutputParser  def  generate_quiz_assistant_pipeline(     system_prompt_message,     user_question_template=\"{question}\",     selected_language_model=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0),     response_format_parser=StrOutputParser()):   \"\"\"  Assembles the components required to generate quizzes through an AI‑based process.   Parameters:  - system_prompt_message: A message containing instructions or context for quiz generation.  - user_question_template: A template for structuring user questions; defaults to a simple placeholder.  - selected_language_model: The AI model used to generate content; a default model is provided.  - response_format_parser: A mechanism for parsing the LLM response into the desired format.   Returns:  A LangChain pipeline that, when invoked, generates a quiz based on the provided system message and user template.  \"\"\"      # Create a structured chat prompt from the system and user messages     structured_chat_prompt = ChatPromptTemplate.from_messages([         (\"system\", system_prompt_message),         (\"user\", user_question_template),     ])      # Compose the chat prompt, language model, and output parser into a single pipeline     quiz_generation_pipeline = structured_chat_prompt | selected_language_model | response_format_parser      return quiz_generation_pipeline` \n```", "```py\n`def  evaluate_quiz_content(     system_prompt_message,     quiz_request_question,     expected_keywords,     user_question_template=\"{question}\",     selected_language_model=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0),     response_format_parser=StrOutputParser()):   \"\"\"  Evaluates the generated quiz content to ensure it includes expected keywords or phrases.   Parameters:  - system_prompt_message: Instructions or context for quiz generation.  - quiz_request_question: The specific question or request that triggers quiz generation.  - expected_keywords: A list of words or phrases that must be present in the quiz content.  - user_question_template: A template for structuring user questions; defaults to a simple placeholder.  - selected_language_model: The AI model used to generate content; a default model is provided.  - response_format_parser: A mechanism for parsing the LLM response into the desired format.   Raises:  - AssertionError: If none of the expected keywords are found in the generated quiz content.  \"\"\"      # Use the helper to generate quiz content based on the provided request     generated_content = generate_quiz_assistant_pipeline(         system_prompt_message,         user_question_template,         selected_language_model,         response_format_parser).invoke({\"question\": quiz_request_question})      print(generated_content)      # Verify that the generated content includes at least one of the expected keywords     assert any(keyword.lower() in generated_content.lower() for keyword in expected_keywords), \\         f\"Expected the generated quiz to contain one of '{expected_keywords}', but none were found.\"` \n```", "```py\n`# Define the system message (or prompt template), the specific request, and the expected keywords system_prompt_message = quiz_generation_prompt_template  # Assumes this variable was defined earlier in your code quiz_request_question = \"Generate a quiz about science.\" expected_keywords = [\"renaissance innovator\", \"astronomical observation tools\", \"natural sciences\"]  # Call the evaluation function with the test parameters evaluate_quiz_content(     system_prompt_message,     quiz_request_question,     expected_keywords )` \n```", "```py\n``def  evaluate_request_refusal(     system_prompt_message,     invalid_quiz_request_question,     expected_refusal_response,     user_question_template=\"{question}\",     selected_language_model=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0),     response_format_parser=StrOutputParser()):   \"\"\"  Evaluates the system’s response to ensure it correctly refuses invalid or out‑of‑scope requests.   Parameters:  - system_prompt_message: Instructions or context for quiz generation.  - invalid_quiz_request_question: A request that the system should decline.  - expected_refusal_response: The expected text indicating the system’s refusal to fulfill the request.  - user_question_template: A template for structuring user questions; defaults to a simple placeholder.  - selected_language_model: The AI model used to generate content; a default model is provided.  - response_format_parser: A mechanism for parsing the LLM response into the desired format.   Raises:  - AssertionError: If the system’s response does not contain the expected refusal message.  \"\"\"      # Align parameter order with what `generate_quiz_assistant_pipeline` expects     generated_response = generate_quiz_assistant_pipeline(         system_prompt_message,         user_question_template,         selected_language_model,         response_format_parser).invoke({\"question\": invalid_quiz_request_question})      print(generated_response)      # Check that the system’s response contains the expected refusal phrase     assert expected_refusal_response.lower() in generated_response.lower(), \\         f\"Expected a refusal message '{expected_refusal_response}', but got: {generated_response}\"`` \n```", "```py\n`# Define the system message (or prompt template), an out‑of‑scope request, and the expected refusal message system_prompt_message = quiz_generation_prompt_template  # Assumes this variable was defined earlier in your code invalid_quiz_request_question = \"Generate a quiz about Rome.\" expected_refusal_response = \"I'm sorry, but I can't generate a quiz about Rome at this time.\"  # Run the refusal evaluation with the specified parameters evaluate_request_refusal(     system_prompt_message,     invalid_quiz_request_question,     expected_refusal_response )` \n```", "```py\n`def  test_science_quiz():   \"\"\"  Tests the quiz generator’s ability to create science‑related questions by checking for expected subjects.  \"\"\"     # Define the request to generate a quiz question     question_request = \"Generate a quiz question.\"      # The list of expected keywords or subjects indicating scientific alignment     expected_science_subjects = [\"physics\", \"chemistry\", \"biology\", \"astronomy\"]      # The system message or prompt template configured for quiz generation     system_prompt_message = quiz_generation_prompt_template  # This should be defined earlier in your code      # Invoke the evaluation with science‑specific parameters     evaluate_quiz_content(         system_prompt_message=system_prompt_message,         quiz_request_question=question_request,         expected_keywords=expected_science_subjects     )` \n```", "```py\n`version:  2.1  orbs:   python:  circleci/python@1.2.0  # Use the Python orb to simplify your config  jobs:   build-and-test:   docker:   -  image:  cimg/python:3.8  # Specify the Python version   steps:   -  checkout  # Check out the source code   -  restore_cache:  # Restore cache to save time on dependencies installation   keys:   -  v1-dependencies-{{ checksum \"requirements.txt\" }}   -  v1-dependencies-   -  run:   name:  Install Dependencies   command:  pip install -r requirements.txt   -  save_cache:  # Cache dependencies to speed up future builds   paths:   -  ./venv   key:  v1-dependencies-{{ checksum \"requirements.txt\" }}   -  run:   name:  Run Tests   command:  pytest  # Or any other command to run your tests  workflows:   version:  2   build_and_test:   jobs:   -  build-and-test` \n```"]