["```py\nimport jax\nimport jax.numpy as jnp\n\n# Create our mesh! We're running on a TPU v2-8 4x2 slice with names 'X' and 'Y'. assert len(jax.devices()) == 8\nmesh = jax.make_mesh(axis_shapes=(4, 2), axis_names=('X', 'Y'))\n\n# A little utility function to help define our sharding. A PartitionSpec is our\n# sharding (a mapping from axes to names). def P(*args):\n  return jax.NamedSharding(mesh, jax.sharding.PartitionSpec(*args))\n\n# We shard both A and B over the non-contracting dimension and A over the contracting dim. A = jnp.zeros((8, 2048), dtype=jnp.bfloat16, device=P('X', 'Y'))\nB = jnp.zeros((2048, 8192), dtype=jnp.bfloat16, device=P(None, 'Y'))\n\n# We can perform a matmul on these sharded arrays! out_shardings tells us how we want\n# the output to be sharded. JAX/XLA handles the rest of the sharding for us. y = jax.jit(lambda A, B: jnp.einsum('BD,DF->BF', A, B), out_shardings=P('X', 'Y'))(A, B) \n```", "```py\n Austin et al., \"How to Scale Your Model\", Google DeepMind, online, 2025. \n```", "```py\n @article{scaling-book,\n      title = {How to Scale Your Model},\n      author = {Austin, Jacob and Douglas, Sholto and Frostig, Roy and Levskaya, Anselm and Chen, Charlie and Vikram, Sharad\n      and Lebron, Federico and Choy, Peter and Ramasesh, Vinay and Webson, Albert and Pope, Reiner},\n      publisher = {Google DeepMind},\n      howpublished = {Online},\n      note = {Retrieved from https://jax-ml.github.io/scaling-book/},\n      year = {2025}\n    } \n```"]