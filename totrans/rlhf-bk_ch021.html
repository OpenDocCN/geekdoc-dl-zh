<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <title>ch021.xhtml</title>
  <link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css" />
</head>
<body epub:type="bodymatter">
<section id="bibliography" class="level1 unnumbered">
<h1 class="unnumbered">Bibliography</h1>
<div id="refs" class="references csl-bib-body" data-entry-spacing="0" role="list">
<div id="ref-christiano2017deep" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei, <span>“Deep reinforcement learning from human preferences,”</span> <em>Advances in neural information processing systems</em>, vol. 30, 2017.</div>
</div>
<div id="ref-stiennon2020learning" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">N. Stiennon <em>et al.</em>, <span>“Learning to summarize with human feedback,”</span> <em>Advances in Neural Information Processing Systems</em>, vol. 33, pp. 3008–3021, 2020.</div>
</div>
<div id="ref-ouyang2022training" class="csl-entry" role="listitem">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline"><span class="nocase">L. Ouyang <em>et al.</em></span>, <span>“Training language models to follow instructions with human feedback,”</span> <em>Advances in neural information processing systems</em>, vol. 35, pp. 27730–27744, 2022.</div>
</div>
<div id="ref-nakano2021webgpt" class="csl-entry" role="listitem">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline"><span class="nocase">R. Nakano <em>et al.</em></span>, <span>“Webgpt: Browser-assisted question-answering with human feedback,”</span> <em>arXiv preprint arXiv:2112.09332</em>, 2021.</div>
</div>
<div id="ref-bai2022training" class="csl-entry" role="listitem">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline"><span class="nocase">Y. Bai <em>et al.</em></span>, <span>“Training a helpful and harmless assistant with reinforcement learning from human feedback,”</span> <em>arXiv preprint arXiv:2204.05862</em>, 2022.</div>
</div>
<div id="ref-lambert2024t" class="csl-entry" role="listitem">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline"><span class="nocase">N. Lambert <em>et al.</em></span>, <span>“T<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>∖</mi><annotation encoding="application/x-tex">\backslash</annotation></semantics></math>" ULU 3: Pushing frontiers in open language model post-training,”</span> <em>arXiv preprint arXiv:2411.15124</em>, 2024.</div>
</div>
<div id="ref-kirk2023understanding" class="csl-entry" role="listitem">
<div class="csl-left-margin">[7] </div><div class="csl-right-inline">R. Kirk <em>et al.</em>, <span>“Understanding the effects of rlhf on llm generalisation and diversity,”</span> <em>arXiv preprint arXiv:2310.06452</em>, 2023.</div>
</div>
<div id="ref-chu2025sft" class="csl-entry" role="listitem">
<div class="csl-left-margin">[8] </div><div class="csl-right-inline">T. Chu <em>et al.</em>, <span>“Sft memorizes, rl generalizes: A comparative study of foundation model post-training,”</span> <em>arXiv preprint arXiv:2501.17161</em>, 2025.</div>
</div>
<div id="ref-singhal2023long" class="csl-entry" role="listitem">
<div class="csl-left-margin">[9] </div><div class="csl-right-inline">P. Singhal, T. Goyal, J. Xu, and G. Durrett, <span>“A long way to go: Investigating length correlations in rlhf,”</span> <em>arXiv preprint arXiv:2310.03716</em>, 2023.</div>
</div>
<div id="ref-park2024disentangling" class="csl-entry" role="listitem">
<div class="csl-left-margin">[10] </div><div class="csl-right-inline">R. Park, R. Rafailov, S. Ermon, and C. Finn, <span>“Disentangling length from quality in direct preference optimization,”</span> <em>arXiv preprint arXiv:2403.19159</em>, 2024.</div>
</div>
<div id="ref-muennighoff2024olmoe" class="csl-entry" role="listitem">
<div class="csl-left-margin">[11] </div><div class="csl-right-inline"><span class="nocase">N. Muennighoff <em>et al.</em></span>, <span>“Olmoe: Open mixture-of-experts language models,”</span> <em>arXiv preprint arXiv:2409.02060</em>, 2024.</div>
</div>
<div id="ref-ai2_olmoe_ios_2025" class="csl-entry" role="listitem">
<div class="csl-left-margin">[12] </div><div class="csl-right-inline">Allen Institute for Artificial Intelligence, <span>“OLMoE, meet iOS.”</span> <a href="https://allenai.org/blog/olmoe-app" class="uri">https://allenai.org/blog/olmoe-app</a>, 2025.</div>
</div>
<div id="ref-zhou2023lima" class="csl-entry" role="listitem">
<div class="csl-left-margin">[13] </div><div class="csl-right-inline"><span class="nocase">C. Zhou <em>et al.</em></span>, <span>“Lima: Less is more for alignment,”</span> <em>Advances in Neural Information Processing Systems</em>, vol. 36, pp. 55006–55021, 2023.</div>
</div>
<div id="ref-alpaca" class="csl-entry" role="listitem">
<div class="csl-left-margin">[14] </div><div class="csl-right-inline">R. Taori <em>et al.</em>, <span>“Stanford alpaca: An instruction-following LLaMA model,”</span> <em>GitHub repository</em>. <a href="https://github.com/tatsu-lab/stanford_alpaca" class="uri">https://github.com/tatsu-lab/stanford_alpaca</a>; GitHub, 2023.</div>
</div>
<div id="ref-vicuna2023" class="csl-entry" role="listitem">
<div class="csl-left-margin">[15] </div><div class="csl-right-inline">W.-L. Chiang <em>et al.</em>, <span>“Vicuna: An open-source chatbot impressing GPT-4 with 90%* ChatGPT quality.”</span> 2023. Available: <a href="https://lmsys.org/blog/2023-03-30-vicuna/">https://lmsys.org/blog/2023-03-30-vicuna/</a></div>
</div>
<div id="ref-koala_blogpost_2023" class="csl-entry" role="listitem">
<div class="csl-left-margin">[16] </div><div class="csl-right-inline">X. Geng <em>et al.</em>, <span>“Koala: A dialogue model for academic research.”</span> Blog post, 2023. Accessed: Apr. 03, 2023. [Online]. Available: <a href="https://bair.berkeley.edu/blog/2023/04/03/koala/">https://bair.berkeley.edu/blog/2023/04/03/koala/</a></div>
</div>
<div id="ref-DatabricksBlog2023DollyV1" class="csl-entry" role="listitem">
<div class="csl-left-margin">[17] </div><div class="csl-right-inline">M. Conover <em>et al.</em>, <span>“Hello dolly: Democratizing the magic of ChatGPT with open models.”</span> Accessed: June 30, 2023. [Online]. Available: <a href="https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html">https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html</a></div>
</div>
<div id="ref-askell2021general" class="csl-entry" role="listitem">
<div class="csl-left-margin">[18] </div><div class="csl-right-inline"><span class="nocase">A. Askell <em>et al.</em></span>, <span>“A general language assistant as a laboratory for alignment,”</span> <em>arXiv preprint arXiv:2112.00861</em>, 2021.</div>
</div>
<div id="ref-bai2022constitutional" class="csl-entry" role="listitem">
<div class="csl-left-margin">[19] </div><div class="csl-right-inline"><span class="nocase">Y. Bai <em>et al.</em></span>, <span>“Constitutional ai: Harmlessness from ai feedback,”</span> <em>arXiv preprint arXiv:2212.08073</em>, 2022.</div>
</div>
<div id="ref-rafailov2024direct" class="csl-entry" role="listitem">
<div class="csl-left-margin">[20] </div><div class="csl-right-inline">R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn, <span>“Direct preference optimization: Your language model is secretly a reward model,”</span> <em>Advances in Neural Information Processing Systems</em>, vol. 36, 2024.</div>
</div>
<div id="ref-tunstall2023zephyr" class="csl-entry" role="listitem">
<div class="csl-left-margin">[21] </div><div class="csl-right-inline">L. Tunstall <em>et al.</em>, <span>“Zephyr: Direct distillation of <span>LM</span> alignment,”</span> in <em>First conference on language modeling</em>, 2024. Available: <a href="https://openreview.net/forum?id=aKkAwZB6JV">https://openreview.net/forum?id=aKkAwZB6JV</a></div>
</div>
<div id="ref-ivison2023camels" class="csl-entry" role="listitem">
<div class="csl-left-margin">[22] </div><div class="csl-right-inline"><span class="nocase">H. Ivison <em>et al.</em></span>, <span>“Camels in a changing climate: Enhancing lm adaptation with tulu 2,”</span> <em>arXiv preprint arXiv:2311.10702</em>, 2023.</div>
</div>
<div id="ref-cui2023ultrafeedback" class="csl-entry" role="listitem">
<div class="csl-left-margin">[23] </div><div class="csl-right-inline">G. Cui <em>et al.</em>, <span>“Ultrafeedback: Boosting language models with high-quality feedback,”</span> 2023.</div>
</div>
<div id="ref-dubey2024llama" class="csl-entry" role="listitem">
<div class="csl-left-margin">[24] </div><div class="csl-right-inline"><span class="nocase">A. Dubey <em>et al.</em></span>, <span>“The llama 3 herd of models,”</span> <em>arXiv preprint arXiv:2407.21783</em>, 2024.</div>
</div>
<div id="ref-adler2024nemotron" class="csl-entry" role="listitem">
<div class="csl-left-margin">[25] </div><div class="csl-right-inline"><span class="nocase">B. Adler <em>et al.</em></span>, <span>“Nemotron-4 340B technical report,”</span> <em>arXiv preprint arXiv:2406.11704</em>, 2024.</div>
</div>
<div id="ref-wirth2017survey" class="csl-entry" role="listitem">
<div class="csl-left-margin">[26] </div><div class="csl-right-inline">C. Wirth, R. Akrour, G. Neumann, and J. Fürnkranz, <span>“A survey of preference-based reinforcement learning methods,”</span> <em>Journal of Machine Learning Research</em>, vol. 18, no. 136, pp. 1–46, 2017.</div>
</div>
<div id="ref-kaufmann2023survey" class="csl-entry" role="listitem">
<div class="csl-left-margin">[27] </div><div class="csl-right-inline">T. Kaufmann, P. Weng, V. Bengs, and E. Hüllermeier, <span>“A survey of reinforcement learning from human feedback,”</span> <em>arXiv preprint arXiv:2312.14925</em>, 2023.</div>
</div>
<div id="ref-casper2023open" class="csl-entry" role="listitem">
<div class="csl-left-margin">[28] </div><div class="csl-right-inline"><span class="nocase">S. Casper <em>et al.</em></span>, <span>“Open problems and fundamental limitations of reinforcement learning from human feedback,”</span> <em>arXiv preprint arXiv:2307.15217</em>, 2023.</div>
</div>
<div id="ref-knox2008tamer" class="csl-entry" role="listitem">
<div class="csl-left-margin">[29] </div><div class="csl-right-inline">W. B. Knox and P. Stone, <span>“Tamer: Training an agent manually via evaluative reinforcement,”</span> in <em>2008 7th IEEE international conference on development and learning</em>, IEEE, 2008, pp. 292–297.</div>
</div>
<div id="ref-macglashan2017interactive" class="csl-entry" role="listitem">
<div class="csl-left-margin">[30] </div><div class="csl-right-inline">J. MacGlashan <em>et al.</em>, <span>“Interactive learning from policy-dependent human feedback,”</span> in <em>International conference on machine learning</em>, PMLR, 2017, pp. 2285–2294.</div>
</div>
<div id="ref-ibarz2018reward" class="csl-entry" role="listitem">
<div class="csl-left-margin">[31] </div><div class="csl-right-inline">B. Ibarz, J. Leike, T. Pohlen, G. Irving, S. Legg, and D. Amodei, <span>“Reward learning from human preferences and demonstrations in atari,”</span> <em>Advances in neural information processing systems</em>, vol. 31, 2018.</div>
</div>
<div id="ref-warnell2018deep" class="csl-entry" role="listitem">
<div class="csl-left-margin">[32] </div><div class="csl-right-inline">G. Warnell, N. Waytowich, V. Lawhern, and P. Stone, <span>“Deep tamer: Interactive agent shaping in high-dimensional state spaces,”</span> in <em>Proceedings of the AAAI conference on artificial intelligence</em>, 2018.</div>
</div>
<div id="ref-leike2018scalable" class="csl-entry" role="listitem">
<div class="csl-left-margin">[33] </div><div class="csl-right-inline">J. Leike, D. Krueger, T. Everitt, M. Martic, V. Maini, and S. Legg, <span>“Scalable agent alignment via reward modeling: A research direction,”</span> <em>arXiv preprint arXiv:1811.07871</em>, 2018.</div>
</div>
<div id="ref-ziegler2019fine" class="csl-entry" role="listitem">
<div class="csl-left-margin">[34] </div><div class="csl-right-inline">D. M. Ziegler <em>et al.</em>, <span>“Fine-tuning language models from human preferences,”</span> <em>arXiv preprint arXiv:1909.08593</em>, 2019.</div>
</div>
<div id="ref-wu2021recursively" class="csl-entry" role="listitem">
<div class="csl-left-margin">[35] </div><div class="csl-right-inline">J. Wu <em>et al.</em>, <span>“Recursively summarizing books with human feedback,”</span> <em>arXiv preprint arXiv:2109.10862</em>, 2021.</div>
</div>
<div id="ref-menick2022teaching" class="csl-entry" role="listitem">
<div class="csl-left-margin">[36] </div><div class="csl-right-inline"><span class="nocase">J. Menick <em>et al.</em></span>, <span>“Teaching language models to support answers with verified quotes,”</span> <em>arXiv preprint arXiv:2203.11147</em>, 2022.</div>
</div>
<div id="ref-glaese2022improving" class="csl-entry" role="listitem">
<div class="csl-left-margin">[37] </div><div class="csl-right-inline"><span class="nocase">A. Glaese <em>et al.</em></span>, <span>“Improving alignment of dialogue agents via targeted human judgements,”</span> <em>arXiv preprint arXiv:2209.14375</em>, 2022.</div>
</div>
<div id="ref-gao2023scaling" class="csl-entry" role="listitem">
<div class="csl-left-margin">[38] </div><div class="csl-right-inline">L. Gao, J. Schulman, and J. Hilton, <span>“Scaling laws for reward model overoptimization,”</span> in <em>International conference on machine learning</em>, PMLR, 2023, pp. 10835–10866.</div>
</div>
<div id="ref-ganguli2022red" class="csl-entry" role="listitem">
<div class="csl-left-margin">[39] </div><div class="csl-right-inline"><span class="nocase">D. Ganguli <em>et al.</em></span>, <span>“Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned,”</span> <em>arXiv preprint arXiv:2209.07858</em>, 2022.</div>
</div>
<div id="ref-ramamurthy2022reinforcement" class="csl-entry" role="listitem">
<div class="csl-left-margin">[40] </div><div class="csl-right-inline">R. Ramamurthy <em>et al.</em>, <span>“Is reinforcement learning (not) for natural language processing: Benchmarks, baselines, and building blocks for natural language policy optimization,”</span> <em>arXiv preprint arXiv:2210.01241</em>, 2022.</div>
</div>
<div id="ref-havrilla-etal-2023-trlx" class="csl-entry" role="listitem">
<div class="csl-left-margin">[41] </div><div class="csl-right-inline">A. Havrilla <em>et al.</em>, <span>“Trl<span>X</span>: A framework for large scale reinforcement learning from human feedback,”</span> in <em>Proceedings of the 2023 conference on empirical methods in natural language processing</em>, Singapore: Association for Computational Linguistics, Dec. 2023, pp. 8578–8595. doi: <a href="https://doi.org/10.18653/v1/2023.emnlp-main.530">10.18653/v1/2023.emnlp-main.530</a>.</div>
</div>
<div id="ref-vonwerra2022trl" class="csl-entry" role="listitem">
<div class="csl-left-margin">[42] </div><div class="csl-right-inline">L. von Werra <em>et al.</em>, <span>“TRL: Transformer reinforcement learning,”</span> <em>GitHub repository</em>. <a href="https://github.com/huggingface/trl" class="uri">https://github.com/huggingface/trl</a>; GitHub, 2020.</div>
</div>
<div id="ref-openai2022chatgpt" class="csl-entry" role="listitem">
<div class="csl-left-margin">[43] </div><div class="csl-right-inline">OpenAI, <span>“ChatGPT: Optimizing language models for dialogue.”</span> <a href="https://openai.com/blog/chatgpt/" class="uri">https://openai.com/blog/chatgpt/</a>, 2022.</div>
</div>
<div id="ref-touvron2023llama" class="csl-entry" role="listitem">
<div class="csl-left-margin">[44] </div><div class="csl-right-inline"><span class="nocase">H. Touvron <em>et al.</em></span>, <span>“Llama 2: Open foundation and fine-tuned chat models,”</span> <em>arXiv preprint arXiv:2307.09288</em>, 2023.</div>
</div>
<div id="ref-lightman2023let" class="csl-entry" role="listitem">
<div class="csl-left-margin">[45] </div><div class="csl-right-inline">H. Lightman <em>et al.</em>, <span>“Let’s verify step by step,”</span> <em>arXiv preprint arXiv:2305.20050</em>, 2023.</div>
</div>
<div id="ref-kumar2024training" class="csl-entry" role="listitem">
<div class="csl-left-margin">[46] </div><div class="csl-right-inline"><span class="nocase">A. Kumar <em>et al.</em></span>, <span>“Training language models to self-correct via reinforcement learning,”</span> <em>arXiv preprint arXiv:2409.12917</em>, 2024.</div>
</div>
<div id="ref-singh2023beyond" class="csl-entry" role="listitem">
<div class="csl-left-margin">[47] </div><div class="csl-right-inline"><span class="nocase">A. Singh <em>et al.</em></span>, <span>“Beyond human data: Scaling self-training for problem-solving with language models,”</span> <em>arXiv preprint arXiv:2312.06585</em>, 2023.</div>
</div>
<div id="ref-openai2024o1" class="csl-entry" role="listitem">
<div class="csl-left-margin">[48] </div><div class="csl-right-inline">OpenAI, <span>“Introducing OpenAI o1-preview.”</span> Sept. 2024. Available: <a href="https://openai.com/index/introducing-openai-o1-preview/">https://openai.com/index/introducing-openai-o1-preview/</a></div>
</div>
<div id="ref-Vaswani2017AttentionIA" class="csl-entry" role="listitem">
<div class="csl-left-margin">[49] </div><div class="csl-right-inline">A. Vaswani <em>et al.</em>, <span>“Attention is all you need,”</span> in <em>Neural information processing systems</em>, 2017. Available: <a href="https://api.semanticscholar.org/CorpusID:13756489">https://api.semanticscholar.org/CorpusID:13756489</a></div>
</div>
<div id="ref-Bahdanau2014NeuralMT" class="csl-entry" role="listitem">
<div class="csl-left-margin">[50] </div><div class="csl-right-inline">D. Bahdanau, K. Cho, and Y. Bengio, <span>“Neural machine translation by jointly learning to align and translate,”</span> <em>CoRR</em>, vol. abs/1409.0473, 2014, Available: <a href="https://api.semanticscholar.org/CorpusID:11212020">https://api.semanticscholar.org/CorpusID:11212020</a></div>
</div>
<div id="ref-hinton2015distilling" class="csl-entry" role="listitem">
<div class="csl-left-margin">[51] </div><div class="csl-right-inline">G. Hinton, O. Vinyals, and J. Dean, <span>“Distilling the knowledge in a neural network,”</span> <em>arXiv preprint arXiv:1503.02531</em>, 2015.</div>
</div>
<div id="ref-team2024gemma" class="csl-entry" role="listitem">
<div class="csl-left-margin">[52] </div><div class="csl-right-inline"><span class="nocase">G. Team <em>et al.</em></span>, <span>“Gemma 2: Improving open language models at a practical size,”</span> <em>arXiv preprint arXiv:2408.00118</em>, 2024.</div>
</div>
<div id="ref-agarwal2024policy" class="csl-entry" role="listitem">
<div class="csl-left-margin">[53] </div><div class="csl-right-inline">R. Agarwal <em>et al.</em>, <span>“On-policy distillation of language models: Learning from self-generated mistakes,”</span> in <em>The twelfth international conference on learning representations</em>, 2024.</div>
</div>
<div id="ref-wei2022chain" class="csl-entry" role="listitem">
<div class="csl-left-margin">[54] </div><div class="csl-right-inline"><span class="nocase">J. Wei <em>et al.</em></span>, <span>“Chain-of-thought prompting elicits reasoning in large language models,”</span> <em>Advances in neural information processing systems</em>, vol. 35, pp. 24824–24837, 2022.</div>
</div>
<div id="ref-sutton2018reinforcement" class="csl-entry" role="listitem">
<div class="csl-left-margin">[55] </div><div class="csl-right-inline">R. S. Sutton, <span>“Reinforcement learning: An introduction,”</span> <em>A Bradford Book</em>, 2018.</div>
</div>
<div id="ref-lambert2022illustrating" class="csl-entry" role="listitem">
<div class="csl-left-margin">[56] </div><div class="csl-right-inline">N. Lambert, L. Castricato, L. von Werra, and A. Havrilla, <span>“Illustrating reinforcement learning from human feedback (RLHF),”</span> <em>Hugging Face Blog</em>, 2022.</div>
</div>
<div id="ref-li2022branch" class="csl-entry" role="listitem">
<div class="csl-left-margin">[57] </div><div class="csl-right-inline">M. Li <em>et al.</em>, <span>“Branch-train-merge: Embarrassingly parallel training of expert language models,”</span> <em>arXiv preprint arXiv:2208.03306</em>, 2022.</div>
</div>
<div id="ref-cohere2025command" class="csl-entry" role="listitem">
<div class="csl-left-margin">[58] </div><div class="csl-right-inline"><span class="nocase">T. Cohere <em>et al.</em></span>, <span>“Command a: An enterprise-ready large language model,”</span> <em>arXiv preprint arXiv:2504.00698</em>, 2025.</div>
</div>
<div id="ref-olmo20242" class="csl-entry" role="listitem">
<div class="csl-left-margin">[59] </div><div class="csl-right-inline"><span class="nocase">T. OLMo <em>et al.</em></span>, <span>“2 OLMo 2 furious,”</span> <em>arXiv preprint arXiv:2501.00656</em>, 2024.</div>
</div>
<div id="ref-alrashed2024smoltulu" class="csl-entry" role="listitem">
<div class="csl-left-margin">[60] </div><div class="csl-right-inline">S. Alrashed, <span>“SmolTulu: Higher learning rate to batch size ratios can lead to better reasoning in SLMs,”</span> <em>arXiv preprint arXiv:2412.08347</em>, 2024.</div>
</div>
<div id="ref-guo2025deepseek" class="csl-entry" role="listitem">
<div class="csl-left-margin">[61] </div><div class="csl-right-inline"><span class="nocase">D. Guo <em>et al.</em></span>, <span>“Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,”</span> <em>arXiv preprint arXiv:2501.12948</em>, 2025.</div>
</div>
<div id="ref-yang2025qwen3" class="csl-entry" role="listitem">
<div class="csl-left-margin">[62] </div><div class="csl-right-inline"><span class="nocase">A. Yang <em>et al.</em></span>, <span>“Qwen3 technical report,”</span> <em>arXiv preprint arXiv:2505.09388</em>, 2025.</div>
</div>
<div id="ref-xia2025mimo" class="csl-entry" role="listitem">
<div class="csl-left-margin">[63] </div><div class="csl-right-inline"><span class="nocase">B. Xia <em>et al.</em></span>, <span>“MiMo: Unlocking the reasoning potential of language model–from pretraining to posttraining,”</span> <em>arXiv preprint arXiv:2505.07608</em>, 2025.</div>
</div>
<div id="ref-seed2025seed" class="csl-entry" role="listitem">
<div class="csl-left-margin">[64] </div><div class="csl-right-inline"><span class="nocase">B. Seed <em>et al.</em></span>, <span>“Seed-thinking-v1. 5: Advancing superb reasoning models with reinforcement learning,”</span> <em>arXiv preprint arXiv:2504.13914</em>, 2025.</div>
</div>
<div id="ref-lambert2023entangled" class="csl-entry" role="listitem">
<div class="csl-left-margin">[65] </div><div class="csl-right-inline">N. Lambert, T. K. Gilbert, and T. Zick, <span>“Entangled preferences: The history and risks of reinforcement learning and human feedback,”</span> <em>arXiv preprint arXiv:2310.13595</em>, 2023.</div>
</div>
<div id="ref-conitzer2024social" class="csl-entry" role="listitem">
<div class="csl-left-margin">[66] </div><div class="csl-right-inline"><span class="nocase">V. Conitzer <em>et al.</em></span>, <span>“Social choice should guide AI alignment in dealing with diverse human feedback,”</span> <em>arXiv preprint arXiv:2404.10271</em>, 2024.</div>
</div>
<div id="ref-mishra2023ai" class="csl-entry" role="listitem">
<div class="csl-left-margin">[67] </div><div class="csl-right-inline">A. Mishra, <span>“Ai alignment and social choice: Fundamental limitations and policy implications,”</span> <em>arXiv preprint arXiv:2310.16048</em>, 2023.</div>
</div>
<div id="ref-kirk2024prism" class="csl-entry" role="listitem">
<div class="csl-left-margin">[68] </div><div class="csl-right-inline"><span class="nocase">H. R. Kirk <em>et al.</em></span>, <span>“The PRISM alignment project: What participatory, representative and individualised human feedback reveals about the subjective and multicultural alignment of large language models,”</span> <em>arXiv preprint arXiv:2404.16019</em>, 2024.</div>
</div>
<div id="ref-poddar2024personalizing" class="csl-entry" role="listitem">
<div class="csl-left-margin">[69] </div><div class="csl-right-inline">S. Poddar, Y. Wan, H. Ivison, A. Gupta, and N. Jaques, <span>“Personalizing reinforcement learning from human feedback with variational preference learning,”</span> <em>arXiv preprint arXiv:2408.10075</em>, 2024.</div>
</div>
<div id="ref-arnauld1861port" class="csl-entry" role="listitem">
<div class="csl-left-margin">[70] </div><div class="csl-right-inline">A. Arnauld, <em>The port-royal logic</em>. 1662.</div>
</div>
<div id="ref-bentham1823hedonic" class="csl-entry" role="listitem">
<div class="csl-left-margin">[71] </div><div class="csl-right-inline">J. Bentham, <em>An introduction to the principles of morals and legislation</em>. 1823.</div>
</div>
<div id="ref-ramsey2016truth" class="csl-entry" role="listitem">
<div class="csl-left-margin">[72] </div><div class="csl-right-inline">F. P. Ramsey, <span>“Truth and probability,”</span> <em>Readings in Formal Epistemology: Sourcebook</em>, pp. 21–45, 2016.</div>
</div>
<div id="ref-hirschman1984against" class="csl-entry" role="listitem">
<div class="csl-left-margin">[73] </div><div class="csl-right-inline">A. O. Hirschman, <span>“Against parsimony: Three easy ways of complicating some categories of economic discourse,”</span> <em>Bulletin of the American Academy of arts and Sciences</em>, vol. 37, no. 8, pp. 11–28, 1984.</div>
</div>
<div id="ref-hadfield2014microfoundations" class="csl-entry" role="listitem">
<div class="csl-left-margin">[74] </div><div class="csl-right-inline">G. K. Hadfield and B. R. Weingast, <span>“Microfoundations of the rule of law,”</span> <em>Annual Review of Political Science</em>, vol. 17, pp. 21–42, 2014.</div>
</div>
<div id="ref-thorndike1927law" class="csl-entry" role="listitem">
<div class="csl-left-margin">[75] </div><div class="csl-right-inline">E. L. Thorndike, <span>“The law of effect,”</span> <em>The American journal of psychology</em>, vol. 39, no. 1/4, pp. 212–222, 1927.</div>
</div>
<div id="ref-skinner2019behavior" class="csl-entry" role="listitem">
<div class="csl-left-margin">[76] </div><div class="csl-right-inline">B. F. Skinner, <em>The behavior of organisms: An experimental analysis</em>. BF Skinner Foundation, 2019.</div>
</div>
<div id="ref-briggs2014normative" class="csl-entry" role="listitem">
<div class="csl-left-margin">[77] </div><div class="csl-right-inline">R. A. Briggs, <span>“Normative theories of rational choice: Expected utility,”</span> 2014.</div>
</div>
<div id="ref-widrow1960adaptive" class="csl-entry" role="listitem">
<div class="csl-left-margin">[78] </div><div class="csl-right-inline">B. Widrow and M. E. Hoff, <span>“Adaptive switching circuits,”</span> Stanford Univ Ca Stanford Electronics Labs, 1960.</div>
</div>
<div id="ref-singh2009rewards" class="csl-entry" role="listitem">
<div class="csl-left-margin">[79] </div><div class="csl-right-inline">S. Singh, R. L. Lewis, and A. G. Barto, <span>“Where do rewards come from,”</span> in <em>Proceedings of the annual conference of the cognitive science society</em>, Cognitive Science Society, 2009, pp. 2601–2606.</div>
</div>
<div id="ref-mcclure2003computational" class="csl-entry" role="listitem">
<div class="csl-left-margin">[80] </div><div class="csl-right-inline">S. M. McClure, N. D. Daw, and P. R. Montague, <span>“A computational substrate for incentive salience,”</span> <em>Trends in neurosciences</em>, vol. 26, no. 8, pp. 423–428, 2003.</div>
</div>
<div id="ref-silver2021reward" class="csl-entry" role="listitem">
<div class="csl-left-margin">[81] </div><div class="csl-right-inline">D. Silver, S. Singh, D. Precup, and R. S. Sutton, <span>“Reward is enough,”</span> <em>Artificial Intelligence</em>, vol. 299, p. 103535, 2021.</div>
</div>
<div id="ref-bellman1957markovian" class="csl-entry" role="listitem">
<div class="csl-left-margin">[82] </div><div class="csl-right-inline">R. Bellman, <span>“A markovian decision process,”</span> <em>Journal of mathematics and mechanics</em>, pp. 679–684, 1957.</div>
</div>
<div id="ref-howard1960dynamic" class="csl-entry" role="listitem">
<div class="csl-left-margin">[83] </div><div class="csl-right-inline">R. A. Howard, <span>“Dynamic programming and markov processes.”</span> 1960.</div>
</div>
<div id="ref-MENDEL1970287" class="csl-entry" role="listitem">
<div class="csl-left-margin">[84] </div><div class="csl-right-inline">J. M. Mendel and R. W. McLaren, <span>“8 reinforcement-learning control and pattern recognition systems,”</span> in <em>Adaptive, learning and pattern recognition systems</em>, vol. 66, J. M. Mendel and K. S. Fu, Eds., in Mathematics in science and engineering, vol. 66., Elsevier, 1970, pp. 287–318. doi: <a href="https://doi.org/10.1016/S0076-5392(08)60497-X">https://doi.org/10.1016/S0076-5392(08)60497-X</a>.</div>
</div>
<div id="ref-waltz1965" class="csl-entry" role="listitem">
<div class="csl-left-margin">[85] </div><div class="csl-right-inline">M. Waltz and K. Fu, <span>“A heuristic approach to reinforcement learning control systems,”</span> <em>IEEE Transactions on Automatic Control</em>, vol. 10, no. 4, pp. 390–398, 1965, doi: <a href="https://doi.org/10.1109/TAC.1965.1098193">10.1109/TAC.1965.1098193</a>.</div>
</div>
<div id="ref-klopf1972brain" class="csl-entry" role="listitem">
<div class="csl-left-margin">[86] </div><div class="csl-right-inline">A. H. Klopf, <em>Brain function and adaptive systems: A heterostatic theory</em>. Air Force Cambridge Research Laboratories, Air Force Systems Command, 1972.</div>
</div>
<div id="ref-sutton1988learning" class="csl-entry" role="listitem">
<div class="csl-left-margin">[87] </div><div class="csl-right-inline">R. S. Sutton, <span>“Learning to predict by the methods of temporal differences,”</span> <em>Machine learning</em>, vol. 3, pp. 9–44, 1988.</div>
</div>
<div id="ref-tesauro1995temporal" class="csl-entry" role="listitem">
<div class="csl-left-margin">[88] </div><div class="csl-right-inline"><span class="nocase">G. Tesauro <em>et al.</em></span>, <span>“Temporal difference learning and TD-gammon,”</span> <em>Communications of the ACM</em>, vol. 38, no. 3, pp. 58–68, 1995.</div>
</div>
<div id="ref-watkins1992q" class="csl-entry" role="listitem">
<div class="csl-left-margin">[89] </div><div class="csl-right-inline">C. J. Watkins and P. Dayan, <span>“Q-learning,”</span> <em>Machine learning</em>, vol. 8, pp. 279–292, 1992.</div>
</div>
<div id="ref-mnih2013playing" class="csl-entry" role="listitem">
<div class="csl-left-margin">[90] </div><div class="csl-right-inline">V. Mnih <em>et al.</em>, <span>“Playing atari with deep reinforcement learning,”</span> <em>arXiv preprint arXiv:1312.5602</em>, 2013.</div>
</div>
<div id="ref-golnaraghi2017automatic" class="csl-entry" role="listitem">
<div class="csl-left-margin">[91] </div><div class="csl-right-inline">F. Golnaraghi and B. C. Kuo, <em>Automatic control systems</em>. McGraw-Hill Education, 2017.</div>
</div>
<div id="ref-silver2017mastering" class="csl-entry" role="listitem">
<div class="csl-left-margin">[92] </div><div class="csl-right-inline"><span class="nocase">D. Silver <em>et al.</em></span>, <span>“Mastering the game of go without human knowledge,”</span> <em>Nature</em>, vol. 550, no. 7676, pp. 354–359, 2017.</div>
</div>
<div id="ref-degrave2022magnetic" class="csl-entry" role="listitem">
<div class="csl-left-margin">[93] </div><div class="csl-right-inline"><span class="nocase">J. Degrave <em>et al.</em></span>, <span>“Magnetic control of tokamak plasmas through deep reinforcement learning,”</span> <em>Nature</em>, vol. 602, no. 7897, pp. 414–419, 2022.</div>
</div>
<div id="ref-Kaufmann2023fpv" class="csl-entry" role="listitem">
<div class="csl-left-margin">[94] </div><div class="csl-right-inline">E. Kaufmann, L. Bauersfeld, A. Loquercio, M. Müller, V. Koltun, and D. Scaramuzza, <span>“Champion-level drone racing using deep reinforcement learning,”</span> <em>Nature</em>, vol. 620, no. 7976, pp. 982–987, 2023, doi: <a href="https://doi.org/10.1038/s41586-023-06419-4">10.1038/s41586-023-06419-4</a>.</div>
</div>
<div id="ref-agarwal2021deep" class="csl-entry" role="listitem">
<div class="csl-left-margin">[95] </div><div class="csl-right-inline">R. Agarwal, M. Schwarzer, P. S. Castro, A. C. Courville, and M. Bellemare, <span>“Deep reinforcement learning at the edge of the statistical precipice,”</span> <em>Advances in neural information processing systems</em>, vol. 34, pp. 29304–29320, 2021.</div>
</div>
<div id="ref-ng2000algorithms" class="csl-entry" role="listitem">
<div class="csl-left-margin">[96] </div><div class="csl-right-inline"><span class="nocase">A. Y. Ng, S. Russell, <em>et al.</em></span>, <span>“Algorithms for inverse reinforcement learning.”</span> in <em>Proceedings of the seventeenth international conference on machine learning</em>, in ICML ’00. 2000, pp. 663--670.</div>
</div>
<div id="ref-salha2011aesthetics" class="csl-entry" role="listitem">
<div class="csl-left-margin">[97] </div><div class="csl-right-inline">N. Salha, <span>“Aesthetics &amp; art in the early development of human-computer interfaces,”</span> PhD thesis, Universit<span>ä</span>t Bremen, 2011.</div>
</div>
<div id="ref-gilbert2022choices" class="csl-entry" role="listitem">
<div class="csl-left-margin">[98] </div><div class="csl-right-inline">T. K. Gilbert, S. Dean, T. Zick, and N. Lambert, <span>“Choices, risks, and reward reports: Charting public policy for reinforcement learning systems,”</span> <em>arXiv preprint arXiv:2202.05716</em>, 2022.</div>
</div>
<div id="ref-von1947theory" class="csl-entry" role="listitem">
<div class="csl-left-margin">[99] </div><div class="csl-right-inline">J. Von Neumann and O. Morgenstern, <span>“Theory of games and economic behavior, 2nd rev,”</span> 1947.</div>
</div>
<div id="ref-pitis2019rethinking" class="csl-entry" role="listitem">
<div class="csl-left-margin">[100] </div><div class="csl-right-inline">S. Pitis, <span>“Rethinking the discount factor in reinforcement learning: A decision theoretic approach,”</span> in <em>Proceedings of the AAAI conference on artificial intelligence</em>, 2019, pp. 7949–7956.</div>
</div>
<div id="ref-pitis2023consistent" class="csl-entry" role="listitem">
<div class="csl-left-margin">[101] </div><div class="csl-right-inline">S. Pitis, <span>“Consistent aggregation of objectives with diverse time preferences requires non-markovian rewards,”</span> <em>arXiv preprint arXiv:2310.00435</em>, 2023.</div>
</div>
<div id="ref-abel2021expressivity" class="csl-entry" role="listitem">
<div class="csl-left-margin">[102] </div><div class="csl-right-inline">D. Abel <em>et al.</em>, <span>“On the expressivity of markov reward,”</span> <em>Advances in Neural Information Processing Systems</em>, vol. 34, pp. 7799–7812, 2021.</div>
</div>
<div id="ref-sen1973behaviour" class="csl-entry" role="listitem">
<div class="csl-left-margin">[103] </div><div class="csl-right-inline">A. Sen, <span>“Behaviour and the concept of preference,”</span> <em>Economica</em>, vol. 40, no. 159, pp. 241–259, 1973.</div>
</div>
<div id="ref-arrow1950difficulty" class="csl-entry" role="listitem">
<div class="csl-left-margin">[104] </div><div class="csl-right-inline">K. J. Arrow, <span>“A difficulty in the concept of social welfare,”</span> <em>Journal of political economy</em>, vol. 58, no. 4, pp. 328–346, 1950.</div>
</div>
<div id="ref-maskin2014arrow" class="csl-entry" role="listitem">
<div class="csl-left-margin">[105] </div><div class="csl-right-inline">E. Maskin and A. Sen, <em>The arrow impossibility theorem</em>. Columbia University Press, 2014.</div>
</div>
<div id="ref-harsanyi1977rule" class="csl-entry" role="listitem">
<div class="csl-left-margin">[106] </div><div class="csl-right-inline">J. C. Harsanyi, <span>“Rule utilitarianism and decision theory,”</span> <em>Erkenntnis</em>, vol. 11, no. 1, pp. 25–53, 1977.</div>
</div>
<div id="ref-hadfield2016cooperative" class="csl-entry" role="listitem">
<div class="csl-left-margin">[107] </div><div class="csl-right-inline">D. Hadfield-Menell, S. J. Russell, P. Abbeel, and A. Dragan, <span>“Cooperative inverse reinforcement learning,”</span> <em>Advances in neural information processing systems</em>, vol. 29, 2016.</div>
</div>
<div id="ref-fickinger2020multi" class="csl-entry" role="listitem">
<div class="csl-left-margin">[108] </div><div class="csl-right-inline">A. Fickinger, S. Zhuang, D. Hadfield-Menell, and S. Russell, <span>“Multi-principal assistance games,”</span> <em>arXiv preprint arXiv:2007.09540</em>, 2020.</div>
</div>
<div id="ref-soares2015corrigibility" class="csl-entry" role="listitem">
<div class="csl-left-margin">[109] </div><div class="csl-right-inline">N. Soares, B. Fallenstein, S. Armstrong, and E. Yudkowsky, <span>“Corrigibility,”</span> in <em>Workshops at the twenty-ninth AAAI conference on artificial intelligence</em>, 2015.</div>
</div>
<div id="ref-pettigrew2019choosing" class="csl-entry" role="listitem">
<div class="csl-left-margin">[110] </div><div class="csl-right-inline">R. Pettigrew, <em>Choosing for changing selves</em>. Oxford University Press, 2019.</div>
</div>
<div id="ref-wang2024helpsteer2p" class="csl-entry" role="listitem">
<div class="csl-left-margin">[111] </div><div class="csl-right-inline">Z. Wang <em>et al.</em>, <span>“HelpSteer2-preference: Complementing ratings with preferences,”</span> <em>arXiv preprint arXiv:2410.01257</em>, 2024.</div>
</div>
<div id="ref-malik2025rewardbench" class="csl-entry" role="listitem">
<div class="csl-left-margin">[112] </div><div class="csl-right-inline">S. Malik <em>et al.</em>, <span>“RewardBench 2: Advancing reward model evaluation,”</span> <em>arXiv preprint arXiv:2506.01937</em>, 2025.</div>
</div>
<div id="ref-chiang2024chatbot" class="csl-entry" role="listitem">
<div class="csl-left-margin">[113] </div><div class="csl-right-inline"><span class="nocase">W.-L. Chiang <em>et al.</em></span>, <span>“Chatbot arena: An open platform for evaluating llms by human preference,”</span> <em>arXiv preprint arXiv:2403.04132</em>, 2024.</div>
</div>
<div id="ref-likert1932technique" class="csl-entry" role="listitem">
<div class="csl-left-margin">[114] </div><div class="csl-right-inline">R. Likert, <span>“A technique for the measurement of attitudes.”</span> <em>Archives of psychology</em>, 1932.</div>
</div>
<div id="ref-zhou2023instructionfollowingevaluationlargelanguage" class="csl-entry" role="listitem">
<div class="csl-left-margin">[115] </div><div class="csl-right-inline">J. Zhou <em>et al.</em>, <span>“Instruction-following evaluation for large language models.”</span> 2023. Available: <a href="https://arxiv.org/abs/2311.07911">https://arxiv.org/abs/2311.07911</a></div>
</div>
<div id="ref-ethayarajh2024kto" class="csl-entry" role="listitem">
<div class="csl-left-margin">[116] </div><div class="csl-right-inline">K. Ethayarajh, W. Xu, N. Muennighoff, D. Jurafsky, and D. Kiela, <span>“Kto: Model alignment as prospect theoretic optimization,”</span> <em>arXiv preprint arXiv:2402.01306</em>, 2024.</div>
</div>
<div id="ref-wu2024fine" class="csl-entry" role="listitem">
<div class="csl-left-margin">[117] </div><div class="csl-right-inline">Z. Wu <em>et al.</em>, <span>“Fine-grained human feedback gives better rewards for language model training,”</span> <em>Advances in Neural Information Processing Systems</em>, vol. 36, 2024.</div>
</div>
<div id="ref-chen2024learning" class="csl-entry" role="listitem">
<div class="csl-left-margin">[118] </div><div class="csl-right-inline">A. Chen <em>et al.</em>, <span>“Learning from natural language feedback,”</span> <em>Transactions on Machine Learning Research</em>, 2024.</div>
</div>
<div id="ref-kumar2025detecting" class="csl-entry" role="listitem">
<div class="csl-left-margin">[119] </div><div class="csl-right-inline">A. Kumar, Y. He, A. H. Markosyan, B. Chern, and I. Arrieta-Ibarra, <span>“Detecting prefix bias in LLM-based reward models,”</span> <em>arXiv preprint arXiv:2505.13487</em>, 2025.</div>
</div>
<div id="ref-bharadwaj2025flatteryflufffogdiagnosing" class="csl-entry" role="listitem">
<div class="csl-left-margin">[120] </div><div class="csl-right-inline">A. Bharadwaj, C. Malaviya, N. Joshi, and M. Yatskar, <span>“Flattery, fluff, and fog: Diagnosing and mitigating idiosyncratic biases in preference models.”</span> 2025. Available: <a href="https://arxiv.org/abs/2506.05339">https://arxiv.org/abs/2506.05339</a></div>
</div>
<div id="ref-sharma2023towards" class="csl-entry" role="listitem">
<div class="csl-left-margin">[121] </div><div class="csl-right-inline">M. Sharma <em>et al.</em>, <span>“Towards understanding sycophancy in language models,”</span> in <em>The twelfth international conference on learning representations</em>, 2024. Available: <a href="https://openreview.net/forum?id=tvhaxkMKAn">https://openreview.net/forum?id=tvhaxkMKAn</a></div>
</div>
<div id="ref-bu2025beyond" class="csl-entry" role="listitem">
<div class="csl-left-margin">[122] </div><div class="csl-right-inline">Y. Bu, L. Huo, Y. Jing, and Q. Yang, <span>“Beyond excess and deficiency: Adaptive length bias mitigation in reward models for RLHF,”</span> in <em>Findings of the association for computational linguistics: NAACL 2025</em>, 2025, pp. 3091–3098.</div>
</div>
<div id="ref-zhang2024lists" class="csl-entry" role="listitem">
<div class="csl-left-margin">[123] </div><div class="csl-right-inline">X. Zhang, W. Xiong, L. Chen, T. Zhou, H. Huang, and T. Zhang, <span>“From lists to emojis: How format bias affects model alignment,”</span> <em>arXiv preprint arXiv:2409.11704</em>, 2024.</div>
</div>
<div id="ref-openai2024modelspec" class="csl-entry" role="listitem">
<div class="csl-left-margin">[124] </div><div class="csl-right-inline">OpenAI, <span>“Introducing the model spec.”</span> May 2024. Available: <a href="https://openai.com/index/introducing-the-model-spec/">https://openai.com/index/introducing-the-model-spec/</a></div>
</div>
<div id="ref-BradleyTerry" class="csl-entry" role="listitem">
<div class="csl-left-margin">[125] </div><div class="csl-right-inline">R. A. Bradley and M. E. Terry, <span>“Rank analysis of incomplete block designs: I. The method of paired comparisons,”</span> <em>Biometrika</em>, vol. 39, no. 3/4, pp. 324–345, 1952, Accessed: Feb. 13, 2023. [Online]. Available: <a href="http://www.jstor.org/stable/2334029">http://www.jstor.org/stable/2334029</a></div>
</div>
<div id="ref-zhu2024starling" class="csl-entry" role="listitem">
<div class="csl-left-margin">[126] </div><div class="csl-right-inline">B. Zhu <em>et al.</em>, <span>“Starling-7b: Improving helpfulness and harmlessness with rlaif,”</span> in <em>First conference on language modeling</em>, 2024.</div>
</div>
<div id="ref-liu2019learning" class="csl-entry" role="listitem">
<div class="csl-left-margin">[127] </div><div class="csl-right-inline">A. Liu, Z. Zhao, C. Liao, P. Lu, and L. Xia, <span>“Learning plackett-luce mixtures from partial preferences,”</span> in <em>Proceedings of the AAAI conference on artificial intelligence</em>, 2019, pp. 4328–4335.</div>
</div>
<div id="ref-zhu2023principled" class="csl-entry" role="listitem">
<div class="csl-left-margin">[128] </div><div class="csl-right-inline">B. Zhu, M. Jordan, and J. Jiao, <span>“Principled reinforcement learning with human feedback from pairwise or k-wise comparisons,”</span> in <em>International conference on machine learning</em>, PMLR, 2023, pp. 43037–43067.</div>
</div>
<div id="ref-cobbe2021gsm8k" class="csl-entry" role="listitem">
<div class="csl-left-margin">[129] </div><div class="csl-right-inline">K. Cobbe <em>et al.</em>, <span>“Training verifiers to solve math word problems,”</span> <em>arXiv preprint arXiv:2110.14168</em>, 2021.</div>
</div>
<div id="ref-lyu2025exploring" class="csl-entry" role="listitem">
<div class="csl-left-margin">[130] </div><div class="csl-right-inline"><span class="nocase">C. Lyu <em>et al.</em></span>, <span>“Exploring the limit of outcome reward for learning mathematical reasoning,”</span> <em>arXiv preprint arXiv:2502.06781</em>, 2025.</div>
</div>
<div id="ref-zheng2023judging" class="csl-entry" role="listitem">
<div class="csl-left-margin">[131] </div><div class="csl-right-inline"><span class="nocase">L. Zheng <em>et al.</em></span>, <span>“Judging llm-as-a-judge with mt-bench and chatbot arena,”</span> <em>Advances in Neural Information Processing Systems</em>, vol. 36, pp. 46595–46623, 2023.</div>
</div>
<div id="ref-dubois2024length" class="csl-entry" role="listitem">
<div class="csl-left-margin">[132] </div><div class="csl-right-inline">Y. Dubois, B. Galambosi, P. Liang, and T. B. Hashimoto, <span>“Length-controlled alpacaeval: A simple way to debias automatic evaluators,”</span> <em>arXiv preprint arXiv:2404.04475</em>, 2024.</div>
</div>
<div id="ref-li2024crowdsourced" class="csl-entry" role="listitem">
<div class="csl-left-margin">[133] </div><div class="csl-right-inline">T. Li <em>et al.</em>, <span>“From crowdsourced data to high-quality benchmarks: Arena-hard and BenchBuilder pipeline,”</span> <em>arXiv preprint arXiv:2406.11939</em>, 2024.</div>
</div>
<div id="ref-lin2024wildbench" class="csl-entry" role="listitem">
<div class="csl-left-margin">[134] </div><div class="csl-right-inline">B. Y. Lin <em>et al.</em>, <span>“WILDBENCH: Benchmarking LLMs with challenging tasks from real users in the wild,”</span> <em>arXiv preprint arXiv:2406.04770</em>, 2024.</div>
</div>
<div id="ref-mahan2024generative" class="csl-entry" role="listitem">
<div class="csl-left-margin">[135] </div><div class="csl-right-inline">D. Mahan <em>et al.</em>, <span>“Generative reward models,”</span> 2024, Available: <a href="https://www.synthlabs.ai/pdf/Generative_Reward_Models.pdf">https://www.synthlabs.ai/pdf/Generative_Reward_Models.pdf</a></div>
</div>
<div id="ref-zhang2024generative" class="csl-entry" role="listitem">
<div class="csl-left-margin">[136] </div><div class="csl-right-inline">L. Zhang, A. Hosseini, H. Bansal, M. Kazemi, A. Kumar, and R. Agarwal, <span>“Generative verifiers: Reward modeling as next-token prediction,”</span> <em>arXiv preprint arXiv:2408.15240</em>, 2024.</div>
</div>
<div id="ref-ankner2024critique" class="csl-entry" role="listitem">
<div class="csl-left-margin">[137] </div><div class="csl-right-inline">Z. Ankner, M. Paul, B. Cui, J. D. Chang, and P. Ammanabrolu, <span>“Critique-out-loud reward models,”</span> <em>arXiv preprint arXiv:2408.11791</em>, 2024.</div>
</div>
<div id="ref-kim2023prometheus" class="csl-entry" role="listitem">
<div class="csl-left-margin">[138] </div><div class="csl-right-inline"><span class="nocase">S. Kim <em>et al.</em></span>, <span>“Prometheus: Inducing fine-grained evaluation capability in language models,”</span> in <em>The twelfth international conference on learning representations</em>, 2023.</div>
</div>
<div id="ref-lambert2024rewardbench" class="csl-entry" role="listitem">
<div class="csl-left-margin">[139] </div><div class="csl-right-inline"><span class="nocase">N. Lambert <em>et al.</em></span>, <span>“Rewardbench: Evaluating reward models for language modeling,”</span> <em>arXiv preprint arXiv:2403.13787</em>, 2024.</div>
</div>
<div id="ref-wen2024rethinking" class="csl-entry" role="listitem">
<div class="csl-left-margin">[140] </div><div class="csl-right-inline">X. Wen <em>et al.</em>, <span>“Rethinking reward model evaluation: Are we barking up the wrong tree?”</span> <em>arXiv preprint arXiv:2410.05584</em>, 2024.</div>
</div>
<div id="ref-zhou2024rmb" class="csl-entry" role="listitem">
<div class="csl-left-margin">[141] </div><div class="csl-right-inline"><span class="nocase">E. Zhou <em>et al.</em></span>, <span>“RMB: Comprehensively benchmarking reward models in LLM alignment,”</span> <em>arXiv preprint arXiv:2410.09893</em>, 2024.</div>
</div>
<div id="ref-frick2024evaluate" class="csl-entry" role="listitem">
<div class="csl-left-margin">[142] </div><div class="csl-right-inline">E. Frick <em>et al.</em>, <span>“How to evaluate reward models for RLHF,”</span> <em>arXiv preprint arXiv:2410.14872</em>, 2024.</div>
</div>
<div id="ref-liu2024rm" class="csl-entry" role="listitem">
<div class="csl-left-margin">[143] </div><div class="csl-right-inline">Y. Liu, Z. Yao, R. Min, Y. Cao, L. Hou, and J. Li, <span>“RM-bench: Benchmarking reward models of language models with subtlety and style,”</span> <em>arXiv preprint arXiv:2410.16184</em>, 2024.</div>
</div>
<div id="ref-gureja2024m" class="csl-entry" role="listitem">
<div class="csl-left-margin">[144] </div><div class="csl-right-inline">S. Gureja <em>et al.</em>, <span>“M-RewardBench: Evaluating reward models in multilingual settings,”</span> <em>arXiv preprint arXiv:2410.15522</em>, 2024.</div>
</div>
<div id="ref-jin2024rag" class="csl-entry" role="listitem">
<div class="csl-left-margin">[145] </div><div class="csl-right-inline">Z. Jin <em>et al.</em>, <span>“RAG-RewardBench: Benchmarking reward models in retrieval augmented generation for preference alignment,”</span> <em>arXiv preprint arXiv:2412.13746</em>, 2024.</div>
</div>
<div id="ref-wu2025rewordbench" class="csl-entry" role="listitem">
<div class="csl-left-margin">[146] </div><div class="csl-right-inline">Z. Wu, M. Yasunaga, A. Cohen, Y. Kim, A. Celikyilmaz, and M. Ghazvininejad, <span>“reWordBench: Benchmarking and improving the robustness of reward models with transformed inputs,”</span> <em>arXiv preprint arXiv:2503.11751</em>, 2025.</div>
</div>
<div id="ref-kim2024evaluating" class="csl-entry" role="listitem">
<div class="csl-left-margin">[147] </div><div class="csl-right-inline">S. Kim <em>et al.</em>, <span>“Evaluating robustness of reward models for mathematical reasoning,”</span> <em>arXiv preprint arXiv:2410.01729</em>, 2024.</div>
</div>
<div id="ref-liu2024acemath" class="csl-entry" role="listitem">
<div class="csl-left-margin">[148] </div><div class="csl-right-inline">Z. Liu, Y. Chen, M. Shoeybi, B. Catanzaro, and W. Ping, <span>“AceMath: Advancing frontier math reasoning with post-training and reward modeling.”</span> 2024. Available: <a href="https://arxiv.org/abs/2412.15084">https://arxiv.org/abs/2412.15084</a></div>
</div>
<div id="ref-song2025prmbench" class="csl-entry" role="listitem">
<div class="csl-left-margin">[149] </div><div class="csl-right-inline">M. Song, Z. Su, X. Qu, J. Zhou, and Y. Cheng, <span>“PRMBench: A fine-grained and challenging benchmark for process-level reward models,”</span> <em>arXiv preprint arXiv:2501.03124</em>, 2025.</div>
</div>
<div id="ref-zheng2024processbench" class="csl-entry" role="listitem">
<div class="csl-left-margin">[150] </div><div class="csl-right-inline">C. Zheng <em>et al.</em>, <span>“ProcessBench: Identifying process errors in mathematical reasoning.”</span> 2024. Available: <a href="https://arxiv.org/abs/2412.06559">https://arxiv.org/abs/2412.06559</a></div>
</div>
<div id="ref-wang2025visualprm" class="csl-entry" role="listitem">
<div class="csl-left-margin">[151] </div><div class="csl-right-inline"><span class="nocase">W. Wang <em>et al.</em></span>, <span>“VisualPRM: An effective process reward model for multimodal reasoning,”</span> <em>arXiv preprint arXiv:2503.10291</em>, 2025.</div>
</div>
<div id="ref-tu2025vilbench" class="csl-entry" role="listitem">
<div class="csl-left-margin">[152] </div><div class="csl-right-inline">H. Tu, W. Feng, H. Chen, H. Liu, X. Tang, and C. Xie, <span>“ViLBench: A suite for vision-language process reward modeling.”</span> Mar. 2025. Available: <a href="https://arxiv.org/abs/2503.20271">https://arxiv.org/abs/2503.20271</a></div>
</div>
<div id="ref-men2025agentrewardbench" class="csl-entry" role="listitem">
<div class="csl-left-margin">[153] </div><div class="csl-right-inline">T. Men, Z. Jin, P. Cao, Y. Chen, K. Liu, and J. Zhao, <span>“Agent-RewardBench: Towards a unified benchmark for reward modeling across perception, planning, and safety in real-world multimodal agents,”</span> in <em>Proceedings of the 63rd annual meeting of the association for computational linguistics (volume 1: Long papers)</em>, Vienna, Austria: Association for Computational Linguistics, July 2025, pp. 17521–17541. doi: <a href="https://doi.org/10.18653/v1/2025.acl-long.857">10.18653/v1/2025.acl-long.857</a>.</div>
</div>
<div id="ref-lin2025cuarewardbench" class="csl-entry" role="listitem">
<div class="csl-left-margin">[154] </div><div class="csl-right-inline">H. Lin <em>et al.</em>, <span>“CUARewardBench: A benchmark for evaluating reward models on computer-using agent.”</span> 2025. Available: <a href="https://arxiv.org/abs/2510.18596">https://arxiv.org/abs/2510.18596</a></div>
</div>
<div id="ref-chen2024mj" class="csl-entry" role="listitem">
<div class="csl-left-margin">[155] </div><div class="csl-right-inline"><span class="nocase">Z. Chen <em>et al.</em></span>, <span>“MJ-bench: Is your multimodal reward model really a good judge for text-to-image generation?”</span> <em>arXiv preprint arXiv:2407.04842</em>, 2024.</div>
</div>
<div id="ref-yasunaga2025multimodal" class="csl-entry" role="listitem">
<div class="csl-left-margin">[156] </div><div class="csl-right-inline">M. Yasunaga, L. Zettlemoyer, and M. Ghazvininejad, <span>“Multimodal rewardbench: Holistic evaluation of reward models for vision language models,”</span> <em>arXiv preprint arXiv:2502.14191</em>, 2025.</div>
</div>
<div id="ref-li2024vlrewardbench" class="csl-entry" role="listitem">
<div class="csl-left-margin">[157] </div><div class="csl-right-inline"><span class="nocase">L. Li <em>et al.</em></span>, <span>“VLRewardBench: A challenging benchmark for vision-language generative reward models,”</span> <em>arXiv preprint arXiv:2411.17451</em>, 2024.</div>
</div>
<div id="ref-ruan2025vlrmbench" class="csl-entry" role="listitem">
<div class="csl-left-margin">[158] </div><div class="csl-right-inline">J. Ruan <em>et al.</em>, <span>“Vlrmbench: A comprehensive and challenging benchmark for vision-language reward models,”</span> <em>arXiv preprint arXiv:2503.07478</em>, 2025.</div>
</div>
<div id="ref-wang2024interpretable" class="csl-entry" role="listitem">
<div class="csl-left-margin">[159] </div><div class="csl-right-inline">H. Wang, W. Xiong, T. Xie, H. Zhao, and T. Zhang, <span>“Interpretable preferences via multi-objective reward modeling and mixture-of-experts,”</span> <em>arXiv preprint arXiv:2406.12845</em>, 2024.</div>
</div>
<div id="ref-wang2024helpsteer2" class="csl-entry" role="listitem">
<div class="csl-left-margin">[160] </div><div class="csl-right-inline">Z. Wang <em>et al.</em>, <span>“HelpSteer2: Open-source dataset for training top-performing reward models,”</span> <em>arXiv preprint arXiv:2406.08673</em>, 2024.</div>
</div>
<div id="ref-park2024offsetbias" class="csl-entry" role="listitem">
<div class="csl-left-margin">[161] </div><div class="csl-right-inline">J. Park, S. Jwa, M. Ren, D. Kim, and S. Choi, <span>“Offsetbias: Leveraging debiased data for tuning evaluators,”</span> <em>arXiv preprint arXiv:2407.06551</em>, 2024.</div>
</div>
<div id="ref-jaques2017sequence" class="csl-entry" role="listitem">
<div class="csl-left-margin">[162] </div><div class="csl-right-inline">N. Jaques, S. Gu, D. Bahdanau, J. M. Hernández-Lobato, R. E. Turner, and D. Eck, <span>“Sequence tutor: Conservative fine-tuning of sequence generation models with kl-control,”</span> in <em>International conference on machine learning</em>, PMLR, 2017, pp. 1645–1654.</div>
</div>
<div id="ref-jaques2020human" class="csl-entry" role="listitem">
<div class="csl-left-margin">[163] </div><div class="csl-right-inline">N. Jaques <em>et al.</em>, <span>“Human-centric dialog training via offline reinforcement learning,”</span> <em>arXiv preprint arXiv:2010.05848</em>, 2020.</div>
</div>
<div id="ref-schulman2016klapprox" class="csl-entry" role="listitem">
<div class="csl-left-margin">[164] </div><div class="csl-right-inline">J. Schulman, <span>“Approximating KL-divergence.”</span> <a href="http://joschu.net/blog/kl-approx.html" class="uri">http://joschu.net/blog/kl-approx.html</a>, 2016.</div>
</div>
<div id="ref-pang2024iterative" class="csl-entry" role="listitem">
<div class="csl-left-margin">[165] </div><div class="csl-right-inline">R. Y. Pang, W. Yuan, K. Cho, H. He, S. Sukhbaatar, and J. Weston, <span>“Iterative reasoning preference optimization,”</span> <em>arXiv preprint arXiv:2404.19733</em>, 2024.</div>
</div>
<div id="ref-gao2024rebel" class="csl-entry" role="listitem">
<div class="csl-left-margin">[166] </div><div class="csl-right-inline">Z. Gao <em>et al.</em>, <span>“Rebel: Reinforcement learning via regressing relative rewards,”</span> <em>arXiv preprint arXiv:2404.16767</em>, 2024.</div>
</div>
<div id="ref-brown2020language" class="csl-entry" role="listitem">
<div class="csl-left-margin">[167] </div><div class="csl-right-inline"><span class="nocase">T. Brown <em>et al.</em></span>, <span>“Language models are few-shot learners,”</span> <em>Advances in neural information processing systems</em>, vol. 33, pp. 1877–1901, 2020.</div>
</div>
<div id="ref-raffel2020exploring" class="csl-entry" role="listitem">
<div class="csl-left-margin">[168] </div><div class="csl-right-inline">C. Raffel <em>et al.</em>, <span>“Exploring the limits of transfer learning with a unified text-to-text transformer,”</span> <em>Journal of machine learning research</em>, vol. 21, no. 140, pp. 1–67, 2020.</div>
</div>
<div id="ref-wei2021finetuned" class="csl-entry" role="listitem">
<div class="csl-left-margin">[169] </div><div class="csl-right-inline">J. Wei <em>et al.</em>, <span>“Finetuned language models are zero-shot learners,”</span> in <em>International conference on learning representations</em>, 2022. Available: <a href="https://openreview.net/forum?id=gEZrGCozdqR">https://openreview.net/forum?id=gEZrGCozdqR</a></div>
</div>
<div id="ref-sanh2021multitask" class="csl-entry" role="listitem">
<div class="csl-left-margin">[170] </div><div class="csl-right-inline">V. Sanh <em>et al.</em>, <span>“Multitask prompted training enables zero-shot task generalization,”</span> in <em>International conference on learning representations</em>, 2022. Available: <a href="https://openreview.net/forum?id=9Vrb9D0WI4">https://openreview.net/forum?id=9Vrb9D0WI4</a></div>
</div>
<div id="ref-mishra2021cross" class="csl-entry" role="listitem">
<div class="csl-left-margin">[171] </div><div class="csl-right-inline">S. Mishra, D. Khashabi, C. Baral, and H. Hajishirzi, <span>“Cross-task generalization via natural language crowdsourcing instructions,”</span> in <em>Proceedings of the 60th annual meeting of the association for computational linguistics (volume 1: Long papers)</em>, Association for Computational Linguistics, May 2022, pp. 3470–3487. doi: <a href="https://doi.org/10.18653/v1/2022.acl-long.244">10.18653/v1/2022.acl-long.244</a>.</div>
</div>
<div id="ref-wallace2024instruction" class="csl-entry" role="listitem">
<div class="csl-left-margin">[172] </div><div class="csl-right-inline">E. Wallace, K. Xiao, R. Leike, L. Weng, J. Heidecke, and A. Beutel, <span>“The instruction hierarchy: Training llms to prioritize privileged instructions,”</span> <em>arXiv preprint arXiv:2404.13208</em>, 2024.</div>
</div>
<div id="ref-dettmers2023qlora" class="csl-entry" role="listitem">
<div class="csl-left-margin">[173] </div><div class="csl-right-inline">T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, <span>“Qlora: Efficient finetuning of quantized llms,”</span> <em>Advances in neural information processing systems</em>, vol. 36, pp. 10088–10115, 2023.</div>
</div>
<div id="ref-no_robots" class="csl-entry" role="listitem">
<div class="csl-left-margin">[174] </div><div class="csl-right-inline">N. Rajani, L. Tunstall, E. Beeching, N. Lambert, A. M. Rush, and T. Wolf, <span>“No robots,”</span> <em>Hugging Face repository</em>. <a href="https://huggingface.co/datasets/HuggingFaceH4/no_robots" class="uri">https://huggingface.co/datasets/HuggingFaceH4/no_robots</a>; Hugging Face, 2023.</div>
</div>
<div id="ref-gilks1992adaptive" class="csl-entry" role="listitem">
<div class="csl-left-margin">[175] </div><div class="csl-right-inline">W. R. Gilks and P. Wild, <span>“Adaptive rejection sampling for gibbs sampling,”</span> <em>Journal of the Royal Statistical Society: Series C (Applied Statistics)</em>, vol. 41, no. 2, pp. 337–348, 1992.</div>
</div>
<div id="ref-dong2023raft" class="csl-entry" role="listitem">
<div class="csl-left-margin">[176] </div><div class="csl-right-inline">H. Dong <em>et al.</em>, <span>“RAFT: Reward rAnked FineTuning for generative foundation model alignment.”</span> 2023. Available: <a href="https://arxiv.org/abs/2304.06767">https://arxiv.org/abs/2304.06767</a></div>
</div>
<div id="ref-liu2023statistical" class="csl-entry" role="listitem">
<div class="csl-left-margin">[177] </div><div class="csl-right-inline">T. Liu <em>et al.</em>, <span>“Statistical rejection sampling improves preference optimization.”</span> 2023. Available: <a href="https://arxiv.org/abs/2309.06657">https://arxiv.org/abs/2309.06657</a></div>
</div>
<div id="ref-ahmadian2024back" class="csl-entry" role="listitem">
<div class="csl-left-margin">[178] </div><div class="csl-right-inline">A. Ahmadian <em>et al.</em>, <span>“Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms,”</span> <em>arXiv preprint arXiv:2402.14740</em>, 2024.</div>
</div>
<div id="ref-schulman2015high" class="csl-entry" role="listitem">
<div class="csl-left-margin">[179] </div><div class="csl-right-inline">J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, <span>“High-dimensional continuous control using generalized advantage estimation,”</span> in <em>Proceedings of the international conference on learning representations (ICLR)</em>, 2016.</div>
</div>
<div id="ref-williams1992simple" class="csl-entry" role="listitem">
<div class="csl-left-margin">[180] </div><div class="csl-right-inline">R. J. Williams, <span>“Simple statistical gradient-following algorithms for connectionist reinforcement learning,”</span> <em>Machine learning</em>, vol. 8, pp. 229–256, 1992.</div>
</div>
<div id="ref-huang2024putting" class="csl-entry" role="listitem">
<div class="csl-left-margin">[181] </div><div class="csl-right-inline">S. C. Huang, A. Ahmadian, and C. F. AI, <span>“Putting RL back in RLHF.”</span> <a href="https://huggingface.co/blog/putting_rl_back_in_rlhf_with_rloo" class="uri">https://huggingface.co/blog/putting_rl_back_in_rlhf_with_rloo</a>, 2024.</div>
</div>
<div id="ref-kool2019buy" class="csl-entry" role="listitem">
<div class="csl-left-margin">[182] </div><div class="csl-right-inline">W. Kool, H. van Hoof, and M. Welling, <span>“Buy 4 reinforce samples, get a baseline for free!”</span> 2019.</div>
</div>
<div id="ref-schulman2017proximal" class="csl-entry" role="listitem">
<div class="csl-left-margin">[183] </div><div class="csl-right-inline">J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, <span>“Proximal policy optimization algorithms,”</span> <em>arXiv preprint arXiv:1707.06347</em>, 2017.</div>
</div>
<div id="ref-berner2019dota" class="csl-entry" role="listitem">
<div class="csl-left-margin">[184] </div><div class="csl-right-inline"><span class="nocase">C. Berner <em>et al.</em></span>, <span>“Dota 2 with large scale deep reinforcement learning,”</span> <em>arXiv preprint arXiv:1912.06680</em>, 2019.</div>
</div>
<div id="ref-liu2025understanding" class="csl-entry" role="listitem">
<div class="csl-left-margin">[185] </div><div class="csl-right-inline">Z. Liu <em>et al.</em>, <span>“Understanding R1-zero-like training: A critical perspective,”</span> <em>arXiv preprint arXiv:2503.20783</em>, Mar. 2025, Available: <a href="https://arxiv.org/abs/2503.20783">https://arxiv.org/abs/2503.20783</a></div>
</div>
<div id="ref-nocedal2006numerical" class="csl-entry" role="listitem">
<div class="csl-left-margin">[186] </div><div class="csl-right-inline">J. Nocedal and S. J. Wright, <em>Numerical optimization</em>. Springer, 2006.</div>
</div>
<div id="ref-schulman2015trust" class="csl-entry" role="listitem">
<div class="csl-left-margin">[187] </div><div class="csl-right-inline">J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, <span>“Trust region policy optimization,”</span> in <em>International conference on machine learning</em>, PMLR, 2015, pp. 1889–1897.</div>
</div>
<div id="ref-shao2024deepseekmath" class="csl-entry" role="listitem">
<div class="csl-left-margin">[188] </div><div class="csl-right-inline"><span class="nocase">Z. Shao <em>et al.</em></span>, <span>“Deepseekmath: Pushing the limits of mathematical reasoning in open language models,”</span> <em>arXiv preprint arXiv:2402.03300</em>, 2024.</div>
</div>
<div id="ref-liu2024deepseek" class="csl-entry" role="listitem">
<div class="csl-left-margin">[189] </div><div class="csl-right-inline"><span class="nocase">A. Liu <em>et al.</em></span>, <span>“Deepseek-v3 technical report,”</span> <em>arXiv preprint arXiv:2412.19437</em>, 2024.</div>
</div>
<div id="ref-ivison2024unpacking" class="csl-entry" role="listitem">
<div class="csl-left-margin">[190] </div><div class="csl-right-inline">H. Ivison <em>et al.</em>, <span>“Unpacking DPO and PPO: Disentangling best practices for learning from preference feedback,”</span> <em>arXiv preprint arXiv:2406.09279</em>, 2024.</div>
</div>
<div id="ref-huang2024n" class="csl-entry" role="listitem">
<div class="csl-left-margin">[191] </div><div class="csl-right-inline">S. Huang, M. Noukhovitch, A. Hosseini, K. Rasul, W. Wang, and L. Tunstall, <span>“The n+ implementation details of <span>RLHF</span> with <span>PPO</span>: A case study on <span>TL</span>;<span>DR</span> summarization,”</span> in <em>First conference on language modeling</em>, 2024. Available: <a href="https://openreview.net/forum?id=kHO2ZTa8e3">https://openreview.net/forum?id=kHO2ZTa8e3</a></div>
</div>
<div id="ref-weng2018PG" class="csl-entry" role="listitem">
<div class="csl-left-margin">[192] </div><div class="csl-right-inline">L. Weng, <span>“Policy gradient algorithms,”</span> <em>lilianweng.github.io</em>, 2018, Available: <a href="https://lilianweng.github.io/posts/2018-04-08-policy-gradient/">https://lilianweng.github.io/posts/2018-04-08-policy-gradient/</a></div>
</div>
<div id="ref-yu2025dapo" class="csl-entry" role="listitem">
<div class="csl-left-margin">[193] </div><div class="csl-right-inline">Q. Yu <em>et al.</em>, <span>“DAPO: An open-source LLM reinforcement learning system at scale.”</span> 2025.</div>
</div>
<div id="ref-baheti2023leftover" class="csl-entry" role="listitem">
<div class="csl-left-margin">[194] </div><div class="csl-right-inline">A. Baheti, X. Lu, F. Brahman, R. L. Bras, M. Sap, and M. Riedl, <span>“Leftover lunch: Advantage-based offline reinforcement learning for language models,”</span> <em>arXiv preprint arXiv:2305.14718</em>, 2023.</div>
</div>
<div id="ref-noukhovitch2024asynchronous" class="csl-entry" role="listitem">
<div class="csl-left-margin">[195] </div><div class="csl-right-inline">M. Noukhovitch, S. Huang, S. Xhonneux, A. Hosseini, R. Agarwal, and A. Courville, <span>“Asynchronous RLHF: Faster and more efficient off-policy RL for language models,”</span> <em>arXiv preprint arXiv:2410.18252</em>, 2024.</div>
</div>
<div id="ref-wu2025llamarl" class="csl-entry" role="listitem">
<div class="csl-left-margin">[196] </div><div class="csl-right-inline"><span class="nocase">B. Wu <em>et al.</em></span>, <span>“LlamaRL: A distributed asynchronous reinforcement learning framework for efficient large-scale LLM trainin,”</span> <em>arXiv preprint arXiv:2505.24034</em>, 2025.</div>
</div>
<div id="ref-fu2025areal" class="csl-entry" role="listitem">
<div class="csl-left-margin">[197] </div><div class="csl-right-inline"><span class="nocase">W. Fu <em>et al.</em></span>, <span>“AReaL: A large-scale asynchronous reinforcement learning system for language reasoning,”</span> <em>arXiv preprint arXiv:2505.24298</em>, 2025.</div>
</div>
<div id="ref-primeintellectteam2025intellect2reasoningmodeltrained" class="csl-entry" role="listitem">
<div class="csl-left-margin">[198] </div><div class="csl-right-inline">P. I. Team <em>et al.</em>, <span>“INTELLECT-2: A reasoning model trained through globally decentralized reinforcement learning.”</span> 2025. Available: <a href="https://arxiv.org/abs/2505.07291">https://arxiv.org/abs/2505.07291</a></div>
</div>
<div id="ref-roux2025tapered" class="csl-entry" role="listitem">
<div class="csl-left-margin">[199] </div><div class="csl-right-inline">N. L. Roux <em>et al.</em>, <span>“Tapered off-policy REINFORCE: Stable and efficient reinforcement learning for LLMs,”</span> <em>arXiv preprint arXiv:2503.14286</em>, 2025.</div>
</div>
<div id="ref-seita2017gae" class="csl-entry" role="listitem">
<div class="csl-left-margin">[200] </div><div class="csl-right-inline">D. Seita, <span>“Notes on the generalized advantage estimation paper.”</span> 2017. Available: <a href="https://danieltakeshi.github.io/2017/04/02/notes-on-the-generalized-advantage-estimation-paper/">https://danieltakeshi.github.io/2017/04/02/notes-on-the-generalized-advantage-estimation-paper/</a></div>
</div>
<div id="ref-wu2023pairwise" class="csl-entry" role="listitem">
<div class="csl-left-margin">[201] </div><div class="csl-right-inline">T. Wu, B. Zhu, R. Zhang, Z. Wen, K. Ramchandran, and J. Jiao, <span>“Pairwise proximal policy optimization: Harnessing relative feedback for llm alignment,”</span> <em>arXiv preprint arXiv:2310.00212</em>, 2023.</div>
</div>
<div id="ref-flet2024contrastive" class="csl-entry" role="listitem">
<div class="csl-left-margin">[202] </div><div class="csl-right-inline"><span class="nocase">Y. Flet-Berliac <em>et al.</em></span>, <span>“Contrastive policy gradient: Aligning LLMs on sequence-level scores in a supervised-friendly fashion,”</span> <em>arXiv preprint arXiv:2406.19185</em>, 2024.</div>
</div>
<div id="ref-li2023remax" class="csl-entry" role="listitem">
<div class="csl-left-margin">[203] </div><div class="csl-right-inline">Z. Li <em>et al.</em>, <span>“Remax: A simple, effective, and efficient reinforcement learning method for aligning large language models,”</span> in <em>Forty-first international conference on machine learning</em>, 2023.</div>
</div>
<div id="ref-gunter2024apple" class="csl-entry" role="listitem">
<div class="csl-left-margin">[204] </div><div class="csl-right-inline"><span class="nocase">T. Gunter <em>et al.</em></span>, <span>“Apple intelligence foundation language models,”</span> <em>arXiv preprint arXiv:2407.21075</em>, 2024.</div>
</div>
<div id="ref-team2025kimi" class="csl-entry" role="listitem">
<div class="csl-left-margin">[205] </div><div class="csl-right-inline"><span class="nocase">K. Team <em>et al.</em></span>, <span>“Kimi k1. 5: Scaling reinforcement learning with llms,”</span> <em>arXiv preprint arXiv:2501.12599</em>, 2025.</div>
</div>
<div id="ref-tomar2020mirror" class="csl-entry" role="listitem">
<div class="csl-left-margin">[206] </div><div class="csl-right-inline">M. Tomar, L. Shani, Y. Efroni, and M. Ghavamzadeh, <span>“Mirror descent policy optimization,”</span> <em>arXiv preprint arXiv:2005.09814</em>, 2020.</div>
</div>
<div id="ref-zhang2025improving" class="csl-entry" role="listitem">
<div class="csl-left-margin">[207] </div><div class="csl-right-inline">Y. Zhang <em>et al.</em>, <span>“Improving LLM general preference alignment via optimistic online mirror descent,”</span> <em>arXiv preprint arXiv:2502.16852</em>, 2025.</div>
</div>
<div id="ref-yuan2025vapo" class="csl-entry" role="listitem">
<div class="csl-left-margin">[208] </div><div class="csl-right-inline"><span class="nocase">Y. Yuan <em>et al.</em></span>, <span>“VAPO: Efficient and reliable reinforcement learning for advanced reasoning tasks,”</span> <em>arXiv preprint arXiv:2504.05118</em>, 2025.</div>
</div>
<div id="ref-yuan2025s" class="csl-entry" role="listitem">
<div class="csl-left-margin">[209] </div><div class="csl-right-inline">Y. Yuan, Y. Yue, R. Zhu, T. Fan, and L. Yan, <span>“What’s behind PPO’s collapse in long-CoT? Value optimization holds the secret,”</span> <em>arXiv preprint arXiv:2503.01491</em>, 2025.</div>
</div>
<div id="ref-zhao2023slic" class="csl-entry" role="listitem">
<div class="csl-left-margin">[210] </div><div class="csl-right-inline">Y. Zhao, R. Joshi, T. Liu, M. Khalman, M. Saleh, and P. J. Liu, <span>“Slic-hf: Sequence likelihood calibration with human feedback,”</span> <em>arXiv preprint arXiv:2305.10425</em>, 2023.</div>
</div>
<div id="ref-azar2024general" class="csl-entry" role="listitem">
<div class="csl-left-margin">[211] </div><div class="csl-right-inline">M. G. Azar <em>et al.</em>, <span>“A general theoretical paradigm to understand learning from human preferences,”</span> in <em>International conference on artificial intelligence and statistics</em>, PMLR, 2024, pp. 4447–4455.</div>
</div>
<div id="ref-amini2024direct" class="csl-entry" role="listitem">
<div class="csl-left-margin">[212] </div><div class="csl-right-inline">A. Amini, T. Vieira, and R. Cotterell, <span>“Direct preference optimization with an offset,”</span> <em>arXiv preprint arXiv:2402.10571</em>, 2024.</div>
</div>
<div id="ref-hong2024reference" class="csl-entry" role="listitem">
<div class="csl-left-margin">[213] </div><div class="csl-right-inline">J. Hong, N. Lee, and J. Thorne, <span>“Reference-free monolithic preference optimization with odds ratio,”</span> <em>arXiv e-prints</em>, pp. arXiv–2403, 2024.</div>
</div>
<div id="ref-meng2025simpo" class="csl-entry" role="listitem">
<div class="csl-left-margin">[214] </div><div class="csl-right-inline">Y. Meng, M. Xia, and D. Chen, <span>“Simpo: Simple preference optimization with a reference-free reward,”</span> <em>Advances in Neural Information Processing Systems</em>, vol. 37, pp. 124198–124235, 2025.</div>
</div>
<div id="ref-razin2024unintentional" class="csl-entry" role="listitem">
<div class="csl-left-margin">[215] </div><div class="csl-right-inline">N. Razin, S. Malladi, A. Bhaskar, D. Chen, S. Arora, and B. Hanin, <span>“Unintentional unalignment: Likelihood displacement in direct preference optimization,”</span> <em>arXiv preprint arXiv:2410.08847</em>, 2024.</div>
</div>
<div id="ref-ren2024learning" class="csl-entry" role="listitem">
<div class="csl-left-margin">[216] </div><div class="csl-right-inline">Y. Ren and D. J. Sutherland, <span>“Learning dynamics of llm finetuning,”</span> <em>arXiv preprint arXiv:2407.10490</em>, 2024.</div>
</div>
<div id="ref-xiao2024cal" class="csl-entry" role="listitem">
<div class="csl-left-margin">[217] </div><div class="csl-right-inline">T. Xiao, Y. Yuan, H. Zhu, M. Li, and V. G. Honavar, <span>“Cal-dpo: Calibrated direct preference optimization for language model alignment,”</span> <em>arXiv preprint arXiv:2412.14516</em>, 2024.</div>
</div>
<div id="ref-gupta2025alphapo" class="csl-entry" role="listitem">
<div class="csl-left-margin">[218] </div><div class="csl-right-inline"><span class="nocase">A. Gupta <em>et al.</em></span>, <span>“AlphaPO–reward shape matters for LLM alignment,”</span> <em>arXiv preprint arXiv:2501.03884</em>, 2025.</div>
</div>
<div id="ref-guo2024direct" class="csl-entry" role="listitem">
<div class="csl-left-margin">[219] </div><div class="csl-right-inline"><span class="nocase">S. Guo <em>et al.</em></span>, <span>“Direct language model alignment from online ai feedback,”</span> <em>arXiv preprint arXiv:2402.04792</em>, 2024.</div>
</div>
<div id="ref-singhal2024d2po" class="csl-entry" role="listitem">
<div class="csl-left-margin">[220] </div><div class="csl-right-inline">P. Singhal, N. Lambert, S. Niekum, T. Goyal, and G. Durrett, <span>“D2po: Discriminator-guided dpo with response evaluation models,”</span> <em>arXiv preprint arXiv:2405.01511</em>, 2024.</div>
</div>
<div id="ref-rosset2024direct" class="csl-entry" role="listitem">
<div class="csl-left-margin">[221] </div><div class="csl-right-inline">C. Rosset, C.-A. Cheng, A. Mitra, M. Santacroce, A. Awadallah, and T. Xie, <span>“Direct nash optimization: Teaching language models to self-improve with general preferences,”</span> <em>arXiv preprint arXiv:2404.03715</em>, 2024.</div>
</div>
<div id="ref-jung2024binary" class="csl-entry" role="listitem">
<div class="csl-left-margin">[222] </div><div class="csl-right-inline">S. Jung, G. Han, D. W. Nam, and K.-W. On, <span>“Binary classifier optimization for large language model alignment,”</span> <em>arXiv preprint arXiv:2404.04656</em>, 2024.</div>
</div>
<div id="ref-zhao2024rainbowpo" class="csl-entry" role="listitem">
<div class="csl-left-margin">[223] </div><div class="csl-right-inline">H. Zhao <em>et al.</em>, <span>“Rainbowpo: A unified framework for combining improvements in preference optimization,”</span> <em>arXiv preprint arXiv:2410.04203</em>, 2024.</div>
</div>
<div id="ref-gorbatovski2025differences" class="csl-entry" role="listitem">
<div class="csl-left-margin">[224] </div><div class="csl-right-inline">A. Gorbatovski, B. Shaposhnikov, V. Sinii, A. Malakhov, and D. Gavrilov, <span>“The differences between direct alignment algorithms are a blur,”</span> <em>arXiv preprint arXiv:2502.01237</em>, 2025.</div>
</div>
<div id="ref-xu2024dpo" class="csl-entry" role="listitem">
<div class="csl-left-margin">[225] </div><div class="csl-right-inline">S. Xu <em>et al.</em>, <span>“Is dpo superior to ppo for llm alignment? A comprehensive study,”</span> <em>arXiv preprint arXiv:2404.10719</em>, 2024.</div>
</div>
<div id="ref-tajwar2024preference" class="csl-entry" role="listitem">
<div class="csl-left-margin">[226] </div><div class="csl-right-inline">F. Tajwar <em>et al.</em>, <span>“Preference fine-tuning of llms should leverage suboptimal, on-policy data,”</span> <em>arXiv preprint arXiv:2404.14367</em>, 2024.</div>
</div>
<div id="ref-lee2023rlaif" class="csl-entry" role="listitem">
<div class="csl-left-margin">[227] </div><div class="csl-right-inline">H. Lee <em>et al.</em>, <span>“Rlaif: Scaling reinforcement learning from human feedback with ai feedback,”</span> 2023.</div>
</div>
<div id="ref-sharma2024critical" class="csl-entry" role="listitem">
<div class="csl-left-margin">[228] </div><div class="csl-right-inline">A. Sharma, S. Keh, E. Mitchell, C. Finn, K. Arora, and T. Kollar, <span>“A critical evaluation of AI feedback for aligning large language models.”</span> 2024. Available: <a href="https://arxiv.org/abs/2402.12366">https://arxiv.org/abs/2402.12366</a></div>
</div>
<div id="ref-castricato2024suppressing" class="csl-entry" role="listitem">
<div class="csl-left-margin">[229] </div><div class="csl-right-inline">L. Castricato, N. Lile, S. Anand, H. Schoelkopf, S. Verma, and S. Biderman, <span>“Suppressing pink elephants with direct principle feedback.”</span> 2024. Available: <a href="https://arxiv.org/abs/2402.07896">https://arxiv.org/abs/2402.07896</a></div>
</div>
<div id="ref-miranda2024hybrid" class="csl-entry" role="listitem">
<div class="csl-left-margin">[230] </div><div class="csl-right-inline">L. J. V. Miranda <em>et al.</em>, <span>“Hybrid preferences: Learning to route instances for human vs. <span>AI</span> feedback,”</span> pp. 7162–7200, July 2025, doi: <a href="https://doi.org/10.18653/v1/2025.acl-long.355">10.18653/v1/2025.acl-long.355</a>.</div>
</div>
<div id="ref-wang2023large" class="csl-entry" role="listitem">
<div class="csl-left-margin">[231] </div><div class="csl-right-inline">P. Wang <em>et al.</em>, <span>“Large language models are not fair evaluators,”</span> <em>arXiv preprint arXiv:2305.17926</em>, 2023.</div>
</div>
<div id="ref-panickssery2024llm" class="csl-entry" role="listitem">
<div class="csl-left-margin">[232] </div><div class="csl-right-inline">A. Panickssery, S. Bowman, and S. Feng, <span>“Llm evaluators recognize and favor their own generations,”</span> <em>Advances in Neural Information Processing Systems</em>, 2024.</div>
</div>
<div id="ref-wang2023shepherd" class="csl-entry" role="listitem">
<div class="csl-left-margin">[233] </div><div class="csl-right-inline">T. Wang <em>et al.</em>, <span>“Shepherd: A critic for language model generation,”</span> <em>arXiv preprint arXiv:2308.04592</em>, 2023.</div>
</div>
<div id="ref-ke2023critiquellm" class="csl-entry" role="listitem">
<div class="csl-left-margin">[234] </div><div class="csl-right-inline"><span class="nocase">P. Ke <em>et al.</em></span>, <span>“CritiqueLLM: Towards an informative critique generation model for evaluation of large language model generation,”</span> <em>arXiv preprint arXiv:2311.18702</em>, 2023.</div>
</div>
<div id="ref-li2023generative" class="csl-entry" role="listitem">
<div class="csl-left-margin">[235] </div><div class="csl-right-inline">J. Li, S. Sun, W. Yuan, R.-Z. Fan, H. Zhao, and P. Liu, <span>“Generative judge for evaluating alignment,”</span> <em>arXiv preprint arXiv:2310.05470</em>, 2023.</div>
</div>
<div id="ref-kim2024prometheus" class="csl-entry" role="listitem">
<div class="csl-left-margin">[236] </div><div class="csl-right-inline">S. Kim <em>et al.</em>, <span>“Prometheus 2: An open source language model specialized in evaluating other language models,”</span> <em>arXiv preprint arXiv:2405.01535</em>, 2024.</div>
</div>
<div id="ref-lee2024prometheus" class="csl-entry" role="listitem">
<div class="csl-left-margin">[237] </div><div class="csl-right-inline">S. Lee, S. Kim, S. Park, G. Kim, and M. Seo, <span>“Prometheus-vision: Vision-language model as a judge for fine-grained evaluation,”</span> in <em>Findings of the association for computational linguistics ACL 2024</em>, 2024, pp. 11286–11315.</div>
</div>
<div id="ref-brown2024large" class="csl-entry" role="listitem">
<div class="csl-left-margin">[238] </div><div class="csl-right-inline">B. Brown <em>et al.</em>, <span>“Large language monkeys: Scaling inference compute with repeated sampling,”</span> <em>arXiv preprint arXiv:2407.21787</em>, 2024.</div>
</div>
<div id="ref-zhao2025sample" class="csl-entry" role="listitem">
<div class="csl-left-margin">[239] </div><div class="csl-right-inline">E. Zhao, P. Awasthi, and S. Gollapudi, <span>“Sample, scrutinize and scale: Effective inference-time search by scaling verification,”</span> <em>arXiv preprint arXiv:2502.01839</em>, 2025.</div>
</div>
<div id="ref-kalra2025verdict" class="csl-entry" role="listitem">
<div class="csl-left-margin">[240] </div><div class="csl-right-inline">N. Kalra and L. Tang, <span>“Verdict: A library for scaling judge-time compute,”</span> <em>arXiv preprint arXiv:2502.18018</em>, 2025.</div>
</div>
<div id="ref-madaan2023self" class="csl-entry" role="listitem">
<div class="csl-left-margin">[241] </div><div class="csl-right-inline"><span class="nocase">A. Madaan <em>et al.</em></span>, <span>“Self-refine: Iterative refinement with self-feedback,”</span> <em>Advances in Neural Information Processing Systems</em>, 2023.</div>
</div>
<div id="ref-pace2024west" class="csl-entry" role="listitem">
<div class="csl-left-margin">[242] </div><div class="csl-right-inline">A. Pace, J. Mallinson, E. Malmi, S. Krause, and A. Severyn, <span>“West-of-n: Synthetic preference generation for improved reward modeling,”</span> <em>arXiv preprint arXiv:2401.12086</em>, 2024.</div>
</div>
<div id="ref-wu2024meta" class="csl-entry" role="listitem">
<div class="csl-left-margin">[243] </div><div class="csl-right-inline">T. Wu <em>et al.</em>, <span>“Meta-rewarding language models: Self-improving alignment with llm-as-a-meta-judge,”</span> <em>arXiv preprint arXiv:2407.19594</em>, 2024.</div>
</div>
<div id="ref-guan2024deliberative" class="csl-entry" role="listitem">
<div class="csl-left-margin">[244] </div><div class="csl-right-inline"><span class="nocase">M. Y. Guan <em>et al.</em></span>, <span>“Deliberative alignment: Reasoning enables safer language models,”</span> <em>arXiv preprint arXiv:2412.16339</em>, 2024.</div>
</div>
<div id="ref-Anthropic2023ClaudesConstitution" class="csl-entry" role="listitem">
<div class="csl-left-margin">[245] </div><div class="csl-right-inline">Anthropic, <span>“Claude’s constitution.”</span> Accessed: Feb. 07, 2024. [Online]. Available: <a href="https://www.anthropic.com/news/claudes-constitution">https://www.anthropic.com/news/claudes-constitution</a></div>
</div>
<div id="ref-ganguli2023" class="csl-entry" role="listitem">
<div class="csl-left-margin">[246] </div><div class="csl-right-inline"><span class="nocase">D. Ganguli <em>et al.</em></span>, <span>“Collective constitutional <span>AI</span>: <span>A</span>ligning a language model with public input.”</span> Anthropic, 2023.</div>
</div>
<div id="ref-Huang2024cai" class="csl-entry" role="listitem">
<div class="csl-left-margin">[247] </div><div class="csl-right-inline">S. Huang <em>et al.</em>, <span>“Constitutional AI recipe,”</span> <em>Hugging Face Blog</em>, 2024.</div>
</div>
<div id="ref-lambert2024self" class="csl-entry" role="listitem">
<div class="csl-left-margin">[248] </div><div class="csl-right-inline">N. Lambert, H. Schoelkopf, A. Gokaslan, L. Soldaini, V. Pyatkin, and L. Castricato, <span>“Self-directed synthetic dialogues and revisions technical report,”</span> <em>arXiv preprint arXiv:2407.18421</em>, 2024.</div>
</div>
<div id="ref-sun2023principledriven" class="csl-entry" role="listitem">
<div class="csl-left-margin">[249] </div><div class="csl-right-inline">Z. Sun <em>et al.</em>, <span>“Principle-driven self-alignment of language models from scratch with minimal human supervision,”</span> in <em>Thirty-seventh conference on neural information processing systems</em>, 2023. Available: <a href="https://openreview.net/forum?id=p40XRfBX96">https://openreview.net/forum?id=p40XRfBX96</a></div>
</div>
<div id="ref-sun2024salmon" class="csl-entry" role="listitem">
<div class="csl-left-margin">[250] </div><div class="csl-right-inline">Z. Sun <em>et al.</em>, <span>“<span>SALMON</span>: Self-alignment with principle-following reward models,”</span> in <em>The twelfth international conference on learning representations</em>, 2024. Available: <a href="https://openreview.net/forum?id=xJbsmB8UMx">https://openreview.net/forum?id=xJbsmB8UMx</a></div>
</div>
<div id="ref-liu2025inference" class="csl-entry" role="listitem">
<div class="csl-left-margin">[251] </div><div class="csl-right-inline">Z. Liu <em>et al.</em>, <span>“Inference-time scaling for generalist reward modeling,”</span> <em>arXiv preprint arXiv:2504.02495</em>, 2025.</div>
</div>
<div id="ref-franken2024self" class="csl-entry" role="listitem">
<div class="csl-left-margin">[252] </div><div class="csl-right-inline">J.-P. Fränken, E. Zelikman, R. Rafailov, K. Gandhi, T. Gerstenberg, and N. Goodman, <span>“Self-supervised alignment with mutual information: Learning to follow principles without preference labels,”</span> <em>Advances in Neural Information Processing Systems</em>, 2024.</div>
</div>
<div id="ref-irpan2018deep" class="csl-entry" role="listitem">
<div class="csl-left-margin">[253] </div><div class="csl-right-inline">A. Irpan, <span>“Deep reinforcement learning doesn’t work yet.”</span> 2018. Available: <a href="https://www.alexirpan.com/2018/02/14/rl-hard.html">https://www.alexirpan.com/2018/02/14/rl-hard.html</a></div>
</div>
<div id="ref-henderson2018deep" class="csl-entry" role="listitem">
<div class="csl-left-margin">[254] </div><div class="csl-right-inline">P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and D. Meger, <span>“Deep reinforcement learning that matters,”</span> in <em>Proceedings of the AAAI conference on artificial intelligence</em>, 2018. Available: <a href="https://ojs.aaai.org/index.php/AAAI/article/view/11694">https://ojs.aaai.org/index.php/AAAI/article/view/11694</a></div>
</div>
<div id="ref-sheng2024hybridflow" class="csl-entry" role="listitem">
<div class="csl-left-margin">[255] </div><div class="csl-right-inline">G. Sheng <em>et al.</em>, <span>“HybridFlow: A flexible and efficient RLHF framework,”</span> <em>arXiv preprint arXiv: 2409.19256</em>, 2024.</div>
</div>
<div id="ref-hu2024openrlhf" class="csl-entry" role="listitem">
<div class="csl-left-margin">[256] </div><div class="csl-right-inline">J. Hu <em>et al.</em>, <span>“OpenRLHF: An easy-to-use, scalable and high-performance RLHF framework,”</span> <em>arXiv preprint arXiv:2405.11143</em>, 2024.</div>
</div>
<div id="ref-liu2023don" class="csl-entry" role="listitem">
<div class="csl-left-margin">[257] </div><div class="csl-right-inline">J. Liu, A. Cohen, R. Pasunuru, Y. Choi, H. Hajishirzi, and A. Celikyilmaz, <span>“Don’t throw away your value model! Generating more preferable text with value-guided monte-carlo tree search decoding,”</span> <em>arXiv preprint arXiv:2309.15028</em>, 2023.</div>
</div>
<div id="ref-muennighoff2025s1" class="csl-entry" role="listitem">
<div class="csl-left-margin">[258] </div><div class="csl-right-inline">N. Muennighoff <em>et al.</em>, <span>“s1: Simple test-time scaling,”</span> <em>arXiv preprint arXiv:2501.19393</em>, 2025.</div>
</div>
<div id="ref-chen2024more" class="csl-entry" role="listitem">
<div class="csl-left-margin">[259] </div><div class="csl-right-inline">L. Chen <em>et al.</em>, <span>“Are more llm calls all you need? Towards scaling laws of compound inference systems,”</span> <em>arXiv preprint arXiv:2403.02419</em>, 2024.</div>
</div>
<div id="ref-zelikman2022star" class="csl-entry" role="listitem">
<div class="csl-left-margin">[260] </div><div class="csl-right-inline">E. Zelikman, Y. Wu, J. Mu, and N. Goodman, <span>“<span>ST</span>aR: Bootstrapping reasoning with reasoning,”</span> in <em>Advances in neural information processing systems</em>, A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, Eds., 2022. Available: <a href="https://openreview.net/forum?id=_3ELRdg2sgI">https://openreview.net/forum?id=_3ELRdg2sgI</a></div>
</div>
<div id="ref-Zelikman2024QuietSTaRLM" class="csl-entry" role="listitem">
<div class="csl-left-margin">[261] </div><div class="csl-right-inline">E. Zelikman, G. Harik, Y. Shao, V. Jayasiri, N. Haber, and N. D. Goodman, <span>“Quiet-STaR: Language models can teach themselves to think before speaking,”</span> <em>COLM</em>, vol. abs/2403.09629, 2024.</div>
</div>
<div id="ref-hoffman2023training" class="csl-entry" role="listitem">
<div class="csl-left-margin">[262] </div><div class="csl-right-inline"><span class="nocase">M. D. Hoffman <em>et al.</em></span>, <span>“Training chain-of-thought via latent-variable inference,”</span> in <em>Thirty-seventh conference on neural information processing systems</em>, 2023. Available: <a href="https://openreview.net/forum?id=a147pIS2Co">https://openreview.net/forum?id=a147pIS2Co</a></div>
</div>
<div id="ref-VinePPO" class="csl-entry" role="listitem">
<div class="csl-left-margin">[263] </div><div class="csl-right-inline">A. Kazemnejad <em>et al.</em>, <span>“VinePPO: Unlocking RL potential for LLM reasoning through refined credit assignment.”</span> 2024. Available: <a href="https://arxiv.org/abs/2410.01679">https://arxiv.org/abs/2410.01679</a></div>
</div>
<div id="ref-gehring2024rlefgroundingcodellms" class="csl-entry" role="listitem">
<div class="csl-left-margin">[264] </div><div class="csl-right-inline">J. Gehring, K. Zheng, J. Copet, V. Mella, T. Cohen, and G. Synnaeve, <span>“RLEF: Grounding code LLMs in execution feedback with reinforcement learning.”</span> 2024. Available: <a href="https://arxiv.org/abs/2410.02089">https://arxiv.org/abs/2410.02089</a></div>
</div>
<div id="ref-xudpoppo" class="csl-entry" role="listitem">
<div class="csl-left-margin">[265] </div><div class="csl-right-inline">S. Xu <em>et al.</em>, <span>“Is DPO superior to PPO for LLM alignment? A comprehensive study,”</span> in <em>ICML</em>, 2024. Available: <a href="https://openreview.net/forum?id=6XH8R7YrSk">https://openreview.net/forum?id=6XH8R7YrSk</a></div>
</div>
<div id="ref-amit2024models" class="csl-entry" role="listitem">
<div class="csl-left-margin">[266] </div><div class="csl-right-inline">N. Amit, S. Goldwasser, O. Paradise, and G. Rothblum, <span>“Models that prove their own correctness,”</span> <em>arXiv preprint arXiv:2405.15722</em>, 2024.</div>
</div>
<div id="ref-hu2025openreasonerzero" class="csl-entry" role="listitem">
<div class="csl-left-margin">[267] </div><div class="csl-right-inline">J. Hu, Y. Zhang, Q. Han, D. Jiang, X. Zhang, and H. Shum, <span>“Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model,”</span> <em>arXiv preprint arXiv:2503.24290</em>, 2025.</div>
</div>
<div id="ref-abdin2025phi4" class="csl-entry" role="listitem">
<div class="csl-left-margin">[268] </div><div class="csl-right-inline"><span class="nocase">M. Abdin, S. Agarwal, A. Awadallah, <em>et al.</em></span>, <span>“Phi-4-reasoning technical report,”</span> <em>arXiv preprint arXiv:2504.21318</em>, 2025.</div>
</div>
<div id="ref-bercovich2025llamanemotron" class="csl-entry" role="listitem">
<div class="csl-left-margin">[269] </div><div class="csl-right-inline"><span class="nocase">A. Bercovich, I. Levy, I. Golan, <em>et al.</em></span>, <span>“Llama‑nemotron: Efficient reasoning models,”</span> <em>arXiv preprint arXiv:2505.00949</em>, 2025.</div>
</div>
<div id="ref-liu2025hunyuan" class="csl-entry" role="listitem">
<div class="csl-left-margin">[270] </div><div class="csl-right-inline"><span class="nocase">A. Liu, B. Zhou, C. Xu, <em>et al.</em></span>, <span>“Hunyuan‑TurboS: Advancing large language models through mamba‑transformer synergy and adaptive chain‑of‑thought,”</span> <em>arXiv preprint arXiv:2505.15431</em>, 2025.</div>
</div>
<div id="ref-he2025skyworkor1" class="csl-entry" role="listitem">
<div class="csl-left-margin">[271] </div><div class="csl-right-inline"><span class="nocase">J. He, J. Liu, C. Y. Liu, <em>et al.</em></span>, <span>“Skywork open reasoner 1 technical report,”</span> <em>arXiv preprint arXiv:2505.22312</em>, 2025.</div>
</div>
<div id="ref-coreteam2025mimovltechnicalreport" class="csl-entry" role="listitem">
<div class="csl-left-margin">[272] </div><div class="csl-right-inline">C. Team <em>et al.</em>, <span>“MiMo-VL technical report.”</span> 2025. Available: <a href="https://arxiv.org/abs/2506.03569">https://arxiv.org/abs/2506.03569</a></div>
</div>
<div id="ref-guha2025openthoughts" class="csl-entry" role="listitem">
<div class="csl-left-margin">[273] </div><div class="csl-right-inline"><span class="nocase">E. Guha, R. Marten, S. Keh, <em>et al.</em></span>, <span>“OpenThoughts: Data recipes for reasoning models,”</span> <em>arXiv preprint arXiv:2506.04178</em>, 2025.</div>
</div>
<div id="ref-mistral2025magistral" class="csl-entry" role="listitem">
<div class="csl-left-margin">[274] </div><div class="csl-right-inline">Mistral AI, <span>“Magistral: Scaling reinforcement learning for reasoning in large language models,”</span> Mistral AI, 2025. Available: <a href="https://mistral.ai/static/research/magistral.pdf">https://mistral.ai/static/research/magistral.pdf</a></div>
</div>
<div id="ref-minimax2025minimaxm1scalingtesttimecompute" class="csl-entry" role="listitem">
<div class="csl-left-margin">[275] </div><div class="csl-right-inline">MiniMax, <span>“MiniMax-M1: Scaling test-time compute efficiently with lightning attention.”</span> 2025. doi: <a href="https://doi.org/10.48550/arXiv.2506.13585">10.48550/arXiv.2506.13585</a>.</div>
</div>
<div id="ref-kimiteam2025kimik2" class="csl-entry" role="listitem">
<div class="csl-left-margin">[276] </div><div class="csl-right-inline">K. Team <em>et al.</em>, <span>“Kimi K2: Open agentic intelligence.”</span> 2025. Available: <a href="https://arxiv.org/abs/2507.20534">https://arxiv.org/abs/2507.20534</a></div>
</div>
<div id="ref-zeng2025glm45" class="csl-entry" role="listitem">
<div class="csl-left-margin">[277] </div><div class="csl-right-inline">A. Zeng <em>et al.</em>, <span>“GLM-4.5: Agentic, reasoning, and coding (ARC) foundation models.”</span> 2025. doi: <a href="https://doi.org/10.48550/arXiv.2508.06471">10.48550/arXiv.2508.06471</a>.</div>
</div>
<div id="ref-nvidia2025nemotronnano2" class="csl-entry" role="listitem">
<div class="csl-left-margin">[278] </div><div class="csl-right-inline">NVIDIA, <span>“NVIDIA nemotron nano 2: An accurate and efficient hybrid mamba-transformer reasoning model.”</span> 2025. Available: <a href="https://arxiv.org/abs/2508.14444">https://arxiv.org/abs/2508.14444</a></div>
</div>
<div id="ref-llm3602025k2think" class="csl-entry" role="listitem">
<div class="csl-left-margin">[279] </div><div class="csl-right-inline">Z. Cheng <em>et al.</em>, <span>“K2-think: A parameter-efficient reasoning system.”</span> 2025. Available: <a href="https://arxiv.org/abs/2509.07604">https://arxiv.org/abs/2509.07604</a></div>
</div>
<div id="ref-mlcteam2025longcat" class="csl-entry" role="listitem">
<div class="csl-left-margin">[280] </div><div class="csl-right-inline">M. L. Team, <span>“Introducing LongCat-flash-thinking: A technical report.”</span> 2025. Available: <a href="https://arxiv.org/abs/2509.18883">https://arxiv.org/abs/2509.18883</a></div>
</div>
<div id="ref-ringteam2025everystepevolves" class="csl-entry" role="listitem">
<div class="csl-left-margin">[281] </div><div class="csl-right-inline">L. Team <em>et al.</em>, <span>“Every step evolves: Scaling reinforcement learning for trillion-scale thinking model.”</span> 2025. Available: <a href="https://arxiv.org/abs/2510.18855">https://arxiv.org/abs/2510.18855</a></div>
</div>
<div id="ref-teamolmo2025olmo3" class="csl-entry" role="listitem">
<div class="csl-left-margin">[282] </div><div class="csl-right-inline">T. Olmo <em>et al.</em>, <span>“Olmo 3.”</span> 2025. Available: <a href="https://arxiv.org/abs/2512.13961">https://arxiv.org/abs/2512.13961</a></div>
</div>
<div id="ref-deepseekai2025v32" class="csl-entry" role="listitem">
<div class="csl-left-margin">[283] </div><div class="csl-right-inline">DeepSeek-AI, <span>“DeepSeek-V3.2: Pushing the frontier of open large language models.”</span> 2025. Available: <a href="https://arxiv.org/abs/2512.02556">https://arxiv.org/abs/2512.02556</a></div>
</div>
<div id="ref-nvidia2025nemotron3nano" class="csl-entry" role="listitem">
<div class="csl-left-margin">[284] </div><div class="csl-right-inline">NVIDIA, <span>“Nemotron 3 nano: Open, efficient mixture-of-experts hybrid mamba-transformer model for agentic reasoning,”</span> NVIDIA, Dec. 2025. Available: <a href="https://research.nvidia.com/labs/nemotron/files/NVIDIA-Nemotron-3-Nano-Technical-Report.pdf">https://research.nvidia.com/labs/nemotron/files/NVIDIA-Nemotron-3-Nano-Technical-Report.pdf</a></div>
</div>
<div id="ref-mimo2025flash" class="csl-entry" role="listitem">
<div class="csl-left-margin">[285] </div><div class="csl-right-inline">L.-C. Xiaomi, <span>“MiMo-V2-flash technical report.”</span> 2025. Available: <a href="https://github.com/XiaomiMiMo/MiMo-V2-Flash/blob/main/paper.pdf">https://github.com/XiaomiMiMo/MiMo-V2-Flash/blob/main/paper.pdf</a></div>
</div>
<div id="ref-wang2025ragenunderstandingselfevolutionllm" class="csl-entry" role="listitem">
<div class="csl-left-margin">[286] </div><div class="csl-right-inline">Z. Wang <em>et al.</em>, <span>“RAGEN: Understanding self-evolution in LLM agents via multi-turn reinforcement learning.”</span> 2025. Available: <a href="https://arxiv.org/abs/2504.20073">https://arxiv.org/abs/2504.20073</a></div>
</div>
<div id="ref-shao2025spurious" class="csl-entry" role="listitem">
<div class="csl-left-margin">[287] </div><div class="csl-right-inline">R. Shao <em>et al.</em>, <span>“Spurious rewards: Rethinking training signals in RLVR.”</span> <a href="https://rethink-rlvr.notion.site/Spurious-Rewards-Rethinking-Training-Signals-in-RLVR-1f4df34dac1880948858f95aeb88872f" class="uri">https://rethink-rlvr.notion.site/Spurious-Rewards-Rethinking-Training-Signals-in-RLVR-1f4df34dac1880948858f95aeb88872f</a>, 2025.</div>
</div>
<div id="ref-anthropic2025claude4" class="csl-entry" role="listitem">
<div class="csl-left-margin">[288] </div><div class="csl-right-inline">Anthropic, <span>“Claude 4.”</span> May 2025. Available: <a href="https://www.anthropic.com/news/claude-4">https://www.anthropic.com/news/claude-4</a></div>
</div>
<div id="ref-aggarwal2025l1" class="csl-entry" role="listitem">
<div class="csl-left-margin">[289] </div><div class="csl-right-inline">P. Aggarwal and S. Welleck, <span>“L1: Controlling how long a reasoning model thinks with reinforcement learning,”</span> <em>arXiv preprint arXiv:2503.04697</em>, 2025.</div>
</div>
<div id="ref-reed2015neural" class="csl-entry" role="listitem">
<div class="csl-left-margin">[290] </div><div class="csl-right-inline">S. Reed and N. De Freitas, <span>“Neural programmer-interpreters,”</span> <em>arXiv preprint arXiv:1511.06279</em>, 2015.</div>
</div>
<div id="ref-lewis2020retrieval" class="csl-entry" role="listitem">
<div class="csl-left-margin">[291] </div><div class="csl-right-inline"><span class="nocase">P. Lewis <em>et al.</em></span>, <span>“Retrieval-augmented generation for knowledge-intensive nlp tasks,”</span> <em>Advances in neural information processing systems</em>, vol. 33, pp. 9459–9474, 2020.</div>
</div>
<div id="ref-gao2023pal" class="csl-entry" role="listitem">
<div class="csl-left-margin">[292] </div><div class="csl-right-inline">L. Gao <em>et al.</em>, <span>“Pal: Program-aided language models,”</span> in <em>International conference on machine learning</em>, PMLR, 2023, pp. 10764–10799.</div>
</div>
<div id="ref-parisi2022talm" class="csl-entry" role="listitem">
<div class="csl-left-margin">[293] </div><div class="csl-right-inline">A. Parisi, Y. Zhao, and N. Fiedel, <span>“Talm: Tool augmented language models,”</span> <em>arXiv preprint arXiv:2205.12255</em>, 2022.</div>
</div>
<div id="ref-schick2023toolformerlanguagemodelsteach" class="csl-entry" role="listitem">
<div class="csl-left-margin">[294] </div><div class="csl-right-inline">T. Schick <em>et al.</em>, <span>“Toolformer: Language models can teach themselves to use tools.”</span> 2023. Available: <a href="https://arxiv.org/abs/2302.04761">https://arxiv.org/abs/2302.04761</a></div>
</div>
<div id="ref-patil2023gorilla" class="csl-entry" role="listitem">
<div class="csl-left-margin">[295] </div><div class="csl-right-inline">S. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez, <span>“Gorilla: Large language model connected with massive APIs,”</span> <em>arXiv preprint arXiv:2305.15334</em>, 2023.</div>
</div>
<div id="ref-anthropic_mcp_2024" class="csl-entry" role="listitem">
<div class="csl-left-margin">[296] </div><div class="csl-right-inline">Anthropic, <span>“Model context protocol (MCP).”</span> <a href="https://modelcontextprotocol.io/" class="uri">https://modelcontextprotocol.io/</a>, 2024.</div>
</div>
<div id="ref-bran2023chemcrow" class="csl-entry" role="listitem">
<div class="csl-left-margin">[297] </div><div class="csl-right-inline">A. M. Bran, S. Cox, O. Schilter, C. Baldassari, A. D. White, and P. Schwaller, <span>“Chemcrow: Augmenting large-language models with chemistry tools,”</span> <em>arXiv preprint arXiv:2304.05376</em>, 2023.</div>
</div>
<div id="ref-li2024mmedagent" class="csl-entry" role="listitem">
<div class="csl-left-margin">[298] </div><div class="csl-right-inline"><span class="nocase">B. Li <em>et al.</em></span>, <span>“Mmedagent: Learning to use medical tools with multi-modal agent,”</span> <em>arXiv preprint arXiv:2407.02483</em>, 2024.</div>
</div>
<div id="ref-zhang2024codeagent" class="csl-entry" role="listitem">
<div class="csl-left-margin">[299] </div><div class="csl-right-inline">K. Zhang, J. Li, G. Li, X. Shi, and Z. Jin, <span>“Codeagent: Enhancing code generation with tool-integrated agent systems for real-world repo-level coding challenges,”</span> <em>arXiv preprint arXiv:2401.07339</em>, 2024.</div>
</div>
<div id="ref-yao2023react" class="csl-entry" role="listitem">
<div class="csl-left-margin">[300] </div><div class="csl-right-inline">S. Yao <em>et al.</em>, <span>“React: Synergizing reasoning and acting in language models,”</span> in <em>International conference on learning representations (ICLR)</em>, 2023.</div>
</div>
<div id="ref-kwon2023efficient" class="csl-entry" role="listitem">
<div class="csl-left-margin">[301] </div><div class="csl-right-inline">W. Kwon <em>et al.</em>, <span>“Efficient memory management for large language model serving with PagedAttention,”</span> in <em>Proceedings of the ACM SIGOPS 29th symposium on operating systems principles</em>, 2023.</div>
</div>
<div id="ref-shumailov2024ai" class="csl-entry" role="listitem">
<div class="csl-left-margin">[302] </div><div class="csl-right-inline">I. Shumailov, Z. Shumaylov, Y. Zhao, N. Papernot, R. Anderson, and Y. Gal, <span>“AI models collapse when trained on recursively generated data,”</span> <em>Nature</em>, vol. 631, no. 8022, pp. 755–759, 2024.</div>
</div>
<div id="ref-gerstgrasser2024model" class="csl-entry" role="listitem">
<div class="csl-left-margin">[303] </div><div class="csl-right-inline"><span class="nocase">M. Gerstgrasser <em>et al.</em></span>, <span>“Is model collapse inevitable? Breaking the curse of recursion by accumulating real and synthetic data,”</span> <em>arXiv preprint arXiv:2404.01413</em>, 2024.</div>
</div>
<div id="ref-feng2024beyond" class="csl-entry" role="listitem">
<div class="csl-left-margin">[304] </div><div class="csl-right-inline">Y. Feng, E. Dohmatob, P. Yang, F. Charton, and J. Kempe, <span>“Beyond model collapse: Scaling up with synthesized data requires reinforcement,”</span> in <em>ICML 2024 workshop on theoretical foundations of foundation models</em>, 2024.</div>
</div>
<div id="ref-wang2022self" class="csl-entry" role="listitem">
<div class="csl-left-margin">[305] </div><div class="csl-right-inline">Y. Wang <em>et al.</em>, <span>“Self-instruct: Aligning language models with self-generated instructions,”</span> <em>arXiv preprint arXiv:2212.10560</em>, 2022.</div>
</div>
<div id="ref-numina_math_7b" class="csl-entry" role="listitem">
<div class="csl-left-margin">[306] </div><div class="csl-right-inline">E. Beeching <em>et al.</em>, <span>“NuminaMath 7B TIR,”</span> <em>Hugging Face repository</em>. <a href="https://huggingface.co/AI-MO/NuminaMath-7B-TIR" class="uri">https://huggingface.co/AI-MO/NuminaMath-7B-TIR</a>; Numina &amp; Hugging Face, 2024.</div>
</div>
<div id="ref-li2024superfiltering" class="csl-entry" role="listitem">
<div class="csl-left-margin">[307] </div><div class="csl-right-inline">M. Li <em>et al.</em>, <span>“Superfiltering: Weak-to-strong data filtering for fast instruction-tuning,”</span> <em>arXiv preprint arXiv:2402.00530</em>, 2024.</div>
</div>
<div id="ref-shridhar2023distilling" class="csl-entry" role="listitem">
<div class="csl-left-margin">[308] </div><div class="csl-right-inline">K. Shridhar, A. Stolfo, and M. Sachan, <span>“Distilling reasoning capabilities into smaller language models,”</span> <em>Findings of the Association for Computational Linguistics: ACL 2023</em>, pp. 7059–7073, 2023.</div>
</div>
<div id="ref-hsieh2023distilling" class="csl-entry" role="listitem">
<div class="csl-left-margin">[309] </div><div class="csl-right-inline">C.-Y. Hsieh <em>et al.</em>, <span>“Distilling step-by-step! Outperforming larger language models with less training data and smaller model sizes,”</span> <em>arXiv preprint arXiv:2305.02301</em>, 2023.</div>
</div>
<div id="ref-hendrycks2020measuring" class="csl-entry" role="listitem">
<div class="csl-left-margin">[310] </div><div class="csl-right-inline">D. Hendrycks <em>et al.</em>, <span>“Measuring massive multitask language understanding,”</span> <em>arXiv preprint arXiv:2009.03300</em>, 2020.</div>
</div>
<div id="ref-mallen2023llm_memorization" class="csl-entry" role="listitem">
<div class="csl-left-margin">[311] </div><div class="csl-right-inline">A. Mallen, A. Asai, V. Zhong, R. Das, H. Hajishirzi, and D. Khashabi, <span>“When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories,”</span> <em>arXiv preprint</em>, 2022.</div>
</div>
<div id="ref-lin2021truthfulqa" class="csl-entry" role="listitem">
<div class="csl-left-margin">[312] </div><div class="csl-right-inline">S. Lin, J. Hilton, and O. Evans, <span>“Truthfulqa: Measuring how models mimic human falsehoods,”</span> <em>arXiv preprint arXiv:2109.07958</em>, 2021.</div>
</div>
<div id="ref-suzgun2022challenging" class="csl-entry" role="listitem">
<div class="csl-left-margin">[313] </div><div class="csl-right-inline">M. Suzgun <em>et al.</em>, <span>“Challenging BIG-bench tasks and whether chain-of-thought can solve them,”</span> <em>arXiv preprint arXiv:2210.09261</em>, 2022.</div>
</div>
<div id="ref-dua2019drop" class="csl-entry" role="listitem">
<div class="csl-left-margin">[314] </div><div class="csl-right-inline">D. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner, <span>“DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs,”</span> <em>arXiv preprint arXiv:1903.00161</em>, 2019.</div>
</div>
<div id="ref-hendrycksmath2021" class="csl-entry" role="listitem">
<div class="csl-left-margin">[315] </div><div class="csl-right-inline">D. Hendrycks <em>et al.</em>, <span>“Measuring mathematical problem solving with the MATH dataset,”</span> <em>NeurIPS</em>, 2021.</div>
</div>
<div id="ref-chen2021codex" class="csl-entry" role="listitem">
<div class="csl-left-margin">[316] </div><div class="csl-right-inline">M. Chen <em>et al.</em>, <span>“Evaluating large language models trained on code,”</span> 2021, Available: <a href="https://arxiv.org/abs/2107.03374">https://arxiv.org/abs/2107.03374</a></div>
</div>
<div id="ref-evalplus" class="csl-entry" role="listitem">
<div class="csl-left-margin">[317] </div><div class="csl-right-inline">J. Liu, C. S. Xia, Y. Wang, and L. Zhang, <span>“Is your code generated by chat<span>GPT</span> really correct? Rigorous evaluation of large language models for code generation,”</span> in <em>Thirty-seventh conference on neural information processing systems</em>, 2023. Available: <a href="https://openreview.net/forum?id=1qvx610Cu7">https://openreview.net/forum?id=1qvx610Cu7</a></div>
</div>
<div id="ref-rein2023gpqa" class="csl-entry" role="listitem">
<div class="csl-left-margin">[318] </div><div class="csl-right-inline">D. Rein <em>et al.</em>, <span>“GPQA: A graduate-level google-proof q&amp;a benchmark,”</span> <em>arXiv preprint arXiv:2311.12022</em>, 2023.</div>
</div>
<div id="ref-phan2025hle" class="csl-entry" role="listitem">
<div class="csl-left-margin">[319] </div><div class="csl-right-inline">L. Phan, A. Gatti, Z. Han, N. Li, and H. et al. Zhang, <span>“Humanity’s last exam,”</span> <em>arXiv preprint arXiv:2501.14249</em>, 2025.</div>
</div>
<div id="ref-aleithan2024swebenchplus" class="csl-entry" role="listitem">
<div class="csl-left-margin">[320] </div><div class="csl-right-inline">R. Aleithan, H. Xue, M. M. Mohajer, E. Nnorom, G. Uddin, and S. Wang, <span>“<span>SWE-Bench+</span>: Enhanced coding benchmark for LLMs,”</span> <em>arXiv preprint arXiv:2410.06992</em>, 2024.</div>
</div>
<div id="ref-jain2024livecodebench" class="csl-entry" role="listitem">
<div class="csl-left-margin">[321] </div><div class="csl-right-inline">N. Jain <em>et al.</em>, <span>“<span>LiveCodeBench</span>: Holistic and contamination-free evaluation of large language models for code,”</span> <em>arXiv preprint arXiv:2403.07974</em>, 2024.</div>
</div>
<div id="ref-scale2024seal" class="csl-entry" role="listitem">
<div class="csl-left-margin">[322] </div><div class="csl-right-inline">S. AI, <span>“SEAL LLM leaderboards: Expert-driven private evaluations.”</span> 2024. Available: <a href="https://scale.com/leaderboard">https://scale.com/leaderboard</a></div>
</div>
<div id="ref-schulhoff2024prompt" class="csl-entry" role="listitem">
<div class="csl-left-margin">[323] </div><div class="csl-right-inline"><span class="nocase">S. Schulhoff <em>et al.</em></span>, <span>“The prompt report: A systematic survey of prompting techniques,”</span> <em>arXiv preprint arXiv:2406.06608</em>, 2024.</div>
</div>
<div id="ref-robinson2023leveraging" class="csl-entry" role="listitem">
<div class="csl-left-margin">[324] </div><div class="csl-right-inline">J. Robinson, C. M. Rytting, and D. Wingate, <span>“Leveraging large language models for multiple choice question answering,”</span> in <em>International conference on learning representations</em>, 2023. Available: <a href="https://openreview.net/forum?id=upQ4o-ygvJ">https://openreview.net/forum?id=upQ4o-ygvJ</a></div>
</div>
<div id="ref-wei2022finetuned" class="csl-entry" role="listitem">
<div class="csl-left-margin">[325] </div><div class="csl-right-inline">J. Wei <em>et al.</em>, <span>“Finetuned language models are zero-shot learners,”</span> in <em>International conference on learning representations</em>, 2022.</div>
</div>
<div id="ref-sanh2022multitask" class="csl-entry" role="listitem">
<div class="csl-left-margin">[326] </div><div class="csl-right-inline"><span class="nocase">V. Sanh <em>et al.</em></span>, <span>“Multitask prompted training enables zero-shot task generalization,”</span> in <em>International conference on learning representations</em>, 2022.</div>
</div>
<div id="ref-kojima2022large" class="csl-entry" role="listitem">
<div class="csl-left-margin">[327] </div><div class="csl-right-inline">T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, <span>“Large language models are zero-shot reasoners,”</span> <em>Advances in neural information processing systems</em>, vol. 35, pp. 22199–22213, 2022.</div>
</div>
<div id="ref-achiam2023gpt" class="csl-entry" role="listitem">
<div class="csl-left-margin">[328] </div><div class="csl-right-inline"><span class="nocase">J. Achiam <em>et al.</em></span>, <span>“Gpt-4 technical report,”</span> <em>arXiv preprint arXiv:2303.08774</em>, 2023.</div>
</div>
<div id="ref-openai2024swebench" class="csl-entry" role="listitem">
<div class="csl-left-margin">[329] </div><div class="csl-right-inline">OpenAI, <span>“Introducing SWE-bench verified.”</span> Aug. 2024. Available: <a href="https://openai.com/index/introducing-swe-bench-verified/">https://openai.com/index/introducing-swe-bench-verified/</a></div>
</div>
<div id="ref-li2024numinamath" class="csl-entry" role="listitem">
<div class="csl-left-margin">[330] </div><div class="csl-right-inline"><span class="nocase">J. Li <em>et al.</em></span>, <span>“Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions,”</span> <em>Hugging Face repository</em>, vol. 13, p. 9, 2024.</div>
</div>
<div id="ref-yu2023metamath" class="csl-entry" role="listitem">
<div class="csl-left-margin">[331] </div><div class="csl-right-inline">L. Yu <em>et al.</em>, <span>“Metamath: Bootstrap your own mathematical questions for large language models,”</span> <em>arXiv preprint arXiv:2309.12284</em>, 2023.</div>
</div>
<div id="ref-singh2024evaluation" class="csl-entry" role="listitem">
<div class="csl-left-margin">[332] </div><div class="csl-right-inline">A. K. Singh <em>et al.</em>, <span>“Evaluation data contamination in LLMs: How do we measure it and (when) does it matter?”</span> <em>arXiv preprint arXiv:2411.03923</em>, 2024.</div>
</div>
<div id="ref-huang2025math" class="csl-entry" role="listitem">
<div class="csl-left-margin">[333] </div><div class="csl-right-inline"><span class="nocase">K. Huang <em>et al.</em></span>, <span>“MATH-perturb: Benchmarking LLMs’ math reasoning abilities against hard perturbations,”</span> <em>arXiv preprint arXiv:2502.06453</em>, 2025.</div>
</div>
<div id="ref-inspectAI2024" class="csl-entry" role="listitem">
<div class="csl-left-margin">[334] </div><div class="csl-right-inline">UK AI Safety Institute, <span>“<span class="nocase">Inspect AI: Framework for Large Language Model Evaluations</span>.”</span> <a href="https://github.com/UKGovernmentBEIS/inspect_ai" class="uri">https://github.com/UKGovernmentBEIS/inspect_ai</a>, 2024.</div>
</div>
<div id="ref-fourrier2023lighteval" class="csl-entry" role="listitem">
<div class="csl-left-margin">[335] </div><div class="csl-right-inline">C. Fourrier, N. Habib, H. Kydlicek, T. Wolf, and L. Tunstall, <span>“<span class="nocase">LightEval: A lightweight framework for LLM evaluation</span>.”</span> <a href="https://github.com/huggingface/lighteval" class="uri">https://github.com/huggingface/lighteval</a>, 2023.</div>
</div>
<div id="ref-open-llm-leaderboard-v2" class="csl-entry" role="listitem">
<div class="csl-left-margin">[336] </div><div class="csl-right-inline">C. Fourrier, N. Habib, A. Lozovskaya, K. Szafer, and T. Wolf, <span>“Open LLM leaderboard v2.”</span> <a href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard" class="uri">https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard</a>; Hugging Face, 2024.</div>
</div>
<div id="ref-gao2023evalharness" class="csl-entry" role="listitem">
<div class="csl-left-margin">[337] </div><div class="csl-right-inline">L. Gao <em>et al.</em>, <span>“<span class="nocase">A Framework for Few-Shot Language Model Evaluation</span>.”</span> Zenodo, 2023. doi: <a href="https://doi.org/10.5281/zenodo.10256836">10.5281/zenodo.10256836</a>.</div>
</div>
<div id="ref-gpt-neox-20b" class="csl-entry" role="listitem">
<div class="csl-left-margin">[338] </div><div class="csl-right-inline">S. Black <em>et al.</em>, <span>“<span>GPT-NeoX-20B</span>: An open-source autoregressive language model,”</span> in <em>Proceedings of the ACL workshop on challenges &amp; perspectives in creating large language models</em>, 2022. Available: <a href="https://arxiv.org/abs/2204.06745">https://arxiv.org/abs/2204.06745</a></div>
</div>
<div id="ref-gu2024olmes" class="csl-entry" role="listitem">
<div class="csl-left-margin">[339] </div><div class="csl-right-inline">Y. Gu, O. Tafjord, B. Kuehl, D. Haddad, J. Dodge, and H. Hajishirzi, <span>“<span class="nocase">OLMES: A Standard for Language Model Evaluations</span>,”</span> <em>arXiv preprint arXiv:2406.08446</em>, 2024.</div>
</div>
<div id="ref-liang2023helm" class="csl-entry" role="listitem">
<div class="csl-left-margin">[340] </div><div class="csl-right-inline">P. Liang <em>et al.</em>, <span>“<span class="nocase">Holistic Evaluation of Language Models</span>,”</span> <em>Transactions on Machine Learning Research</em>, 2023, doi: <a href="https://doi.org/10.1111/nyas.15007">10.1111/nyas.15007</a>.</div>
</div>
<div id="ref-mosaicml2024gauntlet" class="csl-entry" role="listitem">
<div class="csl-left-margin">[341] </div><div class="csl-right-inline">MosaicML, <span>“<span class="nocase">Mosaic Eval Gauntlet v0.3.0 — Evaluation Suite</span>.”</span> <a href="https://github.com/mosaicml/llm-foundry/blob/main/scripts/eval/local_data/EVAL_GAUNTLET.md" class="uri">https://github.com/mosaicml/llm-foundry/blob/main/scripts/eval/local_data/EVAL_GAUNTLET.md</a>, 2024.</div>
</div>
<div id="ref-schulman2023proxy" class="csl-entry" role="listitem">
<div class="csl-left-margin">[342] </div><div class="csl-right-inline">J. Schulman, <span>“Proxy objectives in reinforcement learning from human feedback.”</span> Invited talk at the International Conference on Machine Learning (ICML), 2023. Available: <a href="https://icml.cc/virtual/2023/invited-talk/21549">https://icml.cc/virtual/2023/invited-talk/21549</a></div>
</div>
<div id="ref-zhang2018study" class="csl-entry" role="listitem">
<div class="csl-left-margin">[343] </div><div class="csl-right-inline">C. Zhang, O. Vinyals, R. Munos, and S. Bengio, <span>“A study on overfitting in deep reinforcement learning,”</span> <em>arXiv preprint arXiv:1804.06893</em>, 2018.</div>
</div>
<div id="ref-goodhart1984problems" class="csl-entry" role="listitem">
<div class="csl-left-margin">[344] </div><div class="csl-right-inline">C. A. Goodhart and C. Goodhart, <em>Problems of monetary management: The UK experience</em>. Springer, 1984.</div>
</div>
<div id="ref-hoskin1996awful" class="csl-entry" role="listitem">
<div class="csl-left-margin">[345] </div><div class="csl-right-inline">K. Hoskin, <span>“The <span>‘awful idea of accountability’</span>: Inscribing people into the measurement of objects,”</span> <em>Accountability: Power, ethos and the technologies of managing</em>, vol. 265, 1996.</div>
</div>
<div id="ref-lu2011learning" class="csl-entry" role="listitem">
<div class="csl-left-margin">[346] </div><div class="csl-right-inline">T. Lu and C. Boutilier, <span>“Learning mallows models with pairwise preferences,”</span> in <em>Proceedings of the 28th international conference on machine learning (icml-11)</em>, 2011, pp. 145–152.</div>
</div>
<div id="ref-han2024wildguard" class="csl-entry" role="listitem">
<div class="csl-left-margin">[347] </div><div class="csl-right-inline">S. Han <em>et al.</em>, <span>“Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms,”</span> <em>arXiv preprint arXiv:2406.18495</em>, 2024.</div>
</div>
<div id="ref-inan2023llama" class="csl-entry" role="listitem">
<div class="csl-left-margin">[348] </div><div class="csl-right-inline"><span class="nocase">H. Inan <em>et al.</em></span>, <span>“Llama guard: Llm-based input-output safeguard for human-ai conversations,”</span> <em>arXiv preprint arXiv:2312.06674</em>, 2023.</div>
</div>
<div id="ref-rottger2023xstest" class="csl-entry" role="listitem">
<div class="csl-left-margin">[349] </div><div class="csl-right-inline">P. Röttger, H. R. Kirk, B. Vidgen, G. Attanasio, F. Bianchi, and D. Hovy, <span>“Xstest: A test suite for identifying exaggerated safety behaviours in large language models,”</span> <em>arXiv preprint arXiv:2308.01263</em>, 2023.</div>
</div>
<div id="ref-coste2023reward" class="csl-entry" role="listitem">
<div class="csl-left-margin">[350] </div><div class="csl-right-inline">T. Coste, U. Anwar, R. Kirk, and D. Krueger, <span>“Reward model ensembles help mitigate overoptimization,”</span> <em>arXiv preprint arXiv:2310.02743</em>, 2023.</div>
</div>
<div id="ref-moskovitz2023confronting" class="csl-entry" role="listitem">
<div class="csl-left-margin">[351] </div><div class="csl-right-inline">T. Moskovitz <em>et al.</em>, <span>“Confronting reward model overoptimization with constrained RLHF,”</span> <em>arXiv preprint arXiv:2310.04373</em>, 2023.</div>
</div>
<div id="ref-rafailov2024scaling" class="csl-entry" role="listitem">
<div class="csl-left-margin">[352] </div><div class="csl-right-inline">R. Rafailov <em>et al.</em>, <span>“Scaling laws for reward model overoptimization in direct alignment algorithms,”</span> <em>Advances in Neural Information Processing Systems</em>, vol. 37, pp. 126207–126242, 2024.</div>
</div>
<div id="ref-zhuang2020consequences" class="csl-entry" role="listitem">
<div class="csl-left-margin">[353] </div><div class="csl-right-inline">S. Zhuang and D. Hadfield-Menell, <span>“Consequences of misaligned AI,”</span> <em>Advances in Neural Information Processing Systems</em>, vol. 33, pp. 15763–15773, 2020.</div>
</div>
<div id="ref-yuan2025selfrewardinglanguagemodels" class="csl-entry" role="listitem">
<div class="csl-left-margin">[354] </div><div class="csl-right-inline">W. Yuan <em>et al.</em>, <span>“Self-rewarding language models.”</span> 2025. Available: <a href="https://arxiv.org/abs/2401.10020">https://arxiv.org/abs/2401.10020</a></div>
</div>
<div id="ref-qwen" class="csl-entry" role="listitem">
<div class="csl-left-margin">[355] </div><div class="csl-right-inline">J. Bai <em>et al.</em>, <span>“Qwen technical report,”</span> <em>arXiv preprint arXiv:2309.16609</em>, 2023.</div>
</div>
<div id="ref-wang2023openchat" class="csl-entry" role="listitem">
<div class="csl-left-margin">[356] </div><div class="csl-right-inline">G. Wang, S. Cheng, X. Zhan, X. Li, S. Song, and Y. Liu, <span>“Openchat: Advancing open-source language models with mixed-quality data,”</span> <em>arXiv preprint arXiv:2309.11235</em>, 2023.</div>
</div>
<div id="ref-maiya2025open" class="csl-entry" role="listitem">
<div class="csl-left-margin">[357] </div><div class="csl-right-inline">S. Maiya, H. Bartsch, N. Lambert, and E. Hubinger, <span>“Open character training: Shaping the persona of AI assistants through constitutional AI,”</span> <em>arXiv preprint arXiv:2511.01689</em>, 2025.</div>
</div>
<div id="ref-anthropic2024claude" class="csl-entry" role="listitem">
<div class="csl-left-margin">[358] </div><div class="csl-right-inline">Anthropic, <span>“Claude’s character.”</span> 2024. Available: <a href="https://www.anthropic.com/research/claude-character">https://www.anthropic.com/research/claude-character</a></div>
</div>
</div>
</section>
</body>
</html>
