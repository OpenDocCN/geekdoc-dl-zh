["```r\nlibrary(torch)\nlibrary(torchvision)\nlibrary(luz)\n\nmodel <- model_mobilenet_v2(pretrained = TRUE)\nmodel\n```", "```r\nAn `nn_module` containing 3,504,872 parameters.\n\n── Modules ──────────────────────────────────────────────────────────────────────────────────\n• features: <nn_sequential> #2,223,872 parameters\n• classifier: <nn_sequential> #1,281,000 parameters\n```", "```r\nmodel$features\n```", "```r\nAn `nn_module` containing 2,223,872 parameters.\n\n── Modules ──────────────────────────────────────────────────────────────────────────────────\n• 0: <conv_bn_activation> #928 parameters\n• 1: <inverted_residual> #896 parameters\n• 2: <inverted_residual> #5,136 parameters\n• 3: <inverted_residual> #8,832 parameters\n• 4: <inverted_residual> #10,000 parameters\n• 5: <inverted_residual> #14,848 parameters\n• 6: <inverted_residual> #14,848 parameters\n• 7: <inverted_residual> #21,056 parameters\n• 8: <inverted_residual> #54,272 parameters\n• 9: <inverted_residual> #54,272 parameters\n• 10: <inverted_residual> #54,272 parameters\n• 11: <inverted_residual> #66,624 parameters\n• 12: <inverted_residual> #118,272 parameters\n• 13: <inverted_residual> #118,272 parameters\n• 14: <inverted_residual> #155,264 parameters\n• 15: <inverted_residual> #320,000 parameters\n• 16: <inverted_residual> #320,000 parameters\n• 17: <inverted_residual> #473,920 parameters\n• 18: <conv_bn_activation> #412,160 parameters\n```", "```r\nmodel$features[2]$`0`$conv\n```", "```r\nAn `nn_module` containing 896 parameters.\n\n── Modules ──────────────────────────────────────────────────────────────────────────────────\n• 0: <conv_bn_activation> #352 parameters\n• 1: <nn_conv2d> #512 parameters\n• 2: <nn_batch_norm2d> #32 parameters\n```", "```r\nmodel$features[2]$`0`$conv[1]$`0`\n```", "```r\nAn `nn_module` containing 352 parameters.\n\n── Modules ─────────────────────────────────────────────────────────────────────────────────────────────\n• 0: <nn_conv2d> #288 parameters\n• 1: <nn_batch_norm2d> #64 parameters\n• 2: <nn_relu6> #0 parameters\n```", "```r\nencoder <- nn_module(\n initialize = function() {\n model <- model_mobilenet_v2(pretrained = TRUE)\n self$stages <- nn_module_list(list(\n nn_identity(),\n model$features[1:2],\n model$features[3:4],\n model$features[5:7],\n model$features[8:14],\n model$features[15:18]\n ))\n for (par in self$parameters) {\n par$requires_grad_(FALSE)\n }\n },\n forward = function(x) {\n features <- list()\n for (i in 1:length(self$stages)) {\n x <- self$stages[[i]](x)\n features[[length(features) + 1]] <- x\n }\n features\n }\n)\n```", "```r\nsample <- torch_randn(1, 3, 224, 224)\nsample_features <- encoder()(sample)\npurrr::map(sample_features, purrr::compose(dim, as.array))\n```", "```r\n[[1]]\n[1]   1   3 224 224\n\n[[2]]\n[1]   1  16 112 112\n\n[[3]]\n[1]   1  24  56  56\n\n[[4]]\n[1]   1  32  28  28\n\n[[5]]\n[1]   1  96  14  14\n\n[[6]]\n[1]   1 320   7   7\n```", "```r\ndecoder_block <- nn_module(\n initialize = function(in_channels,\n skip_channels,\n out_channels) {\n self$upsample <- nn_conv_transpose2d(\n in_channels = in_channels,\n out_channels = out_channels,\n kernel_size = 2,\n stride = 2\n )\n self$activation <- nn_relu()\n self$conv <- nn_conv2d(\n in_channels = out_channels + skip_channels,\n out_channels = out_channels,\n kernel_size = 3,\n padding = \"same\"\n )\n },\n forward = function(x, skip) {\n x <- x %>%\n self$upsample() %>%\n self$activation()\n input <- torch_cat(list(x, skip), dim = 2)\n input %>%\n self$conv() %>%\n self$activation()\n }\n)\n```", "```r\nimg <- torch_randn(1, 1, 5, 5)\n\nconv <- nn_conv2d(\n in_channels = 1,\n out_channels = 1,\n kernel_size = 3,\n stride = 2,\n padding = 1\n)\n\nconvolved <- conv(img)\nconvolved\n```", "```r\ntorch_tensor\n(1,1,.,.) = \n -0.4996 -0.2898  0.4643\n  0.6608  1.2109  0.8377\n  0.3615  0.5400  0.1567\n[ CPUFloatType{1,1,3,3} ][ grad_fn = <ConvolutionBackward0> ]\n```", "```r\ntransposed_conv <- nn_conv_transpose2d(\n in_channels = 1,\n out_channels = 1,\n kernel_size = 3,\n stride = 2,\n padding = 1\n)\n\nupsampled <- transposed_conv(convolved)\nupsampled\n```", "```r\ntorch_tensor\n(1,1,.,.) = \n  0.4076  0.0940  0.3424  0.1920  0.1078\n  0.2416  0.6456  0.2473  0.6500  0.2643\n  0.0467  0.5028 -0.1243  0.6425 -0.0083\n  0.2682  0.5003  0.2812  0.4150  0.2720\n  0.1398  0.3832  0.0843  0.4155  0.2035\n[ CPUFloatType{1,1,5,5} ][ grad_fn = <ConvolutionBackward0> ]\n```", "```r\nfirst_decoder_block <- decoder_block(\n in_channels = 320,\n skip_channels = 96,\n out_channels = 256\n)\n```", "```r\nfirst_decoder_block(\n sample_features[[6]],\n sample_features[[5]]\n) %>%\n dim()\n```", "```r\n[1]   1 256  14  14\n```", "```r\ndecoder <- nn_module(\n initialize = function(\n decoder_channels = c(256, 128, 64, 32, 16),\n encoder_channels = c(16, 24, 32, 96, 320)) {\n encoder_channels <- rev(encoder_channels)\n skip_channels <- c(encoder_channels[-1], 3)\n in_channels <- c(encoder_channels[1], decoder_channels)\n\n depth <- length(encoder_channels)\n\n self$blocks <- nn_module_list()\n for (i in seq_len(depth)) {\n self$blocks$append(decoder_block(\n in_channels = in_channels[i],\n skip_channels = skip_channels[i],\n out_channels = decoder_channels[i]\n ))\n }\n },\n forward = function(features) {\n features <- rev(features)\n x <- features[[1]]\n for (i in seq_along(self$blocks)) {\n x <- self$blocks[[i]](x, features[[i + 1]])\n }\n x\n }\n)\n```", "```r\nmodel <- nn_module(\n initialize = function() {\n self$encoder <- encoder()\n self$decoder <- decoder()\n self$output <- nn_conv2d(\n in_channels = 16,\n out_channels = 3,\n kernel_size = 3,\n padding = \"same\"\n )\n },\n forward = function(x) {\n x %>%\n self$encoder() %>%\n self$decoder() %>%\n self$output()\n }\n)\n```", "```r\nlibrary(torchvision)\nlibrary(torchdatasets)\n\ndir <- \"~/.torch-datasets\"\n\nds <- oxford_pet_dataset(root = dir, download = TRUE)\n```", "```r\npet_dataset <- torch::dataset(\n inherit = oxford_pet_dataset,\n initialize = function(...,\n size,\n normalize = TRUE,\n augmentation = NULL) {\n self$augmentation <- augmentation\n input_transform <- function(x) {\n x <- x %>%\n transform_to_tensor() %>%\n transform_resize(size)\n if (normalize) {\n x <- x %>%\n transform_normalize(\n mean = c(0.485, 0.456, 0.406),\n std = c(0.229, 0.224, 0.225)\n )\n }\n x\n }\n target_transform <- function(x) {\n x <- torch_tensor(x, dtype = torch_long())\n x <- x[newaxis, ..]\n # interpolation = 0 makes sure we\n # still end up with integer classes\n x <- transform_resize(x, size, interpolation = 0)\n x[1, ..]\n }\n super$initialize(\n ...,\n transform = input_transform,\n target_transform = target_transform\n )\n },\n .getitem = function(i) {\n item <- super$.getitem(i)\n if (!is.null(self$augmentation)) {\n self$augmentation(item)\n } else {\n list(x = item$x, y = item$y)\n }\n }\n)\n```", "```r\naugmentation <- function(item) {\n vflip <- runif(1) > 0.5\n\n x <- item$x\n y <- item$y\n\n if (vflip) {\n x <- transform_vflip(x)\n y <- transform_vflip(y)\n }\n\n list(x = x, y = y)\n}\n```", "```r\nangle <- runif(1, -12, 12)\nx <- transform_rotate(x, angle)\n\n# same effect as interpolation = 0, above\ny <- transform_rotate(y, angle, resample = 0)\n```", "```r\nangle <- runif(1, -12, 12)\n\nx <- transform_resize(x, size = c(268, 268))\ny <- transform_resize(\n y,\n size = c(268, 268),\n interpolation = 0\n)\n\nx <- transform_rotate(x, angle)\ny <- transform_rotate(y, angle, resample = 0)\n\nx <- transform_center_crop(x, size = c(224, 224))\ny <- transform_center_crop(y, size = c(224, 224))\n```", "```r\ntrain_ds <- pet_dataset(root = dir,\n split = \"train\",\n size = c(224, 224),\n augmentation = augmentation)\n\nvalid_ds <- pet_dataset(root = dir,\n split = \"valid\",\n size = c(224, 224))\n```", "```r\ntrain_dl <- dataloader(\n train_ds,\n batch_size = 32,\n shuffle = TRUE\n)\nvalid_dl <- dataloader(valid_ds, batch_size = 32)\n\nmodel <- model %>%\n setup(\n optimizer = optim_adam,\n loss = nn_cross_entropy_loss()\n )\n\nrates_and_losses <- model %>% lr_finder(train_dl)\nrates_and_losses %>% plot()\n```", "```r\nfitted <- model %>%\n fit(train_dl, epochs = 20, valid_data = valid_dl,\n callbacks = list(\n luz_callback_early_stopping(patience = 2),\n luz_callback_lr_scheduler(\n lr_one_cycle,\n max_lr = 0.01,\n epochs = 20,\n steps_per_epoch = length(train_dl),\n call_on = \"on_batch_end\")\n ),\n verbose = TRUE)\n```", "```r\nEpoch 1/20\nTrain metrics: Loss: 0.6782                                               \nValid metrics: Loss: 0.4433\nEpoch 2/20\nTrain metrics: Loss: 0.3705\nValid metrics: Loss: 0.3331\nEpoch 3/20\nTrain metrics: Loss: 0.315\nValid metrics: Loss: 0.2999\n...\n...\nEpoch 11/20\nTrain metrics: Loss: 0.1803\nValid metrics: Loss: 0.2161\nEpoch 12/20\nTrain metrics: Loss: 0.1751\nValid metrics: Loss: 0.2191\nEpoch 13/20\nTrain metrics: Loss: 0.1708\nValid metrics: Loss: 0.2203\nEarly stopping at epoch 13 of 20\n```", "```r\nlibrary(raster)\n```", "```r\nvalid_ds_4plot <- pet_dataset(\n root = dir,\n split = \"valid\",\n size = c(224, 224),\n normalize = FALSE\n)\n```", "```r\nindices <- 1:8\n\npreds <- predict(\n fitted,\n dataloader(dataset_subset(valid_ds, indices))\n)\n\npng(\n \"pet_segmentation.png\",\n width = 1200,\n height = 600,\n bg = \"black\"\n)\n\npar(mfcol = c(2, 4), mar = rep(2, 4))\n\nfor (i in indices) {\n mask <- as.array(\n torch_argmax(preds[i, ..], 1)$to(device = \"cpu\")\n )\n mask <- raster::ratify(raster::raster(mask))\n\n img <- as.array(valid_ds_4plot[i][[1]]$permute(c(2, 3, 1)))\n cond <- img > 0.99999\n img[cond] <- 0.99999\n img <- raster::brick(img)\n\n # plot image\n raster::plotRGB(img, scale = 1, asp = 1, margins = TRUE)\n # overlay mask\n plot(\n mask,\n alpha = 0.4,\n legend = FALSE,\n axes = FALSE,\n add = TRUE\n )\n}\n\ndev.off()\n```"]