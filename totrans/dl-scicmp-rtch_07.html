<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>4  Autograd</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>4  Autograd</h1>
<blockquote>原文：<a href="https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/autograd.html">https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/autograd.html</a></blockquote>

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">

</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>In the last chapter, we have seen how to manipulate tensors, and encountered a sample of mathematical operations one can perform on them. If those operations, numerous though they may be, were all there was to core <code>torch</code>, you would not be reading this book. Frameworks like <code>torch</code> are so popular because of what you can do with them: deep learning, machine learning, optimization, large-scale scientific computation in general. Most of these application areas involve minimizing some <em>loss function</em>. This, in turn, entails computing function <em>derivatives</em>. Now imagine that as a user, you had to specify the functional form of each derivative yourself. Especially with neural networks, this could get cumbersome pretty fast!</p>
<p>In fact, <code>torch</code> does not work out, nor store, functional representations of derivatives either. Instead, it implements what is called <em>automatic differentiation</em>. In automatic differentiation, and more specifically, its often-used <em>reverse-mode</em> variant, derivatives are computed and combined on a <em>backward pass</em> through the graph of tensor operations. We’ll look at an example for this in a minute. But first, let’s quickly backtrack and talk about <em>why</em> we would want to compute derivatives.</p>
<section id="why-compute-derivatives" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="why-compute-derivatives"><span class="header-section-number">4.1</span> Why compute derivatives?</h2>
<p>In supervised machine learning, we have at our disposal a <em>training set</em>, where the variable we’re hoping to predict is known. This is the target, or <em>ground truth</em>. We now develop and train a prediction algorithm, based on a set of input variables, the <em>predictors</em>. This training, or learning, process, is based on comparing the algorithm’s predictions with the ground truth, a comparison that leads to a number capturing how good or bad the current predictions are. To provide this number is the job of the <em>loss function</em>.</p>
<p>Once it is aware of the current loss, an algorithm can adjust its parameters – the <em>weights</em>, in a neural network – in order to deliver better predictions. It just has to know in which direction to adjust them. This information is made available by the <em>gradient</em>, the vector of derivatives.</p>
<p>As an example, we imagine a loss function that looks like this (<a href="#fig-autograd-paraboloid">fig. <span>4.1</span></a>):</p>
<div id="fig-autograd-paraboloid" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="../Images/47bb9e9ec02e8449deecd5cf858a4714.png" class="quarto-discovered-preview-image img-fluid figure-img" alt="A paraboloid in two dimensions that has a minimum at (0,0)." data-original-src="https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/images/autograd-paraboloid.png"/></p>
<p/><figcaption class="figure-caption">Figure 4.1: Hypothetical loss function (a paraboloid).</figcaption><p/>
</figure>
</div>
<p>This is a quadratic function of two variables: <span class="math inline">\(f(x_1, x_2) = 0.2 {x_1}^2 + 0.2 {x_2}^2 - 5\)</span>. It has its minimum at <code>(0,0)</code>, and this is the point we’d like to be at. As humans, standing at the location designated by the white dot, and looking at the landscape, we have a pretty clear idea how to go downhill fast (assuming we’re not scared by the slope). To find the best direction computationally, however, we compute the gradient.</p>
<p>Take the <span class="math inline">\(x_1\)</span> direction. The derivative of the function with respect to <span class="math inline">\(x_1\)</span> indicates how its value varies as <span class="math inline">\(x_1\)</span> varies. As we know the function in closed form, we can compute that: <span class="math inline">\(\frac{\partial f}{\partial x_1} = 0.4 x_1\)</span>. This tells us that as <span class="math inline">\(x_1\)</span> increases, loss increases, and how fast. But we want loss to <em>decrease</em>, so we have to go in the opposite direction.</p>
<p>The same holds for the <span class="math inline">\(x_2\)</span>-axis. We compute the derivative (<span class="math inline">\(\frac{\partial f}{\partial x_2} = 0.4 x_2\)</span>). Again, we want to take the direction opposite to where the derivative points. Overall, this yields a descent direction of <span class="math inline">\(\begin{bmatrix}-0.4x_1\\-0.4x_2 \end{bmatrix}\)</span>.</p>
<p>Descriptively, this strategy is called <em>steepest descent</em>. Commonly referred to as <em>gradient descent</em>, it is the most basic optimization algorithm in deep learning. Perhaps unintuitively, it is not always the most efficient way. And there’s another question: Can we assume that this direction, computed at the starting point, will remain optimal as we continue descending? Maybe we’d better regularly recompute directions instead? Questions like this will be addressed in later chapters.</p>
</section>
<section id="automatic-differentiation-example" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="automatic-differentiation-example"><span class="header-section-number">4.2</span> Automatic differentiation example</h2>
<p>Now that we know why we need derivatives, let’s see how automatic differentiation (AD) would compute them.</p>
<p>This (<a href="#fig-autograd-compgraph">fig. <span>4.2</span></a>) is how our above function could be represented in a computational graph. <code>x1</code> and <code>x2</code> are input nodes, corresponding to function parameters <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>. <code>x7</code> is the function’s output; all other nodes are intermediate ones, necessary to ensure correct order of execution. (We could have given the constants, <code>-5</code> , <code>0.2</code>, and <code>2</code>, their own nodes as well; but as they’re remaining, well, constant anyway, we’re not too interested in them and prefer having a simpler graph.)</p>
<div id="fig-autograd-compgraph" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="../Images/a587b80614fb71733a2610b278fbd9ec.png" class="img-fluid figure-img" alt="A directed graph where nodes represent data, and arrows, mathematical operations. There are two input nodes, four intermediate nodes, and one output node. Operations used are exponentiation, multiplication, and addition." data-original-src="https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/images/autograd-compgraph.png"/></p>
<p/><figcaption class="figure-caption">Figure 4.2: Example of a computational graph.</figcaption><p/>
</figure>
</div>
<p>In reverse-mode AD, the flavor of automatic differentiation implemented by <code>torch</code>, the first thing that happens is to calculate the function’s output value. This corresponds to a forward pass through the graph. Then, a backward pass is performed to calculate the gradient of the output with respect to both inputs, <code>x1</code> and <code>x2</code>. In this process, information becomes available, and is built up, from the right:</p>
<ul>
<li><p>At <code>x7</code>, we calculate partial derivatives with respect to <code>x5</code> and <code>x6</code>. Basically, the equation to differentiate looks like this: <span class="math inline">\(f(x_5, x_6) = x_5 + x_6 - 5\)</span>. Thus, both partial derivatives are 1.</p></li>
<li><p>From <code>x5</code>, we move to the left to see how it depends on <code>x3</code>. We find that <span class="math inline">\(\frac{\partial x_5}{\partial x_3} = 0.2\)</span>. At this point, applying the chain rule of calculus, we already know how the output depends on <code>x3</code>: <span class="math inline">\(\frac{\partial f}{\partial x_3} = 0.2 * 1 = 0.2\)</span>.</p></li>
<li><p>From <code>x3</code>, we take the final step to <code>x</code>. We learn that <span class="math inline">\(\frac{\partial x_3}{\partial x_1} = 2 x_1\)</span>. Now, we again apply the chain rule, and are able to formulate how the function depends on its first input: <span class="math inline">\(\frac{\partial f}{\partial x_1} = 2 x_1 * 0.2 * 1 = 0.4 x_1\)</span>.</p></li>
<li><p>Analogously, we determine the second partial derivative, and thus, already have the gradient available: <span class="math inline">\(\nabla f = \frac{\partial f}{\partial x_1} + \frac{\partial f}{\partial x_2} = 0.4 x_1 + 0.4 x_2\)</span>.</p></li>
</ul>
<p>That is the principle. In practice, different frameworks implement reverse-mode automatic differentiation differently. We’ll catch a glimpse of how <code>torch</code> does it in the next section.</p>
</section>
<section id="automatic-differentiation-with-torch-autograd" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="automatic-differentiation-with-torch-autograd"><span class="header-section-number">4.3</span> Automatic differentiation with <code>torch</code> <em>autograd</em></h2>
<p>First, a quick note on terminology. In <code>torch</code>, the AD engine is usually referred to as <em>autograd</em>, and that is the way you’ll see it denoted in most of the rest of this book. Now, back to the task.</p>
<p>To construct the above computational graph with <code>torch</code>, we create “source” tensors <code>x1</code> and <code>x2</code>. These will mimic the parameters whose impact we’re interested in. However, if we just proceed “as usual”, creating the tensors the way we’ve been doing so far, <code>torch</code> will not prepare for AD. Instead, we need to pass in <code>requires_grad = TRUE</code> when instantiating those tensors:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"/><span class="fu">library</span>(torch)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"/></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"/>x1 <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(<span class="dv">2</span>, <span class="at">requires_grad =</span> <span class="cn">TRUE</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"/>x2 <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(<span class="dv">2</span>, <span class="at">requires_grad =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>(By the way, the value <code>2</code> for both tensors was chosen completely arbitrarily.)</p>
<p>Now, to create “invisible” nodes <code>x3</code> to <code>x6</code> , we square and multiply accordingly. Then <code>x7</code> stores the final result.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"/>x3 <span class="ot">&lt;-</span> x1<span class="sc">$</span><span class="fu">square</span>()</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"/>x5 <span class="ot">&lt;-</span> x3 <span class="sc">*</span> <span class="fl">0.2</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"/></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"/>x4 <span class="ot">&lt;-</span> x2<span class="sc">$</span><span class="fu">square</span>()</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"/>x6 <span class="ot">&lt;-</span> x4 <span class="sc">*</span> <span class="fl">0.2</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"/></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"/>x7 <span class="ot">&lt;-</span> x5 <span class="sc">+</span> x6 <span class="sc">-</span> <span class="dv">5</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"/>x7</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<pre><code>torch_tensor
-3.4000
[ CPUFloatType{1} ][ grad_fn = &lt;SubBackward1&gt; ]</code></pre>
<p>Note that we have to add <code>requires_grad = TRUE</code> when creating the “source” tensors only. All dependent nodes in the graph inherit this property. For example:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"/>x7<span class="sc">$</span>requires_grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<pre><code>[1] TRUE</code></pre>
<p>Now, all prerequisites are fulfilled to see automatic differentiation at work. All we need to do to determine how <code>x7</code> depends on <code>x1</code> and <code>x2</code> is call <code>backward()</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"/>x7<span class="sc">$</span><span class="fu">backward</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>Due to this call, the <code>$grad</code> fields have been populated in <code>x1</code> and <code>x2</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"/>x1<span class="sc">$</span>grad</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"/>x2<span class="sc">$</span>grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<pre><code> 0.8000
[ CPUFloatType{1} ]
torch_tensor
 0.8000
[ CPUFloatType{1} ]</code></pre>
<p>These are the partial derivatives of <code>x7</code> with respect to <code>x1</code> and <code>x2</code>, respectively. Conforming to our manual calculations above, both amount to 0.8, that is, 0.4 times the tensor values 2 and 2.</p>
<p>How about the accumulation process we said was needed to build up those end-to-end derivatives? Can we “follow” the end-to-end derivative as it’s being built up? For example, can we see how the final output depends on <code>x3</code>?</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"/>x3<span class="sc">$</span>grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<pre><code>[W TensorBody.h:470] Warning: The .grad attribute of a Tensor
that is not a leaf Tensor is being accessed. Its .grad attribute
won't be populated during autograd.backward().
If you indeed want the .grad field to be populated for a 
non-leaf Tensor, use .retain_grad() on the non-leaf Tensor.[...]

torch_tensor
[ Tensor (undefined) ]</code></pre>
<p>The field does not seem to be populated. In fact, while it <em>has</em> to compute them, <code>torch</code> throws away the intermediate aggregates once they are no longer needed, to save memory. We can, however, ask it to keep them, using <code>retain_grad = TRUE</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"/>x3 <span class="ot">&lt;-</span> x1<span class="sc">$</span><span class="fu">square</span>()</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"/>x3<span class="sc">$</span><span class="fu">retain_grad</span>()</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"/></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"/>x5 <span class="ot">&lt;-</span> x3 <span class="sc">*</span> <span class="fl">0.2</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"/>x5<span class="sc">$</span><span class="fu">retain_grad</span>()</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"/></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"/>x4 <span class="ot">&lt;-</span> x2<span class="sc">$</span><span class="fu">square</span>()</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"/>x4<span class="sc">$</span><span class="fu">retain_grad</span>()</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"/></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"/>x6 <span class="ot">&lt;-</span> x4 <span class="sc">*</span> <span class="fl">0.2</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"/>x6<span class="sc">$</span><span class="fu">retain_grad</span>()</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"/></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"/>x7 <span class="ot">&lt;-</span> x5 <span class="sc">+</span> x6 <span class="sc">-</span> <span class="dv">5</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"/>x7<span class="sc">$</span><span class="fu">backward</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>Now, we find that <code>x3</code>’s <code>grad</code> field <em>is</em> populated:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"/>x3<span class="sc">$</span>grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<pre><code>torch_tensor
 0.2000
[ CPUFloatType{1} ]</code></pre>
<p>The same goes for <code>x4</code>, <code>x5</code>, and <code>x6</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"/>x4<span class="sc">$</span>grad</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"/>x5<span class="sc">$</span>grad</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"/>x6<span class="sc">$</span>grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<pre><code>torch_tensor
 0.2000
[ CPUFloatType{1} ]
torch_tensor
 1
[ CPUFloatType{1} ]
torch_tensor
 1
[ CPUFloatType{1} ]</code></pre>
<p>There is one remaining thing we might be curious about. We’ve managed to catch a glimpse of the gradient-accumulation process from the “running gradient” point of view, in a sense; but how about the individual derivatives that need to be taken in order to proceed with accumulation? For example, what <code>x3$grad</code> tells us is how the output depends on the intermediate state at <code>x3</code>; how do we get from there to <code>x1</code>, the actual input node?</p>
<p>It turns out that of that aspect, too, we can get an idea. During the forward pass, <code>torch</code> already takes a note on what it will have to do, later, to calculate the individual derivatives. This “recipe” is stored in a tensor’s <code>grad_fn</code> field. For <code>x3</code>, this adds the “missing link” to <code>x1</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"/>x3<span class="sc">$</span>grad_fn</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<pre><code>PowBackward0</code></pre>
<p>The same works for <code>x4</code>, <code>x5</code>, and <code>x6</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"/>x4<span class="sc">$</span>grad_fn</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"/>x5<span class="sc">$</span>grad_fn</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"/>x6<span class="sc">$</span>grad_fn</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<pre><code>PowBackward0
MulBackward1
MulBackward1</code></pre>
<p>And there we are! We’ve seen how <code>torch</code> computes derivatives for us, and we’ve even caught a glimpse of how it does it. Now, we are ready to play around with our first two applied tasks.</p>


</section>

    
</body>
</html>