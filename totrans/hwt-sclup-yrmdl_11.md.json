["```py\nimport jax\nimport jax.numpy as jnp\n\n# Running on an TPU v5e 4x2\\. This assigns names to the two physical axes of the hardware. mesh = jax.make_mesh(axis_shapes=(4, 2), axis_names=('X', 'Y'))\n\n# This tells JAX to use this mesh for all operations, so you can just specify the PartitionSpec P. jax.set_mesh(mesh)\n\n# We create a matrix W and input activations In sharded across our devices. In = jnp.zeros((8, 2048), dtype=jnp.bfloat16, device=jax.NamedSharding(mesh, jax.P('X', 'Y')))\nW = jnp.zeros((2048, 8192), dtype=jnp.bfloat16, device=jax.NamedSharding(mesh, jax.P('Y', None)))\n\ndef matmul_square(In, W):\n  return jnp.einsum('bd,df->bf', jnp.square(In), W)\n\n# We can explicitly compile the sharded matmul function here. This adds all the\n# necessary comms (e.g. an AllReduce after the matmul). jit_matmul = jax.jit(matmul_square, out_shardings=jax.P('X', None)).lower(In, W).compile()\n\nout = jit_matmul(In, W) \n```", "```py\n# This fusion is the actual matmul of the sharded inputs and matrix %fusion = bf16[2,8192]{1,0:T(4,128)(2,1)S(1)} fusion(bf16[2,1024]{1,0:T(4,128)(2,1)} %param, bf16[8192,1024]{1,0:T(8,128)(2,1)S(1)} %copy-done)\n\n# We reduce the partially summed results across devices ROOT %AllReduce = bf16[2,8192]{1,0:T(4,128)(2,1)} AllReduce(bf16[2,8192]{1,0:T(4,128)(2,1)S(1)} %fusion) \n```", "```py\nimport jax\nimport jax.numpy as jnp\n\nmesh = jax.make_mesh((4, 2), ('X', 'Y'))\n\ndef matmul(x, Win, Wout):\n  hidden = jnp.einsum('bd,df->bf', x, Win)\n  hidden = jax.lax.with_sharding_constraint(hidden, jax.P('x', 'y'))\n  return jnp.einsum('bf,df->bd', hidden, Wout) \n```", "```py\nimport jax\nimport jax.numpy as jnp\nimport jax.sharding as shd\n\n# Running on an TPU v5e 2x2\\. This assigns names to the two physical axes of the hardware. mesh = jax.make_mesh(axis_shapes=(2, 2), axis_names=('X', 'Y'),\n                                       axis_types=(shd.AxisType.Explicit, shd.AxisType.Explicit))\n\n# This tells JAX to use this mesh for all operations, so you can just specify the PartitionSpec P. jax.set_mesh(mesh)\n\nx = jax.device_put(np.arange(16).reshape(8, 2), P('X', 'Y'))\n\n@jax.jit\ndef f(x):\n  print(jax.typeof(x))  # bfloat16[8@X,2@Y]\n  out = x * 2\n  print(jax.typeof(out))  # bfloat16[8@X,2@Y]\n  return out\n\nf(x) \n```", "```py\n# We create a matrix W and input activations In sharded across our devices. In = jnp.zeros((8, 2048), dtype=jnp.bfloat16, out_sharding=jax.P('X', 'Y'))\nW = jnp.zeros((2048, 8192), dtype=jnp.bfloat16, out_sharding=jax.P('Y', None))\n\n@jax.jit\ndef matmul_square(In, W):\n  print(jax.typeof(In))  # bfloat16[8@X, 2048@Y]\n  print(jax.typeof(W))  # bfloat16[2048@Y, 8192]\n  return jnp.einsum('bd,df->bf', jnp.square(In), W)\n\nmatmul_square(In, W)  # This will error \n```", "```py\n@jax.jit\ndef matmul_square(In, W):\n  return jnp.einsum('bd,df->bf', jnp.square(In), W, out_sharding=P('X', 'Y'))\n\nout = matmul_square(In, W)\nprint(jax.typeof(out))  # bfloat16[8@X,8192@Y] \n```", "```py\nimport jax\nimport jax.numpy as jnp\nimport jax.sharding as shd\n\nmesh = jax.make_mesh((2, 4), ('x', 'y'), (shd.AxisType.Explicit, shd.AxisType.Explicit))\njax.set_mesh(mesh)\n\nx = jnp.arange(0, 512, dtype=jnp.int32, out_sharding=P(('x', 'y')))\n\n# This function will operate on 1/8th of the array. @jax.shard_map(in_specs=P(('x', 'y')), out_specs=P())\ndef slice_and_average(x):\n  assert x.shape == (512 // 8,)\n  return jax.lax.pmean(x[:4], axis_name=('x', 'y'))\n\nout = slice_and_average(x)\nassert out.shape == (4,) \n```", "```py\nimport functools\n\nimport jax\nimport jax.numpy as jnp\nimport jax.sharding as shd\nimport numpy as np\n\n# This is intended to run on a TPU v5e-8 runtime. If you can't get this,\n# try setting jax.config.update('jax_num_cpu_devices', 8).\n# mesh = jax.make_mesh(axis_shapes=(2, 4), axis_names=('X', 'Y'),\n                                       axis_types=(shd.AxisType.Explicit, shd.AxisType.Explicit))\njax.set_mesh(mesh)\n\nB, D, F = 1024, 2048, 8192\nA = jnp.arange(np.prod((B, D))).reshape((B, D))\nW = jnp.arange(np.prod((D, F))).reshape((D, F))\n\nA = jax.device_put(A, jax.P('X', 'Y'))\nW = jax.device_put(W, jax.P(None, 'Y'))\n\n@functools.partial(jax.jit, out_shardings=jax.P('X', 'Y'))\ndef matmul(lhs, rhs):\n  return lhs @ rhs\n\ndef collective_matmul_allgather_lhs_contracting(lhs, rhs):\n  # lhs is the looped operand; rhs is the local operand\n  axis_size = jax.lax.axis_size('Y')  # axis_size = 4 for this example\n  idx = jax.lax.axis_index('Y')\n\n  chunk_size = lhs.shape[1]\n  assert rhs.shape[0] % chunk_size == 0\n\n  def f(i, carrys):\n    accum, lhs = carrys\n    rhs_chunk = jax.lax.dynamic_slice_in_dim(rhs, (idx + i) % axis_size * chunk_size, chunk_size)\n    # Matmul for a chunk\n    update = lhs @ rhs_chunk\n    # Circular shift to the left\n    lhs = jax.lax.ppermute(\n        lhs,\n        axis_name='Y',\n        perm=[(j, (j - 1) % axis_size) for j in range(axis_size)]\n    )\n    return accum + update, lhs\n\n  accum = jnp.zeros((lhs.shape[0], rhs.shape[1]), dtype=lhs.dtype)\n  accum = jax.lax.pvary(accum, ('X', 'Y'))\n  accum, lhs = jax.lax.fori_loop(0, axis_size - 1, f, (accum, lhs), unroll=True)\n\n  # Compute the last chunk after the final permute to leave lhs in the state we found it\n  i = axis_size - 1\n  rhs_chunk = jax.lax.dynamic_slice_in_dim(rhs, (idx + i) % axis_size * chunk_size, chunk_size)\n  update = lhs @ rhs_chunk\n  return accum + update\n\njit_sharded_f = jax.jit(jax.shard_map(\n  collective_matmul_allgather_lhs_contracting,\n  in_specs=(jax.P('X', 'Y'), jax.P(None, 'Y')), out_specs=jax.P('X', 'Y')))\n\nshmapped_out = jit_sharded_f(A, W)\nexpected_out = matmul(A, W)\n\nnp.testing.assert_array_equal(shmapped_out, expected_out) \n```", "```py\nimport numpy as np\n\nimport jax\nimport jax.numpy as jnp\n\nP = jax.sharding.PartitionSpec\n\nmesh = jax.make_mesh((4, 2), ('X','Y'))\n\naverage_shmap = jax.shard_map(\n    lambda x: x.mean(keepdims=True),\n    mesh=mesh,\n    in_specs=P('X','Y'), out_specs=P('X','Y')\n)\n\ndef average(x):\n  X, Y = mesh.axis_sizes\n  return x.reshape(X, x.shape[0] // X, Y, x.shape[1] // Y).mean(axis=(1, 3))\n\naverage_jit = jax.jit(average, out_shardings=jax.NamedSharding(mesh, P('X','Y')))\n\nx = jnp.arange(8 * 64 * 8, dtype=jnp.int32).reshape(8 * 64, 8)\nx = jax.device_put(x, jax.NamedSharding(mesh, P('X','Y')))\n\ny1 = average_shmap(x)\ny2 = average_jit(x)\n\nnp.testing.assert_array_equal(y1, y2) \n```", "```py\nimport numpy as np\n\nimport jax\nimport jax.numpy as jnp\n\nimport functools\n\nP = jax.sharding.PartitionSpec\n\nmesh = jax.make_mesh((4, 2), ('X','Y'))\n\ndef shift_shmap(x, shift: int):\n  shmapped = jax.shard_map(\n      lambda x: jnp.roll(x, shift, axis=0),\n      mesh=mesh,\n      in_specs=P('X','Y'), out_specs=P('X','Y')\n  )\n  return shmapped(x)\n\n@functools.partial(jax.jit, static_argnames=['shift'], out_shardings=jax.NamedSharding(mesh, P('X','Y')))\ndef shift_jit(x, shift: int):\n  X, Y = mesh.axis_sizes\n  reshaped = x.reshape(X, x.shape[0] // X, -1)\n  return jnp.roll(reshaped, shift, axis=1).reshape(x.shape[0], x.shape[1])\n\nx = jnp.arange(8 * 64 * 8, dtype=jnp.int32).reshape(8 * 64, 8)\nx = jax.device_put(x, jax.NamedSharding(mesh, P('X','Y')))\n\ny1 = shift_shmap(x, 5)\ny2 = shift_jit(x, 5)\n\nnp.testing.assert_array_equal(y1, y2) \n```", "```py\ndef moe_local(W: jnp.ndarray, A: jnp.ndarray, B: jnp.ndarray) -> jnp.ndarray:\n    S, _ = A.shape\n    E, _, F = W.shape\n\n    def expert_forward(carry, e):\n        output = carry  # [S, F]\n        mask = (B == e)[:, None]  # [S, 1]\n        expert_result = A @ W[e]  # [S, F] - this expert's transform of ALL tokens\n        output = output + expert_result * mask  # Only keep results for assigned tokens\n        return output, None\n\n    output = jnp.zeros((S, F))\n    output, _ = lax.scan(expert_forward, output, jnp.arange(E))\n\n    return output \n```", "```py\nchunk_size = 128\ndef matmul(W, x, B):\n  i = 0\n  x = # sort x according to assignments\n  while (chunk := x[i:i+chunk_size].any()):\n     chunk = all_to_all(chunk)\n     out = matmul_local(W, chunk)\n  return concat(out) \n```", "```py\n Austin et al., \"How to Scale Your Model\", Google DeepMind, online, 2025. \n```", "```py\n @article{scaling-book,\n      title = {How to Scale Your Model},\n      author = {Austin, Jacob and Douglas, Sholto and Frostig, Roy and Levskaya, Anselm and Chen, Charlie and Vikram, Sharad\n      and Lebron, Federico and Choy, Peter and Ramasesh, Vinay and Webson, Albert and Pope, Reiner},\n      publisher = {Google DeepMind},\n      howpublished = {Online},\n      note = {Retrieved from https://jax-ml.github.io/scaling-book/},\n      year = {2025}\n    } \n```"]