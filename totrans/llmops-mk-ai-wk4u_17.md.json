["```py\n`import  os from  openai  import OpenAI from  dotenv  import load_dotenv import  datetime  # Load environment variables and configure the OpenAI API key load_dotenv() client = OpenAI()  # Configure LLM versioning current_date = datetime.datetime.now().date() llm_name = \"gpt-3.5-turbo\" print(f\"Using LLM version: {llm_name}\")` \n```", "```py\n`# Import the vector store and embedding generator from  langchain.vectorstores  import Chroma from  langchain_openai  import OpenAIEmbeddings  # Directory where the vector database persists its data documents_storage_directory = 'docs/chroma/'  # Initialize the embedding generator using OpenAI embeddings embeddings_generator = OpenAIEmbeddings()  # Initialize the vector database with the persistence directory and embedding function vector_database = Chroma(persist_directory=documents_storage_directory, embedding_function=embeddings_generator)  # Show the current number of documents in the vector database print(f\"Documents in VectorDB: {vector_database._collection.count()}\")` \n```", "```py\n`from  langchain_openai  import ChatOpenAI  # Initialize the chat model with the selected LLM language_model = ChatOpenAI(model=llm_name, temperature=0)` \n```", "```py\n`# Import required LangChain modules from  langchain.chains  import RetrievalQA from  langchain.prompts  import PromptTemplate  # Create a custom prompt template to guide the LLM to use the provided context effectively custom_prompt_template = \"\"\"To better assist with the inquiry, consider the details provided below as your reference... {context} Inquiry: {question} Insightful Response:\"\"\"  # Initialize the RetrievalQA chain with the custom prompt a_question_answering_chain = RetrievalQA.from_chain_type(     language_model,     retriever=vector_database.as_retriever(),     return_source_documents=True,     chain_type_kwargs={\"prompt\": PromptTemplate.from_template(custom_prompt_template)} )` \n```", "```py\n`# Provide a sample query query = \"Is probability a class topic?\" response = a_question_answering_chain({\"query\": query}) print(\"Answer:\", response[\"result\"])` \n```", "```py\n`# Configure a QA chain to use MapReduce, aggregating answers from multiple documents question_answering_chain_map_reduce = RetrievalQA.from_chain_type(     language_model,     retriever=vector_database.as_retriever(),     chain_type=\"map_reduce\" )  # Run MapReduce with the user query response_map_reduce = question_answering_chain_map_reduce({\"query\": query})  # Show the aggregated answer print(\"MapReduce answer:\", response_map_reduce[\"result\"])  # Configure a QA chain to use Refine, which iteratively improves the answer question_answering_chain_refine = RetrievalQA.from_chain_type(     language_model,     retriever=vector_database.as_retriever(),     chain_type=\"refine\" )  # Run Refine with the same user query response_refine = question_answering_chain_refine({\"query\": query})  # Show the refined answer print(\"Refine answer:\", response_refine[\"result\"])` \n```", "```py\n`# Import a QA chain from a hypothetical library from  some_library  import question_answering_chain as qa_chain  # Define an initial question related to course content initial_question_about_course_content = \"Does the curriculum cover probability theory?\" # Generate an answer to the initial question response_to_initial_question = qa_chain({\"query\": initial_question_about_course_content})  # Define a follow‑up question without explicitly preserving conversation context follow_up_question_about_prerequisites = \"Why are those prerequisites important?\" # Generate an answer to the follow‑up question response_to_follow_up_question = qa_chain({\"query\": follow_up_question_about_prerequisites})  # Display both answers — initial and follow‑up print(\"Answer to the initial question:\", response_to_initial_question[\"result\"]) print(\"Answer to the follow‑up question:\", response_to_follow_up_question[\"result\"])` \n```"]