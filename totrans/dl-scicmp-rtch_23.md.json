["```r\nlibrary(torch)\nlibrary(torchvision)\nlibrary(torchdatasets)\nlibrary(luz)\n\nset.seed(777)\ntorch_manual_seed(777)\n\ndir <- \"~/.torch-datasets\"\n\ntrain_ds <- tiny_imagenet_dataset(\n dir,\n download = TRUE,\n transform = . %>%\n transform_to_tensor() %>%\n transform_random_affine(\n degrees = c(-30, 30), translate = c(0.2, 0.2)\n ) %>%\n transform_normalize(\n mean = c(0.485, 0.456, 0.406),\n std = c(0.229, 0.224, 0.225)\n )\n)\n\nvalid_ds <- tiny_imagenet_dataset(\n dir,\n split = \"val\",\n transform = function(x) {\n x %>%\n transform_to_tensor() %>%\n transform_normalize(\n mean = c(0.485, 0.456, 0.406),\n std = c(0.229, 0.224, 0.225))\n }\n)\n\ntrain_dl <- dataloader(\n train_ds,\n batch_size = 128,\n shuffle = TRUE\n)\nvalid_dl <- dataloader(valid_ds, batch_size = 128)\n```", "```r\nconvnet <- nn_module(\n \"convnet\",\n initialize = function() {\n self$features <- nn_sequential(\n nn_conv2d(3, 64, kernel_size = 3, padding = 1),\n nn_relu(),\n nn_max_pool2d(kernel_size = 2),\n nn_dropout2d(p = 0.05),\n nn_conv2d(64, 128, kernel_size = 3, padding = 1),\n nn_relu(),\n nn_max_pool2d(kernel_size = 2),\n nn_dropout2d(p = 0.05),\n nn_conv2d(128, 256, kernel_size = 3, padding = 1),\n nn_relu(),\n nn_max_pool2d(kernel_size = 2),\n nn_dropout2d(p = 0.05),\n nn_conv2d(256, 512, kernel_size = 3, padding = 1),\n nn_relu(),\n nn_max_pool2d(kernel_size = 2),\n nn_dropout2d(p = 0.05),\n nn_conv2d(512, 1024, kernel_size = 3, padding = 1), \n nn_relu(),\n nn_adaptive_avg_pool2d(c(1, 1)),\n nn_dropout2d(p = 0.05),\n )\n self$classifier <- nn_sequential(\n nn_linear(1024, 1024),\n nn_relu(),\n nn_dropout(p = 0.05),\n nn_linear(1024, 1024),\n nn_relu(),\n nn_dropout(p = 0.05),\n nn_linear(1024, 200)\n )\n },\n forward = function(x) {\n x <- self$features(x)$squeeze()\n x <- self$classifier(x)\n x\n }\n)\n```", "```r\nmodel <- convnet %>%\n setup(\n loss = nn_cross_entropy_loss(),\n optimizer = optim_adam,\n metrics = list(\n luz_metric_accuracy()\n )\n ) \n\nrates_and_losses <- model %>% lr_finder(train_dl)\nrates_and_losses %>% plot()\n```", "```r\nfitted <- model %>%\n fit(train_dl, epochs = 50, valid_data = valid_dl,\n callbacks = list(\n luz_callback_early_stopping(patience = 2),\n luz_callback_lr_scheduler(\n lr_one_cycle,\n max_lr = 0.01,\n epochs = 50,\n steps_per_epoch = length(train_dl),\n call_on = \"on_batch_end\"),\n luz_callback_model_checkpoint(path = \"cpt_dropout/\"),\n luz_callback_csv_logger(\"logs_dropout.csv\")\n ),\n verbose = TRUE)\n```", "```r\nEpoch 1/50\nTrain metrics: Loss: 5.116 - Acc: 0.0128                                      \nValid metrics: Loss: 4.9144 - Acc: 0.0217\nEpoch 2/50\nTrain metrics: Loss: 4.7217 - Acc: 0.042                                      \nValid metrics: Loss: 4.4143 - Acc: 0.067\nEpoch 3/50\nTrain metrics: Loss: 4.3681 - Acc: 0.0791                                     \nValid metrics: Loss: 4.1145 - Acc: 0.105\n...\n...\nEpoch 33/50\nTrain metrics: Loss: 2.3006 - Acc: 0.4304                                     \nValid metrics: Loss: 2.5863 - Acc: 0.4025\nEpoch 34/50\nTrain metrics: Loss: 2.2717 - Acc: 0.4365                                     \nValid metrics: Loss: 2.6377 - Acc: 0.3889\nEpoch 35/50\nTrain metrics: Loss: 2.2456 - Acc: 0.4402                                     \nValid metrics: Loss: 2.6208 - Acc: 0.4043\nEarly stopping at epoch 35 of 50\n```", "```r\nconvnet <- nn_module(\n \"convnet\",\n initialize = function() {\n self$features <- nn_sequential(\n nn_conv2d(3, 64, kernel_size = 3, padding = 1),\n nn_batch_norm2d(64),\n nn_relu(),\n nn_max_pool2d(kernel_size = 2),\n nn_conv2d(64, 128, kernel_size = 3, padding = 1),\n nn_batch_norm2d(128),\n nn_relu(),\n nn_max_pool2d(kernel_size = 2),\n nn_conv2d(128, 256, kernel_size = 3, padding = 1),\n nn_batch_norm2d(256),\n nn_relu(),\n nn_max_pool2d(kernel_size = 2),\n nn_conv2d(256, 512, kernel_size = 3, padding = 1),\n nn_batch_norm2d(512),\n nn_relu(),\n nn_max_pool2d(kernel_size = 2),\n nn_conv2d(512, 1024, kernel_size = 3, padding = 1), \n nn_batch_norm2d(1024),\n nn_relu(),\n nn_adaptive_avg_pool2d(c(1, 1)),\n )\n self$classifier <- nn_sequential(\n nn_linear(1024, 1024),\n nn_relu(),\n nn_batch_norm1d(1024),\n nn_linear(1024, 1024),\n nn_relu(),\n nn_batch_norm1d(1024),\n nn_linear(1024, 200)\n )\n },\n forward = function(x) {\n x <- self$features(x)$squeeze()\n x <- self$classifier(x)\n x\n }\n)\n```", "```r\nmodel <- convnet %>%\n setup(\n loss = nn_cross_entropy_loss(),\n optimizer = optim_adam,\n metrics = list(\n luz_metric_accuracy()\n )\n ) \n\nrates_and_losses <- model %>% lr_finder(train_dl)\nrates_and_losses %>% plot()\n```", "```r\nfitted <- model %>%\n fit(train_dl, epochs = 50, valid_data = valid_dl,\n callbacks = list(\n luz_callback_early_stopping(patience = 2),\n luz_callback_lr_scheduler(\n lr_one_cycle,\n max_lr = 0.001,\n epochs = 50,\n steps_per_epoch = length(train_dl),\n call_on = \"on_batch_end\"),\n luz_callback_model_checkpoint(path = \"cpt_batchnorm/\"),\n luz_callback_csv_logger(\"logs_batchnorm.csv\")\n ),\n verbose = TRUE)\n```", "```r\nEpoch 1/50\nTrain metrics: Loss: 4.5434 - Acc: 0.0862                                     \nValid metrics: Loss: 4.0914 - Acc: 0.1332\nEpoch 2/50\nTrain metrics: Loss: 3.9534 - Acc: 0.161                                      \nValid metrics: Loss: 3.7865 - Acc: 0.1809\nEpoch 3/50\nTrain metrics: Loss: 3.6425 - Acc: 0.2054                                     \nValid metrics: Loss: 3.5965 - Acc: 0.2115\n...\n...\nEpoch 19/50\nTrain metrics: Loss: 2.1063 - Acc: 0.4859                                     \nValid metrics: Loss: 2.621 - Acc: 0.3912\nEpoch 20/50\nTrain metrics: Loss: 2.0514 - Acc: 0.4987                                     \nValid metrics: Loss: 2.6334 - Acc: 0.3914\nEpoch 21/50\nTrain metrics: Loss: 1.9982 - Acc: 0.5069                                     \nValid metrics: Loss: 2.6603 - Acc: 0.3932\nEarly stopping at epoch 21 of 50\n```", "```r\nconvnet <- nn_module(\n initialize = function() {\n self$model <- model_resnet18(pretrained = TRUE)\n for (par in self$parameters) {\n par$requires_grad_(FALSE)\n }\n self$model$fc <- nn_sequential(\n nn_linear(self$model$fc$in_features, 1024),\n nn_relu(),\n nn_linear(1024, 1024),\n nn_relu(),\n nn_linear(1024, 200)\n )\n },\n forward = function(x) {\n self$model(x)\n }\n)\n```", "```r\nmodel <- convnet %>%\n setup(\n loss = nn_cross_entropy_loss(),\n optimizer = optim_adam,\n metrics = list(\n luz_metric_accuracy()\n )\n ) \n\nrates_and_losses <- model %>% lr_finder(train_dl)\nrates_and_losses %>% plot()\n```", "```r\nfitted <- model %>%\n fit(train_dl, epochs = 50, valid_data = valid_dl,\n callbacks = list(\n luz_callback_early_stopping(patience = 2),\n luz_callback_lr_scheduler(\n lr_one_cycle,\n max_lr = 0.01,\n epochs = 50,\n steps_per_epoch = length(train_dl),\n call_on = \"on_batch_end\"),\n luz_callback_model_checkpoint(path = \"cpt_resnet/\"),\n luz_callback_csv_logger(\"logs_resnet.csv\")\n ),\n verbose = TRUE)\n```", "```r\nEpoch 1/50\nTrain metrics: Loss: 3.4036 - Acc: 0.2322                                     \nValid metrics: Loss: 2.5491 - Acc: 0.3884\nEpoch 2/50\nTrain metrics: Loss: 2.7911 - Acc: 0.3436                                     \nValid metrics: Loss: 2.417 - Acc: 0.4233\nEpoch 3/50\nTrain metrics: Loss: 2.6423 - Acc: 0.3726                                     \nValid metrics: Loss: 2.3492 - Acc: 0.4431\n...\n...\nValid metrics: Loss: 2.1822 - Acc: 0.4868\nEpoch 7/50\nTrain metrics: Loss: 2.4031 - Acc: 0.4198                                     \nValid metrics: Loss: 2.1413 - Acc: 0.4889\nEpoch 8/50\nTrain metrics: Loss: 2.3759 - Acc: 0.4252                                     \nValid metrics: Loss: 2.149 - Acc: 0.4958\nEpoch 9/50\nTrain metrics: Loss: 2.3447 - Acc: 0.433                                      \nValid metrics: Loss: 2.1888 - Acc: 0.484\nEarly stopping at epoch 9 of 50\n```"]