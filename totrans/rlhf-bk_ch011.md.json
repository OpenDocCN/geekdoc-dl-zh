["```py\n[](#cb1-1)# Basic PPO critic targets & loss (no GAE)\n[](#cb1-2)#\n[](#cb1-3)# B: Batch Size\n[](#cb1-4)# L: Completion Length\n[](#cb1-5)# Inputs:\n[](#cb1-6)#   rewards: (B, L) post-KL per-token rewards; EOS row includes outcome\n[](#cb1-7)#   done_mask: (B, L) 1.0 at terminal token (EOS or truncation if penalized), else 0.0\n[](#cb1-8)#   completion_mask: (B, L) 1.0 on response tokens to supervise (ignore the prompt)\n[](#cb1-9)#   values: (B, L) current critic predictions V_theta(s_t)\n[](#cb1-10)#       because a value network is a running update\n[](#cb1-11)#   old_values: (B, L) critic predictions at rollout time V_{theta_old}(s_t)\n[](#cb1-12)#   gamma: discount factor, float (often 1.0 for LM RLHF)\n[](#cb1-13)#   epsilon_v: float value clip range (e.g., 0.2), similar to PPO Loss Update itself, optional\n[](#cb1-14)#\n[](#cb1-15)# Returns:\n[](#cb1-16)#   value_loss: scalar; advantages: (B, L) detached (for policy loss)\n[](#cb1-17)\n[](#cb1-18)B, L = rewards.shape\n[](#cb1-19)\n[](#cb1-20)# 1) Monte Carlo returns per token (reset at terminals)\n[](#cb1-21)# Apply discounting, if enabled\n[](#cb1-22)returns = torch.zeros_like(rewards)\n[](#cb1-23)running = torch.zeros(B, device=rewards.device, dtype=rewards.dtype)\n[](#cb1-24)for t in reversed(range(L)):\n[](#cb1-25)    running = rewards[:, t] + gamma * (1.0 - done_mask[:, t]) * running\n[](#cb1-26)    returns[:, t] = running\n[](#cb1-27)\n[](#cb1-28)targets = returns  # y_t = G_t (post-KL)\n[](#cb1-29)\n[](#cb1-30)# 2) PPO-style value clipping (optional)\n[](#cb1-31)v_pred = values\n[](#cb1-32)v_old  = old_values\n[](#cb1-33)v_clip = torch.clamp(v_pred, v_old - epsilon_v, v_old + epsilon_v)\n[](#cb1-34)\n[](#cb1-35)vf_unclipped = 0.5 * (v_pred - targets) ** 2\n[](#cb1-36)vf_clipped   = 0.5 * (v_clip - targets) ** 2\n[](#cb1-37)vf_loss_tok  = torch.max(vf_unclipped, vf_clipped)\n[](#cb1-38)\n[](#cb1-39)# 3) Mask to response tokens and aggregate\n[](#cb1-40)denom = completion_mask.sum(dim=1).clamp_min(1)\n[](#cb1-41)value_loss = ((vf_loss_tok * completion_mask).sum(dim=1) / denom).mean()\n[](#cb1-42)\n[](#cb1-43)# 4) Advantages for policy loss (no GAE): A_t = G_t - V(s_t)\n[](#cb1-44)advantages = (targets - v_pred).detach()\n[](#cb1-45)\n[](#cb1-46)# The value loss is applied later, often with the PG loss, e.g.\n[](#cb1-47)# total_loss = policy_loss + vf_coef * value_loss\n```", "```py\n[](#cb2-1)pg_loss = -advantages * ratio\n```", "```py\n[](#cb3-1)# Strategy 1: Per-sequence normalization\n[](#cb3-2)sequence_loss = ((per_token_loss * completion_mask).sum(dim=1) / \\\n[](#cb3-3)             completion_mask.sum(dim=1)).mean()\n```", "```py\n[](#cb4-1)# Strategy 2: Per-token normalization\n[](#cb4-2)token_loss = ((per_token_loss * completion_mask).sum() / \\\n[](#cb4-3)            completion_mask.sum())\n```", "```py\n[](#cb5-1)seq_1_losses = [1, 1, 1, 1, 10]  # 5 tokens, mean = 2.8\n[](#cb5-2)seq_2_losses = [1, 1, 1, 1, 1, 1, 1, 1, 1, 10]  # 10 tokens, mean = 1.9\n```", "```py\n[](#cb6-1)from typing import Optional\n[](#cb6-2)import torch\n[](#cb6-3)\n[](#cb6-4)def masked_mean(values: torch.Tensor, mask: torch.Tensor, axis: Optional[int] = None) -> torch.Tensor:\n[](#cb6-5)    \"\"\"Compute mean of tensor with masked values.\"\"\"\n[](#cb6-6)    if axis is not None:\n[](#cb6-7)        return (values * mask).sum(axis=axis) / mask.sum(axis=axis)\n[](#cb6-8)    else:\n[](#cb6-9)        return (values * mask).sum() / mask.sum()\n[](#cb6-10)\n[](#cb6-11)def masked_sum(\n[](#cb6-12)        values: torch.Tensor,\n[](#cb6-13)        mask: torch.Tensor,\n[](#cb6-14)        axis: Optional[int] = None,\n[](#cb6-15)        constant_normalizer: float = 1.0,\n[](#cb6-16)    ) -> torch.Tensor:\n[](#cb6-17)    \"\"\"Compute sum of tensor with masked values. Use a constant to normalize.\"\"\"\n[](#cb6-18)    if axis is not None:\n[](#cb6-19)        return (values * mask).sum(axis=axis) / constant_normalizer\n[](#cb6-20)    else:\n[](#cb6-21)        return (values * mask).sum() / constant_normalizer\n[](#cb6-22)\n[](#cb6-23)ratio = torch.tensor([\n[](#cb6-24)    [1., 1, 1, 1, 1, 1, 1,],\n[](#cb6-25)    [1, 1, 1, 1, 1, 1, 1,],\n[](#cb6-26)], requires_grad=True)\n[](#cb6-27)\n[](#cb6-28)\n[](#cb6-29)advs = torch.tensor([\n[](#cb6-30)    [2, 2, 2, 2, 2, 2, 2,],\n[](#cb6-31)    [2, 2, 2, 2, 2, 2, 2,],\n[](#cb6-32)])\n[](#cb6-33)\n[](#cb6-34)masks = torch.tensor([\n[](#cb6-35)    # generation 1: 4 tokens\n[](#cb6-36)    [1, 1, 1, 1, 0, 0, 0,],\n[](#cb6-37)    # generation 2: 7 tokens\n[](#cb6-38)    [1, 1, 1, 1, 1, 1, 1,],\n[](#cb6-39)])\n[](#cb6-40)\n[](#cb6-41)max_gen_len = 7\n[](#cb6-42)\n[](#cb6-43)masked_mean_result = masked_mean(ratio * advs, masks, axis=1)\n[](#cb6-44)masked_mean_token_level = masked_mean(ratio, masks, axis=None)\n[](#cb6-45)masked_sum_result = masked_sum(ratio * advs, masks, axis=1, constant_normalizer=max_gen_len)\n[](#cb6-46)\n[](#cb6-47)print(\"masked_mean\", masked_mean_result)\n[](#cb6-48)print(\"masked_sum\", masked_sum_result)\n[](#cb6-49)print(\"masked_mean_token_level\", masked_mean_token_level)\n[](#cb6-50)\n[](#cb6-51)# masked_mean tensor([2., 2.], grad_fn=<DivBackward0>)\n[](#cb6-52)# masked_sum tensor([1.1429, 2.0000], grad_fn=<DivBackward0>)\n[](#cb6-53)# masked_mean_token_level tensor(1., grad_fn=<DivBackward0>)\n[](#cb6-54)\n[](#cb6-55)masked_mean_result.mean().backward()\n[](#cb6-56)print(\"ratio.grad\", ratio.grad)\n[](#cb6-57)ratio.grad.zero_()\n[](#cb6-58)# ratio.grad tensor([[0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000],\n[](#cb6-59)# [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429]])\n[](#cb6-60)\n[](#cb6-61)masked_sum_result.mean().backward()\n[](#cb6-62)print(\"ratio.grad\", ratio.grad)\n[](#cb6-63)ratio.grad.zero_()\n[](#cb6-64)# ratio.grad tensor([[0.1429, 0.1429, 0.1429, 0.1429, 0.0000, 0.0000, 0.0000],\n[](#cb6-65)# [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429]])\n[](#cb6-66)\n[](#cb6-67)masked_mean_token_level.mean().backward()\n[](#cb6-68)print(\"ratio.grad\", ratio.grad)\n[](#cb6-69)# ratio.grad tensor([[0.0909, 0.0909, 0.0909, 0.0909, 0.0000, 0.0000, 0.0000],\n[](#cb6-70)# [0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909]])\n```", "```py\n[](#cb7-1)# B: Batch Size, L: Sequence Length, G: Num of Generations\n[](#cb7-2)# Apply KL penalty to rewards\n[](#cb7-3)rewards = rewards - self.beta * per_token_kl  # Shape: (B*G, L)\n[](#cb7-4)\n[](#cb7-5)# Get value predictions\n[](#cb7-6)values = value_net(completions)  # Shape: (B*G, L)\n[](#cb7-7)\n[](#cb7-8)# Compute simple advantages\n[](#cb7-9)advantages = rewards - values.detach()  # Shape: (B*G, L)\n[](#cb7-10)# Note: We detach the value network here to not update the parameters of \n[](#cb7-11)# the value function when computing the policy-gradient loss\n[](#cb7-12)\n[](#cb7-13)# Normalize advantages (optional but stable)\n[](#cb7-14)advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n[](#cb7-15)\n[](#cb7-16)# Compute probability ratio between new and old policies\n[](#cb7-17)ratio = torch.exp(new_per_token_logps - per_token_logps)  # Shape: (B*G, L)\n[](#cb7-18)\n[](#cb7-19)# PPO clipping objective\n[](#cb7-20)eps = self.cliprange  # e.g. 0.2\n[](#cb7-21)pg_losses1 = -advantages * ratio  # Shape: (B*G, L)\n[](#cb7-22)pg_losses2 = -advantages * torch.clamp(ratio, 1.0 - eps, 1.0 + eps)  # Shape: (B*G, L)\n[](#cb7-23)pg_loss_max = torch.max(pg_losses1, pg_losses2)  # Shape: (B*G, L)\n[](#cb7-24)\n[](#cb7-25)# Simple value function loss\n[](#cb7-26)vf_loss = 0.5 * ((rewards - values) ** 2)  # Shape: (B*G, L)\n[](#cb7-27)\n[](#cb7-28)# Combine policy and value losses\n[](#cb7-29)per_token_loss = pg_loss_max + self.vf_coef * vf_loss  # Shape: (B*G, L)\n[](#cb7-30)\n[](#cb7-31)# Apply completion mask and compute final loss\n[](#cb7-32)loss = ((per_token_loss * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)).mean()\n[](#cb7-33) # Scalar\n[](#cb7-34)\n[](#cb7-35)# Compute metrics for logging\n[](#cb7-36)with torch.no_grad():\n[](#cb7-37)    # Compute clipping fraction\n[](#cb7-38)    clip_frac = ((pg_losses2 > pg_losses1).float() * completion_mask).sum() / completion_mask.sum()\n[](#cb7-39) \n[](#cb7-40)    # Compute approximate KL\n[](#cb7-41)    approx_kl = 0.5 * ((new_per_token_logps - per_token_logps)**2).mean()\n[](#cb7-42) \n[](#cb7-43)    # Compute value loss for logging\n[](#cb7-44)    value_loss = vf_loss.mean()\n```", "```py\n[](#cb8-1)pg_losses1 = -advantages * ratio  # Shape: (B*G, L)\n[](#cb8-2)pg_losses2 = -advantages * torch.clamp(ratio, 1.0 - eps, 1.0 + eps)  # Shape: (B*G, L)\n[](#cb8-3)pg_loss_max = torch.max(pg_losses1, pg_losses2)  # Shape: (B*G, L)\n```", "```py\n[](#cb9-1)# B: Batch Size, L: Sequence Length, G: Number of Generations\n[](#cb9-2)# Compute grouped-wise rewards # Shape: (B,)\n[](#cb9-3)mean_grouped_rewards = rewards.view(-1, self.num_generations).mean(dim=1)\n[](#cb9-4)std_grouped_rewards = rewards.view(-1, self.num_generations).std(dim=1) \n[](#cb9-5)\n[](#cb9-6)\n[](#cb9-7)# Normalize the rewards to compute the advantages\n[](#cb9-8)mean_grouped_rewards = mean_grouped_rewards.repeat_interleave(self.num_generations, dim=0)\n[](#cb9-9)std_grouped_rewards = std_grouped_rewards.repeat_interleave(self.num_generations, dim=0)\n[](#cb9-10)# Shape: (B*G,)\n[](#cb9-11)\n[](#cb9-12)# Compute advantages\n[](#cb9-13)advantages = (rewards - mean_grouped_rewards) / (std_grouped_rewards + 1e-4)\n[](#cb9-14)advantages = advantages.unsqueeze(1)\n[](#cb9-15)# Shape: (B*G, 1)\n[](#cb9-16)\n[](#cb9-17)# Compute probability ratio between new and old policies\n[](#cb9-18)ratio = torch.exp(new_per_token_logps - per_token_logps)  # Shape: (B*G, L)\n[](#cb9-19)\n[](#cb9-20)# PPO clipping objective\n[](#cb9-21)eps = self.cliprange  # e.g. 0.2\n[](#cb9-22)pg_losses1 = -advantages * ratio  # Shape: (B*G, L)\n[](#cb9-23)pg_losses2 = -advantages * torch.clamp(ratio, 1.0 - eps, 1.0 + eps)  # Shape: (B*G, L)\n[](#cb9-24)pg_loss_max = torch.max(pg_losses1, pg_losses2)  # Shape: (B*G, L)\n[](#cb9-25)\n[](#cb9-26)# important to GRPO -- PPO applies this in reward traditionally\n[](#cb9-27)# Combine with KL penalty\n[](#cb9-28)per_token_loss = pg_loss_max + self.beta * per_token_kl  # Shape: (B*G, L)\n[](#cb9-29)\n[](#cb9-30)# Apply completion mask and compute final loss\n[](#cb9-31)loss = ((per_token_loss * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)).mean()\n[](#cb9-32) # Scalar\n[](#cb9-33)\n[](#cb9-34)# Compute core metric for logging (KL, reward, etc. also logged)\n[](#cb9-35)with torch.no_grad():\n[](#cb9-36)    # Compute clipping fraction\n[](#cb9-37)    clip_frac = ((pg_losses2 > pg_losses1).float() * completion_mask).sum() / completion_mask.sum()\n[](#cb9-38) \n[](#cb9-39)    # Compute approximate KL\n[](#cb9-40)    approx_kl = 0.5 * ((new_per_token_logps - per_token_logps)**2).mean()\n```", "```py\n[](#cb10-1)# rloo_k --> number of completions per prompt \n[](#cb10-2)# rlhf_reward --> Initially a flat tensor of total rewards for all completions. Length B = N x k\n[](#cb10-3)rlhf_reward = rlhf_reward.reshape(rloo_k, -1) # \n[](#cb10-4)# Now, Shape: (k, N), each column j contains the k rewards for prompt j.\n[](#cb10-5)\n[](#cb10-6)baseline = (rlhf_reward.sum(0) - rlhf_reward) / (rloo_k - 1)\n[](#cb10-7)# baseline --> Leave-one-out baseline rewards. Shape: (k, N)\n[](#cb10-8)#  baseline[i, j] is the avg reward of samples i' != i for prompt j.\n[](#cb10-9)\n[](#cb10-10)advantages = rlhf_reward - baseline\n[](#cb10-11)# advantages --> Same Shape: (k, N)\n[](#cb10-12)\n[](#cb10-13)advantages = advantages.flatten() # Same shape as original tensor\n```", "```py\n[](#cb11-1)# GAE (token-level) for LM RLHF\n[](#cb11-2)#\n[](#cb11-3)# B: Batch Size\n[](#cb11-4)# L: Length\n[](#cb11-5)# Inputs:\n[](#cb11-6)#   rewards: (B, L) post-KL per-token rewards\n[](#cb11-7)#   values:  (B, L) current V_theta(s_t)\n[](#cb11-8)#   done_mask: (B, L) 1.0 at terminal token (EOS or penalized trunc), else 0.0\n[](#cb11-9)#   gamma: float (often 1.0), lam: float in [0,1]\n[](#cb11-10)B, L = rewards.shape\n[](#cb11-11)advantages = torch.zeros_like(rewards)\n[](#cb11-12)next_v = torch.zeros(B, device=rewards.device, dtype=rewards.dtype)\n[](#cb11-13)gae = torch.zeros(B, device=rewards.device, dtype=rewards.dtype)\n[](#cb11-14)\n[](#cb11-15)for t in reversed(range(L)):\n[](#cb11-16)    not_done = 1.0 - done_mask[:, t]\n[](#cb11-17)    delta = rewards[:, t] + gamma * not_done * next_v - values[:, t]\n[](#cb11-18)    gae = delta + gamma * lam * not_done * gae\n[](#cb11-19)    advantages[:, t] = gae\n[](#cb11-20)    next_v = values[:, t]\n[](#cb11-21)\n[](#cb11-22)targets = advantages + values      # y_t for value regression\n[](#cb11-23)advantages = advantages.detach()   # for policy loss\n```"]