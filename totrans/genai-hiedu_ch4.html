<?xml version="1.0" encoding="utf-8"?><!DOCTYPE html []>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xml:lang="en" lang="en">
<head>
<title>4 Redesigning Assessment in the AI Era</title>
<link rel="stylesheet" type="text/css" href="template.css"/></head>
<body epub:type="bodymatter">
<section epub:type="chapter" role="doc-chapter">
<header>
<h1 id="h1"><span epub:type="pagebreak" id="p87" aria-label=" page 87. " role="doc-pagebreak"/><span class="chnum" epub:type="ordinal">4 </span><span class="chtitle">Redesigning Assessment in the AI Era</span></h1>
<p class="doi">DOI: <a href="https://dx.doi.org/10.4324/9781003459026-4" aria-label="D.O.I. link to this document.">10.4324/9781003459026-4</a></p></header>
<blockquote epub:type="epigraph" role="doc-epigraph">
<p>This &#x201C;arms race&#x201D; between assessing GenAI and AI tools used by students could lead to a scenario where genuine learning takes a backseat to merely &#x201C;gaming the system&#x201D;.</p>
<p class="byLine"><span epub:type="credit" role="doc-credit">Cecilia KY Chan</span></p></blockquote>
<section epub:type="introduction" role="doc-introduction">
<h2 id="sec4_1"><span epub:type="ordinal">4.1 </span>Introduction</h2>
<p>The current pedagogical landscape of higher education can be visualised as a complex web where learning outcomes, pedagogy, and assessment mechanisms intersect. At its core, assessment serves as a barometer for both educators and learners, offering insights into educational effectiveness and areas of improvement. From the vantage point of curriculum design, the triad of learning outcomes, pedagogies, and assessment tools plays a fundamental role. While learning outcomes encapsulate the desired educational objectives, pedagogies outline the means to achieve them. Assessment, then, serves as the evaluation mechanism, answering the pivotal question: Have the students truly learned?</p>
<p>As we enter the era of AI, the emergence of GenAI offers a pivotal transformation in this complex landscape. GenAI, with its advanced capabilities, can redefine assessment methodologies, providing real-time, adaptive, and personalised evaluations and feedback. While its potential to streamline and enhance assessment practices is unprecedented, it also brings forth ethical considerations and challenges in maintaining academic integrity, fairness, and human-centric conflicts. The integration of GenAI in assessment transcends traditional boundaries, opening new avenues for innovation while also demanding a thoughtful reckoning with its implications.</p>
<p>For students, assessment often dictates their learning approaches. As noted by <a href="#ref4_34" id="R_ref4_34" epub:type="biblioref" role="doc-biblioref">Ramsden (2003)</a>, students&#x2019; perceptions of assessment can profoundly influence their learning strategies, oscillating between deep, holistic understanding and surface-level memorisation. This places immense responsibility on educators to craft assessments that not only evaluate but also inspire genuine learning. This chapter will explore the multi-faceted role of GenAI in shaping the future of assessment, reflecting both its promises and perils with recent research findings, and providing some strategies and assessment framework for teachers to rethink and redesign their assessment in the GenAI era.</p></section>
<section aria-labelledby="sec4_2">
<h2 id="sec4_2"><span epub:type="ordinal">4.2 </span>The Evolution of Assessment and Outcomes-Based Learning in Higher Education</h2>
<p>In education, assessment stands as an unshakable pillar, underpinning the foundation of academic endeavours and pedagogical aspirations. Traditionally understood as the systematic <span epub:type="pagebreak" id="p88" aria-label=" page 88. " role="doc-pagebreak"/>evaluation of student learning, assessment spans a continuum from gauging knowledge and skill acquisition to understanding changes in attitudes and beliefs; in short, assessment drives learning (<a href="#ref4_36" id="R_ref4_36" epub:type="biblioref" role="doc-biblioref">Rust, 2002</a>). This intricate balance of measurement is designed with four primary purposes: 1). Judging academic achievement, 2). Safeguarding academic standards and quality, 3). Ensuring accountability to stakeholders, and 4). Crucially, steering student learning (<a href="#ref4_9" id="R_ref4_9" epub:type="biblioref" role="doc-biblioref">Chan, 2023</a>, p. 41, Ch 3).</p>
<p>While the purposes of assessment have remained consistent over time, the methodologies, tools, and perspectives surrounding assessment have evolved (<a href="#ref4_22" id="R_ref4_22" epub:type="biblioref" role="doc-biblioref">Fischer et al., 2023</a>; <a href="#ref4_37" id="R_ref4_37" epub:type="biblioref" role="doc-biblioref">Scott, 2020</a>), reflecting changing educational philosophies and societal demands. For example, historically, summative assessment, also known as <i>Assessment of Learning</i>, was primarily viewed as the driving force behind students&#x2019; learning. This type of assessment is often associated with grades and occurs at the end of the learning cycle. This perspective was rooted in the belief that the anticipation of grades, marks, or evaluations would motivate students to study, understand, and retain information. In essence, the looming presence of a final assessment dictated the pace, approach, and intensity of a student&#x2019;s study habits. However, in recent years, the growing recognition of formative assessment has been playing an important role in student learning. Formative assessment, often termed <i>Assessment for Learning</i> (AfL), is the process of providing feedback to students throughout the learning process, with the goal of helping them to improve their understanding and performance. This type of assessment typically uses criterion-reference to compare students&#x2019; achievements to specific goals or benchmarks. Michael Scriven introduced the concept of formative assessment in the 1960s to differentiate it from summative assessment. <a href="#ref4_4" id="R_ref4_4" epub:type="biblioref" role="doc-biblioref">Black and Wiliam (1998)</a> posited that an assessment becomes truly formative when its results guide and modify instructional methods to better suit students&#x2019; needs. Echoing this sentiment, <a href="#ref4_44" id="R_ref4_44" epub:type="biblioref" role="doc-biblioref">Wiggins (1998)</a> emphasised that the primary objective of formative assessment is to facilitate and elevate students&#x2019; learning, rather than just to monitor it.</p>
<p>Traditionally, summative and formative assessments were viewed in distinct silos; the former focused on end-of-learning evaluations and the latter on ongoing feedback during the learning process. However, this binary view has been challenged and reshaped. For instance, at the University of British Columbia, a two-stage exam approach blurs the line between these assessments (<a href="#ref4_23" id="R_ref4_23" epub:type="biblioref" role="doc-biblioref">Gilley &#x0026; Clarkston, 2014</a>). In this method, students initially take the exam individually, followed by a collaborative re-examination in small groups. This not only promotes individual accountability but also encourages peer feedback and collaborative learning, combining the evaluative nature of summative assessment with the feedback-driven approach of formative assessment. Such innovative practices underscore that summative and formative assessments aren&#x2019;t mutually exclusive but can be harmoniously integrated (<a href="#ref4_9" epub:type="biblioref" role="doc-biblioref">Chan, 2023</a>, p. 45, Ch 3; <a href="#ref4_18" id="R_ref4_18" epub:type="biblioref" role="doc-biblioref">Dixson &#x0026; Worrell, 2016</a>). As pedagogical research advances, the dichotomy between these assessments becomes less pronounced, recognising that they can often serve dual purposes and mutually enrich the learning experience.</p>
<p>In a recent article by <a href="#ref4_22" epub:type="biblioref" role="doc-biblioref">Fischer et al. (2023)</a>, the perspective of traditional assessment is once again being challenged. The article demonstrates the contemporary understanding of assessment goes beyond its mere function as a motivator. In fact, in today&#x2019;s educational landscape, the role of assessment is not just to gauge learning but to foster a deeper, more holistic understanding of subjects, skills, and attitudes (<a href="#ref4_12" id="R_ref4_12" epub:type="biblioref" role="doc-biblioref">Chan &#x0026; Luk, 2022</a>). One of the key shifts in this perspective is the recognition of higher education&#x2019;s responsibility to promote lifelong learning. Assessment, in this paper (<a href="#ref4_22" epub:type="biblioref" role="doc-biblioref">Fischer et al., 2023</a>), is seen as a tool to support students in developing evaluative judgement capabilities &#x2013; i.e., the ability to critically assess their work and the work of their peers (<a href="#ref4_40" id="R_ref4_40" epub:type="biblioref" role="doc-biblioref">Tai et al., 2018</a>).</p>
<p><span epub:type="pagebreak" id="p89" aria-label=" page 89. " role="doc-pagebreak"/>In the dynamic sphere of higher education, the assessment landscape continues to evolve, reflecting the multi-faceted nature of the educational journey. Central to this evolution is the understanding that true learning transcends mere knowledge acquisition. Instead, it encompasses a holistic approach that integrates feedback and reflection with conventional methodologies of learning outcomes, learning approaches, and assessment.</p>
<p>Assessment has often relegated feedback to a secondary role, treated as a mere by-product of evaluation. Similarly, reflection, the process of introspection and critical thinking about one&#x2019;s learning experiences, remains an unintentional component in curriculum design. However, to truly understand and optimise the learning process, feedback and reflection must be given their rightful place at the forefront of educational methodologies.</p>
<p>Reflection, as I articulated it in my book <i>Assessment for Experiential Learning</i> (<a href="#ref4_9" epub:type="biblioref" role="doc-biblioref">Chan, 2023</a>, p. 160, Ch. 5), is &#x201C;how you see yourself before, now and after; how you see yourself from different perspectives; how you see yourself after certain situations or experiences; how you see yourself, your actions and your behaviours after you observe others&#x201D;. Such introspection is foundational for metacognitive development, fostering higher-order thinking processes. Furthermore, established educational theories, from Bloom&#x2019;s Taxonomy (<a href="#ref4_5" id="R_ref4_5" epub:type="biblioref" role="doc-biblioref">Bloom, 1956</a>) to Biggs&#x2019; SOLO taxonomy (<a href="#ref4_3" id="R_ref4_3" epub:type="biblioref" role="doc-biblioref">Biggs &#x0026; Collis, 1982</a>), underscore the significance of reflection in deep learning. Reflection not only nurtures higher cognitive skills but also facilitates transformative learning, wherein learners integrate new information, critically evaluate past knowledge, and derive new insights.</p>
<p>Feedback, on the other hand, is more than just a response to student performance. In its essence, feedback is a dialogue &#x2013; a continuous exchange of information between the educator and the learner. As <a href="#ref4_8" id="R_ref4_8" epub:type="biblioref" role="doc-biblioref">Carless and Boud (2018)</a> emphasise, feedback is a process wherein learners interpret information from various sources and utilise it to enhance their work or learning strategies. Without meaningful feedback, the learning process remains incomplete, limiting the depth and breadth of the learning experience.</p>
<p><i>Assessment as Learning</i>, as articulated by <a href="#ref4_19" id="R_ref4_19" epub:type="biblioref" role="doc-biblioref">Earl (2003)</a>, is a profound shift from traditional assessment paradigms. Rather than seeing assessment as an external process imposed upon students, it reframes assessment as a self-regulated learning process. Central to this is the intertwining of feedback and reflection, which together create a dynamic loop of continuous improvement and self-awareness (<a href="#ref4_7" id="R_ref4_7" epub:type="biblioref" role="doc-biblioref">Carless, 2015</a>, p. 199). Feedback, both formal and informal, serves as a mirror for students in the assessment-as-learning approach. As students embark on self-assessment and peer assessment, feedback becomes an integral component, offering insights, clarifications, and directions for improvement. Whether it is feedback they give to themselves, feedback from peers, or feedback from educators, it acts as a guidepost, highlighting areas of strength and those needing further development. While feedback provides direction, reflection is the mechanism that internalises this feedback. Through reflection, students don&#x2019;t just passively receive feedback but actively engage with it. They question their understanding, dissect their thought processes, and, most importantly, they make conscious decisions about their learning paths. Reflecting on feedback allows students to adjust their strategies, realign their goals, and solidify their understanding.</p>
<p>The traditional landscape of higher education assessment, while effective in measuring specific learning outcomes, often falls short in providing a comprehensive understanding of the student&#x2019;s learning journey (<a href="#ref4_6" id="R_ref4_6" epub:type="biblioref" role="doc-biblioref">Boud &#x0026; Falchikov, 2006</a>). The predominant focus on high-stakes examinations promotes rote learning and does not foster critical thinking or self-awareness (<a href="#ref4_39" id="R_ref4_39" epub:type="biblioref" role="doc-biblioref">Stobart, 2008</a>; <a href="#ref4_45" id="R_ref4_45" epub:type="biblioref" role="doc-biblioref">Wiliam, 2011</a>). Feedback, when provided, is often delayed, generic, and not actionable, limiting its potential to guide and improve student performance. <span epub:type="pagebreak" id="p90" aria-label=" page 90. " role="doc-pagebreak"/>Furthermore, the absence of structured reflective practices in curriculum design means that students miss out on opportunities for self-assessment and personal growth. Reflection, despite its recognised importance, is challenging to assess, primarily because it requires understanding a learner&#x2019;s past, present, and future directions (<a href="#ref4_10" id="R_ref4_10" epub:type="biblioref" role="doc-biblioref">Chan &#x0026; Lee, 2021</a>). This complexity, combined with various institutional and sociocultural factors, often relegates reflection to the periphery of the educational process.</p>
<p>Integrating feedback and reflection into the assessment landscape is not just about adding two more components to the curriculum. It is about reimagining the entire educational process. When students receive timely, specific, and actionable feedback, they become active participants in their learning journey, taking responsibility for their growth. With structured reflection opportunities, students can introspect, analyse, and derive insights from their experiences, fostering a deeper understanding of the subject and themselves.</p>
<p>And in the midst of this ongoing evolution, the advent of GenAI promises yet another significant transformation in the assessment dimension. As we currently transition from emphasising mere knowledge acquisition towards a more holistic, formative process that prioritises feedback and reflection, GenAI brings potential to further refine this shift. The unparalleled data-processing capabilities and adaptability of GenAI can augment the assessment process, allowing for more nuanced, real-time feedback and adaptive approach to learning. However, as educators, while we harness the power of GenAI, it is imperative to ensure that assessments remain grounded in authentic learning experiences, challenging students in ways that stimulate genuine reflection, develop evaluative judgement and foster a deeper, metacognitive approach to learning. As GenAI becomes a more integrated tool in the educational landscape, the challenge lies in designing or redesigning assessments that leverage its capabilities while ensuring that the focus remains on promoting profound, meaningful learning. The future of assessment, with the convergence of traditional pedagogies and advanced AI technologies, is on the horizon, beckoning educators to navigate this uncharted territory with both enthusiasm and discernment. <a href="#fig4_1" id="R_fig4_1">Figure 4.1</a> shows a modified outcomes-based approach with an emphasis on assessment.</p>
<figure id="fig4_1"><img src="images/fig4_1_B.jpg" alt="Learning outcomes in student learning that is supplanted with reflection and feedback."/>
<figcaption><a href="#R_fig4_1" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. "><span epub:type="label">Figure</span> <span epub:type="ordinal">4.1 </span>Outcomes-Based Approach to Student Learning with Reflection and Feedback.</a></figcaption></figure></section>
<section aria-labelledby="sec4_3">
<h2 id="sec4_3"><span epub:type="pagebreak" id="p91" aria-label=" page 91. " role="doc-pagebreak"/><span epub:type="ordinal">4.3 </span>Challenges in Assessment in the Traditional Era</h2>
<p>The challenges in academic knowledge assessment are manifold and have been intricately woven into the fabric of education for decades. Central to these challenges is the multi-faceted purpose of assessment. While the primary role of assessment should ideally be to guide and improve student learning, it often dons multiple hats, sometimes serving as a tool for bureaucratic mandates or accountability checks. <a href="#ref4_21" id="R_ref4_21" epub:type="biblioref" role="doc-biblioref">Fendrich (2007)</a> critically observes that many current assessment practices resemble mere &#x201C;accountability&#x201D; exercises rather than genuine efforts to enhance the quality of education. Moreover, there is a stark disparity in how teachers and students perceive assessment. While educators often view it as a tool for reporting, students see it as a mere culminating grade, signalling the end of their learning process (<a href="#ref4_14" id="R_ref4_14" epub:type="biblioref" role="doc-biblioref">Chan et al., 2017</a>; <a href="#ref4_31" id="R_ref4_31" epub:type="biblioref" role="doc-biblioref">Montgomery &#x0026; Baker, 2007</a>; <a href="#ref4_38" id="R_ref4_38" epub:type="biblioref" role="doc-biblioref">Sonnleitner &#x0026; Kovacs, 2020</a>; <a href="#ref4_41" id="R_ref4_41" epub:type="biblioref" role="doc-biblioref">Van de Watering &#x0026; Van der Rijt, 2006</a>).</p>
<p>This perception dilemma is exacerbated by the common pitfalls in assessment design. Constructive misalignment, where there is a mismatch between learning activities, outcomes, and assessment methods, is still prevalent. Often, simplistic assessment methods, like multiple-choice questions, are used in scenarios that demand deeper evaluations, while more complex methods are misapplied in simpler contexts. The historical reliance on high-stakes examinations (<a href="#ref4_39" epub:type="biblioref" role="doc-biblioref">Stobart, 2008</a>; <a href="#ref4_45" epub:type="biblioref" role="doc-biblioref">Wiliam, 2011</a>) further compounds the issue, leading to rote learning and fostering a competitive rather than a collaborative learning environment. Such systems, especially when they employ norm-referencing standards, often discourage true learning and collaboration among students.</p>
<p>Feedback, a crucial element of the learning process, presents its own set of challenges. While timely and personalised feedback can significantly enhance student learning, it also means a considerable workload for educators. This becomes even more pronounced in the era of mass higher education where student numbers are burgeoning. Group assessments, despite their potential benefits, bring logistical nightmares. Without clear objectives and expectations, they can sow discord among students and dilute the educational benefits they are meant to provide. Furthermore, the growing trend towards modular curricula means students grapple with increased assessment workloads, leaving little room for deep learning.</p>
<p>In addition, assessing experiential learning types of pedagogical approaches presents unique challenges distinct from those of traditional academic knowledge assessment (<a href="#ref4_27" id="R_ref4_27" epub:type="biblioref" role="doc-biblioref">Kuh, 2008</a>). One significant issue is the ambiguity surrounding the learning outcomes related to experiential learning. As <a href="#ref4_30" id="R_ref4_30" epub:type="biblioref" role="doc-biblioref">Luk and Chan (2021)</a> elucidated, while there are general learning outcomes, each student&#x2019;s experience and motivation can lead to specific outcomes, complicating the design of aligned assessments. Experiential learning often focuses less on mastering specific knowledge and more on acquiring or enhancing particular competencies, necessitating meticulously crafted assessment designs.</p>
<p>There is an evident lack of clarity around the conceptualisation of competencies (<a href="#ref4_14" epub:type="biblioref" role="doc-biblioref">Chan et al., 2017</a>). The absence of a clear definition or set list of competencies students should develop makes it challenging to determine what and how to assess. This is compounded by the fact that experiential learning outcomes often aren&#x2019;t tied directly to specific academic disciplines, making them harder for educators to evaluate.</p>
<p>The logistics and practicalities of assessment in experiential learning further intensify the challenges. Given that experiential learning often involves various parties, assessments might require input from multiple stakeholders, complicating the logistical aspects. Additionally, determining the optimal individuals to provide feedback, the timing, and the method of delivery presents its own set of dilemmas. Ensuring validity, reliability, and the <span epub:type="pagebreak" id="p92" aria-label=" page 92. " role="doc-pagebreak"/>meaningful integration of assessment as part of the learning journey are paramount yet intricate tasks in experiential learning.</p>
<p>Adding to these challenges is the scepticism around innovative assessment methods. Both educators and students often remain wary of unfamiliar assessment techniques, such as peer, self-assessment and authentic assessment, even when they have the potential to enhance learning outcomes. A further challenge lies in the conception and execution of authentic assessments that mirror real-world scenarios. Students resonate with assessments they perceive as authentic, but creating such assessments demands considerable time and resources. Additionally, perceptions of authenticity vary among students, based on their individual experiences. The desire for authenticity is underscored by the call from education experts globally for assessments to more closely mirror real-world tasks.</p>
<p>Lastly, the spectre of academic dishonesty has morphed into a more insidious form with the rise of contract cheating, where students can easily purchase bespoke essays, undermining the very essence of academic integrity (<a href="#ref4_1" id="R_ref4_1" epub:type="biblioref" role="doc-biblioref">Awdry et al., 2022</a>).</p>
<p>More details on the challenges in assessment can be found in my open access book on Assessment for Experiential Learning (<a href="#ref4_9" epub:type="biblioref" role="doc-biblioref">Chan, 2023</a>, p. 39, Ch. 3).</p></section>
<section aria-labelledby="sec4_4">
<h2 id="sec4_4"><span epub:type="ordinal">4.4 </span>Challenges in Assessment in the GenAI Era</h2>
<p>The introduction of GenAI in academic assessment has ushered in both transformative possibilities and unprecedented challenges. At the forefront of these challenges is the potential erosion of human-centric evaluation. While GenAI can process vast amounts of data and provide real-time, adaptive evaluations, it may lack the nuanced understanding of individual learners&#x2019; journeys, potentially leading to overly standardised assessments that do not cater to individual learning styles or cultural contexts. This poses the risk of oversimplifying complex learning processes and reducing the richness of human experience to mere data points.</p>
<p>Moreover, the ethical implications of GenAI-driven assessment are vast. Issues of data privacy and security come to the fore, as students&#x2019; learning patterns, strengths, and weaknesses are analysed and stored. Without robust safeguards, this sensitive information could be misused or fall into the wrong hands. Furthermore, GenAI, by its very nature, learns and evolves based on the data it receives. If this data is biased or unrepresentative, it could perpetuate and even exacerbate existing educational inequities, reinforcing stereotypes and further marginalising already disadvantaged student populations. For example, in the Department of Social Work and Social Administration at the University of Hong Kong, departmental guidelines are provided to maintain the confidentiality of clients&#x2019; data in accordance with the ethical practices of the social work profession and the requirements of the Personal Data (Privacy) Ordinance. However, as most AI tools use large language models (LLMs), which are typically deployed and hosted in the cloud, and given that some of these AI tools may use data for training or other purposes, stringent guidelines and AI literacy training must be provided for students if they wish to use AI-generated language models for fieldwork courses.</p>
<p>The academic integrity of GenAI-driven assessments also comes under scrutiny. As AI tools become more accessible, there is the looming threat of students leveraging advanced AI tools to artificially enhance their performance or even engage in sophisticated forms of cheating. This &#x201C;arms race&#x201D; between evaluative GenAI and AI tools used by students could lead to a scenario where genuine learning takes a backseat to merely &#x201C;gaming the system&#x201D;. Thus, on one hand, we are trying to prepare our students to be future-ready with AI capability, but on the other hand, we are concerned about our students lacking the genuine knowledge and skills on what the future expected of them if they use AI tools in a non-constructive way. In an Urban Planning and Design course at the University of Hong Kong, <span epub:type="pagebreak" id="p93" aria-label=" page 93. " role="doc-pagebreak"/>a teacher is exploring the design of an essay-based assessment using GenAI. The teacher suggests that students: i) Use GenAI tools to generate a 2000-word essay on a relevant topic, ii) Reflect upon and critique the AI-generated work, analysing its strengths, weaknesses, and overall coherence, and iii) Craft their own comprehensive essay drawing on their evaluations and criticisms from part ii. I was consulted regarding this essay design. While I find the integration of GenAI commendable, the teacher needs to carefully consider the actual objectives and outcomes of both the assessment and the course. If the intent behind having students use AI for generating a 2000-word essay is simply to acquaint them with GenAI tools, that is one aspect of learning. But, if the focus lies on the reflective critique in part ii and the essay composition in part iii, these objectives must be clearly delineated. Would students be deterred from using GenAI to complete part iii, or would that be permissible? If allowed, what then becomes the primary purpose of the assessment? Furthermore, addressing the proper attribution of the tool is crucial. Equally important is the provision of clear criteria and rubrics for the students.</p>
<p>Additionally, while GenAI offers the allure of efficiency, there is a real concern about over-reliance (<a href="#ref4_11" id="R_ref4_11" epub:type="biblioref" role="doc-biblioref">Chan &#x0026; Tsi, 2023</a>). From our research findings, some students mentioned</p>
<blockquote class="bq">
<p>&#x201C;<i>It is concerning especially for students as it can limit genuine development of skills that are necessary in the world/future occupations</i>.&#x201D;</p>
<p><i>&#x201C;Unfortunately, people may get lazier and use less of their own brain to think.&#x201D;</i></p></blockquote>
<p>and some teachers are</p>
<blockquote>
<p>&#x201C;<i>Very concerned. Students already have poor critical thinking and information search skills in general.&#x201D;</i></p></blockquote>
<p>A student actually said</p>
<blockquote>
<p>&#x201C;<i>I think it is safe to say that ChatGPT has spoiled me because it is becoming pretty hard for me to write a full sentence without the temptation to ask ChatGPT to fix it for me. Because they always use better words when you are trying to express.&#x201D;</i></p></blockquote>
<p>The human touch in education, characterised by empathy, understanding, and mentorship, risks being</p>
<blockquote>
<p>sidelined, leading to an impersonal and detached learning environment as a student mentioned. In our findings, some students and teachers are not concerned the rapid adoption and widespread use of AI technologies, as they believe <i>&#x201C;Humans are more creative and empathetic than AI</i>.</p></blockquote>
<p>The reduction of face-to-face interactions and feedback sessions, replaced by automated feedback, might strip the learning process of its inherent human connection.</p>
<p>Looking ahead, as GenAI becomes more integrated into educational systems, the challenge will also lie in ensuring educators and administrators are adequately trained to understand, interpret, and act on GenAI-driven insights. Without proper training, there is a danger of misinterpreting the usage of GenAI or placing undue emphasis on GenAI recommendations without critical evaluation. In the research findings, one teacher stressed</p>
<blockquote>
<p>While it may deeply transform education and work, I believe there is a path to be taken where AI tools can be harnessed and be beneficial to students, workers and society at large. But this is a narrow path, which I think requires very careful thinking, constant <span epub:type="pagebreak" id="p94" aria-label=" page 94. " role="doc-pagebreak"/>education about AI models and their evolution, communication between teachers and students or between policymakers and the public, clear boundaries when it comes to what is permitted and what is not etc. It is a matter to grasping the opportunity quickly and responsibly.</p></blockquote>
<p>While GenAI holds the promise to revolutionise academic assessment by making it more streamlined, personalised, and data-driven, it also brings forth a plethora of challenges. Balancing the potential of GenAI with its pitfalls will be crucial, ensuring that the evolution of assessment remains anchored in the core tenets of education: fostering genuine learning, upholding integrity, and ensuring equity.</p></section>
<section aria-labelledby="sec4_5">
<h2 id="sec4_5"><span epub:type="ordinal">4.5 </span>Redesigning Assessment with GenAI</h2>
<p>The integration of GenAI into higher education offers transformative opportunities, yet it also presents distinct challenges, especially in the domain of assessment. As students gain access to sophisticated tools capable of generating detailed responses, there emerges an urgent need to redefine what authentic student work looks like and how to effectively evaluate it. Redesigning assessment should no longer emphasise only the destination (final product, grades or degrees) but the journey of learning itself (process). Addressing these challenges requires a deep understanding of GenAI&#x2019;s capabilities and limitations, coupled with innovative assessment strategies that emphasise critical thinking, originality, and a comprehensive understanding of the subject matter.</p>
<p>Recognising the intricacies of this new educational environment, it becomes pivotal for educators and institutions to devise adaptive strategies that leverage AI&#x2019;s strengths while safeguarding the core principles of genuine learning and academic integrity. In this section, we will delve into diverse strategies aimed at preserving the integrity and enhancing the effectiveness of assessments in the GenAI era. Subsequently, in <a href="#sec4_6">Section 4.6</a>, we will provide a framework that highlight the assessment types best tailored for the AI-driven landscape.</p>
<p>Outlined here are <b>Six Assessment Redesign Pivotal Strategies (SARPS)</b>, each addressing a specific aspect of assessment in the AI era:</p>
<aside class="box ruled" aria-label="box4_1">
<ol class="lower-roman">
<li><b>Integrate Multiple Assessment Methods:</b> Diversifying assessment techniques to cater to varied learning styles and reduce over-reliance on a singular approach.</li>
<li><b>Promote Authentic Assessments in the AI Era:</b> Shifting focus towards real-world problem solving, encouraging students to apply knowledge in tangible scenarios.</li>
<li><b>Promoting Academic Integrity and Genuineness in the AI Era:</b> Ensuring that despite AI&#x2019;s assistance, the essence of a student&#x2019;s original thought remains intact and authentic.</li>
<li><b>Embracing AI as a Learning Partner:</b> Rather than viewing AI as merely a tool or threat, this approach celebrates the collaborative potential of human and machine in the learning process.</li>
<li><b>Prioritising Soft Skills in Assessments in the AI Era:</b> While AI dominates technical proficiency, the irreplaceable human attributes of empathy, leadership, and communication gain paramount importance.</li>
<li><b>Prioritising Feedback Over Grades in the AI Era:</b> Advocating for a paradigm shift from traditional grading systems to a more holistic feedback-driven approach.</li></ol></aside>
<p><span epub:type="pagebreak" id="p95" aria-label=" page 95. " role="doc-pagebreak"/>Each of these strategies, while distinct in its approach, converges on a shared goal, ensuring that higher education assessments remain real, robust, relevant, and resonant in the age of AI. In the subsequent sections, we will delve deeper into each of these facets, exploring their significance, application, and potential for transforming the assessment landscape in higher education.</p>
<section aria-labelledby="sec4_5_1">
<h3 id="sec4_5_1"><span epub:type="ordinal">4.5.1 </span>Integrate Multiple Assessment Methods</h3>
<p>Assessment is more than just a measure of student learning; it is a multi-faceted tool that can inform instruction, motivate learners, and provide a comprehensive picture of a student&#x2019;s skills, knowledge, and understanding. In today&#x2019;s complex academic landscape, heightened by the advent of technologies like GenAI, relying on a singular method or source of assessment is insufficient. Here are the reasons why and how integrating multiple assessment source and methods can be beneficial:</p>
<p class="list-title"><b>Diversity in Perspective via Multiple Assessment Sources:</b></p>
<p class="noindent">Embracing varied assessment sources becomes vital to cater to the diverse needs of students and ensure a holistic evaluation of their abilities. Among these sources are:</p>
<ul class="disc">
<li>AI-assisted assessment: These leverage the computational prowess of AI to provide quick and objective feedback, especially useful for large cohorts of students or for preliminary checks. For instance, AI can instantly grade multiple-choice or fill-in-the-blank type questions, ensuring consistent and unbiased assessment.</li>
<li>Peer-reviewed assessment: Engaging peers in the review process can foster collaborative learning and offer different perspectives on a piece of work. Peer reviews can also aid in enhancing soft skills such as critical appraisal and constructive feedback. Additionally, by evaluating their peers, students often reflect upon and reinforce their own understanding (<a href="#ref4_13" id="R_ref4_13" epub:type="biblioref" role="doc-biblioref">Chan &#x0026; Wong, 2021</a>).</li>
<li>Teacher-led assessment: These remain irreplaceable, given the depth of feedback and the insightful understanding instructors bring to the table. They can identify not just the correctness of an answer but the underlying thought process, creativity, and problem-solving strategies applied.</li></ul>
<ul class="simple">
<li><b>Mitigation of Bias and Error: </b>Every assessment method has inherent biases and potential for error. For instance, AI assessment might lack perception, while human assessment can sometimes be influenced by implicit biases. By integrating multiple methods, these biases can be counteracted, leading to a more balanced and fair assessment.</li>
<li><b>Holistic Understanding:</b> Different methods probe different facets of understanding. While AI might excel in assessing quantitative skills or factual knowledge, peer reviews can shed light on communicative skills, and teachers can gauge deeper cognitive skills, analytical abilities, and creativity. Together, they provide a more rounded view of a student&#x2019;s capabilities.</li>
<li><b>Flexibility and Adaptability:</b> As the course progresses, educators can choose to lean more on one method over another based on the nature of the content or the skills being assessed. For instance, a coding assignment might benefit from both AI assessment (for syntax and basic functionality) and peer/teacher reviews (for code efficiency, structure, and documentation).</li>
<li><b><span epub:type="pagebreak" id="p96" aria-label=" page 96. " role="doc-pagebreak"/>Feedback Richness:</b> Multiple assessment methods ensure diverse feedback. AI can provide immediate responses, peers can offer relatability and shared learning insights, and teachers can provide expert critiques. This richness can be invaluable for a student&#x2019;s growth and understanding.</li></ul>
<p>In an era where technology is rapidly influencing pedagogical practices, striking a balance between human touch and computational efficiency becomes pivotal. By integrating AI-assisted, peer-reviewed, and teacher-led assessments, educators can harness the strengths of each, ensuring a thorough, equitable, and comprehensive assessment process.</p></section>
<section aria-labelledby="sec4_5_2">
<h3 id="sec4_5_2"><span epub:type="ordinal">4.5.2 </span>Promote Authentic Assessments in the AI Era</h3>
<p>In a rapidly evolving educational landscape where AI-driven tools are becoming increasingly sophisticated, traditional assessment methods often fall short. As mentioned earlier, the true measure of a student&#x2019;s understanding and skills transcends rote memorisation and regurgitation. Authentic assessments, which mimic real-world challenges and require students to apply their knowledge in practical contexts, have become pivotal in this context. Here are some reasons why and how they can be beneficial.</p>
<ul class="simple">
<li><b>Real-world Application:</b> Authentic assessments focus on tasks that mirror real-world challenges. For example, instead of merely asking business students to memorise marketing theories, they might be tasked with creating a full-fledged marketing campaign for a product, factoring in current market trends, budget constraints, and target demographics.</li>
<li><b>Depth of Understanding:</b> Traditional exams may test knowledge at a surface or non-authentic level, but real-world projects and case studies force students to delve deep, synthesise various pieces of information, and apply their knowledge in holistic ways. This approach ensures that students are not just memorising content but truly understanding and internalising it, it is also more meaningful for the students.</li>
<li><b>Diverse Skill Assessment:</b> Projects, case studies, and problem-solving scenarios often require a range of competencies, such as research, collaboration, critical thinking, creativity, and communication, among others. This multi-faceted approach provides a more comprehensive picture of a student&#x2019;s abilities.</li>
<li><b>Reducing the reliance on AI assistant:</b> Tasks like oral examinations, presentations, and interviews inherently require human interaction, making it difficult for students to rely heavily on AI for answers. Such formats not only test knowledge but also gauge a student&#x2019;s communication skills, confidence, and ability to think on their feet.</li>
<li>In the age of AI, authentic assessments that simulate real-world challenges and integrate diverse skills offer a robust way to evaluate and foster genuine student learning and growth. Such assessments ensure that the essence of education, which is the holistic development of an individual, remains uncompromised.</li></ul></section>
<section aria-labelledby="sec4_5_3">
<h3 id="sec4_5_3"><span epub:type="ordinal">4.5.3 </span>Promoting Academic Integrity and Genuineness in the AI Era</h3>
<p>The academic landscape is facing unprecedented challenges concerning integrity and original thought. While AI offers myriad tools that can enrich the learning experience, its ubiquitous presence can tempt students into undue reliance, often blurring the lines between <span epub:type="pagebreak" id="p97" aria-label=" page 97. " role="doc-pagebreak"/>assistance and academic dishonesty. Consequently, it is crucial to establish strong ethical foundations in students and stress the importance of genuine scholarly effort.</p>
<ul class="simple">
<li><b>Understanding AI&#x2019;s Role in Academia:</b> Institutions should offer AI literacy workshops or courses that explicitly address the capabilities and limitations of AI in academic study and research. By understanding what AI can and cannot do, students are better equipped to use it as a tool rather than a crutch. Showcasing case studies on both positive and negative applications of AI in academic contexts can help students understand real-world implications such as the AI literacy course mentioned in <a href="ch2.xhtml">Chapter 2</a>.</li>
<li><b>Reinforcing Academic Integrity:</b> Beyond traditional honour codes, institutions should create AI-specific guidelines, elucidating what constitutes misuse. For example, using AI to generate essay content could be flagged as dishonest, while using AI to analyse data for that essay might be acceptable with proper attributions. Multiple assessment approaches can be used wherein AI tools may not be as effective, ensuring students rely on their understanding and abilities.</li>
<li><b>Ethics Workshops:</b> Regular workshops emphasising ethical considerations in using AI can be beneficial. Topics could range from the philosophical implications of AI dependency to more concrete discussions on plagiarism in the AI age. Engaging students in debates and discussions about these topics can foster a deeper understanding and appreciation of academic ethics.</li>
<li><b>Consequences and Accountability:</b> Clearly defined consequences for AI-related academic misconduct should be established. This can range from redoing assignments to more severe actions for repeated offenses. Peer review systems, where students assess the work of their peers, can also help in maintaining accountability, as students become stakeholders in the process of upholding integrity.</li>
<li><b>Celebrating Original Thought:</b> Institutions should prioritise and celebrate genuine student innovation and effort. Awards or recognitions for outstanding original research or projects can serve as incentives for students to put in authentic work. Encouraging students to publish or present their genuine work can also motivate them to strive for originality and deep understanding, rather than superficial completion of tasks.</li></ul>
<p>Through comprehensive guidelines, consistent ethics training, and a culture that celebrates originality, institutions can foster an environment where students leverage AI responsibly, maintaining the sanctity of academic pursuits.</p></section>
<section aria-labelledby="sec4_5_4">
<h3 id="sec4_5_4"><span epub:type="ordinal">4.5.4 </span>Embracing AI as a Learning Partner</h3>
<p>The dawning of the AI era has significantly transformed the educational ecosystem. Instead of approaching AI with trepidation and merely as a challenge to academic integrity, educators and institutions can channel its profound capabilities to enrich and enhance learning experiences. By positioning AI as a collaborative tool, rather than just a potential problem, students and educators can unearth fresh avenues of mutual learning and reasoning, all while retaining the essence of human intuition and insight.</p>
<ul class="simple">
<li><b>The Dynamics of Dual Learning:</b> Propose assignments where students and AI tackle problems together. For example, in a mathematics assignment, while AI can provide solutions, students can be tasked with explaining the reasoning or even identifying potential <span epub:type="pagebreak" id="p98" aria-label=" page 98. " role="doc-pagebreak"/>errors in the AI&#x2019;s methods. Encourage students to compare their reasoning processes with AI. This could be in the form of essays or reflections where students evaluate the differences in approach, advantages, and limitations of both human and AI logic.</li>
<li><b>Integrating AI in Research:</b> Students can utilise AI tools for vast data analysis tasks particularly with AI tools such as Code Interpreter, allowing them to focus on interpreting results, drawing connections, and crafting conclusions. AI can help students in scouring extensive databases to find relevant literature, with students then critically analysing and synthesising the information.</li>
<li><b>Facilitating Interactive Learning:</b> Use AI-driven platforms to offer personalised learning experiences. These platforms can identify individual student weaknesses and strengths, providing tailored resources and exercises. In some disciplines, AI can also help create accurate simulations or models based on students&#x2019; hypotheses, allowing them to test and refine their ideas in real time. For example, the Center of Innovation in Nursing Education at the University of Hong Kong offers a unique blend of traditional nursing training and cutting-edge technology. The center boasts sophisticated computerised manikins and a simulated clinical environment, serving as a hub for nursing students to sharpen their skills in client care and clinical decision-making. Nursing students of all levels interact with lifelike manikins, engage in virtual reality scenarios, work with robots, and utilise other state-of-the-art equipment to simulate real-world procedures and situations in a risk-free setting. Integration with AI has the potential to further enhance this learning environment. An AI-driven platform can tailor simulations based on individual student performance, pinpointing specific areas of weakness and strength. Such a platform, in future research, could dynamically adjust scenarios to ensure that students are consistently challenged, thereby facilitating deeper learning and understanding. Especially in disciplines like nursing, where precision is paramount, AI can generate accurate simulations or models. This allows students to test hypotheses, make informed decisions, and witness real-time results without the repercussions of real-world consequences.</li>
<li><b>Feedback and Continuous Improvement:</b> AI can provide students with immediate feedback on assignments, enabling them to understand mistakes and rectify them promptly. And, over time, AI can track a student&#x2019;s progress, helping educators identify areas that might need reinforcement or further instruction.</li>
<li><b>Promoting Creativity:</b> For subjects like art and music, AI tools can suggest compositions or designs based on current trends, with students then adding their unique touch or interpretation.</li></ul>
<p>In reframing the narrative around AI in education, from mere utility or challenge to an active collaborative partner, we pave the way for a future where technology and human intellect operate in tandem. Such an alliance not only enriches the learning process but also equips students with the skills and perspectives necessary for a world where AI is increasingly interwoven into the fabric of daily life. By championing collaboration over confrontation, educators can foster a dynamic environment of growth, exploration, and mutual respect between students and the digital tools at their disposal.</p></section>
<section aria-labelledby="sec4_5_5">
<h3 id="sec4_5_5"><span epub:type="ordinal">4.5.5 </span>Prioritising Soft Skills in Assessments in the AI Era</h3>
<p>While AI can sift through vast datasets, generate answers to complex questions, and even simulate human-like tasks, it still falls short in replicating the intrinsic qualities that make <span epub:type="pagebreak" id="p99" aria-label=" page 99. " role="doc-pagebreak"/>us inherently human. These qualities, often referred to as &#x201C;soft skills&#x201D;, encompass our emotional intelligence, empathy, communication abilities, teamwork, leadership, and more. Prioritising the assessment of these skills ensures that the next generation is not just technologically adept but also socially and emotionally competent.</p>
<ul class="simple">
<li><b>The Inimitable Essence of Soft Skills:</b> Soft skills are complex and multi-faceted, often shaped by individual experiences, cultural backgrounds, and personal values. While AI might be able to mimic a sympathetic response, genuine empathy, arising from understanding another&#x2019;s feelings, remains a uniquely human trait, this is why AI affectiveness for human emotions (i.e.) the third element of the AI literacy framework is important (see <a href="ch2.xhtml">Ch 2</a>, <a href="ch2.xhtml#sec2_5">section 2.5</a>). The real essence of teamwork lies in understanding, compromise, and mutual respect, elements challenging for any algorithm to grasp fully. Soft skills development is beyond any algorithm. This is echoed by our research findings, a student mentioned &#x201C;<i>this may lead to a decrease in critical thinking and make decisions only based on the information that AI provides to them</i>.&#x201D; For more information on soft skills, please check out the Holistic Competency and Virtue Education website &#x2013; <a href="https://www.have.hku.hk">https://www.have.hku.hk/</a>.</li>
<li><b>The Long-term Benefits of Soft Skills:</b> As Prof. Peter Salovey, the current president of Yale University, aptly puts it, &#x201C;<i>I.Q. gets you hired, but E.Q. gets you promoted</i>.&#x201D; Technical skills might land you a job, but it is often the soft skills that propel you into promotions, leadership roles, and underpin long-term career success. A society where individuals can communicate effectively, lead with empathy, and collaborate seamlessly is undoubtedly more resilient and harmonious.</li>
<li><b>AI&#x2019;s Role in Enhancing Soft Skills Education:</b> Although soft skills seem to be not penetrable by AI, AI can still assist. AI can analyse student interactions in virtual classrooms to identify areas where soft skills might be lacking, providing educators with insights to fine-tune their teaching methods. In addition, AI-driven platforms can design personalised soft skills training modules for students, ensuring that each individual&#x2019;s unique needs are addressed.</li></ul>
<p>While AI continues to reshape the academic horizon, it is imperative that the education system not lose sight of the intrinsic human qualities that AI can&#x2019;t replicate. By integrating soft skills assessment into the curriculum and leveraging platforms, educators can ensure that students are equipped not just with knowledge but also with the essential skills to navigate the complex tapestry of human interactions and challenges in the real world. The International Holistic Competency Foundation encourages courses with soft skills development to accredit their courses, providing students the evidence and recognition of their skills. More details can be found at <a href="https://www.ihcfoundation.net">https://www.ihcfoundation.net/</a>.</p></section>
<section aria-labelledby="sec4_5_6">
<h3 id="sec4_5_6"><span epub:type="ordinal">4.5.6 </span>Prioritising Feedback Over Grades in the AI Era</h3>
<p>AI provides us an opportunity for a paradigm shift in how students are assessed in educational settings. The traditional grading system, which often boils down a student&#x2019;s performance to a single score, might not adequately capture the depth and breadth of a student&#x2019;s learning journey. Feedback, unlike a static grade, provides insights into the specifics of what a student understood, partially grasped, or missed entirely. This not only gives students a clearer picture of their strengths and areas for improvement but also promotes a growth mindset. Instead of seeing their abilities as fixed, students can view mistakes as <span epub:type="pagebreak" id="p100" aria-label=" page 100. " role="doc-pagebreak"/>opportunities for growth and refinement. Emphasising feedback over grades addresses this by prioritising holistic understanding and continuous improvement.</p>
<ul class="simple">
<li><b>Immediate Feedback Through AI:</b> One of the advantages of integrating AI into the educational process is its ability to provide instant feedback. For example, if a student is working on a math problem or writing an essay, AI can immediately point out calculation errors or grammatical mistakes. This allows the student to correct and learn from errors in real time, rather than waiting for a graded paper to be returned days or weeks later. Immediate feedback is known to enhance learning, as the connection between action and consequence is fresh in the student&#x2019;s mind.</li>
<li><b>Human Machine co-feedback:</b> While AI can provide swift feedback on objective elements, human instructors bring a depth and nuance that AI currently cannot replicate. They can delve into the subtleties of a student&#x2019;s thought process, offer insights on the effectiveness of their argumentation, or even gauge and provide feedback on softer skills such as teamwork or oral presentations. This combination of AI&#x2019;s immediacy and human depth ensures a comprehensive feedback mechanism.</li>
<li><b>Reduced Stress and Anxiety:</b> Traditional grading systems can induce significant stress, as students might fixate on achieving a particular score rather than truly understanding the material. Feedback-centric systems prioritise understanding and growth, potentially reducing the performance pressure students often feel. Additionally, since students sometimes hesitate to ask teachers for feedback on what they perceive as minor issues, AI can provide assistance without any reservations from the students. As a student mentioned, <i>&#x201C;ChatGPT won&#x2019;t judge me.&#x201D;</i></li>
<li><b>Encouraging Lifelong Learning:</b> In the professional world, continuous feedback is a norm. Whether it is a project review, client feedback, or performance appraisals, feedback forms the core of professional growth. By emphasising feedback over grades, educational institutions better prepare students for this reality, fostering adaptive learners equipped to handle the ever-evolving challenges of the AI era.</li>
<li><b>Practical Considerations:</b> Implementing a feedback-focused system would require structural changes. There would be a need for platforms (potentially AI-driven) that allow for continuous student submissions, immediate feedback loops, and easy tracking of student progress over time. Faculty training would also be paramount, ensuring educators can provide constructive feedback and utilise AI tools effectively.</li></ul>
<p>While grades have been a staple of assessment for a long time, the AI era presents an opportune moment to reevaluate their dominance. Transitioning towards a feedback-centric approach, integrating both AI and human insights, ensures that assessments align more closely with the ultimate goal of education &#x2013; that is, fostering understanding, curiosity, and continuous growth.</p></section></section>
<section aria-labelledby="sec4_6">
<h2 id="sec4_6"><span epub:type="ordinal">4.6 </span>AI Assessment Integration Framework</h2>
<p>The inexorable march of AI into the educational sector beckons a rethinking of traditional assessment methods. As educators grapple with the possibilities and challenges posed by AI, it becomes imperative to both understand its capabilities and to find ways to integrate these capabilities harmoniously into assessment strategies. The AI Assessment Integration Framework is an endeavour in this direction, presenting nine distinct types of assessments, each thoughtfully designed to address different facets of learning and evaluation in the AI era.</p>
<p><span epub:type="pagebreak" id="p101" aria-label=" page 101. " role="doc-pagebreak"/>Rather than totally replacing traditional assessment methods, the &#x201C;AI Assessment Integration&#x201D; framework seeks to enhance them, acknowledging that while AI can be a potent tool for certain tasks, there are uniquely human skills and competencies that it cannot replicate and are developed in conjunction with the challenges we so far studied and came across as well as the insights from literature (<a href="#ref4_17" id="R_ref4_17" epub:type="biblioref" role="doc-biblioref">Dillion et al., 2023</a>; <a href="#ref4_29" id="R_ref4_29" epub:type="biblioref" role="doc-biblioref">Lichtenthaler, 2018</a>). This framework is constructed with a vision of symbiosis: marrying the strengths of AI with the depth and breadth of human learning experiences, as shown in <a href="#fig4_2" id="R_fig4_2">Figure 4.2</a>.</p>
<figure id="fig4_2"><img src="images/fig4_2_B.jpg" alt="A list of the components in the A I assessment integration framework."/>
<figcaption><a href="#R_fig4_2" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. "><span epub:type="label">Figure</span> <span epub:type="ordinal">4.2 </span>AI Assessment Integration Framework.</a></figcaption></figure>
<p>Each of the nine types of assessments within the &#x201C;AI Assessment Integration&#x201D; framework represents a distinct lens through which student learning can be evaluated. From Performance-based assessments that focus on real-world applications of knowledge, to Ethical and Societal Impact Assessments that highlight the importance of understanding AI&#x2019;s broader implications, the framework offers a roadmap for educators navigating the intersection of AI and assessment. Through the framework, the hope is to offer educators a structured approach to understanding and harnessing the potential of AI in enhancing educational assessments. The below are the nine categories.</p>
<ol type="1">
<li>Performance-based assessment</li>
<li>Personalised or contextualised type of assessment</li>
<li>Human-centric competency assessment</li>
<li>Human-machine partnership assessment</li>
<li><span epub:type="pagebreak" id="p102" aria-label=" page 102. " role="doc-pagebreak"/>Project- or scenario-based assessment</li>
<li>Time-sensitive AI-generated adaptive assessment</li>
<li>Metacognitive assessment</li>
<li>Ethical and societal impact assessment</li>
<li>Lifelong learning portfolio assessment</li></ol>
<p>We will now discuss each of these assessments in depth, demonstrating with a case scenario for teachers to ponder how they can redesign and integrate their assessment with AI.</p>
<section aria-labelledby="sec4_6_1">
<h3 id="sec4_6_1"><span epub:type="ordinal">4.6.1 </span>Performance-Based Assessment</h3> 
<p>Performance-based assessment stands out as one of the most authentic assessment methods within the educational sphere. Its emphasis on showcasing the application of skills and knowledge in real-world or simulated scenarios is a stark contrast to traditional testing. In higher education, where the goal is not just the acquisition of knowledge but also its <span epub:type="pagebreak" id="p104" aria-label=" page 104. " role="doc-pagebreak"/>practical application, performance-based assessment becomes invaluable. It allows students to demonstrate many competencies, such as critical thinking and problem-solving abilities.</p>
<span epub:type="pagebreak" id="p103" aria-label=" page 103. " role="doc-pagebreak"/>
<aside epub:type="tip" role="doc-tip" class="box shaded">
<h3 id="box4_2"><span epub:type="label">Case Scenario</span> <span epub:type="ordinal">1</span> Performance-based assessment in a Business Course with AI Integration</h3>
<ul class="simple">
<li><b><i>Scenario Overview</i>:</b> Students in a business course are grouped into teams. Each team is tasked with developing a five-year business strategy for a hypothetical startup, starting with the current financial figures provided by the teacher for Year 1. Beyond the financial figures, teams must also dive deep into product research, understand market positioning, devise marketing strategies, and analyse competitors. AI will serve as a guiding tool in this endeavour, offering insights, analysing projections, and validating the various facets of their strategy.</li>
<li><b>Learning Outcomes:</b>
<ol type="1">
<li>Students will critically analyse financial data, products, and market trends to derive informed business decisions.</li>
<li>Students will effectively communicate insights, engage in constructive discussions, and collaboratively formulate a comprehensive business strategy.</li>
<li>Students will integrate AI-derived insights to enhance, refine, and validate their strategic decisions, particularly in product placement and competitive analysis.</li>
<li>Students will conceptualise and articulate a coherent vision for the company&#x2019;s future, aligning financial choices with overarching business objectives, product positioning, and market strategy.</li></ol></li>
<li>
<p><b>Procedures:</b></p>
<ul class="simple">
<li><b><i>Initial Setup and Grouping</i>:</b> Students are grouped into teams. Teacher provides complete financial data for Year 1.</li>
<li><b><i>Product &#x0026; Market Research</i>:</b> Conduct a thorough analysis of potential products. Identify market gaps, opportunities, and positioning strategies.</li>
<li><b><i>Competitive Analysis</i>:</b> Teams analyse competitors using both traditional research methods and AI tools.</li>
<li><b><i>Strategy Formulation</i>:</b> Devise marketing and positioning strategies. Draft a cohesive business strategy for Years 2&#x2013;5, ensuring alignment with Year 1 data. Use AI tools to refine, validate, and strengthen the overall strategy.</li>
<li><b><i>Final Presentation and Submission</i>:</b> Teams present their strategies, covering financial forecasts, product placement, marketing strategies, and competitive positioning.</li>
<li><b><i>Feedback and Grading</i>:</b> Teacher evaluates the presentations based on the provided rubric. Teams receive feedback and final grades.</li></ul></li>
<li><b>Sample Assessment Rubrics:</b>
<section class="tscroll">
<table id="box-table1">
<thead>
<tr>
<th scope="col" class="left">Criteria</th>
<th scope="col" class="left">Excellent</th>
<th scope="col" class="left">Proficient</th>
<th scope="col" class="left">Average</th>
<th scope="col" class="left">Poor</th></tr></thead>
<tbody>
<tr>
<td class="left"><b>Analytical Use of Year 1 Data</b></td>
<td class="left">Seamlessly integrates Year 1 data into future predictions with clear rationale.</td>
<td class="left">Uses Year 1 data for predictions but may have minor inconsistencies.</td>
<td class="left">Basic understanding of Year 1 data but struggles with its application.</td>
<td class="left">Misinterprets or disregards Year 1 data.</td></tr>
<tr>
<td class="left"><b>Product &#x0026; Market Research</b></td>
<td class="left">Provides deep insights into the product and market with actionable strategies.</td>
<td class="left">Solid understanding of product and market but may lack some nuanced insights.</td>
<td class="left">Shows a basic understanding of product and market.</td>
<td class="left">Superficial or incorrect analysis of product and market.</td></tr>
<tr>
<td class="left"><b>Competitive Analysis</b></td>
<td class="left">Comprehensive understanding of competitors with clear differentiation strategies.</td>
<td class="left">Identifies major competitors and outlines differentiation but may miss out on niche players.</td>
<td class="left">Basic understanding of a few competitors.</td>
<td class="left">Inadequate or incorrect competitive analysis.</td></tr>
<tr>
<td class="left"><b>Utilisation of AI Insights</b></td>
<td class="left">Demonstrates mastery in leveraging AI insights, resulting in a comprehensive and data-backed strategy.</td>
<td class="left">Uses AI insights effectively but may miss out on some nuanced recommendations.</td>
<td class="left">Utilises AI but lacks critical analysis of its insights.</td>
<td class="left">Over-reliance on or total disregard of AI insights.</td></tr>
<tr>
<td class="left"><b>Strategic Vision and Justification</b></td>
<td class="left">Offers a compelling vision, integrating financial decisions, product positioning, and market strategy.</td>
<td class="left">Presents a coherent vision but might lack depth in integrating all business elements.</td>
<td class="left">Outlines a basic strategy with some disconnections between elements.</td>
<td class="left">Lacks clarity and coherence in strategic vision.</td></tr></tbody></table></section></li>
<li><b>Description of how AI is integrated and used in the Assessment:</b> This project requires students to plan and develop a business strategy by integrating financial planning with a deep understanding of the product, market positioning, marketing strategies, and competitor analysis. Starting with real financial data from Year 1, students are expected to craft a comprehensive narrative for the hypothetical company&#x2019;s next four years. This narrative should be supported by robust product research in the actual market, an understanding of market dynamics, and a solid marketing strategy. As they journey through the complexities of business planning, AI tools can help with some of the data analysis, provide some ideas to enhance their strategies. The ultimate test lies in their discernment &#x2013; knowing when to lean on AI insights and when to trust their own research and judgment. The concluding presentation is their opportunity to defend their integrated business strategy, showcasing both depth and breadth in their understanding of running a business.</li></ul></aside></section>
<section aria-labelledby="sec4_6_2">
<h3 id="sec4_6_2"><span epub:type="ordinal">4.6.2 </span>Personalised or Contextualised Assessment</h3>
<p>Personalised or contextualised assessment operates on the principle that every student&#x2019;s academic journey is a unique trajectory. In higher education, each student embarks on a unique learning journey, influenced by diverse backgrounds, experiences, and goals. Personalised or contextualised assessment opposes the usual one-size-fits-all approach, the assessment provides uniqueness in student&#x2019;s individual progress and comprehension levels. As students navigate through the assessment, the complexity and context of responses vary based on their learning profile and journey, providing a genuine piece of student work.</p>
<span epub:type="pagebreak" id="p105" aria-label=" page 105. " role="doc-pagebreak"/>
<aside epub:type="tip" role="doc-tip" class="box shaded">
<h3 id="box4_3"><span epub:type="label">Case Scenario</span> <span epub:type="ordinal">2</span> Personalised Poetry Analysis</h3>
<ul class="simple">
<li><b>Scenario Overview:</b> In a literature course at a higher education institution, students are tasked with selecting a poem that resonates deeply with them. The poem may connect with their personal history, cultural background, or current life events. Once chosen, students use an AI-assisted tool to analyse the literary techniques, historical context, and personal significance of the poem. By doing so, they craft a critical composition that intertwines rigorous literary analysis with personal narrative.</li>
<li><b>Learning Outcomes:</b>
<ol type="1">
<li>Students will demonstrate a profound understanding of their chosen poem&#x2019;s literary techniques and structure.</li>
<li>Students will critically analyse the historical and sociocultural context of the poem.</li>
<li>Students will articulate the personal significance and resonance of the poem in their lives.</li>
<li>Students will craft a well-structured and coherent composition that integrates both literary critique and personal reflection.</li>
<li>Students will utilise AI-driven insights to enhance their understanding and analysis of the poem.</li></ol></li>
<li>
<p><b>Procedures:</b></p>
<ul class="simple">
<li><b><i>Introduction and Orientation</i>:</b> Explain the objectives, significance, and structure of the poetry analysis assignment. Introduce the AI research tool and guide students on its optimal usage.</li>
<li><b><i>Poem Selection</i>:</b> Students choose a poem, documenting briefly why it resonates with them.</li>
<li>The teacher approves the selection to ensure literary depth and scope for analysis.</li>
<li><b><i>Research and Analysis Phase</i>:</b> Using libraries, online databases, and the AI tool, students research their poem&#x2019;s literary techniques and historical context.</li>
<li>AI assistance helps by suggesting related literature, contextual readings, and analytical insights.</li>
<li><b><i>Draft Submission</i>:</b> Students submit an initial draft of their analysis. Peer reviews are facilitated, with students guided to provide constructive feedback.</li>
<li><b><i>Final Essay Submission</i>:</b> The final composition should be a blend of literary analysis and personal reflection, supported by research and AI-driven insights.</li>
<li><b><i>Oral Presentation</i>:</b> Students share their personal journey with the poem, discussing both the literary merits and personal connections they&#x2019;ve discovered.</li>
<li><b><i>Reflection and Feedback Session</i>:</b> Students reflect on the assessment experience, with a focus on the integration of AI tools in their research and analysis.</li></ul></li>
<li><b>Sample Assessment Rubrics:</b>
<section class="tscroll">
<table id="box-table2">
<thead>
<tr>
<th scope="col" class="left">Criteria</th>
<th scope="col" class="left">Excellent</th>
<th scope="col" class="left">Proficient</th>
<th scope="col" class="left">Average</th>
<th scope="col" class="left">Poor</th></tr></thead>
<tbody>
<tr>
<td class="left"><b>Understanding of Literary Techniques</b></td>
<td class="left">Demonstrates mastery over literary techniques, providing insightful analyses</td>
<td class="left">Shows a strong understanding with minor inaccuracies</td>
<td class="left">Demonstrates basic understanding but lacks depth</td>
<td class="left">Misunderstands or overlooks key techniques</td></tr>
<tr>
<td class="left"><b>Analysis of Historical Context</b></td>
<td class="left">Provides a comprehensive and nuanced understanding of the poem&#x2019;s context</td>
<td class="left">Understands main historical points but may miss subtleties</td>
<td class="left">Offers a superficial or generalised historical context</td>
<td class="left">Lacks clarity or accuracy in historical context</td></tr>
<tr>
<td class="left"><b>Articulation of Personal Significance</b></td>
<td class="left">Profoundly connects personal narrative with poem, showing deep introspection</td>
<td class="left">Clearly articulates personal connection but may lack depth</td>
<td class="left">Makes vague or generalised personal connections</td>
<td class="left">Struggles to connect personal narrative to poem</td></tr>
<tr>
<td class="left"><b>Essay Structure and Coherence</b></td>
<td class="left">Composition is meticulously structured with seamless integration of analysis and narrative</td>
<td class="left">Well-structured with minor organisational lapses</td>
<td class="left">Shows attempt at structure but may lack flow</td>
<td class="left">Disorganised or disjointed composition</td></tr>
<tr>
<td class="left"><b>Use of AI-driven Insights</b></td>
<td class="left">Integrates AI insights expertly, enhancing analysis depth</td>
<td class="left">Effectively uses AI suggestions to support points</td>
<td class="left">Occasionally references AI insights without clear integration</td>
<td class="left">Over-relies on AI without personal interpretation or critique</td></tr></tbody></table></section></li>
<li><b>Description of how AI is integrated and used in the Assessment:</b> The Personalised Poetry Analysis assessment offers students a deep dive into the world of literature, artfully balanced with introspective exploration. Unlike traditional literary critiques, this assessment values the intersection of personal narratives with literary analysis, championing the belief that true understanding of poetry often lies at this crossroad. Students are not mere analysts but active participants in the poetic discourse, bringing their unique life stories into the literary landscape.</li></ul></aside>
<p><span epub:type="pagebreak" id="p106" aria-label=" page 106. " role="doc-pagebreak"/>Central to this assessment is the integration of AI. Beyond its function as a research assistant, the AI tool acts as an analytical companion, suggesting relevant readings, and offering critical insights. It provides layers of depth to the literary exploration, ensuring students have a well-rounded understanding. However, the AI&#x2019;s role remains complementary; students must still chart their own journey, interpreting both the poem and the AI&#x2019;s insights through their personal lens. This marriage of technology, literature, and personal narrative creates a truly enriching academic experience, pushing boundaries and redefining literary analysis in the modern age.</p></section>
<section aria-labelledby="sec4_6_3">
<h3 id="sec4_6_3"><span epub:type="ordinal">4.6.3 </span>Human-Centric Competency Assessment</h3>
<p>Human-centric competency assessment is a method that evaluates a set of skills that are uniquely human and difficult for AI to replicate. These skills can range from leadership and teamwork to empathy and creative problem solving. Given the rapid advancements in automation, focusing on these competencies ensures that students hone skills that are both valuable in the workforce and resistant to automation.</p>
<span epub:type="pagebreak" id="p107" aria-label=" page 107. " role="doc-pagebreak"/>
<aside epub:type="tip" role="doc-tip" class="box shaded">
<h3 id="box4_4"><span epub:type="label">Case Scenario</span> <span epub:type="ordinal">3</span> Leadership and Empathy Training in Healthcare</h3>
<ul class="simple">
<li><b>Scenario Overview:</b> Students in a healthcare course are presented with a set of virtual reality (VR) scenarios that mimic real-life patient care situations. The objective is not to diagnose or treat the patient (as AI can efficiently mimic with that) but to effectively communicate, lead a team under pressure, and demonstrate empathy towards the patient and the patient&#x2019;s family. Each scenario is designed to be challenging, where students have to make tough decisions, communicate effectively, and demonstrate emotional intelligence.</li>
<li><b>Learning Outcomes:</b>
<ol type="1">
<li>Students will effectively lead a team under challenging situations, making crucial decisions that balance medical needs and emotional considerations.</li>
<li>Students will demonstrate empathy and emotional intelligence when communicating with patients and their families.</li>
<li>Students will effectively reflect upon and articulate their decision-making processes, highlighting areas of improvement.</li>
<li>Students will collaboratively discuss and share insights, learning from peers&#x2019; experiences in the VR scenarios.</li></ol></li>
<li>
<p><b>Procedures:</b></p>
<ul class="simple">
<li><b><i>Scenario Introduction</i>:</b> Students are briefed about the VR scenario and the objectives.</li>
<li><b><i>VR Experience</i>:</b> Each student leads a virtual team in the VR scenario, managing patient care while handling sensitive situations and making crucial decisions.</li>
<li><b><i>Post-Scenario Reflection</i>:</b> After the VR session, students write a reflection on their decisions, their communication effectiveness, and areas of improvement.</li>
<li><b><i>Group Discussion</i>:</b> Students share their experiences and learnings in a group, fostering collaborative learning.</li></ul></li>
<li><b>Sample Assessment Rubrics:</b>
<section class="tscroll">
<table id="box-table3">
<thead>
<tr>
<th scope="col" class="left">Criteria</th>
<th scope="col" class="left">Excellent</th>
<th scope="col" class="left">Proficient</th>
<th scope="col" class="left">Average</th>
<th scope="col" class="left">Poor</th></tr></thead>
<tbody>
<tr>
<td class="left"><b>Leadership in Decision-making</b></td>
<td class="left">Demonstrates clear, decisive, and compassionate leadership.</td>
<td class="left">Leads well but occasionally hesitates or overlooks certain team inputs.</td>
<td class="left">Shows some leadership qualities but struggles in critical decision-making.</td>
<td class="left">Lacks decisive leadership; frequently misses crucial decisions.</td></tr>
<tr>
<td class="left"><b>Empathy and Communication</b></td>
<td class="left">Consistently empathetic; communicates with clarity and compassion.</td>
<td class="left">Mostly empathetic but has occasional lapses in communication.</td>
<td class="left">Struggles to balance empathy with clear communication.</td>
<td class="left">Lacks empathy; communication is unclear or inappropriate.</td></tr>
<tr>
<td class="left"><b>Reflective Abilities</b></td>
<td class="left">Provides a deep, insightful reflection on decisions and communication.</td>
<td class="left">Reflects effectively but might miss out on some nuances.</td>
<td class="left">Reflection lacks depth; misses out on key learning moments.</td>
<td class="left">Minimal reflection; lacks insight into own performance.</td></tr>
<tr>
<td class="left"><b>Collaborative Discussion and Input</b></td>
<td class="left">Actively shares, learns, and contributes to group discussions.</td>
<td class="left">Participates in discussions but may occasionally dominate or be too passive.</td>
<td class="left">Occasionally contributes but often remains on the surface of the discussion.</td>
<td class="left">Rarely participates or detracts from the group discussion.</td></tr></tbody></table></section></li>
<li><b>Description of how AI is integrated and used in the Assessment:</b> This immersive VR assessment places students in real-life healthcare scenarios, challenging them not on medical knowledge but on their human-centric competencies. The scenarios are designed to test and develop their leadership, decision-making, and empathetic communication skills. While AI can assist in medical diagnosis and treatment suggestions, the human touch, the ability to understand emotions, lead a team under stress, and communicate with empathy, remains irreplaceable. Through this assessment, students are equipped with valuable skills that remain crucial in the healthcare field, regardless of technological advancements.</li></ul></aside></section>
<section aria-labelledby="sec4_6_4">
<h3 id="sec4_6_4"><span epub:type="pagebreak" id="p108" aria-label=" page 108. " role="doc-pagebreak"/><span epub:type="ordinal">4.6.4 </span>Human&#x2013; Machine Partnership Assessment</h3>
<p>Human&#x2013; machine partnership assessment is a method that evaluates students&#x2019; capabilities to interact, collaborate, and leverage AI tools effectively. As the world increasingly integrates AI into various sectors, the aptitude to synergise with AI becomes a paramount skill. This assessment focuses on tasks that challenge students to utilise AI tools while ensuring their inputs and decisions remain integral to the outcome. The type of assessment used in this method should involve evaluating students&#x2019; proficiency in using these tools. For instance, this could involve a surgery simulation using AI/VR or 3D drawing with AI tools, such as those found on <a href="http://Autodesk.com">Autodesk.com</a>. In these types of assessments, if the AI component is too advanced or does too much of the work, it might overshadow the student&#x2019;s genuine input. For instance, if an AI-powered design tool automatically optimises 90% of a student&#x2019;s project, it is challenging to assess the student&#x2019;s actual skill, thus, teachers need to decide the assessment appropriately.</p>
<aside epub:type="tip" role="doc-tip" class="box shaded">
<h3 id="box4_5"><span epub:type="label">Case Scenario</span> <span epub:type="ordinal">4</span> Urban Planning with AI Integration</h3>
<ul class="simple">
<li><b>Scenario Overview:</b> Students pursuing urban planning are provided with a simulated city environment. Their task is to design a sustainable and efficient urban space, considering factors like traffic, green spaces, utilities, and housing. While AI provides real-time data analytics, projections, and suggestions, students must apply their judgement, creativity, and understanding of urban sociology to produce a well-rounded plan.</li>
<li><b>Learning Outcomes:</b>
<ol type="1">
<li>Students will demonstrate the ability to integrate AI-derived insights into their urban designs.</li>
<li>Students will make informed decisions in their designs that reflect a balance between AI analytics and human understanding of urban dynamics.</li>
<li>Students will articulate the rationale behind their decisions, emphasising how they incorporated or deviated from AI suggestions.</li>
<li>Students will collaboratively review and critique urban plans, incorporating feedback into their final designs.</li></ol></li>
<li>
<p><b>Procedures:</b></p>
<ul class="simple">
<li><b><i>Introduction</i>:</b> Introduce students to the AI tools they will be working with. This includes understanding the software&#x2019;s capabilities, reading and interpreting the AI&#x2019;s data analytics, and learning how to input their own design elements into the system. A hands-on workshop should be facilitated by a subject matter expert, followed by a short Q&#x0026;A session.</li>
<li><b><i>Design Phase with AI</i>:</b> Allow students to produce a preliminary design for the simulated city, employing AI insights. Students will work individually or in groups to create their first urban design draft. They should engage with the AI tool, seeking suggestions and analytics, and responding appropriately in their designs.</li>
<li><b><i>AI Feedback Loop</i>:</b> AI provides analytics on traffic flow, energy efficiency, and other key metrics. Students refine their designs based on AI feedback, and, at the same time, check the AI suggestions. Simultaneously, peer reviews will be organised, allowing students to critique and receive feedback on their designs.</li>
<li><b><i>Refinement</i>:</b> Refine the urban designs based on feedback. Students rework their designs, integrating insights from both AI and peer feedback. They should focus on striking a balance between AI-driven efficiency and human-centric design elements.</li>
<li><b><i>Final Presentation</i>:</b> Students present their urban designs, articulate the reasoning behind their decisions, justify their choices in the context of AI suggestions and their own understanding of urban needs. Proper attributions of AI usage are required.</li></ul></li>
<li><b>Sample Assessment Rubrics:</b> 
<section class="tscroll">
<table id="box-table4">
<thead>
<tr>
<th scope="col" class="left">Criteria</th>
<th scope="col" class="left">Excellent</th>
<th scope="col" class="left">Proficient</th>
<th scope="col" class="left">Average</th>
<th scope="col" class="left">Poor</th></tr></thead>
<tbody>
<tr>
<td class="left"><b>Integration of AI Insights</b></td>
<td class="left">Seamlessly blends AI analytics with human-centred design principles.</td>
<td class="left">Often integrates AI insights but occasionally misses key suggestions.</td>
<td class="left">Integrates some AI suggestions but lacks a cohesive design strategy.</td>
<td class="left">Rarely or inappropriately uses AI insights in design.</td></tr>
<tr>
<td class="left"><b>Decision-making and Justification</b></td>
<td class="left">Decisions are well-informed, balanced, and thoroughly justified.</td>
<td class="left">Makes solid decisions but occasionally lacks clear justification.</td>
<td class="left">Some decisions are uninformed; justification is often superficial.</td>
<td class="left">Decisions lack coherence; fails to justify most choices.</td></tr>
<tr>
<td class="left"><b>Use of Urban Planning Principles</b></td>
<td class="left">Demonstrates mastery of urban planning principles in design.</td>
<td class="left">Applies most principles effectively but has minor inconsistencies.</td>
<td class="left">Applies basic principles but design lacks depth and innovation.</td>
<td class="left">Rarely applies or misapplies urban planning principles.</td></tr>
<tr>
<td class="left"><b>Collaborative Feedback &#x0026; Revision</b></td>
<td class="left">Actively seeks feedback and refines design with insights.</td>
<td class="left">Is receptive to feedback but revisions lack depth.</td>
<td class="left">Occasionally considers feedback but revisions are minimal.</td>
<td class="left">Ignores feedback; shows resistance to revision.</td></tr></tbody></table></section></li>
<li><b>Description of how AI is integrated and used in the Assessment:</b> In this scenario, urban planning students are plunged into the future of design, where AI tools offer real-time data, projections, and potential design suggestions. The challenge, however, isn&#x2019;t solely using the AI tool but intertwining its capabilities with human creativity, foresight, and understanding of urban sociology. While AI might suggest a certain road to alleviate traffic, the student must consider how it affects community spaces, aesthetics, and pedestrian movement. Through this assessment, students not only become proficient in using advanced tools but also in ensuring the human touch remains central in the age of automation.</li></ul></aside></section>
<section aria-labelledby="sec4_6_5">
<h3 id="sec4_6_5"><span epub:type="pagebreak" id="p109" aria-label=" page 109. " role="doc-pagebreak"/><span epub:type="ordinal">4.6.5 </span>Project- or Scenario-Based Assessment</h3>
<p>Project-based assessment (PBA) is an evaluative approach that emphasises the application of knowledge and skills in a practical, real-world context over an extended period. Unlike traditional assessment that might test students on specific content knowledge, PBA focuses on the process of learning itself. It typically encompasses intricate tasks requiring research, collaboration, critical thinking, and presentation. As students work through the multi-faceted challenges, they not only demonstrate understanding but also develop essential life skills such as teamwork, problem solving, and effective communication. The intricate and diverse nature of these tasks, coupled with the individual and collaborative thought processes involved, makes it challenging for AI to duplicate authentically.</p>
<span epub:type="pagebreak" id="p110" aria-label=" page 110. " role="doc-pagebreak"/>
<span epub:type="pagebreak" id="p111" aria-label=" page 111. " role="doc-pagebreak"/>
<span epub:type="pagebreak" id="p112" aria-label=" page 112. " role="doc-pagebreak"/>
<aside epub:type="tip" role="doc-tip" class="box shaded">
<h3 id="box4_6"><span epub:type="label">Case Scenario</span> <span epub:type="ordinal">5</span> Sustainable Community Development Project</h3>
<ul class="simple">
<li><b>Scenario Overview:</b> Students taking an Environmental Science course are tasked with devising a sustainable development plan for a small community. This requires considering factors such as local biodiversity, socio-economic conditions, available resources, and environmental concerns. While AI can provide data analysis and potential models based on various inputs, the students must integrate this information creatively, considering the holistic wellbeing of the community.</li>
<li><b>Learning Outcomes:</b>
<ol type="1">
<li>Students will integrate a deep understanding of environmental principles with socio-economic considerations to create sustainable solutions.</li>
<li>Students will demonstrate collaboration, dividing tasks efficiently and merging individual research seamlessly.</li>
<li>Students will utilise AI for data analysis, interpreting and implementing insights without solely relying on them.</li>
<li>Students will present their findings convincingly, articulating the rationale behind each decision and predicting potential community impacts.</li></ol></li>
<li>
<p><b>Procedures:</b></p>
<ul class="simple">
<li><b><i>Initial Briefing</i>:</b> The instructor provides an overview of the project, detailing the objectives and expectations. This includes a demonstration of the available AI tools, their capabilities, and potential applications.</li>
<li><b><i>Group Formation</i>:</b> Students are grouped into teams, promoting diversity in expertise and perspectives. Teams are encouraged to delegate roles based on individual strengths and course learning outcomes.</li>
<li><b><i>Community Study</i>:</b> Teams engage in primary research, which might include surveys, interviews, and site visits to understand the community&#x2019;s current state, needs, and challenges.</li>
<li><b><i>Data Analysis with AI</i>:</b> Using the provided AI tools, students input their gathered data to receive analysed results, patterns, and potential models. The AI can help in aspects like predicting water usage, estimating renewable energy potential, or modelling population growth and its impact.</li>
<li><b><i>Collaborative Planning</i>:</b> Teams convene to discuss their findings and AI insights. Here, they brainstorm sustainable solutions that encompass both environmental and socio-economic aspects. They design their community blueprint with clear justifications for each choice, ensuring that human considerations integrate with AI-generated data.</li>
<li><b><i>Draft Submission</i>:</b> Each team submits an initial draft of their project, detailing their research, design choices, and the rationale behind each decision.</li>
<li><b><i>Peer Review Session</i>:</b> Teams exchange their drafts with a peer group. They critically review the received project, noting strengths, areas of improvement, and providing constructive feedback.</li>
<li><b><i>Iterative Refinement</i>:</b> Based on the feedback from the peer review, teams refine their projects, ensuring all aspects are cohesive and well-reasoned.</li>
<li><b><i>Final Presentation and Submission</i>:</b> Teams present their sustainable community model to the class and the instructor through a conference/forum. They articulate their choices, demonstrate the integration of AI insights, and respond to questions. Alongside, a comprehensive report detailing their research, AI data interpretation, and final model is submitted.</li>
<li><b><i>Feedback and Grading</i>:</b> Post-presentation, the instructor provides feedback on each team&#x2019;s project, highlighting areas of excellence and those that could be improved. The project is graded based on the rubrics provided, and individual feedback is given to enhance learning outcomes.</li></ul></li>
<li><b>Sample Assessment Rubrics:</b>
<section class="tscroll">
<table id="box-table5">
<thead>
<tr>
<th scope="col" class="left">Criteria</th>
<th scope="col" class="left">Excellent</th>
<th scope="col" class="left">Proficient</th>
<th scope="col" class="left">Average</th>
<th scope="col" class="left">Poor</th></tr></thead>
<tbody>
<tr>
<td class="left"><b>Integration of Environmental Principles</b></td>
<td class="left">Demonstrates outstanding integration of environmental and socio-economic considerations.</td>
<td class="left">Shows a balanced understanding of both aspects but may lack some depth.</td>
<td class="left">Tends to favour one aspect over the other, with visible gaps.</td>
<td class="left">Lacks a coherent integration, with many discrepancies.</td></tr>
<tr>
<td class="left"><b>Collaborative Skills</b></td>
<td class="left">Teamwork is seamless, with clear evidence of shared responsibility.</td>
<td class="left">Collaboration is evident, but some areas may lack cohesion.</td>
<td class="left">The project shows some signs of individualism, with disjointed sections.</td>
<td class="left">The project lacks unity, suggesting minimal collaboration.</td></tr>
<tr>
<td class="left"><b>Use of AI Insights</b></td>
<td class="left">Expertly interprets and employs AI data, while maintaining a human touch.</td>
<td class="left">Utilises AI insights effectively but occasionally over-relies on them.</td>
<td class="left">Demonstrates a basic use of AI tools, missing some potential insights.</td>
<td class="left">Either disregards AI insights or overuses them without critical thinking.</td></tr>
<tr>
<td class="left"><b>Presentation Skills</b></td>
<td class="left">Articulates decisions confidently, predicting potential impacts and answering questions effectively.</td>
<td class="left">Presents with clarity but may struggle with some unexpected questions.</td>
<td class="left">Lacks depth in presentation, showing surface-level understanding.</td>
<td class="left">Struggles to articulate decisions or justify choices.</td></tr></tbody></table></section></li>
<li><b>Description of how AI is integrated and used in the Assessment:</b> In the Sustainable Community Development Project, students embark on a journey that encapsulates the essence of Environmental Science, married to the realities of socio-economic needs. This project demands a holistic approach, where students not only study and understand a community&#x2019;s environmental fabric but also its human narrative. Using AI as a complementary tool, students can analyse large datasets and generate potential models. However, the true challenge lies in weaving these insights into a compassionate, practical, and sustainable plan for the community. This balance ensures that while AI aids the process, the final product is genuinely human, reflecting the students&#x2019; creativity, understanding, and empathy.</li></ul></aside></section>
<section aria-labelledby="sec4_6_6">
<h3 id="sec4_6_6"><span epub:type="ordinal">4.6.6 </span>Time-Sensitive AI-Generated Adaptive Assessment</h3>
<p>Time-sensitive AI-generated adaptive assessment harnesses the computational prowess of AI to generate questions on the fly, adapting to the student&#x2019;s level of understanding and ensuring that every student gets a unique set of questions. These assessments are time-bound, making it challenging for students to consult external sources or other AI systems.</p>
<aside epub:type="tip" role="doc-tip" class="box shaded">
<h3 id="box4_7"><span epub:type="label">Case Scenario</span> <span epub:type="ordinal">6</span> Adaptive Assessment in Advanced Mathematics</h3><span epub:type="pagebreak" id="p113" aria-label=" page 113. " role="doc-pagebreak"/>
<ul class="simple">
<li><b>Scenario Overview:</b> In an advanced mathematics course, students are introduced to an adaptive assessment system for their mid-term examination. Instead of the traditional pen-and-paper test, students interact with a digital platform that progressively presents problems ranging from basic calculus to advanced topics like differential equations and linear algebra.</li>
<li>Students begin the exam with foundational problems. Based on their performance on these initial problems, the system identifies areas of strength and weakness, subsequently adjusting the difficulty and topic focus. If a student demonstrates prowess in calculus but struggles in linear algebra, for instance, the system would challenge them further in the former and offer more foundational problems in the latter. Additionally, the questions are presented in a time-sensitive manner, demanding swift comprehension and solution formulation.</li>
<li><b>Learning Outcomes:</b>
<ol type="1">
<li>Students will demonstrate proficiency across a broad spectrum of mathematical topics.</li>
<li>Students will identify and focus on areas requiring further study and refinement.</li>
<li>Students will engage with complex problems, applying critical thinking and advanced mathematical techniques.</li>
<li>Students will showcase adaptability, responding efficiently to escalating difficulty levels.</li>
<li>Students will manage time effectively, juggling complexity and the ticking clock.</li></ol></li>
<li><b>Procedures:</b>
<ol type="1">
<li><b><i>Initial Briefing</i>:</b> Students are briefed about the adaptive nature of the test and the importance of time management.</li>
<li><b><i>Test Commencement</i>:</b> The AI system starts with a set of moderate difficulty questions.</li>
<li><b><i>Adaptive Progression</i>:</b> Depending on student responses, the AI system escalates or de-escalates the difficulty of the next set of questions.</li>
<li><b><i>Completion</i>:</b> After a designated time or after a set number of questions, the assessment concludes.</li>
<li><b><i>Feedback and Grading</i>:</b> The AI provides instant feedback on performance, accuracy, and areas for improvement.</li></ol></li>
<li><b>Sample Assessment Rubrics:</b>
<section class="tscroll">
<table id="box-table6">
<thead>
<tr>
<th scope="col" class="left">Criteria</th>
<th scope="col" class="left">Excellent</th>
<th scope="col" class="left">Proficient</th>
<th scope="col" class="left">Average</th>
<th scope="col" class="left">Poor</th></tr></thead>
<tbody>
<tr>
<td class="left"><b>Broad Mathematical Proficiency</b></td>
<td class="left">Demonstrates mastery across all topic areas.</td>
<td class="left">Shows strong understanding in most areas with minor lapses.</td>
<td class="left">Solid in some areas but struggles in others.</td>
<td class="left">Lacks proficiency in multiple areas.</td></tr>
<tr>
<td class="left"><b>Critical Thinking &#x0026; Problem Solving</b></td>
<td class="left">Consistently applies advanced techniques effectively.</td>
<td class="left">Applies most techniques correctly with occasional oversight.</td>
<td class="left">Sometimes fails to apply the right techniques.</td>
<td class="left">Frequently misapplies mathematical methods.</td></tr>
<tr>
<td class="left"><b>Adaptability to Varying Question Types</b></td>
<td class="left">Seamlessly adjusts to all question complexities.</td>
<td class="left">Adjusts well to most questions but struggles occasionally.</td>
<td class="left">Struggles with significant shifts in difficulty.</td>
<td class="left">Consistently overwhelmed by harder questions.</td></tr>
<tr>
<td class="left"><b>Depth of Mathematical Understanding</b></td>
<td class="left">Provides nuanced and in-depth answers.</td>
<td class="left">Provides mostly comprehensive answers with slight misses.</td>
<td class="left">Answers lack depth in some areas.</td>
<td class="left">Responses are superficial or incorrect.</td></tr>
<tr>
<td class="left"><b>Time Management</b></td>
<td class="left">Balances speed and accuracy perfectly, completing all questions within the stipulated time.</td>
<td class="left">Completes most questions in the given time, with minor compromises on speed or accuracy.</td>
<td class="left">Struggles to maintain pace, leaving some questions out.</td>
<td class="left">Unable to manage time, missing out on many questions</td></tr></tbody></table></section></li>
<li><b>Description of how AI is integrated and used in the Assessment:</b> The Adaptive Assessment in Advanced Mathematics provides a fresh, engaging approach to traditional testing, where students are continually challenged according to their capabilities. This ensures that no two students receive the same set of questions, making it almost impossible to rely on peers or external AI tools. The time constraint further compounds the challenge. While the AI-driven system tailors the difficulty to the student, ensuring a balanced challenge, the time pressure demands speedy comprehension, critical thinking, and efficient problem solving. This combination pushes students to truly understand and internalise the course content, demonstrating genuine mastery under pressure. The digital nature allows immediate feedback, helping students understand their performance in-depth and pinpoint areas for further study.</li></ul></aside></section>
<section aria-labelledby="sec4_6_7">
<h3 id="sec4_6_7"><span epub:type="ordinal">4.6.7 </span>Meta-cognitive Assessment</h3>
<p>Meta-cognitive assessment revolves around evaluating a student&#x2019;s awareness and understanding of their cognitive processes. It is not just about assessing what a student knows, but how they approach and reflect on the learning and problem-solving process. Through this method, educators gain insights into a student&#x2019;s ability to analyse their strategies, foresee their performance on future tasks, and reflect on their thought patterns. By honing in on these introspective capabilities, the assessment focuses on skills that AI tools currently cannot replicate, making it especially relevant in an AI-integrated educational environment.</p>
<span epub:type="pagebreak" id="p114" aria-label=" page 114. " role="doc-pagebreak"/>
<span epub:type="pagebreak" id="p115" aria-label=" page 115. " role="doc-pagebreak"/>
<aside epub:type="tip" role="doc-tip" class="box shaded">
<h3 id="box4_8"><span epub:type="label">Case Scenario</span> <span epub:type="ordinal">7</span> Literary Analysis Metacognitive Assessment</h3>
<ul class="simple">
<li><b>Scenario Overview:</b> In a literature course, students are provided with a complex, multi-faceted short story. Rather than just analysing the story&#x2019;s themes and characters, students are tasked to journal their analytical process step-by-step. They will record how they dissect the narrative, why they choose certain analytical strategies, and predict the effectiveness of their approach before submitting their final analysis. AI tool can be used to provide feedback on their literary analysis, but the emphasis will be on students&#x2019; metacognitive reflections.</li>
<li><b>Learning Outcomes:</b>
<ol type="1">
<li>Students will dissect a literary work, applying various analytical techniques.</li>
<li>Students will articulate the reasoning behind their chosen analytical strategies.</li>
<li>Students will reflect on and evaluate their problem-solving journey after receiving feedback.</li></ol></li>
<li><b>Procedures:</b>
<ul class="disc">
<li><b><i>Introduction</i>:</b> The instructor provides students with the chosen multi-faceted short story to read and study. If students are unfamiliar with the AI tool, conduct a brief session to introduce its functionalities and how it can assist in providing feedback on their literary analysis.</li>
<li><b><i>Journaling Phase</i>:</b> After reading the story, students will note down their initial impressions, emotions, and thoughts. Before diving deep into the analysis, students sketch a brief plan or blueprint of their intended analytical approach.</li>
<li><b><i>Analytical Journaling</i>:</b> As they analyse the story, students maintain a detailed journal of their step-by-step thought processes, analytical strategies employed, and their reasons for employing them.</li>
<li><b><i>Prediction Phase</i>:</b> After completing their analysis but before getting feedback, students write down predictions about the effectiveness of their strategies and the expected feedback.</li>
<li><b><i>AI Feedback Integration (Optional)</i>:</b> Students input their literary analysis (not the metacognitive journal) into the AI tool. The AI tool provides feedback on the depth, coherence, and accuracy of their literary analysis.</li>
<li><b><i>Reflection</i>:</b> Students compare their predictions with the AI&#x2019;s feedback and reflect on discrepancies, accuracies, and their entire analytical journey.</li>
<li><b><i>Final Submission</i>:</b> Students compile their initial impressions, analysis blueprint, detailed analytical journal, predictions, AI feedback, and their reflections.</li>
<li><b><i>Submission</i>:</b> The compiled document is submitted to the instructor for assessment.</li>
<li><b><i>Peer Review (Optional)</i>:</b> Students are grouped in pairs or small groups. Each student/group reviews another&#x2019;s journal, focusing not on the literary analysis but the metacognitive processes described. Groups provide feedback to each other, facilitating a richer understanding of varied analytical approaches and meta-cognitive strategies.</li>
<li><b><i>Instructor Assessment</i>:</b> The instructor reviews each student&#x2019;s submission, assessing both the quality of the literary analysis and the depth and coherence of the metacognitive processes described. Using the rubric provided, the instructor grades each component of the submission.</li>
<li><b><i>Feedback Session</i>:</b> The instructor provides individualised feedback to each student, focusing on strengths and areas of improvement. In a class session, the instructor highlights common patterns observed, best practices, and areas where many students might have struggled, facilitating collective learning.</li></ul></li>
<li><b>Sample Assessment Rubrics:</b>
<section class="tscroll">
<table id="box-table7">
<thead>
<tr>
<th scope="col" class="left">Criteria</th>
<th scope="col" class="left">Excellent</th>
<th scope="col" class="left">Proficient</th>
<th scope="col" class="left">Average</th>
<th scope="col" class="left">Poor</th></tr></thead>
<tbody>
<tr>
<td class="left"><b>Literary Analysis</b></td>
<td class="left">Offers deep and insightful analysis with supporting evidence from the text.</td>
<td class="left">Provides a solid analysis with some depth and textual support.</td>
<td class="left">Shows basic understanding but may miss nuances.</td>
<td class="left">Lacks depth or misinterprets the text.</td></tr>
<tr>
<td class="left"><b>Reasoning for Chosen Strategy</b></td>
<td class="left">Clearly and coherently justifies the chosen analytical approach.</td>
<td class="left">Provides reasoning but might lack some clarity or depth.</td>
<td class="left">Offers limited justification for chosen strategies.</td>
<td class="left">Fails to provide a clear reason or misaligns strategy.</td></tr>
<tr>
<td class="left"><b>Reflection Quality</b></td>
<td class="left">Provides a deep, insightful reflection on their analytical journey, integrating feedback.</td>
<td class="left">Reflects on the process and integrates feedback but may lack some depth.</td>
<td class="left">Offers basic reflection with limited integration of feedback.</td>
<td class="left">Minimal reflection without any meaningful engagement with feedback.</td></tr></tbody></table></section></li>
<li><b>Description of how AI is integrated and used in the Assessment:</b> In this metacognitive assessment, students are pushed beyond traditional literary analysis to engage deeply with their analytical thought processes. They are required to journey introspectively, laying out their strategies, predicting outcomes, and reflecting post-feedback. By journaling their analytical journey, students showcase not only their understanding of the text but also their self-awareness in the analytical process. The AI tool serves to challenge their analytical conclusions, further driving their meta-cognitive reflections and making the assessment a genuine piece of student work.</li></ul></aside></section>
<section aria-labelledby="sec4_6_8">
<h3 id="sec4_6_8"><span epub:type="ordinal">4.6.8 </span>Ethical and Societal Impact Assessment</h3>
<p>An ethical and societal impact assessment is an assessment process used to understand, identify, evaluate, and respond to the potential ethical and societal implications of a particular project, policy, programme, technology, or other initiative. It aims to anticipate and address the unintended and unforeseen consequences that may arise, ensuring that these initiatives align with societal values, human rights, and ethical principles. This type of assessment often involves a participatory approach, consulting with a diverse range of stakeholders, including community members, experts, policymakers, and those potentially affected by the initiative. Instead of just assessing the current impacts, ethical and societal <span epub:type="pagebreak" id="p116" aria-label=" page 116. " role="doc-pagebreak"/>impact assessment tries to anticipate future implications, ensuring that long-term societal effects are considered. The assessment usually concludes with a set of recommendations or strategies to mitigate negative impacts and enhance positive outcomes. While AI can identify patterns and provide data-driven insights, making ethical judgements often requires human intuition. What is deemed ethically acceptable can change over time. AI models, which often rely on historical data, might not always be attuned to these shifts. In addition, capturing and interpreting the diverse values and opinions of stakeholders is a complex task.<span epub:type="pagebreak" id="p117" aria-label=" page 117. " role="doc-pagebreak"/></p>
<aside epub:type="tip" role="doc-tip" class="box shaded">
<h3 id="box4_9"><span epub:type="label">Case Scenario</span> <span epub:type="ordinal">8</span> Ethical and Societal Impact Assessment of a Student-Designed AI Chatbot</h3>
<ul class="simple">
<li><b>Scenario Overview:</b> In a technology-focused course, students are tasked with designing an AI chatbot intended for assisting users in managing their mental well-being. Given the sensitive nature of the application, an ethical and societal impact assessment is essential. The assessment will not only evaluate the technical aspects of the chatbot but will also delve deep into the potential societal and ethical implications of such a tool.</li>
<li>
<p><b>Learning Outcomes:</b></p>
<ol type="1">
<li>Students will demonstrate an understanding of the ethical implications of AI applications, especially in sensitive contexts.</li>
<li>Students will engage a diverse range of stakeholders to gather insights and feedback on the potential societal impacts of their AI chatbot.</li>
<li>Students will apply data-driven insights from AI to shape their ethical considerations.</li>
<li>Students will craft a comprehensive ethical and societal report detailing their findings and recommendations.</li>
<li>Students will reflect on the dynamic nature of ethics in technology and anticipate potential shifts in societal values.</li></ol>
<p>(Note: the technical aspects are not mentioned in the learning outcomes but can be included.)</p></li>
<li>
<p><b>Procedures:</b></p>
<ul class="simple">
<li><b><i>Introduction</i>:</b> Introduce students to the concept of ethical and societal assessment, its importance, and its methodology. Provide a brief overview of the role of AI in this assessment, emphasising both its capabilities and limitations.</li>
<li><b><i>Project Commencement</i>:</b> Students start the design process of their AI chatbot, keeping a log of all decisions and considerations, especially those with potential ethical implications.</li>
<li><b><i>Stakeholder Engagement</i>:</b> Organise focus groups or interviews, where students present their preliminary designs to a diverse range of stakeholders: potential users, mental health professionals, ethicists, etc. Use AI tools to analyse feedback from these sessions, identifying common concerns and patterns.</li>
<li><b><i>Drafting the Report</i>:</b> Based on their design process and stakeholder feedback, students draft their initial ethical and societal assessment report. This report should detail potential ethical and societal implications, stakeholder concerns, and AI-derived insights. Peer reviews are conducted, allowing for cross-feedback among student teams.</li>
<li><b><i>Report and Presentation</i>:</b> Students refine their report based on peer feedback and any additional insights. Each student/team presents their chatbot design and accompanying report to the class, detailing their findings and recommendations.</li>
<li><b><i>Reflection</i>:</b> Conclude with a session where students discuss the challenges and insights they encountered during the ethical and societal assessment process.</li></ul></li>
<li><b>Assessment Rubrics:</b>
<section class="tscroll">
<table id="box-table8">
<thead>
<tr>
<th scope="col" class="left">Criteria</th>
<th scope="col" class="left">Excellent</th>
<th scope="col" class="left">Proficient</th>
<th scope="col" class="left">Average</th>
<th scope="col" class="left">Poor</th></tr></thead>
<tbody>
<tr>
<td class="left"><b>Understanding of Ethical Implications</b></td>
<td class="left">Demonstrates profound understanding of complex ethical issues</td>
<td class="left">Shows clear understanding of major ethical concerns</td>
<td class="left">Demonstrates some understanding but misses nuances</td>
<td class="left">Lacks depth in understanding ethical implications</td></tr>
<tr>
<td class="left"><b>Stakeholder Engagement</b></td>
<td class="left">Actively engages a diverse range of stakeholders; deeply considers their feedback</td>
<td class="left">Engages several stakeholders; mostly integrates feedback</td>
<td class="left">Engages limited stakeholders; superficially considers feedback</td>
<td class="left">Neglects stakeholder engagement or dismisses feedback</td></tr>
<tr>
<td class="left"><b>Use of AI Insights</b></td>
<td class="left">Seamlessly integrates AI-derived insights into ethical considerations</td>
<td class="left">Effectively uses AI insights but occasionally misses integration opportunities</td>
<td class="left">Occasionally uses AI insights but lacks depth</td>
<td class="left">Rarely or improperly uses AI-derived insights</td></tr>
<tr>
<td class="left"><b>Quality of Ethical and Societal Assessment Report</b></td>
<td class="left">Comprehensive, insightful, and clearly structured with actionable recommendations</td>
<td class="left">Solid report with clear structure and mostly actionable recommendations</td>
<td class="left">Adequate report but lacks depth or clarity in some areas</td>
<td class="left">Disorganised or superficial report lacking actionable insights</td></tr>
<tr>
<td class="left"><b>Reflection on Ethics&#x2019; Dynamic Nature</b></td>
<td class="left">Deeply considers and articulates the changing landscape of ethics in AI</td>
<td class="left">Recognises and discusses some changes in ethical considerations</td>
<td class="left">Demonstrates some awareness but lacks depth in reflection</td>
<td class="left">Fails to recognise the dynamic nature of ethics</td></tr>
<tr>
<td class="left"><b>Presentation Skills</b></td>
<td class="left">Engaging and insightful presentation with clear articulation of findings</td>
<td class="left">Clear presentation with most findings effectively communicated</td>
<td class="left">Some disorganisation or lack of clarity during presentation</td>
<td class="left">Disjointed presentation with unclear findings</td></tr></tbody></table></section></li>
<li><b>Description of how AI is integrated and used in the Assessment:</b> This assessment requires students to design an AI chatbot for mental well-being, while concurrently conducting an Ethical and Societal Impact Assessment. Through the process, students integrate insights derived from AI tools &#x2013; used to analyse stakeholder feedback &#x2013; with their own ethical considerations. They actively engage with various stakeholders, ensuring a diverse range of perspectives inform their Ethical and Societal Impact Assessment. The ultimate goal is for students to understand the profound implications of AI tools in sensitive contexts necessarily for affectiveness of AI literacy, considering both immediate and future societal and ethical impacts. The assessment culminates in a comprehensive Ethical and Societal Impact Assessment report and presentation, showcasing students&#x2019; depth of understanding and their ability to anticipate potential societal shifts.</li></ul></aside></section>
<section aria-labelledby="sec4_6_9">
<h3 id="sec4_6_9"><span epub:type="pagebreak" id="p118" aria-label=" page 118. " role="doc-pagebreak"/><span epub:type="ordinal">4.6.9 </span>Lifelong Learning Portfolio Assessment</h3>
<p>Lifelong learning portfolio assessment is an approach that centres on continuous self-reflection and skill development over extended periods, often encompassing a student&#x2019;s entire academic and professional journey. By curating a digital portfolio, students collect, organise, and showcase their work, achievements, and milestones. This method provides both students and educators with a rich, longitudinal perspective on a learner&#x2019;s growth, areas of proficiency, and evolution of skills and competencies. Unlike traditional assessments that capture a snapshot of a student&#x2019;s abilities at a single point in time, the lifelong learning portfolio offers a dynamic, in-depth view of the learner&#x2019;s journey, emphasising the process of learning and self-improvement which is not easy to replicate with AI tools.<span epub:type="pagebreak" id="p119" aria-label=" page 119. " role="doc-pagebreak"/></p>
<aside epub:type="tip" role="doc-tip" class="box shaded">
<h3 id="box4_10"><span epub:type="label">Case Scenario</span> <span epub:type="ordinal">9</span> Lifelong Learning Portfolio in the Context of Design and Multimedia Studies</h3>
<ul class="simple">
<li><b>Scenario Overview:</b> Students enrolled in a design and multimedia programme are required to create and maintain a digital portfolio throughout their four-year course. This portfolio will document their projects, research, internships, collaborative works, self-initiated studies, and reflections on their learning processes. While AI tools can be employed to help students organise, enhance, and present their portfolios, the emphasis is on the students&#x2019; genuine contributions and the evolution of their skills and thinking over time.</li>
<li><b>Learning Outcomes:</b>
<ol type="1">
<li>Students will demonstrate continuous development and mastery in design and multimedia techniques.</li>
<li>Students will reflect critically on their learning experiences, challenges, and growth.</li>
<li>Students will effectively curate and present a comprehensive body of work that showcases their skills, versatility, and innovation.</li>
<li>Students will evaluate and incorporate feedback to improve their work and learning strategies.</li></ol></li>
<li>
<p><b>Procedures:</b></p>
<ul class="simple">
<li><b><i>Introduction</i>:</b> At the beginning of their academic journey, students attend an orientation session where the importance and objectives of the lifelong learning portfolio are discussed. Students receive training on various digital platforms suitable for portfolio creation and are introduced to AI tools that can assist in organising, enhancing, and presenting their work.</li>
<li><b><i>Periodic Portfolio Updates</i>:</b> Students are required to update their portfolios at the end of each academic module or after the completion of significant projects. Each update should be accompanied by a brief reflection on the learning and experiences related to the work added.</li>
<li><b><i>Scheduled Reviews</i>:</b> Twice during an academic year, faculties evaluate portfolios against the established rubrics. Feedback is provided regarding content depth, skill progression, reflection quality, and presentation. Once a year, students engage in a structured peer-review process. They assess a peer&#x2019;s portfolio and offer constructive feedback, aiding in mutual growth and shared learning insights.</li>
<li><b><i>Self-assessment Milestones</i>:</b> At the end of each academic year, students undergo a self-assessment process, reflecting on their achievements, challenges, and areas for further development. This reflection becomes a pivotal part of the portfolio, shedding light on the student&#x2019;s metacognitive processes and personal growth journey.</li>
<li><b><i>Integrative Learning Activities</i>:</b> Students are encouraged to participate in internships, workshops, or collaborative projects, the experiences and outputs of which should be documented in the portfolio. These integrative activities provide a real-world context to the skills and knowledge gained, offering an avenue for practical application and deeper understanding.</li>
<li><b><i>Final Presentation</i>:</b> In the concluding phase of their programme, students prepare a comprehensive presentation of their portfolios. They are expected to discuss key projects, highlight their growth trajectory, and reflect on their overall learning journey throughout the course. A panel, consisting of faculty members and industry professionals, assesses the presentation and provides feedback. This acts as the final validation of the student&#x2019;s competency, skillset, and readiness for professional life.</li>
<li><b><i>Continuous Improvement Feedback Loop</i>:</b> Feedback received from faculty reviews, peer assessments, and the final presentation should be constructively incorporated. Students are encouraged to periodically refine the content, structure, and presentation of their portfolios, ensuring that it remains a dynamic and true representation of their academic and personal growth.</li></ul></li>
<li><b>Assessment Rubrics:</b>
<section class="tscroll">
<table id="box-table9">
<thead>
<tr>
<th scope="col" class="left">Criteria</th>
<th scope="col" class="left">Excellent</th>
<th scope="col" class="left">Proficient</th>
<th scope="col" class="left">Average</th>
<th scope="col" class="left">Poor</th></tr></thead>
<tbody>
<tr>
<td class="left"><b>Content Comprehensiveness</b></td>
<td class="left">Portfolio displays a wide range of projects, demonstrating versatility and depth.</td>
<td class="left">Portfolio has varied content but may lack depth in some areas.</td>
<td class="left">Some key projects are missing or under-represented.</td>
<td class="left">Portfolio is sparse and lacks variety.</td></tr>
<tr>
<td class="left"><b>Skill Development</b></td>
<td class="left">Clear trajectory of skill enhancement and innovation over time.</td>
<td class="left">Steady skill development with minor plateaus.</td>
<td class="left">Inconsistent skill development and gaps in learning.</td>
<td class="left">Limited evidence of skill advancement.</td></tr>
<tr>
<td class="left"><b>Reflection &#x0026; Insight</b></td>
<td class="left">Deep, thoughtful reflections that display self-awareness and a hunger for growth.</td>
<td class="left">Good reflections with some insights into personal growth.</td>
<td class="left">Generic reflections with limited self-assessment.</td>
<td class="left">Sparse or superficial reflections.</td></tr>
<tr>
<td class="left"><b>Presentation &#x0026; Organisation</b></td>
<td class="left">Portfolio is immaculately organised, intuitive, and aesthetically pleasing.</td>
<td class="left">Well-organised with minor areas for improvement.</td>
<td class="left">Some organisational flaws; presentation can be enhanced.</td>
<td class="left">Disorganised and lacks a cohesive presentation.</td></tr></tbody></table></section></li>
<li><b>Description of how AI is integrated and used in the Assessment:</b> The Lifelong Learning portfolio in design and multimedia is a holistic approach to assessing a student&#x2019;s journey through the programme. It allows students to document their growth, showcase their best work, and reflect on their learning experiences. AI tools can assist in enhancing the portfolio&#x2019;s presentation, but the essence is the authentic representation of the student&#x2019;s progress and capabilities. Through consistent feedback loops with faculty and peers, students can continually refine their work and learning strategies, making the portfolio a living testament to their academic journey.</li></ul></aside></section></section>
<section aria-labelledby="sec4_7">
<h2 id="sec4_7"><span epub:type="pagebreak" id="p120" aria-label=" page 120. " role="doc-pagebreak"/><span epub:type="ordinal">4.7 </span>GenAI Text Detection</h2>
<p>The chapter of assessment would not be complete without discussing GenAI text detection, given that this GenAI text has emerged as one of the biggest concerns for both educators and students alike, particularly when it comes to text-based assignments (<a href="#ref4_15" id="R_ref4_15" epub:type="biblioref" role="doc-biblioref">Dalalah &#x0026; Dalalah, 2023</a>; <a href="#ref4_33" id="R_ref4_33" epub:type="biblioref" role="doc-biblioref">Perkins et al., 2023</a>; <a href="#ref4_35" id="R_ref4_35" epub:type="biblioref" role="doc-biblioref">Rudolph et al., 2023</a>). The ease with which students can employ GenAI to produce text assignments necessitates a robust mechanism to ensure the authenticity and originality of the submitted work. Therefore, exploring the GenAI text detection approaches and tools is imperative to uphold academic integrity and to foster a genuine learning environment. Through various detection methodologies such as text complexity analysis, perplexity, burstiness, and entropy analysis, among others, educators can discern between human-generated and AI-generated text, ensuring a fair and conducive academic milieu for both teaching and learning endeavours.</p>
<section aria-labelledby="sec4_7_1">
<h3 id="sec4_7_1"><span epub:type="ordinal">4.7.1 </span>GenAI Text Detection Approaches</h3>
<p>The detection of GenAI text encompasses a multitude of methods aimed at distinguishing human-produced text from text generated by AI, especially large language models (LLMs). Below are some of the methods with a brief description to explain how each method works:</p>
<ul class="simple">
<li><b>Text Complexity Analysis:</b> Analysing syntactic complexity, which involves examining the structural intricacies of text, can be pivotal in identifying AI-generated text. Generative models might exhibit distinct patterns in complexity compared to human-authored text (<a href="#ref4_16" id="R_ref4_16" epub:type="biblioref" role="doc-biblioref">Dascalu, 2014</a>).</li>
<li><b><span epub:type="pagebreak" id="p121" aria-label=" page 121. " role="doc-pagebreak"/>Style Analysis:</b> This method entails analysing stylistic elements like word choice, sentence structure, and tone to discern between human and AI-generated text.</li>
<li><b>Perplexity:</b> Perplexity measures how well a model predicts subsequent words based on preceding context. It is a statistical measure of how well a language model can predict the next word in a sequence of words. Lower perplexity indicates better prediction, which can be a hallmark of generative models. It is used for evaluating and comparing model performance (<a href="#ref4_32" id="R_ref4_32" epub:type="biblioref" role="doc-biblioref">Mukherjee, 2023</a>).</li>
<li><b>Burstiness:</b> Coupled with perplexity, burstiness is a measure of the frequency with which certain words or phrases appear in a text. In the context of detecting GenAI text, burstiness can be used to identify patterns of language use that are indicative of machine-generated text. GenAI tends to produce text that is less varied and more repetitive than human-generated text, which can lead to bursty patterns of language use (<a href="#ref4_32" epub:type="biblioref" role="doc-biblioref">Mukherjee, 2023</a>).</li>
<li><b>Entropy Analysis:</b> Entropic (Randomness) measures, reflecting the balance between predictability and unpredictability in language, can be employed to analyse written text. This involves analysing the entropy or randomness of the text, which can indicate whether it was generated by a machine or a human. A measure of the average amount of choice associated with words, can provide insights into vocabulary usage patterns differentiating human and AI-generated text. Such measures might unveil distinguishing characteristics between human and AI-generated text (<a href="#ref4_2" id="R_ref4_2" epub:type="biblioref" role="doc-biblioref">Bentz et al., 2017</a>; <a href="#ref4_20" id="R_ref4_20" epub:type="biblioref" role="doc-biblioref">Estevez-Rams, E et al., 2019</a>).</li>
<li><b>Domain-Specific Knowledge:</b> Leveraging domain-specific knowledge, like constructing domain-specific lexicons, can help in distinguishing between human and machine-generated text, especially in sentiment classification tasks. This involves using knowledge about a specific domain, such as scientific or legal writing, to detect whether the text was generated by AI.</li>
<li><b>Sentence Length Distribution:</b> GenAI text tends to have more evenly distributed sentence lengths than human-written text, which tends to have a wider range of sentence lengths. It generally involves analysing the distribution of sentence lengths to discern patterns unique to AI-generated text.</li>
<li><b>Repetition:</b> GenAI text may repeat certain phrases or ideas more frequently than human-written text, as the AI may be more likely to generate similar patterns. It involves analysing the frequency and patterns of word or phrase repetition, which might vary between human and AI-generated text (<a href="#ref4_24" id="R_ref4_24" epub:type="biblioref" role="doc-biblioref">Jakesch et al., 2023</a>).</li>
<li><b>Coherence:</b> GenAI text may lack coherence, as it may generate text that does not have a clear or logical structure or flow. It typically involves evaluating the logical consistency and structure of text to discern between human and AI-generated text.</li></ul>
<p>These methods can be deployed singularly or in combination to enhance the accuracy and robustness of AI-generated text detection mechanisms. The different metrics used to detect generative AI text are often used in combination with various approaches. For example, text complexity analysis and perplexity scores are often used in combination with metadata analysis, which involves examining information such as authorship, publication date, and source to determine the likelihood of the text being generated by AI. Similarly, style analysis can be used in combination with plagiarism detection to identify similarities in writing style between a suspected generative AI text and known sources.</p>
<p>Overall, a combination of different approaches and metrics is typically used to detect generative AI text, as no single method is foolproof. By examining multiple aspects of a text, researchers and software can increase their accuracy in identifying whether a piece of <span epub:type="pagebreak" id="p122" aria-label=" page 122. " role="doc-pagebreak"/>writing was generated by AI or written by a human. In the next section, we consolidate the results from various research studies pertaining to the effectiveness of existing AI detection software tools.</p></section>
<section aria-labelledby="sec4_7_2">
<h3 id="sec4_7_2"><span epub:type="ordinal">4.7.2 </span>GenAI Text Detection Tools</h3>
<p>The narrative surrounding GenAI detection software is nuanced, with both potential benefits and significant shortcomings. Drawing on a headline from USA Today, &#x201C;Student Was Falsely Accused of Cheating With AI &#x2013; And She Won&#x2019;t Be the Last&#x201D;, it is evident that while these tools are developed to uphold academic integrity, they may also spawn unwarranted accusations of dishonesty. The case at UC Davis, as reported by USA Today, showcases a situation where a student was wrongfully accused of cheating by a GenAI detection tool, leading to a stressful and unjust scenario (<a href="#ref4_25" id="R_ref4_25" epub:type="biblioref" role="doc-biblioref">Jimenez, 2023</a>).</p>
<p>The episode at UC Davis isn&#x2019;t isolated; rather, it is indicative of a broader challenge faced by educational institutions worldwide. The concern about students utilising GenAI like ChatGPT to quickly generate passable essays led to the emergence of startups creating products aimed at distinguishing human-authored text from machine-generated text. However, these tools are not infallible (<a href="#ref4_46" id="R_ref4_46" epub:type="biblioref" role="doc-biblioref">Williams, 2023</a>). A study, spearheaded by <a href="#ref4_43" id="R_ref4_43" epub:type="biblioref" role="doc-biblioref">Weber-Wulff et al. (2023)</a>, evaluated 14 GenAI detection tools including well-known ones like Turnitin, GPT Zero, and Compilatio. These tools generally operate by identifying characteristics typical of AI-generated text, such as repetition, and then calculating the likelihood of the text being AI-generated. The study revealed that these tools struggled to identify ChatGPT-generated text that had been slightly modified by humans or obfuscated with a paraphrasing tool. For instance, the detection tools could identify human-written text with an average accuracy of 96%, but their performance dropped significantly when it came to detecting AI-generated text&#x2013; 74% accuracy for unaltered ChatGPT text, which further plummeted to 42% when the text was slightly edited. These findings underscore the ease with which students could potentially bypass these detection tools simply by making minor modifications to the AI-generated text.</p>
<p>Another study examines the effectiveness of university assessments in detecting Open AI&#x2019;s GPT-4 generated content, aided by the Turnitin AI detection tool (<a href="#ref4_33" epub:type="biblioref" role="doc-biblioref">Perkins et al., 2023</a>). Despite the tool identifying 91% of the experimental submissions as containing AI-generated content, only 54.8% of the content was actually detected, hinting at the use of adversarial techniques like prompt engineering as effective evasion methods. The results suggest a need for enhanced AI detection software and increased awareness and training for faculty in utilising these tools, especially as the scoring between genuine and AI-generated submissions was found to be comparably close. Similar studies (<a href="#ref4_26" id="R_ref4_26" epub:type="biblioref" role="doc-biblioref">Khalil &#x0026; Er, 2023</a>; <a href="#ref4_42" id="R_ref4_42" epub:type="biblioref" role="doc-biblioref">Walters, 2023</a>) also produced similar outcomes. <a href="#ref4_28" id="R_ref4_28" epub:type="biblioref" role="doc-biblioref">Lancaster (2023)</a> introduces a digital watermarking technique as a possible solution to identify AI-generated text, though a small-scale study suggests it is promising but not a foolproof solution against misuse.</p>
<p>One of the major shortcomings of these tools, as highlighted by the researchers, is their inability to fulfil their advertised promises of accurately detecting AI-generated text. Despite the apparent deficiencies, companies continue to roll out products claiming to address AI-text detection, albeit with varying levels of success. For example, Turnitin achieved some level of detection accuracy with a relatively low false-positive rate. However, the overarching sentiment among some experts is that the whole notion of trying to spot AI-written text is flawed, suggesting that the focus should rather be on addressing the underlying issue of AI usage in academia (<a href="#ref4_46" epub:type="biblioref" role="doc-biblioref">Williams, 2023</a>).</p>
<p><span epub:type="pagebreak" id="p123" aria-label=" page 123. " role="doc-pagebreak"/>This evolving narrative underscores the imperative for a balanced approach in deploying GenAI detection tools within educational settings. It is crucial to consider the potential for false positives and the undue stress and unfair treatment that may arise from reliance on these tools. While there are tools available, their effectiveness in accurately detecting AI-generated text, especially when slightly modified, is still a substantial challenge. Moreover, as the creators of these detection tools continue to refine their algorithms in response to the evolving capabilities of generative AI, the dialogue surrounding the ethics, effectiveness, and implementation of such software in academic environments remains a pertinent and ongoing discussion.</p>
<p>As we move forward, it is crucial to have a multi-faceted approach, including policy development, student and staff training, discipline-specific interventions, revised assessment design, improved detection techniques, and a student partnership approach to uphold academic integrity in the face of advancing AI technologies.</p></section></section>
<section epub:type="conclusion" role="doc-conclusion">
<h2 id="sec4_8"><span epub:type="ordinal">4.8 </span>Conclusions</h2>
<p>As we stand on the cusp of an educational revolution, brought forth by the rapid advancements in GenAI, it becomes imperative for educators, learners, and institutions to introspect, innovate, and integrate. The landscape of higher education, with its intricate web of learning outcomes, pedagogies, and assessment tools, is poised for transformative change. But with this change comes responsibility. The potential of GenAI is vast, promising real-time, adaptive evaluations and feedback mechanisms that could redefine the very essence of assessment. However, this potential must be harnessed judiciously, ensuring that the core tenets of education &#x2013; authenticity, integrity, equity, and genuine learning &#x2013; remain at the forefront.</p>
<p>Throughout this chapter, we explored the evolution of assessment, reflecting on its challenges and innovations in both traditional and GenAI contexts. The intricacies of assessment in the traditional era, marked by its multi-faceted purposes and often misaligned perceptions, set the stage for the profound transformations GenAI can bring. However, the introduction of GenAI also raises a plethora of ethical, logistical, and pedagogical questions. The SARPS and the AI Assessment Integration Framework, with its nine distinct assessment types, serve as beacons, guiding educators through the uncharted waters of AI-integrated assessment.</p>
<p>But as we reimagine and redesign assessment for the GenAI era, it is crucial to remember that technology, no matter how advanced, should serve as a tool, not a replacement. The human essence of education, marked by empathy, mentorship, and a deep understanding of individual learning journeys, remains irreplaceable. GenAI offers a platform to augment this human touch, providing educators with insights, automations, and personalisation that can enhance the learning experience. Yet, the onus remains on educators to ensure that GenAI&#x2019;s integration is meaningful, ethical, and centred around the learner.</p>
<p>In conclusion, the GenAI era beckons a renewed focus on the holistic journey of learning. Redesigning assessment is not just about leveraging advanced technologies but also re-envisioning the entire educational process. It is about prioritising feedback over grades, process over products, and authentic learning experiences over rote memorisation. As we navigate this transformative era, it is our collective responsibility as educators, learners, and stakeholders to ensure that the future of assessment remains grounded in the principles that have always defined quality education. With thoughtful integration, critical evaluation, and a learner-centric approach, the future of assessment in the GenAI era looks promising, replete with opportunities for profound, meaningful learning.</p>
<aside epub:type="tip" role="doc-tip" class="box ruled">
<h3 id="box4_11">Questions to Ponder</h3>
<ul class="disc">
<li>With the rise of GenAI tools that can generate detailed responses, how can educators foster environments that emphasise the process of learning over the mere end product?</li>
<li>How might the traditional grading system need to evolve to accommodate a more feedback-driven approach, especially in the context of GenAI?</li>
<li>As AI tools become increasingly accessible and advanced, what proactive measures can be implemented to uphold academic integrity and deter students from &#x201C;gaming the system&#x201D;?</li>
<li>How can educators shift the perception of GenAI from being just a tool or potential threat to a collaborative partner in the learning process?</li></ul></aside>
<aside epub:type="tip" role="doc-tip" class="box shaded">
<h3 id="box4_12">Personal Reflection</h3>
<p>In an age where GenAI can craft sonnets and solve equations faster than the blink of an eye, let&#x2019;s take a moment to playfully ponder: If GenAI were a new student in our class, how would we assess its learning? Would it thrive in a performance-based assessment or struggle with ethical and societal impact reflections? As we navigate the new frontiers of assessment, it is crucial to remember that while GenAI might ace the technical challenges, it is the uniquely human journey of learning, with all its quirks, emotions, and reflections, that truly enriches the educational tapestry. So, as we redesign our assessments in this new world, let&#x2019;s ensure these assessments are challenging and meaningful and ready for the new student &#x2013; GenAI.</p></aside></section>
<section class="reference" epub:type="bibliography" role="doc-bibliography">
<h2 id="r4_1"><span epub:type="pagebreak" id="p124" aria-label=" page 124. " role="doc-pagebreak"/>References</h2>
<ul>
<li id="ref4_1" epub:type="biblioentry"><a href="#R_ref4_1" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Awdry, R., Dawson, P., &#x0026; Sutherland-Smith, W.</a> (2022). Contract cheating: To legislate or not to legislate &#x2013; Is that the question? <cite>Assessment &#x0026; Evaluation in Higher Education</cite>, <i>47</i>(5), 712&#x2013;726. <a href="https://doi.org/10.1080/02602938.2021.1957773" aria-label="D.O.I. link to Assessment &#x0026; Evaluation in Higher Education">https://doi.org/10.1080/02602938.2021.1957773</a></li>
<li id="ref4_2" epub:type="biblioentry"><a href="#R_ref4_2" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Bentz, C., Alikaniotis, D., Cysouw, M., &#x0026; Ferrer-i-Cancho, R.</a> (2017). The entropy of words &#x2013; Learnability and expressivity across more than 1000 languages. <cite>Entropy</cite>, <i>19</i>(6), 275. <a href="https://doi.org/10.3390/e19060275" aria-label="D.O.I. link to Entropy">https://doi.org/10.3390/e19060275</a></li>
<li id="ref4_3" epub:type="biblioentry"><a href="#R_ref4_3" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Biggs, J.B., &#x0026; Collis, K.F.</a> (1982). <cite>Evaluating the quality of learning: The SOLO taxonomy</cite>. Academic Press.</li>
<li id="ref4_4" epub:type="biblioentry"><a href="#R_ref4_4" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Black, P., &#x0026; Wiliam, D.</a> (1998). Inside the black box: Raising standards through classroom assessment. <cite>The Phi Delta Kappan</cite>, <i>80</i>(2), 139&#x2013;148.</li>
<li id="ref4_5" epub:type="biblioentry"><a href="#R_ref4_5" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Bloom, B.</a> (1956). <cite>Taxonomy of educational objectives</cite>. Longmans, Green and Co.</li>
<li id="ref4_6" epub:type="biblioentry"><a href="#R_ref4_6" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Boud, D., &#x0026; Falchikov, N.</a> (2006). Aligning assessment with long-term learning. <cite>Assessment &#x0026; Evaluation in Higher Education</cite>, <i>31</i>(4), 399&#x2013;413. <a href="https://doi.org/10.1080/02602930600679050" aria-label="D.O.I. link to Assessment &#x0026; Evaluation in Higher Education">https://doi.org/10.1080/02602930600679050</a></li>
<li id="ref4_7" epub:type="biblioentry"><a href="#R_ref4_7" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Carless, D.</a> (2015). <cite>Excellence in university assessment: Learning from award-winning practice</cite>. Routledge.</li>
<li id="ref4_8" epub:type="biblioentry"><a href="#R_ref4_8" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Carless, D., &#x0026; Boud, D.</a> (2018). The development of student feedback literacy: Enabling uptake of feedback. <cite>Assessment &#x0026; Evaluation in Higher Education</cite>, <i>43</i>(8), 1315&#x2013;1325. <a href="https://doi.org/10.1080/02602938.2018.1463354" aria-label="D.O.I. link to Assessment &#x0026; Evaluation in Higher Education">https://doi.org/10.1080/02602938.2018.1463354</a></li>
<li id="ref4_9" epub:type="biblioentry"><a href="#R_ref4_9" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Chan, C.K.Y.</a> (2023). <cite>Assessment for experiential learning</cite>. Taylor &#x0026; Francis.</li>
<li id="ref4_10" epub:type="biblioentry"><span epub:type="pagebreak" id="p125" aria-label=" page 125. " role="doc-pagebreak"/><a href="#R_ref4_10" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Chan, C.K.Y., &#x0026; Lee, K.K.W.</a> (2021). Reflection literacy: A multilevel perspective on the challenges of using reflections in higher education through a comprehensive literature review. <cite>Educational Research Review</cite>, <i>32</i>. <a href="https://doi.org/10.1016/j.edurev.2020.100376" aria-label="D.O.I. link to Educational Research Review">https://doi.org/10.1016/j.edurev.2020.100376</a></li>
<li id="ref4_11" epub:type="biblioentry"><a href="#R_ref4_11" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Chan, C. K. Y., &#x0026; Tsi, L.H.Y.</a> (2023). The AI revolution in education: Will AI replace or assist teachers in higher education? <i>arXiv preprint</i>. <a href="https://doi.org/10.48550/arXiv.2305.01185" aria-label="D.O.I. link to arXiv preprint">https://doi.org/10.48550/arXiv.2305.01185</a></li>
<li id="ref4_12" epub:type="biblioentry"><a href="#R_ref4_12" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Chan, C.K.Y., &#x0026; Luk, Y.Y.L.</a> (2022). Eight years after the 3&#x2013;3&#x2013;4 curriculum reform: The current state of undergraduates&#x2019; holistic competency development in Hong Kong. <cite>Studies in Educational Evaluation</cite>, <i>74</i>. <a href="https://doi.org/10.1016/j.stueduc.2022.101168" aria-label="D.O.I. link to Studies in Educational Evaluation">https://doi.org/10.1016/j.stueduc.2022.101168</a></li>
<li id="ref4_13" epub:type="biblioentry"><a href="#R_ref4_13" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Chan, C.K.Y. &#x0026; Wong, Y.H.H.</a> (2021). Students&#x2019; perception of written, audio, video and face-to-face reflective approaches for holistic competency development. <cite>Active Learning in Higher Education</cite>. <a href="https://doi.org/10.1177/14697874211054449" aria-label="D.O.I. link to Active Learning in Higher Education">https://doi.org/10.1177/14697874211054449</a></li>
<li id="ref4_14" epub:type="biblioentry"><a href="#R_ref4_14" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Chan, C.K.Y., Fong, E.T.Y., Luk, L.Y.Y., &#x0026; Ho, R.</a> (2017). A review of literature on challenges in the development and implementation of generic competencies in higher education curriculum. <cite>International Journal of Educational Development</cite>, <i>57</i>, 1&#x2013;10. <a href="https://doi.org/10.1016/j.ijedudev.2017.08.010" aria-label="D.O.I. link to International Journal of Educational Development">https://doi.org/10.1016/j.ijedudev.2017.08.010</a></li>
<li id="ref4_15" epub:type="biblioentry"><a href="#R_ref4_15" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Dalalah, D., &#x0026; Dalalah, O. M.</a> (2023). The false positives and false negatives of generative AI detection tools in education and academic research: The case of ChatGPT. <cite>The International Journal of Management Education</cite>, <i>21</i>(2), 100822.</li>
<li id="ref4_16" epub:type="biblioentry"><a href="#R_ref4_16" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Dascalu, M.</a> (2014). Analyzing discourse and text complexity for learning and collaborating. In <cite>Analyzing discourse and text complexity for learning and collaborating</cite> (pp. 1&#x2013;3). Springer. <a href="https://doi.org/10.1007/978-3-319-03419-5" aria-label="D.O.I. link to Analyzing discourse and text complexity for learning and collaborating">https://doi.org/10.1007/978-3-319-03419-5</a></li>
<li id="ref4_17" epub:type="biblioentry"><a href="#R_ref4_17" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Dillion, D., Tandon, N., Gu, Y., &#x0026; Gray, K.</a> (2023). Can AI language models replace human participants? <cite>Trends in Cognitive Sciences</cite>, <i>27</i>(7), 597&#x2013;600. <a href="https://doi.org/10.1016/j.tics.2023.04.008" aria-label="D.O.I. link to Trends in Cognitive Sciences">https://doi.org/10.1016/j.tics.2023.04.008</a></li>
<li id="ref4_18" epub:type="biblioentry"><a href="#R_ref4_18" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Dixson, D.D., &#x0026; Worrell, F.C.</a> (2016). Formative and summative assessment in the classroom. <cite>Theory into Practice</cite>, <i>55</i>(2), 153&#x2013;159. <a href="https://doi.org/10.1080/00405841.2016.1148989" aria-label="D.O.I. link to Theory into Practice">https://doi.org/10.1080/00405841.2016.1148989</a></li>
<li id="ref4_19" epub:type="biblioentry"><a href="#R_ref4_19" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Earl, L.M.</a> (2003). <cite>Assessment as learning: Using classroom assessment to maximize student learning</cite>. Corwin Press.</li>
<li id="ref4_20" epub:type="biblioentry"><a href="#R_ref4_20" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Estevez-Rams, E., Mesa-Rodriguez, A., &#x0026; Estevez-Moya, D.</a> (2019). Complexity-entropy analysis at different levels of organisation in written language. <cite>PLoS ONE</cite>, <i>14</i>(5), e0214863. <a href="https://doi.org/10.1371/journal.pone.0214863" aria-label="D.O.I. link to PLoS ONE">https://doi.org/10.1371/journal.pone.0214863</a></li>
<li id="ref4_21" epub:type="biblioentry"><a href="#R_ref4_21" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Fendrich, L.</a> (2007). A pedagogical straitjacket. <cite>The Chronicle of Higher Education</cite>, <i>53</i>(40), 1&#x2013;5.</li>
<li id="ref4_22" epub:type="biblioentry"><a href="#R_ref4_22" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Fischer, J., Bearman, M., Boud, D., &#x0026; Tai, J.</a> (2023). How does assessment drive learning? A focus on students&#x2019; development of evaluative judgement. <cite>Assessment &#x0026; Evaluation in Higher Education</cite>. <a href="https://doi.org/10.1080/02602938.2023.2206986" aria-label="D.O.I. link to Assessment &#x0026; Evaluation in Higher Education">https://doi.org/10.1080/02602938.2023.2206986</a></li>
<li id="ref4_23" epub:type="biblioentry"><a href="#R_ref4_23" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Gilley, B.H., &#x0026; Clarkston, B.</a> (2014). Collaborative testing: Evidence of learning in a controlled in-class study of undergraduate students. <cite>Journal of College Science Teaching</cite>, <i>43</i>(3), 83&#x2013;91. <a href="https://doi.org/10.2505/4/jcst14_043_03_83" aria-label="D.O.I. link to Journal of College Science Teaching">https://doi.org/10.2505/4/jcst14_043_03_83</a></li>
<li id="ref4_24" epub:type="biblioentry"><a href="#R_ref4_24" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Jakesch, M., Hancock, J. T., &#x0026; Naaman, M.</a> (2023). Human heuristics for AI-generated language are flawed. <cite>Proceedings of the National Academy of Sciences</cite>, <i>120</i>(11), e2208839120.</li>
<li id="ref4_25" epub:type="biblioentry"><a href="#R_ref4_25" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Jimenez, K.</a> (2023, April 12). Professors are using ChatGPT detector tools to accuse students of cheating. But what if the software is wrong? <cite>USA Today</cite>. <a href="https://eu.usatoday.com">https://eu.usatoday.com/story/news/education/2023/04/12/how-ai-detection-tool-spawned-false-cheating-case-uc-davis/11600777002/</a></li>
<li id="ref4_26" epub:type="biblioentry"><a href="#R_ref4_26" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Khalil, M., &#x0026; Er, E.</a> (2023). Will ChatGPT get you caught? Rethinking of plagiarism detection. arXiv:2302.04335v1. <a href="https://doi.org/10.48550/arXiv.2302.04335" aria-label="D.O.I. link to Will ChatGPT get you caught? Rethinking of plagiarism detection">https://doi.org/10.48550/arXiv.2302.04335</a></li>
<li id="ref4_27" epub:type="biblioentry"><a href="#R_ref4_27" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Kuh, G.D.</a> (2008). <cite>High-impact educational practices: What they are, who has access to them, and why they matter</cite>. AAC&#x0026;U.</li>
<li id="ref4_28" epub:type="biblioentry"><a href="#R_ref4_28" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Lancaster, T.</a> (2023). Artificial intelligence, text generation tools and ChatGPT &#x2013; Does digital watermarking offer a solution? <cite>International Journal for Educational Integrity</cite>, <i>19</i>(10), 1&#x2013;16. Retrieved from <a href="https://link.springer.com">https://link.springer.com/article/10.1007/s40979-023-00131-6</a></li>
<li id="ref4_29" epub:type="biblioentry"><a href="#R_ref4_29" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Lichtenthaler, U.</a> (2018). Substitute or synthesis: The interplay between human and artificial intelligence. <cite>Research-Technology Management</cite>, <i>61</i>(5), 12&#x2013;14. <a href="https://doi.org/10.1080/08956308.2018.1495962" aria-label="D.O.I. link to Research-Technology Management">https://doi.org/10.1080/08956308.2018.1495962</a></li>
<li id="ref4_30" epub:type="biblioentry"><span epub:type="pagebreak" id="p126" aria-label=" page 126. " role="doc-pagebreak"/><a href="#R_ref4_30" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Luk, Y.Y.L., &#x0026; Chan, C.K.Y.</a> (2021). Students&#x2019; learning outcomes from engineering internship: A provisional framework. <cite>Studies in Continuing Education</cite>, <i>44</i>(3), 526&#x2013;545. <a href="https://doi.org/10.1080/0158037X.2021.1917536" aria-label="D.O.I. link to Studies in Continuing Education">https://doi.org/10.1080/0158037X.2021.1917536</a></li>
<li id="ref4_31" epub:type="biblioentry"><a href="#R_ref4_31" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Montgomery, J. L., &#x0026; Baker, W.</a> (2007). Teacher-written feedback: Student perceptions, teacher self-assessment, and actual teacher performance. <cite>Journal of Second Language Writing</cite>, <i>16</i>(2), 82&#x2013;99. <a href="https://doi.org/10.1016/j.jslw.2007.04.002" aria-label="D.O.I. link to Journal of Second Language Writing">https://doi.org/10.1016/j.jslw.2007.04.002</a></li>
<li id="ref4_32" epub:type="biblioentry"><a href="#R_ref4_32" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Mukherjee, S.</a> (2023, June 15). Unveiling perplexity: Measuring success of LLMs and generative AI models. Retrieved from <a href="https://ramblersm.medium.com">https://ramblersm.medium.com/the-significance-of-perplexity-in-evaluating-llms-and-generative-ai-62e290e791bc</a></li>
<li id="ref4_33" epub:type="biblioentry"><a href="#R_ref4_33" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Perkins, M., Roe, J., Postma, D., McGaughran, J., &#x0026; Hickerson, D.</a> (2023). Game of tones: Faculty detection of GPT-4 generated content in university assessments. arXiv preprint. arXiv:2305.18081</li>
<li id="ref4_34" epub:type="biblioentry"><a href="#R_ref4_34" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Ramsden, P.</a> (2003). <cite>Learning to teach in higher education</cite>. Routledge.</li>
<li id="ref4_35" epub:type="biblioentry"><a href="#R_ref4_35" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Rudolph, J., Tan, S., &#x0026; Tan, S.</a> (2023). ChatGPT: Bullshit spewer or the end of traditional assessments in higher education? <cite>Journal of Applied Learning and Teaching</cite>, <i>6</i>(1).</li>
<li id="ref4_36" epub:type="biblioentry"><a href="#R_ref4_36" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Rust, C.</a> (2002). The impact of assessment on student learning: How can the research literature practically help to inform the development of departmental assessment strategies and learner-centred assessment practices? <cite>Active Learning in Higher Education</cite>, <i>3</i>(2), 145&#x2013;158. <a href="https://doi.org/10.1177/1469787402003002004" aria-label="D.O.I. link to Active Learning in Higher Education">https://doi.org/10.1177/1469787402003002004</a></li>
<li id="ref4_37" epub:type="biblioentry"><a href="#R_ref4_37" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Scott, I.M.</a> (2020). Beyond &#x2018;driving&#x2019;: The relationship between assessment, performance and learning. <cite>Medical Education</cite>, <i>54</i>(1), 54&#x2013;59. <a href="https://doi.org/10.1111/medu.13935" aria-label="D.O.I. link to Medical Education">https://doi.org/10.1111/medu.13935</a></li>
<li id="ref4_38" epub:type="biblioentry"><a href="#R_ref4_38" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Sonnleitner, P., &#x0026; Kovacs, C.</a> (2020). Differences between students&#x2019; and teachers&#x2019; fairness perceptions: Exploring the potential of a self-administered questionnaire to improve teachers&#x2019; assessment practices. <cite>Frontiers in Education</cite>, <i>5</i>. <a href="https://doi.org/10.3389/feduc.2020.00017" aria-label="D.O.I. link to Frontiers in Education">https://doi.org/10.3389/feduc.2020.00017</a></li>
<li id="ref4_39" epub:type="biblioentry"><a href="#R_ref4_39" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Stobart, G.</a> (2008). <cite>Testing times: The uses and abuses of assessment</cite>. Routledge.</li>
<li id="ref4_40" epub:type="biblioentry"><a href="#R_ref4_40" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Tai, J., Ajjawi, R., Boud, D., Dawson, P., &#x0026; Panadero, E.</a> (2018). Developing evaluative judgement: Enabling students to make decisions about the quality of work. <cite>Higher Education</cite>, <i>76</i>(3), 467&#x2013;481. <a href="https://doi.org/10.1007/s10734-017-0220-3" aria-label="D.O.I. link to Higher Education">https://doi.org/10.1007/s10734-017-0220-3</a></li>
<li id="ref4_41" epub:type="biblioentry"><a href="#R_ref4_41" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Van de Watering, G., &#x0026; Van der Rijt, J.</a> (2006). Teachers&#x2019; and students&#x2019; perceptions of assessments: A review and a study into the ability and accuracy of estimating the difficulty levels of assessment items. <cite>Educational Research Review</cite>, <i>1</i>(2), 133&#x2013;147. <a href="https://doi.org/10.1016/j.edurev.2006.05.001" aria-label="D.O.I. link to Educational Research Review">https://doi.org/10.1016/j.edurev.2006.05.001</a></li>
<li id="ref4_42" epub:type="biblioentry"><a href="#R_ref4_42" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Walters, W. H.</a> (2023). The effectiveness of software designed to detect AI-generated writing: A comparison of 16 AI text detectors. <cite>Open Information Science</cite>, <i>7</i>(1), 20220158. <a href="https://doi.org/10.1515/opis-2022-0158" aria-label="D.O.I. link to Open Information Science">https://doi.org/10.1515/opis-2022-0158</a></li>
<li id="ref4_43" epub:type="biblioentry"><a href="#R_ref4_43" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Weber-Wulff, D., Anohina-Naumeca, A., Bjelobaba, S., Folt&#x00FD;nek, T., Guerrero-Dib, J., Popoola, O., &#x0160;igut, P., &#x0026; Waddington, L.</a> (2023). Testing of detection tools for AI-generated text. Preprint. <a href="https://doi.org/10.48550/arXiv.2306.15666" aria-label="D.O.I. link to Testing of detection tools for AI-generated text">https://doi.org/10.48550/arXiv.2306.15666</a></li>
<li id="ref4_44" epub:type="biblioentry"><a href="#R_ref4_44" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Wiggins, G.</a> (1998). <cite>Educative assessment. Designing assessments to inform and improve student performance</cite>. Jossey-Bass Publishers</li>
<li id="ref4_45" epub:type="biblioentry"><a href="#R_ref4_45" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Wiliam, D.</a> (2011). <cite>Embedded formative assessment</cite>. Solution Tree Press.</li>
<li id="ref4_46" epub:type="biblioentry"><a href="#R_ref4_46" epub:type="backlink" role="doc-backlink" aria-label=" This link returns to its first in-text citation. ">Williams, R.</a> (2023, July 7). AI-text detection tools are really easy to fool. <cite>MIT Technology Review</cite>. Retrieved from <a href="https://www.technologyreview.com">https://www.technologyreview.com/2023/07/07/1075982/ai-text-detection-tools-are-really-easy-to-fool/</a></li></ul></section></section></body></html>