<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>1.6 Building and Evaluating LLM Applications</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>1.6 Building and Evaluating LLM Applications</h1>
<blockquote>原文：<a href="https://boramorka.github.io/LLM-Book/en/CHAPTER-1/1.6%20Building%20and%20Evaluating%20LLM%20Applications/">https://boramorka.github.io/LLM-Book/en/CHAPTER-1/1.6%20Building%20and%20Evaluating%20LLM%20Applications/</a></blockquote>
                
                  


  
  



<p>Building applications powered by large language models (LLMs) requires more than clean integration — it needs a systematic quality evaluation that covers both objective and subjective aspects. In practice, you combine accuracy, recall, and F1 (when gold answers are available) with user ratings and satisfaction metrics (CSA), while also tracking operational indicators like cost and latency. This blend exposes weak spots, informs release decisions, and guides targeted improvements.</p>
<p>The typical path to production starts with simple prompts and a small dataset for quick iteration; then you broaden coverage, complicate scenarios, refine metrics and quality criteria — remembering that perfection isn’t always necessary. It’s often enough to consistently solve the target tasks within quality and budget constraints. In high‑stakes scenarios (medicine, law enforcement, finance), stricter validation becomes essential: random sampling and hold‑out tests, bias and error checks, and attention to ethical and legal issues — preventing harm, ensuring explainability, and enabling audit.</p>
<p>Good engineering style emphasizes modularity and fast iteration, automated regression tests and measurements, thoughtful metric selection aligned with business goals, and mandatory bias/fairness analysis with regular reviews.</p>
<p>To make evaluation reproducible, use rubrics and evaluation protocols: define criteria in advance — relevance to user intent and context, factual correctness, completeness, and coherence/fluency — as well as the process, scales, and thresholds. For subjective tasks, use multiple independent raters and automatic consistency checks. Where possible, compare answers to ideal (expert) responses — a “gold standard” provides an anchor for more objective judgments. Here’s a small environment scaffold and call function for reproducible experiments and evaluations:</p>
<div class="highlight"><pre><span/><code><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">dotenv</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dotenv</span>

<span class="n">load_dotenv</span><span class="p">()</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">()</span>

<span class="k">def</span><span class="w"> </span><span class="nf">fetch_llm_response</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">"gpt-4o-mini"</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">):</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="n">prompts</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">max_tokens</span><span class="o">=</span><span class="n">max_tokens</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span>
</code></pre></div>
<p>Next, formalize rubric‑based evaluation and assign weights to compute an overall score with detailed feedback. Below is a template where the model produces an assessment according to given criteria; the parsing is a stub and should be replaced with logic suited to your model’s output format:</p>
<div class="highlight"><pre><span/><code><span class="k">def</span><span class="w"> </span><span class="nf">evaluate_response_against_detailed_rubric</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span> <span class="n">llm_response</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Evaluate the answer on accuracy, relevance, completeness, and coherence.</span>
<span class="sd">    Return an overall score and detailed feedback.</span>
<span class="sd">    """</span>
    <span class="n">rubric_criteria</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">'accuracy'</span><span class="p">:</span> <span class="p">{</span><span class="s1">'weight'</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s1">'score'</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">'feedback'</span><span class="p">:</span> <span class="s1">''</span><span class="p">},</span>
        <span class="s1">'relevance'</span><span class="p">:</span> <span class="p">{</span><span class="s1">'weight'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">'score'</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">'feedback'</span><span class="p">:</span> <span class="s1">''</span><span class="p">},</span>
        <span class="s1">'completeness'</span><span class="p">:</span> <span class="p">{</span><span class="s1">'weight'</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s1">'score'</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">'feedback'</span><span class="p">:</span> <span class="s1">''</span><span class="p">},</span>
        <span class="s1">'coherence'</span><span class="p">:</span> <span class="p">{</span><span class="s1">'weight'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">'score'</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">'feedback'</span><span class="p">:</span> <span class="s1">''</span><span class="p">}</span>
    <span class="p">}</span>
    <span class="n">total_weight</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">c</span><span class="p">[</span><span class="s1">'weight'</span><span class="p">]</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">rubric_criteria</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>

    <span class="n">system_prompt</span> <span class="o">=</span> <span class="s2">"Assess the support agent’s answer given the provided context."</span>
    <span class="n">evaluation_prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"""</span><span class="se">\</span>
<span class="s2">    [Question]: </span><span class="si">{</span><span class="n">test_data</span><span class="p">[</span><span class="s1">'customer_query'</span><span class="p">]</span><span class="si">}</span>
<span class="s2">    [Context]: </span><span class="si">{</span><span class="n">test_data</span><span class="p">[</span><span class="s1">'context'</span><span class="p">]</span><span class="si">}</span>
<span class="s2">    [Expected answers]: </span><span class="si">{</span><span class="n">test_data</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">'expected_answers'</span><span class="p">,</span><span class="w"> </span><span class="s1">'N/A'</span><span class="p">)</span><span class="si">}</span>
<span class="s2">    [LLM answer]: </span><span class="si">{</span><span class="n">llm_response</span><span class="si">}</span>

<span class="s2">    Evaluate the answer on accuracy, relevance, completeness, and coherence.</span>
<span class="s2">    Provide scores (0–10) for each criterion and specific feedback.</span>
<span class="s2">    """</span>

    <span class="n">evaluation_results</span> <span class="o">=</span> <span class="n">fetch_llm_response</span><span class="p">([</span>
        <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"system"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="n">system_prompt</span><span class="p">},</span>
        <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="n">evaluation_prompt</span><span class="p">},</span>
    <span class="p">])</span>

    <span class="c1"># Parsing stub — replace with real parsing of your model’s output</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">rubric_criteria</span><span class="p">:</span>
        <span class="n">rubric_criteria</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="s1">'score'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">8</span>
        <span class="n">rubric_criteria</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="s1">'feedback'</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"Good performance on this criterion."</span>

    <span class="n">overall</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">v</span><span class="p">[</span><span class="s1">'score'</span><span class="p">]</span> <span class="o">*</span> <span class="n">v</span><span class="p">[</span><span class="s1">'weight'</span><span class="p">]</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">rubric_criteria</span><span class="o">.</span><span class="n">values</span><span class="p">())</span> <span class="o">/</span> <span class="n">total_weight</span>
    <span class="n">detailed</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="p">{</span><span class="s2">"score"</span><span class="p">:</span> <span class="n">v</span><span class="p">[</span><span class="s1">'score'</span><span class="p">],</span> <span class="s2">"feedback"</span><span class="p">:</span> <span class="n">v</span><span class="p">[</span><span class="s1">'feedback'</span><span class="p">]}</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">rubric_criteria</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">"overall_score"</span><span class="p">:</span> <span class="n">overall</span><span class="p">,</span> <span class="s2">"detailed_scores"</span><span class="p">:</span> <span class="n">detailed</span><span class="p">}</span>
</code></pre></div>
<p>When you need a gold‑standard comparison, explicitly compare the model’s answer with the ideal expert answer and score high‑priority criteria (factual accuracy, alignment, completeness, coherence). Here’s a skeleton that returns both an aggregate score and the raw comparison text for audit:</p>
<div class="highlight"><pre><span/><code><span class="k">def</span><span class="w"> </span><span class="nf">detailed_evaluation_against_ideal_answer</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span> <span class="n">llm_response</span><span class="p">):</span>
    <span class="n">criteria</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">'factual_accuracy'</span><span class="p">:</span> <span class="p">{</span><span class="s1">'weight'</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">'score'</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">'feedback'</span><span class="p">:</span> <span class="s1">''</span><span class="p">},</span>
        <span class="s1">'alignment_with_ideal'</span><span class="p">:</span> <span class="p">{</span><span class="s1">'weight'</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s1">'score'</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">'feedback'</span><span class="p">:</span> <span class="s1">''</span><span class="p">},</span>
        <span class="s1">'completeness'</span><span class="p">:</span> <span class="p">{</span><span class="s1">'weight'</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s1">'score'</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">'feedback'</span><span class="p">:</span> <span class="s1">''</span><span class="p">},</span>
        <span class="s1">'coherence'</span><span class="p">:</span> <span class="p">{</span><span class="s1">'weight'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">'score'</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">'feedback'</span><span class="p">:</span> <span class="s1">''</span><span class="p">}</span>
    <span class="p">}</span>
    <span class="n">total</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">c</span><span class="p">[</span><span class="s1">'weight'</span><span class="p">]</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">criteria</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>

    <span class="n">system_prompt</span> <span class="o">=</span> <span class="s2">"Compare the LLM answer to the ideal answer, focusing on factual content and alignment."</span>
    <span class="n">comparison_prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"""</span><span class="se">\</span>
<span class="s2">    [Question]: </span><span class="si">{</span><span class="n">test_data</span><span class="p">[</span><span class="s1">'customer_query'</span><span class="p">]</span><span class="si">}</span>
<span class="s2">    [Ideal answer]: </span><span class="si">{</span><span class="n">test_data</span><span class="p">[</span><span class="s1">'ideal_answer'</span><span class="p">]</span><span class="si">}</span>
<span class="s2">    [LLM answer]: </span><span class="si">{</span><span class="n">llm_response</span><span class="si">}</span>
<span class="s2">    """</span>

    <span class="n">evaluation_text</span> <span class="o">=</span> <span class="n">fetch_llm_response</span><span class="p">([</span>
        <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"system"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="n">system_prompt</span><span class="p">},</span>
        <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="n">comparison_prompt</span><span class="p">},</span>
    <span class="p">])</span>

    <span class="c1"># Parsing stub</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">criteria</span><span class="p">:</span>
        <span class="n">criteria</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="s1">'score'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">8</span>
        <span class="n">criteria</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="s1">'feedback'</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"Good alignment with the gold answer."</span>

    <span class="n">score</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">v</span><span class="p">[</span><span class="s1">'score'</span><span class="p">]</span> <span class="o">*</span> <span class="n">v</span><span class="p">[</span><span class="s1">'weight'</span><span class="p">]</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">criteria</span><span class="o">.</span><span class="n">values</span><span class="p">())</span> <span class="o">/</span> <span class="n">total</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">"overall_score"</span><span class="p">:</span> <span class="n">score</span><span class="p">,</span> <span class="s2">"details"</span><span class="p">:</span> <span class="n">criteria</span><span class="p">,</span> <span class="s2">"raw"</span><span class="p">:</span> <span class="n">evaluation_text</span><span class="p">}</span>
</code></pre></div>
<p>On top of these basics, add advanced techniques: evaluate semantic similarity via embeddings and similarity metrics (not just surface overlap), bring in independent reviewers for crowd evaluation, include automated checks for coherence and logic, and build adaptive evaluation frameworks tailored to your domain and task types. In production, continuous evaluation is crucial: track version and metric history; close the loop from user feedback back to development; include diverse cases, edge cases, and cultural/linguistic variation; involve experts (including blind reviews to reduce bias); compare with alternative models; and employ specialized “judges” to detect contradictions and factual errors. Together, rigorous methods and constant iteration — plus rubrics, gold standards, expert reviews, and automated checks — help you build reliable and ethical systems.</p>
<h2 id="theory-questions">Theory Questions</h2>
<ol>
<li>Why evaluate LLM answers, and along which dimensions?</li>
<li>Give examples of metrics and explain their role in development.</li>
<li>What does the iterative path from development to production look like?</li>
<li>Why do high‑stakes scenarios require stricter rigor? Give examples.</li>
<li>List best practices for bootstrapping, iteration, and automated testing.</li>
<li>How do automated tests help development?</li>
<li>Why should metrics be tuned to the specific task?</li>
<li>How do you build a rubric and evaluation protocols?</li>
<li>Which advanced evaluation techniques apply and why?</li>
<li>How do continuous evaluation and broad test coverage improve reliability?</li>
</ol>
<h2 id="practical-tasks">Practical Tasks</h2>
<ol>
<li>Write a function that reads the API key from the environment, queries the LLM, and measures runtime and tokens used.</li>
</ol>












                
                  
</body>
</html>