["```r\n# input dimensionality (number of input features)\nd_in <- 3\n# number of observations in training set\nn <- 100\n\nx <- torch_randn(n, d_in)\ncoefs <- c(0.2, -1.3, -0.5)\ny <- x$matmul(coefs)$unsqueeze(2) + torch_randn(n, 1)\n```", "```r\n# dimensionality of hidden layer\nd_hidden <- 32\n# output dimensionality (number of predicted features)\nd_out <- 1\n\nnet <- nn_sequential(\n nn_linear(d_in, d_hidden),\n nn_relu(),\n nn_linear(d_hidden, d_out)\n)\n```", "```r\nopt <- optim_adam(net$parameters)\n\n### training loop --------------------------------------\n\nfor (t in 1:200) {\n\n ### -------- Forward pass --------\n y_pred <- net(x)\n\n ### -------- Compute loss -------- \n loss <- nnf_mse_loss(y_pred, y)\n if (t %% 10 == 0)\n cat(\"Epoch: \", t, \"   Loss: \", loss$item(), \"\\n\")\n\n ### -------- Backpropagation --------\n opt$zero_grad()\n loss$backward()\n\n ### -------- Update weights -------- \n opt$step()\n\n}\n```", "```r\nEpoch:  10    Loss:  2.549933 \nEpoch:  20    Loss:  2.422556 \nEpoch:  30    Loss:  2.298053 \nEpoch:  40    Loss:  2.173909 \nEpoch:  50    Loss:  2.0489 \nEpoch:  60    Loss:  1.924003 \nEpoch:  70    Loss:  1.800404 \nEpoch:  80    Loss:  1.678221 \nEpoch:  90    Loss:  1.56143 \nEpoch:  100    Loss:  1.453637 \nEpoch:  110    Loss:  1.355832 \nEpoch:  120    Loss:  1.269234 \nEpoch:  130    Loss:  1.195116 \nEpoch:  140    Loss:  1.134008 \nEpoch:  150    Loss:  1.085828 \nEpoch:  160    Loss:  1.048921 \nEpoch:  170    Loss:  1.021384 \nEpoch:  180    Loss:  1.0011 \nEpoch:  190    Loss:  0.9857832 \nEpoch:  200    Loss:  0.973796 \n```"]