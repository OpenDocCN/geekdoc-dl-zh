- en: '4'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Optimizing LLMs with Customized Fine-Tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we’ve exclusively used LLMs, both open and closed sourced, as they are
    off the shelf. We were relying on the power of the Transformer’s attention mechanisms
    and their speed of computation to perform pretty complex problems with relative
    ease. As you can probably guess, that isn’t always enough.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will delve into the world of fine-tuning Large Language
    Models (LLMs) to unlock their full potential. Fine-tuning updates off the shelf
    models and empowers them to achieve higher quality results, leads to token savings,
    and often lower latency requests. While GPT-like LLMs’ pre-training on extensive
    text data enables impressive few-shot learning capabilities, fine-tuning takes
    it a step further by refining the model on a multitude of examples, resulting
    in superior performance across various tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Running inference with fine-tuned models can be extremely cost-effective in
    the long run particularly when working with smaller models. For instance, a fine-tuned
    Ada model from OpenAI (only 350M parameters) costs only $0.0016 per 1k tokens,
    while ChatGPT (1.5B parameters) costs $0.002, and Davinci (175B parameters) costs
    $0.002\. Over time, the cost of using a fine tuned model is much more attractive
    as shown in [Figure 4.1](ch04.html#ch04fig01).
  prefs: []
  type: TYPE_NORMAL
- en: '![Images](graphics/04fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 4.1** *Assuming only 1,000 classifications a day and a relatively
    liberal prompt ratio (150 tokens (for few-shot examples, instructions,other) for
    DaVinci or ChatGPT for every 40 tokens), the cost of a fine-tuned model, even
    with an up-front cost, almost always wins the day overall cost-wise. Note that
    this does not take into the cost of fine-tuning a model which we will begin to
    explore later in this chapter.*'
  prefs: []
  type: TYPE_NORMAL
- en: My goal in this chapter is to guide you through the fine-tuning process, beginning
    with the preparation of training data, strategies for training a new or existing
    fine-tuned model, and a discussion of how to incorporate your fine-tuned model
    into real-world applications. This is a big topic and therefore we will have to
    assume some big pieces are being done behind the scenes like data labeling. Labeling
    data can be a huge expense in many cases of complex and specific tasks but for
    now, we will assume we can rely on the labels in our data for the most part. For
    more information on how to handle cases like these, feel free to check out some
    of my other content on feature engineering and label cleaning!
  prefs: []
  type: TYPE_NORMAL
- en: By understanding the nuances of fine-tuning and mastering its techniques, you
    will be well-equipped to harness the power of LLMs and create tailored solutions
    for your specific needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Transfer Learning and Fine-Tuning: A Primer'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Fine-tuning hinges on the idea of transfer learning. **Transfer learning** is
    a technique that leverages pre-trained models to build upon existing knowledge
    for new tasks or domains. In the case of LLMs, this involves utilizing the pre-training
    to transfer general language understanding, including grammar and general knowledge,
    to specific domain-specific tasks. However, the pre-training may not be sufficient
    to understand the nuances of certain closed or specialized topics, such as a company's
    legal structure or guidelines.
  prefs: []
  type: TYPE_NORMAL
- en: '**Fine-tuning** is a specific form of transfer learning that adjusts the parameters
    of a pre-trained model to better suit a “downstream” target task. Through fine-tuning,
    LLMs can learn from custom examples and become more effective at generating relevant
    and accurate responses.'
  prefs: []
  type: TYPE_NORMAL
- en: The Fine-Tuning Process Explained
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Fine-tuning a deep learning model involves updating the model's parameters to
    improve its performance on a specific task or dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![Images](graphics/square.jpg) **Training set:** A collection of labeled examples
    used to train the model. The model learns to recognize patterns and relationships
    in the data by adjusting its parameters based on the training examples.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Images](graphics/square.jpg) **Validation set:** A separate collection of
    labeled examples used to evaluate the model''s performance during training.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Images](graphics/square.jpg) **Test set:** A third collection of labeled
    examples that is separate from both the training and validation sets. It is used
    to evaluate the final performance of the model after the training and fine-tuning
    processes are complete. The test set provides a final, unbiased estimate of the
    model''s ability to generalize to new, unseen data.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Images](graphics/square.jpg) **Loss function:** The loss function quantifies
    the difference between the model''s predictions and the actual target values.
    It serves as a metric of error to evaluate the model''s performance and guide
    the optimization process. During training, the goal is to minimize the loss function
    to achieve better predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of fine-tuning can be broken down into a few steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. **Collecting Labeled data:** The first step in fine-tuning is to gather
    our training, validation, and testing datasets of labeled examples relevant to
    the target task or domain. Labeled data serves as a guide for the model to learn
    the task-specific patterns and relationships. For example, if the goal is to fine-tune
    a model for sentiment classification (our first example), the dataset should contain
    text examples along with their respective sentiment labels, such as positive,
    negative, or neutral.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. **Hyperparameter selection:** Fine-tuning involves adjusting hyperparameters
    that influence the learning process. These include hyperparameters like learning
    rate, batch size, and the number of epochs. The learning rate determines the step
    size of the model's weight updates, while the batch size refers to the number
    of training examples used in a single update. The number of epochs denotes how
    many times the model will iterate over the entire training dataset. Properly setting
    these hyperparameters can significantly impact the model's performance and help
    prevent issues such as overfitting - when a model learns the noise in the training
    data more than the signals - or underfitting - when a model fails to capture the
    underlying structure of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. **Model adaptation:** Once the labeled data and hyperparameters are set,
    the model may have to be adapted to the target task.This involves modifying the
    model's architecture, such as adding custom layers or changing the output structure,
    to better suit the target task. For example, BERT’s architecture cannot perform
    sequence classification as is but they can be modified very slightly to achieve
    the task. In our case study, we will not need to deal with that because OpenAI
    will deal with it for us. We will, however, have to deal with this issue in a
    later chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. **Evaluation and iteration:** After the fine-tuning process is over, we
    have to evaluate the model's performance on a separate holdout validation set
    to ensure that it generalizes well to unseen data. Performance metrics such as
    accuracy, F1-score, or Mean Absolute Error (MAE) can be used, depending on the
    task. If the performance is not satisfactory, adjustments to the hyperparameters
    or dataset may be necessary, followed by retraining the model.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. **Model implementation and further training:** Once the model is fine-tuned
    and we are happy with performance, we need to integrate it with existing infrastructures
    in a way that can handle any errors and collect feedback from users so we can
    add to our total dataset to run the process over again in the future
  prefs: []
  type: TYPE_NORMAL
- en: This process is outlined in [Figure 4.2](ch04.html#ch04fig02).
  prefs: []
  type: TYPE_NORMAL
- en: '![Images](graphics/04fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 4.2** *The fine-tuning process visualized. A dataset is broken up
    into training, validation, and testing tests. The training set is used to update
    the model’s weights and evaluate while the validation set is used to evaluate
    during training. The final model is then tested against the testing set and evaluated
    against a set of criteria. If the model passes our test, it is used in production
    and monitored for further iterations.*'
  prefs: []
  type: TYPE_NORMAL
- en: This process may require several iterations and careful consideration of hyperparameters,
    data quality, and model architecture to achieve the desired results.
  prefs: []
  type: TYPE_NORMAL
- en: Closed-Source Pre-trained Models as a Foundation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Pre-trained LLMs play a vital role in transfer learning and fine-tuning, providing
    a foundation of general language understanding and knowledge. This foundation
    allows for efficient adaptation to specific tasks and domains, reducing the need
    for extensive training resources and data.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter focuses on fine-tuning LLMs using OpenAI's infrastructure, which
    has been specifically designed to facilitate the process. OpenAI has developed
    tools and resources to make it easier for researchers and developers to fine-tune
    smaller models, such as Ada and Babbage, for their specific needs. The infrastructure
    offers a streamlined approach to fine-tuning, allowing users to efficiently adapt
    pre-trained models to a wide variety of tasks and domains.
  prefs: []
  type: TYPE_NORMAL
- en: Benefits of Using OpenAI's Fine-Tuning Infrastructure
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Leveraging OpenAI''s infrastructure for fine-tuning offers several advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Images](graphics/square.jpg) Access to powerful pre-trained models, like
    GPT-3, which have been trained on extensive and diverse datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Images](graphics/square.jpg) A relatively user-friendly interface that simplifies
    the fine-tuning process for people with varying levels of expertise.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Images](graphics/square.jpg) A range of tools and resources that help users
    optimize their fine-tuning process, such as guidelines for selecting hyperparameters,
    tips on preparing custom examples, and advice on model evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: This streamlined process saves time and resources while ensuring the development
    of high-quality models capable of generating accurate and relevant responses in
    a wide array of applications. We will dive deep into open-source fine-tuning and
    the benefits/drawbacks it offers in a later chapter.
  prefs: []
  type: TYPE_NORMAL
- en: A Look at the OpenAI Fine-Tuning API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The GPT-3 API offers developers access to one of the most advanced LLMs available.
    The API provides a range of fine-tuning capabilities, allowing users to adapt
    the model to specific tasks, languages, and domains. This section will discuss
    the key features of the GPT-3 fine-tuning API, the supported methods, and best
    practices for successfully fine-tuning models.
  prefs: []
  type: TYPE_NORMAL
- en: The GPT-3 Fine-Tuning API
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The GPT-3 fine-tuning API is like a treasure chest, brimming with powerful features
    that make customizing the model a breeze. From supporting various fine-tuning
    capabilities to offering a range of methods, it's a one-stop-shop for tailoring
    the model to your specific tasks, languages, or domains. This section will unravel
    the secrets of the GPT-3 fine-tuning API, uncovering the tools and techniques
    that make it such an invaluable resource.
  prefs: []
  type: TYPE_NORMAL
- en: 'Case Study: Amazon Review Sentiment Classification'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s introduce our first case study. We will be working with the **amazon_reviews_multi**
    dataset (previewed in [Figure 4.3](ch04.html#ch04fig03)). This dataset is a collection
    of product reviews from Amazon, spanning multiple product categories and languages
    (English, Japanese, German, French, Chinese and Spanish). Each review in the dataset
    is accompanied by a rating on a scale of 1 to 5 stars, with 1 being the lowest
    and 5 being the highest. The goal of this case study is to fine-tune a pre-trained
    model from OpenAI to perform sentiment classification on these reviews, enabling
    it to predict the number of stars given to a review. Taking a page out of my own
    book (albeit one just a few pages ago), let’s start with taking a look at the
    data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Images](graphics/04fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 4.3** *A snippet of the amazon_reviews_multi dataset shows our input
    context (review titles and bodies) and our response - the thing we are trying
    to predict - the number of stars the review was for (1-5).*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The columns we will care about for this round of fine-tuning are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Images](graphics/square.jpg) `review_title`: The text title of the review.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Images](graphics/square.jpg) `review_body`: The text body of the review.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Images](graphics/square.jpg) `stars`: An int between 1-5 indicating the number
    of stars.'
  prefs: []
  type: TYPE_NORMAL
- en: Our goal will be to use the context of the title and body of the review and
    predict the rating that was given.
  prefs: []
  type: TYPE_NORMAL
- en: Guidelines and Best Practices for Data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In general, there are a few items to consider when selecting data for fine-tuning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Images](graphics/square.jpg) **Data quality:** Ensure that the data used
    for fine-tuning is of high quality, free from noise, and accurately represents
    the target domain or task. This will enable the model to learn effectively from
    the given examples.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Images](graphics/square.jpg) **Data diversity:** Make sure your dataset is
    diverse, covering a broad range of scenarios to help the model generalize well
    across different situations.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Images](graphics/square.jpg) **Data balancing:** Maintaining a balanced distribution
    of examples across different tasks and domains helps prevent overfitting and biases
    in the model''s performance. This can be achieved from unbalanced datasets by
    undersampling majority classes, oversampling minority classes, or adding synthetic
    data. Our sentiment is perfectly balanced due to the fact that this dataset was
    curated but check out an even harder example in our code base where we attempt
    to classify the very unbalanced category classification task.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Images](graphics/square.jpg) **Data Quantity**: The total amount of data
    used to fine-tune the model. Generally, larger language models like LLMs require
    extensive data to capture and learn various patterns effectively but fewer if
    the LLM was pre-trained on similar enough data. The exact quantity needed can
    vary based on the complexity of the task at hand. Any dataset should not only
    be extensive but also diverse and representative of the problem space to avoid
    potential biases and ensure robust performance across a wide range of inputs.
    While a large quantity of data can help to improve model performance, it also
    increases the computational resources required for model training and fine-tuning.
    This trade-off needs to be considered in the context of the specific project requirements
    and resources.'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing Custom Examples with the OpenAI CLI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before diving into fine-tuning, we need to prepare the data by cleaning and
    formatting it according to the API''s requirements. This includes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Images](graphics/square.jpg) **Removing duplicates:** To ensure the highest
    data quality, start by removing any duplicate reviews from the dataset. This will
    prevent the model from overfitting to certain examples and improve its ability
    to generalize to new data.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Images](graphics/square.jpg) **Splitting the data:** Divide the dataset into
    training, validation, and test sets, maintaining a random distribution of examples
    across each set. If necessary, consider using stratified sampling to ensure that
    each set contains a representative proportion of the different sentiment labels,
    thus preserving the overall distribution of the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Images](graphics/square.jpg) **Shuffle the training data:** shuffling training
    data before fine-tuning helps to avoid biases in the learning process by ensuring
    that the model encounters examples in a random order, reducing the risk of learning
    unintended patterns based on the order of the examples. It also improves model
    generalization by exposing the model to a more diverse range of instances at each
    stage of training which also helps to prevent overfitting, as the model is less
    likely to memorize the training examples and instead focuses on learning the underlying
    patterns. [Figure 4.5](ch04.html#ch04fig05) shows the benefits of shuffling training
    data. Note that data are ideally shuffled before every single epoch to reduce
    the chance of the model over-fitting on the data as much as possible.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Images](graphics/square.jpg) **Creating the OpenAI JSONL format:** OpenAI''s
    API expects the training data to be in JSONL (newline-delimited JSON) format.
    For each example in the training and validation sets, create a JSON object with
    two fields: “prompt” (the input) and “completion” (the target class). The “prompt”
    field should contain the review text, and the “completion” field should store
    the corresponding sentiment label (stars). Save these JSON objects as newline-delimited
    records in separate files for the training and validation sets.'
  prefs: []
  type: TYPE_NORMAL
- en: For completion tokens in our dataset, we should make sure there is a leading
    space before the class label, as this enables the model to understand that it
    should generate a new token. Additionally, when preparing the prompts for the
    fine-tuning process, there's no need to include few-shot examples, as the model
    has already been fine-tuned on the task-specific data. Instead, provide a prompt
    that includes the review text and any necessary context, followed by a suffix
    (e.g., “Sentiment:” with no trailing space or “\n\n###\n\n” like in [Figure 4.4](ch04.html#ch04fig04))
    that indicates the desired output format. [Figure 4.4](ch04.html#ch04fig04) shows
    an example of a single line of our JSONL file.
  prefs: []
  type: TYPE_NORMAL
- en: '![Images](graphics/04fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 4.4** *A single JSONL example for our training data that we will feed
    to OpenAI. Every JSON has a prompt key - denoting the input to the model sans
    any few-shot, instructions, etc and a completion key - denoting what we want the
    model to output, which in our case is a single classification token. In this example,
    the user is rating the product with 1 star.*'
  prefs: []
  type: TYPE_NORMAL
- en: I should note that for our input data, I have concatenated the title and the
    body of the review as the singular input. This was a personal choice and I made
    it because I believe that the title can have more direct language to indicate
    general sentiment while the body likely has more nuanced to pinpoint the exact
    number of stars they are going to give. Feel free to explore different ways of
    combining text fields together! We are going to explore this further in later
    case studies along with other ways of formatting fields for a single text input.
  prefs: []
  type: TYPE_NORMAL
- en: '![Images](graphics/04fig05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 4.5** *Unshuffled data makes for bad training data! It gives the model
    room to overfit on specific batches of data and overall lowers the quality of
    the responses. The top three graphs represent a model trained on unshuffled training
    data and the accuracy is horrible compared to a model trained on shuffled data,
    seen in the bottom three graphs.*'
  prefs: []
  type: TYPE_NORMAL
- en: The following listing ([Listing 4.1](ch04.html#list4_1)) loads the Amazon Reviews
    dataset and converts the 'train' subset into a pandas DataFrame. Then, it preprocesses
    the DataFrame using the custom `prepare_df_for_openai` function, which combines
    the review title and review body into a prompt, creates a new completion column,
    and filters the DataFrame to only include English reviews. Finally, it removes
    duplicate rows based on the 'prompt' column and returns a DataFrame with only
    the 'prompt' and 'completion' columns.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.1** Generating a JSONL file for our sentiment training data'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We would do a similar process with the `validation` subset of the dataset and
    the holdout `test` subset for a final test of the fine-tuned model. A quick note
    that we are filtering for English only in this case but you are free to train
    on more languages mixed in. I simply wanted to get some quicker results at an
    efficient price.
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up the OpenAI CLI
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The OpenAI Command Line Interface (CLI) simplifies the process of fine-tuning
    and interacting with the API. The CLI allows you to submit fine-tuning requests,
    monitor training progress, and manage your models, all from your command line.
    Ensure that you have the OpenAI CLI installed and configured with your API key
    before proceeding with the fine-tuning process.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install the OpenAI CLI, you can use pip, the Python package manager. First,
    make sure you have Python 3.6 or later installed on your system. Then, follow
    these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Open a terminal (on macOS or Linux) or a command prompt (on Windows).
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Run the following command to install the openai package: `**pip install
    openai**`'
  prefs: []
  type: TYPE_NORMAL
- en: a. This command installs the OpenAI Python package, which includes the CLI.
  prefs: []
  type: TYPE_NORMAL
- en: '3\. To verify that the installation was successful, run the following command:
    `**openai --version**`'
  prefs: []
  type: TYPE_NORMAL
- en: a. This command should display the version number of the installed OpenAI CLI.
  prefs: []
  type: TYPE_NORMAL
- en: Before you can use the OpenAI CLI, you need to configure it with your API key.
    To do this, set the OPENAI_API_KEY environment variable to your API key value.
    You can find your API key in your OpenAI account dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter Selection and Optimization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'With our JSONL document created and OpenAI CLI installed, we are ready to select
    our hyperparameters! Here''s a list of key hyperparameters and their definitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Images](graphics/square.jpg) **Learning rate**: The learning rate determines
    the size of the steps the model takes during optimization. A smaller learning
    rate leads to slower convergence but potentially better accuracy, while a larger
    learning rate speeds up training but may cause the model to overshoot the optimal
    solution.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Images](graphics/square.jpg) **Batch size**: Batch size refers to the number
    of training examples used in a single iteration of model updates. A larger batch
    size can lead to more stable gradients and faster training, while a smaller batch
    size may result in a more accurate model but slower convergence.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Images](graphics/square.jpg) **Training epochs**: An epoch is a complete
    pass through the entire training dataset. The number of training epochs determines
    how many times the model will iterate over the data, allowing it to learn and
    refine its parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI has done a lot of work to find optimal settings for most cases, so we
    will lean on their recommendations for our first attempt. The only thing we will
    change is to train for 1 epoch instead of the default 4\. We're doing this because
    we want to see how the performance looks before investing too much time and money.
    Experimenting with different values and using techniques like grid search will
    help you find the optimal hyperparameter settings for your task and dataset, but
    be mindful that this process can be time-consuming and costly.
  prefs: []
  type: TYPE_NORMAL
- en: Our First Fine-Tuned LLM!
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s kick off our first fine-tuning! [Listing 4.2](ch04.html#list4_2) makes
    a call to OpenAI to train an ada model (fastest, cheapest, weakest) for 1 epoch
    on our training and validation data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.2** making our first fine-tuning call'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Evaluating Fine-Tuned Models with Quantitative Metrics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Measuring the performance of fine-tuned models is essential for understanding
    their effectiveness and identifying areas for improvement. Utilizing metrics and
    benchmarks, such as accuracy, F1 score, or perplexity, will provide quantitative
    measures of the model's performance. In addition to quantitative metrics, qualitative
    evaluation techniques, such as human evaluation or analyzing example outputs,
    can offer valuable insights into the model's strengths and weaknesses, helping
    identify areas for further fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: After one epoch (further metrics shown in [Figure 4.6](ch04.html#ch04fig06)),
    our classifier is getting above 63% accuracy on the holdout testing dataset! Remember
    the testing subset was not given to OpenAI but rather we held it out for final
    model comparisons.
  prefs: []
  type: TYPE_NORMAL
- en: '![Images](graphics/04fig06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 4.6** *Our model is performing pretty well after only one epoch on
    de-duplicated shuffled training data*'
  prefs: []
  type: TYPE_NORMAL
- en: '63% accuracy might sound low to you but hear me out: predicting the **exact**
    number of stars is tricky because people aren’t always consistent in what they
    write and how they finally review the product so I’ll offer two more metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Images](graphics/square.jpg) Relaxing our accuracy calculation to be binary
    (did the model predict <= 3 stars and was the review <=3 stars) is **92%** so
    the model can tell between “good” and “bad”'
  prefs: []
  type: TYPE_NORMAL
- en: '![Images](graphics/square.jpg) Relaxing the calculation to be “one-off” so
    for the example the model predicting 2 would count as correct if the actual rating
    was 1, 2, or 3 is **93%**'
  prefs: []
  type: TYPE_NORMAL
- en: So you know what? Not bad. Our classifier is definitely learning the difference
    between good and bad so the next logical thought might be, “let’s keep the training
    going”! We only trained for a single epoch so more epochs must be better, right?
  prefs: []
  type: TYPE_NORMAL
- en: This process of taking smaller steps in training and updating already fine-tuned
    models for more training steps/epochs potentially with new labeled datapoints
    is called **incremental learning** aka continuous learning or online learning.
    Incremental learning often results in more controlled learning, which can be ideal
    when working with smaller datasets or when you want to preserve some of the model's
    general knowledge. Let’s try some incremental learning! Let’s take our already
    fine-tuned ada model and let it run for 3 more epochs on the same data and see
    the results in [Figure 4.7](ch04.html#ch04fig07).
  prefs: []
  type: TYPE_NORMAL
- en: '![Images](graphics/04fig07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 4.7**: The model’s performance seems to barely move during a further
    3 epochs of incremental learning after a successful single epoch. 4x the cost
    for 1.02x the performance? No thank you.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Uh oh, more epochs didn’t seem to really do anything, but nothing is set in
    stone until we test on our holdout test data subset and compare it to our first
    model. [Table 4.1](ch04.html#ch04tab01) shows our results:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 4.1** *Results*'
  prefs: []
  type: TYPE_NORMAL
- en: '![Images](graphics/04tab01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So for 4x the price, we get a single % point increase in accuracy? Not worth
    it in my book but maybe it is for you! Some industries demand near perfection
    in their models and single percentage points matter. I’ll leave that up to you
    but in general more epochs will not always lead to better results. Incremental/online
    learning can help you find the right stopping point at the cost of more up-front
    effort but will be well worth it in the long run.
  prefs: []
  type: TYPE_NORMAL
- en: Qualitative Evaluation Techniques
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Alongside quantitative metrics, qualitative evaluation techniques offer valuable
    insights into the strengths and weaknesses of our fine-tuned model. Examining
    generated outputs or employing human evaluators can help identify areas where
    the model excels or falls short, guiding our future fine-tuning efforts.
  prefs: []
  type: TYPE_NORMAL
- en: To help, we can get the probability for our classification by looking at the
    probabilities of predicting the first token in either the Playground (as seen
    in [Figure 4.8](ch04.html#ch04fig08)) or via the API’s `logprobs` value (as seen
    in [Listing 4.3](ch04.html#list4_3))
  prefs: []
  type: TYPE_NORMAL
- en: '![Images](graphics/04fig08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 4.8** *The playground and the API for GPT-3 like models (including
    our fine-tuned ada model as seen in this figure) offer token probabilities that
    we can use to check the model’s confidence on a particular classification. Note
    that the main option is “ 1“ with a leading space just like we have in our training
    data but one of the tokens on the top of the list is “1” with no leading space.
    These are two separate tokens according to many LLMs which is why I am calling
    it out so often. It can be easy to forget and mix them up.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.3** getting token probabilities from the OpenAI API'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Output**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Between quantitative and qualitative measures, let’s assume we believe our model
    is ready to go into production – if not at least a dev or staging environment
    for further testing, let’s take a minute to talk about how we can incorporate
    our new model into our applications.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating Fine-Tuned GPT-3 Models into Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Integrating a fine-tuned GPT-3 model into your application is identical to
    using a base model provided by OpenAI. The primary difference is that you''ll
    need to reference your fine-tuned model''s unique identifier when making API calls.
    Here are the key steps to follow:'
  prefs: []
  type: TYPE_NORMAL
- en: '1\. **Identify your fine-tuned model**: After completing the fine-tuning process,
    you will receive a unique identifier for your fine-tuned model like `‘ada:ft-personal-2023-03-31-05-30-46’`.
    Make sure to note this identifier, as it will be required for API calls.'
  prefs: []
  type: TYPE_NORMAL
- en: '2\. **Use the OpenAI API Normally**: Use yourOpenAI API to make requests to
    your fine-tuned model. When making requests, replace the base model''s name with
    your fine-tuned model''s unique identifier. [Listing 4.3](ch04.html#list4_3) offers
    an example of doing this.'
  prefs: []
  type: TYPE_NORMAL
- en: '3\. **Adapt any application logic**: Since fine-tuned models may require different
    prompt structures or generate different output formats, you may need to update
    your application''s logic to handle these variations. For example in our prompts,
    we concatenated the review title with the body and added a custom suffix “\n\n###\n\n”.'
  prefs: []
  type: TYPE_NORMAL
- en: '4\. **Monitor and evaluate performance**: Continuously monitor your fine-tuned
    model''s performance and collect user feedback. You may need to iteratively fine-tune
    your model with even more data to improve its accuracy and effectiveness.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Case Study 2: Amazon Review Category Classification'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With a successfully fine-tuned ada model for a relatively simple example like
    sentiment classification, let's up the stakes and tackle a more challenging task.
    In a second case study, we will explore how fine-tuning a GPT-3 model can improve
    its performance on the task of Amazon review category classification from the
    same dataset. This task involves classifying Amazon product reviews into their
    respective product categories based on the review title and body - just like we
    did for sentiment. We no longer have 5 classes for example, we now have 31 unbalanced
    classes (see [Figure 4.9](ch04.html#ch04fig09))!
  prefs: []
  type: TYPE_NORMAL
- en: '![Images](graphics/04fig09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 4.9** *The category classification task has 31 unique categories to
    choose from and a very unbalanced class distribution which is a perfect storm
    for a difficult classification task*'
  prefs: []
  type: TYPE_NORMAL
- en: The much harder category classification task reveals a lot of hidden difficulties
    of ML, such as dealing with unbalanced data and **ill-defined data**—where the
    distinction between categories is subtle or ambiguous. In these cases, the model
    may struggle to discern the correct category. To improve performance, consider
    refining the problem definition, deleting redundant or confusing training examples,
    merging similar categories, or providing additional context to the model through
    prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Check out all of that work in our code repository!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Fine-tuning LLMs like GPT-3 is an effective way to enhance their performance
    on specific tasks or domains. By integrating a fine-tuned model into your application
    and following best practices for deployment, you can create a more efficient,
    accurate, and cost-effective language processing solution. Continuously monitor
    and evaluate your model's performance, and iterate on its fine-tuning to ensure
    it meets the evolving needs of your application and users.
  prefs: []
  type: TYPE_NORMAL
- en: We will revisit the idea of fine-tuning in later chapters with some more complicated
    examples while also exploring the fine-tuning strategies for open-source models
    for even further cost reductions.
  prefs: []
  type: TYPE_NORMAL
