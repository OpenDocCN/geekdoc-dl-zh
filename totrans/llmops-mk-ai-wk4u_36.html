<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Answers 2.6</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Answers 2.6</h1>
<blockquote>原文：<a href="https://boramorka.github.io/LLM-Book/en/CHAPTER-2/Answers%202.6/">https://boramorka.github.io/LLM-Book/en/CHAPTER-2/Answers%202.6/</a></blockquote>
                
                  


  
  



<h2 id="theory">Theory</h2>
<ol>
<li>Three stages of RAG‑QA: accept the query, retrieve relevant documents, and generate the answer.</li>
<li>Context window constraints: because the LLM context is limited, you cannot pass every fragment. MapReduce and Refine help aggregate or iteratively refine information across multiple documents.</li>
<li>Vector database: stores document embeddings and provides fast retrieval of the most relevant documents based on semantic similarity.</li>
<li>RetrievalQA chain: combines retrieval and answer generation, improving relevance and accuracy of results.</li>
<li>MapReduce and Refine: MapReduce quickly produces a summary from many documents; Refine sequentially improves the answer, which is useful when precision is critical. Choose based on the task.</li>
<li>Distributed systems: account for network latency and serialization when operating in distributed setups.</li>
<li>Experimentation: try MapReduce and Refine; effectiveness depends heavily on data types and question styles.</li>
<li>RetrievalQA limitation: no built‑in dialogue memory, which makes maintaining context across follow‑ups difficult.</li>
<li>Dialogue memory: needed to incorporate previous turns and provide contextual answers during longer conversations.</li>
<li>Further study: new LLM approaches, their impact on RAG systems, and memory strategies in RAG chains.</li>
</ol>
<h2 id="practical-tasks">Practical Tasks</h2>
<p>1.
</p><div class="highlight"><pre><span/><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain.vectorstores</span><span class="w"> </span><span class="kn">import</span> <span class="n">Chroma</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAIEmbeddings</span>

<span class="k">def</span><span class="w"> </span><span class="nf">initialize_vector_database</span><span class="p">(</span><span class="n">directory_path</span><span class="p">):</span>
    <span class="c1"># Initialize an embeddings generator (OpenAI) to create vector representations for text</span>
    <span class="n">embeddings_generator</span> <span class="o">=</span> <span class="n">OpenAIEmbeddings</span><span class="p">()</span>

    <span class="c1"># Initialize a Chroma vector database pointing to a persistence directory</span>
    <span class="c1"># and the embedding function to use</span>
    <span class="n">vector_database</span> <span class="o">=</span> <span class="n">Chroma</span><span class="p">(</span><span class="n">persist_directory</span><span class="o">=</span><span class="n">directory_path</span><span class="p">,</span> <span class="n">embedding_function</span><span class="o">=</span><span class="n">embeddings_generator</span><span class="p">)</span>

    <span class="c1"># Display current document count to verify initialization</span>
    <span class="c1"># Assumes Chroma exposes `_collection.count()`</span>
    <span class="n">document_count</span> <span class="o">=</span> <span class="n">vector_database</span><span class="o">.</span><span class="n">_collection</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Documents in VectorDB: </span><span class="si">{</span><span class="n">document_count</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="c1"># Example usage of initialize_vector_database:</span>
<span class="n">documents_storage_directory</span> <span class="o">=</span> <span class="s1">'path/to/your/directory'</span>
<span class="n">initialize_vector_database</span><span class="p">(</span><span class="n">documents_storage_directory</span><span class="p">)</span>
</code></pre></div>
<p>2.
</p><div class="highlight"><pre><span/><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain.vectorstores</span><span class="w"> </span><span class="kn">import</span> <span class="n">Chroma</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAIEmbeddings</span><span class="p">,</span> <span class="n">ChatOpenAI</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.chains</span><span class="w"> </span><span class="kn">import</span> <span class="n">RetrievalQA</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.prompts</span><span class="w"> </span><span class="kn">import</span> <span class="n">PromptTemplate</span>

<span class="k">def</span><span class="w"> </span><span class="nf">setup_retrieval_qa_chain</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">documents_storage_directory</span><span class="p">):</span>
    <span class="c1"># Initialize embeddings and Chroma vector store</span>
    <span class="n">embeddings_generator</span> <span class="o">=</span> <span class="n">OpenAIEmbeddings</span><span class="p">()</span>
    <span class="n">vector_database</span> <span class="o">=</span> <span class="n">Chroma</span><span class="p">(</span><span class="n">persist_directory</span><span class="o">=</span><span class="n">documents_storage_directory</span><span class="p">,</span> <span class="n">embedding_function</span><span class="o">=</span><span class="n">embeddings_generator</span><span class="p">)</span>

    <span class="c1"># Initialize the language model (LLM) used in the RetrievalQA chain</span>
    <span class="n">language_model</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Define a custom prompt template to format LLM inputs</span>
    <span class="n">custom_prompt_template</span> <span class="o">=</span> <span class="s2">"""To better assist with the inquiry, consider the details provided below as your reference...</span>
<span class="si">{context}</span>
<span class="s2">Inquiry: </span><span class="si">{question}</span>
<span class="s2">Insightful Response:"""</span>

    <span class="c1"># Create the RetrievalQA chain, passing the LLM, a retriever from the vector DB,</span>
    <span class="c1"># requesting source documents, and using the custom prompt</span>
    <span class="n">question_answering_chain</span> <span class="o">=</span> <span class="n">RetrievalQA</span><span class="o">.</span><span class="n">from_chain_type</span><span class="p">(</span>
        <span class="n">language_model</span><span class="p">,</span>
        <span class="n">retriever</span><span class="o">=</span><span class="n">vector_database</span><span class="o">.</span><span class="n">as_retriever</span><span class="p">(),</span>
        <span class="n">return_source_documents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">chain_type_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">"prompt"</span><span class="p">:</span> <span class="n">PromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="n">custom_prompt_template</span><span class="p">)}</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">question_answering_chain</span>

<span class="c1"># Example usage of setup_retrieval_qa_chain:</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">"gpt-4o-mini"</span>
<span class="n">documents_storage_directory</span> <span class="o">=</span> <span class="s1">'path/to/your/documents'</span>
<span class="n">qa_chain</span> <span class="o">=</span> <span class="n">setup_retrieval_qa_chain</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">documents_storage_directory</span><span class="p">)</span>
</code></pre></div>
<p>3.
</p><div class="highlight"><pre><span/><code><span class="c1"># Assume setup_retrieval_qa_chain has been defined in the same script or imported.</span>

<span class="c1"># Configure to demonstrate both techniques (MapReduce and Refine)</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">"gpt-3.5-turbo"</span>
<span class="n">documents_storage_directory</span> <span class="o">=</span> <span class="s1">'path/to/your/documents'</span>
<span class="n">qa_chain</span> <span class="o">=</span> <span class="n">setup_retrieval_qa_chain</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">documents_storage_directory</span><span class="p">)</span>

<span class="c1"># Create QA chains: one for MapReduce, one for Refine</span>
<span class="n">question_answering_chain_map_reduce</span> <span class="o">=</span> <span class="n">RetrievalQA</span><span class="o">.</span><span class="n">from_chain_type</span><span class="p">(</span>
    <span class="n">qa_chain</span><span class="o">.</span><span class="n">llm</span><span class="p">,</span>
    <span class="n">retriever</span><span class="o">=</span><span class="n">qa_chain</span><span class="o">.</span><span class="n">retriever</span><span class="p">,</span>
    <span class="n">chain_type</span><span class="o">=</span><span class="s2">"map_reduce"</span>  <span class="c1"># Use MapReduce chain type</span>
<span class="p">)</span>

<span class="n">question_answering_chain_refine</span> <span class="o">=</span> <span class="n">RetrievalQA</span><span class="o">.</span><span class="n">from_chain_type</span><span class="p">(</span>
    <span class="n">qa_chain</span><span class="o">.</span><span class="n">llm</span><span class="p">,</span>
    <span class="n">retriever</span><span class="o">=</span><span class="n">qa_chain</span><span class="o">.</span><span class="n">retriever</span><span class="p">,</span>
    <span class="n">chain_type</span><span class="o">=</span><span class="s2">"refine"</span>  <span class="c1"># Use Refine chain type</span>
<span class="p">)</span>

<span class="c1"># Example query to test both techniques</span>
<span class="n">query</span> <span class="o">=</span> <span class="s2">"What is the importance of probability in machine learning?"</span>

<span class="c1"># Run MapReduce and print the answer</span>
<span class="n">response_map_reduce</span> <span class="o">=</span> <span class="n">question_answering_chain_map_reduce</span><span class="p">({</span><span class="s2">"query"</span><span class="p">:</span> <span class="n">query</span><span class="p">})</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"MapReduce answer:"</span><span class="p">,</span> <span class="n">response_map_reduce</span><span class="p">[</span><span class="s2">"result"</span><span class="p">])</span>

<span class="c1"># Run Refine and print the answer</span>
<span class="n">response_refine</span> <span class="o">=</span> <span class="n">question_answering_chain_refine</span><span class="p">({</span><span class="s2">"query"</span><span class="p">:</span> <span class="n">query</span><span class="p">})</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Refine answer:"</span><span class="p">,</span> <span class="n">response_refine</span><span class="p">[</span><span class="s2">"result"</span><span class="p">])</span>
</code></pre></div>
<p>4.
</p><div class="highlight"><pre><span/><code><span class="k">def</span><span class="w"> </span><span class="nf">handle_conversational_context</span><span class="p">(</span><span class="n">initial_query</span><span class="p">,</span> <span class="n">follow_up_query</span><span class="p">,</span> <span class="n">qa_chain</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Simulate handling a follow‑up question in a longer conversation.</span>

<span class="sd">    Args:</span>
<span class="sd">    - initial_query (str): First user query.</span>
<span class="sd">    - follow_up_query (str): Follow‑up query referring to prior context.</span>
<span class="sd">    - qa_chain (RetrievalQA): Initialized QA chain that can answer queries.</span>

<span class="sd">    Returns:</span>
<span class="sd">    - None: Prints both answers directly to the console.</span>
<span class="sd">    """</span>
    <span class="c1"># Generate the answer to the initial query</span>
    <span class="n">initial_response</span> <span class="o">=</span> <span class="n">qa_chain</span><span class="p">({</span><span class="s2">"query"</span><span class="p">:</span> <span class="n">initial_query</span><span class="p">})</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Answer to initial query:"</span><span class="p">,</span> <span class="n">initial_response</span><span class="p">[</span><span class="s2">"result"</span><span class="p">])</span>

    <span class="c1"># Generate the answer to the follow‑up query (note: no dialogue memory)</span>
    <span class="n">follow_up_response</span> <span class="o">=</span> <span class="n">qa_chain</span><span class="p">({</span><span class="s2">"query"</span><span class="p">:</span> <span class="n">follow_up_query</span><span class="p">})</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Answer to follow‑up query:"</span><span class="p">,</span> <span class="n">follow_up_response</span><span class="p">[</span><span class="s2">"result"</span><span class="p">])</span>

<span class="c1"># Example usage</span>
<span class="n">a_initial</span> <span class="o">=</span> <span class="s2">"Does the curriculum cover probability theory?"</span>
<span class="n">a_follow_up</span> <span class="o">=</span> <span class="s2">"Why are those prerequisites important?"</span>
<span class="n">handle_conversational_context</span><span class="p">(</span><span class="n">a_initial</span><span class="p">,</span> <span class="n">a_follow_up</span><span class="p">,</span> <span class="n">qa_chain</span><span class="p">)</span>
</code></pre></div>












                
                  
</body>
</html>