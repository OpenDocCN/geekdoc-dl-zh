<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>10  Function minimization with L-BFGS</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>10  Function minimization with L-BFGS</h1>
<blockquote>原文：<a href="https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/optim_2.html">https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/optim_2.html</a></blockquote>

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">

</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>Now that we’ve become acquainted with <code>torch</code> modules and optimizers, we can go back to the two tasks we already approached without either: function minimization, and training a neural network. Again, we start with minimization, and leave the network to the next chapter.</p>
<p>Thinking back to what we did when minimizing the Rosenbrock function, in essence it was this:</p>
<ol type="1">
<li><p>Define a tensor to hold the parameter to be optimized, namely, the <span class="math inline">\(\mathbf{x}\)</span>-position where the function attains its minimum.</p></li>
<li><p>Iteratively update the parameter, subtracting a fraction of the current gradient.</p></li>
</ol>
<p>While as a strategy, this was straightforward, a problem remained: How big a fraction of the gradient should we subtract? It’s exactly here that optimizers come in useful.</p>
<section id="meet-l-bfgs" class="level2" data-number="10.1">
<h2 data-number="10.1" class="anchored" data-anchor-id="meet-l-bfgs"><span class="header-section-number">10.1</span> Meet L-BFGS</h2>
<p>So far, we’ve only talked about the kinds of optimizers often used in deep learning – stochastic gradient descent (SGD), SGD with momentum, and a few classics from the <em>adaptive</em> <em>learning rate</em> family: RMSProp, Adadelta, Adagrad, Adam. All these have in common one thing: They only make use of the <em>gradient</em>, that is, the vector of first derivatives. Accordingly, they are all <em>first-order</em> algorithms. This means, however, that they are missing out on helpful information provided by the <em>Hessian</em>, the matrix of second derivatives.</p>
<section id="changing-slopes" class="level3" data-number="10.1.1">
<h3 data-number="10.1.1" class="anchored" data-anchor-id="changing-slopes"><span class="header-section-number">10.1.1</span> Changing slopes</h3>
<p>First derivatives tell us about the <em>slope</em> of the landscape: Does it go up? Does it go down? How much so? Going a step further, second derivatives encode how much that slope <em>changes</em>.</p>
<p>Why should that be important?</p>
<p>Assume we’re at point <span class="math inline">\(\mathbf{x}_n\)</span>, and have just decided on a suitable descent direction. We take a step, of length determined by some pre-chosen learning rate, all set to arrive at point <span class="math inline">\(\mathbf{x}_{n+1}\)</span>. What we don’t know is how the slope will have changed by the time we’ll have gotten there. Maybe it’s become much flatter in the meantime: In this case, we’ll have gone way too far, overshooting and winding up in a far-off area where anything could have happened in-between (including the slope going <em>up</em> again!).</p>
<p>We can illustrate this on a function of a single variable. Take a parabola, such as</p>
<p><span class="math display">\[
y = 10x^2
\]</span></p>
<p>Its derivative is <span class="math inline">\(\frac{dy}{dx} = 20x\)</span>. If our current <span class="math inline">\(x\)</span> is, say, <span class="math inline">\(3\)</span>, and we work with a learning rate of <span class="math inline">\(0.1\)</span>, we’ll subtract <span class="math inline">\(20 * 3 * 0.1= 6\)</span>, winding up at <span class="math inline">\(-3\)</span>.</p>
<p>But say we had slowed down at <span class="math inline">\(2\)</span> and inspected the current slope. We’d have seen that there, the slope was less steep; in fact, when at that point, we should just have subtracted <span class="math inline">\(20 * 2 * 0.1= 4\)</span>.</p>
<p>By sheer luck, this “close-your-eyes-and-jump” strategy can still work out – <em>if</em> we happen to be using just the right learning rate for the function in question. (At the chosen learning rate, this would have been the case for a different parabola, <span class="math inline">\(y = 5x^2\)</span>, for example.) But wouldn’t it make sense to include second derivatives in the decision from the outset?</p>
<p>Algorithms that do this form the family of Newton methods. First, we look at their “purest” specimen, which best illustrates the principle but seldom is feasible in practice.</p>
</section>
<section id="exact-newton-method" class="level3" data-number="10.1.2">
<h3 data-number="10.1.2" class="anchored" data-anchor-id="exact-newton-method"><span class="header-section-number">10.1.2</span> Exact Newton method</h3>
<p>In higher dimensions, the exact Newton method multiplies the gradient by the inverse of the Hessian, thus scaling the descent direction coordinate-by-coordinate. Our current example has just a single independent variable; so this means for us: take the first derivative, and divide by the second.</p>
<p>We now have a scaled gradient – but what portion of it should we subtract? In its original version, the exact Newton method does not make use of a learning rate, thus freeing us of the familiar trial-and-error game. Let’s see, then: In our example, the second derivative is <span class="math inline">\(20\)</span>, meaning that at <span class="math inline">\(x=3\)</span> we have to subtract <span class="math inline">\((20 * 3)/20=3\)</span>. Voilà, we end up at <span class="math inline">\(0\)</span>, the location of the minimum, in a single step.</p>
<p>Seeing how that turned out just great, why don’t we do it all the time? For one, it will work perfectly only with quadratic functions, like the one we chose for the demonstration. In other cases, it, too, will normally need some “tuning”, for example, by using a learning rate here as well.</p>
<p>But the main reason is another one. In more realistic applications, and certainly in the areas of machine learning and deep learning, computing the inverse of the Hessian at every step is way too costly. (It may, in fact, not even be possible.) This is where <em>approximate</em>, a.k.a. <em>Quasi-Newton</em>, methods come in.</p>
</section>
<section id="approximate-newton-bfgs-and-l-bfgs" class="level3" data-number="10.1.3">
<h3 data-number="10.1.3" class="anchored" data-anchor-id="approximate-newton-bfgs-and-l-bfgs"><span class="header-section-number">10.1.3</span> Approximate Newton: BFGS and L-BFGS</h3>
<p>Among approximate Newton methods, probably the most-used is the <em>Broyden-Goldfarb-Fletcher-Shanno</em> algorithm, or <em>BFGS</em>. Instead of continually computing the exact inverse of the Hessian, it keeps an iteratively-updated approximation of that inverse. BFGS is often implemented in a more memory-friendly version, referred to as <em>Limited-Memory BFGS</em> (<em>L-BFGS</em>). This is the one provided as part of the core <code>torch</code> optimizers.</p>
<p>Before we get there, though, there is one last conceptual thing to discuss.</p>
</section>
<section id="line-search" class="level3" data-number="10.1.4">
<h3 data-number="10.1.4" class="anchored" data-anchor-id="line-search"><span class="header-section-number">10.1.4</span> Line search</h3>
<p>Like their exact counterpart, approximate Newton methods can work without a learning rate. In that case, they compute a descent direction and follow the scaled gradient as-is. We already talked about how, depending on the function in question, this can work more or less well. When it does not, there are two things one could do: Firstly, take small steps, or put differently, introduce a learning rate. And secondly, do a <em>line search</em>.</p>
<p>With line search, we spend some time evaluating how far to follow the descent direction. There are two principal ways of doing this.</p>
<p>The first, <em>exact</em> line search, involves yet another optimization problem: Take the current point, compute the descent direction, and hard-code them as givens in a <em>second</em> function that depends on the learning rate only. Then, differentiate this function to find <em>its</em> minimum. The solution will be the learning rate that optimizes the step length taken.</p>
<p>The alternative strategy is to do an approximate search. By now, you’re probably not surprised: Just as approximate Newton is more realistically-feasible than exact Newton, approximate line search is more practicable than exact line search.</p>
<p>For line search, approximating the best solution means following a set of proven heuristics. Essentially, we look for something that is <em>just</em> <em>good enough</em>. Among the most established heuristics are the <em>Strong Wolfe conditions</em>, and this is the strategy implemented in <code>torch</code>’s <code>optim_lbfgs()</code>. In the next section, we’ll see how to use <code>optim_lbfgs()</code> to minimize the Rosenbrock function, both with and without line search.</p>
</section>
</section>
<section id="minimizing-the-rosenbrock-function-with-optim_lbfgs" class="level2" data-number="10.2">
<h2 data-number="10.2" class="anchored" data-anchor-id="minimizing-the-rosenbrock-function-with-optim_lbfgs"><span class="header-section-number">10.2</span> Minimizing the Rosenbrock function with <code>optim_lbfgs()</code></h2>
<p>Here is the Rosenbrock function again:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"/><span class="fu">library</span>(torch)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"/></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"/>a <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"/>b <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"/></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"/>rosenbrock <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"/>  x1 <span class="ot">&lt;-</span> x[<span class="dv">1</span>]</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"/>  x2 <span class="ot">&lt;-</span> x[<span class="dv">2</span>]</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"/>  (a <span class="sc">-</span> x1)<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> b <span class="sc">*</span> (x2 <span class="sc">-</span> x1<span class="sc">^</span><span class="dv">2</span>)<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"/>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>In our manual minimization efforts, the procedure was the following. A one-time action, we first defined the parameter tensor destined to hold the current <span class="math inline">\(\mathbf{x}\)</span>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"/>x <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(<span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>), <span class="at">requires_grad =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>Then, we iteratively executed the following operations:</p>
<ol type="1">
<li><p>Calculate the function value at the current <span class="math inline">\(\mathbf{x}\)</span>.</p></li>
<li><p>Compute the gradient of that value at the position in question.</p></li>
<li><p>Subtract a fraction of the gradient from the current <span class="math inline">\(\mathbf{x}\)</span>.</p></li>
</ol>
<p>How, if so, does that blueprint change?</p>
<p>The first step remains unchanged. We still have</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"/>value <span class="ot">&lt;-</span> <span class="fu">rosenbrock</span>(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>The second step stays the same, as well. We still call <code>backward()</code> directly on the output tensor:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"/>value<span class="sc">$</span><span class="fu">backward</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>This is because an optimizer does not <em>compute</em> gradients; it <em>decides what to do with the gradient</em> once it’s been computed.</p>
<p>What changes, thus, is the third step, the one that also was the most cumbersome. Now, it is the optimizer that applies the update. To be able to do that, there is a prerequisite: Prior to starting the loop, the optimizer will need to be told which parameter it is supposed to work on. In fact, this is so important that you can’t even create an optimizer without passing it that parameter:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"/>opt <span class="ot">&lt;-</span> <span class="fu">optim_lbfgs</span>(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>In the loop, we now call the <code>step()</code> method on the optimizer object to update the parameter. There is just one part from our manual procedure that needs to get carried over to the new way: We still need to zero out the gradient on each iteration. Just this time, not on the parameter tensor, <code>x</code>, but the optimizer object itself. <em>In principle</em>, this then yields the following actions to be performed on each iteration:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"/>value <span class="ot">&lt;-</span> <span class="fu">rosenbrock</span>(x)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"/></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"/>opt<span class="sc">$</span><span class="fu">zero_grad</span>()</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"/>value<span class="sc">$</span><span class="fu">backward</span>()</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"/></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"/>opt<span class="sc">$</span><span class="fu">step</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>Why “in principle”? In fact, this is what we’d write for every optimizer <em>but</em> <code>optim_lbfgs()</code>.</p>
<p>For <code>optim_lbfgs()</code>, <code>step()</code> needs to be called passing in an anonymous function, a closure. Zeroing of previous gradients, function call, and gradient calculation, all these happen inside the closure:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"/>calc_loss <span class="ot">&lt;-</span> <span class="cf">function</span>() {</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"/>  optimizer<span class="sc">$</span><span class="fu">zero_grad</span>()</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"/>  value <span class="ot">&lt;-</span> <span class="fu">rosenbrock</span>(x_star)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"/>  value<span class="sc">$</span><span class="fu">backward</span>()</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"/>  value</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"/>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>Having executed those actions, the closure returns the function value. Here is how it is called by <code>step()</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"/><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>num_iterations) {</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"/>  optimizer<span class="sc">$</span><span class="fu">step</span>(calc_loss)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"/>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>Now we put it all together, add some logging output, and compare what happens with and without line search.</p>
<section id="optim_lbfgs-default-behavior" class="level3" data-number="10.2.1">
<h3 data-number="10.2.1" class="anchored" data-anchor-id="optim_lbfgs-default-behavior"><span class="header-section-number">10.2.1</span> <code>optim_lbfgs()</code> default behavior</h3>
<p>As a baseline, we first run without line search. Two iterations are enough. In the below output, you can see that in each iteration, the closure is evaluated several times. This is the technical reason we had to create it in the first place.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"/>num_iterations <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"/></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"/>x <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(<span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>), <span class="at">requires_grad =</span> <span class="cn">TRUE</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"/></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"/>optimizer <span class="ot">&lt;-</span> <span class="fu">optim_lbfgs</span>(x)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"/></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"/>calc_loss <span class="ot">&lt;-</span> <span class="cf">function</span>() {</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"/>  optimizer<span class="sc">$</span><span class="fu">zero_grad</span>()</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"/></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"/>  value <span class="ot">&lt;-</span> <span class="fu">rosenbrock</span>(x)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"/>  <span class="fu">cat</span>(<span class="st">"Value is: "</span>, <span class="fu">as.numeric</span>(value), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"/></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"/>  value<span class="sc">$</span><span class="fu">backward</span>()</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"/>  value</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"/>}</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"/></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"/><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>num_iterations) {</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"/>  <span class="fu">cat</span>(<span class="st">"</span><span class="sc">\n</span><span class="st">Iteration: "</span>, i, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"/>  optimizer<span class="sc">$</span><span class="fu">step</span>(calc_loss)</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"/>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<pre><code>Iteration:  1 
Value is:  4 
Value is:  6 
Value is:  318.0431 
Value is:  5.146369 
Value is:  4.443705 
Value is:  0.8787204 
Value is:  0.8543001 
Value is:  2.001667 
Value is:  0.5656172 
Value is:  0.400589 
Value is:  7.726219 
Value is:  0.3388008 
Value is:  0.2861604 
Value is:  1.951176 
Value is:  0.2071857 
Value is:  0.150776 
Value is:  0.411357 
Value is:  0.08056168 
Value is:  0.04880721 
Value is:  0.0302862 

Iteration:  2 
Value is:  0.01697086 
Value is:  0.01124081 
Value is:  0.0006622815 
Value is:  3.300996e-05 
Value is:  1.35731e-07 
Value is:  1.111701e-09 
Value is:  4.547474e-12 </code></pre>
<p>To make sure we really have found the minimum, we check <code>x</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"/>x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<pre><code>torch_tensor
 1.0000
 1.0000
[ CPUFloatType{2} ]</code></pre>
<p>Can this still be improved upon?</p>
</section>
<section id="optim_lbfgs-with-line-search" class="level3" data-number="10.2.2">
<h3 data-number="10.2.2" class="anchored" data-anchor-id="optim_lbfgs-with-line-search"><span class="header-section-number">10.2.2</span> <code>optim_lbfgs()</code> with line search</h3>
<p>Let’s see. Below, the only line that’s changed is the one where we construct the optimizer.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"/>num_iterations <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"/></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"/>x <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(<span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>), <span class="at">requires_grad =</span> <span class="cn">TRUE</span>)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"/></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"/>optimizer <span class="ot">&lt;-</span> <span class="fu">optim_lbfgs</span>(x, <span class="at">line_search_fn =</span> <span class="st">"strong_wolfe"</span>)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"/></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"/>calc_loss <span class="ot">&lt;-</span> <span class="cf">function</span>() {</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"/>  optimizer<span class="sc">$</span><span class="fu">zero_grad</span>()</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"/></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"/>  value <span class="ot">&lt;-</span> <span class="fu">rosenbrock</span>(x)</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"/>  <span class="fu">cat</span>(<span class="st">"Value is: "</span>, <span class="fu">as.numeric</span>(value), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"/></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"/>  value<span class="sc">$</span><span class="fu">backward</span>()</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"/>  value</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"/>}</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"/></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"/><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>num_iterations) {</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"/>  <span class="fu">cat</span>(<span class="st">"</span><span class="sc">\n</span><span class="st">Iteration: "</span>, i, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"/>  optimizer<span class="sc">$</span><span class="fu">step</span>(calc_loss)</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"/>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<pre><code>Iteration:  1 
Value is:  4 
Value is:  6 
Value is:  3.802412 
Value is:  3.680712 
Value is:  2.883048 
Value is:  2.5165 
Value is:  2.064779 
Value is:  1.38384 
Value is:  1.073063 
Value is:  0.8844351 
Value is:  0.5554555 
Value is:  0.2501077 
Value is:  0.8948895 
Value is:  0.1619074 
Value is:  0.06823064 
Value is:  0.01653575 
Value is:  0.004060207 
Value is:  0.00353789 
Value is:  0.000391416 
Value is:  4.303527e-06 
Value is:  2.036851e-08 
Value is:  6.870948e-12 

Iteration:  2 
Value is:  6.870948e-12 </code></pre>
<p>With line search, a single iteration is sufficient to reach the minimum. Inspecting the individual losses, we also see that the algorithm reduces the loss nearly every time it probes the function, which without line search, had not been the case.</p>


</section>
</section>

    
</body>
</html>