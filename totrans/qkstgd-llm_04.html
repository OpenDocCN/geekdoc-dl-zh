<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><h2 class="h2" id="ch03"><span class="ash">3</span></h2>
<h2 class="h2a">First Steps with Prompt Engineering</h2>
<h3 class="h3" id="ch03lev1sec1">Introduction</h3>
<p>In our previous chapter, we built a semantic search system that leveraged the power of Large Language Models (LLMs) to find relevant documents based on natural language queries. The system was able to understand the meaning behind the queries and retrieve accurate results, thanks to the pre-training of the LLMs on vast amounts of text.</p>
<p>However, building an effective LLM-based application can require more than just plugging in a pre-trained model and feeding it data and we might want to lean on the learnings of massively large language models to help complete the loop. This is where prompt engineering begins to come into the picture.</p>
<h3 class="h3" id="ch03lev1sec2">Prompt Engineering</h3>
<p><strong>Prompt engineering</strong> involves crafting prompts that effectively communicate the task at hand to the LLM, leading to accurate and useful outputs (<a href="ch03.html#ch03fig01">Figure 3.1</a>). It is a skill that requires an understanding of the nuances of language, the specific domain being worked on, and the capabilities and limitations of the LLM being used.</p>
<div class="group">
<div class="image" id="ch03fig01"><img src="graphics/03fig01.jpg" alt="Images" width="655" height="781"/></div>
<p class="fig-caption"><strong>Figure 3.1</strong> <em>Prompt engineering is how we construct inputs to LLMs to get a desired output.</em></p>
</div>
<p>In this chapter, we will delve deeper into the art of prompt engineering, exploring techniques and best practices for crafting effective prompts that lead to accurate and relevant outputs. We will cover topics such as structuring prompts for different types of tasks, fine-tuning models for specific domains, and evaluating the quality of LLM outputs.</p>
<p>By the end of this chapter, you will have the skills and knowledge needed to create powerful LLM-based applications that leverage the full potential of these cutting-edge models.</p>
<h4 class="h4" id="ch03lev2sec1">Alignment in Language Models</h4>
<p><strong>Alignment</strong> in language models refers to how well the model can understand and respond to input prompts that are in line with what the user expected. In standard language modeling, a model is trained to predict the next word or sequence of words based on the context of the preceding words. However, this approach does not allow for specific instructions or prompts to be given to the model, which can limit its usefulness for certain applications.</p>
<p>Prompt engineering can be challenging if the language model has not been aligned with the prompts, as it may generate irrelevant or incorrect responses. However, some language models have been developed with extra alignment features, such as Constitutional AI-driven Reinforcement Learning from AI Feedback (RLAIF) from Anthropic or Reinforcement Learning with Human Feedback (RLHF) in OpenAI’s GPT series, which can incorporate explicit instructions and feedback into the model’s training. These alignment techniques can improve the model’s ability to understand and respond to specific prompts, making them more useful for applications such as question-answering or language translation (<a href="ch03.html#ch03fig02">Figure 3.2</a>).</p>
<div class="group">
<div class="image" id="ch03fig02"><img src="graphics/03fig02.jpg" alt="Images" width="756" height="511"/></div>
<p class="fig-caption"><strong>Figure 3.2</strong> <em>Even modern LLMs like GPT-3 need alignment to behave how we want them to. The original GPT-3 model released in 2020 is a pure auto-regressive language model and tries to “complete the thought” and gives me some misinformation pretty freely. In January 2022, GPT-3’s first aligned version was released (InstructGPT) and was able to answer questions in a more succinct and accurate manner.</em></p>
</div>
<p>This chapter will focus on language models that have been specifically designed and trained to be aligned with instructional prompts. These models have been developed with the goal of improving their ability to understand and respond to specific instructions or tasks. These include models like GPT-3, ChatGPT (closed-source models from OpenAI), FLAN-T5 (an open-source model from Google), and Cohere’s command series (closed-source), which have been trained using large amounts of data and techniques such as transfer learning and fine-tuning to be more effective at generating responses to instructional prompts. Through this exploration, we will see the beginnings of fully working NLP products and features that utilize these models, and gain a deeper understanding of how to leverage aligned language models’ full capabilities.</p>
<h4 class="h4" id="ch03lev2sec2">Just Ask</h4>
<p>The first and most important rule of prompt engineering for instruction aligned language models is to be clear and direct in what you are asking for. When we give an LLM a task to complete, we want to make sure that we are communicating that task as clearly as possible. This is especially true for simple tasks that are straightforward for the LLM to accomplish.</p>
<p>In the case of asking GPT-3 to correct the grammar of a sentence, a direct instruction of “Correct the grammar of this sentence” is all you need to get a clear and accurate response. The prompt should also clearly indicate the phrase to be corrected (<a href="ch03.html#ch03fig03">Figure 3.3</a>).</p>
<div class="group">
<div class="image" id="ch03fig03"><img src="graphics/03fig03.jpg" alt="Images" width="715" height="212"/></div>
<p class="fig-caption"><strong>Figure 3.3</strong> <em>The best way to get started with an LLM aligned to answer queries from humans is to simply ask.</em></p>
</div>
<div class="note">
<p class="note-title"><strong>Note</strong></p>
<p class="note-para">Many figures are screenshots of an LLM’s playground. Experimenting with prompt formats in the playground or via an online interface can help identify effective approaches, which can then be tested more rigorously using larger data batches and the code/API for optimal output.</p>
</div>
<p>To be even more confident in the LLM’s response, we can provide a clear indication of the input and output for the task by adding prefixes. Let’s take another simple example asking GPT-3 to translate a sentence from English to Turkish.</p>
<p>A simple “just ask” prompt will consist of three elements:</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> A direct instruction: “Translate from English to Turkish.” which belongs at the top of the prompt so the LLM can pay attention to it (pun intended) while reading the input, which is next</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> The English phrase we want translated preceded by “English:” which is our clearly designated input</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> A space designated for the LLM to answer to give it’s answer which we will add the intentionally similar prefix “Turkish:”</p>
<p>These three elements are all part of a direct set of instructions with an organized answer area. By giving GPT-3 this clearly constructed prompt, it will be able to recognize the task being asked of it and fill in the answer correctly (<a href="ch03.html#ch03fig04">Figure 3.4</a>).</p>
<div class="group">
<div class="image" id="ch03fig04"><img src="graphics/03fig04.jpg" alt="Images" width="762" height="186"/></div>
<p class="fig-caption"><strong>Figure 3.4</strong> <em>This more fleshed out version of our just ask prompt has three components: a clear and concise set of instructions, our input prefixed by an explanatory label and a prefix for our output followed by a colon and no further whitespace.</em></p>
</div>
<p>We can expand on this even further by asking the GPT-3 to output multiple options for our corrected grammar by asking GPT-3 to give results back as a numbered list (<a href="ch03.html#ch03fig05">Figure 3.5</a>).</p>
<div class="group">
<div class="image" id="ch03fig05"><img src="graphics/03fig05.jpg" alt="Images" width="725" height="354"/></div>
<p class="fig-caption"><strong>Figure 3.5</strong> <em>Part of giving clear and direct instructions is telling the LLM how to structure the output. In this example, we ask GPT-3 to give grammatically correct versions as a numbered list.</em></p>
</div>
<p>Therefore, when it comes to prompt engineering, the rule of thumb is simple: when in doubt, just ask. Providing clear and direct instructions is crucial to getting the most accurate and useful outputs from an LLM.</p>
<h4 class="h4" id="ch03lev2sec3">Few-shot Learning</h4>
<p>When it comes to more complex tasks that require a deeper understanding of language, giving an LLM a few examples can go a long way in helping an LLM produce accurate and consistent outputs. Few-shot learning is a powerful technique that involves providing an LLM with a few examples of a task to help it understand the context and nuances of the problem.</p>
<p>Few-shot learning has been a pretty major focus of research in the field of LLMs. The creators of GPT-3 even recognized the potential of this technique, which is evident from the fact that the original GPT-3 research paper was titled “Language Models are Few-Shot Learners”.</p>
<p>Few-shot learning is particularly useful for tasks that require a certain tone, syntax, or style, and for fields where the language used is specific to a particular domain. <a href="ch03.html#ch03fig06">Figure 3.6</a> shows an example of asking GPT-3 to classify a review as being subjective or not. Basically this is a binary classification task.</p>
<div class="group">
<div class="image" id="ch03fig06"><img src="graphics/03fig06.jpg" alt="Images" width="755" height="558"/></div>
<p class="fig-caption"><strong>Figure 3.6</strong> <em>A simple binary classification for whether a given review is subjective or not. The top two examples show how LLMs can intuit a task’s answer from only a few examples where the bottom two examples show the same prompt structure without any examples (referred to as “zero-shot”) and cannot seem to answer how we want it to.</em></p>
</div>
<p>In the following figure, we can see that the few-shot examples are more likely to produce expected results because the LLM can look back at some examples to intuit from.</p>
<p>Few-shot learning opens up new possibilities for how we can interact with LLMs. With this technique, we can provide an LLM with an understanding of a task without explicitly providing instructions, making it more intuitive and user-friendly. This breakthrough capability has paved the way for the development of a wide range of LLM-based applications, from chatbots to language translation tools.</p>
<h4 class="h4" id="ch03lev2sec4">Output Structuring</h4>
<p>LLMs can generate text in a variety of formats, sometimes with too much variety. It can be helpful to structure the output in a specific way to make it easier to work with and integrate into other systems. We’ve actually seen this previously in this chapter when we asked GPT-3 to give us an answer in a numbered list. We can also make an LLM give back structured data formats like JSON (JavaScript Object Notation) as the output <a href="ch03.html#ch03fig07">Figure 3.7</a>).</p>
<div class="group">
<div class="image" id="ch03fig07"><img src="graphics/03fig07.jpg" alt="Images" width="716" height="332"/></div>
<p class="fig-caption"><strong>Figure 3.7</strong> <em>Simply asking GPT-3 to give a response back as a JSON (top) does give back a valid JSON but the keys are also in Turkish which may not be what we want. We can be more specific in our instruction by giving a one-shot example (bottom) which makes the LLM output the translation in the exact JSON format we requested.</em></p>
</div>
<p>By structuring LLM output in structured formats, developers can more easily extract specific information and pass it on to other services. Additionally, using a structured format can help ensure consistency in the output and reduce the risk of errors or inconsistencies when working with the model.</p>
<h4 class="h4" id="ch03lev2sec5">Prompting Personas</h4>
<p>Specific word choices in our prompts can greatly influence the output of the model. Even small changes to the prompt can lead to vastly different results. For example, adding or removing a single word can cause the LLM to shift its focus or change its interpretation of the task. In some cases, this may result in incorrect or irrelevant responses, while in other cases, it may produce the exact output desired.</p>
<p>To account for these variations, researchers and practitioners often create different “personas” for the LLM, representing different styles or voices that the model can adopt depending on the prompt. These personas can be based on specific topics, genres, or even fictional characters, and are designed to elicit specific types of responses from the LLM (<a href="ch03.html#ch03fig08">Figure 3.8</a>).</p>
<div class="group">
<div class="image" id="ch03fig08"><img src="graphics/03fig08.jpg" alt="Images" width="708" height="596"/></div>
<p class="fig-caption"><strong>Figure 3.8</strong> <em>Starting from the top left and moving down we see a baseline prompt of asking GPT-3 to respond as a store attendant. We can inject some more personality by asking it to respond in an “excitable” way or even as a pirate! We can also abuse this system by asking the LLM to respond in a rude manner or even horribly as an anti-Semite. Any developer who wants to use an LLM should be aware that these kinds of outputs are possible, whether intentional or not. We will talk about advanced output validation techniques in a future chapter that can help mitigate this behavior.</em></p>
</div>
<p>By taking advantage of personas, LLM developers can better control the output of the model and end-users of the system can get a more unique and tailored experience.</p>
<p>Personas may not always be used for positive purposes. Just like any tool or technology, some people may use LLMs to evoke harmful messages like if we asked an LLM to imitate an anti-Semite like in the last figure. By feeding the LLMs with prompts that promote hate speech or other harmful content, individuals can generate text that perpetuates harmful ideas and reinforces negative stereotypes. Creators of LLMs tend to take steps to mitigate this potential misuse, such as implementing content filters and working with human moderators to review the output of the model. Individuals who want to use LLMs must also be responsible and ethical when using LLMs and consider the potential impact of their actions (or the actions the LLM take on their behalf) on others.</p>
<h3 class="h3" id="ch03lev1sec3">Working with Prompts Across Models</h3>
<p>Prompts are highly dependent on the architecture and training of the language model, meaning that what works for one model may not work for another. For example, ChatGPT, GPT-3 (which is different from ChatGPT), T5, and models in the Cohere command series all have different underlying architectures, pre-training data sources, and training approaches, which all impact the effectiveness of prompts when working with them. While some prompts may transfer between models, others may need to be adapted or re-engineered to work with a specific model.</p>
<p>In this section, we will explore how to work with prompts across models, taking into account the unique features and limitations of each model to develop effective prompts that can guide language models to generate the desired output.</p>
<h4 class="h4" id="ch03lev2sec6">ChatGPT</h4>
<p>Some LLMs can take in more than just a single “prompt”. Models that are aligned to conversational dialogue like ChatGPT can take in a <strong>system prompt</strong> and multiple “user” and “assistant” prompts (<a href="ch03.html#ch03fig08a">Figure 3.8</a>). The system prompt is meant to be a general directive for the conversation and will generally include overarching rules and personas to follow. The user and assistant prompts are messages between the user and the LLM respectively. For any LLM you choose to look at, be sure to check out their documentation for specifics on how to structure input prompts.</p>
<div class="group">
<div class="image" id="ch03fig08a"><img src="graphics/03fig08a.jpg" alt="Images" width="508" height="518"/></div>
<p class="fig-caption"><strong>Figure 3.8</strong> <em>ChatGPT takes in an overall system prompt as well as any number of user and assistant prompts that simulate an ongoing conversation.</em></p>
</div>
<h4 class="h4" id="ch03lev2sec7">Cohere</h4>
<p>We’ve already seen Cohere’s command series of models in action previously in this chapter but as an alternative to OpenAI, it’s a good time to show that prompts cannot always be simply ported over from one model to another. Usually we need to alter the prompt slightly to allow another LLM to do its work.</p>
<p>Let’s return to our simple translation example. Let’s ask OpenAI and Cohere to translate something from English to Turkish (<a href="ch03.html#ch03fig10">Figure 3.10</a>).</p>
<div class="group">
<div class="image" id="ch03fig10"><img src="graphics/03fig10.jpg" alt="Images" width="744" height="443"/></div>
<p class="fig-caption"><strong>Figure 3.10</strong> <em>OpenAI’s GPT-3 can take a translation instruction without much hand-holding whereas the cohere model seems to require a bit more structure.</em></p>
</div>
<p>It seems that the Cohere model I chose required a bit more structuring than the OpenAI version. That doesn’t mean that the Cohere is worse than GPT-3, it just means that we need to think about how our prompt is structured for a given LLM.</p>
<h4 class="h4" id="ch03lev2sec8">Open-Source Prompt Engineering</h4>
<p>It wouldn’t be fair to talk about prompt engineering and not talk about open-source models like GPT-J and FLAN-T5. When working with them, prompt engineering is a critical step to get the most out of their pre-training and fine-tuning which we will start to cover in the next chapter. These models can generate high-quality text output just like their closed-source counterparts but unlike closed-source models like GPT and Cohere, open-source models offer greater flexibility and control over prompt engineering, enabling developers to customize prompts and tailor output to specific use cases during fine-tuning.</p>
<p>For example, a developer working on a medical chatbot may want to create prompts that focus on medical terminology and concepts, while a developer working on a language translation model may want to create prompts that emphasize grammar and syntax. With open-source models, developers have the flexibility to fine-tune prompts to their specific use cases, resulting in more accurate and relevant text output.</p>
<p>Another advantage of prompt engineering in open-source models is collaboration with other developers and researchers. Open-source models have a large and active community of users and contributors, which allows developers to share their prompt engineering strategies, receive feedback, and collaborate on improving the overall performance of the model. This collaborative approach to prompt engineering can lead to faster progress and more significant breakthroughs in natural language processing research.</p>
<p>It pays to remember how open-source models were pre-trained and fine-tuned (if they were at all). For example, GPT-J is simply an auto-regressive language model, so we’d expect things like few shot prompting to work better than simply asking a direct instructional promp,t whereas FLAN-T5 was specifically fine-tuned with instructional prompting in mind so while few-shots will still be on the table, we can also rely on the simplicity of just asking (<a href="ch03.html#ch03fig011">Figure 3.11</a>).</p>
<div class="group">
<div class="image" id="ch03fig011"><img src="graphics/03fig11.jpg" alt="Images" width="762" height="412"/></div>
<p class="fig-caption"><strong>Figure 3.11</strong> <em>Open source models can vary drastically in how they were trained and how they expect prompts. Models like GPT-J which is not instruction aligned has a hard time answering a direct instruction (bottom left) whereas FLAN-T5 which was aligned to instructions does know how to accept instructions (bottom right). Both models are able to intuit from few-shot learning but FLAN-T5 seems to be having trouble with our subjective task. Perhaps a great candidate for some fine-tuning! Coming soon to a chapter near you.</em></p>
</div>
<h3 class="h3" id="ch03lev1sec4">Building a Q/A bot with ChatGPT</h3>
<p>Let’s build a very simple Q/A bot using ChatGPT and the semantic retrieval system we built in the last chapter. Recall that one of our API endpoints is used to retrieve documents from our BoolQ dataset given a natural query.</p>
<div class="note">
<p class="note-title"><strong>Note</strong></p>
<p class="note-para">Both ChatGPT (GPT 3.5) and GPT-4 are conversational LLMs and take in the same kind of system prompt as well as user prompts assistant prompts. When I say we are using ChatGPT, we could be using either GPT 3.5 or GPT-4. Our repository uses the most up to date model (which at the time of writing is GPT-4).</p>
</div>
<p>All we need to do to get off the ground is:</p>
<p class="numbera">1. Design a system prompt for ChatGPT</p>
<p class="numbera">2. Search for context in our knowledge with every new user message</p>
<p class="numbera">3. Inject any context we find from our DB directly into ChatGPT’s system prompt</p>
<p class="numbera">4. Let ChatGPT do its job and answer the question</p>
<p><a href="ch03.html#ch03fig011">Figure 3.12</a> outlines these high level steps:</p>
<div class="group">
<div class="image" id="ch03fig012"><img src="graphics/03fig12.jpg" alt="Images" width="716" height="337"/></div>
<p class="fig-caption"><strong>Figure 3.12</strong> <em>A 10,000 foot view of our chatbot that uses ChatGPT to provide a conversational interface in front of our semantic search API.</em></p>
</div>
<p>To dig into it one step deeper, <a href="ch03.html#ch03fig013">Figure 3.13</a> shows how this will work at the prompt level, step by step:</p>
<div class="group">
<div class="image" id="ch03fig013"><img src="graphics/03fig13.jpg" alt="Images" width="765" height="549"/></div>
<p class="fig-caption"><strong>Figure 3.13</strong> <em>Starting from the top left and reading left to right, these four states represent how our bot is architected. Every time a user says something that surfaces a confident document from our knowledge base, that document is inserted directly into the system prompt where we tell ChatGPT to only use documents from our knowledge base.</em></p>
</div>
<p>Let’s wrap all of this logic into a Python class that will have a skeleton like in <a href="ch03.html#list3_1">Listing 3.1</a>.</p>
<p class="ex-caption" id="list3_1"><strong>Listing 3.1 <em>A ChatGPT Q/A bot</em></strong></p>
<div class="pre-box">
<pre><code># Define a system prompt that gives the bot context throughout the conversation and will be amended with content from our knowledge base.
SYSTEM_PROMPT = '''
You are a helpful Q/A bot that can only reference material from a knowledge base.
All context was pulled from a knowledge base.
If a user asks anything that is not "from the knowledge base", say that you cannot answer.
'''

# Define the ChatbotGPT class
class ChatbotGPT():
    
    # Define the constructor method for the class
    def __init__(self, system_prompt, threshold=.8):
        # Initialize the conversation list with the system prompt as the first turn
        # Set a threshold for the similarity score between the user's input and the knowledge base
        pass
        
    # Define a method to display the conversation in a readable format
    def display_conversation(self):
        # Iterate through each turn in the conversation
        # Get the role and content of the turn
        # Print out the role and content in a readable format
        pass
                
    # Define a method to handle the user's input
    def user_turn(self, message):
        # Add the user's input as a turn in the conversation
        # Get the best matching result from the knowledge base using Pinecone
        # Check if the confidence score between the user's input and the document meets the threshold
        # Add the context from the knowledge base to the system prompt if we meet the threshold
        # Generate a response from the ChatGPT model using OpenAI's API
        # Add the GPT-3.5 response as a turn in the conversation
        # Return the assistant's response
        pass</code></pre>
</div>
<p>A full implementation of this code using GPT-4 is in the book’s repository and <a href="ch03.html#ch03fig014">Figure 3.14</a> presents a sample conversation we can have with it.</p>
<div class="group">
<div class="image" id="ch03fig014"><img src="graphics/03fig14.jpg" alt="Images" width="731" height="391"/></div>
<p class="fig-caption"><strong>Figure 3.14</strong> <em>Asking our bot about information from the BoolQ dataset yields cohesive and conversational answers whereas when I ask about Barack Obama’s age (which is information not present in the knowledge base) the AI politely declines to answer even though that is general knowledge it would try to use otherwise.</em></p>
</div>
<p>As a part of testing, I decided to try something out of the box and built a new namespace in the same vector database (Thank you, Pinecone) and I chunked documents out of a PDF of a Star Wars-themed card game I like. I wanted to use the chatbot to ask basic questions about the game and let ChatGPT retrieve portions of the manual to answer my questions. <a href="ch03.html#ch03fig015">Figure 3.15</a> was the result!</p>
<div class="group">
<div class="image" id="ch03fig015"><img src="graphics/03fig15.jpg" alt="Images" width="734" height="337"/></div>
<p class="fig-caption"><strong>Figure 3.15</strong> <em>The same architecture and system prompt against a new knowledge base of a card game manual. Now I can ask questions in the manual but my questions from BoolQ are no longer in scope.</em></p>
</div>
<p>Not bad at all if I may say so.</p>
<h3 class="h3" id="ch03lev1sec5">Summary</h3>
<p>Prompt engineering, the process of designing and optimizing prompts to improve the performance of language models can be fun, iterative, and sometimes tricky! We saw many tips and tricks on how to get started such as understanding alignment, just asking, few-shot learning, output structuring, prompting personas, and working with prompts across models. We also built our own chatbot using ChatGPT’s prompt interface that was able to tie into the API we built in the last chapter.</p>
<p>There is a strong correlation between proficient prompt engineering and effective writing. A well-crafted prompt provides the model with clear instructions, resulting in an output that closely aligns with the desired response. When a human can comprehend and create the expected output from a given prompt, it is indicative of a well-structured and useful prompt for the LLM. However, if a prompt allows for multiple responses or is in general vague, then it is likely too ambiguous for an LLM. This parallel between prompt engineering and writing highlights that the art of writing effective prompts is more like crafting data annotation guidelines or engaging in skillful writing than it is similar to traditional engineering practices.</p>
<p>Prompt engineering is an important process for improving the performance of language models. By designing and optimizing prompts, language models can better understand and respond to user inputs. In a later chapter, we will revisit prompt engineering with some more advanced topics like LLM output validation, chain of thought prompting to force an LLM to think out loud, and chaining multiple prompts together into larger workflows.</p>
</div>
</div>
</body></html>