<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>6  A neural network from scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>6  A neural network from scratch</h1>
<blockquote>原文：<a href="https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/network_1.html">https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/network_1.html</a></blockquote>

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">

</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>In this chapter, we are going to solve a regression task. But wait – not the <code>lm</code> way. We’ll be building a real neural network, making use of tensors only (<code>autograd</code>-enabled ones, it goes without saying). Of course, this is not how you’ll be using <code>torch</code>, later; but this does not make it a useless endeavor. On the contrary. Having seen the raw mechanics, you’ll be able to appreciate even more the hard work that <code>torch</code> saves you. What’s more, understanding the basics will be an efficient antidote against the surprisingly common temptation to think of deep learning as some kind of “magic”. It’s all just matrix computations; one has to learn how to orchestrate them though.</p>
<p>Let’s start with what we need for a network that can perform regression.</p>
<section id="idea" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="idea"><span class="header-section-number">6.1</span> Idea</h2>
<p>In a nutshell, a network is a <em>function</em> from inputs to outputs. A suitable function, thus, is what we’re looking for.</p>
<p>To find it, let’s first think of regression as <em>linear</em> regression. What linear regression does is multiply and add. For each independent variable, there is a <em>coefficient</em> that multiplies it. On top of that, there is a so-called <em>bias</em> term that gets added at the end. (In two dimensions, regression coefficient and bias correspond to slope and x-intercept of the regression line.)</p>
<p>Thinking about it, multiplication and addition are things we can do with tensors – one could even say they are made for exactly that. Let’s take an example where the input data consist of a hundred observations, with three features each. For example:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"/><span class="fu">library</span>(torch)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"/></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"/>x <span class="ot">&lt;-</span> <span class="fu">torch_randn</span>(<span class="dv">100</span>, <span class="dv">3</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"/>x<span class="sc">$</span><span class="fu">size</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<pre><code>[1] 100   3</code></pre>
<p>To store the per-feature coefficients that should multiply <code>x</code>, we need a column vector of length 3, the number of features. Alternatively, preparing for a modification we’re going to make very soon, this can be a matrix whose columns are of length three, that is, a matrix with three rows. How many columns should it have? Let’s say we want to predict a single output feature. In that case, the matrix should be of size 3 x 1.</p>
<p>Here comes a suitable candidate, initialized randomly. Note how the tensor is created with <code>requires_grad = TRUE</code>, as it represents a parameter we’ll want the network to <em>learn</em>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"/>w <span class="ot">&lt;-</span> <span class="fu">torch_randn</span>(<span class="dv">3</span>, <span class="dv">1</span>, <span class="at">requires_grad =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>The bias tensor then has to be of size 1 x 1:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"/>b <span class="ot">&lt;-</span> <span class="fu">torch_zeros</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="at">requires_grad =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>Now, we can get a “prediction” by multiplying the data with the weight matrix <code>w</code> and adding the bias <code>b</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"/>y <span class="ot">&lt;-</span> x<span class="sc">$</span><span class="fu">matmul</span>(w) <span class="sc">+</span> b</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"/><span class="fu">print</span>(y, <span class="at">n =</span> <span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<pre><code>torch_tensor
-2.1600
-3.3244
 0.6046
 0.4472
-0.4971
-0.0530
 5.1259
-1.1595
-0.5960
-1.4584
... [the output was truncated (use n=-1 to disable)]
[ CPUFloatType{100,1} ][ grad_fn = &lt;AddBackward0&gt; ]</code></pre>
<p>In math notation, what we’ve done here is implement the function:</p>
<p><span class="math display">\[
f(\mathbf{X}) = \mathbf{X}\mathbf{W} + \mathbf{b}
\]</span></p>
<p>How does this relate to neural networks?</p>
</section>
<section id="layers" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="layers"><span class="header-section-number">6.2</span> Layers</h2>
<p>Circling back to neural-network terminology, what we’ve done here is prototype the action of a network that has a <em>single</em> <em>layer</em>: the output layer. However, a single-layer network is hardly the type you’d be interested in building – why would you, when you could simply do linear regression instead? In fact, one of the defining features of neural networks is their ability to chain an unlimited (in theory) number of layers. Of these, all but the output layer may be referred to as “hidden” layers, although from the point of view of someone who uses a deep learning framework such as <code>torch</code>, they are not that <em>hidden</em> after all.</p>
<p>Let’s say we want our network to have one hidden layer. Its size, meaning, the number of <em>units</em> it has, will be an important factor in determining the network’s power. This number is reflected in the weight matrix we create: A layer with eight units will need a weight matrix with eight columns.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"/>w1 <span class="ot">&lt;-</span> <span class="fu">torch_randn</span>(<span class="dv">3</span>, <span class="dv">8</span>, <span class="at">requires_grad =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>Each unit has its own value for bias, too.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"/>b1 <span class="ot">&lt;-</span> <span class="fu">torch_zeros</span>(<span class="dv">1</span>, <span class="dv">8</span>, <span class="at">requires_grad =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>Just like we saw before, the hidden layer will multiply the input it receives by the weights and add the bias. That is, it applies the function <span class="math inline">\(f\)</span> displayed above. Then, another function is applied. This function receives its input from the hidden layer and produces the final output. In a nutshell, what is happening here is function composition: Calling the second function <span class="math inline">\(g\)</span>, the overall transformation is <span class="math inline">\(g(f(\mathbf{X})\)</span>, or <span class="math inline">\(g \circ f\)</span>.</p>
<p>For <span class="math inline">\(g\)</span> to yield an output analogous to the single-layer architecture above, its weight matrix has to take the eight-column hidden layer to a single column. That is, <code>w2</code> looks like this:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"/>w2 <span class="ot">&lt;-</span> <span class="fu">torch_randn</span>(<span class="dv">8</span>, <span class="dv">1</span>, <span class="at">requires_grad =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>The bias, <code>b2</code>, is a single value, like <code>b1</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"/>b2 <span class="ot">&lt;-</span> <span class="fu">torch_randn</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="at">requires_grad =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>Of course, there is no reason to stop at <em>one</em> hidden layer, and once we’ve built up the complete apparatus, please feel invited to experiment with the code. But first, we need to add in a few other types of components. For one, with our most recent architecture, what we’re doing is chain, or compose, functions – which is good. But all these functions are doing is add and multiply, implying that they are linear. The power of neural networks, however, is usually associated with <em>nonlinearity</em>. Why?</p>
</section>
<section id="activation-functions" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="activation-functions"><span class="header-section-number">6.3</span> Activation functions</h2>
<p>Imagine, for a moment, that we had a network with three layers, and all each layer did was multiply its input by its weight matrix. (Having a bias term doesn’t really change anything. But it makes the example more complex, so we’re “abstracting it out”.)</p>
<p>This gives us a chain of matrix multiplications: <span class="math inline">\(f(\mathbf{X}) = ((\mathbf{X} \mathbf{W}_1)\mathbf{W}_2)\mathbf{W}_3\)</span>. Now, this can be rearranged so that all the weight matrices are multiplied together before application to <span class="math inline">\(\mathbf{X}\)</span>: <span class="math inline">\(f(\mathbf{X}) = \mathbf{X} (\mathbf{W}_1\mathbf{W}_2\mathbf{W}_3)\)</span>. Thus, this three-layer network can be simplified to a single-layer one, where <span class="math inline">\(f(\mathbf{X}) = \mathbf{X} \mathbf{W}_4\)</span>. And now, we have lost all advantages associated with deep neural networks.</p>
<p>This is where activation functions, sometimes called “nonlinearities”, come in. They introduce non-linear operations that cannot be modeled by matrix multiplication. Historically, the prototypical activation function has been the <em>sigmoid</em>, and it’s still extremely important today. Its constitutive action is to squish its input between zero and one, yielding a value that can be interpreted as a probability. But in regression, this is not usually what we want, and neither would it be for most hidden layers.</p>
<p>Instead, the most-used activation function inside a network is the so-called <em>ReLU</em>, or Rectified Linear Unit. This is a long name for something rather straightforward: All negative values are set to zero. In <code>torch</code>, this can be accomplished using the <code>relu()</code> function:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"/>t <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(<span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">5</span>, <span class="sc">-</span><span class="dv">7</span>))</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"/>t<span class="sc">$</span><span class="fu">relu</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<pre><code>torch_tensor
 0
 1
 5
 0
[ CPUFloatType{4} ]</code></pre>
<p>Why would this be nonlinear? One criterion for a linear function is that when you have two inputs, it doesn’t matter if you first add them and then, apply the transformation, or if you start by applying the transformation independently to both inputs and then, go ahead and add them. But with ReLU, this does not work:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"/>t1 <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>))</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"/>t2 <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="sc">-</span><span class="dv">2</span>, <span class="dv">3</span>))</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"/></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"/>t1<span class="sc">$</span><span class="fu">add</span>(t2)<span class="sc">$</span><span class="fu">relu</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<pre><code>torch_tensor
 2
 0
 6
[ CPUFloatType{3} ]</code></pre>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"/>t1_clamped <span class="ot">&lt;-</span> t1<span class="sc">$</span><span class="fu">relu</span>()</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"/>t2_clamped <span class="ot">&lt;-</span> t2<span class="sc">$</span><span class="fu">relu</span>()</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"/></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"/>t1_clamped<span class="sc">$</span><span class="fu">add</span>(t2_clamped)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<pre><code>torch_tensor
 2
 2
 6
[ CPUFloatType{3} ]</code></pre>
<p>The results are not the same.</p>
<p>Wrapping up so far, we’ve talked about how to code layers and activation functions. There is just one further concept to discuss before we can build the complete network. This is the loss function.</p>
</section>
<section id="loss-functions" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="loss-functions"><span class="header-section-number">6.4</span> Loss functions</h2>
<p>Put abstractly, the loss is a measure of how far away we are from our goal. When minimizing a function, like we did in the previous chapter, this is the difference between the current function value and the smallest value it can take. With neural networks, we are free to choose a suitable loss function as we like, provided it matches our task. For regression-type tasks, this often will be mean squared error (MSE), although it doesn’t have to be. For example, there could be reasons to use mean absolute error instead.</p>
<p>In <code>torch</code>, computation of mean squared error is a one-liner:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"/>y <span class="ot">&lt;-</span> <span class="fu">torch_randn</span>(<span class="dv">5</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"/>y_pred <span class="ot">&lt;-</span> y <span class="sc">+</span> <span class="fl">0.01</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"/></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"/>loss <span class="ot">&lt;-</span> (y_pred <span class="sc">-</span> y)<span class="sc">$</span><span class="fu">pow</span>(<span class="dv">2</span>)<span class="sc">$</span><span class="fu">mean</span>()</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"/></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"/>loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<pre><code>torch_tensor
9.99999e-05
[ CPUFloatType{} ]</code></pre>
<p>As soon as we have the loss, we’ll be able to update the weights, subtracting a fraction of its gradient. We’ve already seen how to do this in the last chapter, and will see it again shortly.</p>
<p>We now take the pieces discussed and put them together.</p>
</section>
<section id="implementation" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="implementation"><span class="header-section-number">6.5</span> Implementation</h2>
<p>We split this into three parts. This way, when later we refactor individual components to make use of higher-level <code>torch</code> functionality, it will be easier to see the areas where encapsulation and modularization are occurring.</p>
<section id="generate-random-data" class="level3" data-number="6.5.1">
<h3 data-number="6.5.1" class="anchored" data-anchor-id="generate-random-data"><span class="header-section-number">6.5.1</span> Generate random data</h3>
<p>Our example data consist of one hundred observations. The input, <code>x</code>, has three features; the target, <code>y</code>, just one. <code>y</code> is generated from <code>x</code>, but with some noise added.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"/><span class="fu">library</span>(torch)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"/></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"/><span class="co"># input dimensionality (number of input features)</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"/>d_in <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"/><span class="co"># number of observations in training set</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"/>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"/></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"/>x <span class="ot">&lt;-</span> <span class="fu">torch_randn</span>(n, d_in)</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"/>coefs <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.2</span>, <span class="sc">-</span><span class="fl">1.3</span>, <span class="sc">-</span><span class="fl">0.5</span>)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"/>y <span class="ot">&lt;-</span> x<span class="sc">$</span><span class="fu">matmul</span>(coefs)<span class="sc">$</span><span class="fu">unsqueeze</span>(<span class="dv">2</span>) <span class="sc">+</span> <span class="fu">torch_randn</span>(n, <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>Next, the network.</p>
</section>
<section id="build-the-network" class="level3" data-number="6.5.2">
<h3 data-number="6.5.2" class="anchored" data-anchor-id="build-the-network"><span class="header-section-number">6.5.2</span> Build the network</h3>
<p>The network has two layers: a hidden layer and the output layer. This means that we need two weight matrices and two bias tensors. For no special reason, the hidden layer here has thirty-two units:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"/><span class="co"># dimensionality of hidden layer</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"/>d_hidden <span class="ot">&lt;-</span> <span class="dv">32</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"/><span class="co"># output dimensionality (number of predicted features)</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"/>d_out <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"/></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"/><span class="co"># weights connecting input to hidden layer</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"/>w1 <span class="ot">&lt;-</span> <span class="fu">torch_randn</span>(d_in, d_hidden, <span class="at">requires_grad =</span> <span class="cn">TRUE</span>)</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"/><span class="co"># weights connecting hidden to output layer</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"/>w2 <span class="ot">&lt;-</span> <span class="fu">torch_randn</span>(d_hidden, d_out, <span class="at">requires_grad =</span> <span class="cn">TRUE</span>)</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"/></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"/><span class="co"># hidden layer bias</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"/>b1 <span class="ot">&lt;-</span> <span class="fu">torch_zeros</span>(<span class="dv">1</span>, d_hidden, <span class="at">requires_grad =</span> <span class="cn">TRUE</span>)</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"/><span class="co"># output layer bias</span></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"/>b2 <span class="ot">&lt;-</span> <span class="fu">torch_zeros</span>(<span class="dv">1</span>, d_out, <span class="at">requires_grad =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<p>With their current values – results of random initialization – those weights and biases won’t be of much use. Time to train the network.</p>
</section>
<section id="train-the-network" class="level3" data-number="6.5.3">
<h3 data-number="6.5.3" class="anchored" data-anchor-id="train-the-network"><span class="header-section-number">6.5.3</span> Train the network</h3>
<p>Training the network means passing the input through its layers, calculating the loss, and adjusting the parameters (weights and biases) in a way that predictions improve. These activities we keep repeating until performance seems sufficient (which, in real-life applications, would have to be defined very carefully). Technically, each repeated application of these steps is called an <em>epoch</em>.</p>
<p>Just like with function minimization, deciding on a suitable learning rate (the fraction of the gradient to subtract) needs some experimentation.</p>
<p>Looking at the below training loop, you see that, logically, it consists of four parts:</p>
<ul>
<li><p>do a forward pass, yielding the network’s predictions (if you dislike the one-liner, feel free to split it up);</p></li>
<li><p>compute the loss (this, too, being a one-liner – we merely added some logging);</p></li>
<li><p>have <em>autograd</em> calculate the gradient of the loss with respect to the parameters; and</p></li>
<li><p>update the parameters accordingly (again, taking care to wrap the whole action in <code>with_no_grad()</code>, and zeroing the <code>grad</code> fields on every iteration).</p></li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"/>learning_rate <span class="ot">&lt;-</span> <span class="fl">1e-4</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"/></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"/><span class="do">### training loop ----------------------------------------</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"/></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"/><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">200</span>) {</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"/>  </span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"/>  <span class="do">### -------- Forward pass --------</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"/>  </span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"/>  y_pred <span class="ot">&lt;-</span> x<span class="sc">$</span><span class="fu">mm</span>(w1)<span class="sc">$</span><span class="fu">add</span>(b1)<span class="sc">$</span><span class="fu">relu</span>()<span class="sc">$</span><span class="fu">mm</span>(w2)<span class="sc">$</span><span class="fu">add</span>(b2)</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"/>  </span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"/>  <span class="do">### -------- Compute loss -------- </span></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"/>  loss <span class="ot">&lt;-</span> (y_pred <span class="sc">-</span> y)<span class="sc">$</span><span class="fu">pow</span>(<span class="dv">2</span>)<span class="sc">$</span><span class="fu">mean</span>()</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"/>  <span class="cf">if</span> (t <span class="sc">%%</span> <span class="dv">10</span> <span class="sc">==</span> <span class="dv">0</span>)</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"/>    <span class="fu">cat</span>(<span class="st">"Epoch: "</span>, t, <span class="st">"   Loss: "</span>, loss<span class="sc">$</span><span class="fu">item</span>(), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"/>  </span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"/>  <span class="do">### -------- Backpropagation --------</span></span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"/>  </span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"/>  <span class="co"># compute gradient of loss w.r.t. all tensors with</span></span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"/>  <span class="co"># requires_grad = TRUE</span></span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"/>  loss<span class="sc">$</span><span class="fu">backward</span>()</span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"/>  </span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"/>  <span class="do">### -------- Update weights -------- </span></span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"/>  </span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"/>  <span class="co"># Wrap in with_no_grad() because this is a part we don't </span></span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"/>  <span class="co"># want to record for automatic gradient computation</span></span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"/>   <span class="fu">with_no_grad</span>({</span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"/>     w1 <span class="ot">&lt;-</span> w1<span class="sc">$</span><span class="fu">sub_</span>(learning_rate <span class="sc">*</span> w1<span class="sc">$</span>grad)</span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"/>     w2 <span class="ot">&lt;-</span> w2<span class="sc">$</span><span class="fu">sub_</span>(learning_rate <span class="sc">*</span> w2<span class="sc">$</span>grad)</span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"/>     b1 <span class="ot">&lt;-</span> b1<span class="sc">$</span><span class="fu">sub_</span>(learning_rate <span class="sc">*</span> b1<span class="sc">$</span>grad)</span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"/>     b2 <span class="ot">&lt;-</span> b2<span class="sc">$</span><span class="fu">sub_</span>(learning_rate <span class="sc">*</span> b2<span class="sc">$</span>grad)  </span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"/>     </span>
<span id="cb21-32"><a href="#cb21-32" aria-hidden="true" tabindex="-1"/>     <span class="co"># Zero gradients after every pass, as they'd</span></span>
<span id="cb21-33"><a href="#cb21-33" aria-hidden="true" tabindex="-1"/>     <span class="co"># accumulate otherwise</span></span>
<span id="cb21-34"><a href="#cb21-34" aria-hidden="true" tabindex="-1"/>     w1<span class="sc">$</span>grad<span class="sc">$</span><span class="fu">zero_</span>()</span>
<span id="cb21-35"><a href="#cb21-35" aria-hidden="true" tabindex="-1"/>     w2<span class="sc">$</span>grad<span class="sc">$</span><span class="fu">zero_</span>()</span>
<span id="cb21-36"><a href="#cb21-36" aria-hidden="true" tabindex="-1"/>     b1<span class="sc">$</span>grad<span class="sc">$</span><span class="fu">zero_</span>()</span>
<span id="cb21-37"><a href="#cb21-37" aria-hidden="true" tabindex="-1"/>     b2<span class="sc">$</span>grad<span class="sc">$</span><span class="fu">zero_</span>()  </span>
<span id="cb21-38"><a href="#cb21-38" aria-hidden="true" tabindex="-1"/>   })</span>
<span id="cb21-39"><a href="#cb21-39" aria-hidden="true" tabindex="-1"/></span>
<span id="cb21-40"><a href="#cb21-40" aria-hidden="true" tabindex="-1"/>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
</div>
<pre><code>Epoch: 10 Loss: 24.92771
Epoch: 20 Loss: 23.56143
Epoch: 30 Loss: 22.3069
Epoch: 40 Loss: 21.14102
Epoch: 50 Loss: 20.05027
Epoch: 60 Loss: 19.02925
Epoch: 70 Loss: 18.07328
Epoch: 80 Loss: 17.16819
Epoch: 90 Loss: 16.31367
Epoch: 100 Loss: 15.51261
Epoch: 110 Loss: 14.76012
Epoch: 120 Loss: 14.05348
Epoch: 130 Loss: 13.38944
Epoch: 140 Loss: 12.77219
Epoch: 150 Loss: 12.19302
Epoch: 160 Loss: 11.64823
Epoch: 170 Loss: 11.13535
Epoch: 180 Loss: 10.65219
Epoch: 190 Loss: 10.19666
Epoch: 200 Loss: 9.766989</code></pre>
<p>The loss decreases quickly at first, and then, not so rapidly anymore. But this example was not created to exhibit magnificent performance; the idea was to show how few lines of code are needed to build a “real” neural network.</p>
<p>Now, the layers, the loss, the parameter updates – all that is still pretty “raw”: It’s (literally) <em>just tensors</em>. For such a small network this works fine, but it would get cumbersome pretty fast for more complex designs. The following two chapters, thus, will show how to abstract away weights and biases into neural network <em>modules</em>, swap self-made loss functions with built-in ones, and get rid of the verbose parameter update routine.</p>


</section>
</section>

    
</body>
</html>