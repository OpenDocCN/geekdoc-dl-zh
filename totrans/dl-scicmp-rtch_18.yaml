- en: 13  Loading data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 13 加载数据
- en: 原文：[https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/data.html](https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/data.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/data.html](https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/data.html)
- en: Our toy example, which we’ll see a third (and last) version of in the next chapter,
    had the model train on a tiny set of data – small enough to pass all observations
    to the model in one go. What if that wasn’t the case? Say we had 10,000 items
    instead, and every item was an RGB image of size 256 x 256 pixels. Even on very
    powerful hardware, we could not possibly train a model on the complete data all
    at once.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一章看到我们的玩具示例的第三个（也是最后一个）版本，该模型在一个很小的数据集上训练——小到可以一次性将所有观测值传递给模型。如果情况不是这样呢？比如说我们有10,000个项目，每个项目都是一个256
    x 256像素大小的RGB图像。即使在非常强大的硬件上，我们也不可能一次性在完整的数据上训练模型。
- en: 'For that reason, deep-learning frameworks like `torch` include an input pipeline
    that lets you pass data to the model in *batches* – that is, subsets of observations.
    Involved in this process are two classes: `dataset()` and `dataloader()`. Before
    we look at how to construct instances of these, let’s characterize them by what
    they’re *for*.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，深度学习框架如 `torch` 包含一个输入管道，允许你以 *批处理* 的形式将数据传递给模型——也就是说，观测值的子集。在这个过程中涉及两个类：`dataset()`
    和 `dataloader()`。在我们查看如何构建这些实例之前，让我们通过它们的作用来描述它们。
- en: 13.1 Data vs. `dataset()` vs. `dataloader()` – what’s the difference?
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.1 数据 vs. `dataset()` vs. `dataloader()` – 有何区别？
- en: 'In this book, “dataset” (variable-width font, no parentheses), or just “the
    data”, usually refers to things like R matrices, `data.frame`s, and what’s contained
    therein. A `dataset()` (fixed-width font, parentheses), however, is a `torch`
    object that knows how to do one thing: *deliver to the caller a* *single item.*
    That item, usually, will be a list, consisting of one input and one target tensor.
    (It could be anything, though – whatever makes sense for the task. For example,
    it could be a single tensor, if input and target are the same. Or more than two
    tensors, in case different inputs should be passed to different modules.)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，“dataset”（变量宽度字体，无括号），或简称为“数据”，通常指的是像R矩阵、`data.frame`以及其中包含的内容。然而，`dataset()`（固定宽度字体，括号），是一个`torch`对象，它知道如何做一件事：*向调用者提供一个*
    *单个项目*。这个项目通常是一个列表，包含一个输入和一个目标张量。（它可以是任何东西，只要对任务有意义。例如，如果输入和目标是相同的，它可能是一个单独的张量。或者，如果需要将不同的输入传递给不同的模块，它可能包含超过两个张量。）
- en: As long as it fulfills the above-stated contract, a `dataset()` is free to do
    whatever needs to be done. It could, for example, download data from the internet,
    store them in some temporary location, do some pre-processing, and when asked,
    return bite-sized chunks of data in just the shape expected by a certain class
    of models. No matter what it does in the background, all its caller cares about
    is that it return a single item. Its caller, that’s the `dataloader()`.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 只要它满足上述声明的合同，`dataset()` 就可以自由地做任何需要做的事情。例如，它可以从互联网上下载数据，将它们存储在某个临时位置，进行一些预处理，并在需要时，以特定类模型期望的形状返回数据的小块。无论它在后台做什么，调用者所关心的只是它返回一个单独的项目。它的调用者，就是
    `dataloader()`。
- en: 'A `dataloader()`’s role is to feed input to the model in *batches*. One immediate
    reason is computer memory: Most `dataset()`s will be far too large to pass them
    to the model in one go. But there are additional benefits to batching. Since gradients
    are computed (and model weights updated) once per *batch*, there is an inherent
    stochasticity to the process, a stochasticity that helps with model training.
    We’ll talk more about that in an upcoming chapter.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '`dataloader()` 的作用是以 *批处理* 的形式向模型提供输入。一个直接的原因是计算机内存：大多数 `dataset()` 都会太大，无法一次性传递给模型。但是，批处理还有其他好处。由于梯度是在每个
    *批处理* 中计算（并且模型权重更新）的，因此这个过程具有固有的随机性，这种随机性有助于模型训练。我们将在下一章中更多地讨论这一点。'
- en: 13.2 Using `dataset()`s
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.2 使用 `dataset()`
- en: '`dataset()`s come in all flavors, from ready-to-use – and brought to you by
    some package, `torchvision` or `torchdatasets`, say, or any package that chooses
    to provide access to data in `torch`-ready form – to fully customized (made by
    you, that is). Creating `dataset()`s is straightforward, since they are R6 objects,
    and there’s just three methods to be implemented. These methods are:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '`dataset()` 有各种风味，从现成的——由某些包提供，比如 `torchvision` 或 `torchdatasets`，或者任何选择以 `torch`
    准备形式提供数据访问的包——到完全自定义（由你制作）。创建 `dataset()` 很简单，因为它们是 R6 对象，并且只需要实现三个方法。这些方法是：'
- en: '`initialize(...)`. Parameters to `initialize()` are passed when a `dataset()`
    is instantiated. Possibilities include, but are not limited to, references to
    R `data.frame`s, filesystem paths, download URLs, and any configurations and parameterizations
    expected by the `dataset()`.'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`initialize(...)`. 当实例化 `dataset()` 时传递给 `initialize()` 的参数。可能包括但不限于对 R `data.frame`
    的引用、文件系统路径、下载 URL 以及 `dataset()` 预期的任何配置和参数化。'
- en: '`.getitem(i)`. This is the method responsible for fulfilling the contract.
    Whatever it returns counts as a single item. The parameter, `i`, is an index that,
    in many cases, will be used to determine the starting position in the underlying
    data structure (a `data.frame` of file system paths, for example). However, the
    `dataset()` is not *obliged* to actually make use of that parameter. With extremely
    huge `dataset()`s, for example, or given serious class imbalance, it could instead
    decide to return items based on *sampling*.'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`.getitem(i)`. 这是负责履行合同的方法。它返回的任何内容都算作一个单独的项目。参数 `i` 是一个索引，在许多情况下，它将被用来确定底层数据结构（例如文件系统路径的
    `data.frame`）的起始位置。然而，`dataset()` 并不 *必须* 实际使用该参数。例如，对于极其巨大的 `dataset()` 或严重类别不平衡的情况，它可以选择基于
    *抽样* 返回项目。'
- en: '`.length()`. This, usually, is a one-liner, its only purpose being to inform
    about the number of available items in a `dataset()`.'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`.length()`。这通常是一行代码，它的唯一目的是告知 `dataset()` 中可用的项目数量。'
- en: 'Here is a blueprint for creating a `dataset()`:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是创建 `dataset()` 的蓝图：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*That said, let’s compare three ways of obtaining a `dataset()` to work with,
    from tailor-made to maximally effortless.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*话虽如此，让我们比较三种获取 `dataset()` 以便使用的方法，从量身定制到最大程度上的省力。'
- en: 13.2.1 A self-built `dataset()`
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2.1 自定义的 `dataset()`
- en: Let’s say we wanted to build a classifier based on the popular `iris` alternative,
    `palmerpenguins`.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要基于流行的 `iris` 替代品 `palmerpenguins` 构建一个分类器。
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE2]'
- en: 'In predicting `species`, we want to make use of just a subset of columns: `bill_length_mm`,
    `bill_depth_mm`, `flipper_length_mm`, and `body_mass_g`. We build a `dataset()`
    that returns exactly what is needed:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测 `species` 时，我们只想使用子集的列：`bill_length_mm`、`bill_depth_mm`、`flipper_length_mm`
    和 `body_mass_g`。我们构建一个返回所需内容的 `dataset()`：
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '*Once we’ve instantiated a `penguins_dataset()`, we should immediately perform
    some checks. First, does it have the expected length?'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*一旦我们实例化了 `penguins_dataset()`，我们应该立即进行一些检查。首先，它是否有预期的长度？'
- en: '[PRE4]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '*[PRE5]'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE5]'
- en: 'And second, do individual elements have the expected shape and data type? Conveniently,
    we can access `dataset()` items like tensor values, through indexing:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，单个元素是否具有预期的形状和数据类型？方便的是，我们可以像访问张量值一样访问 `dataset()` 项目，通过索引：
- en: '[PRE6]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '*[PRE7]'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE7]'
- en: 'This also works for items “further down” in the `dataset()` – it has to: When
    indexing into a `dataset()`, what happens in the background is a call to `.getitem(i)`,
    passing along the desired position `i`.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这也适用于 `dataset()` 中“更深层”的项目——它必须这样做：当索引 `dataset()` 时，后台发生的是对 `.getitem(i)`
    的调用，传递所需的 `i` 位置。
- en: 'Truth be told, in this case we didn’t really have to build our own `dataset()`.
    With so little pre-processing to be done, there is an alternative: `tensor_dataset()`.****  ***###
    13.2.2 `tensor_dataset()`'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 真相是，在这种情况下，我们实际上并不需要构建自己的 `dataset()`。由于预处理的工作量很小，有一个替代方案：`tensor_dataset()`。****  ***###
    13.2.2 `tensor_dataset()`
- en: 'When you already have a tensor around, or something that’s readily converted
    to one, you can make use of a built-in `dataset()` generator: `tensor_dataset()`.
    This function can be passed any number of tensors; each batch item then is a list
    of tensor values:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 当你已经有一个张量或可以轻松转换为张量的东西时，你可以使用内置的 `dataset()` 生成器：`tensor_dataset()`。这个函数可以传递任意数量的张量；每个批次的项目随后是一个张量值的列表：
- en: '[PRE8]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '*[PRE9]'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE9]'
- en: 'In our `penguins` scenario, we end up with two lines of code:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的 `penguins` 场景中，我们最终得到两行代码：
- en: '[PRE10]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '*Admittedly though, we have not made use of all the dataset’s columns. The
    more pre-processing you need a `dataset()` to do, the more likely you are to want
    to code your own.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*诚然，我们还没有使用数据集的所有列。你需要`dataset()`执行更多预处理，你更有可能想要编写自己的代码。'
- en: Thirdly and finally, here is the most effortless possible way.**  **### 13.2.3
    `torchvision::mnist_dataset()`
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 第三点也是最后一点，这是最简单的方法。**  **### 13.2.3 `torchvision::mnist_dataset()`
- en: When you’re working with packages in the `torch` ecosystem, chances are that
    they already include some `dataset()`s, be it for demonstration purposes or for
    the sake of the data themselves. `torchvision`, for example, packages a number
    of classic image datasets – among those, that archetype of archetypes, MNIST.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在`torch`生态系统中的包工作时，它们很可能已经包含了一些`dataset()`，无论是为了演示目的还是为了数据本身。例如，`torchvision`打包了多个经典图像数据集——其中，那个原型中的原型，MNIST。
- en: 'Since we’re going to talk about image processing in a later chapter, I won’t
    comment on the arguments to `mnist_dataset()` here; we do, however, include a
    quick check that the data delivered conform to what we’d expect:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将在后面的章节中讨论图像处理，因此在这里我不会对`mnist_dataset()`的参数进行评论；然而，我们确实包含了一个快速检查，以确保提供的数据符合我们的预期：
- en: '[PRE11]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '*[PRE12]'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE12]'
- en: At this point, that is all we need to know about `dataset()`s – we’ll encounter
    plenty of them in the course of this book. Now, we move on from the one to the
    many.*******  ***## 13.3 Using `dataloader()`s
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，这就是我们需要了解的关于`dataset()`的所有内容——在这本书的过程中，我们将遇到很多。现在，我们从单一的数据集转向多个数据集。*******  ***##
    13.3 使用 `dataloader()`s
- en: 'Continuing to work with the newly created MNIST `dataset()`, we instantiate
    a `dataloader()` for it. The `dataloader()` will deliver pairs of images and labels
    in batches: thirty-two at a time. In every epoch, it will return them in different
    order (`shuffle = TRUE`):'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 继续使用新创建的MNIST `dataset()`，我们为它实例化一个`dataloader()`。`dataloader()`将按批次提供图像和标签对：每次32个。在每个epoch中，它将以不同的顺序返回它们（`shuffle
    = TRUE`）：
- en: '[PRE13]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '*Just like `dataset()`s, `dataloader()`s can be queried about their length:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*就像`dataset()`一样，`dataloader()`也可以查询它们的长度：'
- en: '[PRE14]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '*[PRE15]'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE15]'
- en: This time, though, the returned value is not the number of items; it is the
    number of batches.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，返回的值不是项目数量；它是批次的数量。
- en: 'To loop over batches, we first obtain an iterator, an object that knows how
    to traverse the elements in this `dataloader()`. Calling `dataloader_next()`,
    we can then access successive batches, one by one:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 要遍历批次，我们首先获取一个迭代器，这是一个知道如何遍历`dataloader()`中元素的对象。调用`dataloader_next()`后，我们可以逐个访问连续的批次：
- en: '[PRE16]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '*[PRE17]'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE17]'
- en: If you compare the batch shape of `x` – the image part – with the shape of an
    individual image (as inspected above), you see that now, there is an additional
    dimension in front, reflecting the number of images in a batch.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你比较`x`（图像部分）的批次形状与单个图像的形状（如上所述），你会发现现在前面有一个额外的维度，反映了批次中的图像数量。
- en: The next step is passing the batches to a model. This – in fact, this as well
    as the complete, end-to-end deep-learning workflow – is what the next chapter
    is about.******
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将批次传递给模型。实际上，这以及完整的端到端深度学习工作流程都是下一章的主题。******
