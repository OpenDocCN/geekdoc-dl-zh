- en: 1.6 Building and Evaluating LLM Applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1.6 构建和评估LLM应用程序
- en: 原文：[https://boramorka.github.io/LLM-Book/en/CHAPTER-1/1.6%20Building%20and%20Evaluating%20LLM%20Applications/](https://boramorka.github.io/LLM-Book/en/CHAPTER-1/1.6%20Building%20and%20Evaluating%20LLM%20Applications/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[https://boramorka.github.io/LLM-Book/en/CHAPTER-1/1.6%20Building%20and%20Evaluating%20LLM%20Applications/](https://boramorka.github.io/LLM-Book/en/CHAPTER-1/1.6%20Building%20and%20Evaluating%20LLM%20Applications/)'
- en: Building applications powered by large language models (LLMs) requires more
    than clean integration — it needs a systematic quality evaluation that covers
    both objective and subjective aspects. In practice, you combine accuracy, recall,
    and F1 (when gold answers are available) with user ratings and satisfaction metrics
    (CSA), while also tracking operational indicators like cost and latency. This
    blend exposes weak spots, informs release decisions, and guides targeted improvements.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 建立由大型语言模型（LLMs）驱动的应用程序需要不仅仅是干净的集成——它需要一个系统的质量评估，涵盖客观和主观方面。在实践中，你将准确性、召回率和F1（当有黄金答案时）与用户评分和满意度指标（CSA）相结合，同时跟踪运营指标，如成本和延迟。这种组合揭示了弱点，为发布决策提供了信息，并指导了有针对性的改进。
- en: 'The typical path to production starts with simple prompts and a small dataset
    for quick iteration; then you broaden coverage, complicate scenarios, refine metrics
    and quality criteria — remembering that perfection isn’t always necessary. It’s
    often enough to consistently solve the target tasks within quality and budget
    constraints. In high‑stakes scenarios (medicine, law enforcement, finance), stricter
    validation becomes essential: random sampling and hold‑out tests, bias and error
    checks, and attention to ethical and legal issues — preventing harm, ensuring
    explainability, and enabling audit.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的生产路径从简单的提示和少量数据集开始，以快速迭代；然后扩大覆盖范围，复杂化场景，细化指标和质量标准——记住，完美并不总是必要的。通常，在质量和预算约束内持续解决目标任务就足够了。在高风险场景（如医学、执法、金融）中，更严格的验证变得至关重要：随机抽样和保留测试、偏差和错误检查，以及关注伦理和法律问题——防止伤害，确保可解释性，并启用审计。
- en: Good engineering style emphasizes modularity and fast iteration, automated regression
    tests and measurements, thoughtful metric selection aligned with business goals,
    and mandatory bias/fairness analysis with regular reviews.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 良好的工程风格强调模块化和快速迭代，自动化的回归测试和测量，与业务目标相一致的有意度量选择，以及定期的偏差/公平性分析。
- en: 'To make evaluation reproducible, use rubrics and evaluation protocols: define
    criteria in advance — relevance to user intent and context, factual correctness,
    completeness, and coherence/fluency — as well as the process, scales, and thresholds.
    For subjective tasks, use multiple independent raters and automatic consistency
    checks. Where possible, compare answers to ideal (expert) responses — a “gold
    standard” provides an anchor for more objective judgments. Here’s a small environment
    scaffold and call function for reproducible experiments and evaluations:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使评估可重复，使用评分标准和评估协议：提前定义标准——与用户意图和上下文的相关性、事实准确性、完整性以及连贯性/流畅性——以及过程、量表和阈值。对于主观任务，使用多个独立的评分者和自动一致性检查。在可能的情况下，将答案与理想（专家）答案进行比较——一个“黄金标准”为更客观的判断提供了一个基准。以下是一个用于可重复实验和评估的小型环境框架和调用函数：
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, formalize rubric‑based evaluation and assign weights to compute an overall
    score with detailed feedback. Below is a template where the model produces an
    assessment according to given criteria; the parsing is a stub and should be replaced
    with logic suited to your model’s output format:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，正式化基于评分标准的评估，并分配权重以计算一个综合评分，并提供详细的反馈。以下是一个模板，其中模型根据给定的标准进行评估；解析是一个占位符，应替换为适合您模型输出格式的逻辑：
- en: '[PRE1]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'When you need a gold‑standard comparison, explicitly compare the model’s answer
    with the ideal expert answer and score high‑priority criteria (factual accuracy,
    alignment, completeness, coherence). Here’s a skeleton that returns both an aggregate
    score and the raw comparison text for audit:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当你需要黄金标准比较时，明确地将模型的答案与理想的专家答案进行比较，并评分高优先级标准（事实准确性、一致性、完整性、连贯性）。以下是一个返回综合评分和原始比较文本的框架，用于审计：
- en: '[PRE2]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'On top of these basics, add advanced techniques: evaluate semantic similarity
    via embeddings and similarity metrics (not just surface overlap), bring in independent
    reviewers for crowd evaluation, include automated checks for coherence and logic,
    and build adaptive evaluation frameworks tailored to your domain and task types.
    In production, continuous evaluation is crucial: track version and metric history;
    close the loop from user feedback back to development; include diverse cases,
    edge cases, and cultural/linguistic variation; involve experts (including blind
    reviews to reduce bias); compare with alternative models; and employ specialized
    “judges” to detect contradictions and factual errors. Together, rigorous methods
    and constant iteration — plus rubrics, gold standards, expert reviews, and automated
    checks — help you build reliable and ethical systems.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些基础知识之上，增加高级技术：通过嵌入和相似度指标评估语义相似度（而不仅仅是表面重叠），引入独立审稿人进行群体评估，包括对一致性和逻辑的自动化检查，并构建适应您领域和任务类型的自适应评估框架。在生产中，持续评估至关重要：跟踪版本和指标历史；从用户反馈回到开发的闭环；包括多样化的案例、边缘案例和文化/语言差异；涉及专家（包括盲审以减少偏见）；与替代模型进行比较；并雇佣专门的“裁判”来检测矛盾和事实错误。严格的方法和持续的迭代——加上评分标准、黄金标准、专家评审和自动化检查——有助于您构建可靠和道德的系统。
- en: Theory Questions
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理论问题
- en: Why evaluate LLM answers, and along which dimensions?
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么需要评估LLM答案，以及从哪些维度进行评估？
- en: Give examples of metrics and explain their role in development.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供指标示例并解释它们在开发中的作用。
- en: What does the iterative path from development to production look like?
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从开发到生产的迭代路径是什么样的？
- en: Why do high‑stakes scenarios require stricter rigor? Give examples.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么高风险场景需要更严格的严谨性？请举例说明。
- en: List best practices for bootstrapping, iteration, and automated testing.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列出启动、迭代和自动化测试的最佳实践。
- en: How do automated tests help development?
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自动化测试如何帮助开发？
- en: Why should metrics be tuned to the specific task?
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么指标应该针对特定任务进行调整？
- en: How do you build a rubric and evaluation protocols?
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何构建评分标准和评估协议？
- en: Which advanced evaluation techniques apply and why?
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪些高级评估技术适用，以及为什么？
- en: How do continuous evaluation and broad test coverage improve reliability?
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 持续评估和广泛的测试覆盖率如何提高可靠性？
- en: Practical Tasks
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实践任务
- en: Write a function that reads the API key from the environment, queries the LLM,
    and measures runtime and tokens used.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个函数，从环境中读取API密钥，查询LLM，并测量运行时间和使用的令牌。
