<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><h2 class="h2" id="ch02"><span class="ash">2</span></h2>
<h2 class="h2a">Semantic Search with LLMs</h2>
<h3 class="h3" id="ch02lev1sec1">Introduction</h3>
<p>In the last chapter, we explored the inner workings of language models and the impact that modern LLMs have had on NLP tasks like text classification, generation, and machine translation. There is another powerful application of LLMs that has been gaining traction in recent years: semantic search.</p>
<p>Now you might be thinking that it’s time to finally learn the best ways to talk to ChatGPT and GPT-4 to get the optimal results, and we will start to do that in the next chapter, I promise. In the meantime, I want to show you what else we can build on top of this novel transformer architecture. While text-to-text generative models like GPT are extremely impressive in their own right, one of the most versatile solutions that AI companies offer is the ability to generate text embeddings based on powerful LLMs.</p>
<p>Text embeddings are a way to represent words or phrases as vectors in a high-dimensional space based on their contextual meaning within a corpus of text data. The idea is that if two phrases are similar (we will explore that word in more detail later on in this chapter) then the vectors that represent those phrases should be close together and vice versa. <a href="ch02.html#ch02fig01">Figure 2.1</a> shows an example of a simple search algorithm. When a user searches for an item to buy – say a magic the gathering trading card they might simply search for “a vintage magic card”. The system should then embed the query such that if two text embeddings that are near each other should indicate that the phrases that were used to generate them are similar.</p>
<div class="group">
<div class="image" id="ch02fig01"><img src="graphics/02fig01.jpg" alt="Images" width="735" height="411"/></div>
<p class="fig-caption"><strong>Figure 2.1</strong> <em>Vectors that represent similar phrases should be close together and those that represent dissimilar phrases should be far apart. In this case, if a user wants a trading card they might ask for “a vintage magic card”. A proper semantic search system should embed the query in such a way that it ends up near relevant results (like “magic card”) and far apart from non relevant items (like “a vintage magic kit”) even if they share certain keywords.</em></p>
</div>
<p>This map from text to vectors can be thought of as a kind of hash with meaning. We can’t really reverse vectors back to text but rather they are a representation of the text that has the added benefit of carrying the ability to compare points while in their encoded state.</p>
<p>LLM-enabled text embeddings allow us to capture the semantic value of words and phrases beyond just their surface-level syntax or spelling. We can rely on the pre-training and fine-tuning of LLMs to build virtually unlimited applications on top of them by leveraging this rich source of information about language use.</p>
<p>This chapter introduces us to the world of semantic search using LLMs to explore how they can be used to create powerful tools for information retrieval and analysis. In the next chapter, we will build a chatbot on top of GPT-4 that leverages a fully realized semantic search system that we will build in this chapter.</p>
<p>Without further ado, let’s get right into it, shall we?</p>
<h3 class="h3" id="ch02lev1sec2">The Task</h3>
<p>A traditional search engine would generally take what you type in and then give you a bunch of links to websites or items that contain those words or permutations of the characters that you typed in. So if you typed in “Vintage Magic the Gathering Cards” on a marketplace, you would get items with a title/description that contains combinations of those words. That’s a pretty standard way to search, but it’s not always the best way. For example I might get vintage magic sets to help me learn how to pull a rabbit out of a hat. Fun but not what I asked for.</p>
<p>The terms you input into a search engine may not always align with the <em>exact</em> words used in the items you want to see. It could be that the words in the query are too general, resulting in a slew of unrelated findings. This issue often extends beyond just differing words in the results; the same words might carry different meanings than what was searched for. This is where semantic search comes into play, as exemplified by the earlier-mentioned Magic: The Gathering cards scenario.</p>
<h4 class="h4" id="ch02lev2sec1">Asymmetric Semantic Search</h4>
<p>A <strong>semantic search</strong> system can understand the meaning and context of your search query and match it against the meaning and context of the documents that are available to retrieve. This kind of system can find relevant results in a database without having to rely on exact keyword or n-gram matching but rather rely on a pre-trained LLM to understand the nuance of the query and the documents (<a href="ch02.html#ch02fig02">Figure 2.2</a>).</p>
<div class="group">
<div class="image" id="ch02fig02"><img src="graphics/02fig02.jpg" alt="Images" width="505" height="349"/></div>
<p class="fig-caption"><strong>Figure 2.2</strong> <em>A traditional keyword-based search might rank a vintage magic kit with the same weight as the item we actually want whereas a semantic search system can understand the actual concept we are searching for</em></p>
</div>
<p>The <strong>asymmetric</strong> part of asymmetric semantic search refers to the fact that there is generally an imbalance between the semantic information (basically the size) of the input query and the documents/information that the search system has to retrieve. For example, the search system is trying to match “magic the gathering card” to paragraphs of item descriptions on a marketplace. The four-word search query has much less information than the paragraphs but nonetheless it is what we are comparing.</p>
<p>Asymmetric semantic search systems can get very accurate and relevant search results, even if you don’t use the exact right words in your search. They rely on the learnings of LLMs rather than the user being able to know exactly what needle to search for in the haystack.</p>
<p>I am of course, vastly oversimplifying the traditional method. There are many ways to make them more performant without switching to a more complex LLM approach and pure semantic search systems are not always the answer. They are not simply “the better way to do search”. Semantic algorithms have their own deficiencies like:</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> They can be overly sensitive to small variations in text, such as differences in capitalization or punctuation.</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> They struggle with nuanced concepts, such as sarcasm or irony that rely on localized cultural knowledge.</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> They can be more computationally expensive to implement and maintain than the traditional method, especially when launching a home-grown system with many open-source components.</p>
<p>Semantic search systems can be a valuable tool in certain contexts, so let’s jump right into how we will architect our solution.</p>
<h3 class="h3" id="ch02lev1sec3">Solution Overview</h3>
<p>The general flow of our asymmetric semantic search system will follow these steps:</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> PART I - Ingesting documents (<a href="ch02.html#ch02fig03">Figure 2.3</a>)</p>
<p class="bullet-number">1. Collect documents for embedding</p>
<p class="bullet-number">2. Create text embeddings to encode semantic information</p>
<p class="bullet-number">3. Store embeddings in a database for later retrieval given a query</p>
<div class="group">
<div class="image" id="ch02fig03"><img src="graphics/02fig03.jpg" alt="Images" width="527" height="240"/></div>
<p class="fig-caption"><strong>Figure 2.3</strong> <em>Zooming in on Part I, storing documents will consist of doing some pre-processing on our documents, embedding them, and then storing them in some database</em></p>
</div>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> PART II - Retrieving documents (<a href="ch02.html#ch02fig04">Figure 2.4</a>)</p>
<p class="bullet-number">1. User has a query which may be pre-processed and cleaned</p>
<p class="bullet-number">2. Retrieve candidate documents</p>
<p class="bullet-number">3. Re-rank the candidate documents if necessary</p>
<p class="bullet-number">4. Return the final search results</p>
<div class="group">
<div class="image" id="ch02fig04"><img src="graphics/02fig04.jpg" alt="Images" width="757" height="253"/></div>
<p class="fig-caption"><strong>Figure 2.4</strong> <em>Zooming in on Part II, when retrieving documents we will have to embed our query using the same embedding scheme as we used for the documents and then compare them against the previously stored documents and return the best (closest) document</em></p>
</div>
<h3 class="h3" id="ch02lev1sec4">The Components</h3>
<p>Let’s go over each of our components in more detail to understand the choices we’re making and what considerations we need to take into account.</p>
<h4 class="h4" id="ch02lev2sec2">Text Embedder</h4>
<p>As we now know, at the heart of any semantic search system is the text embedder. This is the component that takes in a text document, or a single word or phrase, and converts it into a vector. The vector is unique to that text and should capture the contextual meaning of the phrase.</p>
<p>The choice of the text embedder is critical as it determines the quality of the vector representation of the text. We have many options in how we vectorize with LLMs, both open and closed source. To get off of the ground quicker, we are going to use OpenAI’s closed-source “Embeddings” product. In a later section, I’ll go over some open-source options.</p>
<p>OpenAI’s “Embeddings” is a powerful tool that can quickly provide high-quality vectors, but it is a closed-source product, which means we have limited control over its implementation and potential biases. It’s important to keep in mind that when using closed-source products, we may not have access to the underlying algorithms, which can make it difficult to troubleshoot any issues that may arise.</p>
<h5 class="h5" id="ch01lev3sec1">What makes pieces of text “similar”</h5>
<p>Once we convert our text into vectors, we have to find a mathematical representation of figuring out if pieces of text are “similar” or not. Cosine similarity is a way to measure how similar two things are. It looks at the angle between two vectors and gives a score based on how close they are in direction. If the vectors point in exactly the same direction, the cosine similarity is 1. If they’re perpendicular (90 degrees apart), it’s 0. And if they point in opposite directions, it’s -1. The size of the vectors doesn’t matter, only their orientation does.</p>
<p><a href="ch02.html#ch02fig05">Figure 2.5</a> shows how the cosine similarity would help us retrieve documents given a query.</p>
<div class="group">
<div class="image" id="ch02fig05"><img src="graphics/02fig05.jpg" alt="Images" width="776" height="628"/></div>
<p class="fig-caption"><strong>Figure 2.5</strong> <em>In an ideal semantic search scenario, the Cosine Similarity (formula given at the top) gives us a computationally efficient way to compare pieces of text at scale, given that embeddings are tuned to place semantically similar pieces of text near each other (bottom). We start by embedding all items – including the query (bottom left) and then checking the angle between them. The smaller the angle, the larger the cosine similarity (bottom right)</em></p>
</div>
<p>We could also turn to other similarity metrics like the dot product or the Euclidean distance but OpenAI embeddings have a special property. The magnitudes (lengths) of their vectors are normalized to length 1, which basically means that we benefit mathematically on two fronts:</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> Cosine similarity is identical to the dot product</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> Cosine similarity and Euclidean distance will result in the identical rankings</p>
<p>TL;DR: Having normalized vectors (all having a magnitude of 1) is great because we can use a cheap cosine calculation to see how close two vectors are and therefore how close two phrases are semantically via the cosine similarity.</p>
<h5 class="h5" id="ch01lev3sec2">OpenAI’s embedding</h5>
<p>Getting embeddings from OpenAI is as simple as a few lines of code (<a href="ch02.html#list2_1">Listing 2.1</a>). As mentioned previously, this entire system relies on an embedding mechanism that places semantically similar items near each other so that the cosine similiarty is large when the items are actually similar. There are multiple methods we could use to create these embeddings, but we will for now rely on OpenAI’s embedding <strong>engines</strong> to do this work for us. Engines are different embedding mechanism that OpenAI offer. We will use their most recent engine that they recommend for most use-cases.</p>
<p class="ex-caption" id="list2_1"><strong>Listing 2.1</strong> <em>Getting text embeddings from OpenAI</em></p>
<div class="pre-box">
<pre><code># Importing the necessary modules for the script to run
import openai
from openai.embeddings_utils import get_embeddings, get_embedding

# Setting the OpenAI API key using the value stored in the environment variable 'OPENAI_API_KEY'
openai.api_key = os.environ.get('OPENAI_API_KEY')

# Setting the engine to be used for text embedding
ENGINE = 'text-embedding-ada-002'

# Generating the vector representation of the given text using the specified engine.
embedded_text = get_embedding('I love to be vectorized', engine=ENGINE)

# Checking the length of the resulting vector to ensure it is the expected size (1536)
len(embedded_text) == '1536'</code></pre>
</div>
<p>It’s worth noting that OpenAI provides several engine options that can be used for text embedding. Each engine may provide different levels of accuracy and may be optimized for different types of text data. At the time of writing, the engine used in the code block is the most recent and the one they recommend using.</p>
<p>Additionally, it is possible to pass in multiple pieces of text at once to the “get_embeddings” function, which can generate embeddings for all of them in a single API call. This can be more efficient than calling “get_embedding” multiple times for each individual text. We will see an example of this later on.</p>
<h5 class="h5" id="ch01lev3sec3">Open-source Embedding Alternatives</h5>
<p>While OpenAI and other companies provide powerful text embedding products, there are also several open-source alternatives available for text embedding. A popular one is the bi-encoder with BERT, a powerful deep learning-based algorithm that has been shown to produce state-of-the-art results on a range of natural language processing tasks. We can find pre-trained bi-encoders in many open source repositories, including the <strong>Sentence Transformers</strong> library, which provides pre-trained models for a variety of natural language processing tasks to use off the shelf.</p>
<p>A bi-encoder involves training two BERT models, one to encode the input text and the other to encode the output text (<a href="ch02.html#ch02fig06">Figure 2.6</a>). The two models are trained simultaneously on a large corpus of text data, with the goal of maximizing the similarity between corresponding pairs of input and output text. The resulting embeddings capture the semantic relationship between the input and output text.</p>
<div class="group">
<div class="image" id="ch02fig06"><img src="graphics/02fig06.jpg" alt="Images" width="329" height="377"/></div>
<p class="fig-caption"><strong>Figure 2.6</strong> <em>A bi-encoder is trained in a unique way with two clones of a single LLM trained in parallel to learn similarities between documents. For example, a bi-encoder can learn to associate questions to paragraphs so they appear near each other in a vector space</em></p>
</div>
<p><a href="ch02.html#list2_2">Listing 2.2</a> is an example of embedding text with a pre-trained bi-encoder with the “sentence_transformer” package:</p>
<p class="ex-caption" id="list2_2"><strong>Listing 2.2</strong> <em>Getting text embeddings from a pre-trained open source bi-encoder</em></p>
<div class="pre-box">
<pre><code># Importing the SentenceTransformer library
from sentence_transformers import SentenceTransformer

# Initializing a SentenceTransformer model with the 'multi-qa-mpnet-base-cos-v1' pre-trained model
model = SentenceTransformer(
  'sentence-transformers/multi-qa-mpnet-base-cos-v1')

# Defining a list of documents to generate embeddings for
docs = [
          "Around 9 Million people live in London",
          "London is known for its financial district"
       ]

# Generate vector embeddings for the documents
doc_emb = model.encode(
    docs,                   # our documents (an iterable of strings)
    batch_size=32,          # batch the embeddings by this size
    show_progress_bar=True  # display a progress bar

)

# The shape of the embeddings is (2, 768), indicating a length of 768 and two embeddings generated
doc_emb.shape  #  == (2, 768)</code></pre>
</div>
<p>This code creates an instance of the ‘SentenceTransformer’ class, which is initialized with the pre-trained model ‘multi-qa-mpnet-base-cos-v1’. This model is designed for multi-task learning, specifically for tasks such as question-answering and text classification. This one in particular was pre-trained using asymmetric data so we know it can handle both short queries and long documents and be able to compare them well. We use the ‘encode’ function from the SentenceTransformer class to generate vector embeddings for the documents, with the resulting embeddings stored in the ‘doc_emb’ variable.</p>
<p>Different algorithms may perform better on different types of text data and will have different vector sizes. The choice of algorithm can have a significant impact on the quality of the resulting embeddings. Additionally, open-source alternatives may require more customization and fine-tuning than closed-source products, but they also provide greater flexibility and control over the embedding process. For more examples of using open-source bi-encoders to embed text, check out the code portion of this book!</p>
<h4 class="h4" id="ch02lev2sec3">Document Chunker</h4>
<p>Once we have our text embedding engine set up, we need to consider the challenge of embedding large documents. It is often not practical to embed entire documents as a single vector, particularly when dealing with long documents such as books or research papers. One solution to this problem is to use document chunking, which involves dividing a large document into smaller, more manageable chunks for embedding.</p>
<h5 class="h5" id="ch01lev3sec4">Max Token Window Chunking</h5>
<p>One approach to document chunking is max token window chunking. This is one of the easiest methods to implement and involves splitting the document into chunks of a given max size. So if we set a token window to be 500, then we’d expect each chunk to be just below 500 tokens. Having our chunks all be around the same size will also help make our system more consistent.</p>
<p>One common concern of this method is that we might accidentally cut off some important text between chunks, splitting up the context. To mitigate this, we can set overlapping windows with a specified amount of tokens to overlap so we have tokens shared between chunks. This of course introduces a sense of redundancy but this is often fine in service of higher accuracy and latency.</p>
<p>Let’s see an example of overlapping window chunking with some sample text (<a href="ch02.html#list2_3">Listing 2.3</a>). Let’s begin by ingesting a large document. How about a recent book I wrote with over 400 pages?</p>
<p class="ex-caption" id="list2_3"><strong>Listing 2.3</strong> <em>Ingesting an entire textbook</em></p>
<div class="pre-box">
<pre><code># Use the PyPDF2 library to read a PDF file
import PyPDF2

# Open the PDF file in read-binary mode
with open('../data/pds2.pdf', 'rb') as file:

    # Create a PDF reader object
    reader = PyPDF2.PdfReader(file)

    # Initialize an empty string to hold the text
    principles_of_ds = ''

    # Loop through each page in the PDF file
    for page in tqdm(reader.pages):
       
        # Extract the text from the page
        text = page.extract_text()

        # Find the starting point of the text we want to extract
        # In this case, we are extracting text starting from the string ' ]'
        principles_of_ds += '\n\n' + text[text.find(' ]')+2:]

# Strip any leading or trailing whitespace from the resulting string
principles_of_ds = principles_of_ds.strip()</code></pre>
</div>
<p>And now let’s chunk this document by getting chunks of at most a certain token size (<a href="ch02.html#list2_4">Listing 2.4</a>).</p>
<p class="ex-caption" id="list2_4"><strong>Listing 2.4</strong> <em>Chunking the textbook with and without overlap</em></p>
<div class="pre-box">
<pre><code># Function to split the text into chunks of a maximum number of tokens. Inspired by OpenAI
def overlapping_chunks(text, max_tokens = 500, overlapping_factor = 5):
    '''
    max_tokens: tokens we want per chunk
    overlapping_factor: number of sentences to start each chunk with that overlaps with the previous chunk
    '''

    # Split the text using punctuation
    sentences = re.split(r'[.?!]', text)

    # Get the number of tokens for each sentence
    n_tokens = [len(tokenizer.encode(" " + sentence)) for sentence in sentences]
   
    chunks, tokens_so_far, chunk = [], 0, []

    # Loop through the sentences and tokens joined together in a tuple
    for sentence, token in zip(sentences, n_tokens):

        # If the number of tokens so far plus the number of tokens in the current sentence is greater
        # than the max number of tokens, then add the chunk to the list of chunks and reset
        # the chunk and tokens so far
        if tokens_so_far + token &gt; max_tokens:
            chunks.append(". ".join(chunk) + ".")
            if overlapping_factor &gt; 0:
                chunk = chunk[-overlapping_factor:]
                tokens_so_far = sum([len(tokenizer.encode(c)) for c in chunk])
            else:
                chunk = []
                tokens_so_far = 0

        # If the number of tokens in the current sentence is greater than the max number of
        # tokens, go to the next sentence
        if token &gt; max_tokens:
            continue

        # Otherwise, add the sentence to the chunk and add the number of tokens to the total
        chunk.append(sentence)
        tokens_so_far += token + 1

    return chunks

split = overlapping_chunks(principles_of_ds, overlapping_factor=0)
avg_length = sum([len(tokenizer.encode(t)) for t in split]) / len(split)
print(f'non-overlapping chunking approach has {len(split)} documents with average length {avg_length:.1f} tokens')
<strong>non-overlapping chunking approach has 286 documents with average length 474.1 tokens</strong>

# with 5 overlapping sentences per chunk
split = overlapping_chunks(principles_of_ds, overlapping_factor=5)
avg_length = sum([len(tokenizer.encode(t)) for t in split]) / len(split)
print(f'overlapping chunking approach has {len(split)} documents with average length {avg_length:.1f} tokens')
<strong>overlapping chunking approach has 391 documents with average length 485.4 tokens</strong></code></pre>
</div>
<p>With overlap, we see an increase in the number of document chunks but around the same size. The higher the overlapping factor, the more redundancy we introduce into the system. The max token window method does not take into account the natural structure of the document and may result in information being split up between chunks or chunks with overlapping information, confusing the retrieval system.</p>
<h6 class="h6" id="ch01lev4sec1">Finding Custom Delimiters</h6>
<p>To help aid our chunking method, we could search for custom natural delimiters. We would identify natural white spaces within the text and use them to create more meaningful units of text that will end up in document chunks that will eventually get embedded (<a href="ch02.html#ch02fig07">Figure 2.7</a>).</p>
<div class="group">
<div class="image" id="ch02fig07"><img src="graphics/02fig07.jpg" alt="Images" width="662" height="350"/></div>
<p class="fig-caption"><strong>Figure 2.7</strong> <em>Max-token chunking (on the left) and natural whitespace chunking (on the right) can be done with or without overlap. The natural whitespace chunking tends to end up with non-uniform chunk sizes.</em></p>
</div>
<p>Let’s look for common whitespaces in the textbook (<a href="ch02.html#list2_5">Listing 2.5</a>).</p>
<p class="ex-caption" id="list2_5"><strong>Listing 2.5</strong> <em>Chunking the textbook with natural whitespace</em></p>
<div class="pre-box">
<pre><code># Importing the Counter and re libraries
from collections import Counter
import re

# Find all occurrences of one or more spaces in 'principles_of_ds'
matches = re.findall(r'[\s]{1,}', principles_of_ds)

# The 5 most frequent spaces that occur in the document
most_common_spaces = Counter(matches).most_common(5)

# Print the most common spaces and their frequencies
print(most_common_spaces)

[(' ', 82259),
 ('\n', 9220),
 ('  ', 1592),
 ('\n\n', 333),
 ('\n   ', 250)]</code></pre>
</div>
<p>The most common double white space is two newline characters in a row which is actually how I earlier distinguished between pages which makes sense. The most natural whitespace in a book is by page. In other cases, we may have found natural whitespace between paragraphs as well. This method is very hands-on and requires a good amount of familiarity and knowledge of the source documents.</p>
<p>We can also turn to more machine learning to get slightly more creative with how we architect document chunks.</p>
<h5 class="h5" id="ch01lev3sec5">Using Clustering to Create Semantic Documents</h5>
<p>Another approach to document chunking is to use clustering to create semantic documents. This approach involves creating new documents by combining small chunks of information that are semantically similar (<a href="ch02.html#ch02fig08">Figure 2.8</a>). This approach requires some creativity, as any modifications to the document chunks will alter the resulting vector. We could use an instance of Agglomerative clustering from scikit-learn, for example, where similar sentences or paragraphs are grouped together to form new documents.</p>
<div class="group">
<div class="image" id="ch02fig08"><img src="graphics/02fig08.jpg" alt="Images" width="769" height="397"/></div>
<p class="fig-caption"><strong>Figure 2.8</strong> <em>We can group any kinds of document chunks together by using some separate semantic clustering system (shown on the right) to create brand new documents with chunks of information in them that are similar to each other.</em></p>
</div>
<p>Let’s try to cluster together those chunks we found from the textbook in our last section (<a href="ch02.html#list2_6">Listing 2.6</a>).</p>
<p class="ex-caption" id="list2_6"><strong>Listing 2.6</strong> <em>Clustering pages of the document by semantic similarity</em></p>
<div class="pre-box">
<pre><code>from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# Assume you have a list of text embeddings called `embeddings`
# First, compute the cosine similarity matrix between all pairs of embeddings
cosine_sim_matrix = cosine_similarity(embeddings)

# Instantiate the AgglomerativeClustering model
agg_clustering = AgglomerativeClustering(
    n_clusters=None,         # the algorithm will determine the optimal number of clusters based on the data
    distance_threshold=0.1,  # clusters will be formed until all pairwise distances between clusters are greater than 0.1
    affinity='precomputed',  # we are providing a precomputed distance matrix (1 - similarity matrix) as input
    linkage='complete'       # form clusters by iteratively merging the smallest clusters based on the maximum distance between their components
)

# Fit the model to the cosine distance matrix (1 - similarity matrix)
agg_clustering.fit(1 - cosine_sim_matrix)

# Get the cluster labels for each embedding
cluster_labels = agg_clustering.labels_

# Print the number of embeddings in each cluster
unique_labels, counts = np.unique(cluster_labels, return_counts=True)
for label, count in zip(unique_labels, counts):
    print(f'Cluster {label}: {count} embeddings')

<strong>Cluster 0: 2 embeddings</strong>
<strong>Cluster 1: 3 embeddings</strong>
<strong>Cluster 2: 4 embeddings</strong>
...</code></pre>
</div>
<p>This approach tends to yield chunks that are more cohesive semantically but suffer from pieces of content being out of context with surrounding text. This approach works well when the chunks you start with are known to not necessarily relate to each other i.e. chunks are more independent of one another.</p>
<h5 class="h5" id="ch01lev3sec6">Use Entire Documents Without Chunking</h5>
<p>Alternatively, it is possible to use entire documents without chunking. This approach is probably the easiest option overall but will have drawbacks when documents are far too long and we hit a context window limit when we embed the text. We also might fall victim to the documents being filled with extraneous disparate context points and the resulting embeddings may be trying to encode too much and may suffer in quality. These drawbacks compound for very large (multi-page) documents.</p>
<p>It is important to consider the trade-offs between chunking and using entire documents when selecting an approach for document embedding (<a href="ch02.html#ch02tab01">Table 2.1</a>). Once we decide how we want to chunk our documents, we need a home for the embeddings we create. Locally, we can rely on matrix operations for quick retrieval, but we are building for the cloud here, so let’s look at our database options.</p>
<div class="group">
<p class="tab-caption"><strong>Table 2.1</strong> <em>Outlining different document chunking methods with pros and cons</em></p>
<div class="imaget" id="ch02tab01"><img src="graphics/02tab01.jpg" alt="Images" width="777" height="903"/></div>
</div>
<h4 class="h4" id="ch02lev2sec4">Vector Databases</h4>
<p>A <strong>vector database</strong> is a data storage system that is specifically designed to both store and retrieve vectors quickly. This type of database is useful for storing embeddings generated by an LLM which encode and store the semantic meaning of our documents or chunks of documents. By storing embeddings in a vector database, we can efficiently perform nearest-neighbor searches to retrieve similar pieces of text based on their semantic meaning.</p>
<h4 class="h4" id="ch02lev2sec5">Pinecone</h4>
<p>Pinecone is a vector database that is designed for small to medium-sized datasets (usually ideal for less than 1 million entries). It is easy to get started with Pinecone for free, but it also has a pricing plan that provides additional features and increased scalability. Pinecone is optimized for fast vector search and retrieval, making it a great choice for applications that require low-latency search, such as recommendation systems, search engines, and chatbots.</p>
<h4 class="h4" id="ch02lev2sec6">Open-source Alternatives</h4>
<p>There are several open-source alternatives to Pinecone that can be used to build a vector database for LLM embeddings. One such alternative is Pgvector, a PostgreSQL extension that adds support for vector data types and provides fast vector operations. Another option is Weaviate, a cloud-native, open-source vector database that is designed for machine learning applications. Weaviate provides support for semantic search and can be integrated with other machine learning tools such as TensorFlow and PyTorch. ANNOY is an open-source library for approximate nearest neighbor search that is optimized for large-scale datasets. It can be used to build a custom vector database that is tailored to specific use cases.</p>
<h4 class="h4" id="ch02lev2sec7">Re-ranking the Retrieved Results</h4>
<p>After retrieving potential results from a vector database given a query using a similarity like cosine similarity, it is often useful to re-rank them to ensure that the most relevant results are presented to the user (<a href="ch02.html#ch02fig09">Figure 2.9</a>). One way to re-rank results is by using a cross-encoder, which is a type of transformer model that takes pairs of input sequences and predicts a score indicating how relevant the second sequence is to the first. By using a cross-encoder to re-rank search results, we can take into account the entire query context rather than just individual keywords. This of course will add some overhead and worsen our latency but it could help us in terms of performance. I will take the time to outline some results in a later section to compare and contrast using and not using a cross-encoder.</p>
<div class="group">
<div class="image" id="ch02fig09"><img src="graphics/02fig09.jpg" alt="Images" width="772" height="236"/></div>
<p class="fig-caption"><strong>Figure 2.9</strong> <em>A cross-encoder (left) takes in two pieces of text and outputs a similarity score without returning a vectorized format of the text. A bi-encoder (right), on the other hand, embeds a bunch of pieces of text into vectors up front and then retrieves them later in real time given a query (e.g. looking up “I’m a Data Scientist”)</em></p>
</div>
<p>One popular source of cross-encoder models is the Sentence Transformers library, which is where we found our bi-encoders earlier. We can also fine-tune a pre-trained cross-encoder model on our task-specific dataset to improve the relevance of search results and provide more accurate recommendations.</p>
<p>Another option for re-ranking search results is by using a traditional retrieval model like BM25, which ranks results by the frequency of query terms in the document and takes into account term proximity and inverse document frequency. While BM25 does not take into account the entire query context, it can still be a useful way to re-rank search results and improve the overall relevance of the results.</p>
<h4 class="h4" id="ch02lev2sec8">API</h4>
<p>We now need a place to put all of these components so that users can access the documents in a fast, secure, and easy way. To do this, let’s create an API.</p>
<h5 class="h5" id="ch01lev3sec7">FastAPI</h5>
<p><strong>FastAPI</strong> is a web framework for building APIs with Python quickly. It is designed to be both fast and easy to set up, making it an excellent choice for our semantic search API. FastAPI uses the Pydantic data validation library to validate request and response data and uses the high-performance ASGI server, uvicorn.</p>
<p>Setting up a FastAPI project is straightforward and requires minimal configuration. FastAPI provides automatic documentation generation with the OpenAPI standard, which makes it easy to build API documentation and client libraries. <a href="ch02.html#list2_7">Listing 2.7</a> is a skeleton of what that file would look like.</p>
<p class="ex-caption" id="list2_7"><strong>Listing 2.7</strong> <em>FastAPI skeleton code</em></p>
<div class="pre-box">
<pre><code>import hashlib
import os
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()

openai.api_key = os.environ.get('OPENAI_API_KEY', '')
pinecone_key = os.environ.get('PINECONE_KEY', '')

# Create an index in Pinecone with necessary properties

def my_hash(s):
    # Return the MD5 hash of the input string as a hexadecimal string
    return hashlib.md5(s.encode()).hexdigest()


class DocumentInputRequest(BaseModel):
    # define input to /document/ingest


class DocumentInputResponse(BaseModel):
    # define output from /document/ingest

class DocumentRetrieveRequest(BaseModel):
    # define input to /document/retrieve

class DocumentRetrieveResponse(BaseModel):
    # define output from /document/retrieve

# API route to ingest documents
@app.post("/document/ingest", response_model=DocumentInputResponse)
async def document_ingest(request: DocumentInputRequest):
    # Parse request data and chunk it
    # Create embeddings and metadata for each chunk
    # Upsert embeddings and metadata to Pinecone
    # Return number of upserted chunks
    return DocumentInputResponse(chunks_count=num_chunks)


# API route to retrieve documents
@app.post("/document/retrieve", response_model=DocumentRetrieveResponse)
async def document_retrieve(request: DocumentRetrieveRequest):
    # Parse request data and query Pinecone for matching embeddings
    # Sort results based on re-ranking strategy, if any
    # Return a list of document responses
    return DocumentRetrieveResponse(documents=documents)


if __name__ == "__main__":
    uvicorn.run("api:app", host="0.0.0.0", port=8000, reload=True)</code></pre>
</div>
<p>For the full file, be sure to check out the code repository for this book!</p>
<h3 class="h3" id="ch02lev1sec5">Putting It All Together</h3>
<p>We now have a solution for all of our components. Let’s take a look at where we are in our solution. Items in bold are new from the last time we outlined this solution.</p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> PART I - Ingesting documents</p>
<p class="bullet-number">1. Collect documents for embedding - <strong>Chunk them</strong></p>
<p class="bullet-number">2. Create text embeddings to encode semantic information - <strong>OpenAI’s Embedding</strong></p>
<p class="bullet-number">3. Store embeddings in a database for later retrieval given a query - <strong>Pinecone</strong></p>
<p class="bullet"><img src="graphics/square.jpg" alt="Images" width="6" height="6"/> PART II - Retrieving documents</p>
<p class="bullet-number">1. User has a query which may be pre-processed and cleaned - <strong>FastAPI</strong></p>
<p class="bullet-number">2. Retrieve candidate documents - <strong>OpenAI’s Embedding + Pinecone</strong></p>
<p class="bullet-number">3. Re-rank the candidate documents if necessary - <strong>Cross-Encoder</strong></p>
<p class="bullet-number">4. Return the final search results - <strong>FastAPI</strong></p>
<p>With all of these moving parts, let’s take a look at our final system architecture in <a href="ch02.html#ch02fig010">Figure 2.10</a>.</p>
<div class="group">
<div class="image" id="ch02fig010"><img src="graphics/02fig10.jpg" alt="Images" width="760" height="256"/></div>
<p class="fig-caption"><strong>Figure 2.10</strong> <em>Our complete semantic search architecture using two closed-source systems (OpenAI and Pinecone) and an open source API framework (FastAPI)</em></p>
</div>
<p>We now have a complete end to end solution for our semantic search. Let’s see how well the system performs against a validation set.</p>
<h4 class="h4" id="ch02lev2sec9">Performance</h4>
<p>I’ve outlined a solution to the problem of semantic search, but I want to also talk about how to test how these different components work together. For this, let’s use a well-known dataset to run against: the <strong>BoolQ</strong> dataset - a question answering dataset for yes/no questions containing nearly 16K examples. This dataset has pairs of (question, passage) that indicate for a given question, that passage would be the best passage to answer the question.</p>
<p><a href="ch02.html#ch02tab02">Table 2.2</a> outlines a few trials I ran and coded up in the code for this book. I use combinations of embedders, re-ranking solutions, and a bit of fine-tuning to try and see how well the system performs on two fronts:</p>
<p class="numbera">1. Performance - as indicated by the <strong>top result accuracy</strong>. For each known pair of (question, passage) in our BoolQ validation set - 3,270 examples, we will test if the system’s top result is the intended passage. This is not the only metric we could have used. The sentence_transformers library has other metrics including ranking evaluation, correlation evaluation, and more</p>
<p class="numbera">2. Latency - I want to see how long it takes to run through these examples using Pinecone, so for each embedder, I reset the index and uploaded new vectors and used cross-encoders in my laptop’s memory to keep things simple and standardized. I will measure latency in <strong>minutes</strong> it took to run against the validation set of the BoolQ dataset</p>
<div class="group">
<p class="tab-caption"><strong>Table 2.2</strong> <em>Performance results from various combinations against the BoolQ validation set</em></p>
<div class="imaget" id="ch02tab02"><img src="graphics/02tab02.jpg" alt="Images" width="779" height="1068"/></div>
<div class="imaget"><img src="graphics/02tab02a.jpg" alt="Images" width="779" height="619"/></div>
</div>
<p>Some experiments I didn’t try include the following:</p>
<p class="numbera">1. Fine-tuning the cross-encoder for more epochs and spending more time finding optimal learning parameters (e.g. weight decay, learning rate scheduler, etc)</p>
<p class="numbera">2. Using other OpenAI embedding engines</p>
<p class="numbera">3. Fine-tuning an open-source bi-encoder on the training set</p>
<p>Note that the models I used for the cross-encoder and the bi-encoder were both specifically pre-trained on data that is similar to asymmetric semantic search. This is important because we want the embedder to produce vectors for both short queries and long documents and place them near each other when they are related.</p>
<p>Let’s assume we want to keep things simple to get things off of the ground and use only the OpenAI embedder and do no re-ranking (row 1) in our application. Let’s consider the costs associated with using FastAPI, Pinecone, and OpenAI for text embeddings.</p>
<h3 class="h3" id="ch02lev1sec6">The Cost of Closed-Source</h3>
<p>We have a few components in play and not all of them are free. Fortunately FastAPI is an open-source framework and does not require any licensing fees. Our cost with FastAPI is hosting which could be on a free tier depending on what service we use. I like Render which has a free tier but also pricing starts at $7/month for 100% uptime. At the time of writing, Pinecone offers a free tier with a limit of 100,000 embeddings and up to 3 indexes, but beyond that, they charge based on the number of embeddings and indexes used. Their Standard plan charges $49/month for up to 1 million embeddings and 10 indexes.</p>
<p>OpenAI offers a free tier of their text embedding service, but it is limited to 100,000 requests per month. Beyond that, they charge $0.0004 per 1,000 tokens for the embedding engine we used - Ada-002. If we assume an average of 500 tokens per document, the cost per document would be $0.0002. For example, if we wanted to embed 1 million documents, it would cost approximately $200.</p>
<p>If we want to build a system with 1 million embeddings, and we expect to update the index once a month with totally fresh embeddings, the total cost per month would be:</p>
<p>Pinecone Cost = $49</p>
<p>OpenAI Cost  = $200</p>
<p>FastAPI Cost = $7</p>
<p>Total Cost = $49 + $200 + $7 = <strong>$256/month</strong></p>
<p>A nice binary number :) Not intended but still fun.</p>
<p>These costs can quickly add up as the system scales, and it may be worth exploring open-source alternatives or other strategies to reduce costs - like using open-source bi-encoders for embedding or Pgvector as your vector database.</p>
<h3 class="h3" id="ch02lev1sec7">Summary</h3>
<p>With all of these components accounted for, our pennies added up, and alternatives available at every step of the way, I’ll leave you all to it. Enjoy setting up your new semantic search system and be sure to check out the complete code for this - including a fully working FastAPI app with instructions on how to deploy it - on the book’s code repository and experiment to your heart’s content to try and make this work as well as possible for your domain-specific data.</p>
<p>Stay tuned for our next chapter where we will build on this API with a chatbot built using GPT-4 and our retrieval system.</p>
</div>
</div>
</body></html>