<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <title>ch011.xhtml</title>
  <style>
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
/*
 * Custom CSS file. Override it as you like.
 *
 * Credits to @killercup (https://gist.github.com/killercup); Extracted from this Gist:
 *   https://gist.github.com/killercup/5917178
 * Substantial modifications made by natolambert
 */

html {
    font-size: 100%;
    overflow-y: scroll;
    -webkit-text-size-adjust: 100%;
    -ms-text-size-adjust: 100%;
}

body {
    color: #444;
    font-family: Georgia, Palatino, 'Palatino Linotype', Times, 'Times New Roman', serif;
    font-size: 12px;
    line-height: 1.7;
    padding: 1em;
    margin: auto;
    max-width: 42em;
    background: #fefefe;
}

a {
    color: #0645ad;
    text-decoration: none;
}

a:visited {
    color: #0b0080;
}

a:hover {
    color: #06e;
}

a:active {
    color: #faa700;
}

a:focus {
    outline: thin dotted;
}

*::-moz-selection {
    background: rgba(255, 255, 0, 0.3);
    color: #000;
}

*::selection {
    background: rgba(255, 255, 0, 0.3);
    color: #000;
}

a::-moz-selection {
    background: rgba(255, 255, 0, 0.3);
    color: #0645ad;
}

a::selection {
    background: rgba(255, 255, 0, 0.3);
    color: #0645ad;
}

p {
    margin: 1em 0;
}

img {
    max-width: 100%;
}

h1,
h2,
h3,
h4,
h5,
h6 {
    color: #111;
    line-height: 125%;
    margin-top: 2em;
    font-weight: normal;
    position: relative;
}

/* Heading anchor link styles */
.header-anchor {
    opacity: 0;
    font-size: 0.8em;
    vertical-align: middle;
    position: absolute;
    margin-left: 0.3em;
    transition: opacity 0.2s ease-in-out;
}

h2:hover .header-anchor,
h3:hover .header-anchor,
h4:hover .header-anchor,
h5:hover .header-anchor,
h6:hover .header-anchor {
    opacity: 1;
}

h4,
h5,
h6 {
    font-weight: bold;
}

h1 {
    font-size: 2.5em;
}

h1.title {
    hyphens: none;
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    word-break: keep-all;
}

h2 {
    font-size: 2em;
}

h3 {
    font-size: 1.5em;
}

h4 {
    font-size: 1.2em;
}

h5 {
    font-size: 1em;
}

h6 {
    font-size: 0.9em;
}

blockquote {
    color: #666666;
    margin: 0;
    padding-left: 3em;
    border-left: 0.5em #EEE solid;
}

hr {
    display: block;
    height: 2px;
    border: 0;
    border-top: 1px solid #aaa;
    border-bottom: 1px solid #eee;
    margin: 1em 0;
    padding: 0;
}

pre,
code,
kbd,
samp {
    color: #000;
    font-family: monospace, monospace;
    _font-family: 'courier new', monospace;
    font-size: 0.98em;
}

pre {
    white-space: pre;
    white-space: pre-wrap;
    word-wrap: break-word;
}


b,
strong {
    font-weight: bold;
}

dfn {
    font-style: italic;
}

ins {
    background: #ff9;
    color: #000;
    text-decoration: none;
}

mark {
    background: #ff0;
    color: #000;
    font-style: italic;
    font-weight: bold;
}

sub,
sup {
    font-size: 75%;
    line-height: 0;
    position: relative;
    vertical-align: baseline;
}

sup {
    top: -0.5em;
}

sub {
    bottom: -0.25em;
}

ul,
ol {
    margin: 1em 0;
    padding: 0 0 0 2em;
}

li p:last-child {
    margin-bottom: 0;
}

ul ul,
ol ol {
    margin: .3em 0;
}

dl {
    margin-bottom: 1em;
}

dt {
    font-weight: bold;
    margin-bottom: .8em;
}

dd {
    margin: 0 0 .8em 2em;
}

dd:last-child {
    margin-bottom: 0;
}

img {
    border: 0;
    -ms-interpolation-mode: bicubic;
    vertical-align: middle;
}

figure {
    display: block;
    text-align: center;
    margin: 1em 0;
}

figure img {
    border: none;
    margin: 0 auto;
}

figcaption {
    font-size: 0.8em;
    font-style: italic;
    margin: 0 0 .8em;
}

/* for html tables */
table {
    margin-bottom: 2em;
    border-bottom: 1px solid #ddd;
    border-right: 1px solid #ddd;
    border-spacing: 0;
    box-shadow: none;
    border: none;
    border-collapse: collapse;
    margin-left: auto;
    margin-right: auto;
    width: auto; /* Keeps natural width; wrapper handles overflow */
    display: table;
}

th {
    padding: 12px;
    text-align: center;
    background-color: #eee;
    border: 1px solid #ddd;
}

td {
    padding: 12px;
    text-align: left; /* Keeps data cells left-aligned */
    border: 1px solid #ddd;
    vertical-align: top;
}

.table-scroll {
    max-width: 100%;
    overflow-x: auto;
    -webkit-overflow-scrolling: touch;
    margin: 1.5em auto 2em;
}

.table-scroll table {
    margin: 0 auto;
    display: table;
    width: auto;
    width: fit-content;
    width: max-content;
}

.table-wrap {
    margin: 1.5em auto 2em;
}

.table-wrap table {
    margin: 0 auto;
}

.table-scroll::-webkit-scrollbar {
    height: 8px;
}

.table-scroll::-webkit-scrollbar-thumb {
    background-color: rgba(0, 0, 0, 0.2);
    border-radius: 4px;
}

.table-scroll::-webkit-scrollbar-track {
    background: rgba(0, 0, 0, 0.05);
}

.author {
    font-size: 1.2em;
    text-align: center;
}

/* Target only mobile screens */
@media only screen and (max-width: 479px) {
    body {
        font-size: 14px;
    }
}

@media only screen and (min-width: 480px) {
    body {
        font-size: 15px;
    }
}

@media only screen and (min-width: 768px) {
    body {
        font-size: 16px;
    }
}

@media print {
    * {
        background: transparent !important;
        color: black !important;
        filter: none !important;
        -ms-filter: none !important;
    }
    body {
        font-size: 12pt;
        max-width: 100%;
    }
    a,
    a:visited {
        text-decoration: underline;
    }
    hr {
        height: 1px;
        border: 0;
        border-bottom: 1px solid black;
    }
    a[href]:after {
        content: " (" attr(href) ")";
    }
    abbr[title]:after {
        content: " (" attr(title) ")";
    }
    .ir a:after,
    a[href^="javascript:"]:after,
    a[href^="#"]:after {
        content: "";
    }
    pre,
    blockquote {
        border: 1px solid #999;
        padding-right: 1em;
        page-break-inside: avoid;
    }
    tr,
    img {
        page-break-inside: avoid;
    }
    img {
        max-width: 100% !important;
    }
    @page :left {
        margin: 15mm 20mm 15mm 10mm;
    }
    @page :right {
        margin: 15mm 10mm 15mm 20mm;
    }
    p,
    h2,
    h3 {
        orphans: 3;
        widows: 3;
    }
    h2,
    h3 {
        page-break-after: avoid;
    }
}

.dropdown-content {
  display: none;
}



thead {
    background-color: #f5f5f5;
}


.dropdown-content.open {
  display: block;
  max-height: 2000px;
}
/* Header Nav Block */
    .chapter-nav {
        display: grid;
        grid-template-columns: repeat(3, 1fr);
        /* grid-template-rows: auto auto;  */
        gap: 0.5rem;
        padding: 0.5rem;
        max-width: 1200px;
        text-align: left;
    }  
    .section {
        background-color: #ffffff;
        padding-top: 5px;
        padding-right: 12px;
        padding-bottom: 8px;
        padding-left: 12px;
        border-radius: 5px;
        text-align: left;
    }
  .dropdown-content {
    max-height: 0;
    overflow: hidden;
    transition: max-height 0.3s ease-out;
    background: #f8f8f8;  /* Unified with section background */
  }
  /* dropdown row */
  .dropdown-button {
    width: 100%;
    text-align: left;
    padding: 0.5rem;
    background: #f8f8f8;
    display: flex;
    align-items: center;
    gap: 0.5rem;
    cursor: pointer;
    border: none;
    font-size: 0.9rem;
  }
  /* carrot button */
  .dropdown-button .chevron {
    width: 14px;
    height: 14px;
    transition: transform 0.2s;
  }
  /* dropdown animation */
  .dropdown-button[aria-expanded="true"] .chevron {
    transform: rotate(180deg);
  }
  .section h3 {
    font-weight: bold;
    font-size: 0.8rem;
    margin-top: 5px;
    margin-bottom: 5px;  /* or whatever bottom spacing you prefer */
  }
  .section ol, .section ul {
    margin: 0;
    padding-left: 20px;
    text-align: left;
  }
  .section li {
    font-size: 12px;
    line-height: 1.3;
    margin-bottom: 3px;
    text-align: left;
  }
  .section a {
    color: #0066cc;
    text-decoration: none;
  }
  .section a:hover {
    text-decoration: underline;
  }

  /* Mobile Responsiveness */
  @media screen and (max-width: 768px) {
    .chapter-nav {
      grid-template-columns: 1fr;
    }
    .section {
      margin-bottom: 10px;
    }
    .section p {
      font-size: 16px;
    }
    .section li {
      font-size: 14px;
    }
  }  </style>
  <link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css" />
</head>
<body epub:type="bodymatter">
<section id="reinforcement-learning-i.e.-policy-gradient-algorithms" class="level1">
<h1>Reinforcement Learning (i.e.¬†Policy Gradient Algorithms)</h1>
<p>In the RLHF process, the reinforcement learning algorithm slowly updates the model‚Äôs weights with respect to feedback from a reward model. The policy ‚Äì the model being trained ‚Äì generates completions to prompts in the training set, then the reward model scores them, and then the reinforcement learning optimizer takes gradient steps based on this information. This chapter explains the mathematics and trade-offs across various algorithms used to learn from the signal the reward model gives to on-policy data. These algorithms are run for a period of many epochs, often thousands or millions of batches across a larger set of prompts, with gradient updates in between each of them.</p>
<p>The algorithms that popularized RLHF for language models were policy-gradient reinforcement learning algorithms. These algorithms, such as Proximal Policy Optimization (PPO), Group Relative Policy Optimization (GRPO), and REINFORCE, use recently generated samples to update their model (rather than storing scores in a replay buffer like algorithms, e.g.¬†Deep Q-Networks, DQN, used in popular projects such as AlphaGo). In this section we will cover the fundamentals of the policy gradient algorithms and how they are used in the modern RLHF framework.</p>
<p>At a machine learning level, this section is the subject with the highest complexity in the RLHF process. Though, as with most modern AI models, the largest determining factor on its success is the data provided as inputs to the process.</p>
<!-- The most popular algorithms used for RLHF have evolved over time. -->
<p>When RLHF came onto the scene with ChatGPT, it was largely known that they used a variant of PPO, and many initial efforts were built upon that. Over time, multiple research projects showed the promise of REINFORCE-style algorithms <span class="citation" data-cites="ahmadian2024back"><a href="ch021.xhtml#ref-ahmadian2024back">[178]</a></span> <span class="citation" data-cites="wang2024helpsteer2p"><a href="ch021.xhtml#ref-wang2024helpsteer2p">[111]</a></span>, touted for its simplicity over PPO without a reward model (saves memory and therefore the number of GPUs required) and with simpler value estimation (no Generalized Advantage Estimation, GAE, which is a method to compute advantages used for variance reduction in policy gradient algorithms). More algorithms have emerged, including Group Relative Policy Optimization, which is particularly popular with reasoning tasks, but in general many of these algorithms can be tuned to fit a specific task. In this chapter, we cover the core policy gradient setup and the three algorithms mentioned above due to their central role in the establishment of a canonical RLHF literature.</p>
<p>For definitions of symbols, see the problem setup chapter.</p>
<p><em>This chapter uses <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(s, a)</annotation></semantics></math> notation from the reinforcement learning literature, where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math> denotes states and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics></math> denotes actions. In the language model context, you will often see <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(x, y)</annotation></semantics></math> instead, where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> is the prompt and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math> is the completion. The <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(s, a)</annotation></semantics></math> framing is more general‚Äîthese algorithms were designed for sequential decision problems where actions are taken at each timestep. However, many RLHF implementations treat the entire completion as a single action, making the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(x, y)</annotation></semantics></math> notation equally valid.</em></p>
<section id="policy-gradient-algorithms" class="level2">
<h2>Policy Gradient Algorithms</h2>
<p>Reinforcement learning algorithms are designed to maximize the future, discounted reward across a trajectory of states, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mo>‚àà</mo><mi>ùíÆ</mi></mrow><annotation encoding="application/x-tex">s \in \mathcal{S}</annotation></semantics></math>, and actions, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>‚àà</mo><mi>ùíú</mi></mrow><annotation encoding="application/x-tex">a \in \mathcal{A}</annotation></semantics></math> (for more notation, see Chapter 3, Definitions). The objective of the agent, often called the <em>return</em>, is the sum of discounted future rewards (where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ≥</mi><mo>‚àà</mo><mo stretchy="false" form="prefix">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">\gamma\in [0,1]</annotation></semantics></math> is a factor that prioritizes near-term rewards) at a given time <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>:</p>
<p><span id="eq:return_definition"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>G</mi><mi>t</mi></msub><mo>=</mo><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>Œ≥</mi><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>2</mn></mrow></msub><mo>+</mo><mi>‚ãØ</mi><mo>=</mo><munderover><mo>‚àë</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mo accent="false">‚àû</mo></munderover><msup><mi>Œ≥</mi><mi>k</mi></msup><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mi>.</mi><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>33</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">G_t = R_{t+1} + \gamma R_{t+2} + \cdots = \sum_{k=0}^\infty \gamma^k R_{t+k+1}.\qquad{(33)}</annotation></semantics></math></span></p>
<p>The return definition can also be estimated as: <span id="eq:recursive_return"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>G</mi><mi>t</mi></msub><mo>=</mo><mi>Œ≥</mi><msub><mi>G</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mi>.</mi><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>34</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">G_{t} = \gamma{G_{t+1}} + R_{t+1}.\qquad{(34)}</annotation></semantics></math></span></p>
<p>This return is the basis for learning a value function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">V(s)</annotation></semantics></math> that is the estimated future return given a current state:</p>
<p><span id="eq:value_function"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>ùîº</mi><mo minsize="120%" maxsize="120%" stretchy="true" form="prefix">[</mo><msub><mi>G</mi><mi>t</mi></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi><mo minsize="120%" maxsize="120%" stretchy="true" form="postfix">]</mo><mi>.</mi><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>35</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">V(s) = \mathbb{E}\big[G_t | S_t = s \big].\qquad{(35)}</annotation></semantics></math></span></p>
<p>All policy gradient algorithms optimize a policy <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo>‚à£</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\pi_\theta(a\mid s)</annotation></semantics></math> to maximize expected return; this objective can be expressed using the induced value function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>V</mi><msub><mi>œÄ</mi><mi>Œ∏</mi></msub></msup><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">V^{\pi_\theta}(s)</annotation></semantics></math>.</p>
<p>Where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>d</mi><msub><mi>œÄ</mi><mi>Œ∏</mi></msub></msup><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">d^{\pi_\theta}(s)</annotation></semantics></math> is the state-visitation distribution induced by policy <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo>‚à£</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\pi_\theta(a \mid s)</annotation></semantics></math>, the objective we maximize can be written as: <span id="eq:policy_objective"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><mspace width="0.278em"></mspace><mo>=</mo><mspace width="0.278em"></mspace><munder><mo>‚àë</mo><mi>s</mi></munder><msup><mi>d</mi><msub><mi>œÄ</mi><mi>Œ∏</mi></msub></msup><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo><msup><mi>V</mi><msub><mi>œÄ</mi><mi>Œ∏</mi></msub></msup><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo><mo>,</mo><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>36</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
J(\theta)
\;=\;
\sum_{s} d^{\pi_\theta}(s) V^{\pi_\theta}(s),
\qquad{(36)}</annotation></semantics></math></span></p>
<p>In a finite MDP this is a sum over all states, but in practice we never compute it exactly. Instead, we estimate it from data by sampling rollouts from the current policy. In RLHF this typically means sampling prompts <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding="application/x-tex">x_i</annotation></semantics></math> from a dataset and generating completions <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>‚àº</mo><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>‚ãÖ</mi><mo>‚à£</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">y_i \sim \pi_\theta(\cdot\mid x_i)</annotation></semantics></math>, then taking an empirical average such as:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>J</mi><mo accent="true">ÃÇ</mo></mover><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>B</mi></mfrac><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><mi>R</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>,</mo></mrow><annotation encoding="application/x-tex">
\hat{J}(\theta) = \frac{1}{B}\sum_{i=1}^{B} R(x_i, y_i),
</annotation></semantics></math></p>
<p>or, in an MDP view with per-step rewards,</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>J</mi><mo accent="true">ÃÇ</mo></mover><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>B</mi></mfrac><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><munderover><mo>‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><msub><mi>T</mi><mi>i</mi></msub></munderover><msup><mi>Œ≥</mi><mi>t</mi></msup><msub><mi>r</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mi>.</mi></mrow><annotation encoding="application/x-tex">
\hat{J}(\theta) = \frac{1}{B}\sum_{i=1}^{B} \sum_{t=0}^{T_i} \gamma^t r_{i,t}.
</annotation></semantics></math></p>
<p>The core of policy gradient algorithms is computing the gradient with respect to the finite-time expected return over the current policy. With this expected return, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>J</mi><annotation encoding="application/x-tex">J</annotation></semantics></math>, the parameter update can be computed as follows, where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ±</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> is the learning rate:</p>
<p><span id="eq:policy_update"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ∏</mi><mo>‚Üê</mo><mi>Œ∏</mi><mo>+</mo><mi>Œ±</mi><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>37</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)\qquad{(37)}</annotation></semantics></math></span></p>
<p>The core implementation detail is how to compute said gradient.</p>
<p>Another way to pose the RL objective we want to maximize is as follows: <span id="eq:policy_objective_expectation"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ùîº</mi><mrow><mi>œÑ</mi><mo>‚àº</mo><msub><mi>œÄ</mi><mi>Œ∏</mi></msub></mrow></msub><mrow><mo stretchy="true" form="prefix">[</mo><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>œÑ</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow><mo>,</mo><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>38</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ R(\tau) \right],
\qquad{(38)}</annotation></semantics></math></span></p>
<p>where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>œÑ</mi><mo>=</mo><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo>,</mo><msub><mi>a</mi><mn>0</mn></msub><mo>,</mo><msub><mi>s</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><mi>‚Ä¶</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\tau = (s_0, a_0, s_1, a_1, \ldots)</annotation></semantics></math> is a trajectory and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>œÑ</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msubsup><mo>‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>‚àû</mi></msubsup><msub><mi>r</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">R(\tau) = \sum_{t=0}^\infty r_t</annotation></semantics></math> is the total reward of the trajectory. Alternatively, we can write the expectation as an integral over all possible trajectories: <span id="eq:policy_objective_integral"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mo>‚à´</mo><mi>œÑ</mi></msub><msub><mi>p</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>œÑ</mi><mo stretchy="false" form="postfix">)</mo><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>œÑ</mi><mo stretchy="false" form="postfix">)</mo><mi>d</mi><mi>œÑ</mi><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>39</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
J(\theta) = \int_\tau p_\theta (\tau) R(\tau) d\tau
\qquad{(39)}</annotation></semantics></math></span></p>
<p>Notice that we can express the trajectory probability as follows, where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mi>p</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\pi_\theta(a_t|s_t) p(s_{t+1}|s_t, a_t)</annotation></semantics></math> is the transition probability to a group of next states from one state and action: <span id="eq:trajectory_probability"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>œÑ</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>p</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo stretchy="false" form="postfix">)</mo><munderover><mo>‚àè</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mo accent="false">‚àû</mo></munderover><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mi>p</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>,</mo><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>40</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
p_\theta (\tau) = p(s_0) \prod_{t=0}^\infty \pi_\theta(a_t|s_t) p(s_{t+1}|s_t, a_t),
\qquad{(40)}</annotation></semantics></math></span></p>
<p>If we take the gradient of the objective (eq.¬†<a href="ch011.xhtml#eq:policy_objective_expectation">38</a>) with respect to the policy parameters <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ∏</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>: <span id="eq:policy_gradient_integral"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mo>‚à´</mo><mi>œÑ</mi></msub><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><msub><mi>p</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>œÑ</mi><mo stretchy="false" form="postfix">)</mo><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>œÑ</mi><mo stretchy="false" form="postfix">)</mo><mi>d</mi><mi>œÑ</mi><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>41</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
\nabla_\theta J(\theta) = \int_\tau \nabla_\theta p_\theta (\tau) R(\tau) d\tau
\qquad{(41)}</annotation></semantics></math></span></p>
<p>Notice that we can use the <a href="https://andrewcharlesjones.github.io/journal/log-derivative.html">log-derivative trick</a> in order to rewrite the gradient of the integral as an expectation: <span id="eq:log_chain_rule"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtable><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><msub><mi>p</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>œÑ</mi><mo stretchy="false" form="postfix">)</mo></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mfrac><mrow><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><msub><mi>p</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>œÑ</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>p</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>œÑ</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac></mtd><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">(from chain rule)</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mo>‚üπ</mo><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><msub><mi>p</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>œÑ</mi><mo stretchy="false" form="postfix">)</mo></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msub><mi>p</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>œÑ</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><msub><mi>p</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>œÑ</mi><mo stretchy="false" form="postfix">)</mo></mtd><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">(rearranging)</mtext></mtd></mtr></mtable><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>42</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
\begin{aligned}
\nabla_\theta \log p_\theta(\tau) &amp;= \frac{\nabla_\theta p_\theta(\tau)}{p_\theta(\tau)} &amp;\text{(from chain rule)} \\
\implies \nabla_\theta p_\theta(\tau) &amp;= p_\theta(\tau) \nabla_\theta \log p_\theta(\tau) &amp;\text{(rearranging)}
\end{aligned}
\qquad{(42)}</annotation></semantics></math></span></p>
<p>Using this log-derivative trick: <span id="eq:policy_gradient_expectation"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtable><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><msub><mo>‚à´</mo><mi>œÑ</mi></msub><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><msub><mi>p</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>œÑ</mi><mo stretchy="false" form="postfix">)</mo><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>œÑ</mi><mo stretchy="false" form="postfix">)</mo><mi>d</mi><mi>œÑ</mi></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><msub><mo>‚à´</mo><mi>œÑ</mi></msub><msub><mi>p</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>œÑ</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><msub><mi>p</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>œÑ</mi><mo stretchy="false" form="postfix">)</mo><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>œÑ</mi><mo stretchy="false" form="postfix">)</mo><mi>d</mi><mi>œÑ</mi></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><msub><mi>ùîº</mi><mrow><mi>œÑ</mi><mo>‚àº</mo><msub><mi>œÄ</mi><mi>Œ∏</mi></msub></mrow></msub><mrow><mo stretchy="true" form="prefix">[</mo><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mi mathvariant="normal">log</mi><msub><mi>p</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>œÑ</mi><mo stretchy="false" form="postfix">)</mo><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>œÑ</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow></mtd></mtr></mtable><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>43</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
\begin{aligned}
\nabla_\theta J(\theta) &amp;= \int_\tau \nabla_\theta p_\theta (\tau) R(\tau) d\tau \\
&amp;= \int_\tau p_\theta (\tau) \nabla_\theta \log p_\theta (\tau) R(\tau) d\tau \\
&amp;= \mathbb{E}_{\tau \sim \pi_\theta} \left[ \nabla_\theta \log p_\theta (\tau) R(\tau) \right]
\end{aligned}
\qquad{(43)}</annotation></semantics></math></span></p>
<p>Where the final step uses the definition of an expectation under the trajectory distribution <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>œÑ</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">p_\theta(\tau)</annotation></semantics></math>: for any function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ùîº</mi><mrow><mi>œÑ</mi><mo>‚àº</mo><msub><mi>p</mi><mi>Œ∏</mi></msub></mrow></msub><mo stretchy="false" form="prefix">[</mo><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>œÑ</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">]</mo><mo>=</mo><msub><mo>‚à´</mo><mi>œÑ</mi></msub><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>œÑ</mi><mo stretchy="false" form="postfix">)</mo><mspace width="0.167em"></mspace><msub><mi>p</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>œÑ</mi><mo stretchy="false" form="postfix">)</mo><mspace width="0.167em"></mspace><mi>d</mi><mi>œÑ</mi></mrow><annotation encoding="application/x-tex">\mathbb{E}_{\tau \sim p_\theta}[f(\tau)] = \int_\tau f(\tau)\,p_\theta(\tau)\,d\tau</annotation></semantics></math> (or a sum in the discrete case). Writing it as an expectation is useful because we can approximate it with Monte Carlo rollouts, e.g., <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mi>B</mi></mfrac><msubsup><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></msubsup><mi>f</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>œÑ</mi><mi>i</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\frac{1}{B}\sum_{i=1}^{B} f(\tau_i)</annotation></semantics></math> for trajectories <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>œÑ</mi><mi>i</mi></msub><mo>‚àº</mo><msub><mi>œÄ</mi><mi>Œ∏</mi></msub></mrow><annotation encoding="application/x-tex">\tau_i \sim \pi_\theta</annotation></semantics></math>.</p>
<p>Back to the derivation, expanding the log probability of the trajectory:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><msub><mi>p</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>œÑ</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><mi>p</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo stretchy="false" form="postfix">)</mo><mo>+</mo><munderover><mo>‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mo accent="false">‚àû</mo></munderover><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>+</mo><munderover><mo>‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mo accent="false">‚àû</mo></munderover><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><mi>p</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">
\log p_\theta (\tau) = \log p(s_0) + \sum_{t=0}^\infty \log \pi_\theta(a_t|s_t) + \sum_{t=0}^\infty \log p(s_{t+1}|s_t, a_t)
</annotation></semantics></math></p>
<p>Now, if we take the gradient of the above, we get:</p>
<ul>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><mi>p</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\nabla_\theta \log p(s_0) = 0</annotation></semantics></math> (initial state doesn‚Äôt depend on <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ∏</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>)</li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><mi>p</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\nabla_\theta \log p(s_{t+1}|s_t, a_t) = 0</annotation></semantics></math> (environment transition dynamics don‚Äôt depend on <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ∏</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>)</li>
<li>only <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\nabla_\theta \log \pi_\theta(a_t|s_t)</annotation></semantics></math> survives</li>
</ul>
<p>Therefore, the gradient of the log probability of the trajectory simplifies to: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><msub><mi>p</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>œÑ</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><munderover><mo>‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mo accent="false">‚àû</mo></munderover><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">
\nabla_\theta \log p_\theta (\tau) = \sum_{t=0}^\infty \nabla_\theta \log \pi_\theta(a_t|s_t)
</annotation></semantics></math></p>
<p>Substituting this back in eq.¬†<a href="ch011.xhtml#eq:policy_gradient_expectation">43</a>, we get: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ùîº</mi><mrow><mi>œÑ</mi><mo>‚àº</mo><msub><mi>œÄ</mi><mi>Œ∏</mi></msub></mrow></msub><mrow><mo stretchy="true" form="prefix">[</mo><munderover><mo>‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mo accent="false">‚àû</mo></munderover><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mi mathvariant="normal">log</mi><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>œÑ</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^\infty \nabla_\theta \log \pi_\theta(a_t|s_t) R(\tau) \right]
</annotation></semantics></math></p>
<p>Quite often, people use a more general formulation of the policy gradient: <span id="eq:general_gradient"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mo>=</mo><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ùîº</mi><mrow><mi>œÑ</mi><mo>‚àº</mo><msub><mi>œÄ</mi><mi>Œ∏</mi></msub></mrow></msub><mrow><mo stretchy="true" form="prefix">[</mo><munderover><mo>‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mo accent="false">‚àû</mo></munderover><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mi mathvariant="normal">log</mi><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><msub><mi mathvariant="normal">Œ®</mi><mi>t</mi></msub><mo stretchy="true" form="postfix">]</mo></mrow><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>44</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
g = \nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^\infty \nabla_\theta \log \pi_\theta(a_t|s_t) \Psi_t \right]
\qquad{(44)}</annotation></semantics></math></span></p>
<p>Where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi mathvariant="normal">Œ®</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\Psi_t</annotation></semantics></math> can be the following (where the rewards can also often be discounted by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ≥</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math>), a taxonomy adopted from Schulman et al.¬†2015 <span class="citation" data-cites="schulman2015high"><a href="ch021.xhtml#ref-schulman2015high">[179]</a></span>:</p>
<ol type="1">
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>œÑ</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msubsup><mo>‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>‚àû</mi></msubsup><msub><mi>r</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">R(\tau) = \sum_{t=0}^{\infty} r_t</annotation></semantics></math>: total reward of the trajectory.</li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mo>‚àë</mo><mrow><msup><mi>t</mi><mo>‚Ä≤</mo></msup><mo>=</mo><mi>t</mi></mrow><mi>‚àû</mi></msubsup><msub><mi>r</mi><msup><mi>t</mi><mo>‚Ä≤</mo></msup></msub></mrow><annotation encoding="application/x-tex">\sum_{t&#39;=t}^{\infty} r_{t&#39;}</annotation></semantics></math>: reward following action <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>a</mi><mi>t</mi></msub><annotation encoding="application/x-tex">a_t</annotation></semantics></math>, also described as the return, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math>.</li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mo>‚àë</mo><mrow><msup><mi>t</mi><mo>‚Ä≤</mo></msup><mo>=</mo><mi>t</mi></mrow><mi>‚àû</mi></msubsup><msub><mi>r</mi><msup><mi>t</mi><mo>‚Ä≤</mo></msup></msub><mo>‚àí</mo><mi>b</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\sum_{t&#39;=t}^{\infty} r_{t&#39;} - b(s_t)</annotation></semantics></math>: baselined version of previous formula.</li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Q</mi><mi>œÄ</mi></msup><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">Q^{\pi}(s_t, a_t)</annotation></semantics></math>: state-action value function.</li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>A</mi><mi>œÄ</mi></msup><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">A^{\pi}(s_t, a_t)</annotation></semantics></math>: advantage function, which yields the lowest possible theoretical variance if it can be computed accurately.</li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mi>t</mi></msub><mo>+</mo><msup><mi>V</mi><mi>œÄ</mi></msup><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false" form="postfix">)</mo><mo>‚àí</mo><msup><mi>V</mi><mi>œÄ</mi></msup><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">r_t + V^{\pi}(s_{t+1}) - V^{\pi}(s_t)</annotation></semantics></math>: Temporal Difference (TD) residual.</li>
</ol>
<p>The <em>baseline</em> is a value used to reduce variance of policy updates (more on this below).</p>
<p>For language models, some of these concepts do not make as much sense. For example, for a deterministic policy <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>œÄ</mi><annotation encoding="application/x-tex">\pi</annotation></semantics></math> the state value is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>V</mi><mi>œÄ</mi></msup><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msup><mi>Q</mi><mi>œÄ</mi></msup><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">V^{\pi}(s) = Q^{\pi}(s, \pi(s))</annotation></semantics></math> (and for the optimal value function one has <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>V</mi><mo>*</mo></msup><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mrow><mi mathvariant="normal">max</mi><mo>&#8289;</mo></mrow><mi>a</mi></msub><msup><mi>Q</mi><mo>*</mo></msup><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">V^*(s)=\max_a Q^*(s,a)</annotation></semantics></math>). For a stochastic policy, the analogous identity is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>V</mi><mi>œÄ</mi></msup><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ùîº</mi><mrow><mi>a</mi><mo>‚àº</mo><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><mi>‚ãÖ</mi><mo>‚à£</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msub><mo stretchy="false" form="prefix">[</mo><msup><mi>Q</mi><mi>œÄ</mi></msup><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">V^{\pi}(s) = \mathbb{E}_{a \sim \pi(\cdot\mid s)}[Q^{\pi}(s,a)]</annotation></semantics></math>. If we define <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mo>+</mo><mi>a</mi></mrow><annotation encoding="application/x-tex">s+a</annotation></semantics></math> as the continuation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics></math> to the prompt <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math>, then <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>V</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>+</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">Q(s, a) = V(s+a)</annotation></semantics></math>, which gives a different advantage trick:</p>
<p><span id="eq:advantage_trick"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>Q</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo><mo>‚àí</mo><mi>V</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>V</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>+</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo><mo>‚àí</mo><mi>V</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>r</mi><mo>+</mo><mi>Œ≥</mi><mi>V</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>+</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo><mo>‚àí</mo><mi>V</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>45</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">A(s,a) = Q(s,a) - V(s) = V(s + a) - V(s) = r + \gamma V(s + a) - V(s)\qquad{(45)}</annotation></semantics></math></span></p>
<p>Which is a combination of the reward, the value of the prompt, and the discounted value of the entire utterance.</p>
<section id="vanilla-policy-gradient" class="level3">
<h3>Vanilla Policy Gradient</h3>
<p>The vanilla policy gradient implementation optimizes the above expression for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">J(\theta)</annotation></semantics></math> by differentiating with respect to the policy parameters. A simple version, with respect to the overall return, is:</p>
<p><span id="eq:vanilla_policy_gradient"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ùîº</mi><mi>œÑ</mi></msub><mrow><mo stretchy="true" form="prefix">[</mo><munderover><mo>‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>T</mi></munderover><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mi mathvariant="normal">log</mi><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><msub><mi>R</mi><mi>t</mi></msub><mo stretchy="true" form="postfix">]</mo></mrow><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>46</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\nabla_\theta J(\theta) = \mathbb{E}_\tau \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) R_t \right]\qquad{(46)}</annotation></semantics></math></span></p>
<p>A common problem with vanilla policy gradient algorithms is the high variance in gradient updates, which can be mitigated in multiple ways. The high variance comes from the gradient updates being computed from estimating the return <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> from an often small set of rollouts in the environment that tend to be susceptible to noise (e.g.¬†the stochastic nature of generating from language models with temperature <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">&gt;0</annotation></semantics></math>). The variance across return estimates is higher in domains with sparse rewards, as more of the samples are 0 or 1, rather than closely clustered. In order to alleviate this, various techniques are used to normalize the value estimation, called <em>baselines</em>. Baselines accomplish this in multiple ways, effectively normalizing by the value of the state relative to the downstream action (e.g.¬†in the case of Advantage, which is the difference between the Q value and the value). The simplest baselines are averages over the batch of rewards or a moving average. Even these baselines can de-bias the gradients so <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ùîº</mi><mrow><mi>a</mi><mo>‚àº</mo><mi>œÄ</mi><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="prefix">|</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msub><mo stretchy="false" form="prefix">[</mo><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="prefix">|</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">]</mo><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\mathbb{E}_{a \sim \pi(a|s)}[\nabla_\theta \log \pi_\theta(a|s)] = 0</annotation></semantics></math>, improving the learning signal substantially.</p>
<p>Many of the policy gradient algorithms discussed in this chapter build on the advantage formulation of policy gradient:</p>
<p><span id="eq:advantage_policy_gradient"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ùîº</mi><mi>œÑ</mi></msub><mrow><mo stretchy="true" form="prefix">[</mo><munderover><mo>‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>T</mi></munderover><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mi mathvariant="normal">log</mi><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><msup><mi>A</mi><msub><mi>œÄ</mi><mi>Œ∏</mi></msub></msup><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>47</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\nabla_\theta J(\theta) = \mathbb{E}_\tau \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) A^{\pi_\theta}(s_t, a_t) \right]\qquad{(47)}</annotation></semantics></math></span></p>
</section>
<section id="reinforce" class="level3">
<h3>REINFORCE</h3>
<p>The algorithm REINFORCE is likely a backronym, but the components of the algorithm it represents are quite relevant for modern reinforcement learning algorithms. Defined in the seminal paper <em>Simple statistical gradient-following algorithms for connectionist reinforcement learning</em> <span class="citation" data-cites="williams1992simple"><a href="ch021.xhtml#ref-williams1992simple">[180]</a></span>:</p>
<blockquote>
<p>The name is an acronym for ‚ÄúREward Increment = Nonnegative Factor X Offset Reinforcement X Characteristic Eligibility.‚Äù</p>
</blockquote>
<p>The three components of this are how to do the <em>reward increment</em>, a.k.a. the policy gradient step. It has three pieces to the update rule:</p>
<ol type="1">
<li>Nonnegative factor: This is the learning rate (step size) that must be a positive number, e.g.¬†<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ±</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> below.</li>
<li>Offset Reinforcement: This is a baseline <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics></math> or other normalizing factor of the reward to improve stability.</li>
<li>Characteristic Eligibility: This is how the learning becomes attributed per token. It can be a general value, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>e</mi><annotation encoding="application/x-tex">e</annotation></semantics></math> per parameter, but is often log probabilities of the policy in modern equations.</li>
</ol>
<p>Thus, the form looks quite familiar:</p>
<p><span id="eq:REINFORCE_BASIC"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="normal">Œî</mi><mi>Œ∏</mi></msub><mo>=</mo><mi>Œ±</mi><mo stretchy="false" form="prefix">(</mo><mi>r</mi><mo>‚àí</mo><mi>b</mi><mo stretchy="false" form="postfix">)</mo><mi>e</mi><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>48</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex"> \Delta_\theta = \alpha(r - b)e \qquad{(48)}</annotation></semantics></math></span></p>
<p>With more modern notation and the generalized return <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math>, the REINFORCE operator appears as:</p>
<p><span id="eq:REINFORCE_with_baseline"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mspace width="0.167em"></mspace><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><mspace width="0.278em"></mspace><mo>=</mo><mspace width="0.278em"></mspace><msub><mi>ùîº</mi><mrow><mi>œÑ</mi><mo>‚àº</mo><msub><mi>œÄ</mi><mi>Œ∏</mi></msub></mrow></msub><mspace width="-0.167em"></mspace><mo minsize="180%" maxsize="180%" stretchy="true" form="prefix">[</mo><munderover><mo>‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>T</mi></munderover><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>‚à£</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mspace width="0.167em"></mspace><mo stretchy="false" form="prefix">(</mo><msub><mi>G</mi><mi>t</mi></msub><mo>‚àí</mo><mi>b</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">)</mo><mo minsize="180%" maxsize="180%" stretchy="true" form="postfix">]</mo><mo>,</mo><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>49</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
\nabla_{\theta}\,J(\theta)
\;=\;
\mathbb{E}_{\tau \sim \pi_{\theta}}\!\Big[
    \sum_{t=0}^{T}
    \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t)\,(G_t - b(s_t))
\Big],
\qquad{(49)}</annotation></semantics></math></span></p>
<p>Here, the value <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>G</mi><mi>t</mi></msub><mo>‚àí</mo><mi>b</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">G_t - b(s_t)</annotation></semantics></math> is the <em>advantage</em> of the policy at the current state, so we can reformulate the policy gradient in a form that we continue later with the advantage, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>:</p>
<p><span id="eq:REINFORCE_with_advantage"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mspace width="0.167em"></mspace><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><mspace width="0.278em"></mspace><mo>=</mo><mspace width="0.278em"></mspace><msub><mi>ùîº</mi><mrow><mi>œÑ</mi><mo>‚àº</mo><msub><mi>œÄ</mi><mi>Œ∏</mi></msub></mrow></msub><mspace width="-0.167em"></mspace><mo minsize="180%" maxsize="180%" stretchy="true" form="prefix">[</mo><munderover><mo>‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>T</mi></munderover><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>‚à£</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mspace width="0.167em"></mspace><msub><mi>A</mi><mi>t</mi></msub><mo minsize="180%" maxsize="180%" stretchy="true" form="postfix">]</mo><mo>,</mo><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>50</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
\nabla_{\theta}\,J(\theta)
\;=\;
\mathbb{E}_{\tau \sim \pi_{\theta}}\!\Big[
    \sum_{t=0}^{T}
    \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t)\,A_t
\Big],
\qquad{(50)}</annotation></semantics></math></span></p>
<p>REINFORCE is a specific implementation of vanilla policy gradient that uses a Monte Carlo estimator of the gradient.</p>
<section id="reinforce-leave-one-out-rloo" class="level4">
<h4>REINFORCE Leave One Out (RLOO)</h4>
<p>The core implementation detail of REINFORCE Leave One Out versus standard REINFORCE is that it takes the average reward of the <em>other</em> samples in the batch to compute the baseline ‚Äì rather than averaging over all rewards in the batch <span class="citation" data-cites="huang2024putting"><a href="ch021.xhtml#ref-huang2024putting">[181]</a></span>, <span class="citation" data-cites="ahmadian2024back"><a href="ch021.xhtml#ref-ahmadian2024back">[178]</a></span>, <span class="citation" data-cites="kool2019buy"><a href="ch021.xhtml#ref-kool2019buy">[182]</a></span>.</p>
<p>Crucially, this only works when generating multiple trajectories (completions) per state (prompt), which is common practice in multiple domains of finetuning language models with RL.</p>
<p>Specifically, for the REINFORCE Leave-One-Out (RLOO) baseline, given <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math> sampled trajectories (actions taken conditioned on a prompt) <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><msub><mi>a</mi><mi>K</mi></msub></mrow><annotation encoding="application/x-tex">a_1, \dots, a_K</annotation></semantics></math>, to a given prompt <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math> we define the baseline explicitly as the following <em>per-prompt</em>:</p>
<p><span id="eq:RLOO_baseline"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><msub><mi>a</mi><mi>k</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mi>K</mi><mo>‚àí</mo><mn>1</mn></mrow></mfrac><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>i</mi><mo>‚â†</mo><mi>k</mi></mrow><mi>K</mi></munderover><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>,</mo><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>51</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
b(s, a_k) = \frac{1}{K-1}\sum_{i=1, i\neq k}^{K} R(s, a_i),
\qquad{(51)}</annotation></semantics></math></span></p>
<p>resulting in the advantage:</p>
<p><span id="eq:RLOO_advantage"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><msub><mi>a</mi><mi>k</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><msub><mi>a</mi><mi>k</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>‚àí</mo><mi>b</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><msub><mi>a</mi><mi>k</mi></msub><mo stretchy="false" form="postfix">)</mo><mi>.</mi><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>52</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
A(s, a_k) = R(s, a_k) - b(s, a_k).
\qquad{(52)}</annotation></semantics></math></span></p>
<p>Equivalently, this can be expressed as:</p>
<p><span id="eq:RLOO_advantage_alt"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><msub><mi>a</mi><mi>k</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mi>K</mi><mrow><mi>K</mi><mo>‚àí</mo><mn>1</mn></mrow></mfrac><mrow><mo stretchy="true" form="prefix">(</mo><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><msub><mi>a</mi><mi>k</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>‚àí</mo><mfrac><mn>1</mn><mi>K</mi></mfrac><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>53</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
A(s, a_k) = \frac{K}{K - 1}\left(R(s, a_k) - \frac{1}{K}\sum_{i=1}^{K} R(s, a_i)\right).
\qquad{(53)}</annotation></semantics></math></span></p>
<p>This is a simple, low-variance <em>per-prompt</em> advantage estimate that is closely related to the group-relative advantage used in Group Relative Policy Optimization, GRPO (discussed shortly, after Proximal Policy Optimization, PPO). In practice, GRPO-style training mainly differs in how it applies the KL regularizer (as an explicit loss term vs.¬†folded into the reward) and whether it uses PPO-style ratio clipping. To be specific, the canonical GRPO implementation applies the KL penalty at the loss level, where the derivation for RLOO or traditional policy-gradients apply the KL penalty to the reward itself. With the transition from RLHF to reasoning and reinforcement learning with verifiable rewards (RLVR), the prevalence of KL penalties has decreased overall, with many reasoning adaptations of RLHF code turning them off entirely. Still, the advantage from RLOO could be combined with the clipping of PPO, showing how similar many of these algorithms are.</p>
<p>RLOO and other algorithms that do not use a value network ‚Äì an additional model copy (a critic) that predicts a scalar value <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">V(s_t)</annotation></semantics></math> per token ‚Äì assign the same sequence-level advantage (or reward) to every token when computing the loss. Algorithms that use a learned value network, such as PPO, assign a different value to every token individually, discounting from the final reward achieved at the EOS token. With a KL distance penalty, RLOO aggregates the per-token KL over the completion and folds that scalar into the sequence reward, so the resulting advantage is broadcast to all tokens. PPO subtracts a per-token KL from the per-token reward before computing <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>A</mi><mi>t</mi></msub><annotation encoding="application/x-tex">A_t</annotation></semantics></math>, giving token-level credit assignment. GRPO typically retains a sequence-level advantage but adds a separate per-token term to the loss, rather than subtracting it from the reward. These details and trade-offs are discussed later in the chapter.</p>
<!-- A nice formulation of LM RL loss functions is found here https://arxiv.org/pdf/2502.01600 -->
</section>
</section>
<section id="proximal-policy-optimization" class="level3">
<h3>Proximal Policy Optimization</h3>
<p>Proximal Policy Optimization (PPO) <span class="citation" data-cites="schulman2017proximal"><a href="ch021.xhtml#ref-schulman2017proximal">[183]</a></span> is one of the foundational algorithms behind Deep RL‚Äôs successes (such as OpenAI‚Äôs Five, which mastered DOTA 2 <span class="citation" data-cites="berner2019dota"><a href="ch021.xhtml#ref-berner2019dota">[184]</a></span> and large amounts of research). The objective that PPO maximizes, with respect to the advantages and the policy probabilities, is as follows:</p>
<p><span id="eq:PPO_EQN"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mrow><mi mathvariant="normal">min</mi><mo>&#8289;</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="prefix">|</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><msub><mi>Œ∏</mi><mtext mathvariant="normal">old</mtext></msub></msub><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="prefix">|</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mi>A</mi><mo>,</mo><mtext mathvariant="normal">clip</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="prefix">|</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><msub><mi>Œ∏</mi><mtext mathvariant="normal">old</mtext></msub></msub><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="prefix">|</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo>,</mo><mn>1</mn><mo>‚àí</mo><mi>Œµ</mi><mo>,</mo><mn>1</mn><mo>+</mo><mi>Œµ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>54</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">J(\theta) = \min\left(\frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)}A, \text{clip} \left( \frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)}, 1-\varepsilon, 1+\varepsilon \right) A \right).\qquad{(54)}</annotation></semantics></math></span></p>
<p>Here, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="prefix">|</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\pi_\theta(a|s)</annotation></semantics></math> is the current policy being optimized and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>œÄ</mi><msub><mi>Œ∏</mi><mtext mathvariant="normal">old</mtext></msub></msub><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="prefix">|</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\pi_{\theta_{\text{old}}}(a|s)</annotation></semantics></math> is the policy that was used to collect the training data (i.e., the policy from the previous iteration). The ratio between these two policies emerges from <em>importance sampling</em>, which allows us to reuse data collected under an old policy to estimate gradients for a new policy.</p>
<p>Recall from the advantage formulation of the policy gradient (eq.¬†<a href="ch011.xhtml#eq:advantage_policy_gradient">47</a>) that we have: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ùîº</mi><mrow><mi>œÑ</mi><mo>‚àº</mo><msub><mi>œÄ</mi><mi>Œ∏</mi></msub></mrow></msub><mrow><mo stretchy="true" form="prefix">[</mo><munderover><mo>‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>T</mi></munderover><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mi mathvariant="normal">log</mi><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><msup><mi>A</mi><msub><mi>œÄ</mi><mi>Œ∏</mi></msub></msup><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) A^{\pi_\theta}(s_t, a_t) \right].</annotation></semantics></math></p>
<p>This expectation is taken over trajectories sampled from <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><annotation encoding="application/x-tex">\pi_\theta</annotation></semantics></math>, but in practice we want to take multiple gradient steps on a batch of data that was collected from a fixed policy <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>œÄ</mi><msub><mi>Œ∏</mi><mtext mathvariant="normal">old</mtext></msub></msub><annotation encoding="application/x-tex">\pi_{\theta_{\text{old}}}</annotation></semantics></math>. To correct for this distribution mismatch, we multiply by the importance weight <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="prefix">|</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><msub><mi>Œ∏</mi><mtext mathvariant="normal">old</mtext></msub></msub><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="prefix">|</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><annotation encoding="application/x-tex">\frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)}</annotation></semantics></math>, which reweights samples to account for how much more or less likely they are under the current policy versus the data-collection policy. Without constraints, optimizing this importance-weighted objective can lead to destructively large policy updates when the ratio diverges far from 1. PPO addresses this by clipping the ratio to the range <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">[</mo><mn>1</mn><mo>‚àí</mo><mi>Œµ</mi><mo>,</mo><mn>1</mn><mo>+</mo><mi>Œµ</mi><mo stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[1-\varepsilon, 1+\varepsilon]</annotation></semantics></math>, ensuring that the policy cannot change too drastically in a single update.</p>
<p>For completeness, PPO is typically written as an <em>expected</em> clipped surrogate objective over timesteps:</p>
<p><span id="eq:PPO_EQN_EXPECTED"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ùîº</mi><mi>t</mi></msub><mrow><mo stretchy="true" form="prefix">[</mo><mi mathvariant="normal">min</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>r</mi><mi>t</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub><mo>,</mo><mspace width="0.222em"></mspace><mtext mathvariant="normal">clip</mtext><mo stretchy="false" form="prefix">(</mo><msub><mi>r</mi><mi>t</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><mo>,</mo><mn>1</mn><mo>‚àí</mo><mi>Œµ</mi><mo>,</mo><mn>1</mn><mo>+</mo><mi>Œµ</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mo>,</mo><mspace width="2.0em"></mspace><msub><mi>R</mi><mi>t</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>‚à£</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><msub><mi>Œ∏</mi><mtext mathvariant="normal">old</mtext></msub></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>‚à£</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mi>.</mi><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>55</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
J(\theta)
=
\mathbb{E}_{t}\left[
\min\left(r_t(\theta)A_t,\ \text{clip}(r_t(\theta),1-\varepsilon,1+\varepsilon)A_t\right)
\right],
\qquad
R_t(\theta)=\frac{\pi_\theta(a_t\mid s_t)}{\pi_{\theta_{\text{old}}}(a_t\mid s_t)}.
\qquad{(55)}</annotation></semantics></math></span></p>
<p>The objective is often converted into a loss function by simply adding a negative sign, which makes the optimizer seek to make it as negative as possible.</p>
<p>For language models, the objective (or loss) is computed per token, which intuitively can be grounded in how one would compute the probability of the entire sequence of autoregressive predictions ‚Äì by a product of probabilities. From there, the common implementation is with <em>log-probabilities</em> that make the computation simpler to perform in modern language modeling frameworks.</p>
<p><span id="eq:PPO_EQN_EXPANDED"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mo stretchy="false" form="prefix">|</mo><mi>a</mi><mo stretchy="false" form="prefix">|</mo></mrow></mfrac><munderover><mo>‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mrow><mo stretchy="false" form="prefix">|</mo><mi>a</mi><mo stretchy="false" form="prefix">|</mo></mrow></munderover><mrow><mi mathvariant="normal">min</mi><mo>&#8289;</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><msub><mi>Œ∏</mi><mtext mathvariant="normal">old</mtext></msub></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><msub><mi>A</mi><mi>t</mi></msub><mo>,</mo><mtext mathvariant="normal">clip</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><msub><mi>Œ∏</mi><mtext mathvariant="normal">old</mtext></msub></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo>,</mo><mn>1</mn><mo>‚àí</mo><mi>Œµ</mi><mo>,</mo><mn>1</mn><mo>+</mo><mi>Œµ</mi><mo stretchy="true" form="postfix">)</mo></mrow><msub><mi>A</mi><mi>t</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>56</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex"> J(\theta) = \frac{1}{|a|} \sum_{t=0}^{|a|} \min\left(\frac{\pi_\theta(a_{t}|s_t)}{\pi_{\theta_{\text{old}}}(a_{t}|s_t)}A_{t}, \text{clip} \left( \frac{\pi_\theta(a_{t}|s_t)}{\pi_{\theta_{\text{old}}}(a_{t}|s_t)}, 1-\varepsilon, 1+\varepsilon \right) A_{t} \right).  \qquad{(56)}</annotation></semantics></math></span></p>
<p>This is the per-token version of PPO, which also applies to other policy-gradient methods, but is explored further later in the implementation section of this chapter. Here, the term for averaging by the number of tokens in the action, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mn>1</mn><mrow><mo stretchy="false" form="prefix">|</mo><mi>a</mi><mo stretchy="false" form="prefix">|</mo></mrow></mfrac><annotation encoding="application/x-tex">\frac{1}{|a|}</annotation></semantics></math>, comes from common implementation practices, but is not in a formal derivation of the loss (shown in <span class="citation" data-cites="liu2025understanding"><a href="ch021.xhtml#ref-liu2025understanding">[185]</a></span>).</p>
<p>Here we will explain the different cases this loss function triggers given various advantages and policy ratios. At an implementation level, the inner computations for PPO involve two main terms: 1) a standard policy gradient with a learned advantage and 2) a clipped policy gradient based on a maximum step size.</p>
<p>To understand how different situations emerge, we can define the policy ratio as:</p>
<p><span id="eq:PPO_POL_RATIO"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="prefix">|</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><msub><mi>Œ∏</mi><mtext mathvariant="normal">old</mtext></msub></msub><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="prefix">|</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>57</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">R(\theta) = \frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)}\qquad{(57)}</annotation></semantics></math></span></p>
<p>The policy ratio is a centerpiece of PPO and related algorithms. It emerges from computing the gradient of a policy and controls the parameter updates in a very intuitive way. For any batch of data, the policy ratio starts at 1 for the first gradient step for that batch, since <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><annotation encoding="application/x-tex">\pi_{\theta}</annotation></semantics></math> is the same as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>œÄ</mi><msub><mi>Œ∏</mi><mtext mathvariant="normal">old</mtext></msub></msub><annotation encoding="application/x-tex">\pi_{\theta_{\text{old}}}</annotation></semantics></math> at this point. Then, in the next gradient step, the policy ratio will be above one if that gradient step increased the likelihood of certain tokens with an associated positive advantage, or less than one for the other case. A common practice is to take 1-4 gradient steps per batch with policy gradient algorithms before updating <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>œÄ</mi><msub><mi>Œ∏</mi><mtext mathvariant="normal">old</mtext></msub></msub><annotation encoding="application/x-tex">\pi_{\theta_{\text{old}}}</annotation></semantics></math>.</p>
<section id="understanding-the-ppo-objective" class="level4">
<h4>Understanding the PPO Objective</h4>
<p>Overall, the PPO objective can be visualized by two lines of a plot of objective versus policy ratio, which is shown in fig.¬†<a href="#fig:ppo-obj">15</a>. The PPO objective is maximized by changing the probability of the sampled actions. Numerically, the objective controls for both positive and negative advantage cases by clever use of the minimum operation, making it so the update is at most pushed by an epsilon distance away from a policy ratio of 1.</p>
<p>Within the trust region, PPO operates the same as other policy gradient algorithms. This is by design! The trust region is a concept used to cap the maximum step size of PPO and its peer algorithms for stability of updates. The core of the PPO algorithm, the clip and min/max functions, is to define this region. The objective becomes flat outside of it.</p>
<p>The idea of a ‚Äútrust region‚Äù comes from the numerical optimization literature <span class="citation" data-cites="nocedal2006numerical"><a href="ch021.xhtml#ref-nocedal2006numerical">[186]</a></span>, but was popularized within Deep RL from the algorithm Trust Region Policy Optimization (TRPO), which is accepted as the predecessor to PPO <span class="citation" data-cites="schulman2015trust"><a href="ch021.xhtml#ref-schulman2015trust">[187]</a></span>. The trust region is the area where the full policy-gradient steps are applied, as the updates are not ‚Äúclipped‚Äù by the max/min operations of the PPO objective.</p>
<figure id="fig:ppo-obj">
<img src="../media/file13.png" alt="Figure 15: Visualization of the different regions of the PPO objective for a hypothetical advantage. The ‚Äútrust region‚Äù would be described as the region where the log-ratio is within 1\pm\varepsilon." />
<figcaption aria-hidden="true">Figure 15: Visualization of the different regions of the PPO objective for a hypothetical advantage. The ‚Äútrust region‚Äù would be described as the region where the log-ratio is within <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>¬±</mo><mi>Œµ</mi></mrow><annotation encoding="application/x-tex">1\pm\varepsilon</annotation></semantics></math>.</figcaption>
</figure>
<p>The policy ratio and advantage together can occur in a few different configurations. We will split the cases into two groups: positive and negative advantage.</p>
<p><strong>Positive Advantage (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>A</mi><mi>t</mi></msub><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">A_t &gt; 0</annotation></semantics></math>)</strong></p>
<p>This means that the action taken was beneficial according to the value function, and we want to increase the likelihood of taking that action in the future. Now, let‚Äôs look at different cases for the policy ratio <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">R(\theta)</annotation></semantics></math>:</p>
<ol type="1">
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><mo>&lt;</mo><mn>1</mn><mo>‚àí</mo><mi>Œµ</mi></mrow><annotation encoding="application/x-tex">R(\theta) &lt; 1 - \varepsilon</annotation></semantics></math>:</p>
<ul>
<li><strong>Interpretation</strong>: Action is less likely with the new policy than the old policy</li>
<li><strong>Unclipped Term</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">R(\theta) A_t</annotation></semantics></math></li>
<li><strong>Clipped Term</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>‚àí</mo><mi>Œµ</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">(1 - \varepsilon) A_t</annotation></semantics></math></li>
<li><strong>Objective</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">R(\theta) A_t</annotation></semantics></math></li>
<li><strong>Gradient</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub><mo>‚â†</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\nabla_\theta R(\theta) A_t \neq 0</annotation></semantics></math></li>
<li><strong>What happens</strong>: Normal policy-gradient update - increase likelihood of action</li>
</ul></li>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>‚àí</mo><mi>Œµ</mi><mo>‚â§</mo><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><mo>‚â§</mo><mn>1</mn><mo>+</mo><mi>Œµ</mi></mrow><annotation encoding="application/x-tex">1 - \varepsilon \leq R(\theta) \leq 1 + \varepsilon</annotation></semantics></math>:</p>
<ul>
<li><strong>Interpretation</strong>: Action is almost equally likely with the new policy as the old policy</li>
<li><strong>Unclipped Term</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">R(\theta) A_t</annotation></semantics></math></li>
<li><strong>Clipped Term</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">R(\theta) A_t</annotation></semantics></math></li>
<li><strong>Objective</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">R(\theta) A_t</annotation></semantics></math></li>
<li><strong>Gradient</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub><mo>‚â†</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\nabla_\theta R(\theta) A_t \neq 0</annotation></semantics></math></li>
<li><strong>What happens</strong>: Normal policy-gradient update - increase likelihood of action</li>
</ul></li>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>+</mo><mi>Œµ</mi><mo>&lt;</mo><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">1 + \varepsilon &lt; R(\theta)</annotation></semantics></math>:</p>
<ul>
<li><strong>Interpretation</strong>: Action is more likely with the new policy than the old policy</li>
<li><strong>Unclipped Term</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">R(\theta) A_t</annotation></semantics></math></li>
<li><strong>Clipped Term</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>+</mo><mi>Œµ</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">(1 + \varepsilon) A_t</annotation></semantics></math></li>
<li><strong>Objective</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>+</mo><mi>Œµ</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">(1 + \varepsilon) A_t</annotation></semantics></math></li>
<li><strong>Gradient</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>+</mo><mi>Œµ</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\nabla_\theta (1 + \varepsilon) A_t = 0</annotation></semantics></math></li>
<li><strong>What happens</strong>: NO UPDATE - action is already more likely under the new policy</li>
</ul></li>
</ol>
<p>To summarize, when the advantage is positive (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>A</mi><mi>t</mi></msub><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">A_t&gt;0</annotation></semantics></math>), we want to boost the probability of the action. Therefore:</p>
<ul>
<li>We perform gradient steps only in the case when <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">new</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo><mo>‚â§</mo><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>+</mo><mi>Œµ</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>œÄ</mi><mtext mathvariant="normal">old</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\pi_{\text{new}}(a) \leq (1+\varepsilon) \pi_{\text{old}}(a)</annotation></semantics></math>. Intuitively, we want to boost the probability of the action, since the advantage was positive, but not boost it so much that we have made it substantially more likely.</li>
<li>Crucially, when <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">new</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo><mo>&gt;</mo><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>+</mo><mi>Œµ</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>œÄ</mi><mtext mathvariant="normal">old</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\pi_{\text{new}}(a) &gt; (1+\varepsilon) \pi_{\text{old}}(a)</annotation></semantics></math>, then we don‚Äôt perform any update, and the gradient of the clipped objective is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>0</mn><annotation encoding="application/x-tex">0</annotation></semantics></math>. Intuitively, the action is already more expressed with the new policy, so we don‚Äôt want to over-reinforce it.</li>
</ul>
<p><strong>Negative Advantage (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>A</mi><mi>t</mi></msub><mo>&lt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">A_t &lt; 0</annotation></semantics></math>)</strong></p>
<p>This means that the action taken was detrimental according to the value function, and we want to decrease the likelihood of taking that action in the future. Now, let‚Äôs look at different cases for the policy ratio <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">R(\theta)</annotation></semantics></math>:</p>
<ol type="1">
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><mo>&lt;</mo><mn>1</mn><mo>‚àí</mo><mi>Œµ</mi></mrow><annotation encoding="application/x-tex">R(\theta) &lt; 1 - \varepsilon</annotation></semantics></math>:</p>
<ul>
<li><strong>Interpretation</strong>: Action is less likely with the new policy than the old policy</li>
<li><strong>Unclipped Term</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">R(\theta) A_t</annotation></semantics></math></li>
<li><strong>Clipped Term</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>‚àí</mo><mi>Œµ</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">(1 - \varepsilon) A_t</annotation></semantics></math></li>
<li><strong>Objective</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>‚àí</mo><mi>Œµ</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">(1 - \varepsilon) A_t</annotation></semantics></math></li>
<li><strong>Gradient</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>‚àí</mo><mi>Œµ</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\nabla_\theta (1 - \varepsilon) A_t = 0</annotation></semantics></math></li>
<li><strong>What happens</strong>: NO UPDATE - action is already less likely under the new policy</li>
</ul></li>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>‚àí</mo><mi>Œµ</mi><mo>‚â§</mo><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><mo>‚â§</mo><mn>1</mn><mo>+</mo><mi>Œµ</mi></mrow><annotation encoding="application/x-tex">1 - \varepsilon \leq R(\theta) \leq 1 + \varepsilon</annotation></semantics></math>:</p>
<ul>
<li><strong>Interpretation</strong>: Action is almost equally likely with the new policy as the old policy</li>
<li><strong>Unclipped Term</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">R(\theta) A_t</annotation></semantics></math></li>
<li><strong>Clipped Term</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">R(\theta) A_t</annotation></semantics></math></li>
<li><strong>Objective</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">R(\theta) A_t</annotation></semantics></math></li>
<li><strong>Gradient</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub><mo>‚â†</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\nabla_\theta R(\theta) A_t \neq 0</annotation></semantics></math></li>
<li><strong>What happens</strong>: Normal policy-gradient update - decrease likelihood of action</li>
</ul></li>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>+</mo><mi>Œµ</mi><mo>&lt;</mo><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">1 + \varepsilon &lt; R(\theta)</annotation></semantics></math>:</p>
<ul>
<li><strong>Interpretation</strong>: Action is more likely with the new policy than the old policy</li>
<li><strong>Unclipped Term</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">R(\theta) A_t</annotation></semantics></math></li>
<li><strong>Clipped Term</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>+</mo><mi>Œµ</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">(1 + \varepsilon) A_t</annotation></semantics></math></li>
<li><strong>Objective</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">R(\theta) A_t</annotation></semantics></math></li>
<li><strong>Gradient</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>‚àá</mi><mi>Œ∏</mi></msub><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub><mo>‚â†</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\nabla_\theta R(\theta) A_t \neq 0</annotation></semantics></math></li>
<li><strong>What happens</strong>: Normal policy-gradient update - decrease likelihood of action</li>
</ul></li>
</ol>
<p>To summarize, when the advantage is negative (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>A</mi><mi>t</mi></msub><mo>&lt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">A_t &lt; 0</annotation></semantics></math>), we want to decrease the probability of the action. Therefore:</p>
<ul>
<li>We perform gradient steps only in the case when <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">new</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo><mo>‚â•</mo><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>‚àí</mo><mi>Œµ</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>œÄ</mi><mtext mathvariant="normal">old</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\pi_{\text{new}}(a) \geq (1-\varepsilon) \pi_{\text{old}}(a)</annotation></semantics></math>. Intuitively, we want to decrease the probability of the action, since the advantage was negative, and we do so proportional to the advantage.</li>
<li>Crucially, when <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>œÄ</mi><mtext mathvariant="normal">new</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo><mo>&lt;</mo><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>‚àí</mo><mi>Œµ</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>œÄ</mi><mtext mathvariant="normal">old</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\pi_{\text{new}}(a) &lt; (1-\varepsilon) \pi_{\text{old}}(a)</annotation></semantics></math>, then we don‚Äôt perform any update, and the gradient of the clipped objective is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>0</mn><annotation encoding="application/x-tex">0</annotation></semantics></math>. Intuitively, the action is already less likely under the new policy, so we don‚Äôt want to over-suppress it.</li>
</ul>
<p>It is crucial to remember that PPO within the trust region is roughly the same as standard forms of policy gradient.</p>
</section>
<section id="value-functions-and-ppo" class="level4">
<h4>Value Functions and PPO</h4>
<p>The value function within PPO is an additional copy of the model that is used to predict the value per token. The value of a token (or state) in traditional RL is predicting the future return from that moment, often with discounting. This value in PPO is used as a learned baseline, representing an evolution of the simple Monte Carlo version used with REINFORCE (which doesn‚Äôt need the learned value network). This highlights how PPO is an evolution of REINFORCE and vanilla policy-gradient in multiple forms, across the optimization form, baseline, etc. In practice, with PPO and other algorithms used for language models, this is predicting the return of each token after the deduction of KL penalties (the per-token loss includes the KL from the reward traditionally, as discussed).</p>
<p>There are a few different methods (or targets) used to learn the value functions. Generalized Advantage Estimation (GAE) is considered the state-of-the-art and canonical implementation in modern systems, but it carries more complexity by computing the value prediction error over multiple steps ‚Äì see the later section on GAE in this chapter. A value function can also be learned with Monte Carlo estimates from the rollouts used to update the policy. PPO has two losses ‚Äì one to learn the value function and another to use that value function to update the policy.</p>
<p>A simple example implementation of a value network loss is shown below.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Basic PPO critic targets &amp; loss (no GAE)</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># B: Batch Size</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># L: Completion Length</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Inputs:</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">#   rewards: (B, L) post-KL per-token rewards; EOS row includes outcome</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">#   done_mask: (B, L) 1.0 at terminal token (EOS or truncation if penalized), else 0.0</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">#   completion_mask: (B, L) 1.0 on response tokens to supervise (ignore the prompt)</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">#   values: (B, L) current critic predictions V_theta(s_t)</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">#       because a value network is a running update</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">#   old_values: (B, L) critic predictions at rollout time V_{theta_old}(s_t)</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">#   gamma: discount factor, float (often 1.0 for LM RLHF)</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co">#   epsilon_v: float value clip range (e.g., 0.2), similar to PPO Loss Update itself, optional</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Returns:</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co">#   value_loss: scalar; advantages: (B, L) detached (for policy loss)</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>B, L <span class="op">=</span> rewards.shape</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co"># 1) Monte Carlo returns per token (reset at terminals)</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply discounting, if enabled</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>returns <span class="op">=</span> torch.zeros_like(rewards)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>running <span class="op">=</span> torch.zeros(B, device<span class="op">=</span>rewards.device, dtype<span class="op">=</span>rewards.dtype)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">reversed</span>(<span class="bu">range</span>(L)):</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    running <span class="op">=</span> rewards[:, t] <span class="op">+</span> gamma <span class="op">*</span> (<span class="fl">1.0</span> <span class="op">-</span> done_mask[:, t]) <span class="op">*</span> running</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    returns[:, t] <span class="op">=</span> running</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>targets <span class="op">=</span> returns  <span class="co"># y_t = G_t (post-KL)</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="co"># 2) PPO-style value clipping (optional)</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>v_pred <span class="op">=</span> values</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>v_old  <span class="op">=</span> old_values</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>v_clip <span class="op">=</span> torch.clamp(v_pred, v_old <span class="op">-</span> epsilon_v, v_old <span class="op">+</span> epsilon_v)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>vf_unclipped <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> (v_pred <span class="op">-</span> targets) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>vf_clipped   <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> (v_clip <span class="op">-</span> targets) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>vf_loss_tok  <span class="op">=</span> torch.<span class="bu">max</span>(vf_unclipped, vf_clipped)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a><span class="co"># 3) Mask to response tokens and aggregate</span></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>denom <span class="op">=</span> completion_mask.<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>).clamp_min(<span class="dv">1</span>)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>value_loss <span class="op">=</span> ((vf_loss_tok <span class="op">*</span> completion_mask).<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>) <span class="op">/</span> denom).mean()</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="co"># 4) Advantages for policy loss (no GAE): A_t = G_t - V(s_t)</span></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>advantages <span class="op">=</span> (targets <span class="op">-</span> v_pred).detach()</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a><span class="co"># The value loss is applied later, often with the PG loss, e.g.</span></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a><span class="co"># total_loss = policy_loss + vf_coef * value_loss</span></span></code></pre></div>
</section>
</section>
<section id="group-relative-policy-optimization" class="level3">
<h3>Group Relative Policy Optimization</h3>
<p>Group Relative Policy Optimization (GRPO) is introduced in DeepSeekMath <span class="citation" data-cites="shao2024deepseekmath"><a href="ch021.xhtml#ref-shao2024deepseekmath">[188]</a></span>, and used in other DeepSeek works, e.g.¬†DeepSeek-V3 <span class="citation" data-cites="liu2024deepseek"><a href="ch021.xhtml#ref-liu2024deepseek">[189]</a></span> and DeepSeek-R1 <span class="citation" data-cites="guo2025deepseek"><a href="ch021.xhtml#ref-guo2025deepseek">[61]</a></span>. GRPO can be viewed as a PPO-inspired algorithm with a very similar surrogate loss, but it avoids learning a value function with another copy of the original policy language model (or another checkpoint for initialization). This brings two posited benefits:</p>
<ol type="1">
<li>Avoiding the challenge of learning a value function from a LM backbone, where research hasn‚Äôt established best practices.</li>
<li>Saves memory by not needing to keep the extra set of model weights in memory (going from needing the current policy, the reference policy, and a value function, to just the first two copies).</li>
</ol>
<p>GRPO does this by simplifying the value estimation and assigning the same value to every token in the episode (i.e.¬†in the completion to a prompt, each token gets assigned the same value rather than discounted rewards in a standard value function) by estimating the advantage or baseline. The estimate is done by collecting multiple completions (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>a</mi><mi>i</mi></msub><annotation encoding="application/x-tex">a_i</annotation></semantics></math>) and rewards (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>r</mi><mi>i</mi></msub><annotation encoding="application/x-tex">r_i</annotation></semantics></math>), i.e.¬†a Monte Carlo estimate, from the same initial state / prompt (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math>).</p>
<p>To state this formally, the GRPO objective is very similar to the PPO objective above. For GRPO, the objective (or loss) is accumulated over a group of completions <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">{</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi><mo>,</mo><msub><mi>a</mi><mi>G</mi></msub><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\{a_1, a_2, ..., a_G\}</annotation></semantics></math> to a given prompt <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math>. Here, we show the GRPO objective:</p>
<p><span id="eq:GRPO"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>G</mi></mfrac><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>G</mi></munderover><mrow><mo stretchy="true" form="prefix">(</mo><mi mathvariant="normal">min</mi><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">|</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><msub><mi>Œ∏</mi><mtext mathvariant="normal">old</mtext></msub></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">|</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><msub><mi>A</mi><mi>i</mi></msub><mo>,</mo><mtext mathvariant="normal">clip</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">|</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><msub><mi>Œ∏</mi><mtext mathvariant="normal">old</mtext></msub></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">|</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo>,</mo><mn>1</mn><mo>‚àí</mo><mi>Œµ</mi><mo>,</mo><mn>1</mn><mo>+</mo><mi>Œµ</mi><mo stretchy="true" form="postfix">)</mo></mrow><msub><mi>A</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>‚àí</mo><mi>Œ≤</mi><msub><mi>ùíü</mi><mtext mathvariant="normal">KL</mtext></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">|</mo><mo stretchy="false" form="prefix">|</mo><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>58</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">J(\theta) = \frac{1}{G}\sum_{i=1}^G \left(\min\left(\frac{\pi_\theta(a_i|s)}{\pi_{\theta_{\text{old}}}(a_i|s)}A_i, \text{clip} \left( \frac{\pi_\theta(a_i|s)}{\pi_{\theta_{\text{old}}}(a_i|s)}, 1-\varepsilon, 1+\varepsilon \right) A_i \right) - \beta \mathcal{D}_{\text{KL}}(\pi_\theta||\pi_{\text{ref}})\right).\qquad{(58)}</annotation></semantics></math></span></p>
<p>Note that relative to PPO, the standard implementation of GRPO includes the KL distance in the loss. As above, we can expand this into a per-token computation:</p>
<p><span id="eq:GRPO_token"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>G</mi></mfrac><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>G</mi></munderover><mfrac><mn>1</mn><mrow><mo stretchy="false" form="prefix">|</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">|</mo></mrow></mfrac><munderover><mo>‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mo stretchy="false" form="prefix">|</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">|</mo></mrow></munderover><mrow><mo stretchy="true" form="prefix">(</mo><mi mathvariant="normal">min</mi><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>i</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><msub><mi>Œ∏</mi><mtext mathvariant="normal">old</mtext></msub></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>i</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><msub><mi>A</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>,</mo><mtext mathvariant="normal">clip</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>i</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><msub><mi>Œ∏</mi><mtext mathvariant="normal">old</mtext></msub></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>i</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo>,</mo><mn>1</mn><mo>‚àí</mo><mi>Œµ</mi><mo>,</mo><mn>1</mn><mo>+</mo><mi>Œµ</mi><mo stretchy="true" form="postfix">)</mo></mrow><msub><mi>A</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>‚àí</mo><mi>Œ≤</mi><msub><mi>ùíü</mi><mtext mathvariant="normal">KL</mtext></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>‚ãÖ</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>i</mi></msub><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="prefix">|</mo><mo stretchy="false" form="prefix">|</mo><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>‚ãÖ</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>i</mi></msub><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>59</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex"> J(\theta) = \frac{1}{G}\sum_{i=1}^G  \frac{1}{|a_i|} \sum_{t=1}^{|a_i|} \left( \min\left(\frac{\pi_\theta(a_{i,t}|s_{i})}{\pi_{\theta_{\text{old}}}(a_{i,t}|s_{i})}A_{i,t}, \text{clip} \left( \frac{\pi_\theta(a_{i,t}|s_{i})}{\pi_{\theta_{\text{old}}}(a_{i,t}|s_{i})}, 1-\varepsilon, 1+\varepsilon \right) A_{i,t} \right) - \beta \mathcal{D}_{\text{KL}}(\pi_\theta(\cdot|s_{i})||\pi_{\text{ref}}(\cdot|s_{i})) \right)  \qquad{(59)}</annotation></semantics></math></span></p>
<p>With the advantage computation for the completion index <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>:</p>
<p><span id="eq:GRPO_ADV"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>A</mi><mi>i</mi></msub><mo>=</mo><mfrac><mrow><msub><mi>r</mi><mi>i</mi></msub><mo>‚àí</mo><mtext mathvariant="normal">mean</mtext><mo stretchy="false" form="prefix">(</mo><mrow><msub><mi>r</mi><mn>1</mn></msub><mo>,</mo><msub><mi>r</mi><mn>2</mn></msub><mo>,</mo><mi>‚ãØ</mi><mo>,</mo><msub><mi>r</mi><mi>G</mi></msub></mrow><mo stretchy="false" form="postfix">)</mo></mrow><mrow><mtext mathvariant="normal">std</mtext><mo stretchy="false" form="prefix">(</mo><mrow><msub><mi>r</mi><mn>1</mn></msub><mo>,</mo><msub><mi>r</mi><mn>2</mn></msub><mo>,</mo><mi>‚ãØ</mi><mo>,</mo><msub><mi>r</mi><mi>G</mi></msub></mrow><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mi>.</mi><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>60</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">A_i = \frac{r_i - \text{mean}({r_1, r_2, \cdots, r_G})}{\text{std}({r_1, r_2, \cdots, r_G})}.\qquad{(60)}</annotation></semantics></math></span></p>
<p>Intuitively, the GRPO update is comparing multiple answers to a single question within a batch. The model learns to become more like the answers marked as correct and less like the others. This is a very simple way to compute the advantage, which is the measure of how much better a specific action is than the average at a given state. Relative to PPO, REINFORCE, and broadly RLHF performed with a reward model rating (relative to output reward), GRPO is often run with a far higher number of samples per prompt because the advantage is entirely about the relative value of a completion to its peers from that prompt. Here, the current policy generates multiple responses to a given prompt, and the group-wise GRPO advantage estimate is given valuable context. PPO and vanilla policy-gradient algorithms were designed to accurately estimate the reward of every completion (in fact, more completions can do little to improve the value estimate in some cases). GRPO and its variants are particularly well-suited to modern language model tools, where multiple completions to a given prompt is very natural (especially when compared to, e.g., multiple actions from a set environment state in a robotic task).</p>
<p>The advantage computation for GRPO has trade-offs in its biases. The normalization by standard deviation is rewarding questions in a batch that have a low variation in answer correctness. For questions with either nearly all correct or all incorrect answers, the standard deviation will be lower and the advantage will be higher. <span class="citation" data-cites="liu2025understanding"><a href="ch021.xhtml#ref-liu2025understanding">[185]</a></span> proposes removing the standard deviation term given this bias, but this comes at the cost of down-weighing questions that were all incorrect with a few correct answers, which could be seen as valuable learning signal for the model. Those high-variance prompts can be exactly the hardest cases, where only a few sampled completions find the correct answer and provide a strong training signal.</p>
<p>eq.¬†<a href="ch011.xhtml#eq:GRPO_ADV">60</a> is the implementation of GRPO when working with outcome supervision (either a standard reward model or a single verifiable reward) and a different implementation is needed with process supervision. In this case, GRPO computes the advantage as the sum of the normalized rewards for the following reasoning steps.</p>
<p>Finally, GRPO‚Äôs advantage estimation can also be applied without the PPO clipping to more vanilla versions of policy gradient (e.g.¬†REINFORCE), but it is not the canonical form. As an example of how these algorithms are intertwined, we can show that the advantage estimation in a variant of GRPO, Dr.¬†GRPO (GRPO Done Right) <span class="citation" data-cites="liu2025understanding"><a href="ch021.xhtml#ref-liu2025understanding">[185]</a></span>, is equivalent to the RLOO estimation (which uses the average reward of other samples as its baseline) up to a constant scaling factor (which normally does not matter due to implementation details to normalize the advantage). Dr.¬†GRPO removes the standard deviation normalization term from eq.¬†<a href="ch011.xhtml#eq:GRPO_ADV">60</a> ‚Äì note that this also scales the advantage <em>up</em>, which is equivalent to increasing the GRPO learning rate on samples with a variance in answer scores. This addresses a bias towards questions with low reward variance ‚Äì i.e.¬†almost all the answers are right or wrong ‚Äì but comes at a potential cost where problems where just one sample gets the answer right are important to learn from. The Dr.¬†GRPO advantage for completion <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> within a group of size <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> is defined as:</p>
<p><span id="eq:DrGRPO_ADV"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>A</mi><mo accent="true">ÃÉ</mo></mover><mi>i</mi></msub><mo>=</mo><msub><mi>r</mi><mi>i</mi></msub><mo>‚àí</mo><mtext mathvariant="normal">mean</mtext><mo stretchy="false" form="prefix">(</mo><mrow><msub><mi>r</mi><mn>1</mn></msub><mo>,</mo><msub><mi>r</mi><mn>2</mn></msub><mo>,</mo><mi>‚ãØ</mi><mo>,</mo><msub><mi>r</mi><mi>G</mi></msub></mrow><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>r</mi><mi>i</mi></msub><mo>‚àí</mo><mfrac><mn>1</mn><mi>G</mi></mfrac><munderover><mo>‚àë</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>G</mi></munderover><msub><mi>r</mi><mi>j</mi></msub><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>61</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex"> \tilde{A}_i = r_i - \text{mean}({r_1, r_2, \cdots, r_G}) = r_i - \frac{1}{G}\sum_{j=1}^G r_j \qquad{(61)}</annotation></semantics></math></span></p>
<p>Here, in the same notation, we can recall the RLOO advantage estimation as:</p>
<p><span id="eq:RLOO_ADV_AGAIN"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>A</mi><mi>i</mi><mtext mathvariant="normal">RLOO</mtext></msubsup><mo>=</mo><msub><mi>r</mi><mi>i</mi></msub><mo>‚àí</mo><mfrac><mn>1</mn><mrow><mi>G</mi><mo>‚àí</mo><mn>1</mn></mrow></mfrac><munderover><mo>‚àë</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>i</mi><mo>‚â†</mo><mi>j</mi></mrow><mi>G</mi></munderover><msub><mi>r</mi><mi>j</mi></msub><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>62</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex"> A_i^\text{RLOO} = r_i - \frac{1}{G-1}\sum_{j=1, i\neq j}^G r_j \qquad{(62)}</annotation></semantics></math></span></p>
<p>Thus, if we multiply the Dr.¬†GRPO advantage definition by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mi>G</mi><mrow><mi>G</mi><mo>‚àí</mo><mn>1</mn></mrow></mfrac><annotation encoding="application/x-tex">\frac{G}{G-1}</annotation></semantics></math> we can see a scaled equivalence:</p>
<p><span id="eq:RLOO_GRPO_EQUIV"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtable><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"><mfrac><mi>G</mi><mrow><mi>G</mi><mo>‚àí</mo><mn>1</mn></mrow></mfrac><msub><mover><mi>A</mi><mo accent="true">ÃÉ</mo></mover><mi>i</mi></msub></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><mfrac><mi>G</mi><mrow><mi>G</mi><mo>‚àí</mo><mn>1</mn></mrow></mfrac><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>r</mi><mi>i</mi></msub><mo>‚àí</mo><mfrac><mn>1</mn><mi>G</mi></mfrac><munderover><mo>‚àë</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>G</mi></munderover><msub><mi>r</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><mfrac><mi>G</mi><mrow><mi>G</mi><mo>‚àí</mo><mn>1</mn></mrow></mfrac><msub><mi>r</mi><mi>i</mi></msub><mo>‚àí</mo><mfrac><mn>1</mn><mrow><mi>G</mi><mo>‚àí</mo><mn>1</mn></mrow></mfrac><munderover><mo>‚àë</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>G</mi></munderover><msub><mi>r</mi><mi>j</mi></msub></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><mfrac><mi>G</mi><mrow><mi>G</mi><mo>‚àí</mo><mn>1</mn></mrow></mfrac><msub><mi>r</mi><mi>i</mi></msub><mo>‚àí</mo><mfrac><mn>1</mn><mrow><mi>G</mi><mo>‚àí</mo><mn>1</mn></mrow></mfrac><munderover><mo>‚àë</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>j</mi><mo>‚â†</mo><mi>i</mi></mrow><mi>G</mi></munderover><msub><mi>r</mi><mi>j</mi></msub><mo>‚àí</mo><mfrac><mn>1</mn><mrow><mi>G</mi><mo>‚àí</mo><mn>1</mn></mrow></mfrac><msub><mi>r</mi><mi>i</mi></msub></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><msub><mi>r</mi><mi>i</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mi>G</mi><mrow><mi>G</mi><mo>‚àí</mo><mn>1</mn></mrow></mfrac><mo>‚àí</mo><mfrac><mn>1</mn><mrow><mi>G</mi><mo>‚àí</mo><mn>1</mn></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mo>‚àí</mo><mfrac><mn>1</mn><mrow><mi>G</mi><mo>‚àí</mo><mn>1</mn></mrow></mfrac><munderover><mo>‚àë</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>j</mi><mo>‚â†</mo><mi>i</mi></mrow><mi>G</mi></munderover><msub><mi>r</mi><mi>j</mi></msub></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><msub><mi>r</mi><mi>i</mi></msub><mo>‚àí</mo><mfrac><mn>1</mn><mrow><mi>G</mi><mo>‚àí</mo><mn>1</mn></mrow></mfrac><munderover><mo>‚àë</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>j</mi><mo>‚â†</mo><mi>i</mi></mrow><mi>G</mi></munderover><msub><mi>r</mi><mi>j</mi></msub></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><msubsup><mi>A</mi><mi>i</mi><mtext mathvariant="normal">RLOO</mtext></msubsup></mtd></mtr></mtable><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>63</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
\begin{aligned}
\frac{G}{G-1} \tilde{A}_i &amp;= \frac{G}{G-1} \left( r_i - \frac{1}{G}\sum_{j=1}^G r_j \right) \\
&amp;= \frac{G}{G-1} r_i - \frac{1}{G-1} \sum_{j=1}^G r_j \\
&amp;= \frac{G}{G-1} r_i - \frac{1}{G-1} \sum_{j=1, j\neq i}^G r_j - \frac{1}{G-1} r_i \\
&amp;= r_i \left( \frac{G}{G-1} - \frac{1}{G-1} \right) - \frac{1}{G-1} \sum_{j=1, j\neq i}^G r_j \\
&amp;= r_i - \frac{1}{G-1} \sum_{j=1, j\neq i}^G r_j \\
&amp;= A_i^{\text{RLOO}}
\end{aligned}
\qquad{(63)}</annotation></semantics></math></span></p>
</section>
</section>
<section id="implementation-1" class="level2">
<h2>Implementation</h2>
<p>Compared to the original Deep RL literature where many of these algorithms were developed, implementing RL for optimizing language models or other large AI models requires many small implementation details. In this section, we highlight some key factors that differentiate the implementations of popular algorithms.</p>
<p>There are many other small details that go into this training. For example, when doing RLHF with language models a crucial step is generating text that will then be rated by the reward model. Under normal circumstances, the model should generate an end-of-sequence (EOS) token indicating it finished generating, but a common practice is to put a hard cap on generation length to efficiently utilize infrastructure. A failure mode of RLHF is that the model is regularly truncated in its answers, driving the ratings from the reward model out of distribution and to unpredictable scores. The solution to this is to <em>only</em> run reward model scoring on the <code>eos_token</code>, and to otherwise assign a penalty to the model for generating too long.</p>
<p>The popular open-source tools for RLHF have a large variance in implementation details across the algorithms (see table 10 in <span class="citation" data-cites="ivison2024unpacking"><a href="ch021.xhtml#ref-ivison2024unpacking">[190]</a></span>). Some decisions not covered here include:</p>
<ul>
<li><strong>Value network initialization</strong>: The internal learned value network used by PPO and other similar algorithms can be started from a different model of the same architecture or randomly selected weights. This can have a large impact on performance. The standard established in InstructGPT <span class="citation" data-cites="ouyang2022training"><a href="ch021.xhtml#ref-ouyang2022training">[3]</a></span> (and re-used in T√ºlu 3 for its work on RLVR <span class="citation" data-cites="lambert2024t"><a href="ch021.xhtml#ref-lambert2024t">[6]</a></span>) is to initialize the value network from the reward model used during RLHF. Others have used the previous checkpoint to RLHF training (normally an SFT model) with a value head appened randomly initialized, or fully re-initialized language models (less common as it will take longer for RLHF to converge, but possible).</li>
<li><strong>Reward normalization, reward whitening, and/or advantage whitening</strong>: Normalization bounds all the values from the RM (or environment) to be between 0 and 1, which can help with learning stability. <a href="https://en.wikipedia.org/wiki/Whitening_transformation">Whitening</a> goes further by transforming rewards or advantage estimates to have zero mean and unit variance, providing an even stronger boost to stability.</li>
<li><strong>Different KL estimators</strong>: With complex language models, precisely computing the KL divergence between models can be complex, so multiple approximations are used to substitute for an exact calculation <span class="citation" data-cites="schulman2016klapprox"><a href="ch021.xhtml#ref-schulman2016klapprox">[164]</a></span>.</li>
<li><strong>KL controllers</strong>: Original implementations of PPO and related algorithms had dynamic controllers that targeted specific KLs and changed the penalty based on recent measurements. Most modern RLHF implementations use static KL penalties, but this can also vary.</li>
</ul>
<p>For more details on implementation details for RLHF, see <span class="citation" data-cites="huang2024n"><a href="ch021.xhtml#ref-huang2024n">[191]</a></span>. For further information on the algorithms, see <span class="citation" data-cites="weng2018PG"><a href="ch021.xhtml#ref-weng2018PG">[192]</a></span>.</p>
<section id="policy-gradient-basics" class="level3">
<h3>Policy Gradient Basics</h3>
<p>A simple implementation of policy gradient, using advantages to estimate the gradient to prepare for advanced algorithms such as PPO and GRPO follows:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>pg_loss <span class="op">=</span> <span class="op">-</span>advantages <span class="op">*</span> ratio</span></code></pre></div>
<p>Ratio here is the (per-token) probability ratio (often computed from a log-probability difference) of the new policy model probabilities relative to the reference model.</p>
<p>In order to understand this equation, it is good to understand different cases that can fall within a batch of updates. Remember that we want the loss to <em>decrease</em> as the model gets better at the task.</p>
<p>Case 1: Positive advantage, so the action was better than the expected value of the state. We want to reinforce this. In this case, the model will make this more likely with the negative sign. To do so, it‚Äôll increase the logratio. A positive logratio, or sum of log probabilities of the tokens, means that the model is more likely to generate those tokens.</p>
<p>Case 2: Negative advantage, so the action was worse than the expected value of the state. This follows very similarly. Here, the loss will be positive if the new model was more likely, so the model will try to make it so the policy parameters make this completion less likely.</p>
<p>Case 3: Zero advantage, so no update is needed. The loss is zero, don‚Äôt change the policy model.</p>
</section>
<section id="loss-aggregation" class="level3">
<h3>Loss Aggregation</h3>
<p>The question when implementing any policy gradient algorithm with language models is: How do you aggregate per-token losses into a final scalar loss? Given per-token losses <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>‚Ñì</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><annotation encoding="application/x-tex">\ell_{i,t}</annotation></semantics></math> for sample <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> at token <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>, with completion lengths <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">|</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">|</mo></mrow><annotation encoding="application/x-tex">|a_i|</annotation></semantics></math> and batch size <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>, there are three main strategies:</p>
<p><strong>Strategy 1: Per-sequence normalization</strong> (standard GRPO; also used in some PPO implementations)</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>=</mo><mfrac><mn>1</mn><mi>B</mi></mfrac><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><mfrac><mn>1</mn><mrow><mo stretchy="false" form="prefix">|</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">|</mo></mrow></mfrac><munderover><mo>‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mo stretchy="false" form="prefix">|</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">|</mo></mrow></munderover><msub><mi>‚Ñì</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">L = \frac{1}{B} \sum_{i=1}^{B} \frac{1}{|a_i|} \sum_{t=1}^{|a_i|} \ell_{i,t}</annotation></semantics></math></p>
<p>Each sequence contributes equally to the batch loss, regardless of length. In code:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Strategy 1: Per-sequence normalization</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>sequence_loss <span class="op">=</span> ((per_token_loss <span class="op">*</span> completion_mask).<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>) <span class="op">/</span> <span class="op">\</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>             completion_mask.<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>)).mean()</span></code></pre></div>
<p><strong>Strategy 2: Per-token normalization</strong> (DAPO <span class="citation" data-cites="yu2025dapo"><a href="ch021.xhtml#ref-yu2025dapo">[193]</a></span>)</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>=</mo><mfrac><mrow><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><munderover><mo>‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mo stretchy="false" form="prefix">|</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">|</mo></mrow></munderover><msub><mi>‚Ñì</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub></mrow><mrow><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><mo stretchy="false" form="prefix">|</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">|</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">L = \frac{\sum_{i=1}^{B} \sum_{t=1}^{|a_i|} \ell_{i,t}}{\sum_{i=1}^{B} |a_i|}</annotation></semantics></math></p>
<p>Each token contributes equally; longer sequences have proportionally more influence on the gradient. In code:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Strategy 2: Per-token normalization</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>token_loss <span class="op">=</span> ((per_token_loss <span class="op">*</span> completion_mask).<span class="bu">sum</span>() <span class="op">/</span> <span class="op">\</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>            completion_mask.<span class="bu">sum</span>())</span></code></pre></div>
<p><strong>Strategy 3: Fixed-length normalization</strong> (Dr.¬†GRPO <span class="citation" data-cites="liu2025understanding"><a href="ch021.xhtml#ref-liu2025understanding">[185]</a></span>)</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>=</mo><mfrac><mn>1</mn><mi>B</mi></mfrac><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><mfrac><mn>1</mn><msub><mi>L</mi><mi mathvariant="normal">max</mi></msub></mfrac><munderover><mo>‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mo stretchy="false" form="prefix">|</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">|</mo></mrow></munderover><msub><mi>‚Ñì</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">L = \frac{1}{B} \sum_{i=1}^{B} \frac{1}{L_{\max}} \sum_{t=1}^{|a_i|} \ell_{i,t}</annotation></semantics></math></p>
<p>Normalizes by max sequence length <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>L</mi><mi mathvariant="normal">max</mi></msub><annotation encoding="application/x-tex">L_{\max}</annotation></semantics></math>, equalizing the per-token scale across sequences while still letting longer sequences contribute more total gradient because they contain more active tokens.</p>
<p>Note that <code>completion_mask</code> in the code above is a matrix of 1s and 0s, where the prompt tokens are masked out (0s) because we don‚Äôt want the model to learn from predicting prompt tokens.</p>
<section id="why-does-this-matter" class="level4">
<h4>Why does this matter?</h4>
<p>Intuitively, per-sequence normalization (Strategy 1) seems best since we care about <em>outcomes</em>, not individual tokens. However, this introduces subtle biases based on sequence length, which can cause the model to overthink of down-weight strategies that naturally need to use more tokens, depending on the direction of the bias. Consider two sequences of different lengths with per-token losses:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>seq_1_losses <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">10</span>]  <span class="co"># 5 tokens, mean = 2.8</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>seq_2_losses <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">10</span>]  <span class="co"># 10 tokens, mean = 1.9</span></span></code></pre></div>
<p>With <strong>Strategy 1</strong> (per-sequence): The batch loss is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><mn>2.8</mn><mo>+</mo><mn>1.9</mn><mo stretchy="false" form="postfix">)</mo><mi>/</mi><mn>2</mn><mo>=</mo><mn>2.35</mn></mrow><annotation encoding="application/x-tex">(2.8 + 1.9)/2 = 2.35</annotation></semantics></math>, and crucially, each token in the short sequence receives a larger gradient than tokens in the long sequence.</p>
<p>With <strong>Strategy 2</strong> (per-token): The batch loss is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><mn>14</mn><mo>+</mo><mn>19</mn><mo stretchy="false" form="postfix">)</mo><mi>/</mi><mn>15</mn><mo>=</mo><mn>2.2</mn></mrow><annotation encoding="application/x-tex">(14 + 19)/15 = 2.2</annotation></semantics></math>, and all tokens receive equal gradient magnitude.</p>
<p>With <strong>Strategy 3</strong> (fixed-length with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mi mathvariant="normal">max</mi></msub><mo>=</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">L_{\max}=10</annotation></semantics></math>): The short sequence contributes <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>1.4</mn><annotation encoding="application/x-tex">1.4</annotation></semantics></math> and the long sequence contributes <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>1.9</mn><annotation encoding="application/x-tex">1.9</annotation></semantics></math>, balancing per-token gradients while still weighting by sequence.</p>
<p>For a more complete example showing how these strategies affect gradients, see the script below.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Optional</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> masked_mean(values: torch.Tensor, mask: torch.Tensor, axis: Optional[<span class="bu">int</span>] <span class="op">=</span> <span class="va">None</span>) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Compute mean of tensor with masked values.&quot;&quot;&quot;</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> axis <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (values <span class="op">*</span> mask).<span class="bu">sum</span>(axis<span class="op">=</span>axis) <span class="op">/</span> mask.<span class="bu">sum</span>(axis<span class="op">=</span>axis)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (values <span class="op">*</span> mask).<span class="bu">sum</span>() <span class="op">/</span> mask.<span class="bu">sum</span>()</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> masked_sum(</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>        values: torch.Tensor,</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>        mask: torch.Tensor,</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>        axis: Optional[<span class="bu">int</span>] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>        constant_normalizer: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1.0</span>,</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Compute sum of tensor with masked values. Use a constant to normalize.&quot;&quot;&quot;</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> axis <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (values <span class="op">*</span> mask).<span class="bu">sum</span>(axis<span class="op">=</span>axis) <span class="op">/</span> constant_normalizer</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (values <span class="op">*</span> mask).<span class="bu">sum</span>() <span class="op">/</span> constant_normalizer</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>ratio <span class="op">=</span> torch.tensor([</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">1.</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>,],</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>,],</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>], requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>advs <span class="op">=</span> torch.tensor([</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>,],</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>,],</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>masks <span class="op">=</span> torch.tensor([</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># generation 1: 4 tokens</span></span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>,],</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># generation 2: 7 tokens</span></span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>,],</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>max_gen_len <span class="op">=</span> <span class="dv">7</span></span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>masked_mean_result <span class="op">=</span> masked_mean(ratio <span class="op">*</span> advs, masks, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>masked_mean_token_level <span class="op">=</span> masked_mean(ratio, masks, axis<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>masked_sum_result <span class="op">=</span> masked_sum(ratio <span class="op">*</span> advs, masks, axis<span class="op">=</span><span class="dv">1</span>, constant_normalizer<span class="op">=</span>max_gen_len)</span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;masked_mean&quot;</span>, masked_mean_result)</span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;masked_sum&quot;</span>, masked_sum_result)</span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;masked_mean_token_level&quot;</span>, masked_mean_token_level)</span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a><span class="co"># masked_mean tensor([2., 2.], grad_fn=&lt;DivBackward0&gt;)</span></span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a><span class="co"># masked_sum tensor([1.1429, 2.0000], grad_fn=&lt;DivBackward0&gt;)</span></span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a><span class="co"># masked_mean_token_level tensor(1., grad_fn=&lt;DivBackward0&gt;)</span></span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a>masked_mean_result.mean().backward()</span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;ratio.grad&quot;</span>, ratio.grad)</span>
<span id="cb6-57"><a href="#cb6-57" aria-hidden="true" tabindex="-1"></a>ratio.grad.zero_()</span>
<span id="cb6-58"><a href="#cb6-58" aria-hidden="true" tabindex="-1"></a><span class="co"># ratio.grad tensor([[0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000],</span></span>
<span id="cb6-59"><a href="#cb6-59" aria-hidden="true" tabindex="-1"></a><span class="co"># [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429]])</span></span>
<span id="cb6-60"><a href="#cb6-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-61"><a href="#cb6-61" aria-hidden="true" tabindex="-1"></a>masked_sum_result.mean().backward()</span>
<span id="cb6-62"><a href="#cb6-62" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;ratio.grad&quot;</span>, ratio.grad)</span>
<span id="cb6-63"><a href="#cb6-63" aria-hidden="true" tabindex="-1"></a>ratio.grad.zero_()</span>
<span id="cb6-64"><a href="#cb6-64" aria-hidden="true" tabindex="-1"></a><span class="co"># ratio.grad tensor([[0.1429, 0.1429, 0.1429, 0.1429, 0.0000, 0.0000, 0.0000],</span></span>
<span id="cb6-65"><a href="#cb6-65" aria-hidden="true" tabindex="-1"></a><span class="co"># [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429]])</span></span>
<span id="cb6-66"><a href="#cb6-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-67"><a href="#cb6-67" aria-hidden="true" tabindex="-1"></a>masked_mean_token_level.mean().backward()</span>
<span id="cb6-68"><a href="#cb6-68" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;ratio.grad&quot;</span>, ratio.grad)</span>
<span id="cb6-69"><a href="#cb6-69" aria-hidden="true" tabindex="-1"></a><span class="co"># ratio.grad tensor([[0.0909, 0.0909, 0.0909, 0.0909, 0.0000, 0.0000, 0.0000],</span></span>
<span id="cb6-70"><a href="#cb6-70" aria-hidden="true" tabindex="-1"></a><span class="co"># [0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909]])</span></span></code></pre></div>
<p>The output shows that with Strategy 1 (<code>masked_mean</code>), the short sequence has larger per-token gradients (0.25) than the long sequence (0.14). Strategies 2 and 3 equalize the per-token gradients across sequences. Note that these results can vary substantially if gradient accumulation is used, where the gradients are summed across multiple minibatches before taking a backward step‚Äîin this case, the balance between shorter and longer sequences can flip.</p>
<p>In practice, the best strategy depends on the specific training setup. Often in RLHF the method with the best numerical stability or the least variance in loss is preferred.</p>
</section>
<section id="related-mdp-vs-bandit-framing" class="level4">
<h4>Related: MDP vs Bandit Framing</h4>
<p>The choice of loss aggregation connects to a deeper distinction in how we frame the RL problem. The <strong>MDP (token-level)</strong> view treats each token <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>a</mi><mi>t</mi></msub><annotation encoding="application/x-tex">a_t</annotation></semantics></math> as an action with state <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>s</mi><mi>t</mi></msub><annotation encoding="application/x-tex">s_t</annotation></semantics></math> being the running prefix. In practice, this is the framing used when we compute token-level advantages with a learned value function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">V(s_t)</annotation></semantics></math> (e.g., GAE <span class="citation" data-cites="schulman2015high"><a href="ch021.xhtml#ref-schulman2015high">[179]</a></span>) and apply KL penalties per token. PPO with a learned value network is the canonical example <span class="citation" data-cites="schulman2017proximal"><a href="ch021.xhtml#ref-schulman2017proximal">[183]</a></span>.</p>
<p>In contrast, the <strong>bandit (sequence-level)</strong> view treats the whole completion as a single action with one scalar reward <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math>. In code, this means computing a sequence-level advantage <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>A</mi><mtext mathvariant="normal">seq</mtext></msub><annotation encoding="application/x-tex">A_{\text{seq}}</annotation></semantics></math> and broadcasting it to all tokens. RLOO and GRPO-style advantages are often used in this bandit-style setting <span class="citation" data-cites="kool2019buy"><a href="ch021.xhtml#ref-kool2019buy">[182]</a></span> <span class="citation" data-cites="ahmadian2024back"><a href="ch021.xhtml#ref-ahmadian2024back">[178]</a></span> <span class="citation" data-cites="shao2024deepseekmath"><a href="ch021.xhtml#ref-shao2024deepseekmath">[188]</a></span>. Direct alignment methods like DPO and A-LoL also define sequence-level objectives, although they are not policy-gradient estimators <span class="citation" data-cites="baheti2023leftover"><a href="ch021.xhtml#ref-baheti2023leftover">[194]</a></span>.</p>
<p>Note that many GRPO implementations use a bandit-style advantage <em>and</em> add a separate per-token KL term in the loss, while many PPO/RLOO implementations fold KL into the reward before computing advantages; both conventions exist in practice.</p>
</section>
</section>
<section id="asynchronicity" class="level3">
<h3>Asynchronicity</h3>
<p>The default implementation for policy-gradient algorithms is what is called <strong>on-policy</strong> execution, where the actions (generations) taken by the agent (language model) are scored before updating the model. The theoretical derivations of policy-gradient rely on all actions being exactly on-policy where the model is always up to date with the results from the latest trials/roll-outs. In practice, maintaining exact on-policy execution substantially slows training <span class="citation" data-cites="noukhovitch2024asynchronous"><a href="ch021.xhtml#ref-noukhovitch2024asynchronous">[195]</a></span>‚Äîand perfect synchronization is technically impossible regardless. Therefore, all of the recent empirical results with language models tend to be slightly outside of the theoretical proofs. What happens in practice is designing the algorithms and systems for what actually works.</p>
<figure id="fig:async">
<img src="../media/file14.png" alt="Figure 16: A comparison of the generation-update phases for synchronous or asynchronous RL training following Noukhovitch et al.¬†2024." />
<figcaption aria-hidden="true">Figure 16: A comparison of the generation-update phases for synchronous or asynchronous RL training following Noukhovitch et al.¬†2024.</figcaption>
</figure>
<p>The common solution used is to constantly run inference and training on separate GPU nodes with software designed to efficiently run both, as shown in the bottom of fig.¬†<a href="#fig:async">16</a>. Common practice in popular open-source RL tools for language models is to use a distributed process management library such as Ray to hand information off between the policy-gradient learning loop and the inference loop using an efficient inference engine, e.g., VLLM. In these setups, the GPUs dedicated to taking the RL steps are called the ‚Äúleaners‚Äù and the GPUs dedicated to sampling from the language model are called the ‚Äúactors‚Äù The primary challenges faced when making training more asynchronous are keeping training stable and maintaining learning signal.</p>
<figure id="fig:async_system">
<img src="../media/file15.png" alt="Figure 17: An example distributed RL system, where two queues are managed to pass data to the learner and actor GPUs, which can both be synchonized with a distributed computing library such as Ray. Olmo Team 2025, license CC-BY." />
<figcaption aria-hidden="true">Figure 17: An example distributed RL system, where two queues are managed to pass data to the learner and actor GPUs, which can both be synchonized with a distributed computing library such as Ray. Olmo Team 2025, license CC-BY.</figcaption>
</figure>
<p>These systems are designed and implemented with the presumption that nearly on-policy data is good enough for stable learning. Here, the generation and update phases can easily be synced to avoid idle compute on either piece of the training system, which would be passing model weights from the leaners to the actors in fig.¬†<a href="#fig:async_system">17</a>. With reasoning models, the extremely long inference characteristics of problems requiring 10K to 100K+ tokens per answer makes the generation of roll-outs a far stronger bottleneck. A common problem when training reasoning models on more synchronous RL infrastructure is that an answer to one prompt in the batch can take substantially more time to generate (either through more tokens or more tool calls), resulting in the majority of the allocated compute being idle until it completes. A second solution to this length mismatch issue, called sequence-level packing, is to stack shorter samples within a batch with clever masking to enable continued roll-outs from the model and better distribute length normalization across samples within a batch. The full complexity of distributed RL infrastructure is out of scope for this book, as it can cause many other subtle issues that slow down training or cause instability.</p>
<p>Following the emergence of these reasoning models, further interest has been taken to make the training and inference loops fully off-policy, where training batches for the policy gradient updates are filled with the most recently completed roll-outs across multiple instances generating answers <span class="citation" data-cites="wu2025llamarl"><a href="ch021.xhtml#ref-wu2025llamarl">[196]</a></span> <span class="citation" data-cites="fu2025areal"><a href="ch021.xhtml#ref-fu2025areal">[197]</a></span>. Fully asynchronous training would also enable scaling RL training runs across multiple datacenters more easily due to the option of increasing the time between weight syncs between the learner node (taking policy gradient steps) and the actor (trying to solve problems) <span class="citation" data-cites="primeintellectteam2025intellect2reasoningmodeltrained"><a href="ch021.xhtml#ref-primeintellectteam2025intellect2reasoningmodeltrained">[198]</a></span>.</p>
<p>Related methods are exploring fully off-policy policy gradient algorithms <span class="citation" data-cites="roux2025tapered"><a href="ch021.xhtml#ref-roux2025tapered">[199]</a></span>.</p>
</section>
<section id="proximal-policy-optimization-1" class="level3">
<h3>Proximal Policy Optimization</h3>
<p>There are many, many implementations of PPO available. The core <em>loss</em> computation is shown below. Crucial to stable performance is also the <em>value</em> computation, where multiple options exist (including multiple options for the <em>value model</em> loss).</p>
<p>Note that the reference policy (or old logprobs) here are from the time the generations were sampled and not necessarily the reference policy. The reference policy is only used for the KL distance constraint/penalty.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># B: Batch Size, L: Sequence Length, G: Num of Generations</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply KL penalty to rewards</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>rewards <span class="op">=</span> rewards <span class="op">-</span> <span class="va">self</span>.beta <span class="op">*</span> per_token_kl  <span class="co"># Shape: (B*G, L)</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Get value predictions</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> value_net(completions)  <span class="co"># Shape: (B*G, L)</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute simple advantages</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>advantages <span class="op">=</span> rewards <span class="op">-</span> values.detach()  <span class="co"># Shape: (B*G, L)</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: We detach the value network here to not update the parameters of </span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="co"># the value function when computing the policy-gradient loss</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Normalize advantages (optional but stable)</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>advantages <span class="op">=</span> (advantages <span class="op">-</span> advantages.mean()) <span class="op">/</span> (advantages.std() <span class="op">+</span> <span class="fl">1e-8</span>)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute probability ratio between new and old policies</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>ratio <span class="op">=</span> torch.exp(new_per_token_logps <span class="op">-</span> per_token_logps)  <span class="co"># Shape: (B*G, L)</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a><span class="co"># PPO clipping objective</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>eps <span class="op">=</span> <span class="va">self</span>.cliprange  <span class="co"># e.g. 0.2</span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>pg_losses1 <span class="op">=</span> <span class="op">-</span>advantages <span class="op">*</span> ratio  <span class="co"># Shape: (B*G, L)</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>pg_losses2 <span class="op">=</span> <span class="op">-</span>advantages <span class="op">*</span> torch.clamp(ratio, <span class="fl">1.0</span> <span class="op">-</span> eps, <span class="fl">1.0</span> <span class="op">+</span> eps)  <span class="co"># Shape: (B*G, L)</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>pg_loss_max <span class="op">=</span> torch.<span class="bu">max</span>(pg_losses1, pg_losses2)  <span class="co"># Shape: (B*G, L)</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple value function loss</span></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>vf_loss <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> ((rewards <span class="op">-</span> values) <span class="op">**</span> <span class="dv">2</span>)  <span class="co"># Shape: (B*G, L)</span></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine policy and value losses</span></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>per_token_loss <span class="op">=</span> pg_loss_max <span class="op">+</span> <span class="va">self</span>.vf_coef <span class="op">*</span> vf_loss  <span class="co"># Shape: (B*G, L)</span></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply completion mask and compute final loss</span></span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> ((per_token_loss <span class="op">*</span> completion_mask).<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>) <span class="op">/</span> completion_mask.<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>)).mean()</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a> <span class="co"># Scalar</span></span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute metrics for logging</span></span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute clipping fraction</span></span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>    clip_frac <span class="op">=</span> ((pg_losses2 <span class="op">&gt;</span> pg_losses1).<span class="bu">float</span>() <span class="op">*</span> completion_mask).<span class="bu">sum</span>() <span class="op">/</span> completion_mask.<span class="bu">sum</span>()</span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute approximate KL</span></span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>    approx_kl <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> ((new_per_token_logps <span class="op">-</span> per_token_logps)<span class="op">**</span><span class="dv">2</span>).mean()</span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute value loss for logging</span></span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a>    value_loss <span class="op">=</span> vf_loss.mean()</span></code></pre></div>
<p>The core piece to understand with PPO is how the policy gradient loss is updated. Focus on these three lines:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>pg_losses1 <span class="op">=</span> <span class="op">-</span>advantages <span class="op">*</span> ratio  <span class="co"># Shape: (B*G, L)</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>pg_losses2 <span class="op">=</span> <span class="op">-</span>advantages <span class="op">*</span> torch.clamp(ratio, <span class="fl">1.0</span> <span class="op">-</span> eps, <span class="fl">1.0</span> <span class="op">+</span> eps)  <span class="co"># Shape: (B*G, L)</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>pg_loss_max <span class="op">=</span> torch.<span class="bu">max</span>(pg_losses1, pg_losses2)  <span class="co"># Shape: (B*G, L)</span></span></code></pre></div>
<p><code>pg_losses1</code> is the vanilla advantage-weighted policy gradient loss. <code>pg_losses2</code> applies the same formula but with the probability ratio clamped to the range <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">[</mo><mn>1</mn><mo>‚àí</mo><mi>Œµ</mi><mo>,</mo><mn>1</mn><mo>+</mo><mi>Œµ</mi><mo stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[1-\varepsilon, 1+\varepsilon]</annotation></semantics></math>, limiting how much the policy can change in a single update.</p>
<p>The key insight is taking <code>torch.max</code> of the two losses. Because we‚Äôre minimizing a <em>negative</em> loss (recall the negative sign in front of advantages), taking the maximum selects the more pessimistic gradient‚Äîthe one that produces a smaller policy update. When the advantage is positive (good action), clipping prevents the policy from increasing that action‚Äôs probability too aggressively. When the advantage is negative (bad action), clipping prevents over-correction in the other direction.</p>
<p>By clamping the log-probability ratio, PPO bounds how far the policy can drift from the version that generated the training data, stabilizing learning without requiring an explicit trust region computation.</p>
<p>The code above also shows PPO learning a value function alongside the policy, which adds implementation complexity, but the clipped objective is the core mechanism.</p>
<section id="ppogrpo-simplification-with-1-gradient-step-per-sample-no-clipping" class="level4">
<h4>PPO/GRPO simplification with 1 gradient step per sample (no clipping)</h4>
<p>PPO (and GRPO) implementations can be handled much more elegantly if the hyperparameter ‚Äúnumber of gradient steps per sample‚Äù is equal to 1. Many typical values for this are from 2-4 or higher. In the main PPO or GRPO equations, see eq.¬†<a href="ch011.xhtml#eq:PPO_EQN">54</a>, the ‚Äúreference‚Äù policy is the previous parameters ‚Äì those used to generate the completions or actions. Thus, if only one gradient step is taken, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo>=</mo><msub><mi>œÄ</mi><msub><mi>Œ∏</mi><mtext mathvariant="normal">old</mtext></msub></msub></mrow><annotation encoding="application/x-tex">\pi_\theta = \pi_{\theta_{\text{old}}}</annotation></semantics></math>, and the update rule reduces to the following (the notation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">[</mo><msub><mo stretchy="false" form="postfix">]</mo><mi>‚àá</mi></msub></mrow><annotation encoding="application/x-tex">[]_\nabla</annotation></semantics></math> indicates a stop gradient):</p>
<p><span id="eq:ppo_1step"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>G</mi></mfrac><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>G</mi></munderover><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">|</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><msub><mrow><mo stretchy="true" form="prefix">[</mo><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">|</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow><mi>‚àá</mi></msub></mfrac><msub><mi>A</mi><mi>i</mi></msub><mo>‚àí</mo><mi>Œ≤</mi><msub><mi>ùíü</mi><mtext mathvariant="normal">KL</mtext></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">|</mo><mo stretchy="false" form="prefix">|</mo><msub><mi>œÄ</mi><mtext mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>64</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">J(\theta) = \frac{1}{G}\sum_{i=1}^G \left(\frac{\pi_\theta(a_i|s)}{\left[\pi_{\theta}(a_i|s)\right]_\nabla}A_i - \beta \mathcal{D}_{\text{KL}}(\pi_\theta||\pi_{\text{ref}})\right). \qquad{(64)}</annotation></semantics></math></span></p>
<p>This leads to PPO or GRPO implementations where the second policy gradient and clipping logic can be omitted, making the optimizer far closer to standard policy gradient.</p>
</section>
</section>
<section id="group-relative-policy-optimization-1" class="level3">
<h3>Group Relative Policy Optimization</h3>
<p>The DeepSeekMath paper describes some implementation details of GRPO that differ from PPO <span class="citation" data-cites="shao2024deepseekmath"><a href="ch021.xhtml#ref-shao2024deepseekmath">[188]</a></span>, especially if comparing to a standard application of PPO from Deep RL rather than language models. For example, the KL penalty within the RLHF optimization (recall the KL penalty is also used when training reasoning models on verifiable rewards without a reward model) is applied directly in the loss update rather than to the reward function. Where the standard KL penalty application for RLHF is applied as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo>=</mo><msub><mi>r</mi><mi>Œ∏</mi></msub><mo>‚àí</mo><mi>Œ≤</mi><msub><mi>ùíü</mi><mtext mathvariant="normal">KL</mtext></msub></mrow><annotation encoding="application/x-tex">r=r_\theta - \beta \mathcal{D}_{\text{KL}}</annotation></semantics></math>, the GRPO implementation is along the lines of:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>=</mo><msub><mi>L</mi><mtext mathvariant="normal">policy gradient</mtext></msub><mo>+</mo><mi>Œ≤</mi><mo>*</mo><msub><mi>ùíü</mi><mtext mathvariant="normal">KL</mtext></msub></mrow><annotation encoding="application/x-tex"> L = L_{\text{policy gradient}} + \beta * \mathcal{D}_{\text{KL}} </annotation></semantics></math></p>
<p>Though, there are multiple ways to implement this. Traditionally, the KL distance is computed with respect to each token in the completion to a prompt <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math>. For reasoning training, multiple completions are sampled from one prompt, and there are multiple prompts in one batch, so the KL distance will have a shape of [B, L, N], where B is the batch size, L is the sequence length, and N is the number of completions per prompt.</p>
<p>Putting it together, using the first loss accumulation, the pseudocode can be written as below.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># B: Batch Size, L: Sequence Length, G: Number of Generations</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute grouped-wise rewards # Shape: (B,)</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>mean_grouped_rewards <span class="op">=</span> rewards.view(<span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.num_generations).mean(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>std_grouped_rewards <span class="op">=</span> rewards.view(<span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.num_generations).std(dim<span class="op">=</span><span class="dv">1</span>)    </span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Normalize the rewards to compute the advantages</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>mean_grouped_rewards <span class="op">=</span> mean_grouped_rewards.repeat_interleave(<span class="va">self</span>.num_generations, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>std_grouped_rewards <span class="op">=</span> std_grouped_rewards.repeat_interleave(<span class="va">self</span>.num_generations, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Shape: (B*G,)</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute advantages</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>advantages <span class="op">=</span> (rewards <span class="op">-</span> mean_grouped_rewards) <span class="op">/</span> (std_grouped_rewards <span class="op">+</span> <span class="fl">1e-4</span>)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>advantages <span class="op">=</span> advantages.unsqueeze(<span class="dv">1</span>)</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Shape: (B*G, 1)</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute probability ratio between new and old policies</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>ratio <span class="op">=</span> torch.exp(new_per_token_logps <span class="op">-</span> per_token_logps)  <span class="co"># Shape: (B*G, L)</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a><span class="co"># PPO clipping objective</span></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>eps <span class="op">=</span> <span class="va">self</span>.cliprange  <span class="co"># e.g. 0.2</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>pg_losses1 <span class="op">=</span> <span class="op">-</span>advantages <span class="op">*</span> ratio  <span class="co"># Shape: (B*G, L)</span></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>pg_losses2 <span class="op">=</span> <span class="op">-</span>advantages <span class="op">*</span> torch.clamp(ratio, <span class="fl">1.0</span> <span class="op">-</span> eps, <span class="fl">1.0</span> <span class="op">+</span> eps)  <span class="co"># Shape: (B*G, L)</span></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>pg_loss_max <span class="op">=</span> torch.<span class="bu">max</span>(pg_losses1, pg_losses2)  <span class="co"># Shape: (B*G, L)</span></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a><span class="co"># important to GRPO -- PPO applies this in reward traditionally</span></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine with KL penalty</span></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>per_token_loss <span class="op">=</span> pg_loss_max <span class="op">+</span> <span class="va">self</span>.beta <span class="op">*</span> per_token_kl  <span class="co"># Shape: (B*G, L)</span></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply completion mask and compute final loss</span></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> ((per_token_loss <span class="op">*</span> completion_mask).<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>) <span class="op">/</span> completion_mask.<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>)).mean()</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a> <span class="co"># Scalar</span></span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute core metric for logging (KL, reward, etc. also logged)</span></span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute clipping fraction</span></span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>    clip_frac <span class="op">=</span> ((pg_losses2 <span class="op">&gt;</span> pg_losses1).<span class="bu">float</span>() <span class="op">*</span> completion_mask).<span class="bu">sum</span>() <span class="op">/</span> completion_mask.<span class="bu">sum</span>()</span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute approximate KL</span></span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a>    approx_kl <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> ((new_per_token_logps <span class="op">-</span> per_token_logps)<span class="op">**</span><span class="dv">2</span>).mean()</span></code></pre></div>
<p>For more details on how to interpret this code, see the PPO section above. The core differences from the PPO example are:</p>
<ul>
<li><strong>Advantage computation</strong>: GRPO normalizes rewards relative to the group (mean and std across generations for the same prompt) rather than using a learned value function as baseline.</li>
<li><strong>No value network</strong>: GRPO removes the value model entirely, eliminating <code>vf_loss</code> and the associated complexity.</li>
<li><strong>KL penalty placement</strong>: GRPO adds the KL penalty directly to the loss rather than subtracting it from the reward (this is the standard implementation, but more versions exist on how the KL is applied).</li>
</ul>
<section id="rloo-vs.-grpo" class="level4">
<h4>RLOO vs.¬†GRPO</h4>
<p>The advantage updates for RLOO follow very closely to GRPO, highlighting the conceptual similarity of the algorithm when taken separately from the PPO style clipping and KL penalty details. Specifically, for RLOO, the advantage is computed relative to a baseline that is extremely similar to that of GRPO ‚Äì the completion reward relative to the others for that same question. Concisely, the RLOO advantage estimate follows as (expanded from <a href="https://github.com/huggingface/trl/blob/bfe20756082488350091352d1cdc19c172e42cd8/trl/trainer/rloo_trainer.py#L433">TRL</a>‚Äôs implementation):</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># rloo_k --&gt; number of completions per prompt </span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="co"># rlhf_reward --&gt; Initially a flat tensor of total rewards for all completions. Length B = N x k</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>rlhf_reward <span class="op">=</span> rlhf_reward.reshape(rloo_k, <span class="op">-</span><span class="dv">1</span>) <span class="co"># </span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Now, Shape: (k, N), each column j contains the k rewards for prompt j.</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>baseline <span class="op">=</span> (rlhf_reward.<span class="bu">sum</span>(<span class="dv">0</span>) <span class="op">-</span> rlhf_reward) <span class="op">/</span> (rloo_k <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="co"># baseline --&gt; Leave-one-out baseline rewards. Shape: (k, N)</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="co">#  baseline[i, j] is the avg reward of samples i&#39; != i for prompt j.</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>advantages <span class="op">=</span> rlhf_reward <span class="op">-</span> baseline</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="co"># advantages --&gt; Same Shape: (k, N)</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>advantages <span class="op">=</span> advantages.flatten() <span class="co"># Same shape as original tensor</span></span></code></pre></div>
<p>The rest of the implementation details for RLOO follow the other trade-offs of implementing policy-gradient.</p>
</section>
</section>
</section>
<section id="auxiliary-topics" class="level2">
<h2>Auxiliary Topics</h2>
<p>In order to master the application of policy-gradient algorithms, there are countless other considerations. Here we consider some of the long-tail of complexities in successfully deploying a policy-gradient RL algorithm.</p>
<section id="comparing-algorithms" class="level3">
<h3>Comparing Algorithms</h3>
<p>Here‚Äôs a summary of some of the discussed material (and foreshadowing to coming material on Direct Preference Optimization) when applied to RLHF. Here, on- or off-policy indicates the derivation (where most are applied slightly off-policy in practice). A reference policy here indicates if it is required for the optimization itself, rather than for a KL penalty.</p>
<table id="tbl:pg_compare">
<caption>Table 5: Comparing policy gradient algorithms (and friends).</caption>
<colgroup>
<col style="width: 6%" />
<col style="width: 12%" />
<col style="width: 13%" />
<col style="width: 15%" />
<col style="width: 17%" />
<col style="width: 34%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">Reward Model</th>
<th style="text-align: center;">Value Function</th>
<th style="text-align: center;">Reference Policy</th>
<th style="text-align: left;">Core Loss <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>‚Ñí</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{L}(\theta)</annotation></semantics></math></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>REINFORCE</strong></td>
<td style="text-align: center;">On-policy</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">No</td>
<td style="text-align: left;"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>‚àí</mi><mfrac><mn>1</mn><mi>T</mi></mfrac><msubsup><mo>‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></msubsup><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>‚à£</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mspace width="0.167em"></mspace><mo minsize="120%" maxsize="120%" stretchy="true" form="prefix">(</mo><msub><mi>G</mi><mi>t</mi></msub><mo>‚àí</mo><mi>b</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mo minsize="120%" maxsize="120%" stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">-\frac{1}{T}\sum_{t=1}^{T}\log \pi_\theta(a_t\mid s_t)\,\big(G_t - b(s_t)\big)</annotation></semantics></math></td>
</tr>
<tr>
<td style="text-align: left;"><strong>RLOO</strong></td>
<td style="text-align: center;">On-policy</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">No</td>
<td style="text-align: left;"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>‚àí</mi><mfrac><mn>1</mn><mi>K</mi></mfrac><msubsup><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><msub><mo>‚àë</mo><mi>t</mi></msub><mrow><mi mathvariant="normal">log</mi><mo>&#8289;</mo></mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>‚à£</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo stretchy="false" form="postfix">)</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>R</mi><mi>i</mi></msub><mo>‚àí</mo><mfrac><mn>1</mn><mrow><mi>K</mi><mo>‚àí</mo><mn>1</mn></mrow></mfrac><msub><mo>‚àë</mo><mrow><mi>j</mi><mo>‚â†</mo><mi>i</mi></mrow></msub><msub><mi>R</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">-\frac{1}{K}\sum_{i=1}^{K}\sum_t \log \pi_\theta(a_{i,t}\mid s_{i,t})\left(R_i-\frac{1}{K-1}\sum_{j\neq i}R_j\right)</annotation></semantics></math></td>
</tr>
<tr>
<td style="text-align: left;"><strong>PPO</strong></td>
<td style="text-align: center;">On-policy</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: left;"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>‚àí</mi><mfrac><mn>1</mn><mi>T</mi></mfrac><msubsup><mo>‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></msubsup><mrow><mi mathvariant="normal">min</mi><mo>&#8289;</mo></mrow><mspace width="-0.167em"></mspace><mo minsize="120%" maxsize="120%" stretchy="true" form="prefix">(</mo><msub><mi>œÅ</mi><mi>t</mi></msub><msub><mi>A</mi><mi>t</mi></msub><mo>,</mo><mspace width="0.222em"></mspace><mrow><mi mathvariant="normal">c</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">p</mi></mrow><mo stretchy="false" form="prefix">(</mo><msub><mi>œÅ</mi><mi>t</mi></msub><mo>,</mo><mn>1</mn><mo>‚àí</mo><mi>Œµ</mi><mo>,</mo><mn>1</mn><mo>+</mo><mi>Œµ</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub><mo minsize="120%" maxsize="120%" stretchy="true" form="postfix">)</mo><mo>;</mo><mspace width="0.222em"></mspace><msub><mi>œÅ</mi><mi>t</mi></msub><mo>=</mo><mfrac><mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>‚à£</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><msub><mi>Œ∏</mi><mtext mathvariant="normal">old</mtext></msub></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>‚à£</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">-\frac{1}{T}\sum_{t=1}^{T}\min\!\big(\rho_t A_t,\ \mathrm{clip}(\rho_t,1-\varepsilon,1+\varepsilon) A_t\big);\ \rho_t = \frac{\pi_\theta(a_t\mid s_t)}{\pi_{\theta_{\text{old}}}(a_t\mid s_t)}</annotation></semantics></math></td>
</tr>
<tr>
<td style="text-align: left;"><strong>GRPO</strong></td>
<td style="text-align: center;">On-policy</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: left;"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>‚àí</mi><mfrac><mn>1</mn><mi>G</mi></mfrac><msubsup><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>G</mi></msubsup><mrow><mi mathvariant="normal">min</mi><mo>&#8289;</mo></mrow><mspace width="-0.167em"></mspace><mo minsize="120%" maxsize="120%" stretchy="true" form="prefix">(</mo><msub><mi>œÅ</mi><mi>i</mi></msub><msub><mi>A</mi><mi>i</mi></msub><mo>,</mo><mspace width="0.222em"></mspace><mrow><mi mathvariant="normal">c</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">p</mi></mrow><mo stretchy="false" form="prefix">(</mo><msub><mi>œÅ</mi><mi>i</mi></msub><mo>,</mo><mn>1</mn><mo>‚àí</mo><mi>Œµ</mi><mo>,</mo><mn>1</mn><mo>+</mo><mi>Œµ</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>i</mi></msub><mo minsize="120%" maxsize="120%" stretchy="true" form="postfix">)</mo><mo>;</mo><mspace width="0.222em"></mspace><msub><mi>œÅ</mi><mi>i</mi></msub><mo>=</mo><mfrac><mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo>‚à£</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>œÄ</mi><msub><mi>Œ∏</mi><mtext mathvariant="normal">old</mtext></msub></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo>‚à£</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo>,</mo><mspace width="0.222em"></mspace><msub><mi>A</mi><mi>i</mi></msub><mo>=</mo><mfrac><mrow><msub><mi>r</mi><mi>i</mi></msub><mo>‚àí</mo><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">n</mi></mrow><mo stretchy="false" form="prefix">(</mo><msub><mi>r</mi><mrow><mn>1</mn><mo>:</mo><mi>G</mi></mrow></msub><mo stretchy="false" form="postfix">)</mo></mrow><mrow><mrow><mi mathvariant="normal">s</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">d</mi></mrow><mo stretchy="false" form="prefix">(</mo><msub><mi>r</mi><mrow><mn>1</mn><mo>:</mo><mi>G</mi></mrow></msub><mo stretchy="false" form="postfix">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">-\frac{1}{G}\sum_{i=1}^{G}\min\!\big(\rho_i A_i,\ \mathrm{clip}(\rho_i,1-\varepsilon,1+\varepsilon) A_i\big);\ \rho_i = \frac{\pi_\theta(a_i\mid s)}{\pi_{\theta_{\text{old}}}(a_i\mid s)},\ A_i = \frac{r_i-\mathrm{mean}(r_{1:G})}{\mathrm{std}(r_{1:G})}</annotation></semantics></math></td>
</tr>
<tr>
<td style="text-align: left;"><strong>DPO</strong></td>
<td style="text-align: center;">Off-policy</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: left;"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>‚àí</mi><msub><mi>ùîº</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><msup><mi>y</mi><mi>w</mi></msup><mo>,</mo><msup><mi>y</mi><mi>l</mi></msup><mo stretchy="false" form="postfix">)</mo></mrow></msub><mspace width="-0.167em"></mspace><mrow><mo stretchy="true" form="prefix">[</mo><mi mathvariant="normal">log</mi><mi>œÉ</mi><mspace width="-0.167em"></mspace><mo minsize="120%" maxsize="120%" stretchy="true" form="prefix">(</mo><mi>Œ≤</mi><mo stretchy="false" form="prefix">[</mo><mi mathvariant="normal">Œî</mi><mi mathvariant="normal">log</mi><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>‚àí</mo><mi mathvariant="normal">Œî</mi><mi mathvariant="normal">log</mi><msub><mi>œÄ</mi><mrow><mi mathvariant="normal">r</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">f</mi></mrow></msub><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">]</mo><mo minsize="120%" maxsize="120%" stretchy="true" form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">-\mathbb{E}_{(x,y^{w},y^{l})}\!\left[\log \sigma\!\big(\beta[\Delta\log \pi_\theta(x)-\Delta\log \pi_{\mathrm{ref}}(x)]\big)\right]</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</section>
<section id="generalized-advantage-estimation-gae" class="level3">
<h3>Generalized Advantage Estimation (GAE)</h3>
<p>Generalized Advantage Estimation (GAE) is an alternate method to compute the advantage for policy gradient algorithms <span class="citation" data-cites="schulman2015high"><a href="ch021.xhtml#ref-schulman2015high">[179]</a></span> that better balances the bias-variance tradeoff. Traditional single-step advantage estimates can introduce too much bias, while using complete trajectories can suffer from high variance. GAE computes an exponentially-weighted average of multi-step advantage estimates, where the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œª</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> hyperparameter controls the bias-variance tradeoff‚Äîranging from single-step TD (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œª</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\lambda=0</annotation></semantics></math>) to full trajectory returns (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œª</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\lambda=1</annotation></semantics></math>).</p>
<p>Advantage estimates can take many forms, but we can define a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> step advantage estimator (similar to the TD residual at the beginning of the chapter) as follows:</p>
<p><span id="eq:K_STEP_ADV"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mover><mi>A</mi><mo accent="true">ÃÇ</mo></mover><mi>t</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow></msubsup><mo>=</mo><mrow><mo stretchy="true" form="prefix">{</mo><mtable><mtr><mtd columnalign="left" style="text-align: left"><msub><mi>r</mi><mi>t</mi></msub><mo>+</mo><mi>Œ≥</mi><mi>V</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false" form="postfix">)</mo><mo>‚àí</mo><mi>V</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>,</mo></mtd><mtd columnalign="left" style="text-align: left"><mi>n</mi><mo>=</mo><mn>1</mn></mtd></mtr><mtr><mtd columnalign="left" style="text-align: left"><msub><mi>r</mi><mi>t</mi></msub><mo>+</mo><mi>Œ≥</mi><msub><mi>r</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><msup><mi>Œ≥</mi><mn>2</mn></msup><mi>V</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>2</mn></mrow></msub><mo stretchy="false" form="postfix">)</mo><mo>‚àí</mo><mi>V</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>,</mo></mtd><mtd columnalign="left" style="text-align: left"><mi>n</mi><mo>=</mo><mn>2</mn></mtd></mtr><mtr><mtd columnalign="left" style="text-align: left"><mi>‚ãÆ</mi></mtd></mtr><mtr><mtd columnalign="left" style="text-align: left"><msub><mi>r</mi><mi>t</mi></msub><mo>+</mo><mi>Œ≥</mi><msub><mi>r</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><msup><mi>Œ≥</mi><mn>2</mn></msup><msub><mi>r</mi><mrow><mi>t</mi><mo>+</mo><mn>2</mn></mrow></msub><mo>+</mo><mi>‚ãØ</mi><mo>‚àí</mo><mi>V</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>,</mo></mtd><mtd columnalign="left" style="text-align: left"><mi>n</mi><mo>=</mo><mi>‚àû</mi></mtd></mtr></mtable></mrow><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>65</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
\hat{A}_t^{(n)} = \begin{cases}
r_t + \gamma V(s_{t+1}) - V(s_t), &amp; n = 1 \\
r_t + \gamma r_{t+1} + \gamma^2 V(s_{t+2}) - V(s_t), &amp; n = 2 \\
\vdots \\
r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \cdots - V(s_t), &amp; n = \infty
\end{cases}
\qquad{(65)}</annotation></semantics></math></span></p>
<p>Here a shorter <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> will have lower variance but higher bias as we are attributing more learning power to each trajectory ‚Äì it can overfit. GAE attempts to generalize this formulation into a weighted multi-step average instead of a specific <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>. To start, we must define the temporal difference (TD) residual of predicted value.</p>
<p><span id="eq:TD_RESIDUAL"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>Œ¥</mi><mi>t</mi><mi>V</mi></msubsup><mo>=</mo><msub><mi>r</mi><mi>t</mi></msub><mo>+</mo><mi>Œ≥</mi><mi>V</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false" form="postfix">)</mo><mo>‚àí</mo><mi>V</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>66</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
\delta_t^V = r_t + \gamma V(s_{t+1}) - V(s_t)
\qquad{(66)}</annotation></semantics></math></span></p>
<p>To utilize this, we introduce another variable <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œª</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> as the GAE mixing parameter. This folds into an exponential decay of future advantages we wish to estimate:</p>
<p><span id="eq:GAE_DFN"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtable><mtr><mtd columnalign="left" style="text-align: left"><msubsup><mover><mi>A</mi><mo accent="true">ÃÇ</mo></mover><mi>t</mi><mrow><mi>G</mi><mi>A</mi><mi>E</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ≥</mi><mo>,</mo><mi>Œª</mi><mo stretchy="false" form="postfix">)</mo></mrow></msubsup><mo>=</mo><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>‚àí</mo><mi>Œª</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="prefix">(</mo><msubsup><mover><mi>A</mi><mo accent="true">ÃÇ</mo></mover><mi>t</mi><mrow><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo stretchy="false" form="postfix">)</mo></mrow></msubsup><mo>+</mo><mi>Œª</mi><msubsup><mover><mi>A</mi><mo accent="true">ÃÇ</mo></mover><mi>t</mi><mrow><mo stretchy="false" form="prefix">(</mo><mn>2</mn><mo stretchy="false" form="postfix">)</mo></mrow></msubsup><mo>+</mo><msup><mi>Œª</mi><mn>2</mn></msup><msubsup><mover><mi>A</mi><mo accent="true">ÃÇ</mo></mover><mi>t</mi><mrow><mo stretchy="false" form="prefix">(</mo><mn>3</mn><mo stretchy="false" form="postfix">)</mo></mrow></msubsup><mo>+</mo><mi>‚ãØ</mi><mo stretchy="false" form="postfix">)</mo></mtd></mtr><mtr><mtd columnalign="left" style="text-align: left"><mo>=</mo><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>‚àí</mo><mi>Œª</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="prefix">(</mo><msubsup><mi>Œ¥</mi><mi>t</mi><mi>V</mi></msubsup><mo>+</mo><mi>Œª</mi><mo stretchy="false" form="prefix">(</mo><msubsup><mi>Œ¥</mi><mi>t</mi><mi>V</mi></msubsup><mo>+</mo><mi>Œ≥</mi><msubsup><mi>Œ¥</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mi>V</mi></msubsup><mo stretchy="false" form="postfix">)</mo><mo>+</mo><msup><mi>Œª</mi><mn>2</mn></msup><mo stretchy="false" form="prefix">(</mo><msubsup><mi>Œ¥</mi><mi>t</mi><mi>V</mi></msubsup><mo>+</mo><mi>Œ≥</mi><msubsup><mi>Œ¥</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mi>V</mi></msubsup><mo>+</mo><msup><mi>Œ≥</mi><mn>2</mn></msup><msubsup><mi>Œ¥</mi><mrow><mi>t</mi><mo>+</mo><mn>2</mn></mrow><mi>V</mi></msubsup><mo stretchy="false" form="postfix">)</mo><mo>+</mo><mi>‚ãØ</mi><mo stretchy="false" form="postfix">)</mo></mtd></mtr><mtr><mtd columnalign="left" style="text-align: left"><mo>=</mo><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>‚àí</mo><mi>Œª</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="prefix">(</mo><msubsup><mi>Œ¥</mi><mi>t</mi><mi>V</mi></msubsup><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>+</mo><mi>Œª</mi><mo>+</mo><msup><mi>Œª</mi><mn>2</mn></msup><mo>+</mo><mi>‚ãØ</mi><mo stretchy="false" form="postfix">)</mo><mo>+</mo><mi>Œ≥</mi><msubsup><mi>Œ¥</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mi>V</mi></msubsup><mo stretchy="false" form="prefix">(</mo><mi>Œª</mi><mo>+</mo><msup><mi>Œª</mi><mn>2</mn></msup><mo>+</mo><mi>‚ãØ</mi><mo stretchy="false" form="postfix">)</mo><mo>+</mo><mi>‚ãØ</mi><mo stretchy="false" form="postfix">)</mo></mtd></mtr><mtr><mtd columnalign="left" style="text-align: left"><mo>=</mo><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>‚àí</mo><mi>Œª</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="prefix">(</mo><msubsup><mi>Œ¥</mi><mi>t</mi><mi>V</mi></msubsup><mfrac><mn>1</mn><mrow><mn>1</mn><mo>‚àí</mo><mi>Œª</mi></mrow></mfrac><mo>+</mo><mi>Œ≥</mi><msubsup><mi>Œ¥</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mi>V</mi></msubsup><mfrac><mi>Œª</mi><mrow><mn>1</mn><mo>‚àí</mo><mi>Œª</mi></mrow></mfrac><mo>+</mo><mi>‚ãØ</mi><mo stretchy="false" form="postfix">)</mo></mtd></mtr><mtr><mtd columnalign="left" style="text-align: left"><mo>=</mo><munderover><mo>‚àë</mo><mrow><mi>l</mi><mo>=</mo><mn>0</mn></mrow><mo accent="false">‚àû</mo></munderover><mo stretchy="false" form="prefix">(</mo><mi>Œ≥</mi><mi>Œª</mi><msup><mo stretchy="false" form="postfix">)</mo><mi>l</mi></msup><msubsup><mi>Œ¥</mi><mrow><mi>t</mi><mo>+</mo><mi>l</mi></mrow><mi>V</mi></msubsup></mtd></mtr></mtable><mspace width="2.0em"></mspace><mrow><mo stretchy="false" form="prefix">(</mo><mn>67</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
\begin{array}{l}
\hat{A}_t^{GAE(\gamma,\lambda)} = (1-\lambda)(\hat{A}_t^{(1)} + \lambda\hat{A}_t^{(2)} + \lambda^2\hat{A}_t^{(3)} + \cdots) \\
= (1-\lambda)(\delta_t^V + \lambda(\delta_t^V + \gamma\delta_{t+1}^V) + \lambda^2(\delta_t^V + \gamma\delta_{t+1}^V + \gamma^2\delta_{t+2}^V) + \cdots) \\
= (1-\lambda)(\delta_t^V(1 + \lambda + \lambda^2 + \cdots) + \gamma\delta_{t+1}^V(\lambda + \lambda^2 + \cdots) + \cdots) \\
= (1-\lambda)(\delta_t^V\frac{1}{1-\lambda} + \gamma\delta_{t+1}^V\frac{\lambda}{1-\lambda} + \cdots) \\
= \sum_{l=0}^{\infty}(\gamma\lambda)^l\delta_{t+l}^V
\end{array}
\qquad{(67)}</annotation></semantics></math></span></p>
<p>Intuitively, this can be used to average multi-step estimates of Advantage in an elegant fashion. An example implementation is shown below:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># GAE (token-level) for LM RLHF</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co"># B: Batch Size</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co"># L: Length</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Inputs:</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="co">#   rewards: (B, L) post-KL per-token rewards</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co">#   values:  (B, L) current V_theta(s_t)</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="co">#   done_mask: (B, L) 1.0 at terminal token (EOS or penalized trunc), else 0.0</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co">#   gamma: float (often 1.0), lam: float in [0,1]</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>B, L <span class="op">=</span> rewards.shape</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>advantages <span class="op">=</span> torch.zeros_like(rewards)</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>next_v <span class="op">=</span> torch.zeros(B, device<span class="op">=</span>rewards.device, dtype<span class="op">=</span>rewards.dtype)</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>gae <span class="op">=</span> torch.zeros(B, device<span class="op">=</span>rewards.device, dtype<span class="op">=</span>rewards.dtype)</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">reversed</span>(<span class="bu">range</span>(L)):</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>    not_done <span class="op">=</span> <span class="fl">1.0</span> <span class="op">-</span> done_mask[:, t]</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>    delta <span class="op">=</span> rewards[:, t] <span class="op">+</span> gamma <span class="op">*</span> not_done <span class="op">*</span> next_v <span class="op">-</span> values[:, t]</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>    gae <span class="op">=</span> delta <span class="op">+</span> gamma <span class="op">*</span> lam <span class="op">*</span> not_done <span class="op">*</span> gae</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>    advantages[:, t] <span class="op">=</span> gae</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>    next_v <span class="op">=</span> values[:, t]</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>targets <span class="op">=</span> advantages <span class="op">+</span> values      <span class="co"># y_t for value regression</span></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>advantages <span class="op">=</span> advantages.detach()   <span class="co"># for policy loss</span></span></code></pre></div>
<p><em>For further reading, see <span class="citation" data-cites="seita2017gae"><a href="ch021.xhtml#ref-seita2017gae">[200]</a></span>.</em></p>
</section>
<section id="double-regularization" class="level3">
<h3>Double Regularization</h3>
<p>We‚Äôve seen in this chapter two types of regularization. One is built into algorithms like PPO with step-size constraints, and the other is a KL divergence based distance penalty relative to the start of the optimization.</p>
<p>Many popular policy gradient algorithms from Deep Reinforcement Learning, including PPO and its predecessors, originated due to the need to control the learning process of the agent. In RLHF, as discussed extensively in Chapter 8 on Regularization and in Chapter 4 on Problem Formulation, there is a built-in regularization term via the distance penalty relative to the original policy one is finetuning. In this view, a large part of the difference between algorithms like PPO (which have internal step-size regularization) and REINFORCE (which is simpler, and to which PPO reduces under certain hyperparameters) is far less meaningful for finetuning language models than training agents from scratch.</p>
<p>In PPO, the objective that handles capping the step-size of the update is known as the <a href="https://huggingface.co/blog/deep-rl-ppo#introducing-the-clipped-surrogate-objective">surrogate objective</a>. To monitor how much the PPO regularization is impacting updates in RLHF, one can look at the clip fraction variable in many popular implementations, which is the percentage of samples in the batch where the gradients are clipped by this regularizer in PPO. These gradients are <em>reduced</em> to a maximum value.</p>
<p>In practice with language models, algorithms like PPO and GRPO are run with only one gradient step per batch, which means that the PPO-native regularization is never applied (as clipping can only occur within a batch when the policy changes substantially) and the KL distances penalties predominate.</p>
</section>
<section id="further-reading-1" class="level3">
<h3>Further Reading</h3>
<p>As RLHF has cemented itself at the center of modern post-training, other policy-gradient RL algorithms and RL algorithms generally have been proposed to improve the training process, but they have not had a central role in governing best practices. Examples for further reading include:</p>
<ul>
<li><strong>Pairwise Proximal Policy Optimization (P3O)</strong> <span class="citation" data-cites="wu2023pairwise"><a href="ch021.xhtml#ref-wu2023pairwise">[201]</a></span> uses pairwise data directly in a PPO-style policy update without learning an intermediate reward model.</li>
<li>Off-policy policy-gradient algorithms could enable further asynchronous training, such as <strong>Contrastive Policy Gradient (CoPG)</strong> <span class="citation" data-cites="flet2024contrastive"><a href="ch021.xhtml#ref-flet2024contrastive">[202]</a></span> (a generalization of the direct alignment algorithm IPO and vanilla policy gradient), which was used by Cohere for their Command A model <span class="citation" data-cites="cohere2025command"><a href="ch021.xhtml#ref-cohere2025command">[58]</a></span>.</li>
<li>Other implementations of REINFORCE algorithms have been designed for language models, such as <strong>ReMax</strong> <span class="citation" data-cites="li2023remax"><a href="ch021.xhtml#ref-li2023remax">[203]</a></span>, which implements a baseline normalization designed specifically to accommodate the sources of uncertainty from reward model inference.</li>
<li>Some foundation models, such as Apple Intelligence Foundation Models <span class="citation" data-cites="gunter2024apple"><a href="ch021.xhtml#ref-gunter2024apple">[204]</a></span> or Kimi k1.5 reasoning model <span class="citation" data-cites="team2025kimi"><a href="ch021.xhtml#ref-team2025kimi">[205]</a></span>, have used variants of <strong>Mirror Descent Policy Optimization (MDPO)</strong> <span class="citation" data-cites="tomar2020mirror"><a href="ch021.xhtml#ref-tomar2020mirror">[206]</a></span>. Research is still developing further on the fundamentals here <span class="citation" data-cites="zhang2025improving"><a href="ch021.xhtml#ref-zhang2025improving">[207]</a></span>, but Mirror Descent is an optimization method rather than directly a policy gradient algorithm. What is important here is that it is substituted in very similarly to existing RL infrastructure.</li>
<li><strong>Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO)</strong> proposes 4 modifications to GRPO to better suit reasoning language models, where long traces are needed and new, underutilized tokens need to be increased in probability <span class="citation" data-cites="yu2025dapo"><a href="ch021.xhtml#ref-yu2025dapo">[193]</a></span>. The changes are: 1, have two different clip hyperparameters, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œµ</mi><mtext mathvariant="normal">low</mtext></msub><annotation encoding="application/x-tex">\varepsilon_\text{low}</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œµ</mi><mtext mathvariant="normal">high</mtext></msub><annotation encoding="application/x-tex">\varepsilon_\text{high}</annotation></semantics></math>, so clipping on the positive side of the logratio can take bigger steps for better exploration; 2, dynamic sampling, which removes all samples with reward = 0 or reward = 1 for all samples in the batch (no learning signal); 3, use the per token loss as discussed above in Implementation: GRPO; and 4, a soft penalty on samples that are too long to avoid trying to learn from truncated answers.</li>
<li><strong>Value-based Augmented Proximal Policy Optimization (VAPO)</strong> <span class="citation" data-cites="yuan2025vapo"><a href="ch021.xhtml#ref-yuan2025vapo">[208]</a></span> combines optimizations from DAPO (including clip-higher, token level policy-gradient, and different length normalization) with insights from Value-Calibrated PPO <span class="citation" data-cites="yuan2025s"><a href="ch021.xhtml#ref-yuan2025s">[209]</a></span> to pretrain the value function and length-adaptive GAE to show the promise of value base methods relative to GRPO.</li>
</ul>
</section>
</section>
</section>
</body>
</html>
