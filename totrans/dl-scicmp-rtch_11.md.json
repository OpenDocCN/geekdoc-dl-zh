["```r\nlibrary(torch)\n\n# compute gradient of loss w.r.t. all tensors with\n# requires_grad = TRUE\nloss$backward()\n\n### -------- Update weights -------- \n\n# Wrap in with_no_grad() because this is a part we don't \n# want to record for automatic gradient computation\nwith_no_grad({\n w1 <- w1$sub_(learning_rate * w1$grad)\n w2 <- w2$sub_(learning_rate * w2$grad)\n b1 <- b1$sub_(learning_rate * b1$grad)\n b2 <- b2$sub_(learning_rate * b2$grad) \n\n # Zero gradients after every pass, as they'd accumulate\n # otherwise\n w1$grad$zero_()\n w2$grad$zero_()\n b1$grad$zero_()\n b2$grad$zero_() \n})\n```", "```r\nl <- nn_linear(10, 2)\n\nopt <- optim_sgd(l$parameters, lr = 0.1)\n```", "```r\n# compute gradient of loss w.r.t. all tensors with\n# requires_grad = TRUE\n# no change here\nloss$backward()\n\n# Still need to zero out gradients before the backward pass,\n# only this time, on the optimizer object\noptimizer$zero_grad()\n\n# use the optimizer to update model parameters\noptimizer$step()\n```"]