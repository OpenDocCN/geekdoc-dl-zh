- en: Reward Modeling
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¥–åŠ±æ¨¡å‹
- en: Reward models are core to the modern approach to RLHF by being where the complex
    human preferences are learned. They are what enable our models to learn from hard
    to specify signals. They compress complex features in the data into a representation
    that can be used in downstream training â€“ a sort of magic that once again shows
    the complex capacity of modern deep learning. These models act as the proxy objectives
    by which the core optimization is done, as studied in the following chapters.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å¥–åŠ±æ¨¡å‹åœ¨ç°ä»£å¼ºåŒ–å­¦ä¹ ä¸äººç±»åé¦ˆå¼ºåŒ–ï¼ˆRLHFï¼‰æ–¹æ³•ä¸­å¤„äºæ ¸å¿ƒåœ°ä½ï¼Œå› ä¸ºå®ƒä»¬æ˜¯å­¦ä¹ å¤æ‚äººç±»åå¥½çš„åœ°æ–¹ã€‚å®ƒä»¬ä½¿å¾—æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿä»éš¾ä»¥æŒ‡å®šçš„ä¿¡å·ä¸­å­¦ä¹ ã€‚å®ƒä»¬å°†æ•°æ®ä¸­çš„å¤æ‚ç‰¹å¾å‹ç¼©æˆå¯ä»¥åœ¨ä¸‹æ¸¸è®­ç»ƒä¸­ä½¿ç”¨çš„è¡¨ç¤ºâ€”â€”è¿™æ˜¯ä¸€ç§å†æ¬¡å±•ç¤ºç°ä»£æ·±åº¦å­¦ä¹ å¤æ‚å®¹é‡çš„é­”æ³•ã€‚è¿™äº›æ¨¡å‹ä½œä¸ºä»£ç†ç›®æ ‡ï¼Œé€šè¿‡ä»¥ä¸‹ç« èŠ‚ç ”ç©¶çš„æ ¸å¿ƒä¼˜åŒ–è¿‡ç¨‹ã€‚
- en: Reward models broadly have historically been used extensively in reinforcement
    learning research as a proxy for environment rewards [[55]](ch021.xhtml#ref-sutton2018reinforcement).
    Reward models were proposed, in their modern form, as a tool for studying the
    value alignment problem [[33]](ch021.xhtml#ref-leike2018scalable). These models
    tend to take in some sort of input and output a single scalar value of reward.
    This reward can take multiple forms â€“ in traditional RL problems it was attempting
    to approximate the exact environment reward for the problem, but we will see in
    RLHF that reward models actually output a probability of a certain input being
    â€œof high qualityâ€ (i.e.Â the chosen answer among a pairwise preference relation).
    The practice of reward modeling for RLHF is closely related to inverse reinforcement
    learning, where the problem is to approximate an agentâ€™s reward function given
    trajectories of behavior [[96]](ch021.xhtml#ref-ng2000algorithms), and other areas
    of deep reinforcement learning. The high level problem statement is the same,
    but the implementation and focus areas are entirely different, so theyâ€™re often
    considered as totally separate areas of study.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: å¥–åŠ±æ¨¡å‹åœ¨å†å²ä¸Šè¢«å¹¿æ³›ç”¨äºå¼ºåŒ–å­¦ä¹ ç ”ç©¶ï¼Œä½œä¸ºç¯å¢ƒå¥–åŠ±çš„ä»£ç† [[55]](ch021.xhtml#ref-sutton2018reinforcement)ã€‚åœ¨ç°ä»£å½¢å¼ä¸­ï¼Œå¥–åŠ±æ¨¡å‹è¢«æå‡ºä½œä¸ºä¸€ç§ç ”ç©¶ä»·å€¼å¯¹é½é—®é¢˜çš„å·¥å…·
    [[33]](ch021.xhtml#ref-leike2018scalable)ã€‚è¿™äº›æ¨¡å‹é€šå¸¸æ¥å—æŸç§å½¢å¼çš„è¾“å…¥ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªå•ä¸€çš„å¥–åŠ±æ ‡é‡å€¼ã€‚è¿™ç§å¥–åŠ±å¯ä»¥æœ‰å¤šç§å½¢å¼â€”â€”åœ¨ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ é—®é¢˜ä¸­ï¼Œå®ƒè¯•å›¾è¿‘ä¼¼é—®é¢˜çš„ç¡®åˆ‡ç¯å¢ƒå¥–åŠ±ï¼Œä½†åœ¨RLHFä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¥–åŠ±æ¨¡å‹å®é™…ä¸Šè¾“å‡ºçš„æ˜¯æŸä¸ªè¾“å…¥â€œé«˜è´¨é‡â€çš„æ¦‚ç‡ï¼ˆå³åœ¨ä¸€å¯¹åå¥½å…³ç³»ä¸­é€‰æ‹©çš„ç­”æ¡ˆï¼‰ã€‚RLHFçš„å¥–åŠ±å»ºæ¨¡å®è·µä¸é€†å¼ºåŒ–å­¦ä¹ å¯†åˆ‡ç›¸å…³ï¼Œå…¶ä¸­é—®é¢˜æ˜¯åœ¨ç»™å®šè¡Œä¸ºè½¨è¿¹çš„æƒ…å†µä¸‹è¿‘ä¼¼ä»£ç†çš„å¥–åŠ±å‡½æ•°
    [[96]](ch021.xhtml#ref-ng2000algorithms)ï¼Œä»¥åŠå…¶ä»–æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„é¢†åŸŸã€‚é«˜å±‚é—®é¢˜é™ˆè¿°æ˜¯ç›¸åŒçš„ï¼Œä½†å®ç°å’Œå…³æ³¨é¢†åŸŸå®Œå…¨ä¸åŒï¼Œå› æ­¤å®ƒä»¬é€šå¸¸è¢«è§†ä¸ºå®Œå…¨ä¸åŒçš„ç ”ç©¶é¢†åŸŸã€‚
- en: The most common reward model, often called a Bradley-Terry reward model and
    the primary focus of this chapter, predicts the probability that a piece of text
    was close to a â€œpreferredâ€ piece of text from the training comparisons. Later
    in this section we also compare these to Outcome Reward Models (ORMs), Process
    Reward Model (PRM), and other types of reward models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€å¸¸è§çš„å¥–åŠ±æ¨¡å‹ï¼Œé€šå¸¸ç§°ä¸ºå¸ƒæ‹‰å¾·åˆ©-ç‰¹é‡Œå¥–åŠ±æ¨¡å‹ï¼Œä¹Ÿæ˜¯æœ¬ç« çš„ä¸»è¦ç„¦ç‚¹ï¼Œå®ƒé¢„æµ‹ä¸€æ®µæ–‡æœ¬æ¥è¿‘â€œé¦–é€‰â€æ–‡æœ¬çš„æ¦‚ç‡ï¼Œè¿™æ˜¯åŸºäºè®­ç»ƒæ¯”è¾ƒçš„ã€‚åœ¨æœ¬èŠ‚ç¨åï¼Œæˆ‘ä»¬è¿˜å°†å°†è¿™äº›æ¨¡å‹ä¸ç»“æœå¥–åŠ±æ¨¡å‹ï¼ˆORMsï¼‰ã€è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰å’Œå…¶ä»–ç±»å‹çš„å¥–åŠ±æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚
- en: '*Throughout this chapter, we use <semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics>
    to denote prompts and <semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics>
    to denote completions. This notation is common in the language model literature,
    where methods operate on full prompt-completion pairs rather than individual tokens.*'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨<semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics>æ¥è¡¨ç¤ºæç¤ºï¼Œä½¿ç”¨<semantics><mi>y</mi><annotation
    encoding="application/x-tex">y</annotation></semantics>æ¥è¡¨ç¤ºå®Œæˆã€‚è¿™ç§ç¬¦å·åœ¨è¯­è¨€æ¨¡å‹æ–‡çŒ®ä¸­å¾ˆå¸¸è§ï¼Œå…¶ä¸­æ–¹æ³•æ“ä½œçš„æ˜¯å®Œæ•´çš„æç¤º-å®Œæˆå¯¹ï¼Œè€Œä¸æ˜¯å•ä¸ªæ ‡è®°ã€‚*'
- en: Training Reward Models
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒå¥–åŠ±æ¨¡å‹
- en: 'The canonical implementation of a reward model is derived from the Bradley-Terry
    model of preference [[125]](ch021.xhtml#ref-BradleyTerry). There are two popular
    expressions for how to train a standard reward model for RLHF â€“ they are mathematically
    equivalent. To start, a Bradley-Terry model of preferences defines the probability
    that, in a pairwise comparison between two items <semantics><mi>i</mi><annotation
    encoding="application/x-tex">i</annotation></semantics> and <semantics><mi>j</mi><annotation
    encoding="application/x-tex">j</annotation></semantics>, a judge prefers <semantics><mi>i</mi><annotation
    encoding="application/x-tex">i</annotation></semantics> over <semantics><mi>j</mi><annotation
    encoding="application/x-tex">j</annotation></semantics>:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: å¥–åŠ±æ¨¡å‹çš„ç»å…¸å®ç°æºè‡ªäºå¸ƒæ‹‰å¾·åˆ©-ç‰¹é‡Œåå¥½æ¨¡å‹ [[125]](ch021.xhtml#ref-BradleyTerry)ã€‚å¯¹äºå¦‚ä½•è®­ç»ƒæ ‡å‡†å¥–åŠ±æ¨¡å‹ï¼ŒRLHF
    æœ‰ä¸¤ç§æµè¡Œçš„è¡¨è¾¾æ–¹å¼â€”â€”å®ƒä»¬åœ¨æ•°å­¦ä¸Šæ˜¯ç­‰ä»·çš„ã€‚é¦–å…ˆï¼Œå¸ƒæ‹‰å¾·åˆ©-ç‰¹é‡Œåå¥½æ¨¡å‹å®šä¹‰äº†åœ¨ä¸¤ä¸ªç‰©å“ <semantics><mi>i</mi><annotation
    encoding="application/x-tex">i</annotation></semantics> å’Œ <semantics><mi>j</mi><annotation
    encoding="application/x-tex">j</annotation></semantics> ä¹‹é—´çš„æˆå¯¹æ¯”è¾ƒä¸­ï¼Œä¸€ä¸ªè¯„åˆ¤è€…æ›´å–œæ¬¢ <semantics><mi>i</mi><annotation
    encoding="application/x-tex">i</annotation></semantics> è€Œä¸æ˜¯ <semantics><mi>j</mi><annotation
    encoding="application/x-tex">j</annotation></semantics> çš„æ¦‚ç‡ï¼š
- en: <semantics><mrow><mi>P</mi><mo stretchy="false" form="prefix">(</mo><mi>i</mi><mo>></mo><mi>j</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><msub><mi>p</mi><mi>i</mi></msub><mrow><msub><mi>p</mi><mi>i</mi></msub><mo>+</mo><msub><mi>p</mi><mi>j</mi></msub></mrow></mfrac><mi>.</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>11</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">P(i > j) = \frac{p_i}{p_i + p_j}.\qquad{(11)}</annotation></semantics>
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><mi>P</mi><mo stretchy="false" form="prefix">(</mo><mi>i</mi><mo>></mo><mi>j</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><msub><mi>p</mi><mi>i</mi></msub><mrow><msub><mi>p</mi><mi>i</mi></msub><mo>+</mo><msub><mi>p</mi><mi>j</mi></msub></mrow></mfrac><mi>.</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>11</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">P(i > j) = \frac{p_i}{p_i + p_j}.\qquad{(11)}</annotation></semantics>
- en: 'The Bradley-Terry model assumes that each item has a latent strength <semantics><mrow><msub><mi>p</mi><mi>i</mi></msub><mo>></mo><mn>0</mn></mrow><annotation
    encoding="application/x-tex">p_i > 0</annotation></semantics>, and that observed
    preferences are a noisy reflection of these underlying strengths. It is common
    to reparametrize the Bradley-Terry model with unbounded scores, where <semantics><mrow><msub><mi>p</mi><mi>i</mi></msub><mo>=</mo><msup><mi>e</mi><msub><mi>r</mi><mi>i</mi></msub></msup></mrow><annotation
    encoding="application/x-tex">p_i = e^{r_i}</annotation></semantics>, which results
    in the following form:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å¸ƒæ‹‰å¾·åˆ©-ç‰¹é‡Œæ¨¡å‹å‡è®¾æ¯ä¸ªç‰©å“éƒ½æœ‰ä¸€ä¸ªæ½œåœ¨çš„å¼ºåº¦ <semantics><mrow><msub><mi>p</mi><mi>i</mi></msub><mo>></mo><mn>0</mn></mrow><annotation
    encoding="application/x-tex">p_i > 0</annotation></semantics>ï¼Œå¹¶ä¸”è§‚å¯Ÿåˆ°çš„åå¥½æ˜¯è¿™äº›æ½œåœ¨å¼ºåº¦çš„æœ‰å™ªå£°çš„åæ˜ ã€‚é€šå¸¸ä½¿ç”¨æ— ç•Œåˆ†æ•°é‡æ–°å‚æ•°åŒ–å¸ƒæ‹‰å¾·åˆ©-ç‰¹é‡Œæ¨¡å‹ï¼Œå…¶ä¸­
    <semantics><mrow><msub><mi>p</mi><mi>i</mi></msub><mo>=</mo><msup><mi>e</mi><msub><mi>r</mi><mi>i</mi></msub></msup></mrow><annotation
    encoding="application/x-tex">p_i = e^{r_i}</annotation></semantics>ï¼Œè¿™å¯¼è‡´ä»¥ä¸‹å½¢å¼ï¼š
- en: <semantics><mrow><mi>P</mi><mo stretchy="false" form="prefix">(</mo><mi>i</mi><mo>></mo><mi>j</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><msup><mi>e</mi><msub><mi>r</mi><mi>i</mi></msub></msup><mrow><msup><mi>e</mi><msub><mi>r</mi><mi>i</mi></msub></msup><mo>+</mo><msup><mi>e</mi><msub><mi>r</mi><mi>j</mi></msub></msup></mrow></mfrac><mo>=</mo><mi>Ïƒ</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>r</mi><mi>i</mi></msub><mo>âˆ’</mo><msub><mi>r</mi><mi>j</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mi>.</mi><mrow><mo stretchy="false" form="prefix">(</mo><mn>12</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(i
    > j) = \frac{e^{r_i}}{e^{r_i} + e^{r_j}} = \sigma(r_i-r_j).\qquad{(12)}</annotation></semantics>
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><mi>P</mi><mo stretchy="false" form="prefix">(</mo><mi>i</mi><mo>></mo><mi>j</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><msup><mi>e</mi><msub><mi>r</mi><mi>i</mi></msub></msup><mrow><msup><mi>e</mi><msub><mi>r</mi><mi>i</mi></msub></msup><mo>+</mo><msup><mi>e</mi><msub><mi>r</mi><mi>j</mi></msub></msup></mrow></mfrac><mo>=</mo><mi>Ïƒ</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>r</mi><mi>i</mi></msub><mo>âˆ’</mo><msub><mi>r</mi><mi>j</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mi>.</mi><mrow><mo stretchy="false" form="prefix">(</mo><mn>12</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(i
    > j) = \frac{e^{r_i}}{e^{r_i} + e^{r_j}} = \sigma(r_i-r_j).\qquad{(12)}</annotation></semantics>
- en: 'Only differences in scores matter: adding the same constant to all <semantics><msub><mi>r</mi><mi>i</mi></msub><annotation
    encoding="application/x-tex">r_i</annotation></semantics> leaves <semantics><mrow><mi>P</mi><mo
    stretchy="false" form="prefix">(</mo><mi>i</mi><mo>></mo><mi>j</mi><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">P(i > j)</annotation></semantics>
    unchanged. These forms are not a law of nature, but a useful approximation of
    human preferences that often works well in RLHF.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åªæœ‰åˆ†æ•°çš„å·®å¼‚æ‰æ˜¯é‡è¦çš„ï¼šå¯¹æ‰€æœ‰<semantics><msub><mi>r</mi><mi>i</mi></msub><annotation encoding="application/x-tex">r_i</annotation></semantics>æ·»åŠ ç›¸åŒçš„å¸¸æ•°ä¸ä¼šæ”¹å˜<semantics><mrow><mi>P</mi><mo
    stretchy="false" form="prefix">(</mo><mi>i</mi><mo>></mo><mi>j</mi><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">P(i > j)</annotation></semantics>ã€‚è¿™äº›å½¢å¼ä¸æ˜¯è‡ªç„¶æ³•åˆ™ï¼Œä½†å®ƒä»¬æ˜¯å¯¹äººç±»åå¥½çš„æœ‰ç”¨è¿‘ä¼¼ï¼Œåœ¨RLHFä¸­é€šå¸¸å·¥ä½œå¾—å¾ˆå¥½ã€‚
- en: To train a reward model, we must formulate a loss function that satisfies the
    above relation. In practice, this is done by converting a language model into
    a model that outputs a scalar score, often via a small linear head that produces
    a single logit. Given a prompt <semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics>
    and two sampled completions <semantics><msub><mi>y</mi><mn>1</mn></msub><annotation
    encoding="application/x-tex">y_1</annotation></semantics> and <semantics><msub><mi>y</mi><mn>2</mn></msub><annotation
    encoding="application/x-tex">y_2</annotation></semantics>, we score both with
    a reward model <semantics><msub><mi>r</mi><mi>Î¸</mi></msub><annotation encoding="application/x-tex">r_\theta</annotation></semantics>
    and write the conditional scores as <semantics><mrow><msub><mi>r</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">r_\theta(y_i
    \mid x)</annotation></semantics>.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œæˆ‘ä»¬å¿…é¡»åˆ¶å®šä¸€ä¸ªæ»¡è¶³ä¸Šè¿°å…³ç³»çš„æŸå¤±å‡½æ•°ã€‚åœ¨å®è·µä¸­ï¼Œè¿™æ˜¯é€šè¿‡å°†è¯­è¨€æ¨¡å‹è½¬æ¢ä¸ºè¾“å‡ºæ ‡é‡åˆ†æ•°çš„æ¨¡å‹æ¥å®Œæˆçš„ï¼Œé€šå¸¸æ˜¯é€šè¿‡ä¸€ä¸ªå°çš„çº¿æ€§å¤´éƒ¨æ¥äº§ç”Ÿå•ä¸ªlogitã€‚ç»™å®šæç¤º<semantics><mi>x</mi><annotation
    encoding="application/x-tex">x</annotation></semantics>å’Œä¸¤ä¸ªé‡‡æ ·çš„å®Œæˆ<semantics><msub><mi>y</mi><mn>1</mn></msub><annotation
    encoding="application/x-tex">y_1</annotation></semantics>å’Œ<semantics><msub><mi>y</mi><mn>2</mn></msub><annotation
    encoding="application/x-tex">y_2</annotation></semantics>ï¼Œæˆ‘ä»¬ä½¿ç”¨å¥–åŠ±æ¨¡å‹<semantics><msub><mi>r</mi><mi>Î¸</mi></msub><annotation
    encoding="application/x-tex">r_\theta</annotation></semantics>å¯¹å®ƒä»¬è¿›è¡Œè¯„åˆ†ï¼Œå¹¶å°†æ¡ä»¶åˆ†æ•°å†™æˆ<semantics><mrow><msub><mi>r</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">r_\theta(y_i
    \mid x)</annotation></semantics>.
- en: 'The probability of success for a given reward model in a pairwise comparison
    becomes:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆå¯¹æ¯”è¾ƒä¸­ï¼Œç»™å®šå¥–åŠ±æ¨¡å‹çš„æˆåŠŸæ¦‚ç‡å˜ä¸ºï¼š
- en: <semantics><mrow><mi>P</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>></mo><msub><mi>y</mi><mn>2</mn></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mrow><mrow><mi mathvariant="normal">exp</mi><mo>â¡</mo></mrow><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>âˆ£</mo><mi>x</mi><mo stretchy="false"
    form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mrow><mi
    mathvariant="normal">exp</mi><mo>â¡</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mi
    mathvariant="normal">exp</mi><mo>â¡</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>2</mn></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mi>.</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>13</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">P(y_1 > y_2 \mid x) = \frac{\exp\left(r_\theta(y_1
    \mid x)\right)}{\exp\left(r_\theta(y_1 \mid x)\right) + \exp\left(r_\theta(y_2
    \mid x)\right)}.\qquad{(13)}</annotation></semantics>
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><mi>P</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>></mo><msub><mi>y</mi><mn>2</mn></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mrow><mrow><mi mathvariant="normal">exp</mi><mo>â¡</mo></mrow><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>âˆ£</mo><mi>x</mi><mo stretchy="false"
    form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mrow><mi
    mathvariant="normal">exp</mi><mo>â¡</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mi
    mathvariant="normal">exp</mi><mo>â¡</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mn>2</mn></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mi>.</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>13</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">P(y_1 > y_2 \mid x) = \frac{\exp\left(r_\theta(y_1
    \mid x)\right)}{\exp\left(r_\theta(y_1 \mid x)\right) + \exp\left(r_\theta(y_2
    \mid x)\right)}.\qquad{(13)}</annotation></semantics>
- en: We denote the preferred completion as <semantics><msub><mi>y</mi><mi>c</mi></msub><annotation
    encoding="application/x-tex">y_c</annotation></semantics> (chosen) and the rejected
    completion as <semantics><msub><mi>y</mi><mi>r</mi></msub><annotation encoding="application/x-tex">y_r</annotation></semantics>.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†é¦–é€‰çš„å®Œæˆé¡¹è¡¨ç¤ºä¸º <semantics><msub><mi>y</mi><mi>c</mi></msub><annotation encoding="application/x-tex">y_c</annotation></semantics>ï¼ˆé€‰æ‹©ï¼‰å’Œè¢«æ‹’ç»çš„å®Œæˆé¡¹è¡¨ç¤ºä¸º
    <semantics><msub><mi>y</mi><mi>r</mi></msub><annotation encoding="application/x-tex">y_r</annotation></semantics>ã€‚
- en: 'Then, by maximizing the log-likelihood of the above function (or alternatively
    minimizing the negative log-likelihood), we can arrive at the loss function to
    train a reward model:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œé€šè¿‡æœ€å¤§åŒ–ä¸Šè¿°å‡½æ•°çš„å¯¹æ•°ä¼¼ç„¶ï¼ˆæˆ–è€…ç­‰ä»·åœ°ï¼Œæœ€å°åŒ–è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°ç”¨äºè®­ç»ƒå¥–åŠ±æ¨¡å‹çš„æŸå¤±å‡½æ•°ï¼š
- en: '<semantics><mrow><mtable><mtr><mtd columnalign="right" style="text-align: right;
    padding-right: 0"><msup><mi>Î¸</mi><mo>*</mo></msup><mo>=</mo><mrow><mi mathvariant="normal">arg</mi><mo>â¡</mo></mrow><munder><mi
    mathvariant="normal">max</mi><mi>Î¸</mi></munder><mi>P</mi><mo stretchy="false"
    form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>></mo><msub><mi>y</mi><mi>r</mi></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mtd><mtd columnalign="left" style="text-align:
    left; padding-left: 0"><mo>=</mo><mrow><mi mathvariant="normal">arg</mi><mo>â¡</mo></mrow><munder><mi
    mathvariant="normal">max</mi><mi>Î¸</mi></munder><mfrac><mrow><mrow><mi mathvariant="normal">exp</mi><mo>â¡</mo></mrow><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>âˆ£</mo><mi>x</mi><mo stretchy="false"
    form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mrow><mi
    mathvariant="normal">exp</mi><mo>â¡</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mi
    mathvariant="normal">exp</mi><mo>â¡</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>r</mi></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac></mtd></mtr><mtr><mtd
    columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><mrow><mi
    mathvariant="normal">arg</mi><mo>â¡</mo></mrow><munder><mi mathvariant="normal">max</mi><mi>Î¸</mi></munder><mfrac><mrow><mrow><mi
    mathvariant="normal">exp</mi><mo>â¡</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mrow><mi
    mathvariant="normal">exp</mi><mo>â¡</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><mfrac><mrow><mrow><mi
    mathvariant="normal">exp</mi><mo>â¡</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>r</mi></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mrow><mi
    mathvariant="normal">exp</mi><mo>â¡</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac></mtd></mtr><mtr><mtd
    columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><mrow><mi
    mathvariant="normal">arg</mi><mo>â¡</mo></mrow><munder><mi mathvariant="normal">max</mi><mi>Î¸</mi></munder><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mfrac><mrow><mrow><mi
    mathvariant="normal">exp</mi><mo>â¡</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>r</mi></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mrow><mi
    mathvariant="normal">exp</mi><mo>â¡</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac></mrow></mfrac></mtd></mtr><mtr><mtd
    columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><mrow><mi
    mathvariant="normal">arg</mi><mo>â¡</mo></mrow><munder><mi mathvariant="normal">max</mi><mi>Î¸</mi></munder><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mrow><mi
    mathvariant="normal">exp</mi><mo>â¡</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>âˆ’</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>âˆ£</mo><mi>x</mi><mo stretchy="false"
    form="postfix">)</mo><mo>âˆ’</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><msub><mi>y</mi><mi>r</mi></msub><mo>âˆ£</mo><mi>x</mi><mo stretchy="false"
    form="postfix">)</mo><mo stretchy="false" form="postfix">)</mo><mo stretchy="true"
    form="postfix">)</mo></mrow></mrow></mfrac></mtd></mtr><mtr><mtd columnalign="left"
    style="text-align: left; padding-left: 0"><mo>=</mo><mrow><mi mathvariant="normal">arg</mi><mo>â¡</mo></mrow><munder><mi
    mathvariant="normal">max</mi><mi>Î¸</mi></munder><mi>Ïƒ</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>r</mi></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd
    columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><mrow><mi
    mathvariant="normal">arg</mi><mo>â¡</mo></mrow><munder><mi mathvariant="normal">min</mi><mi>Î¸</mi></munder><mo>âˆ’</mo><mrow><mi
    mathvariant="normal">log</mi><mo>â¡</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>Ïƒ</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>âˆ£</mo><mi>x</mi><mo stretchy="false"
    form="postfix">)</mo><mo>âˆ’</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><msub><mi>y</mi><mi>r</mi></msub><mo>âˆ£</mo><mi>x</mi><mo stretchy="false"
    form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true"
    form="postfix">)</mo></mrow></mtd></mtr></mtable><mrow><mo stretchy="false" form="prefix">(</mo><mn>14</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\begin{aligned}
    \theta^* = \arg\max_\theta P(y_c > y_r \mid x) &= \arg\max_\theta \frac{\exp\left(r_\theta(y_c
    \mid x)\right)}{\exp\left(r_\theta(y_c \mid x)\right) + \exp\left(r_\theta(y_r
    \mid x)\right)} \\ &= \arg\max_\theta \frac{\exp\left(r_\theta(y_c \mid x)\right)}{\exp\left(r_\theta(y_c
    \mid x)\right)\left(1 + \frac{\exp\left(r_\theta(y_r \mid x)\right)}{\exp\left(r_\theta(y_c
    \mid x)\right)}\right)} \\ &= \arg\max_\theta \frac{1}{1 + \frac{\exp\left(r_\theta(y_r
    \mid x)\right)}{\exp\left(r_\theta(y_c \mid x)\right)}} \\ &= \arg\max_\theta
    \frac{1}{1 + \exp\left(-(r_\theta(y_c \mid x) - r_\theta(y_r \mid x))\right)}
    \\ &= \arg\max_\theta \sigma \left( r_\theta(y_c \mid x) - r_\theta(y_r \mid x)
    \right) \\ &= \arg\min_\theta - \log \left( \sigma \left(r_\theta(y_c \mid x)
    - r_\theta(y_r \mid x)\right) \right) \end{aligned} \qquad{(14)}</annotation></semantics>'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'The first form, as in [[3]](ch021.xhtml#ref-ouyang2022training) and other works:
    <semantics><mrow><mi>â„’</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mi>âˆ’</mi><mrow><mi mathvariant="normal">log</mi><mo>â¡</mo></mrow><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>Ïƒ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>r</mi></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow><mo
    stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="false" form="prefix">(</mo><mn>15</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}(\theta)
    = - \log \left( \sigma \left( r_{\theta}(y_c \mid x) - r_{\theta}(y_r \mid x)
    \right) \right)\qquad{(15)}</annotation></semantics>'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€ç§å½¢å¼ï¼Œæ­£å¦‚[[3]](ch021.xhtml#ref-ouyang2022training)å’Œå…¶ä»–ä½œå“æ‰€è¿°ï¼š<semantics><mrow><mi>â„’</mi><mo
    stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>âˆ’</mi><mrow><mi
    mathvariant="normal">log</mi><mo>â¡</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>Ïƒ</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>âˆ£</mo><mi>x</mi><mo stretchy="false"
    form="postfix">)</mo><mo>âˆ’</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="postfix">(</mo><msub><mi>y</mi><mi>r</mi></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow><mo
    stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="false" form="prefix">(</mo><mn>15</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}(\theta)
    = - \log \left( \sigma \left( r_{\theta}(y_c \mid x) - r_{\theta}(y_r \mid x)
    \right) \right)\qquad{(15)}</annotation></semantics>
- en: 'Second, as in [[18]](ch021.xhtml#ref-askell2021general) and other works: <semantics><mrow><mi>â„’</mi><mo
    stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mrow><mi
    mathvariant="normal">log</mi><mo>â¡</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><msub><mi>r</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>r</mi></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>16</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">\mathcal{L}(\theta) = \log \left( 1 + e^{r_{\theta}(y_r
    \mid x) - r_{\theta}(y_c \mid x)} \right)\qquad{(16)}</annotation></semantics>'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶æ¬¡ï¼Œæ­£å¦‚[[18]](ch021.xhtml#ref-askell2021general)å’Œå…¶ä»–ä½œå“æ‰€è¿°ï¼š<semantics><mrow><mi>â„’</mi><mo
    stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mrow><mi
    mathvariant="normal">log</mi><mo>â¡</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><msub><mi>r</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>r</mi></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>âˆ£</mi><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>16</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">\mathcal{L}(\theta) = \log \left( 1 + e^{r_{\theta}(y_r
    \mid x) - r_{\theta}(y_c \mid x)} \right)\qquad{(16)}</annotation></semantics>
- en: These are equivalent by letting <semantics><mrow><mi mathvariant="normal">Î”</mi><mo>=</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>r</mi></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\Delta
    = r_{\theta}(y_c \mid x) - r_{\theta}(y_r \mid x)</annotation></semantics> and
    using <semantics><mrow><mi>Ïƒ</mi><mo stretchy="false" form="prefix">(</mo><mi
    mathvariant="normal">Î”</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mi>âˆ’</mi><mi
    mathvariant="normal">Î”</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">\sigma(\Delta)
    = \frac{1}{1 + e^{-\Delta}}</annotation></semantics>, which implies <semantics><mrow><mi>âˆ’</mi><mrow><mi
    mathvariant="normal">log</mi><mo>â¡</mo></mrow><mi>Ïƒ</mi><mo stretchy="false" form="prefix">(</mo><mi
    mathvariant="normal">Î”</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mrow><mi
    mathvariant="normal">log</mi><mo>â¡</mo></mrow><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mi>âˆ’</mi><mi
    mathvariant="normal">Î”</mi></mrow></msup><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mrow><mi
    mathvariant="normal">log</mi><mo>â¡</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><msub><mi>r</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>r</mi></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">-\log\sigma(\Delta) = \log(1 + e^{-\Delta}) = \log\left(1
    + e^{r_{\theta}(y_r \mid x) - r_{\theta}(y_c \mid x)}\right)</annotation></semantics>.
    They both appear in the RLHF literature.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›é€šè¿‡è®© <semantics><mrow><mi mathvariant="normal">Î”</mi><mo>=</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>r</mi></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\Delta
    = r_{\theta}(y_c \mid x) - r_{\theta}(y_r \mid x)</annotation></semantics> å’Œä½¿ç”¨
    <semantics><mrow><mi>Ïƒ</mi><mo stretchy="false" form="prefix">(</mo><mi mathvariant="normal">Î”</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mi>âˆ’</mi><mi
    mathvariant="normal">Î”</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">\sigma(\Delta)
    = \frac{1}{1 + e^{-\Delta}}</annotation></semantics> æ¥å®ç°ï¼Œè¿™æš—ç¤ºäº† <semantics><mrow><mi>âˆ’</mi><mrow><mi
    mathvariant="normal">log</mi><mo>â¡</mo></mrow><mi>Ïƒ</mi><mo stretchy="false" form="prefix">(</mo><mi
    mathvariant="normal">Î”</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mrow><mi
    mathvariant="normal">log</mi><mo>â¡</mo></mrow><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mi>âˆ’</mi><mi
    mathvariant="normal">Î”</mi></mrow></msup><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mrow><mi
    mathvariant="normal">log</mi><mo>â¡</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><msub><mi>r</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>r</mi></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="postfix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">-\log\sigma(\Delta) = \log(1 + e^{-\Delta}) = \log\left(1
    + e^{r_{\theta}(y_r \mid x) - r_{\theta}(y_c \mid x)}\right)</annotation></semantics>ã€‚å®ƒä»¬éƒ½å‡ºç°åœ¨å¼ºåŒ–å­¦ä¹ ä¸äººç±»åé¦ˆï¼ˆRLHFï¼‰æ–‡çŒ®ä¸­ã€‚
- en: Architecture
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¶æ„
- en: The most common way reward models are implemented is through an abstraction
    similar to Transformerâ€™s `AutoModelForSequenceClassification`, which appends a
    small linear head to the language model that performs classification between two
    outcomes â€“ chosen and rejected. At inference time, the model outputs the *probability
    that the piece of text is chosen* as a single logit from the model.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€å¸¸è§çš„å¥–åŠ±æ¨¡å‹å®ç°æ–¹å¼æ˜¯é€šè¿‡ç±»ä¼¼äº Transformer çš„ `AutoModelForSequenceClassification` çš„æŠ½è±¡ï¼Œå®ƒå°†ä¸€ä¸ªå°å‹çº¿æ€§å¤´éƒ¨é™„åŠ åˆ°æ‰§è¡Œä¸¤ä¸ªç»“æœä¹‹é—´åˆ†ç±»çš„è¯­è¨€æ¨¡å‹ä¸Šâ€”â€”é€‰æ‹©å’Œæ‹’ç»ã€‚åœ¨æ¨ç†æ—¶é—´ï¼Œæ¨¡å‹ä»æ¨¡å‹ä¸­è¾“å‡ºä½œä¸ºå•ä¸ª
    logit çš„ *æ–‡æœ¬ç‰‡æ®µè¢«é€‰æ‹©çš„æ¦‚ç‡*ã€‚
- en: Other implementation options exist, such as just taking a linear layer directly
    from the final embeddings, but they are less common in open tooling.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä»–å®ç°é€‰é¡¹ä¹Ÿå­˜åœ¨ï¼Œä¾‹å¦‚ç›´æ¥ä»æœ€ç»ˆåµŒå…¥ä¸­å–ä¸€ä¸ªçº¿æ€§å±‚ï¼Œä½†åœ¨å¼€æºå·¥å…·ä¸­å®ƒä»¬è¾ƒå°‘è§ã€‚
- en: Implementation Example
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®ç°ç¤ºä¾‹
- en: 'Implementing the reward modeling loss is quite simple. More of the implementation
    challenge is on setting up a separate data loader and inference pipeline. Given
    the correct dataloader with tokenized, chosen and rejected prompts with completions,
    the loss is implemented as:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: å®ç°å¥–åŠ±å»ºæ¨¡æŸå¤±ç›¸å½“ç®€å•ã€‚æ›´å¤šçš„å®ç°æŒ‘æˆ˜åœ¨äºè®¾ç½®å•ç‹¬çš„æ•°æ®åŠ è½½å™¨å’Œæ¨ç†ç®¡é“ã€‚ç»™å®šæ­£ç¡®çš„æ•°æ®åŠ è½½å™¨ï¼Œå…¶ä¸­åŒ…å«æ ‡è®°åŒ–ã€é€‰æ‹©å’Œæ‹’ç»çš„æç¤ºä»¥åŠå®Œæˆé¡¹ï¼ŒæŸå¤±å®ç°å¦‚ä¸‹ï¼š
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As for the bigger picture, this is often within a causal language model that
    has an additional head added (and learned with the above loss) that transitions
    from the final hidden state to the score of the inputs. This model will have a
    structure as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ›´å¤§çš„å›¾æ™¯ï¼Œè¿™é€šå¸¸æ˜¯åœ¨ä¸€ä¸ªå› æœè¯­è¨€æ¨¡å‹ä¸­ï¼Œé¢å¤–æ·»åŠ äº†ä¸€ä¸ªå¤´éƒ¨ï¼ˆå¹¶ä¸”ä¸ä¸Šè¿°æŸå¤±ä¸€èµ·å­¦ä¹ ï¼‰ï¼Œå®ƒä»æœ€ç»ˆçš„éšè—çŠ¶æ€è½¬æ¢åˆ°è¾“å…¥çš„åˆ†æ•°ã€‚æ­¤æ¨¡å‹çš„ç»“æ„å¦‚ä¸‹ï¼š
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In this section and what follows, most of the implementation complexity for
    reward models (and much of post-training) is around constructing the data-loaders
    correctly and distributed learning systems. Note, when training reward models,
    the most common practice is to train for only 1 epoch to avoid overfitting.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚åŠä»¥ä¸‹å†…å®¹ä¸­ï¼Œå¥–åŠ±æ¨¡å‹çš„å¤§éƒ¨åˆ†å®ç°å¤æ‚æ€§ï¼ˆä»¥åŠå¤§é‡è®­ç»ƒåï¼‰éƒ½å›´ç»•ç€æ­£ç¡®æ„å»ºæ•°æ®åŠ è½½å™¨å’Œåˆ†å¸ƒå¼å­¦ä¹ ç³»ç»Ÿã€‚è¯·æ³¨æ„ï¼Œåœ¨è®­ç»ƒå¥–åŠ±æ¨¡å‹æ—¶ï¼Œæœ€å¸¸è§çš„æ–¹æ³•æ˜¯ä»…è®­ç»ƒ
    1 ä¸ª epoch ä»¥é¿å…è¿‡æ‹Ÿåˆã€‚
- en: Variants
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å˜ä½“
- en: Reward modeling is a relatively under-explored area of RLHF. The traditional
    reward modeling loss has been modified in many popular works, but the modifications
    have not solidified into a single best practice.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å¥–åŠ±å»ºæ¨¡æ˜¯ RLHF ä¸­ç›¸å¯¹æœªè¢«å……åˆ†æ¢ç´¢çš„é¢†åŸŸã€‚ä¼ ç»Ÿçš„å¥–åŠ±å»ºæ¨¡æŸå¤±åœ¨è®¸å¤šæµè¡Œçš„å·¥ä½œä¸­å·²ç»è¢«ä¿®æ”¹ï¼Œä½†è¿™äº›ä¿®æ”¹å°šæœªå½¢æˆå•ä¸€çš„æœ€ä½³å®è·µã€‚
- en: Preference Margin Loss
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åå¥½é—´éš”æŸå¤±
- en: 'In the case where annotators are providing either scores or rankings on a Likert
    Scale, the magnitude of the relational quantities can be used in training. The
    most common practice is to binarize the data along the preference direction, reducing
    the mixed information of relative ratings or the strength of the ranking to just
    chosen and rejected completions. The additional information, such as the magnitude
    of the preference, has been used to improve model training, but it has not converged
    as a standard practice. Llama 2 proposes using the margin between two datapoints,
    <semantics><mrow><mi>m</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>,</mo><msub><mi>y</mi><mi>r</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">m(y_c,
    y_r)</annotation></semantics>, to distinguish the magnitude of preference:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ³¨é‡Šè€…æä¾›æå…‹ç‰¹é‡è¡¨ä¸Šçš„åˆ†æ•°æˆ–æ’åçš„æƒ…å†µä¸‹ï¼Œå¯ä»¥ä½¿ç”¨å…³ç³»é‡çš„å¹…åº¦è¿›è¡Œè®­ç»ƒã€‚æœ€å¸¸è§çš„æ–¹æ³•æ˜¯å°†æ•°æ®æ²¿åå¥½æ–¹å‘äºŒå€¼åŒ–ï¼Œå°†ç›¸å¯¹è¯„åˆ†æˆ–æ’åçš„æ··åˆä¿¡æ¯æˆ–å¼ºåº¦ç®€åŒ–ä¸ºä»…é€‰æ‹©å’Œæ‹’ç»çš„å®Œæˆé¡¹ã€‚åå¥½å¹…åº¦çš„é™„åŠ ä¿¡æ¯å·²è¢«ç”¨äºæ”¹è¿›æ¨¡å‹è®­ç»ƒï¼Œä½†å°šæœªä½œä¸ºæ ‡å‡†å®è·µæ”¶æ•›ã€‚Llama
    2 æå‡ºä½¿ç”¨ä¸¤ä¸ªæ•°æ®ç‚¹ä¹‹é—´çš„é—´éš”ï¼Œ<semantics><mrow><mi>m</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>,</mo><msub><mi>y</mi><mi>r</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">m(y_c,
    y_r)</annotation></semantics>ï¼Œæ¥åŒºåˆ†åå¥½çš„å¹…åº¦ï¼š
- en: <semantics><mrow><mi>â„’</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mi>âˆ’</mi><mrow><mi mathvariant="normal">log</mi><mo>â¡</mo></mrow><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>Ïƒ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>r</mi></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><mi>m</mi><mo stretchy="false"
    form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>,</mo><msub><mi>y</mi><mi>r</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow><mo
    stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="false" form="prefix">(</mo><mn>17</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}(\theta)
    = - \log \left( \sigma \left( r_{\theta}(y_c \mid x) - r_{\theta}(y_r \mid x)
    - m(y_c, y_r) \right) \right)\qquad{(17)}</annotation></semantics>
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><mi>â„’</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mi>âˆ’</mi><mrow><mi mathvariant="normal">log</mi><mo>â¡</mo></mrow><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>Ïƒ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>r</mi></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><mi>m</mi><mo stretchy="false"
    form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>,</mo><msub><mi>y</mi><mi>r</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><mi>m</mi><mo stretchy="true"
    form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>17</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">\mathcal{L}(\theta) = - \log \left( \sigma \left(
    r_{\theta}(y_c \mid x) - r_{\theta}(y_r \mid x) - m(y_c, y_r) \right) \right)\qquad{(17)}</annotation></semantics>
- en: For example, each completion is often given a ranking from 1 to 5 in terms of
    quality. In the case where the chosen sample was assigned a score of 5 and rejected
    a score of 2, the margin <semantics><mrow><mi>m</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>,</mo><msub><mi>y</mi><mi>r</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mn>5</mn><mo>âˆ’</mo><mn>2</mn><mo>=</mo><mn>3</mn></mrow><annotation
    encoding="application/x-tex">m(y_c, y_r)= 5 - 2 = 3</annotation></semantics>.
    Other functions for computing margins can be explored.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œæ¯ä¸ªå®Œæˆé¡¹é€šå¸¸æ ¹æ®è´¨é‡ä»1åˆ°5è¿›è¡Œæ’åã€‚åœ¨æ‰€é€‰æ ·æœ¬è¢«åˆ†é…5åˆ†è€Œæ‹’ç»2åˆ†çš„æƒ…å†µä¸‹ï¼Œè¾¹ç¼˜ <semantics><mrow><mi>m</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>,</mo><msub><mi>y</mi><mi>r</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mn>5</mn><mo>âˆ’</mo><mn>2</mn><mo>=</mo><mn>3</mn></mrow><annotation
    encoding="application/x-tex">m(y_c, y_r)= 5 - 2 = 3</annotation></semantics>ã€‚å¯ä»¥æ¢ç´¢å…¶ä»–è®¡ç®—è¾¹ç¼˜çš„å‡½æ•°ã€‚
- en: Note that in Llama 3 the margin term was removed as the team observed diminishing
    improvements after scaling.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œåœ¨Llama 3ä¸­ï¼Œç”±äºå›¢é˜Ÿè§‚å¯Ÿåˆ°åœ¨æ‰©å±•åæ”¹è¿›æ•ˆæœé€æ¸å‡å°‘ï¼Œå› æ­¤ç§»é™¤äº†è¾¹ç¼˜é¡¹ã€‚
- en: Balancing Multiple Comparisons Per Prompt
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ¯ä¸ªæç¤ºä¸‹çš„å¤šé‡æ¯”è¾ƒå¹³è¡¡
- en: 'InstructGPT studies the impact of using a variable number of completions per
    prompt, yet balancing them in the reward model training [[3]](ch021.xhtml#ref-ouyang2022training).
    To do this, they weight the loss updates per comparison per prompt. At an implementation
    level, this can be done automatically by including all examples with the same
    prompt in the same training batch, naturally weighing the different pairs â€“ not
    doing this caused overfitting to the prompts. The loss function becomes:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: InstructGPTç ”ç©¶äº†åœ¨æ¯ä¸ªæç¤ºä¸­ä½¿ç”¨ä¸åŒæ•°é‡çš„å®Œæˆé¡¹å¯¹çš„å½±å“ï¼Œä½†åœ¨å¥–åŠ±æ¨¡å‹è®­ç»ƒä¸­ä¿æŒå®ƒä»¬çš„å¹³è¡¡ [[3]](ch021.xhtml#ref-ouyang2022training)ã€‚ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œä»–ä»¬ä¸ºæ¯ä¸ªæç¤ºçš„æ¯ä¸ªæ¯”è¾ƒåŠ æƒæ›´æ–°æŸå¤±ã€‚åœ¨å®ç°å±‚é¢ï¼Œè¿™å¯ä»¥é€šè¿‡å°†å…·æœ‰ç›¸åŒæç¤ºçš„æ‰€æœ‰ç¤ºä¾‹åŒ…å«åœ¨åŒä¸€ä¸ªè®­ç»ƒæ‰¹æ¬¡ä¸­è‡ªåŠ¨å®Œæˆï¼Œä»è€Œè‡ªç„¶åœ°åŠ æƒä¸åŒçš„å¯¹â€”â€”ä¸è¿™æ ·åšä¼šå¯¼è‡´å¯¹æç¤ºçš„è¿‡åº¦æ‹Ÿåˆã€‚æŸå¤±å‡½æ•°å˜ä¸ºï¼š
- en: <semantics><mrow><mi>â„’</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mi>âˆ’</mi><mfrac><mn>1</mn><mrow><mo
    stretchy="true" form="prefix">(</mo><mfrac linethickness="0"><mi>K</mi><mn>2</mn></mfrac><mo
    stretchy="true" form="postfix">)</mo></mrow></mfrac><msub><mi>ğ”¼</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><msub><mi>y</mi><mi>c</mi></msub><mo>,</mo><msub><mi>y</mi><mi>r</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo>âˆ¼</mo><mi>D</mi></mrow></msub><mrow><mi
    mathvariant="normal">log</mi><mo>â¡</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>Ïƒ</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>âˆ£</mo><mi>x</mi><mo stretchy="false"
    form="postfix">)</mo><mo>âˆ’</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><msub><mi>y</mi><mi>r</mi></msub><mo>âˆ£</mo><mi>x</mi><mo stretchy="false"
    form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true"
    form="postfix">)</mo></mrow><mrow><mo stretchy="false" form="prefix">(</mo><mn>18</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}(\theta)
    = - \frac{1}{\binom{K}{2}} \mathbb{E}_{(x, y_c, y_r)\sim D} \log \left( \sigma
    \left( r_{\theta}(y_c \mid x) - r_{\theta}(y_r \mid x) \right) \right)\qquad{(18)}</annotation></semantics>
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><mi>â„’</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mi>âˆ’</mi><mfrac><mn>1</mn><mrow><mo
    stretchy="true" form="prefix">(</mo><mfrac linethickness="0"><mi>K</mi><mn>2</mn></mfrac><mo
    stretchy="true" form="postfix">)</mo></mrow></mfrac><msub><mi>ğ”¼</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><msub><mi>y</mi><mi>c</mi></msub><mo>,</mo><msub><mi>y</mi><mi>r</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo>âˆ¼</mo><mi>D</mi></mrow></msub><mrow><mi
    mathvariant="normal">log</mi><mo>â¡</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>Ïƒ</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>âˆ£</mo><mi>x</mi><mo stretchy="false"
    form="postfix">)</mo><mo>âˆ’</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><msub><mi>y</mi><mi>r</mi></msub><mo>âˆ£</mo><mi>x</mi><mo stretchy="false"
    form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true"
    form="postfix">)</mo></mrow><mrow><mo stretchy="false" form="prefix">(</mo><mn>18</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}(\theta)
    = - \frac{1}{\binom{K}{2}} \mathbb{E}_{(x, y_c, y_r)\sim D} \log \left( \sigma
    \left( r_{\theta}(y_c \mid x) - r_{\theta}(y_r \mid x) \right) \right)\qquad{(18)}</annotation></semantics>
- en: K-wise Loss Function
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: K-wise æŸå¤±å‡½æ•°
- en: There are many other formulations that can create suitable models of human preferences
    for RLHF. One such example, used in the popular, early RLHFâ€™d models Starling
    7B and 34B [[126]](ch021.xhtml#ref-zhu2024starling), is a K-wise loss function
    based on the Plackett-Luce model [[127]](ch021.xhtml#ref-liu2019learning).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºRLHFï¼Œæœ‰è®¸å¤šå…¶ä»–å…¬å¼å¯ä»¥åˆ›å»ºé€‚åˆäººç±»åå¥½çš„æ¨¡å‹ã€‚å…¶ä¸­ä¸€ä¸ªä¾‹å­ï¼Œåœ¨æµè¡Œçš„æ—©æœŸRLHFæ¨¡å‹Starling 7Bå’Œ34Bä¸­ä½¿ç”¨ï¼Œæ˜¯åŸºäºPlackett-Luceæ¨¡å‹çš„K-wiseæŸå¤±å‡½æ•°
    [[126]](ch021.xhtml#ref-zhu2024starling)ï¼Œ[[127]](ch021.xhtml#ref-liu2019learning)ã€‚
- en: 'Zhu et al.Â 2023 [[128]](ch021.xhtml#ref-zhu2023principled) formalizes the setup
    as follows. With a prompt, or state, <semantics><msup><mi>s</mi><mi>i</mi></msup><annotation
    encoding="application/x-tex">s^i</annotation></semantics>, <semantics><mi>K</mi><annotation
    encoding="application/x-tex">K</annotation></semantics> actions <semantics><mrow><mo
    stretchy="false" form="prefix">(</mo><msubsup><mi>a</mi><mn>0</mn><mi>i</mi></msubsup><mo>,</mo><msubsup><mi>a</mi><mn>1</mn><mi>i</mi></msubsup><mo>,</mo><mi>â‹¯</mi><mo>,</mo><msubsup><mi>a</mi><mrow><mi>K</mi><mo>âˆ’</mo><mn>1</mn></mrow><mi>i</mi></msubsup><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(a_0^i,
    a_1^i, \cdots, a_{K-1}^i)</annotation></semantics> are sampled from <semantics><mrow><mi>P</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mn>0</mn></msub><mo>,</mo><mi>â‹¯</mi><mo>,</mo><msub><mi>a</mi><mrow><mi>K</mi><mo>âˆ’</mo><mn>1</mn></mrow></msub><mo
    stretchy="false" form="prefix">|</mo><msup><mi>s</mi><mi>i</mi></msup><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">P(a_0,\cdots,a_{K-1}|s^i)</annotation></semantics>.
    Then, labelers are used to rank preferences with <semantics><mrow><msup><mi>Ïƒ</mi><mi>i</mi></msup><mo>:</mo><mo
    stretchy="false" form="prefix">[</mo><mi>K</mi><mo stretchy="false" form="postfix">]</mo><mo>â†¦</mo><mo
    stretchy="false" form="prefix">[</mo><mi>K</mi><mo stretchy="false" form="postfix">]</mo></mrow><annotation
    encoding="application/x-tex">\sigma^i: [K] \mapsto [K]</annotation></semantics>
    is a function representing action rankings, where <semantics><mrow><msup><mi>Ïƒ</mi><mi>i</mi></msup><mo
    stretchy="false" form="prefix">(</mo><mn>0</mn><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">\sigma^i(0)</annotation></semantics> is the most
    preferred action. This yields a preference model capturing the following:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 'Zhuç­‰äººäº2023å¹´[[128]](ch021.xhtml#ref-zhu2023principled)å°†è®¾ç½®å½¢å¼åŒ–ä¸ºå¦‚ä¸‹ã€‚åœ¨æç¤ºæˆ–çŠ¶æ€<semantics><msup><mi>s</mi><mi>i</mi></msup><annotation
    encoding="application/x-tex">s^i</annotation></semantics>ä¸‹ï¼Œ<semantics><mi>K</mi><annotation
    encoding="application/x-tex">K</annotation></semantics>åŠ¨ä½œ<semantics><mrow><mo
    stretchy="false" form="prefix">(</mo><msubsup><mi>a</mi><mn>0</mn><mi>i</mi></msubsup><mo>,</mo><msubsup><mi>a</mi><mn>1</mn><mi>i</mi></msubsup><mo>,</mo><mi>â‹¯</mi><mo>,</mo><msubsup><mi>a</mi><mrow><mi>K</mi><mo>âˆ’</mo><mn>1</mn></mrow><mi>i</mi></msubsup><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(a_0^i,
    a_1^i, \cdots, a_{K-1}^i)</annotation></semantics>æ˜¯ä»<semantics><mrow><mi>P</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mn>0</mn></msub><mo>,</mo><mi>â‹¯</mi><mo>,</mo><msub><mi>a</mi><mrow><mi>K</mi><mo>âˆ’</mo><mn>1</mn></mrow></msub><mo
    stretchy="false" form="prefix">|</mo><msup><mi>s</mi><mi>i</mi></msup><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">P(a_0,\cdots,a_{K-1}|s^i)</annotation></semantics>ä¸­é‡‡æ ·çš„ã€‚ç„¶åï¼Œä½¿ç”¨æ ‡ç­¾å™¨é€šè¿‡<semantics><mrow><msup><mi>Ïƒ</mi><mi>i</mi></msup><mo>:</mo><mo
    stretchy="false" form="prefix">[</mo><mi>K</mi><mo stretchy="false" form="postfix">]</mo><mo>â†¦</mo><mo
    stretchy="false" form="prefix">[</mo><mi>K</mi><mo stretchy="false" form="postfix">]</mo></mrow><annotation
    encoding="application/x-tex">\sigma^i: [K] \mapsto [K]</annotation></semantics>å¯¹åå¥½è¿›è¡Œæ’åºï¼Œå…¶ä¸­<semantics><mrow><msup><mi>Ïƒ</mi><mi>i</mi></msup><mo
    stretchy="false" form="prefix">(</mo><mn>0</mn><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">\sigma^i(0)</annotation></semantics>æ˜¯æœ€å—åå¥½çš„åŠ¨ä½œã€‚è¿™äº§ç”Ÿäº†ä¸€ä¸ªåå¥½æ¨¡å‹ï¼Œå®ƒæ•æ‰ä»¥ä¸‹å†…å®¹ï¼š'
- en: <semantics><mrow><mi>P</mi><mo stretchy="false" form="prefix">(</mo><msup><mi>Ïƒ</mi><mi>i</mi></msup><mo
    stretchy="false" form="prefix">|</mo><msup><mi>s</mi><mi>i</mi></msup><mo>,</mo><msubsup><mi>a</mi><mn>0</mn><mi>i</mi></msubsup><mo>,</mo><msubsup><mi>a</mi><mn>1</mn><mi>i</mi></msubsup><mo>,</mo><mi>â€¦</mi><mo>,</mo><msubsup><mi>a</mi><mrow><mi>K</mi><mo>âˆ’</mo><mn>1</mn></mrow><mi>i</mi></msubsup><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><munderover><mo>âˆ</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>K</mi><mo>âˆ’</mo><mn>1</mn></mrow></munderover><mfrac><mrow><mrow><mi
    mathvariant="normal">exp</mi><mo>â¡</mo></mrow><mo stretchy="false" form="prefix">(</mo><msub><mi>r</mi><mrow><mi>Î¸</mi><mo>â‹†</mo></mrow></msub><mo
    stretchy="false" form="prefix">(</mo><msup><mi>s</mi><mi>i</mi></msup><mo>,</mo><msubsup><mi>a</mi><mrow><msup><mi>Ïƒ</mi><mi>i</mi></msup><mo
    stretchy="false" form="prefix">(</mo><mi>k</mi><mo stretchy="false" form="postfix">)</mo></mrow><mi>i</mi></msubsup><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">)</mo></mrow><mrow><munderover><mo>âˆ‘</mo><mrow><mi>j</mi><mo>=</mo><mi>k</mi></mrow><mrow><mi>K</mi><mo>âˆ’</mo><mn>1</mn></mrow></munderover><mrow><mi
    mathvariant="normal">exp</mi><mo>â¡</mo></mrow><mo stretchy="false" form="prefix">(</mo><msub><mi>r</mi><mrow><mi>Î¸</mi><mo>â‹†</mo></mrow></msub><mo
    stretchy="false" form="prefix">(</mo><msup><mi>s</mi><mi>i</mi></msup><mo>,</mo><msubsup><mi>a</mi><mrow><msup><mi>Ïƒ</mi><mi>i</mi></msup><mo
    stretchy="false" form="prefix">(</mo><mi>j</mi><mo stretchy="false" form="postfix">)</mo></mrow><mi>i</mi></msubsup><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>19</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">P(\sigma^i|s^i,a_0^i,a_1^i,\ldots,a_{K-1}^i) = \prod_{k=0}^{K-1}
    \frac{\exp(r_{\theta\star}(s^i,a_{\sigma^i(k)}^i))}{\sum_{j=k}^{K-1}\exp(r_{\theta\star}(s^i,a_{\sigma^i(j)}^i))}\qquad{(19)}</annotation></semantics>
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><mi>P</mi><mo stretchy="false" form="prefix">(</mo><msup><mi>Ïƒ</mi><mi>i</mi></msup><mo
    stretchy="false" form="prefix">|</mo><msup><mi>s</mi><mi>i</mi></msup><mo>,</mo><msubsup><mi>a</mi><mn>0</mn><mi>i</mi></msubsup><mo>,</mo><msubsup><mi>a</mi><mn>1</mn><mi>i</mi></msubsup><mo>,</mo><mi>â€¦</mi><mo>,</mo><msubsup><mi>a</mi><mrow><mi>K</mi><mo>âˆ’</mo><mn>1</mn></mrow><mi>i</mi></msubsup><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><munderover><mo>âˆ</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>K</mi><mo>âˆ’</mo><mn>1</mn></mrow></munderover><mfrac><mrow><mrow><mi
    mathvariant="normal">exp</mi><mo>â¡</mo></mrow><mo stretchy="false" form="prefix">(</mo><msub><mi>r</mi><mrow><mi>Î¸</mi><mo>â‹†</mo></mrow></msub><mo
    stretchy="false" form="prefix">(</mo><msup><mi>s</mi><mi>i</mi></msup><mo>,</mo><msubsup><mi>a</mi><mrow><msup><mi>Ïƒ</mi><mi>i</mi></msup><mo
    stretchy="false" form="prefix">(</mo><mi>k</mi><mo stretchy="false" form="postfix">)</mo></mrow><mi>i</mi></msubsup><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">)</mo></mrow><mrow><munderover><mo>âˆ‘</mo><mrow><mi>j</mi><mo>=</mo><mi>k</mi></mrow><mrow><mi>K</mi><mo>âˆ’</mo><mn>1</mn></mrow></munderover><mrow><mi
    mathvariant="normal">exp</mi><mo>â¡</mo></mrow><mo stretchy="false" form="prefix">(</mo><msub><mi>r</mi><mrow><mi>Î¸</mi><mo>â‹†</mo></mrow></msub><mo
    stretchy="false" form="prefix">(</mo><msup><mi>s</mi><mi>i</mi></msup><mo>,</mo><msubsup><mi>a</mi><mrow><msup><mi>Ïƒ</mi><mi>i</mi></msup><mo
    stretchy="false" form="prefix">(</mo><mi>j</mi><mo stretchy="false" form="postfix">)</mo></mrow><mi>i</mi></msubsup><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>19</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">P(\sigma^i|s^i,a_0^i,a_1^i,\ldots,a_{K-1}^i) = \prod_{k=0}^{K-1}
    \frac{\exp(r_{\theta\star}(s^i,a_{\sigma^i(k)}^i))}{\sum_{j=k}^{K-1}\exp(r_{\theta\star}(s^i,a_{\sigma^i(j)}^i))}\qquad{(19)}</annotation></semantics>
- en: When <semantics><mrow><mi>K</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">K
    = 2</annotation></semantics>, this reduces to the Bradley-Terry (BT) model for
    pairwise comparisons. Regardless, once trained, these models are used similarly
    to other reward models during RLHF training.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ <semantics><mrow><mi>K</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">K
    = 2</annotation></semantics> æ—¶ï¼Œè¿™ç®€åŒ–ä¸ºå¸ƒæ‹‰å¾·åˆ©-ç‰¹é‡Œï¼ˆBTï¼‰æ¨¡å‹ç”¨äºæˆå¯¹æ¯”è¾ƒã€‚æ— è®ºå¦‚ä½•ï¼Œä¸€æ—¦è®­ç»ƒå®Œæˆï¼Œè¿™äº›æ¨¡å‹åœ¨RLHFè®­ç»ƒæœŸé—´ä¸å…¶ä»–å¥–åŠ±æ¨¡å‹çš„ä½¿ç”¨æ–¹å¼ç›¸ä¼¼ã€‚
- en: Outcome Reward Models
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»“æœå¥–åŠ±æ¨¡å‹
- en: The majority of *preference tuning* for language models and other AI systems
    is done with the Bradley Terry models discussed above. For reasoning heavy tasks,
    one can use an Outcome Reward Model (ORM). The training data for an ORM is constructed
    in a similar manner to standard preference tuning. Here, we have a problem statement
    or prompt, <semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics>
    and two completions <semantics><msub><mi>y</mi><mn>1</mn></msub><annotation encoding="application/x-tex">y_1</annotation></semantics>
    and <semantics><msub><mi>y</mi><mn>2</mn></msub><annotation encoding="application/x-tex">y_2</annotation></semantics>.
    The inductive bias used here is that one completion should be a correct solution
    to the problem and one incorrect, resulting in <semantics><mrow><mo stretchy="false"
    form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>,</mo><msub><mi>y</mi><mrow><mi>i</mi><mi>c</mi></mrow></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(y_c,y_{ic})</annotation></semantics>.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¯­è¨€æ¨¡å‹å’Œå…¶ä»–AIç³»ç»Ÿçš„*åå¥½è°ƒæ•´*ï¼Œå¤§å¤šæ•°éƒ½æ˜¯ä½¿ç”¨ä¸Šé¢è®¨è®ºçš„Bradley Terryæ¨¡å‹è¿›è¡Œçš„ã€‚å¯¹äºæ¨ç†å¯†é›†å‹ä»»åŠ¡ï¼Œå¯ä»¥ä½¿ç”¨ç»“æœå¥–åŠ±æ¨¡å‹ï¼ˆORMï¼‰ã€‚ORMçš„è®­ç»ƒæ•°æ®æ„å»ºæ–¹å¼ä¸æ ‡å‡†åå¥½è°ƒæ•´ç±»ä¼¼ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªé—®é¢˜é™ˆè¿°æˆ–æç¤ºï¼Œ<semantics><mi>x</mi><annotation
    encoding="application/x-tex">x</annotation></semantics>å’Œä¸¤ä¸ªè¡¥å…¨<semantics><msub><mi>y</mi><mn>1</mn></msub><annotation
    encoding="application/x-tex">y_1</annotation></semantics>å’Œ<semantics><msub><mi>y</mi><mn>2</mn></msub><annotation
    encoding="application/x-tex">y_2</annotation></semantics>ã€‚è¿™é‡Œä½¿ç”¨çš„å½’çº³åå·®æ˜¯ï¼Œä¸€ä¸ªè¡¥å…¨åº”è¯¥æ˜¯é—®é¢˜çš„æ­£ç¡®è§£å†³æ–¹æ¡ˆï¼Œå¦ä¸€ä¸ªæ˜¯é”™è¯¯çš„ï¼Œä»è€Œå¾—åˆ°<semantics><mrow><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>,</mo><msub><mi>y</mi><mrow><mi>i</mi><mi>c</mi></mrow></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(y_c,y_{ic})</annotation></semantics>ã€‚
- en: 'The shape of the models used is very similar to a standard reward model, with
    a linear layer appended to a model that can output a single logit (in the case
    of an RM) â€“ with an ORM, the training objective that follows is slightly different
    [[129]](ch021.xhtml#ref-cobbe2021gsm8k):'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨çš„æ¨¡å‹å½¢çŠ¶ä¸æ ‡å‡†å¥–åŠ±æ¨¡å‹éå¸¸ç›¸ä¼¼ï¼Œéƒ½æ˜¯é™„åŠ äº†ä¸€ä¸ªçº¿æ€§å±‚åˆ°å¯ä»¥è¾“å‡ºå•ä¸ªlogitçš„æ¨¡å‹ï¼ˆåœ¨RMçš„æƒ…å†µä¸‹ï¼‰â€”â€”å¯¹äºORMï¼Œæ¥ä¸‹æ¥çš„è®­ç»ƒç›®æ ‡ç•¥æœ‰ä¸åŒ[[129]](ch021.xhtml#ref-cobbe2021gsm8k)ï¼š
- en: '[We] train verifiers with a joint objective where the model learns to label
    a model completion as correct or incorrect, in addition to the original language
    modeling objective. Architecturally, this means our verifiers are language models,
    with a small scalar head that outputs predictions on a per-token basis. We implement
    this scalar head as a single bias parameter and single gain parameter that operate
    on the logits outputted by the language modelâ€™s final unembedding layer.'
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[æˆ‘ä»¬]ä½¿ç”¨è”åˆç›®æ ‡æ¥è®­ç»ƒéªŒè¯å™¨ï¼Œæ¨¡å‹é™¤äº†å­¦ä¹ å¯¹æ¨¡å‹è¡¥å…¨è¿›è¡Œæ­£ç¡®æˆ–é”™è¯¯æ ‡è®°ä¹‹å¤–ï¼Œè¿˜è¦å­¦ä¹ åŸå§‹çš„è¯­è¨€æ¨¡å‹ç›®æ ‡ã€‚åœ¨æ¶æ„ä¸Šï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬çš„éªŒè¯å™¨æ˜¯è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰ä¸€ä¸ªå°çš„æ ‡é‡å¤´ï¼Œè¯¥å¤´åœ¨æ¯ä¸ªæ ‡è®°çš„åŸºç¡€ä¸Šè¾“å‡ºé¢„æµ‹ã€‚æˆ‘ä»¬å°†è¿™ä¸ªæ ‡é‡å¤´å®ç°ä¸ºä¸€ä¸ªå•åç½®å‚æ•°å’Œä¸€ä¸ªå•å¢ç›Šå‚æ•°ï¼Œå®ƒä»¬ä½œç”¨äºè¯­è¨€æ¨¡å‹æœ€ç»ˆååµŒå…¥å±‚è¾“å‡ºçš„logitsã€‚'
- en: 'To translate, this is implemented as a language modeling head that can predict
    two classes per token (1 for correct, 0 for incorrect), rather than a classification
    head of a traditional RM that outputs one logit for the entire sequence. Formally,
    following [[130]](ch021.xhtml#ref-lyu2025exploring) this can be shown as:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç¿»è¯‘ï¼Œè¿™è¢«å®ç°ä¸ºä¸€ä¸ªè¯­è¨€æ¨¡å‹å¤´ï¼Œå®ƒå¯ä»¥å¯¹æ¯ä¸ªæ ‡è®°é¢„æµ‹ä¸¤ä¸ªç±»åˆ«ï¼ˆ1è¡¨ç¤ºæ­£ç¡®ï¼Œ0è¡¨ç¤ºé”™è¯¯ï¼‰ï¼Œè€Œä¸æ˜¯ä¼ ç»ŸRMçš„åˆ†ç±»å¤´ï¼Œè¯¥å¤´è¾“å‡ºæ•´ä¸ªåºåˆ—çš„ä¸€ä¸ªlogitã€‚æ­£å¼æ¥è¯´ï¼Œæ ¹æ®[[130]](ch021.xhtml#ref-lyu2025exploring)ï¼Œè¿™å¯ä»¥è¡¨ç¤ºä¸ºï¼š
- en: <semantics><mrow><msub><mi>â„’</mi><mtext mathvariant="normal">CE</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>âˆ’</mi><msub><mi>ğ”¼</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>r</mi><mo stretchy="false"
    form="postfix">)</mo><mo>âˆ¼</mo><mi>ğ’Ÿ</mi></mrow></msub><mo stretchy="false" form="prefix">[</mo><mi>r</mi><mrow><mi
    mathvariant="normal">log</mi><mo>â¡</mo></mrow><msub><mi>p</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo><mo>+</mo><mo
    stretchy="false" form="prefix">(</mo><mn>1</mn><mo>âˆ’</mo><mi>r</mi><mo stretchy="false"
    form="postfix">)</mo><mrow><mi mathvariant="normal">log</mi><mo>â¡</mo></mrow><mo
    stretchy="false" form="prefix">(</mo><mn>1</mn><mo>âˆ’</mo><msub><mi>p</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">]</mo><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>20</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">\mathcal{L}_{\text{CE}}(\theta) = -\mathbb{E}_{(s,r)\sim
    \mathcal{D}}[r\log p_\theta(s) + (1-r)\log(1-p_\theta(s))]\qquad{(20)}</annotation></semantics>
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><msub><mi>â„’</mi><mtext mathvariant="normal">CE</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>âˆ’</mi><msub><mi>ğ”¼</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>r</mi><mo stretchy="false"
    form="postfix">)</mo><mo>âˆ¼</mo><mi>ğ’Ÿ</mi></mrow></msub><mo stretchy="false" form="prefix">[</mo><mi>r</mi><mrow><mi
    mathvariant="normal">log</mi><mo>â¡</mo></mrow><msub><mi>p</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo><mo>+</mo><mo
    stretchy="false" form="prefix">(</mo><mn>1</mn><mo>âˆ’</mo><mi>r</mi><mo stretchy="false"
    form="postfix">)</mo><mrow><mi mathvariant="normal">log</mi><mo>â¡</mo></mrow><mo
    stretchy="false" form="prefix">(</mo><mn>1</mn><mo>âˆ’</mo><msub><mi>p</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">]</mo><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>20</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">\mathcal{L}_{\text{CE}}(\theta) = -\mathbb{E}_{(s,r)\sim
    \mathcal{D}}[r\log p_\theta(s) + (1-r)\log(1-p_\theta(s))]\qquad{(20)}</annotation></semantics>
- en: where <semantics><mrow><mi>r</mi><mo>âˆˆ</mo><mrow><mn>0</mn><mo>,</mo><mn>1</mn></mrow></mrow><annotation
    encoding="application/x-tex">r \in {0,1}</annotation></semantics> is a binary
    label where 1 applies to a correct answer to a given prompt and 0 applies to an
    incorrect, and <semantics><mrow><msub><mi>p</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">p_\theta(s)</annotation></semantics> is the scalar
    proportional to predicted probability of correctness from the model being trained.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼Œ<semantics><mrow><mi>r</mi><mo>âˆˆ</mo><mrow><mn>0</mn><mo>,</mo><mn>1</mn></mrow></mrow><annotation
    encoding="application/x-tex">r \in {0,1}</annotation></semantics>æ˜¯ä¸€ä¸ªäºŒè¿›åˆ¶æ ‡ç­¾ï¼Œå…¶ä¸­1è¡¨ç¤ºå¯¹ç»™å®šæç¤ºçš„æ­£ç¡®ç­”æ¡ˆï¼Œ0è¡¨ç¤ºé”™è¯¯ç­”æ¡ˆï¼Œè€Œ<semantics><mrow><msub><mi>p</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">p_\theta(s)</annotation></semantics>æ˜¯æ¨¡å‹è®­ç»ƒä¸­é¢„æµ‹æ­£ç¡®æ€§æ¦‚ç‡çš„æ ‡é‡ã€‚
- en: Implementing an outcome reward model (and other types, as weâ€™ll see with the
    Process Reward Model) involves applying the cross-entropy loss per-token based
    on if the completion is a correct sample. This is far closer to the language modeling
    loss, where it does not need the structured chosen-rejected nature of standard
    Bradley-Terry reward models.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å®ç°ç»“æœå¥–åŠ±æ¨¡å‹ï¼ˆä»¥åŠå…¶ä»–ç±»å‹ï¼Œå¦‚æˆ‘ä»¬å°†é€šè¿‡è¿‡ç¨‹å¥–åŠ±æ¨¡å‹çœ‹åˆ°çš„ï¼‰æ¶‰åŠæ ¹æ®å®Œæˆæ˜¯å¦ä¸ºæ­£ç¡®æ ·æœ¬åº”ç”¨åŸºäºæ ‡è®°çš„äº¤å‰ç†µæŸå¤±ã€‚è¿™æ¯”æ ‡å‡†å¸ƒæ‹‰å¾·åˆ©-ç‰¹é‡Œå¥–åŠ±æ¨¡å‹çš„æœ‰åºé€‰æ‹©-æ‹’ç»æ€§è´¨æ›´æ¥è¿‘è¯­è¨€æ¨¡å‹æŸå¤±ï¼Œå› ä¸ºå®ƒä¸éœ€è¦è¿™ç§ç»“æ„ã€‚
- en: 'The model structure could follow as:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹ç»“æ„å¯èƒ½å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'A simplified version of the loss follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æŸå¤±å‡½æ•°çš„ç®€åŒ–ç‰ˆæœ¬å¦‚ä¸‹ï¼š
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The important intuition here is that an ORM will output a probability of correctness
    at every token in the sequence. This can be a noisy process, as the updates and
    loss propagates per token depending on outcomes and attention mappings.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œçš„é‡è¦ç›´è§‰æ˜¯ï¼ŒORMä¼šåœ¨åºåˆ—ä¸­çš„æ¯ä¸ªæ ‡è®°å¤„è¾“å‡ºä¸€ä¸ªæ­£ç¡®æ€§çš„æ¦‚ç‡ã€‚è¿™ä¸ªè¿‡ç¨‹å¯èƒ½æ˜¯æœ‰å™ªå£°çš„ï¼Œå› ä¸ºæ›´æ–°å’ŒæŸå¤±ä¼ æ’­æ˜¯æŒ‰æ ‡è®°è¿›è¡Œçš„ï¼Œè¿™å–å†³äºç»“æœå’Œæ³¨æ„åŠ›æ˜ å°„ã€‚
- en: These models have continued in use, but are less supported in open-source RLHF
    tools. For example, the same type of ORM was used in the seminal work *Letâ€™s Verify
    Step by Step* [[45]](ch021.xhtml#ref-lightman2023let), but without the language
    modeling prediction piece of the loss. Then, the final loss is a cross entropy
    loss on every token predicting if the final answer is correct.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ¨¡å‹ä»åœ¨ä½¿ç”¨ä¸­ï¼Œä½†åœ¨å¼€æº RLHF å·¥å…·ä¸­æ”¯æŒè¾ƒå°‘ã€‚ä¾‹å¦‚ï¼Œåœ¨å¼€åˆ›æ€§å·¥ä½œ *Letâ€™s Verify Step by Step* [[45]](ch021.xhtml#ref-lightman2023let)
    ä¸­ä½¿ç”¨äº†ç›¸åŒç±»å‹çš„ ORMï¼Œä½†æ²¡æœ‰ä½¿ç”¨æŸå¤±ä¸­çš„è¯­è¨€å»ºæ¨¡é¢„æµ‹éƒ¨åˆ†ã€‚ç„¶åï¼Œæœ€ç»ˆçš„æŸå¤±æ˜¯æ¯ä¸ªæ ‡è®°é¢„æµ‹æœ€ç»ˆç­”æ¡ˆæ˜¯å¦æ­£ç¡®çš„äº¤å‰ç†µæŸå¤±ã€‚
- en: Given the lack of support, the term outcome reward model (ORM) has been used
    in multiple ways. Some literature, e.g. [[130]](ch021.xhtml#ref-lyu2025exploring),
    continues to use the original definition from Cobbe et al.Â 2021\. Others do not.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºç¼ºä¹æ”¯æŒï¼Œç»“æœå¥–åŠ±æ¨¡å‹ï¼ˆORMï¼‰è¢«ä»¥å¤šç§æ–¹å¼ä½¿ç”¨ã€‚ä¸€äº›æ–‡çŒ®ï¼Œä¾‹å¦‚ [[130]](ch021.xhtml#ref-lyu2025exploring)ï¼Œç»§ç»­ä½¿ç”¨
    Cobbe ç­‰äººäº 2021 å¹´æå‡ºçš„åŸå§‹å®šä¹‰ã€‚è€Œå¦ä¸€äº›åˆ™ä¸è¿™æ ·åšã€‚
- en: Process Reward Models
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æµç¨‹å¥–åŠ±æ¨¡å‹
- en: Process Reward Models (PRMs), originally called Process-supervised Reward Models,
    are reward models trained to output scores at every *step* in a chain of thought
    reasoning process. These differ from a standard RM that outputs a score only at
    an EOS token or a ORM that outputs a score at every token. Process Reward Models
    require supervision at the end of each reasoning step, and then are trained similarly
    where the tokens in the step are trained to their relevant target â€“ the target
    is the step in PRMs and the entire response for ORMs.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: æµç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰ï¼Œæœ€åˆè¢«ç§°ä¸ºæµç¨‹ç›‘ç£å¥–åŠ±æ¨¡å‹ï¼Œæ˜¯è®­ç»ƒç”¨äºåœ¨æ¯ä¸ªæ€ç»´æ¨ç†è¿‡ç¨‹çš„é“¾ä¸­æ¯ä¸€æ­¥è¾“å‡ºåˆ†æ•°çš„å¥–åŠ±æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹ä¸ä»…åœ¨ EOS æ ‡è®°å¤„è¾“å‡ºåˆ†æ•°çš„æ ‡å‡†
    RM æˆ–åœ¨æ¯ä¸ªæ ‡è®°å¤„è¾“å‡ºåˆ†æ•°çš„ ORM ä¸åŒã€‚æµç¨‹å¥–åŠ±æ¨¡å‹éœ€è¦åœ¨æ¯ä¸ªæ¨ç†æ­¥éª¤çš„æœ«å°¾è¿›è¡Œç›‘ç£ï¼Œç„¶åä»¥ç±»ä¼¼çš„æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œå…¶ä¸­æ­¥éª¤ä¸­çš„æ ‡è®°è¢«è®­ç»ƒåˆ°å®ƒä»¬çš„ç›¸å…³ç›®æ ‡â€”â€”åœ¨
    PRMs ä¸­ç›®æ ‡æ˜¯æ­¥éª¤ï¼Œè€Œåœ¨ ORMs ä¸­æ˜¯æ•´ä¸ªå“åº”ã€‚
- en: 'Following [[45]](ch021.xhtml#ref-lightman2023let), a binary-labeled PRM is
    commonly optimized with a per-step cross-entropy loss:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ® [[45]](ch021.xhtml#ref-lightman2023let)ï¼ŒäºŒåˆ†ç±»æ ‡ç­¾çš„æµç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰é€šå¸¸ä½¿ç”¨æ¯æ­¥äº¤å‰ç†µæŸå¤±è¿›è¡Œä¼˜åŒ–ï¼š
- en: <semantics><mrow><msub><mi>â„’</mi><mtext mathvariant="normal">PRM</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>âˆ’</mi><msub><mi>ğ”¼</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>s</mi><mo stretchy="false"
    form="postfix">)</mo><mo>âˆ¼</mo><mi>ğ’Ÿ</mi></mrow></msub><mrow><mo stretchy="true"
    form="prefix">[</mo><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><msub><mi>y</mi><msub><mi>s</mi><mi>i</mi></msub></msub><mi
    mathvariant="normal">log</mi><msub><mi>r</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><msub><mi>s</mi><mi>i</mi></msub><mo>âˆ£</mo><mi>x</mi><mo stretchy="false"
    form="postfix">)</mo><mo>+</mo><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>âˆ’</mo><msub><mi>y</mi><msub><mi>s</mi><mi>i</mi></msub></msub><mo
    stretchy="false" form="postfix">)</mo><mi mathvariant="normal">log</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo>âˆ’</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>i</mi></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow><mo
    stretchy="true" form="postfix">]</mo></mrow><mrow><mo stretchy="false" form="prefix">(</mo><mn>21</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}_{\text{PRM}}(\theta)
    = - \mathbb{E}_{(x, s) \sim \mathcal{D}} \left[ \sum_{i=1}^{K} y_{s_i} \log r_\theta(s_i
    \mid x) + (1 - y_{s_i}) \log \left(1 - r_\theta(s_i \mid x)\right) \right] \qquad{(21)}</annotation></semantics>
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><msub><mi>â„’</mi><mtext mathvariant="normal">PRM</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>âˆ’</mi><msub><mi>ğ”¼</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>s</mi><mo stretchy="false"
    form="postfix">)</mo><mo>âˆ¼</mo><mi>ğ’Ÿ</mi></mrow></msub><mrow><mo stretchy="true"
    form="prefix">[</mo><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><msub><mi>y</mi><msub><mi>s</mi><mi>i</mi></msub></msub><mi
    mathvariant="normal">log</mi><msub><mi>r</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><msub><mi>s</mi><mi>i</mi></msub><mo>âˆ£</mo><mi>x</mi><mo stretchy="false"
    form="postfix">)</mo><mo>+</mo><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>âˆ’</mo><msub><mi>y</mi><msub><mi>s</mi><mi>i</mi></msub></msub><mo
    stretchy="false" form="postfix">)</mo><mi mathvariant="normal">log</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo>âˆ’</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>i</mi></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow><mo
    stretchy="true" form="postfix">]</mo></mrow><mrow><mo stretchy="false" form="prefix">(</mo><mn>21</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}_{\text{PRM}}(\theta)
    = - \mathbb{E}_{(x, s) \sim \mathcal{D}} \left[ \sum_{i=1}^{K} y_{s_i} \log r_\theta(s_i
    \mid x) + (1 - y_{s_i}) \log \left(1 - r_\theta(s_i \mid x)\right) \right] \qquad{(21)}</annotation></semantics>
- en: where <semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics>
    is a sampled chain-of-thought with <semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics>
    annotated steps, <semantics><mrow><msub><mi>y</mi><msub><mi>s</mi><mi>i</mi></msub></msub><mo>âˆˆ</mo><mo
    stretchy="false" form="prefix">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false"
    form="postfix">}</mo></mrow><annotation encoding="application/x-tex">y_{s_i} \in
    \{0,1\}</annotation></semantics> denotes whether the <semantics><mi>i</mi><annotation
    encoding="application/x-tex">i</annotation></semantics>-th step is correct, and
    <semantics><mrow><msub><mi>r</mi><mi>Î¸</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>i</mi></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">r_\theta(s_i
    \mid x)</annotation></semantics> is the PRMâ€™s predicted probability that step
    <semantics><msub><mi>s</mi><mi>i</mi></msub><annotation encoding="application/x-tex">s_i</annotation></semantics>
    is valid conditioned on the original prompt <semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics>.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ <semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics>
    æ˜¯ä¸€ä¸ªå¸¦æœ‰ <semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics>
    ä¸ªæ ‡è®°æ­¥éª¤çš„é‡‡æ ·æ€ç»´é“¾ï¼Œ<semantics><mrow><msub><mi>y</mi><msub><mi>s</mi><mi>i</mi></msub></msub><mo>âˆˆ</mo><mo
    stretchy="false" form="prefix">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false"
    form="postfix">}</mo></mrow><annotation encoding="application/x-tex">y_{s_i} \in
    \{0,1\}</annotation></semantics> è¡¨ç¤ºç¬¬ <semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics>
    ä¸ªæ­¥éª¤æ˜¯å¦æ­£ç¡®ï¼Œè€Œ <semantics><mrow><msub><mi>r</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><msub><mi>s</mi><mi>i</mi></msub><mo>âˆ£</mo><mi>x</mi><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">r_\theta(s_i
    \mid x)</annotation></semantics> æ˜¯PRMé¢„æµ‹çš„ç¬¬ <semantics><msub><mi>s</mi><mi>i</mi></msub><annotation
    encoding="application/x-tex">s_i</annotation></semantics> æ­¥éª¤åœ¨ç»™å®šåŸå§‹æç¤º <semantics><mi>x</mi><annotation
    encoding="application/x-tex">x</annotation></semantics> ä¸‹æœ‰æ•ˆçš„æ¦‚ç‡ã€‚
- en: 'Hereâ€™s an example of how this per-step label can be packaged in a trainer,
    from HuggingFaceâ€™s TRL (Transformer Reinforcement Learning) [[42]](ch021.xhtml#ref-vonwerra2022trl):'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ä¸€ä¸ªå¦‚ä½•å°†æ¯æ­¥æ ‡ç­¾æ‰“åŒ…åˆ°è®­ç»ƒå™¨ä¸­çš„ç¤ºä¾‹ï¼Œæ¥è‡ªHuggingFaceçš„TRLï¼ˆTransformer Reinforcement Learningï¼‰[[42]](ch021.xhtml#ref-vonwerra2022trl)ï¼š
- en: '[PRE4]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Traditionally PRMs are trained with a language modeling head that outputs a
    token only at the end of a reasoning step, e.g.Â at the token corresponding to
    a double new line or other special token. These predictions tend to be -1 for
    incorrect, 0 for neutral, and 1 for correct. These labels do not necessarily tie
    with whether or not the model is on the right path, but if the step is correct.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼ ç»Ÿä¸Šï¼ŒPRMsä½¿ç”¨è¯­è¨€æ¨¡å‹å¤´éƒ¨è¿›è¡Œè®­ç»ƒï¼Œä»…åœ¨æ¨ç†æ­¥éª¤çš„æœ«å°¾è¾“å‡ºä¸€ä¸ªæ ‡è®°ï¼Œä¾‹å¦‚åœ¨å¯¹åº”äºåŒæ¢è¡Œç¬¦æˆ–å…¶ä»–ç‰¹æ®Šæ ‡è®°çš„æ ‡è®°å¤„ã€‚è¿™äº›é¢„æµ‹é€šå¸¸ä¸º-1è¡¨ç¤ºé”™è¯¯ï¼Œ0è¡¨ç¤ºä¸­æ€§ï¼Œ1è¡¨ç¤ºæ­£ç¡®ã€‚è¿™äº›æ ‡ç­¾ä¸ä¸€å®šä¸æ¨¡å‹æ˜¯å¦åœ¨æ­£ç¡®è·¯å¾„ä¸Šç›¸å…³ï¼Œä½†å¦‚æœæ˜¯æ­£ç¡®æ­¥éª¤ã€‚
- en: An example construction of a PRM is shown below.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢å±•ç¤ºäº†PRMçš„ä¸€ä¸ªç¤ºä¾‹æ„å»ºã€‚
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The core loss function looks very similar to outcome reward models, with the
    labels being applied at different intervals.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¸å¿ƒæŸå¤±å‡½æ•°çœ‹èµ·æ¥ä¸ç»“æœå¥–åŠ±æ¨¡å‹éå¸¸ç›¸ä¼¼ï¼Œæ ‡ç­¾åº”ç”¨åœ¨ä¸åŒçš„é—´éš”ã€‚
- en: '[PRE6]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Reward Models vs.Â Outcome RMs vs.Â Process RMs vs.Â Value Functions
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¥–åŠ±æ¨¡å‹ vs. ç»“æœRM vs. è¿‡ç¨‹RM vs. ä»·å€¼å‡½æ•°
- en: The various types of reward models covered indicate the spectrum of ways that
    â€œqualityâ€ can be measured in RLHF and other post-training methods. Below, a summary
    of what the models predict and how they are trained.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: æ¶µç›–çš„å„ç§å¥–åŠ±æ¨¡å‹ç±»å‹è¡¨æ˜äº†åœ¨RLHFå’Œå…¶ä»–åè®­ç»ƒæ–¹æ³•ä¸­â€œè´¨é‡â€å¯ä»¥è¡¡é‡çš„èŒƒå›´ã€‚ä»¥ä¸‹æ˜¯å¯¹æ¨¡å‹é¢„æµ‹å†…å®¹å’Œè®­ç»ƒæ–¹å¼çš„æ€»ç»“ã€‚
- en: 'Table 4: Comparing types of reward models.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨4ï¼šæ¯”è¾ƒå¥–åŠ±æ¨¡å‹ç±»å‹ã€‚
- en: '| Model Class | What They Predict | How They Are Trained | LM structure |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| æ¨¡å‹ç±»åˆ« | é¢„æµ‹çš„å†…å®¹ | è®­ç»ƒæ–¹å¼ | LMç»“æ„ |'
- en: '| --- | --- | --- | --- |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Reward Models** | Quality of text via probability of chosen response at
    EOS token | Contrastive loss between pairwise (or N-wise) comparisons between
    completions | Regression or classification head on top of LM features |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| **å¥–åŠ±æ¨¡å‹** | é€šè¿‡EOSæ ‡è®°é€‰æ‹©çš„å“åº”çš„æ¦‚ç‡æ¥è¡¡é‡æ–‡æœ¬è´¨é‡ | åœ¨å®Œæˆä¹‹é—´çš„æˆå¯¹ï¼ˆæˆ–N-wiseï¼‰æ¯”è¾ƒçš„å¯¹æ¯”æŸå¤± | åœ¨LMç‰¹å¾ä¹‹ä¸Šçš„å›å½’æˆ–åˆ†ç±»å¤´éƒ¨
    |'
- en: '| **Outcome Reward Models** | Probability that an answer is correct per-token
    | Labeled outcome pairs (e.g., success/failure on verifiable domains) | Language
    modeling head per-token cross-entropy, where every label is the outcome level
    label |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| **ç»“æœå¥–åŠ±æ¨¡å‹** | æ¯ä¸ªæ ‡è®°çš„æ­£ç¡®ç­”æ¡ˆæ¦‚ç‡ | æ ‡è®°çš„ç»“æœå¯¹ï¼ˆä¾‹å¦‚ï¼Œåœ¨å¯éªŒè¯é¢†åŸŸä¸Šçš„æˆåŠŸ/å¤±è´¥ï¼‰ | æ¯ä¸ªæ ‡è®°çš„è¯­è¨€æ¨¡å‹äº¤å‰ç†µï¼Œå…¶ä¸­æ¯ä¸ªæ ‡ç­¾éƒ½æ˜¯ç»“æœçº§åˆ«æ ‡ç­¾
    |'
- en: '| **Process Reward Models** | A reward or score for intermediate steps at end
    of reasoning steps | Trained using intermediate feedback or stepwise annotations
    (trained per token in reasoning step) | Language modeling head only running inference
    per reasoning step, predicts three classes -1, 0, 1 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| **è¿‡ç¨‹å¥–åŠ±æ¨¡å‹** | æ¨ç†æ­¥éª¤ç»“æŸæ—¶çš„ä¸­é—´æ­¥éª¤çš„å¥–åŠ±æˆ–åˆ†æ•° | ä½¿ç”¨ä¸­é—´åé¦ˆæˆ–é€æ­¥æ³¨é‡Šï¼ˆæ¨ç†æ­¥éª¤ä¸­æ¯tokenè®­ç»ƒï¼‰è¿›è¡Œè®­ç»ƒ | æ¯ä¸ªæ¨ç†æ­¥éª¤åªè¿è¡Œä¸€æ¬¡æ¨ç†çš„è¯­è¨€æ¨¡å‹å¤´éƒ¨ï¼Œé¢„æµ‹ä¸‰ä¸ªç±»åˆ«
    -1, 0, 1 |'
- en: '| **Value Functions** | The expected return given the current state | Trained
    via regression to each point in sequence | A classification with output per-token
    |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| **ä»·å€¼å‡½æ•°** | ç»™å®šå½“å‰çŠ¶æ€ä¸‹çš„é¢„æœŸå›æŠ¥ | é€šè¿‡å›å½’åˆ°åºåˆ—ä¸­çš„æ¯ä¸ªç‚¹è¿›è¡Œè®­ç»ƒ | æ¯tokenè¾“å‡ºä¸€ä¸ªåˆ†ç±» |'
- en: Some notes, given the above table has a lot of edge cases.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€äº›æ³¨æ„äº‹é¡¹ï¼Œé‰´äºä¸Šè¿°è¡¨æ ¼æœ‰è®¸å¤šè¾¹ç¼˜æƒ…å†µã€‚
- en: Both in preference tuning and reasoning training, the value functions often
    have a discount factor of 1, which makes a value function even closer to an outcome
    reward model, but with a different training loss.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨åå¥½è°ƒæ•´å’Œæ¨ç†è®­ç»ƒä¸­ï¼Œä»·å€¼å‡½æ•°é€šå¸¸æœ‰ä¸€ä¸ªæŠ˜æ‰£å› å­ä¸º1ï¼Œè¿™ä½¿å¾—ä»·å€¼å‡½æ•°æ›´æ¥è¿‘ç»“æœå¥–åŠ±æ¨¡å‹ï¼Œä½†å…·æœ‰ä¸åŒçš„è®­ç»ƒæŸå¤±ã€‚
- en: A process reward model can be supervised by doing rollouts from an intermediate
    state and collecting outcome data. This blends multiple ideas, but if the *loss*
    is per reasoning step labels, it is best referred to as a PRM.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡ä»ä¸­é—´çŠ¶æ€è¿›è¡Œå›æ»šå¹¶æ”¶é›†ç»“æœæ•°æ®æ¥ç›‘ç£è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ã€‚è¿™èåˆäº†å¤šä¸ªæƒ³æ³•ï¼Œä½†å¦‚æœæŸå¤±æ˜¯æŒ‰æ¨ç†æ­¥éª¤æ ‡ç­¾è®¡ç®—çš„ï¼Œåˆ™æœ€å¥½å°†å…¶ç§°ä¸ºPRMã€‚
- en: Generative Reward Modeling
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç”Ÿæˆå¥–åŠ±å»ºæ¨¡
- en: 'With the cost of preference data, a large research area emerged to use existing
    language models as a judge of human preferences or in other evaluation settings
    [[131]](ch021.xhtml#ref-zheng2023judging). The core idea is to prompt a language
    model with instructions on how to judge, a prompt, and two completions (much as
    would be done with human labelers). An example prompt, from one of the seminal
    works here for the chat evaluation MT-Bench [[131]](ch021.xhtml#ref-zheng2023judging),
    follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºåå¥½æ•°æ®çš„æˆæœ¬ï¼Œå‡ºç°äº†ä¸€ä¸ªå¤§å‹ç ”ç©¶é¢†åŸŸï¼Œå³ä½¿ç”¨ç°æœ‰çš„è¯­è¨€æ¨¡å‹ä½œä¸ºäººç±»åå¥½çš„è¯„åˆ¤è€…æˆ–åœ¨å…¶ä»–è¯„ä¼°ç¯å¢ƒä¸­ [[131]](ch021.xhtml#ref-zheng2023judging)ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯å‘è¯­è¨€æ¨¡å‹æä¾›å¦‚ä½•è¯„åˆ¤çš„æŒ‡ä»¤ï¼Œä¸€ä¸ªæç¤ºï¼Œä»¥åŠä¸¤ä¸ªå®Œæˆï¼ˆå°±åƒå¯¹äººç±»æ ‡æ³¨è€…æ‰€åšçš„é‚£æ ·ï¼‰ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºä¾‹æç¤ºï¼Œæ¥è‡ªè¿™é‡Œçš„ä¸€ä¸ªå¼€åˆ›æ€§å·¥ä½œï¼Œç”¨äºèŠå¤©è¯„ä¼°MT-Bench
    [[131]](ch021.xhtml#ref-zheng2023judging)ï¼š
- en: '[PRE7]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Given the efficacy of LLM-as-a-judge for evaluation, spawning many other evaluations
    such as AlpacaEval [[132]](ch021.xhtml#ref-dubois2024length), Arena-Hard [[133]](ch021.xhtml#ref-li2024crowdsourced),
    and WildBench [[134]](ch021.xhtml#ref-lin2024wildbench), many began using LLM-as-a-judge
    instead of reward models to create and use preference data.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºLLM-as-a-judgeåœ¨è¯„ä¼°ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå‚¬ç”Ÿäº†è®¸å¤šå…¶ä»–è¯„ä¼°ï¼Œå¦‚AlpacaEval [[132]](ch021.xhtml#ref-dubois2024length)ã€Arena-Hard
    [[133]](ch021.xhtml#ref-li2024crowdsourced)å’ŒWildBench [[134]](ch021.xhtml#ref-lin2024wildbench)ï¼Œè®¸å¤šäººå¼€å§‹ä½¿ç”¨LLM-as-a-judgeè€Œä¸æ˜¯å¥–åŠ±æ¨¡å‹æ¥åˆ›å»ºå’Œä½¿ç”¨åå¥½æ•°æ®ã€‚
- en: An entire field of study has emerged to study how to use so called â€œGenerative
    Reward Modelsâ€ [[135]](ch021.xhtml#ref-mahan2024generative) [[136]](ch021.xhtml#ref-zhang2024generative)
    [[137]](ch021.xhtml#ref-ankner2024critique) (including models trained *specifically*
    to be effective judges [[138]](ch021.xhtml#ref-kim2023prometheus)), but on RM
    evaluations they tend to be behind existing reward models, showing that reward
    modeling is an important technique for current RLHF.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: å‡ºç°äº†ä¸€ä¸ªç ”ç©¶é¢†åŸŸï¼Œä¸“é—¨ç ”ç©¶å¦‚ä½•ä½¿ç”¨æ‰€è°“çš„â€œç”Ÿæˆå¥–åŠ±æ¨¡å‹â€ [[135]](ch021.xhtml#ref-mahan2024generative) [[136]](ch021.xhtml#ref-zhang2024generative)
    [[137]](ch021.xhtml#ref-ankner2024critique)ï¼ˆåŒ…æ‹¬ä¸“é—¨è®­ç»ƒä»¥æˆä¸ºæœ‰æ•ˆè¯„åˆ¤è€…çš„æ¨¡å‹ [[138]](ch021.xhtml#ref-kim2023prometheus)ï¼‰ï¼Œä½†åœ¨RMè¯„ä¼°ä¸­ï¼Œå®ƒä»¬å¾€å¾€è½åäºç°æœ‰çš„å¥–åŠ±æ¨¡å‹ï¼Œè¿™è¡¨æ˜å¥–åŠ±å»ºæ¨¡æ˜¯å½“å‰RLHFçš„é‡è¦æŠ€æœ¯ã€‚
- en: A common trick to improve the robustness of LLM-as-a-judge workflows is to use
    a sampling temperature of 0 to reduce variance of ratings.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: æé«˜LLM-as-a-judgeå·¥ä½œæµç¨‹é²æ£’æ€§çš„ä¸€ä¸ªå¸¸è§æŠ€å·§æ˜¯ä½¿ç”¨0çš„é‡‡æ ·æ¸©åº¦ä»¥å‡å°‘è¯„åˆ†çš„æ–¹å·®ã€‚
- en: Further Reading
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¿›ä¸€æ­¥é˜…è¯»
- en: The academic literature for reward modeling established itself in 2024\. The
    bulk of progress in reward modeling early on has been in establishing benchmarks
    and identifying behavior modes. The first RM benchmark, RewardBench, provided
    common infrastructure for testing reward models [[139]](ch021.xhtml#ref-lambert2024rewardbench).
    Since then, RM evaluation has expanded to be similar to the types of evaluations
    available to general post-trained models, where some evaluations test the accuracy
    of prediction on domains with known true answers [[139]](ch021.xhtml#ref-lambert2024rewardbench)
    or those more similar to â€œvibesâ€ performed with LLM-as-a-judge or correlations
    to other benchmarks [[140]](ch021.xhtml#ref-wen2024rethinking).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: å¥–åŠ±å»ºæ¨¡çš„å­¦æœ¯æ–‡çŒ®åœ¨ 2024 å¹´ç¡®ç«‹äº†è‡ªå·±ã€‚æ—©æœŸå¥–åŠ±å»ºæ¨¡çš„å¤§éƒ¨åˆ†è¿›å±•åœ¨äºå»ºç«‹åŸºå‡†å’Œè¯†åˆ«è¡Œä¸ºæ¨¡å¼ã€‚ç¬¬ä¸€ä¸ª RM åŸºå‡†ï¼ŒRewardBenchï¼Œä¸ºæµ‹è¯•å¥–åŠ±æ¨¡å‹æä¾›äº†å…±åŒçš„åŸºç¡€è®¾æ–½
    [[139]](ch021.xhtml#ref-lambert2024rewardbench)ã€‚ä»é‚£æ—¶èµ·ï¼ŒRM è¯„ä¼°å·²ç»æ‰©å±•åˆ°ç±»ä¼¼äºå¯ç”¨äºé€šç”¨åè®­ç»ƒæ¨¡å‹çš„è¯„ä¼°ç±»å‹ï¼Œå…¶ä¸­ä¸€äº›è¯„ä¼°æµ‹è¯•äº†åœ¨å·²çŸ¥çœŸå®ç­”æ¡ˆçš„é¢†åŸŸæˆ–ä¸â€œæ„Ÿè§‰â€æ›´ç›¸ä¼¼çš„é¢†åŸŸä¸Šçš„é¢„æµ‹å‡†ç¡®æ€§
    [[139]](ch021.xhtml#ref-lambert2024rewardbench) æˆ–ä¸ LLM ä½œä¸ºè£åˆ¤æˆ–ä¸å…¶ä»–åŸºå‡†çš„ç›¸å…³æ€§ [[140]](ch021.xhtml#ref-wen2024rethinking)ã€‚
- en: 'Examples of new benchmarks include:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: æ–°åŸºå‡†çš„ä¾‹å­åŒ…æ‹¬ï¼š
- en: '**Text-only (general chat / preferences):** RMB [[141]](ch021.xhtml#ref-zhou2024rmb),
    RewardBench2 [[112]](ch021.xhtml#ref-malik2025rewardbench), Preference Proxy Evaluations
    [[142]](ch021.xhtml#ref-frick2024evaluate), or RM-Bench [[143]](ch021.xhtml#ref-liu2024rm).'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**çº¯æ–‡æœ¬ï¼ˆé€šç”¨èŠå¤©/åå¥½ï¼‰ï¼š** RMB [[141]](ch021.xhtml#ref-zhou2024rmb)ï¼ŒRewardBench2 [[112]](ch021.xhtml#ref-malik2025rewardbench)ï¼Œåå¥½ä»£ç†è¯„ä¼°
    [[142]](ch021.xhtml#ref-frick2024evaluate)ï¼Œæˆ– RM-Bench [[143]](ch021.xhtml#ref-liu2024rm)ã€‚'
- en: '**Specialized text-only (math, etc.):** multilingual reward bench (M-RewardBench)
    [[144]](ch021.xhtml#ref-gureja2024m), RAG-RewardBench for retrieval augmented
    generation (RAG) [[145]](ch021.xhtml#ref-jin2024rag), ReWordBench for typos [[146]](ch021.xhtml#ref-wu2025rewordbench),
    RewardMATH [[147]](ch021.xhtml#ref-kim2024evaluating), or AceMath-RewardBench
    [[148]](ch021.xhtml#ref-liu2024acemath).'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä¸“é—¨åŒ–çš„çº¯æ–‡æœ¬ï¼ˆæ•°å­¦ç­‰ï¼‰ï¼š** å¤šè¯­è¨€å¥–åŠ±åŸºå‡† (M-RewardBench) [[144]](ch021.xhtml#ref-gureja2024m)ï¼Œç”¨äºæ£€ç´¢å¢å¼ºç”Ÿæˆ
    (RAG) çš„ RAG-RewardBench [[145]](ch021.xhtml#ref-jin2024rag)ï¼Œç”¨äºæ‹¼å†™é”™è¯¯çš„ ReWordBench
    [[146]](ch021.xhtml#ref-wu2025rewordbench)ï¼ŒRewardMATH [[147]](ch021.xhtml#ref-kim2024evaluating)ï¼Œæˆ–
    AceMath-RewardBench [[148]](ch021.xhtml#ref-liu2024acemath)ã€‚'
- en: '**Process RMs:** PRM Bench [[149]](ch021.xhtml#ref-song2025prmbench) or ProcessBench
    [[150]](ch021.xhtml#ref-zheng2024processbench) and visual benchmarks of VisualProcessBench
    [[151]](ch021.xhtml#ref-wang2025visualprm) or ViLBench [[152]](ch021.xhtml#ref-tu2025vilbench).'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è¿‡ç¨‹ RMsï¼š** PRM Bench [[149]](ch021.xhtml#ref-song2025prmbench) æˆ– ProcessBench
    [[150]](ch021.xhtml#ref-zheng2024processbench) ä»¥åŠ VisualProcessBench [[151]](ch021.xhtml#ref-wang2025visualprm)
    æˆ– ViLBench [[152]](ch021.xhtml#ref-tu2025vilbench) çš„è§†è§‰åŸºå‡†ã€‚'
- en: '**Agentic RMs:** Agent-RewardBench [[153]](ch021.xhtml#ref-men2025agentrewardbench)
    or CUARewardBench [[154]](ch021.xhtml#ref-lin2025cuarewardbench).'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä»£ç†å¼ RMsï¼š** Agent-RewardBench [[153]](ch021.xhtml#ref-men2025agentrewardbench)
    æˆ– CUARewardBench [[154]](ch021.xhtml#ref-lin2025cuarewardbench)ã€‚'
- en: '**Multimodal:** MJ-Bench [[155]](ch021.xhtml#ref-chen2024mj), Multimodal RewardBench
    [[156]](ch021.xhtml#ref-yasunaga2025multimodal), VL RewardBench [[157]](ch021.xhtml#ref-li2024vlrewardbench),
    or VLRMBench [[158]](ch021.xhtml#ref-ruan2025vlrmbench).'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¤šæ¨¡æ€ï¼š** MJ-Bench [[155]](ch021.xhtml#ref-chen2024mj), å¤šæ¨¡æ€å¥–åŠ±åŸºå‡† [[156]](ch021.xhtml#ref-yasunaga2025multimodal),
    VL å¥–åŠ±åŸºå‡† [[157]](ch021.xhtml#ref-li2024vlrewardbench), æˆ– VLRMBench [[158]](ch021.xhtml#ref-ruan2025vlrmbench)ã€‚'
- en: To understand progress on *training* reward models, one can reference new reward
    model training methods, with aspect-conditioned models [[159]](ch021.xhtml#ref-wang2024interpretable),
    high quality human datasets [[160]](ch021.xhtml#ref-wang2024helpsteer2) [[111]](ch021.xhtml#ref-wang2024helpsteer2p),
    scaling experiments [[25]](ch021.xhtml#ref-adler2024nemotron), extensive experimentation
    [[44]](ch021.xhtml#ref-touvron2023llama), or debiasing data [[161]](ch021.xhtml#ref-park2024offsetbias).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: è¦äº†è§£å¥–åŠ±æ¨¡å‹è®­ç»ƒçš„è¿›å±•ï¼Œå¯ä»¥å‚è€ƒæ–°çš„å¥–åŠ±æ¨¡å‹è®­ç»ƒæ–¹æ³•ï¼ŒåŒ…æ‹¬æ–¹é¢æ¡ä»¶æ¨¡å‹ [[159]](ch021.xhtml#ref-wang2024interpretable)ï¼Œé«˜è´¨é‡çš„äººç±»æ•°æ®é›†
    [[160]](ch021.xhtml#ref-wang2024helpsteer2) [[111]](ch021.xhtml#ref-wang2024helpsteer2p)ï¼Œæ‰©å±•å®éªŒ
    [[25]](ch021.xhtml#ref-adler2024nemotron)ï¼Œå¹¿æ³›çš„å®éªŒ [[44]](ch021.xhtml#ref-touvron2023llama)ï¼Œæˆ–å»åæ•°æ®
    [[161]](ch021.xhtml#ref-park2024offsetbias)ã€‚
