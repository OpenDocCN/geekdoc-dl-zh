- en: '4'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '4'
- en: Optimizing LLMs with Customized Fine-Tuning
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用定制微调优化LLMs
- en: Introduction
  id: totrans-2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 引言
- en: So far, we’ve exclusively used LLMs, both open and closed sourced, as they are
    off the shelf. We were relying on the power of the Transformer’s attention mechanisms
    and their speed of computation to perform pretty complex problems with relative
    ease. As you can probably guess, that isn’t always enough.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直仅使用LLMs，无论是开源的还是闭源的，因为它们是现成的。我们依赖Transformer的注意力机制及其计算速度来相对轻松地执行相当复杂的问题。正如你可能猜到的，这并不总是足够的。
- en: In this chapter, we will delve into the world of fine-tuning Large Language
    Models (LLMs) to unlock their full potential. Fine-tuning updates off the shelf
    models and empowers them to achieve higher quality results, leads to token savings,
    and often lower latency requests. While GPT-like LLMs’ pre-training on extensive
    text data enables impressive few-shot learning capabilities, fine-tuning takes
    it a step further by refining the model on a multitude of examples, resulting
    in superior performance across various tasks.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入探讨大型语言模型（LLMs）的微调世界，以释放其全部潜力。微调更新现成的模型，并赋予它们实现更高质量结果的能力，导致令牌节省，并且通常降低延迟请求。虽然类似GPT的LLMs在大量文本数据上的预训练使得令人印象深刻的少样本学习能力成为可能，但微调通过在众多示例上细化模型，进一步提升了在各种任务上的性能。
- en: Running inference with fine-tuned models can be extremely cost-effective in
    the long run particularly when working with smaller models. For instance, a fine-tuned
    Ada model from OpenAI (only 350M parameters) costs only $0.0016 per 1k tokens,
    while ChatGPT (1.5B parameters) costs $0.002, and Davinci (175B parameters) costs
    $0.002\. Over time, the cost of using a fine tuned model is much more attractive
    as shown in [Figure 4.1](ch04.html#ch04fig01).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 使用微调模型进行推理在长期来看可以非常经济高效，尤其是在处理较小模型时。例如，来自OpenAI的微调Ada模型（只有3.5亿个参数）每1k个令牌的成本仅为0.0016美元，而ChatGPT（15亿个参数）的成本为0.002美元，Davinci（1750亿个参数）的成本为0.002美元。随着时间的推移，使用微调模型的成本将更加吸引人，如图4.1所示。
- en: '![Images](graphics/04fig01.jpg)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/04fig01.jpg)'
- en: '**Figure 4.1** *Assuming only 1,000 classifications a day and a relatively
    liberal prompt ratio (150 tokens (for few-shot examples, instructions,other) for
    DaVinci or ChatGPT for every 40 tokens), the cost of a fine-tuned model, even
    with an up-front cost, almost always wins the day overall cost-wise. Note that
    this does not take into the cost of fine-tuning a model which we will begin to
    explore later in this chapter.*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**图4.1** *假设每天只有1,000次分类，并且相对宽松的提示比例（对于DaVinci或ChatGPT，每40个令牌有150个令牌（用于少样本示例、指令和其他）），即使有前期成本，微调模型的成本在总体成本上几乎总是占上风。请注意，这并不包括微调模型的成本，我们将在本章后面开始探讨这一点。*'
- en: My goal in this chapter is to guide you through the fine-tuning process, beginning
    with the preparation of training data, strategies for training a new or existing
    fine-tuned model, and a discussion of how to incorporate your fine-tuned model
    into real-world applications. This is a big topic and therefore we will have to
    assume some big pieces are being done behind the scenes like data labeling. Labeling
    data can be a huge expense in many cases of complex and specific tasks but for
    now, we will assume we can rely on the labels in our data for the most part. For
    more information on how to handle cases like these, feel free to check out some
    of my other content on feature engineering and label cleaning!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我在本章中的目标是指导你通过微调过程，从准备训练数据开始，到训练新或现有微调模型的战略，以及讨论如何将你的微调模型融入实际应用中。这是一个很大的主题，因此我们不得不假设一些大的工作是在幕后完成的，比如数据标注。在许多复杂和特定任务的情况下，数据标注可能是一项巨大的开销，但到目前为止，我们将假设我们大部分可以依赖数据中的标签。有关如何处理此类情况的更多信息，请随时查看我关于特征工程和标签清洗的一些其他内容！
- en: By understanding the nuances of fine-tuning and mastering its techniques, you
    will be well-equipped to harness the power of LLMs and create tailored solutions
    for your specific needs.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 通过理解微调的细微差别并掌握其技术，你将准备好利用LLMs的力量，并为你的特定需求创建定制解决方案。
- en: 'Transfer Learning and Fine-Tuning: A Primer'
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 转移学习和微调：入门
- en: Fine-tuning hinges on the idea of transfer learning. **Transfer learning** is
    a technique that leverages pre-trained models to build upon existing knowledge
    for new tasks or domains. In the case of LLMs, this involves utilizing the pre-training
    to transfer general language understanding, including grammar and general knowledge,
    to specific domain-specific tasks. However, the pre-training may not be sufficient
    to understand the nuances of certain closed or specialized topics, such as a company's
    legal structure or guidelines.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 微调基于迁移学习理念。**迁移学习**是一种利用预训练模型来构建新任务或领域现有知识的技术。在LLMs的情况下，这涉及到利用预训练来转移一般语言理解，包括语法和一般知识，到特定领域特定任务。然而，预训练可能不足以理解某些封闭或专业主题的细微差别，例如一家公司的法律结构或指南。
- en: '**Fine-tuning** is a specific form of transfer learning that adjusts the parameters
    of a pre-trained model to better suit a “downstream” target task. Through fine-tuning,
    LLMs can learn from custom examples and become more effective at generating relevant
    and accurate responses.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**微调**是迁移学习的一种特定形式，它调整预训练模型的参数以更好地适应“下游”目标任务。通过微调，LLMs可以从定制示例中学习，并更有效地生成相关且准确的响应。'
- en: The Fine-Tuning Process Explained
  id: totrans-13
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 微调过程解析
- en: Fine-tuning a deep learning model involves updating the model's parameters to
    improve its performance on a specific task or dataset.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 微调深度学习模型涉及更新模型的参数，以提高其在特定任务或数据集上的性能。
- en: '![Images](graphics/square.jpg) **Training set:** A collection of labeled examples
    used to train the model. The model learns to recognize patterns and relationships
    in the data by adjusting its parameters based on the training examples.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](graphics/square.jpg) **训练集**：用于训练模型的标记示例集合。模型通过根据训练示例调整其参数来学习数据中的模式和关系。'
- en: '![Images](graphics/square.jpg) **Validation set:** A separate collection of
    labeled examples used to evaluate the model''s performance during training.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](graphics/square.jpg) **验证集**：用于在训练期间评估模型性能的单独标记示例集合。'
- en: '![Images](graphics/square.jpg) **Test set:** A third collection of labeled
    examples that is separate from both the training and validation sets. It is used
    to evaluate the final performance of the model after the training and fine-tuning
    processes are complete. The test set provides a final, unbiased estimate of the
    model''s ability to generalize to new, unseen data.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](graphics/square.jpg) **测试集**：一个与训练集和验证集都分开的标记示例集合。它在训练和微调过程完成后评估模型的最终性能。测试集提供了模型泛化到新、未见数据的能力的最终、无偏估计。'
- en: '![Images](graphics/square.jpg) **Loss function:** The loss function quantifies
    the difference between the model''s predictions and the actual target values.
    It serves as a metric of error to evaluate the model''s performance and guide
    the optimization process. During training, the goal is to minimize the loss function
    to achieve better predictions.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](graphics/square.jpg) **损失函数**：损失函数量化了模型预测值与实际目标值之间的差异。它作为误差的度量标准，用于评估模型性能并指导优化过程。在训练过程中，目标是最小化损失函数以实现更好的预测。'
- en: 'The process of fine-tuning can be broken down into a few steps:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 微调的过程可以分解为几个步骤：
- en: 1\. **Collecting Labeled data:** The first step in fine-tuning is to gather
    our training, validation, and testing datasets of labeled examples relevant to
    the target task or domain. Labeled data serves as a guide for the model to learn
    the task-specific patterns and relationships. For example, if the goal is to fine-tune
    a model for sentiment classification (our first example), the dataset should contain
    text examples along with their respective sentiment labels, such as positive,
    negative, or neutral.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 1. **收集标记数据**：微调的第一步是收集与目标任务或领域相关的训练、验证和测试数据集的标记示例。标记数据作为模型学习特定任务模式和关系的指南。例如，如果目标是微调用于情感分类的模型（我们的第一个示例），则数据集应包含文本示例及其相应的情感标签，如正面、负面或中性。
- en: 2\. **Hyperparameter selection:** Fine-tuning involves adjusting hyperparameters
    that influence the learning process. These include hyperparameters like learning
    rate, batch size, and the number of epochs. The learning rate determines the step
    size of the model's weight updates, while the batch size refers to the number
    of training examples used in a single update. The number of epochs denotes how
    many times the model will iterate over the entire training dataset. Properly setting
    these hyperparameters can significantly impact the model's performance and help
    prevent issues such as overfitting - when a model learns the noise in the training
    data more than the signals - or underfitting - when a model fails to capture the
    underlying structure of the data.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 2. **超参数选择：** 微调涉及调整影响学习过程的学习率、批量大小和训练轮数等超参数。学习率决定了模型权重更新的步长，而批量大小指的是单个更新中使用的训练示例数量。训练轮数表示模型将遍历整个训练数据集的次数。正确设置这些超参数可以显著影响模型的表现，并有助于防止过拟合（当模型学习训练数据中的噪声多于信号时）或欠拟合（当模型未能捕捉到数据的潜在结构时）的问题。
- en: 3\. **Model adaptation:** Once the labeled data and hyperparameters are set,
    the model may have to be adapted to the target task.This involves modifying the
    model's architecture, such as adding custom layers or changing the output structure,
    to better suit the target task. For example, BERT’s architecture cannot perform
    sequence classification as is but they can be modified very slightly to achieve
    the task. In our case study, we will not need to deal with that because OpenAI
    will deal with it for us. We will, however, have to deal with this issue in a
    later chapter.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 3. **模型适应：** 一旦设置了标记数据和超参数，模型可能需要适应目标任务。这包括修改模型的架构，例如添加自定义层或更改输出结构，以更好地适应目标任务。例如，BERT的架构不能直接执行序列分类，但它们可以非常轻微地进行修改以实现该任务。在我们的案例研究中，我们不需要处理这个问题，因为OpenAI会为我们处理。然而，我们将在后面的章节中处理这个问题。
- en: 4\. **Evaluation and iteration:** After the fine-tuning process is over, we
    have to evaluate the model's performance on a separate holdout validation set
    to ensure that it generalizes well to unseen data. Performance metrics such as
    accuracy, F1-score, or Mean Absolute Error (MAE) can be used, depending on the
    task. If the performance is not satisfactory, adjustments to the hyperparameters
    or dataset may be necessary, followed by retraining the model.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 4. **评估和迭代：** 微调过程结束后，我们必须在单独的保留验证集上评估模型的性能，以确保它对未见数据具有良好的泛化能力。根据任务，可以使用性能指标，如准确率、F1分数或平均绝对误差（MAE）。如果性能不满意，可能需要调整超参数或数据集，然后重新训练模型。
- en: 5\. **Model implementation and further training:** Once the model is fine-tuned
    and we are happy with performance, we need to integrate it with existing infrastructures
    in a way that can handle any errors and collect feedback from users so we can
    add to our total dataset to run the process over again in the future
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 5. **模型实现和进一步训练：** 一旦模型经过微调并且我们对性能感到满意，我们需要以能够处理任何错误并从用户那里收集反馈的方式将其与现有基础设施集成，以便我们可以将数据添加到我们的总数据集中，以便未来再次运行该过程。
- en: This process is outlined in [Figure 4.2](ch04.html#ch04fig02).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 此过程在[图4.2](ch04.html#ch04fig02)中概述。
- en: '![Images](graphics/04fig02.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/04fig02.jpg)'
- en: '**Figure 4.2** *The fine-tuning process visualized. A dataset is broken up
    into training, validation, and testing tests. The training set is used to update
    the model’s weights and evaluate while the validation set is used to evaluate
    during training. The final model is then tested against the testing set and evaluated
    against a set of criteria. If the model passes our test, it is used in production
    and monitored for further iterations.*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**图4.2** *微调过程可视化。数据集被分成训练集、验证集和测试集。训练集用于更新模型的权重并评估，而验证集用于训练期间的评估。然后，最终模型与测试集进行测试，并针对一系列标准进行评估。如果模型通过我们的测试，它将用于生产并监控进一步的迭代。*'
- en: This process may require several iterations and careful consideration of hyperparameters,
    data quality, and model architecture to achieve the desired results.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 此过程可能需要多次迭代，并仔细考虑超参数、数据质量和模型架构，以达到预期的结果。
- en: Closed-Source Pre-trained Models as a Foundation
  id: totrans-29
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 作为基础的闭源预训练模型
- en: Pre-trained LLMs play a vital role in transfer learning and fine-tuning, providing
    a foundation of general language understanding and knowledge. This foundation
    allows for efficient adaptation to specific tasks and domains, reducing the need
    for extensive training resources and data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的 LLM 在迁移学习和微调中扮演着至关重要的角色，为通用的语言理解和知识提供了一个基础。这个基础使得对特定任务和领域的适应变得高效，减少了大量训练资源和数据的需求。
- en: This chapter focuses on fine-tuning LLMs using OpenAI's infrastructure, which
    has been specifically designed to facilitate the process. OpenAI has developed
    tools and resources to make it easier for researchers and developers to fine-tune
    smaller models, such as Ada and Babbage, for their specific needs. The infrastructure
    offers a streamlined approach to fine-tuning, allowing users to efficiently adapt
    pre-trained models to a wide variety of tasks and domains.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点介绍使用 OpenAI 基础设施微调 LLM，该基础设施专门设计用于简化微调过程。OpenAI 开发了工具和资源，使研究人员和开发者更容易为他们的特定需求微调较小的模型，如
    Ada 和 Babbage。该基础设施提供了一种简化的微调方法，使用户能够高效地将预训练模型适应到广泛的任务和领域。
- en: Benefits of Using OpenAI's Fine-Tuning Infrastructure
  id: totrans-32
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用 OpenAI 微调基础设施的好处
- en: 'Leveraging OpenAI''s infrastructure for fine-tuning offers several advantages:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 利用 OpenAI 的基础设施进行微调具有几个优势：
- en: '![Images](graphics/square.jpg) Access to powerful pre-trained models, like
    GPT-3, which have been trained on extensive and diverse datasets.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](graphics/square.jpg) 访问经过广泛和多样化数据集训练的强大预训练模型，如 GPT-3。'
- en: '![Images](graphics/square.jpg) A relatively user-friendly interface that simplifies
    the fine-tuning process for people with varying levels of expertise.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](graphics/square.jpg) 一个相对用户友好的界面，简化了不同技术水平的人进行微调的过程。'
- en: '![Images](graphics/square.jpg) A range of tools and resources that help users
    optimize their fine-tuning process, such as guidelines for selecting hyperparameters,
    tips on preparing custom examples, and advice on model evaluation.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](graphics/square.jpg) 一系列工具和资源，帮助用户优化他们的微调过程，例如选择超参数的指南、准备自定义示例的技巧以及模型评估的建议。'
- en: This streamlined process saves time and resources while ensuring the development
    of high-quality models capable of generating accurate and relevant responses in
    a wide array of applications. We will dive deep into open-source fine-tuning and
    the benefits/drawbacks it offers in a later chapter.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简化的流程节省了时间和资源，同时确保开发出高质量的模型，能够在广泛的领域中生成准确和相关的响应。我们将在后面的章节中深入探讨开源微调和它提供的优势/劣势。
- en: A Look at the OpenAI Fine-Tuning API
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 概览 OpenAI 微调 API
- en: The GPT-3 API offers developers access to one of the most advanced LLMs available.
    The API provides a range of fine-tuning capabilities, allowing users to adapt
    the model to specific tasks, languages, and domains. This section will discuss
    the key features of the GPT-3 fine-tuning API, the supported methods, and best
    practices for successfully fine-tuning models.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 API 为开发者提供了访问最先进的 LLM 之一——GPT-3 的权限。该 API 提供了一系列微调功能，使用户能够将模型适应到特定的任务、语言和领域。本节将讨论
    GPT-3 微调 API 的关键特性、支持的方法以及成功微调模型的最佳实践。
- en: The GPT-3 Fine-Tuning API
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: GPT-3 微调 API
- en: The GPT-3 fine-tuning API is like a treasure chest, brimming with powerful features
    that make customizing the model a breeze. From supporting various fine-tuning
    capabilities to offering a range of methods, it's a one-stop-shop for tailoring
    the model to your specific tasks, languages, or domains. This section will unravel
    the secrets of the GPT-3 fine-tuning API, uncovering the tools and techniques
    that make it such an invaluable resource.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 微调 API 就像一个宝库，充满了强大的功能，使得定制模型变得轻而易举。从支持各种微调功能到提供一系列方法，它是一个定制模型到特定任务、语言或领域的全能商店。本节将揭示
    GPT-3 微调 API 的秘密，揭示使其成为如此宝贵资源的工具和技术。
- en: 'Case Study: Amazon Review Sentiment Classification'
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 案例研究：亚马逊评论情感分类
- en: Let’s introduce our first case study. We will be working with the **amazon_reviews_multi**
    dataset (previewed in [Figure 4.3](ch04.html#ch04fig03)). This dataset is a collection
    of product reviews from Amazon, spanning multiple product categories and languages
    (English, Japanese, German, French, Chinese and Spanish). Each review in the dataset
    is accompanied by a rating on a scale of 1 to 5 stars, with 1 being the lowest
    and 5 being the highest. The goal of this case study is to fine-tune a pre-trained
    model from OpenAI to perform sentiment classification on these reviews, enabling
    it to predict the number of stars given to a review. Taking a page out of my own
    book (albeit one just a few pages ago), let’s start with taking a look at the
    data.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们介绍我们的第一个案例研究。我们将使用**amazon_reviews_multi**数据集（如[图4.3](ch04.html#ch04fig03)所示进行预览）。这个数据集是亚马逊产品评论的集合，涵盖了多个产品类别和语言（英语、日语、德语、法语、中文和西班牙语）。数据集中的每个评论都附有一个1到5星的评分，1为最低，5为最高。本案例研究的目的是微调来自OpenAI的预训练模型，以对这些评论进行情感分类，使其能够预测评论获得的星级数。借鉴我自己的经验（尽管只是几页之前），让我们先看看数据。
- en: '![Images](graphics/04fig03.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/04fig03.jpg)'
- en: '**Figure 4.3** *A snippet of the amazon_reviews_multi dataset shows our input
    context (review titles and bodies) and our response - the thing we are trying
    to predict - the number of stars the review was for (1-5).*'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**图4.3** *亚马逊评论多语言数据集的一个片段显示了我们的输入上下文（评论标题和内容）以及我们的响应——我们试图预测的东西——评论的星级数（1-5）。*'
- en: 'The columns we will care about for this round of fine-tuning are:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将关注本次微调的列是：
- en: '![Images](graphics/square.jpg) `review_title`: The text title of the review.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](graphics/square.jpg) `review_title`: 评论的文本标题。'
- en: '![Images](graphics/square.jpg) `review_body`: The text body of the review.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](graphics/square.jpg) `review_body`: 评论的文本内容。'
- en: '![Images](graphics/square.jpg) `stars`: An int between 1-5 indicating the number
    of stars.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](graphics/square.jpg) `stars`: 一个介于1-5之间的整数，表示星级的数量。'
- en: Our goal will be to use the context of the title and body of the review and
    predict the rating that was given.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标将是使用评论标题和内容的上下文来预测给出的评分。
- en: Guidelines and Best Practices for Data
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据的指南和最佳实践
- en: 'In general, there are a few items to consider when selecting data for fine-tuning:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在选择用于微调的数据时，需要考虑以下几点：
- en: '![Images](graphics/square.jpg) **Data quality:** Ensure that the data used
    for fine-tuning is of high quality, free from noise, and accurately represents
    the target domain or task. This will enable the model to learn effectively from
    the given examples.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](graphics/square.jpg) **数据质量：** 确保用于微调的数据质量高，无噪声，并准确代表目标领域或任务。这将使模型能够有效地从给定示例中学习。'
- en: '![Images](graphics/square.jpg) **Data diversity:** Make sure your dataset is
    diverse, covering a broad range of scenarios to help the model generalize well
    across different situations.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](graphics/square.jpg) **数据多样性：** 确保您的数据集具有多样性，涵盖广泛的场景，以帮助模型在不同情况下良好地泛化。'
- en: '![Images](graphics/square.jpg) **Data balancing:** Maintaining a balanced distribution
    of examples across different tasks and domains helps prevent overfitting and biases
    in the model''s performance. This can be achieved from unbalanced datasets by
    undersampling majority classes, oversampling minority classes, or adding synthetic
    data. Our sentiment is perfectly balanced due to the fact that this dataset was
    curated but check out an even harder example in our code base where we attempt
    to classify the very unbalanced category classification task.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](graphics/square.jpg) **数据平衡：** 在不同任务和领域之间保持示例的平衡分布有助于防止模型性能的过拟合和偏差。这可以通过对多数类进行欠采样、对少数类进行过采样或添加合成数据来实现。由于这个数据集是经过精心挑选的，我们的情感数据完全平衡，但请查看我们代码库中的一个更具挑战性的例子，我们在其中尝试对非常不平衡的分类任务进行分类。'
- en: '![Images](graphics/square.jpg) **Data Quantity**: The total amount of data
    used to fine-tune the model. Generally, larger language models like LLMs require
    extensive data to capture and learn various patterns effectively but fewer if
    the LLM was pre-trained on similar enough data. The exact quantity needed can
    vary based on the complexity of the task at hand. Any dataset should not only
    be extensive but also diverse and representative of the problem space to avoid
    potential biases and ensure robust performance across a wide range of inputs.
    While a large quantity of data can help to improve model performance, it also
    increases the computational resources required for model training and fine-tuning.
    This trade-off needs to be considered in the context of the specific project requirements
    and resources.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](graphics/square.jpg) **数据量：** 用于微调模型的总数据量。通常，像 LLM 这样的大型语言模型需要大量数据来有效地捕捉和学习各种模式，但如果
    LLM 在足够相似的数据上进行了预训练，则可能需要较少的数据。所需的确切数量可以根据任务的复杂性而变化。任何数据集不仅应该是广泛的，还应该是多样化的，并且能够代表问题空间，以避免潜在的偏差并确保在广泛的输入上具有稳健的性能。虽然大量数据可以帮助提高模型性能，但它也增加了模型训练和微调所需的计算资源。这种权衡需要在特定项目需求和资源的情况下进行考虑。'
- en: Preparing Custom Examples with the OpenAI CLI
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 OpenAI CLI 准备自定义示例
- en: 'Before diving into fine-tuning, we need to prepare the data by cleaning and
    formatting it according to the API''s requirements. This includes the following:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入微调之前，我们需要根据 API 的要求对数据进行清理和格式化，以便准备数据。这包括以下内容：
- en: '![Images](graphics/square.jpg) **Removing duplicates:** To ensure the highest
    data quality, start by removing any duplicate reviews from the dataset. This will
    prevent the model from overfitting to certain examples and improve its ability
    to generalize to new data.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](graphics/square.jpg) **删除重复项：** 为了确保最高的数据质量，首先从数据集中删除任何重复的评论。这将防止模型对某些示例过度拟合，并提高其对新数据泛化的能力。'
- en: '![Images](graphics/square.jpg) **Splitting the data:** Divide the dataset into
    training, validation, and test sets, maintaining a random distribution of examples
    across each set. If necessary, consider using stratified sampling to ensure that
    each set contains a representative proportion of the different sentiment labels,
    thus preserving the overall distribution of the dataset.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](graphics/square.jpg) **分割数据：** 将数据集分为训练集、验证集和测试集，在每个集中保持示例的随机分布。如果需要，考虑使用分层抽样以确保每个集包含不同情感标签的代表比例，从而保持数据集的整体分布。'
- en: '![Images](graphics/square.jpg) **Shuffle the training data:** shuffling training
    data before fine-tuning helps to avoid biases in the learning process by ensuring
    that the model encounters examples in a random order, reducing the risk of learning
    unintended patterns based on the order of the examples. It also improves model
    generalization by exposing the model to a more diverse range of instances at each
    stage of training which also helps to prevent overfitting, as the model is less
    likely to memorize the training examples and instead focuses on learning the underlying
    patterns. [Figure 4.5](ch04.html#ch04fig05) shows the benefits of shuffling training
    data. Note that data are ideally shuffled before every single epoch to reduce
    the chance of the model over-fitting on the data as much as possible.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](graphics/square.jpg) **打乱训练数据：** 在微调之前打乱训练数据有助于通过确保模型以随机顺序遇到示例来避免学习过程中的偏差。这减少了基于示例顺序学习意外模式的风险。它还通过在训练的每个阶段向模型展示更多样化的实例来提高模型泛化能力，这也有助于防止过拟合，因为模型不太可能记住训练示例，而是专注于学习潜在的模式。[图
    4.5](ch04.html#ch04fig05) 展示了打乱训练数据的益处。请注意，理想情况下，在每个单独的周期之前都应该打乱数据，以尽可能减少模型在数据上过拟合的机会。'
- en: '![Images](graphics/square.jpg) **Creating the OpenAI JSONL format:** OpenAI''s
    API expects the training data to be in JSONL (newline-delimited JSON) format.
    For each example in the training and validation sets, create a JSON object with
    two fields: “prompt” (the input) and “completion” (the target class). The “prompt”
    field should contain the review text, and the “completion” field should store
    the corresponding sentiment label (stars). Save these JSON objects as newline-delimited
    records in separate files for the training and validation sets.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](graphics/square.jpg) **创建 OpenAI JSONL 格式：** OpenAI 的 API 预期训练数据应以 JSONL（换行分隔的
    JSON）格式提供。对于训练集和验证集中的每个示例，创建一个包含两个字段的 JSON 对象：“prompt”（输入）和“completion”（目标类别）。“prompt”字段应包含评论文本，而“completion”字段应存储相应的情感标签（星级）。将这些
    JSON 对象作为换行分隔的记录保存在单独的文件中，分别用于训练集和验证集。'
- en: For completion tokens in our dataset, we should make sure there is a leading
    space before the class label, as this enables the model to understand that it
    should generate a new token. Additionally, when preparing the prompts for the
    fine-tuning process, there's no need to include few-shot examples, as the model
    has already been fine-tuned on the task-specific data. Instead, provide a prompt
    that includes the review text and any necessary context, followed by a suffix
    (e.g., “Sentiment:” with no trailing space or “\n\n###\n\n” like in [Figure 4.4](ch04.html#ch04fig04))
    that indicates the desired output format. [Figure 4.4](ch04.html#ch04fig04) shows
    an example of a single line of our JSONL file.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们数据集中的完成标记，我们应确保类别标签之前有一个前置空格，因为这使模型能够理解它应该生成一个新的标记。此外，在准备微调过程的提示时，无需包含少样本示例，因为模型已经针对特定任务数据进行了微调。相反，提供一个包含评论文本和任何必要上下文的提示，后跟一个后缀（例如，“情感：”后面没有空格或“\n\n###\n\n”如[图
    4.4](ch04.html#ch04fig04)所示），以指示所需的输出格式。[图 4.4](ch04.html#ch04fig04)展示了我们 JSONL
    文件中的一行示例。
- en: '![Images](graphics/04fig04.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/04fig04.jpg)'
- en: '**Figure 4.4** *A single JSONL example for our training data that we will feed
    to OpenAI. Every JSON has a prompt key - denoting the input to the model sans
    any few-shot, instructions, etc and a completion key - denoting what we want the
    model to output, which in our case is a single classification token. In this example,
    the user is rating the product with 1 star.*'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 4.4** *我们将要提供给 OpenAI 的训练数据的一个 JSONL 示例。每个 JSON 都有一个提示键 - 表示模型输入（无任何少样本、指令等）和一个完成键
    - 表示我们希望模型输出的内容，在我们的案例中是一个单一的分类标记。在这个例子中，用户对产品给出了 1 星的评价。*'
- en: I should note that for our input data, I have concatenated the title and the
    body of the review as the singular input. This was a personal choice and I made
    it because I believe that the title can have more direct language to indicate
    general sentiment while the body likely has more nuanced to pinpoint the exact
    number of stars they are going to give. Feel free to explore different ways of
    combining text fields together! We are going to explore this further in later
    case studies along with other ways of formatting fields for a single text input.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我应该指出，对于我们的输入数据，我已经将标题和评论正文连接起来作为单一输入。这是一个个人选择，我这样做是因为我相信标题可以具有更直接的语言来表明总体情感，而正文可能具有更细微的语言来精确指出他们将给出的星级数。请随意探索不同的文本字段组合方式！我们将在后续的案例研究中进一步探讨这一点，以及其他为单个文本输入格式化字段的方法。
- en: '![Images](graphics/04fig05.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/04fig05.jpg)'
- en: '**Figure 4.5** *Unshuffled data makes for bad training data! It gives the model
    room to overfit on specific batches of data and overall lowers the quality of
    the responses. The top three graphs represent a model trained on unshuffled training
    data and the accuracy is horrible compared to a model trained on shuffled data,
    seen in the bottom three graphs.*'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 4.5** *未打乱的数据不适合作为训练数据！它给模型提供了在特定数据批次上过拟合的空间，并且总体上降低了响应的质量。顶部三个图表表示在未打乱的训练数据上训练的模型，与底部三个图表中在打乱数据上训练的模型相比，准确率非常糟糕。*'
- en: The following listing ([Listing 4.1](ch04.html#list4_1)) loads the Amazon Reviews
    dataset and converts the 'train' subset into a pandas DataFrame. Then, it preprocesses
    the DataFrame using the custom `prepare_df_for_openai` function, which combines
    the review title and review body into a prompt, creates a new completion column,
    and filters the DataFrame to only include English reviews. Finally, it removes
    duplicate rows based on the 'prompt' column and returns a DataFrame with only
    the 'prompt' and 'completion' columns.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表 ([列表 4.1](ch04.html#list4_1)) 加载了亚马逊评论数据集，并将 'train' 子集转换为 pandas DataFrame。然后，使用自定义的
    `prepare_df_for_openai` 函数对 DataFrame 进行预处理，该函数将评论标题和评论正文合并为一个提示，创建一个新的完成列，并过滤
    DataFrame 以仅包含英文评论。最后，根据 'prompt' 列删除重复行，并返回仅包含 'prompt' 和 'completion' 列的 DataFrame。
- en: '**Listing 4.1** Generating a JSONL file for our sentiment training data'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**列表 4.1** 为我们的情感训练数据生成 JSONL 文件'
- en: '[PRE0]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We would do a similar process with the `validation` subset of the dataset and
    the holdout `test` subset for a final test of the fine-tuned model. A quick note
    that we are filtering for English only in this case but you are free to train
    on more languages mixed in. I simply wanted to get some quicker results at an
    efficient price.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对数据集的 'validation' 子集和用于最终模型测试的保留 'test' 子集执行类似的过程。快速说明一下，在这种情况下我们仅过滤英文，但您可以在更多语言混合中进行训练。我只是想以高效的价格快速得到一些结果。
- en: Setting Up the OpenAI CLI
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 设置 OpenAI CLI
- en: The OpenAI Command Line Interface (CLI) simplifies the process of fine-tuning
    and interacting with the API. The CLI allows you to submit fine-tuning requests,
    monitor training progress, and manage your models, all from your command line.
    Ensure that you have the OpenAI CLI installed and configured with your API key
    before proceeding with the fine-tuning process.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 命令行界面 (CLI) 简化了微调和与 API 交互的过程。CLI 允许您从命令行提交微调请求、监控训练进度以及管理您的模型。在开始微调过程之前，请确保您已安装并配置了
    OpenAI CLI 以及您的 API 密钥。
- en: 'To install the OpenAI CLI, you can use pip, the Python package manager. First,
    make sure you have Python 3.6 or later installed on your system. Then, follow
    these steps:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装 OpenAI CLI，您可以使用 pip，Python 软件包管理器。首先，请确保您的系统上已安装 Python 3.6 或更高版本。然后，按照以下步骤操作：
- en: 1\. Open a terminal (on macOS or Linux) or a command prompt (on Windows).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 打开终端（在 macOS 或 Linux 上）或命令提示符（在 Windows 上）。
- en: '2\. Run the following command to install the openai package: `**pip install
    openai**`'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 运行以下命令安装 openai 包：`**pip install openai**`
- en: a. This command installs the OpenAI Python package, which includes the CLI.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: a. 此命令安装了 OpenAI Python 包，其中包含 CLI。
- en: '3\. To verify that the installation was successful, run the following command:
    `**openai --version**`'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 要验证安装是否成功，请运行以下命令：`**openai --version**`
- en: a. This command should display the version number of the installed OpenAI CLI.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: a. 此命令应显示已安装的 OpenAI CLI 的版本号。
- en: Before you can use the OpenAI CLI, you need to configure it with your API key.
    To do this, set the OPENAI_API_KEY environment variable to your API key value.
    You can find your API key in your OpenAI account dashboard.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在您可以使用 OpenAI CLI 之前，您需要使用您的 API 密钥对其进行配置。为此，将 OPENAI_API_KEY 环境变量设置为您的 API
    密钥值。您可以在 OpenAI 账户仪表板中找到您的 API 密钥。
- en: Hyperparameter Selection and Optimization
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 超参数选择和优化
- en: 'With our JSONL document created and OpenAI CLI installed, we are ready to select
    our hyperparameters! Here''s a list of key hyperparameters and their definitions:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建 JSONL 文档并安装 OpenAI CLI 之后，我们准备好选择超参数了！以下是一些关键超参数及其定义：
- en: '![Images](graphics/square.jpg) **Learning rate**: The learning rate determines
    the size of the steps the model takes during optimization. A smaller learning
    rate leads to slower convergence but potentially better accuracy, while a larger
    learning rate speeds up training but may cause the model to overshoot the optimal
    solution.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](graphics/square.jpg) **学习率**：学习率决定了模型在优化过程中所采取的步长大小。较小的学习率会导致收敛速度较慢，但可能获得更好的准确性，而较大的学习率会加快训练速度，但可能导致模型超出最佳解。'
- en: '![Images](graphics/square.jpg) **Batch size**: Batch size refers to the number
    of training examples used in a single iteration of model updates. A larger batch
    size can lead to more stable gradients and faster training, while a smaller batch
    size may result in a more accurate model but slower convergence.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](graphics/square.jpg) **批大小**：批大小指的是在模型更新单次迭代中使用的训练示例数量。较大的批大小可能导致梯度更稳定且训练速度更快，而较小的批大小可能导致模型更准确但收敛速度较慢。'
- en: '![Images](graphics/square.jpg) **Training epochs**: An epoch is a complete
    pass through the entire training dataset. The number of training epochs determines
    how many times the model will iterate over the data, allowing it to learn and
    refine its parameters.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](graphics/square.jpg) **训练轮数**：一个轮次是指完整地遍历整个训练数据集。训练轮数的数量决定了模型将迭代数据的次数，从而使其能够学习和优化其参数。'
- en: OpenAI has done a lot of work to find optimal settings for most cases, so we
    will lean on their recommendations for our first attempt. The only thing we will
    change is to train for 1 epoch instead of the default 4\. We're doing this because
    we want to see how the performance looks before investing too much time and money.
    Experimenting with different values and using techniques like grid search will
    help you find the optimal hyperparameter settings for your task and dataset, but
    be mindful that this process can be time-consuming and costly.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 已经为大多数情况找到了最优设置，因此我们将在我们的第一次尝试中依赖他们的建议。我们唯一要改变的是将训练轮数改为1轮，而不是默认的4轮。我们这样做是因为我们想在投入太多时间和金钱之前看看性能表现如何。尝试不同的值和使用如网格搜索等技术将帮助您找到适合您任务和数据集的最佳超参数设置，但请注意，这个过程可能耗时且成本高昂。
- en: Our First Fine-Tuned LLM!
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们的第一款微调大型语言模型！
- en: Let’s kick off our first fine-tuning! [Listing 4.2](ch04.html#list4_2) makes
    a call to OpenAI to train an ada model (fastest, cheapest, weakest) for 1 epoch
    on our training and validation data.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始我们的第一次微调！[列表4.2](ch04.html#list4_2)调用OpenAI在训练和验证数据上训练一个ada模型（最快、最便宜、最弱）进行1轮训练。
- en: '**Listing 4.2** making our first fine-tuning call'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**列表4.2** 进行第一次微调调用'
- en: '[PRE1]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Evaluating Fine-Tuned Models with Quantitative Metrics
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用定量指标评估微调模型
- en: Measuring the performance of fine-tuned models is essential for understanding
    their effectiveness and identifying areas for improvement. Utilizing metrics and
    benchmarks, such as accuracy, F1 score, or perplexity, will provide quantitative
    measures of the model's performance. In addition to quantitative metrics, qualitative
    evaluation techniques, such as human evaluation or analyzing example outputs,
    can offer valuable insights into the model's strengths and weaknesses, helping
    identify areas for further fine-tuning.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 测量微调模型的表现对于理解其有效性和确定改进领域至关重要。利用准确率、F1分数或困惑度等指标和基准，将为模型的表现提供定量度量。除了定量指标外，定性评估技术，如人工评估或分析示例输出，可以提供有关模型优势和劣势的宝贵见解，帮助确定进一步微调的领域。
- en: After one epoch (further metrics shown in [Figure 4.6](ch04.html#ch04fig06)),
    our classifier is getting above 63% accuracy on the holdout testing dataset! Remember
    the testing subset was not given to OpenAI but rather we held it out for final
    model comparisons.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 经过一轮训练（进一步指标见[图4.6](ch04.html#ch04fig06)），我们的分类器在保留测试数据集上的准确率已经超过了63%！请记住，测试子集并没有给OpenAI，而是我们保留它以进行最终模型比较。
- en: '![Images](graphics/04fig06.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/04fig06.jpg)'
- en: '**Figure 4.6** *Our model is performing pretty well after only one epoch on
    de-duplicated shuffled training data*'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**图4.6** *我们的模型在去重后的随机训练数据上仅经过一轮训练后表现相当不错*'
- en: '63% accuracy might sound low to you but hear me out: predicting the **exact**
    number of stars is tricky because people aren’t always consistent in what they
    write and how they finally review the product so I’ll offer two more metrics:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 63%的准确率可能听起来很低，但请听我说：预测**确切**的星星数量是困难的，因为人们并不总是对他们的写作和最终的产品评价保持一致，所以我将提供两个额外的指标：
- en: '![Images](graphics/square.jpg) Relaxing our accuracy calculation to be binary
    (did the model predict <= 3 stars and was the review <=3 stars) is **92%** so
    the model can tell between “good” and “bad”'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](graphics/square.jpg) 将我们的准确率计算放宽为二进制（模型是否预测<=3颗星星，且评论是否<=3颗星星）准确率为**92**%，因此模型可以区分“好”和“坏”'
- en: '![Images](graphics/square.jpg) Relaxing the calculation to be “one-off” so
    for the example the model predicting 2 would count as correct if the actual rating
    was 1, 2, or 3 is **93%**'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](graphics/square.jpg) 将计算放宽为“一次性”的，例如，对于示例，如果实际评分是1、2或3，则预测2的模型将被视为正确，准确率为**93**%'
- en: So you know what? Not bad. Our classifier is definitely learning the difference
    between good and bad so the next logical thought might be, “let’s keep the training
    going”! We only trained for a single epoch so more epochs must be better, right?
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你知道吗？还不错。我们的分类器肯定学会了区分好与坏，所以下一个合乎逻辑的想法可能是，“让我们继续训练”！我们只训练了一轮，所以更多的轮数肯定更好，对吧？
- en: This process of taking smaller steps in training and updating already fine-tuned
    models for more training steps/epochs potentially with new labeled datapoints
    is called **incremental learning** aka continuous learning or online learning.
    Incremental learning often results in more controlled learning, which can be ideal
    when working with smaller datasets or when you want to preserve some of the model's
    general knowledge. Let’s try some incremental learning! Let’s take our already
    fine-tuned ada model and let it run for 3 more epochs on the same data and see
    the results in [Figure 4.7](ch04.html#ch04fig07).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这种在训练中采取更小步骤并更新已经微调的模型以进行更多训练步骤/epochs的过程被称为**增量学习**，也称为持续学习或在线学习。增量学习通常会导致更受控的学习，这在处理较小数据集或希望保留模型的一些通用知识时可能很理想。让我们尝试一些增量学习！让我们用已经微调的
    ada 模型，让它在这相同的数据上再运行3个epochs，并查看[图 4.7](ch04.html#ch04fig07)中的结果。
- en: '![Images](graphics/04fig07.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/04fig07.jpg)'
- en: '**Figure 4.7**: The model’s performance seems to barely move during a further
    3 epochs of incremental learning after a successful single epoch. 4x the cost
    for 1.02x the performance? No thank you.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 4.7**：在成功完成一个epoch后，模型在进一步3个epochs的增量学习中的表现似乎几乎没有变化。成本增加4倍，性能只提高了1.02倍？不，谢谢。'
- en: 'Uh oh, more epochs didn’t seem to really do anything, but nothing is set in
    stone until we test on our holdout test data subset and compare it to our first
    model. [Table 4.1](ch04.html#ch04tab01) shows our results:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 哎呀，更多的epochs似乎并没有真正起到什么作用，但直到我们在我们的保留测试数据子集上进行测试并将结果与我们的第一个模型进行比较，一切都没有定论。[表
    4.1](ch04.html#ch04tab01)显示了我们的结果：
- en: '**Table 4.1** *Results*'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 4.1** *结果*'
- en: '![Images](graphics/04tab01.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/04tab01.jpg)'
- en: So for 4x the price, we get a single % point increase in accuracy? Not worth
    it in my book but maybe it is for you! Some industries demand near perfection
    in their models and single percentage points matter. I’ll leave that up to you
    but in general more epochs will not always lead to better results. Incremental/online
    learning can help you find the right stopping point at the cost of more up-front
    effort but will be well worth it in the long run.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，以4倍的价格，我们只得到了一个百分点的准确率提升？在我的书中，这并不值得，但也许对你来说值得！一些行业对他们的模型要求近乎完美，单个百分点很重要。我将那留给你们，但一般来说，更多的epochs并不总是导致更好的结果。增量/在线学习可以帮助你找到正确的停止点，但需要更多的前期努力，但从长远来看将是非常值得的。
- en: Qualitative Evaluation Techniques
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 定性评估技术
- en: Alongside quantitative metrics, qualitative evaluation techniques offer valuable
    insights into the strengths and weaknesses of our fine-tuned model. Examining
    generated outputs or employing human evaluators can help identify areas where
    the model excels or falls short, guiding our future fine-tuning efforts.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 除了定量指标外，定性评估技术为我们微调模型的优点和缺点提供了宝贵的见解。检查生成的输出或使用人工评估者可以帮助确定模型表现优异或不足的领域，指导我们未来的微调工作。
- en: To help, we can get the probability for our classification by looking at the
    probabilities of predicting the first token in either the Playground (as seen
    in [Figure 4.8](ch04.html#ch04fig08)) or via the API’s `logprobs` value (as seen
    in [Listing 4.3](ch04.html#list4_3))
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助，我们可以通过查看在Playground（如图[图 4.8](ch04.html#ch04fig08)所示）或通过API的`logprobs`值（如图[列表
    4.3](ch04.html#list4_3)所示）预测第一个标记的概率来获取我们的分类概率。
- en: '![Images](graphics/04fig08.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/04fig08.jpg)'
- en: '**Figure 4.8** *The playground and the API for GPT-3 like models (including
    our fine-tuned ada model as seen in this figure) offer token probabilities that
    we can use to check the model’s confidence on a particular classification. Note
    that the main option is “ 1“ with a leading space just like we have in our training
    data but one of the tokens on the top of the list is “1” with no leading space.
    These are two separate tokens according to many LLMs which is why I am calling
    it out so often. It can be easy to forget and mix them up.*'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 4.8** *GPT-3 类模型（包括如图所示的我们微调的 ada 模型）提供的标记概率，我们可以使用这些概率来检查模型对特定分类的置信度。请注意，主要选项是“
    1”，前面有一个空格，就像我们在训练数据中有的那样，但列表顶部的其中一个标记是“1”，没有前导空格。根据许多大型语言模型，这些是两个不同的标记，这就是我经常强调它的原因。很容易忘记并将它们混淆。*'
- en: '**Listing 4.3** getting token probabilities from the OpenAI API'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**列表 4.3** 从 OpenAI API 获取标记概率'
- en: '[PRE2]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Output**:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出**:'
- en: '[PRE3]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Between quantitative and qualitative measures, let’s assume we believe our model
    is ready to go into production – if not at least a dev or staging environment
    for further testing, let’s take a minute to talk about how we can incorporate
    our new model into our applications.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在定量和定性指标之间，假设我们相信我们的模型已经准备好投入生产——如果不是至少一个用于进一步测试的开发或预发布环境，那么让我们花一分钟时间讨论如何将我们的新模型集成到我们的应用程序中。
- en: Integrating Fine-Tuned GPT-3 Models into Applications
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将微调后的 GPT-3 模型集成到应用程序中
- en: 'Integrating a fine-tuned GPT-3 model into your application is identical to
    using a base model provided by OpenAI. The primary difference is that you''ll
    need to reference your fine-tuned model''s unique identifier when making API calls.
    Here are the key steps to follow:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 将微调后的 GPT-3 模型集成到您的应用程序中与使用 OpenAI 提供的基础模型相同。主要区别在于，您在调用 API 时需要引用您的微调模型的唯一标识符。以下是需要遵循的关键步骤：
- en: '1\. **Identify your fine-tuned model**: After completing the fine-tuning process,
    you will receive a unique identifier for your fine-tuned model like `‘ada:ft-personal-2023-03-31-05-30-46’`.
    Make sure to note this identifier, as it will be required for API calls.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. **识别您的微调模型**：完成微调过程后，您将收到一个唯一的标识符，例如 `‘ada:ft-personal-2023-03-31-05-30-46’`。请确保记下此标识符，因为它将用于
    API 调用。
- en: '2\. **Use the OpenAI API Normally**: Use yourOpenAI API to make requests to
    your fine-tuned model. When making requests, replace the base model''s name with
    your fine-tuned model''s unique identifier. [Listing 4.3](ch04.html#list4_3) offers
    an example of doing this.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. **正常使用 OpenAI API**：使用您的 OpenAI API 向您的微调模型发送请求。在发送请求时，将基础模型名称替换为您的微调模型的唯一标识符。[列表
    4.3](ch04.html#list4_3) 提供了如何进行此操作的示例。
- en: '3\. **Adapt any application logic**: Since fine-tuned models may require different
    prompt structures or generate different output formats, you may need to update
    your application''s logic to handle these variations. For example in our prompts,
    we concatenated the review title with the body and added a custom suffix “\n\n###\n\n”.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. **调整任何应用程序逻辑**：由于微调模型可能需要不同的提示结构或生成不同的输出格式，您可能需要更新应用程序的逻辑以处理这些变化。例如，在我们的提示中，我们将评论标题与正文连接起来，并添加了一个自定义后缀
    “\n\n###\n\n”。
- en: '4\. **Monitor and evaluate performance**: Continuously monitor your fine-tuned
    model''s performance and collect user feedback. You may need to iteratively fine-tune
    your model with even more data to improve its accuracy and effectiveness.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. **监控和评估性能**：持续监控您的微调模型性能并收集用户反馈。您可能需要使用更多数据迭代微调模型以提高其准确性和有效性。
- en: 'Case Study 2: Amazon Review Category Classification'
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 案例研究 2：亚马逊评论类别分类
- en: With a successfully fine-tuned ada model for a relatively simple example like
    sentiment classification, let's up the stakes and tackle a more challenging task.
    In a second case study, we will explore how fine-tuning a GPT-3 model can improve
    its performance on the task of Amazon review category classification from the
    same dataset. This task involves classifying Amazon product reviews into their
    respective product categories based on the review title and body - just like we
    did for sentiment. We no longer have 5 classes for example, we now have 31 unbalanced
    classes (see [Figure 4.9](ch04.html#ch04fig09))!
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个相对简单的例子，如情感分类，我们成功微调了 ada 模型，现在让我们提高难度，处理一个更具挑战性的任务。在第二个案例研究中，我们将探讨如何通过微调
    GPT-3 模型来提高其在亚马逊评论类别分类任务上的性能，该任务基于相同的数据集。这项任务涉及根据评论标题和正文将亚马逊产品评论分类到相应的产品类别——就像我们处理情感分类一样。例如，我们不再有
    5 个类别，现在我们有 31 个不平衡的类别（见[图 4.9](ch04.html#ch04fig09)）！
- en: '![Images](graphics/04fig09.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/04fig09.jpg)'
- en: '**Figure 4.9** *The category classification task has 31 unique categories to
    choose from and a very unbalanced class distribution which is a perfect storm
    for a difficult classification task*'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 4.9** *类别分类任务有 31 个独特的类别可供选择，并且类别分布非常不平衡，这对于一个困难的分类任务来说是一个完美的风暴*'
- en: The much harder category classification task reveals a lot of hidden difficulties
    of ML, such as dealing with unbalanced data and **ill-defined data**—where the
    distinction between categories is subtle or ambiguous. In these cases, the model
    may struggle to discern the correct category. To improve performance, consider
    refining the problem definition, deleting redundant or confusing training examples,
    merging similar categories, or providing additional context to the model through
    prompts.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 非常困难的类别分类任务揭示了机器学习中的许多隐藏困难，例如处理不平衡数据以及**定义不明确的数据**——在这些情况下，类别之间的区别可能是微妙的或模糊的。在这些情况下，模型可能难以辨别正确的类别。为了提高性能，考虑细化问题定义，删除冗余或混淆的训练示例，合并相似类别，或通过提示为模型提供额外的上下文。
- en: Check out all of that work in our code repository!
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 查看我们代码仓库中的所有工作！
- en: Summary
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Fine-tuning LLMs like GPT-3 is an effective way to enhance their performance
    on specific tasks or domains. By integrating a fine-tuned model into your application
    and following best practices for deployment, you can create a more efficient,
    accurate, and cost-effective language processing solution. Continuously monitor
    and evaluate your model's performance, and iterate on its fine-tuning to ensure
    it meets the evolving needs of your application and users.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 微调LLM（如GPT-3）是提高其在特定任务或领域性能的有效方法。通过将微调模型集成到您的应用程序中并遵循最佳部署实践，您可以创建一个更高效、准确且成本效益更高的语言处理解决方案。持续监控和评估您的模型性能，并迭代其微调以确保它满足应用程序和用户不断变化的需求。
- en: We will revisit the idea of fine-tuning in later chapters with some more complicated
    examples while also exploring the fine-tuning strategies for open-source models
    for even further cost reductions.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在后面的章节中，通过一些更复杂的例子重新审视微调的概念，同时探索开源模型微调策略，以进一步降低成本。
