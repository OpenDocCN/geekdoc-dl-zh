- en: 16  Making models generalize
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 16  使模型泛化
- en: 原文：[https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/overfitting.html](https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/overfitting.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/overfitting.html](https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/overfitting.html)'
- en: In deep learning, as in machine learning in general, we face a problem of generalization.
    The training set is limited, after all. There is no guarantee that what a model
    has learned generalizes to the “outside world”.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习，就像在机器学习一般，我们面临一个泛化问题。毕竟，训练集是有限的。没有保证模型所学的知识可以推广到“外部世界”。
- en: How big of a problem is this? It depends. When we construct a toy problem ourselves,
    we can actively make sure that training and test set come from the very same distribution.
    On the other end of the spectrum are configurations where significant structural
    disparities are known to exist from the outset. In the latter case, the measures
    described in this chapter will be useful, but not sufficient to resolve the problem.
    Instead, expert domain knowledge and appropriate workflow (re-)design will be
    required.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题有多大？这取决于。当我们自己构建一个玩具问题时，我们可以积极确保训练集和测试集来自完全相同的分布。在光谱的另一端是那些从一开始就知道存在显著结构差异的配置。在后一种情况下，本章中描述的措施将是有用的，但不足以解决问题。相反，需要专家领域知识和适当的流程（重新）设计。
- en: 'Between both extremes, there is a wide range of possibilities. For example:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在两个极端之间，存在广泛的可能。例如：
- en: An image dataset has had all pictures taken using excellent photographic equipment,
    and as viewed from a 180 degrees angle; but prediction will take place “in the
    wild”, using cell phones.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个图像数据集使用了优秀的摄影设备拍摄的所有图片，并且从180度的角度进行观察；但是预测将在“野外”进行，使用手机。
- en: A application designed to diagnose skin cancer has been trained on white males
    mostly, but is supposed to be used by people of any gender and/or skin color.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于诊断皮肤癌的应用程序主要是在白人男性身上训练的，但应该由任何性别和/或肤色的人使用。
- en: The second example should make clear that lack of generalization (a.k.a. *overfitting*[¹](#fn1))
    – is anything but an exclusively technical problem. Insufficient representation,
    or even *lack* of representation, is one of the major manifestations of *dataset
    bias*. Among the measures intended to prevent overfitting we discuss below, just
    one – the most important of all – applies to the second scenario. (This book is
    dedicated to technical topics; but as emphasized in the preface, we all live in
    a “real world” that is not just extremely complex, but also, fraught with inequality,
    inequity, and injustice. However strong our technical focus, this is something
    we should never lose awareness of.)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个例子应该清楚地表明，缺乏泛化（即*过拟合*¹](#fn1)）——绝对不是一个纯粹的技术问题。代表性不足，甚至*缺乏*代表性，是*数据集偏差*的主要表现之一。在我们下面讨论的防止过拟合的措施中，只有一项——最重要的——适用于第二种场景。（这本书是关于技术主题的；但如前言所强调的，我们生活在一个“现实世界”中，这个世界不仅极其复杂，而且充满了不平等、不公正和不正义。无论我们的技术焦点有多强，这都是我们永远不应该忽视的事情。）
- en: 'The counter-measures I’ll be presenting can be ordered by stage in the machine
    learning workflow they apply to: data collection, data pre-processing, model definition,
    and model training. As you’ll be expecting by now, the most important measure,
    as hinted at above, is to collect more representative data.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我将要介绍的对策可以根据它们在机器学习工作流程中应用的阶段进行排序：数据收集、数据预处理、模型定义和模型训练。正如你现在所期望的，最重要的措施，如上所述，是收集更多具有代表性的数据。
- en: '16.1 The royal road: more – and more representative! – data'
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.1 通往王者的道路：更多——更多代表性！——的数据
- en: Depending on your situation, you may be forced to work with pre-existing datasets.
    In that case, you may still have the option of supplementing external sources
    with data you’ve collected yourself, albeit of lesser quantity.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的情况，你可能被迫使用现有的数据集。在这种情况下，你仍然可以选择用你自己收集的数据补充外部来源，尽管数量较少。
- en: 'In the next chapter, we’ll encounter the technique of *transfer learning*.
    Transfer learning means making use of *pre-trained* models, employing them as
    feature extractors for your “downstream” task. Often, these models have been trained
    on huge datasets. In consequence, they are not just “very good at what they do”,
    but they also generalize to unseen data – *provided* the new data are similar
    to those they have been trained on. What if they’re not? For many tasks, it will
    still be possible to make use of a pre-trained model: Take it the way it comes,
    and go on training it – but now, adding in the kinds of data you want it to generalize
    to.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将遇到迁移学习的技术。迁移学习意味着利用**预训练**的模型，将它们用作你“下游”任务的特性提取器。通常，这些模型已经在大型数据集上进行了训练。因此，它们不仅“非常擅长它们所做的事情”，而且它们还能泛化到未见过的数据——**前提**是新数据与它们训练过的数据相似。如果它们不相似怎么办？对于许多任务，仍然可以使用预训练模型：按照原样使用它，并继续训练它——但现在，添加你希望它泛化的数据类型。
- en: Of course, being able to add in *any* data of your own may still be a dream.
    In that case, all you can do is think hard about how your results will be biased,
    and be honest about it.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，能够添加任何你自己的数据可能仍然是一个梦想。在这种情况下，你所能做的就是深思熟虑你的结果将如何产生偏差，并且对此保持诚实。
- en: Now, let’s say you’ve thought it through, and are confident that there are no
    systematic deficiencies in your training data preventing generalization to use
    cases in the wild. Or maybe you’ve restricted the application domain of your model
    as required. Then, if you have a small training set and want it to generalize
    as much as possible, what can you do?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设你已经深思熟虑，并且自信你的训练数据中没有系统性缺陷，这会阻止它们泛化到现实世界的用例。或者，也许你已经根据需要限制了你的模型的应用领域。那么，如果你有一个小的训练集并且希望它尽可能泛化，你能做什么呢？
- en: '16.2 Pre-processing stage: Data augmentation'
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.2 预处理阶段：数据增强
- en: Data augmentation means taking the data you have and modifying them, so as to
    force the algorithm to abstract over some things. What things? It depends on the
    domain operated upon. This should become clear by means of a concrete example.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强意味着取你已有的数据并对其进行修改，以便迫使算法抽象出某些事物。是什么事物？这取决于操作的领域。这应该通过一个具体的例子变得清晰。
- en: 'In this chapter, I’ll introduce two popular variants of data augmentation:
    one I’ll refer to as “classic”, the other one going by the name of “mixup”.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我将介绍两种流行的数据增强变体：一个我将称之为“经典”，另一个被称为“mixup”。
- en: 16.2.1 Classic data augmentation
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.2.1 经典数据增强
- en: Classically, when people talk about (image) data augmentation, it is the following
    they’re having in mind. You take an image, and apply some random transformation
    to it. That transformation could be geometric, such as when rotating, translating,
    or scaling the image. Alternatively, instead of moving things around, the operation
    could affect the colors, as when changing brightness or saturation. Other options
    include blurring the image, or, quite the contrary, sharpening it. Technically,
    you are free to implement whatever algorithm you desire – you don’t have to use
    any of the (numerous!) transformations provided by `torchvision`. In practice,
    though, you’ll likely find much of what you need among the transformations already
    available.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，当人们谈论（图像）数据增强时，他们心中所想的是以下内容。你取一张图片，并对其应用一些随机变换。这种变换可以是几何变换，例如旋转、平移或缩放图片。或者，不是移动物体，操作可以影响颜色，例如改变亮度或饱和度。其他选项包括模糊图像，或者相反，锐化图像。技术上，你可以自由实现你想要的任何算法——你不必使用`torchvision`提供的（众多！）任何变换。然而，在实践中，你可能会发现你需要的许多变换已经可用。
- en: In our running example, we’ll work with MNIST, the *Hello World* of image-classification
    datasets we’ve already made quick use of before. It contains 70,000 images of
    the digits `0` to `9`, split into training and test sets in a ratio of six to
    one. Like before, we get the data from `torchvision`.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的运行示例中，我们将使用MNIST，这是我们之前已经快速使用过的图像分类数据集的“Hello World”。它包含70,000个数字`0`到`9`的图像，以六比一的比例分为训练集和测试集。像以前一样，我们从`torchvision`获取数据。
- en: 'To see how the digits appear without any data augmentation, take a look at
    the first thirty-two images in the test set ([fig. 16.1](#fig-overfitting-mnist-test)):'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看没有数据增强的数字是如何出现的，请查看测试集中的前32个图像（[图16.1](#fig-overfitting-mnist-test)）：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*![Thirty-two digits, standing upright.](../Images/79ad6250a77b4c2a00a0d16af75da4eb.png)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*![三十二位数字，直立站立。（图片路径：../Images/79ad6250a77b4c2a00a0d16af75da4eb.png）*'
- en: 'Figure 16.1: MNIST: The first thirty-two images in the test set.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.1：MNIST：测试集中的前32个图像。
- en: 'Now, we use the training set to experiment with data augmentation. Like the
    `dogs_vs_cats_dataset()` we made use of in the last chapter – in fact, like all
    `torchvision` datasets – `mnist_dataset()` takes a `transform` argument, allowing
    you to pass in arbitrary transformations to be performed on the input images.
    Of the four transformations that appear in the code snippet below, one we have
    already seen: `transform_to_tensor()`, used to convert from R `double`s to `torch`
    tensors. The other three, that all share the infix `_random_`, each trigger non-deterministic
    data augmentation: be it by flipping the image horizontally (`transform_random_horizontal_flip()`)
    or vertically (`transform_random_vertical_flip()`), or through rotations and translations(`transform_random_affine()`).
    In all cases, the amount of distortion is configurable.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们使用训练集来实验数据增强。就像我们在上一章中使用的`dogs_vs_cats_dataset()`一样——实际上，就像所有`torchvision`数据集一样——`mnist_dataset()`接受一个`transform`参数，允许你传入任意变换来对输入图像进行操作。在下面的代码片段中出现的四种变换中，我们已经见过一种：`transform_to_tensor()`，用于将R
    `double`s转换为`torch`张量。其他三种，都带有后缀`_random_`，都会触发非确定性的数据增强：无论是通过水平翻转图像(`transform_random_horizontal_flip()`)或垂直翻转(`transform_random_vertical_flip()`)，还是通过旋转和平移(`transform_random_affine()`)。在所有情况下，扭曲的程度都是可配置的。
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*Again, let’s look at a sample result ([fig. 16.2](#fig-overfitting-mnist-transforms)).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*再次，让我们看看一个样本结果([图16.2](#fig-overfitting-mnist-transforms))。'
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '*![Thirty-two digits, standing upright, rotated, translated, and/or flipped
    horizontally.](../Images/2a62dbce6a0744a2b3bfe19a8e0c826e.png)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*![三十二位数字，直立、旋转、平移、水平翻转等。](../Images/2a62dbce6a0744a2b3bfe19a8e0c826e.png)'
- en: 'Figure 16.2: MNIST, with random rotations, translations, and flips.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.2：MNIST，带有随机旋转、平移和翻转。
- en: The effects are clearly visible, and the ranges chosen for rotations and translations
    seem sensible. But let’s think about the flips. Does it actually make *sense*
    to include them?
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这些效果非常明显，旋转和平移选择的范围看起来也很有道理。但是，让我们思考一下翻转。包含翻转是否真的*有意义*？
- en: 'In general, this would depend on the dataset, and more even, on the task. Think
    of a cat, comfortably residing on a fluffy sofa. If the cat were looking to the
    right instead of to the left, it would still be a cat; if it was positioned upside-down,
    we’d probably assume the image had been loaded incorrectly. Neither transformation
    would affect its “catness”. It’s different with digits, though. A flipped \(1\)
    is not the same as a \(1\), at least not in a default context. Thus, for MNIST,
    I’d rather go with rotations and translations only:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这取决于数据集，甚至更多，取决于任务。想象一下一只猫，舒适地坐在蓬松的沙发上。如果猫朝右边看而不是朝左边看，它仍然是一只猫；如果它倒置了，我们可能会认为图像加载错误了。这两种变换都不会影响它的“猫性”。然而，对于数字来说就不同了。翻转的\(1\)和\(1\)在默认情况下并不相同。因此，对于MNIST，我宁愿只使用旋转和平移：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '*Now, to compare what happens with and without augmentation, you’d separately
    train a model for both an augmented and a non-augmented version of the training
    set. Here is an example setup:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*现在，为了比较增强和不增强的情况，你需要分别训练两个模型，一个用于增强版本，另一个用于非增强版本的训练集。以下是一个示例设置：'
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '*With MNIST, we have at our disposition a huge training set, and at the same
    time, we’re dealing with a very homogeneous domain. Those are exactly the conditions
    where we don’t expect to see much overfitting[²](#fn2).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*对于MNIST，我们有一个巨大的训练集，同时我们处理的是一个非常同质化的领域。这正是我们不期望看到过度拟合[²](#fn2)的情况。'
- en: However, even with MNIST, you’ll notice that with augmentation, as well as the
    other “overfitting antidotes” to be introduced, it takes a lot longer to achieve
    better performance on the training than on the test set (if ever there *is* better
    performance on the training set!). For example, in the setup described above,
    with no data augmentation applied, training-set accuracy surpassed that on the
    test set from the third epoch onwards; whereas with augmentation, there was no
    sign of overfitting during all five epochs I trained for.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，即使对于MNIST，你也会注意到，在增强以及其他即将介绍的“过度拟合解药”的情况下，在训练集上取得更好的性能比在测试集上（如果训练集上真的有更好的性能）要花更长的时间！例如，在上面的设置中，没有应用数据增强的情况下，从第三个epoch开始，训练集的准确率就超过了测试集；而使用增强的情况下，我在训练的五个epoch中都没有看到过度拟合的迹象。
- en: Next – still in the realm of data augmentation – we look at a technique that
    is domain-independent; that is, it can be applied to all kinds of data, not just
    images.*****  ***### 16.2.2 Mixup
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来——仍然在数据增强的领域内——我们来看一种与领域无关的技术；也就是说，它可以应用于各种数据，而不仅仅是图像。*****  ***### 16.2.2
    Mixup
- en: Classic data augmentation, whatever it may be doing to the entities involved
    – move them, distort them, blur them – it leaves them *intact*. A rotated cat
    is still a cat. *Mixup* ([Zhang et al. 2017](references.html#ref-2017arXiv171009412Z)),
    on the other hand, takes two entities and “mixes them together”. With mixup, we
    may have something that’s half-cat and half-squirrel. Or rather, in practice,
    with strongly unequal *mixing weights* used, ninety percent squirrel and ten percent
    cat.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的数据增强，无论它对涉及的实体做了什么——移动它们、扭曲它们、模糊它们——它都使它们保持 *完整*。一个旋转的猫仍然是猫。*Mixup* ([张等人
    2017](references.html#ref-2017arXiv171009412Z))，另一方面，将两个实体“混合在一起”。使用 mixup，我们可能会有半猫半松鼠的东西。或者更确切地说，在实践中，使用强不平等的
    *混合权重*，可能百分之九十是松鼠，百分之十是猫。
- en: 'As an idea, mixup generalizes to any domain. We could mix time series, say,
    or categorical data of any kind. We could also mix numerical data, although it’s
    not clear why we would do it. After all, mixup is nothing else but linear combination:
    take two values \(x1\) and \(x2\), and construct \(x3 = w1 x1 + w2 x2\), where
    \(w1\) and \(w2\) are weights summing to one. In neural networks, linear combination
    of numerical values (“automatic mixup”) happens all the time, so that normally,
    we wouldn’t expect “manual mixup” to add much value.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种想法，mixup 可以推广到任何领域。我们可以混合时间序列，或者任何类型的分类数据。我们也可以混合数值数据，尽管不清楚我们为什么要这样做。毕竟，mixup
    什么也不是，只是线性组合：取两个值 \(x1\) 和 \(x2\)，构造 \(x3 = w1 x1 + w2 x2\)，其中 \(w1\) 和 \(w2\)
    是加起来等于一的权重。在神经网络中，数值值的线性组合（“自动 mixup”）一直在发生，所以通常我们不会期望“手动 mixup”能增加多少价值。
- en: For visual demonstration, however, images are still best. Starting from MNIST’s
    test set, we can apply mixup with different weight patterns – equal, very unequal,
    and somewhere in-between – and see what happens. `luz` has a function, called
    `nnf_mixup()`, that lets you play around with this.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了视觉演示，图像仍然是最好的。从 MNIST 的测试集开始，我们可以应用具有不同权重模式的 mixup——相等、非常不平等，以及介于两者之间——并看看会发生什么。`luz`
    有一个名为 `nnf_mixup()` 的函数，让你可以玩转这个功能。
- en: (By the way, I’m introducing this function just so you can picture (literally!)
    what is going on. To actually *use* mixup, all that is required is to pass the
    appropriate callback to `fit()`, and let `setup()` know which loss function you
    want to use.)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: （顺便说一下，我引入这个函数只是为了让你（字面上！）想象一下正在发生的事情。实际上 *使用* mixup，所需的一切只是将适当的回调传递给 `fit()`，并让
    `setup()` 知道你想要使用哪种损失函数。）
- en: 'Besides the input and target batches, `nnf_mixup()` expects to be passed the
    *mixing weights*, one value per batch item. We start with the most “tame” variant:
    with weights that are very unequal between classes. Every resultant image will
    be composed, to ninety percent, of the original item at that position, and to
    ten percent, of a randomly-chosen different one ([fig. 16.3](#fig-overfitting-mnist-mixup-0.9)):'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 除了输入和目标批次外，`nnf_mixup()` 需要传递 *混合权重*，每个批次项一个值。我们首先从最“驯服”的变体开始：不同类之间的权重非常不平等。每个结果图像将由该位置的原始项目组成，占九十
    percent，以及随机选择的不同项目占十 percent（[图 16.3](#fig-overfitting-mnist-mixup-0.9)）：
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '*![Thirty-two bold and clearly visible digits, each overlayed with another
    digit, which is barely visible.](../Images/13fe1d809dfb70b857eddea76e713ea1.png)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*![三十二个清晰可见的数字，每个数字上叠加另一个几乎看不见的数字。](../Images/13fe1d809dfb70b857eddea76e713ea1.png)'
- en: 'Figure 16.3: Mixing up MNIST, with mixing weights of 0.9.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.3：使用 0.9 的混合权重混合 MNIST。
- en: Do you agree that the mixed-in digits are just barely visible, if at all? Still,
    the callback’s default configuration results in mixing ratios pretty close to
    this one. For MNIST, this probably is too cautious a choice. But think of datasets
    where the objects are less shape-like, less sharp-edged. Mixing two landscapes,
    at an equal-ish ratio, would result in total gibberish. And here, too, the task
    plays a role; not just the dataset per se. Mixing apples and oranges one-to-one
    can make sense if we’re looking for a higher-level concept – a superset, of sorts.
    But if all we’re looking for is to correctly discern oranges that look a bit like
    an apple, or apples that have something “orange-y” in them, then a ratio such
    as 9:1 might be just fine.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否同意混合的数字几乎看不见，如果有的话？尽管如此，回调的默认配置导致混合比例相当接近这个值。对于MNIST来说，这可能是过于谨慎的选择。但是，想想那些对象形状不那么明显、边缘不那么锐利的数据集。以大致相等的比例混合两个景观，将导致完全的混乱。在这里，任务也起到了作用；不仅仅是数据集本身。如果我们寻找的是更高层次的概念——一种类型的大集，那么苹果和橙子一对一的混合是有意义的。但如果我们只是想正确区分看起来有点像苹果的橙子，或者含有“橙味”的苹果，那么9:1的比例可能就足够了。
- en: To develop an idea of what would happen for other proportions, let’s successively
    make the mixing ratio more equal.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解其他比例会发生什么，让我们逐步使混合比例更加均衡。
- en: 'First, here ([fig. 16.4](#fig-overfitting-mnist-mixup-0.7)) is 0.7:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，这里（[图16.4](#fig-overfitting-mnist-mixup-0.7)）是0.7：
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '*![Thirty-two bold and rather clearly visible digits, each overlayed with another
    digit, which is visible but not as easy to classify.](../Images/f71ee51a740fe049bf798162290e797e.png)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*![三十二个清晰可见的数字，每个数字上叠加了另一个数字，后者可见但不易分类。](../Images/f71ee51a740fe049bf798162290e797e.png)'
- en: 'Figure 16.4: Mixing up MNIST, with mixing weights of 0.7.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.4：MNIST的混合，混合权重为0.7。
- en: 'And here ([fig. 16.5](#fig-overfitting-mnist-mixup-0.5)), 0.5:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 而这里（[图16.5](#fig-overfitting-mnist-mixup-0.5)），0.5：
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '*![Thirty-two digits, each overlayed with another digit, which is displayed
    with the same intensity.](../Images/7321e050b5d985b0bd1f4e18c6a71f06.png)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*![三十二个数字，每个数字上叠加了另一个数字，以相同的强度显示。](../Images/7321e050b5d985b0bd1f4e18c6a71f06.png)'
- en: 'Figure 16.5: Mixing up MNIST, with mixing weights of 0.5.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.5：MNIST的混合，混合权重为0.5。
- en: 'To use mixup while training, all you need to do is:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 要在训练时使用mixup，你只需要做的是：
- en: Add `luz_callback_mixup()` to the list of callbacks passed to `luz::fit()`.
    The callback takes an optional parameter, `alpha`, used in determining the mixing
    ratios. I’d recommend starting with the default, though, and start tweaking from
    there.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`luz_callback_mixup()`添加到传递给`luz::fit()`的回调列表中。该回调有一个可选参数`alpha`，用于确定混合比例。不过，我建议从默认值开始，然后从那里开始调整。
- en: Wrap the loss you’re using for the task (here, cross entropy) in `nn_mixup_loss()`.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将用于任务的损失（这里，交叉熵）包裹在`nn_mixup_loss()`中。
- en: Use the loss, not the accuracy metric, for monitoring training progress, since
    accuracy in this case is not well defined.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用损失，而不是准确率指标来监控训练进度，因为在这种情况下准确率没有很好地定义。
- en: 'Here is an example:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个例子：
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '*Mixup is an appealing technique that makes a lot of intuitive sense. If you
    feel like, go ahead and experiment with it on different tasks and different types
    of data.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*Mixup是一种吸引人的技术，它有很多直观的意义。如果你愿意，可以尝试在不同任务和不同类型的数据上对其进行实验。'
- en: 'Next, we move on to the next stage in the workflow: model definition.*******  ***##
    16.3 Modeling stage: dropout and regularization'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们继续到工作流程的下一阶段：模型定义。*******  ***## 16.3 模型阶段：dropout和正则化
- en: 'Inside a neural network, there are two kinds of “data”: *activations* – tensors
    propagated from one layer to the next – and *weights*, tensors associated with
    individual layers. From the two techniques we’ll look at in this section, one
    (*dropout*) affects the former; the other (*regularization*), the latter.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中，有两种“数据”：*激活*——从一层传播到下一层的张量——和*权重*，与单个层关联的张量。在本节中我们将探讨的两种技术中，一种（*dropout*）影响前者；另一种（*正则化*）影响后者。
- en: 16.3.1 Dropout
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.3.1 Dropout
- en: 'Dropout ([Srivastava et al. 2014](references.html#ref-10.5555/2627435.2670313))
    happens during training only. At each forward pass, individual activations – single
    values in the tensors being passed on – are *dropped* (meaning: set to zero),
    with configurable probability. Due to randomness, actual positions of zeroed-out
    values in a tensor vary from pass to pass.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout ([Srivastava et al. 2014](references.html#ref-10.5555/2627435.2670313))仅在训练期间发生。在每次前向传递中，单个激活——传递中的张量中的单个值——会*丢弃*（即：设置为0），具有可配置的概率。由于随机性，张量中零值的位置在每次传递中都会变化。
- en: 'Put differently: Dynamically and reversibly, individual inter-neuron connections
    are “cut off”. Why would this help to avoid overfitting?'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说：动态且可逆地，单个神经元之间的连接会被“切断”。为什么这有助于避免过拟合？
- en: If different connections between neurons could be dropped out at unforeseeable
    times, the network as a whole had better not get too dependent on cooperation
    between individual units. But it is just this kind of inter-individual cooperation
    that results in strong memorization of examples presented during training. If
    that is made impossible, the network as a whole has to focus on more general features,
    ones that that emerge from more distributed, more random cooperation. Put differently,
    we’re introducing *randomness*, or *noise*, and no hyper-specialized model will
    be able to deal with that.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果神经元之间的不同连接可以在不可预见的时间被丢弃，那么整个网络最好不要过于依赖单个单元之间的合作。但正是这种个体间的合作导致了在训练期间呈现的示例的强大记忆。如果这变得不可能，整个网络就必须专注于更一般化的特征，这些特征来自更分布、更随机的合作。换句话说，我们引入了**随机性**或**噪声**，没有任何高度专业化的模型能够处理这种情况。
- en: 'Application of this technique in `torch` is very straightforward. A dedicated
    layer takes care of it: `nn_dropout()`. By “takes care” I mean:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `torch` 中应用这种技术非常简单。一个专门的层负责它：`nn_dropout()`。通过“负责”我指的是：
- en: In its `forward()` method, the layer checks whether we’re in training or test
    mode. If the latter, nothing happens.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在其 `forward()` 方法中，该层检查我们是否处于训练或测试模式。如果是后者，则不执行任何操作。
- en: If we’re training, it uses the dropout probability \(p\) it’s been initialized
    with to zero out parts of the input tensor.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们在训练，它将使用初始化时设置的丢弃概率 \(p\) 来将输入张量的部分置零。
- en: The partly-zeroed tensor is scaled up by the inverse of \(1-p\), to keep the
    overall magnitude of the tensor unchanged. The result is then passed on to the
    next layer.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部分归零的张量通过 \(1-p\) 的逆来放大，以保持张量的整体幅度不变。然后结果传递到下一层。
- en: 'Here is our convnet from above, with a few dropout layers interspersed:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们的卷积神经网络，其中穿插了一些丢弃层：
- en: '[PRE9]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '*As always, experimentation will help in determining a good dropout rate, as
    well as the number of dropout layers introduced. You may be wondering, though
    – how does this technique go together with the data-related ones we presented
    before?'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*就像往常一样，实验将有助于确定一个好的丢弃率，以及引入的丢弃层数量。你可能想知道——这种技术与我们之前介绍的数据相关技术是如何结合在一起的？'
- en: 'In practice (in the “real world”), you would basically always use data augmentation.
    As to dropout, it probably is *the* go-to technique in this area. A priori, there
    is no reason to *not* use them together – all the more since both are configurable.
    One way to see it is like this: You have a fixed budget of randomness; every technique
    you use that adds randomness will take up some of that budget. How do you know
    if you’ve exceeded it? By seeing no (or insufficient) progress on the training
    set. There is a clear ranking of priorities here: It’s no use worrying about generalization
    to the test set as long as the model is not learning at all. The number one requirement
    always is to get the model to learn in the first place.*  *### 16.3.2 Regularization'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中（在“现实世界”中），你基本上总是使用数据增强。至于丢弃，它可能是这个领域的**首选**技术。事先，没有理由**不**将它们一起使用——毕竟两者都是可配置的。一种看待它的方法是：你有一个固定的随机性预算；你使用的每个增加随机性的技术都会占用一些预算。你怎么知道你是否超过了预算？通过观察训练集上没有（或不足）进展。这里的优先级排序很明确：只要模型根本不学习，就没有必要担心泛化到测试集。首要要求始终是首先让模型学习。*  *###
    16.3.2 正则化
- en: Both dropout and regularization affect how the model’s inner workings, but they
    are very different in spirit.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 丢弃和正则化都会影响模型内部的工作方式，但它们在精神上非常不同。
- en: Dropout introduces randomness. Looking for analogies in machine learning overall,
    it has something in common with ensemble modeling. (By the way, the idea of ensembling
    is as applicable, in theory, to neural networks, as to other algorithms. It’s
    just not that popular because training a neural network takes a long time.)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 丢弃引入了随机性。在机器学习整体中寻找类比，它与集成建模有共同之处。（顺便说一句，集成思想在理论上既适用于神经网络，也适用于其他算法。只是它并不那么受欢迎，因为训练神经网络需要很长时间。）
- en: 'Regularization, on the other hand, is similar to – regularization. If you know
    what is meant by this term in machine learning in general, you know what is meant
    in deep learning. In deep learning, though, it is often referred to by a different
    name: “weight decay”. Personally, I find this a little misleading. “Decay” seems
    to hint at some sort of *temporal* development; in fact, this is exactly its meaning
    in “learning rate decay”, a training strategy that makes use of decreasing learning
    rates over time.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化，另一方面，类似于——正则化。如果你知道在机器学习一般意义上这个术语的含义，你就知道在深度学习中的含义。然而，在深度学习中，它通常被不同的名字所称呼：“权重衰减”。我个人觉得这个名称有点误导。
    “衰减”似乎暗示着某种*时间*上的发展；实际上，这正是“学习率衰减”这个训练策略的含义，它利用随时间降低的学习率。
- en: 'In weight decay, or regularization, however, there’s no dynamics involved at
    all. Instead, we follow a fixed rule. That rule is: When computing the loss, add
    to it a quantity proportional to the aggregate size of the weights. The idea is
    to keep the weights small and homogeneous, to prevent sharp cliffs and canyons
    in the loss function.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在权重衰减或正则化中，根本不涉及任何动态。相反，我们遵循一个固定的规则。这个规则是：在计算损失时，向其中添加一个与权重总和成比例的量。其想法是保持权重小而均匀，以防止损失函数中出现尖锐的悬崖和峡谷。
- en: In deep learning, regularization as a strategy is nowhere as central as data
    augmentation or dropout, which is why I’m not going to go into great detail. If
    you want to learn more, check out one of the many great introductions to statistical
    learning around.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，正则化作为一种策略，其重要性远不如数据增强或dropout，因此我不会详细介绍。如果你想了解更多，可以查看关于统计学习的大量优秀介绍之一。
- en: Among data scientists, regularization probably is most often associated with
    different variants of linear regression. Variants differ in what they understand
    by a penalty “proportional” to the weights. In *ridge regression*, this quantity
    will be a fraction of the sum of the *squared* weights; in the *Lasso*, their
    absolute values. It is only the former algorithm that is implemented in `torch`.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据科学家中，正则化最常与线性回归的不同变体相关联。这些变体在“与权重成比例的惩罚”这一概念上有所不同。在*岭回归*中，这个量将是平方权重总和的一部分；在*Lasso*中，是它们的绝对值。只有前者在`torch`中实现。
- en: Although semantically, regularization forms part of the “business logic” – which
    is why I’m listing it in the “model” section – it technically is implemented as
    part of an optimizer object. All of the classic optimizers – SGD, RMSProp, Adam,
    and relatives – take a `weight_decay` argument used to indicate what fraction
    of the sum of squared weights you’d like to have added to the loss.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然从语义上讲，正则化是“业务逻辑”的一部分——这就是为什么我将其列入“模型”部分——但从技术上讲，它是作为优化器对象的一部分实现的。所有的经典优化器——SGD、RMSProp、Adam及其相关优化器——都接受一个`weight_decay`参数，用于指示你希望添加到损失中的平方权重总和的分数。
- en: 'In our example, you’d pass this argument to `luz::set_opt_hparams()`, like
    so:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，你会像这样将此参数传递给`luz::set_opt_hparams()`：
- en: '[PRE10]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '*As already hinted at above, regularization is not seen that often in the context
    of neural networks. Nevertheless, given the wide range of problems deep learning
    is applied to, it’s good to be aware of its availability.**  **## 16.4 Training
    stage: Early stopping'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*如上所述，正则化在神经网络的应用中并不常见。尽管如此，鉴于深度学习应用的范围广泛，了解其可用性是很好的。**  **## 16.4 训练阶段：提前停止'
- en: 'We conclude this chapter with an example of an anything-but-sophisticated,
    but very effective training technique: early stopping. So far, in our running
    example, we’ve always let the model learn for a pre-determined number of epochs.
    Thanks to the existence of callbacks – those “hooks” into the training process
    that let you modify “almost everything” dynamically – however, training duration
    does not have to be fixed from the outset.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以一个既不复杂但非常有效的训练技术——提前停止——来结束本章。到目前为止，在我们的运行示例中，我们总是让模型学习预定的多个时期。多亏了回调的存在——那些“钩子”可以让你动态地修改“几乎所有”内容——然而，训练时间不必一开始就固定。
- en: One of several callbacks related to the learning rate – besides, e.g., `luz_callback_lr_scheduler()`
    , that allows you to dynamically adjust learning rate while training – `luz_callback_early_stopping()`
    will trigger an early exit once some configurable condition is satisfied.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 与学习率相关的几个回调之一——除了例如`luz_callback_lr_scheduler()`，它允许你在训练过程中动态调整学习率之外——`luz_callback_early_stopping()`一旦满足某些可配置的条件，就会触发提前退出。
- en: Called without parameters, it will monitor loss on the validation set, and once
    validation loss stops decreasing, it will immediately cause training to stop.
    However, less strict policies are possible. For example, `luz_callback_early_stopping(patience
    = 2)` will allow for two consecutive epochs without improvement before triggering
    an exit.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 不带参数调用时，它将监控验证集上的损失，一旦验证损失停止下降，它将立即导致训练停止。然而，也可以有更宽松的策略。例如，`luz_callback_early_stopping(patience
    = 2)`将允许在触发退出之前有两个连续的epoch没有改进。
- en: 'To make use of `luz_callback_early_stopping()`, you add it to the callbacks
    list in `fit()`:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`luz_callback_early_stopping()`，你需要在`fit()`中的回调列表中添加它：
- en: '[PRE11]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '*In deep learning, early stopping is ubiquitous; it’s hard to imagine why one
    would *not* want to use it.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，提前停止无处不在；很难想象为什么有人会*不*想使用它。
- en: 'Having discussed overfitting, we now go on to a complementary aspect of model
    training: True, we want our models to generalize; but we also want them to learn
    fast. That’s what the next chapter is dedicated to.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论了过拟合之后，我们现在转向模型训练的一个互补方面：是的，我们希望我们的模型能够泛化；但我们还希望它们能够快速学习。这就是下一章要讨论的内容。
- en: 'Srivastava, Nitish, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
    Salakhutdinov. 2014\. “Dropout: A Simple Way to Prevent Neural Networks from Overfitting.”
    *J. Mach. Learn. Res.* 15 (1): 1929–58.Zhang, Hongyi, Moustapha Cisse, Yann N.
    Dauphin, and David Lopez-Paz. 2017\. “mixup: Beyond Empirical Risk Minimization.”
    *arXiv e-Prints*, October, arXiv:1710.09412\. [https://arxiv.org/abs/1710.09412](https://arxiv.org/abs/1710.09412).*
    ** * *'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 'Srivastava, Nitish, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
    Salakhutdinov. 2014\. “Dropout: A Simple Way to Prevent Neural Networks from Overfitting.”
    *《机器学习研究杂志》* 15 (1): 1929–58.Zhang, Hongyi, Moustapha Cisse, Yann N. Dauphin,
    and David Lopez-Paz. 2017\. “mixup: Beyond Empirical Risk Minimization.” *arXiv
    e-Prints*, October, arXiv:1710.09412\. [https://arxiv.org/abs/1710.09412](https://arxiv.org/abs/1710.09412).*
    ** * *'
- en: 'Meaning: “overly fitting” the model to the training data.[↩︎](#fnref1)'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 意味着“过度拟合”模型到训练数据上。[↩︎](#fnref1)
- en: I chose MNIST for this overview because it allows to easily discern the effects
    of various augmentation techniques (especially the next one we’re going to discuss).[↩︎](#fnref2)******
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我选择MNIST作为这个概述的例子，因为它可以轻松地辨别各种增强技术（尤其是我们接下来要讨论的那个）的效果。[↩︎](#fnref2)******
