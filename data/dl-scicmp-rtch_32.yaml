- en: '25  Matrix computations: Convolution'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 25  矩阵计算：卷积
- en: 原文：[https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/matrix_computations_convolution.html](https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/matrix_computations_convolution.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/matrix_computations_convolution.html](https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/matrix_computations_convolution.html)
- en: 'In deep learning, we talk about convolutions, convolutional layers, and convolutional
    neural networks. However, as explained in the chapter on image processing, the
    thing we’re referring to when doing so really is something different: cross-correlation.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，我们谈论卷积、卷积层和卷积神经网络。然而，正如图像处理章节中解释的那样，当我们这样做时，我们真正指的是不同的事情：交叉相关。
- en: 'Formally, the difference is minor: A sign is flipped. Semantically, these are
    not the same at all. As we saw, cross-correlation lets us spot similarities: It
    serves as a *feature detector*. Convolution is harder to characterize in an abstract
    way. Whole books could be written on the eminent role it plays in signal processing,
    as well as on its mathematical significance. Here, we have to leave aside the
    deeper underpinnings. Instead, we hope to gain some insight into its operation
    - firstly, by thinking through and picturing the steps involved, and secondly,
    by implementing it in code. As in the previous chapter, the focus is on understanding,
    and creating a basis for further explorations, should you be so inclined.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，差异很小：符号被翻转了。语义上，这些完全不同。正如我们所见，交叉相关使我们能够发现相似性：它充当一个*特征检测器*。卷积在抽象方式上更难以描述。关于它在信号处理中扮演的显赫角色以及它的数学意义，可以写整本书。在这里，我们必须暂时放下更深层次的基础。相反，我们希望对其操作有所了解——首先，通过思考和想象涉及的步骤，其次，通过在代码中实现它。正如前一章所做的那样，重点是理解，并为进一步的探索打下基础，如果你有这个意向的话。
- en: 25.1 Why convolution?
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 25.1 为什么是卷积？
- en: In signal processing, *filters* are used to modify a signal in some desired
    way – for example, to cut off the high frequencies. Imagine you have the Fourier-transformed
    representation of a time series; meaning, a set of frequencies with associated
    magnitudes and phases. You’d like to set all frequencies higher than some threshold
    to zero. The easiest way is to multiply the set of frequencies by a sequence of
    ones and zeroes. If you do that, filtering is happening in the frequency domain,
    and often, that’s by far the most convenient way.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在信号处理中，*滤波器*被用来以某种期望的方式修改信号——例如，截断高频。想象一下，你有一个时间序列的傅里叶变换表示；这意味着一组具有相关幅度和相位的频率。你希望将所有高于某个阈值的频率设置为零。最简单的方法是将频率集乘以一个由一和零组成的序列。如果你这样做，滤波就是在频域中进行的，而且通常这是最方便的方法。
- en: What, though, if the same result should be achieved in the time domain – that
    is, working with the raw time series? In that case, you’d have to find the time-domain
    representation of the filter (achieved by the *Inverse Fourier Transform*). This
    representation would then have to be *convolved* with the time series. Put differently,
    convolution in the time domain corresponds to multiplication in the frequency
    domain. This basic fact gets made use of all the time.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如果在时域中达到相同的结果——也就是说，处理原始时间序列呢？在这种情况下，你将不得不找到滤波器在时域中的表示（通过*逆傅里叶变换*实现）。然后，这个表示必须与时间序列进行*卷积*。换句话说，时域中的卷积对应于频域中的乘法。这个基本事实经常被利用。
- en: Now, let’s try to understand better what convolution does, and how it is implemented.
    We begin with a single dimension, and then, explore a bit of what happens in the
    two-dimensional case.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们更好地理解卷积的作用以及它是如何实现的。我们从一维开始，然后探索一下二维情况中发生的事情。
- en: 25.2 Convolution in one dimension
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 25.2 一维卷积
- en: We start by creating a simple signal, `x`, and a simple filter, `h`. That choice
    of variable names is not a whim; in signal processing, \(h\) is the usual symbol
    denoting the *impulse response*, a term we’ll get to very soon.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建一个简单的信号`x`和一个简单的滤波器`h`。这种变量名的选择并非一时兴起；在信号处理中，`h`是表示*脉冲响应*的常用符号，我们很快就会接触到这个术语。
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*Now – given that we *do* have `torch_conv1d()` available – why don’t we call
    it and see what happens? The way convolution is defined, output length equals
    input length plus filter length, minus one. Using `torch_conv1d()`, to obtain
    length-six output, given a filter of length three, we need to pad it by two on
    both sides.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*现在——既然我们确实有`torch_conv1d()`可用——为什么我们不调用它并看看会发生什么？根据卷积的定义，输出长度等于输入长度加滤波器长度减一。使用`torch_conv1d()`，为了获得长度为六的输出，给定长度为三的滤波器，我们需要在两侧填充两个。'
- en: In the following code, don’t let the calls to `view()` distract you – they’re
    present only due to `torch` expecting three-dimensional input, with dimensions
    one and two relating to batch item and channel, as usual.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，不要让对`view()`的调用分散你的注意力——它们的存在只是因为`torch`期望三维输入，其中维度一和二与批处理项和通道相关，就像往常一样。
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE2]'
- en: But wait, you’ll be thinking – didn’t we say that what `torch_conv1d()` computes
    is cross-correlation, not convolution? Well, R has `convolve()` – let’s double-check:[¹](#fn1)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，你可能正在想——我们不是说过`torch_conv1d()`计算的是交叉相关而不是卷积吗？好吧，R有`convolve()`——让我们双重检查：[¹](#fn1)
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '*[PRE4]'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE4]'
- en: 'The result is the same. However, looking into the documentation for `convolve()`,
    we see:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是相同的。然而，查看`convolve()`的文档，我们看到：
- en: Note that the usual definition of convolution of two sequences `x` and `y` is
    given by `convolve(x, rev(y), type = "o")`.
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意，两个序列`x`和`y`的卷积的通常定义是`convolve(x, rev(y), type = "o")`。
- en: 'Evidently, we need to reverse the order of items in the filter:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们需要反转滤波器中项的顺序：
- en: '[PRE5]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '*[PRE6]'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE6]'
- en: 'Indeed, the result is different now. Let’s do the same with `torch_conv1d()`:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，结果现在不同了。让我们用`torch_conv1d()`做同样的事情：
- en: '[PRE7]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '*[PRE8]'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE8]'
- en: 'Again, the outcome is the same between `torch` and R. So: That laconic phrase,
    found in the “Details” section of `convolve()`’s documentation, captures the complete
    difference between cross-correlation and convolution: In convolution, the second
    argument is reversed. Or *flipped*, in signal-processing speak. (“Flipped”, indeed,
    happens to be a far better term, since it generalizes to higher dimensions.)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，`torch`和R的结果是相同的。所以：在`convolve()`文档的“详细信息”部分中找到的这个简洁短语，捕捉了交叉相关和卷积之间的完整差异：在卷积中，第二个参数是反转的。或者用信号处理的话说，是*翻转*（“翻转”实际上是一个更好的术语，因为它可以推广到更高维度。）
- en: Technically, the difference is tiny – just a change in sign. But mathematically,
    it is essential – in the sense that it directly derives from what a filter *is*,
    and what it *does*. We’ll be able to get some insight into this soon.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 技术上，差异很小——只是符号的改变。但从数学上讲，这是至关重要的——在意义上，它直接来源于滤波器*是什么*，以及它*做什么*。我们很快就能对此有所了解。
- en: The operation underlying convolution can be pictured in two ways.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积操作的基本原理可以用两种方式来理解。
- en: 25.2.1 Two ways to think about convolution
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 25.2.1 思考卷积的两种方式
- en: For one, we can look at a single output value and determine how it comes about.
    That is, we ask which input elements contribute to its value, and how those are
    being combined. This may be called the “output view”, and it’s one we’re already
    familiar with from cross-correlation.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以观察单个输出值，并确定它是如何产生的。也就是说，我们询问哪些输入元素对其值有贡献，以及这些元素是如何被组合的。这可以称为“输出视图”，这是我们已经在交叉相关中熟悉的一种视图。
- en: As to cross-correlation, we described it like this. A filter “slides” over an
    image, and at each image location (pixel), we sum up the products of surrounding
    input pixels with the corresponding “overlayed” filter values. Put differently,
    each output pixel results from computing the *dot product* between matched input
    and filter values.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 至于交叉相关，我们这样描述它。一个滤波器“滑动”过图像，并在每个图像位置（像素）处，我们将周围输入像素与相应的“覆盖”滤波器值的乘积相加。换句话说，每个输出像素是由计算匹配输入和滤波器值的*点积*得到的。
- en: 'The second way of looking at things is from the point of view of the input
    (named, accordingly, the “input view”). It asks: In what way does each input value
    contribute to the output? This view takes some more getting-used-to than the first;
    but maybe that’s just a matter of socialization – the manner in which the topic
    is usually presented in a neural-networks context. In any case, the input view
    is highly instructive, in that it allows us to learn about the mathematical *meaning*
    of convolution.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种看待事物的方式是从输入的角度（因此命名为“输入视图”）。它询问：每个输入值是如何影响输出的？这种视图比第一种更难适应；但也许这只是社会化的问题——在神经网络环境中通常介绍这个主题的方式。无论如何，输入视图非常有教育意义，因为它使我们能够了解卷积的数学*意义*。
- en: We’re going to look at both, starting with more familiar one, the output view.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将查看两者，从更熟悉的一个开始，即输出视图。
- en: 25.2.1.1 Output view
  id: totrans-34
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 25.2.1.1 输出视图
- en: In the output view, we start by padding the input signal on both sides, just
    like we did when calling `torch_conv2d()` with `padding = 2`. As required, we
    flip the impulse response, turning it into `1, 0, -1`. Then, we picture the “sliding”.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出视图中，我们首先在输入信号的两边填充，就像我们调用 `torch_conv2d()` 时使用 `padding = 2` 一样。按照要求，我们翻转冲激响应，将其变为
    `1, 0, -1`。然后，我们想象“滑动”。
- en: Below, you find this visualized in tabular form ([tbl. 25.1](#tbl-convolution-output)).
    The bottom row holds the result, obtained from summing up the individual products
    at each position.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 下面，你可以在表格形式中找到这个可视化（[表 25.1](#tbl-convolution-output)）。最后一行持有从每个位置单独乘积的总和中得到的结果。
- en: 'Table 25.1: Convolution: Output view.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 表 25.1：卷积：输出视图。
- en: '| Signal | Flipped IR |  |  |  |  |  |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 信号 | 翻转IR |  |  |  |  |  |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| `0` | `1` |  |  |  |  |  |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| `0` | `1` |  |  |  |  |  |'
- en: '| `0` | `0` | `1` |  |  |  |  |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| `0` | `0` | `1` |  |  |  |  |'
- en: '| `1` | `-1` | `0` | `1` |  |  |  |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| `1` | `-1` | `0` | `1` |  |  |  |'
- en: '| `2` |  | `-1` | `0` | `1` |  |  |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| `2` |  | `-1` | `0` | `1` |  |  |'
- en: '| `3` |  |  | `-1` | `0` | `1` |  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| `3` |  |  | `-1` | `0` | `1` |  |'
- en: '| `4` |  |  |  | `-1` | `0` | `1` |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| `4` |  |  |  | `-1` | `0` | `1` |'
- en: '| `0` |  |  |  |  | `-1` | `0` |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| `0` |  |  |  |  | `-1` | `0` |'
- en: '| `0` |  |  |  |  |  | `-1` |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| `0` |  |  |  |  |  | `-1` |'
- en: '| **Result** | `-1` | `-2` | `-2` | `-2` | `3` | `4` |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| **结果** | `-1` | `-2` | `-2` | `-2` | `3` | `4` |'
- en: After all we’ve said on the topic, this depiction should offer few surprises.
    On to the input view.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们对该主题的所有讨论之后，这种表示应该不会带来太多惊喜。接下来是输入视图。
- en: 25.2.1.2 Input view
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 25.2.1.2 输入视图
- en: 'The essential thing about the input view is the way we conceptualize the input
    signal: Each individual element is seen as a – *scaled* and *shifted – impulse*.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 输入视图的关键在于我们如何概念化输入信号：每个单独的元素被视为一个 – *缩放* 和 *平移 – 冲激*。
- en: 'The *impulse* is given by the *unit sample* (or: impulse) function, delta (\(\delta\)).
    This function is zero everywhere, except at zero, where its value is one:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 冲激由单位样本（或：冲激）函数，delta (\(\delta\)) 给出。此函数在除零以外的所有地方为零，在零处其值为一：
- en: \[ \delta [n]={\begin{cases}1\ \ \ if \ n=0\\0\ \ \ if \ n \ne 0\end{cases}}
    \]
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \delta [n]={\begin{cases}1\ \ \ if \ n=0\\0\ \ \ if \ n \ne 0\end{cases}}
    \]
- en: 'This is like a Kronecker delta, \(\delta_{ij}\)[²](#fn2), with one of the indices
    being fixed at 0:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这就像一个克罗内克δ，\(\delta_{ij}\)[²](#fn2)，其中一个索引被固定在 0：
- en: \[ \delta [n]= \delta _{n0}= \delta _{0n} \]
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \delta [n]= \delta _{n0}= \delta _{0n} \]
- en: Thus, equipped with only that function, \(\delta[n]\) – with \(n\) representing
    discrete time, say – we can represent exactly one signal value, the one at time
    \(n = 0\)[³](#fn3), and its only possible value is `1`. Now we add to this the
    operations *scale* and *shift*.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，仅使用该函数，\(\delta[n]\) – 其中 \(n\) 代表离散时间，例如 – 我们可以精确表示一个信号值，即在时间 \(n = 0\)
    的那个值[³](#fn3)，并且它唯一的可能值是 `1`。现在我们添加 *缩放* 和 *平移* 操作。
- en: 'By scaling, we can produce any value at \(n = 0\); for example: \(x_0 = 0 *
    \delta [n]\).'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过缩放，我们可以在 \(n = 0\) 处产生任何值；例如：\(x_0 = 0 * \delta [n]\)。
- en: By shifting, we can affect values at other points in time. For example, time
    \(n = 3\) can be addressed as \(\delta [n - 3]= \delta _{n3}\), since \(n - 3
    = 0\).
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过平移，我们可以影响其他时间点的值。例如，时间 \(n = 3\) 可以表示为 \(\delta [n - 3]= \delta _{n3}\)，因为
    \(n - 3 = 0\)。
- en: 'Combining both, we can represent any value at any point in time. For example:
    \(x_5 = 1.11 * \delta [n - 5]\).'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结合两者，我们可以在任何时间点表示任何值。例如：\(x_5 = 1.11 * \delta [n - 5]\)。
- en: 'So far, we’ve talked just about the signal. What about the filter? Just like
    the impulse is essential in characterizing a signal, a filter is completely described
    by its *impulse response*[⁴](#fn4). The impulse response, by definition, is what
    comes out when the input is an impulse (that is, happens at time \(n = 0\)). In
    notation analogous to that used for the signal, with \(h\) denoting the impulse
    response, we have:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只谈论了信号。那么滤波器呢？就像冲激对于表征信号是至关重要的，滤波器完全由其 *冲激响应*[⁴](#fn4) 描述。根据定义，冲激响应是当输入是冲激（即在时间
    \(n = 0\) 发生）时输出的内容。与信号使用的符号类似，用 \(h\) 表示冲激响应，我们有：
- en: \[ h[n] = h[n- 0] \equiv h(\delta[n- 0]) \]
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: \[ h[n] = h[n- 0] \equiv h(\delta[n- 0]) \]
- en: 'In our example, that would be the sequence `-1, 0, 1`. But just like the signal
    needs to be represented at additional times, not just at \(0\), the filter has
    to be applicable to other positions, as well. To that purpose, again, a shift
    operation is employed, and it is formalized in an analogous way: For instance,
    \(h[n - 1]\) means the filter is applied to time \(1\), the time when \(n - 1\)
    equals zero. These shifts correspond to what we informally refer to as “sliding”.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，这将是一个序列 `-1, 0, 1`。但就像信号需要表示在额外的时刻，而不仅仅是 \(0\) 时刻一样，滤波器也必须适用于其他位置。为此，再次使用移位操作，并以类似的方式形式化：例如，\(h[n
    - 1]\) 表示滤波器应用于时间 \(1\)，即 \(n - 1\) 等于零的时间。这些移位对应于我们非正式所说的“滑动”。
- en: 'Now, all that remains to be done is combine the pieces. At time \(n = 0\),
    we take the un-shifted impulse response, and *scale* it by the amplitude of the
    signal. In our example, that value was \(1\). Thus: \(1 * h[n - 0] = 1 * [-1,
    0, 1] = [-1, 0, 1]\). For the other times, we shift the impulse response to the
    input position in question, and multiply. Finally, once we’ve obtained all contributions
    from all input positions, we add them up, thus obtaining the convolved output.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，剩下要做的就是组合这些部分。在 \(n = 0\) 时刻，我们取未移位的冲激响应，并 *缩放* 它以匹配信号的幅度。在我们的例子中，这个值是 \(1\)。因此：\(1
    * h[n - 0] = 1 * [-1, 0, 1] = [-1, 0, 1]\)。对于其他时间，我们将冲激响应移位到相应的输入位置，并相乘。最后，一旦我们从所有输入位置获得了所有贡献，我们将它们相加，从而得到卷积输出。
- en: 'The following table aims to illustrate that ([tbl. 25.2](#tbl-convolution-input)):'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 下表旨在说明这一点（[表25.2](#tbl-convolution-input)）：
- en: 'Table 25.2: Convolution: Input view.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 表25.2：卷积：输入视图。
- en: '| Signal | Impulse response | Product |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 信号 | 冲激响应 | 积 |'
- en: '| --- | --- | --- |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `1` | `h[n - 0]` | `-1 0 1 0 0 0` |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| `1` | `h[n - 0]` | `-1 0 1 0 0 0` |'
- en: '| `2` | `h[n - 1]` | `0 -2 0 2 0 0` |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| `2` | `h[n - 1]` | `0 -2 0 2 0 0` |'
- en: '| `3` | `h[n - 2]` | `0 0 -3 0 3 0` |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| `3` | `h[n - 2]` | `0 0 -3 0 3 0` |'
- en: '| `4` | `h[n - 3]` | `0 0 0 -4 0 4` |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| `4` | `h[n - 3]` | `0 0 0 -4 0 4` |'
- en: '| **Sum** |  | `-1 -2 -2 -2 3 4` |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| **总和** |  | `-1 -2 -2 -2 3 4` |'
- en: 'Personally, while I do find the output view easier to grasp, I feel I can derive
    more insight from the input view. In particular, it answers the – unavoidable
    – question: So *why* do we flip the impulse response?'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 个人而言，虽然我发现输出视图更容易理解，但我感觉我可以从输入视图中获得更多的见解。特别是，它回答了——不可避免的——问题：那么 *为什么* 我们要翻转冲激响应？
- en: 'It turns out that, far from being due to whatever mysterious forces, the minus
    sign is merely a mechanical outcome of the *way signals are represented*: The
    signal measured at time \(n = 2\) is denoted by \(\delta [n - 2]\) (two minus
    two yielding zero); and the filter applied to that signal, accordingly, as \(h[n
    -2]\).'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，这并非由于任何神秘的力量，负号仅仅是由于 *信号表示方式* 的机械结果：在 \(n = 2\) 时刻测量的信号表示为 \(\delta [n
    - 2]\)（两个减去两个等于零）；相应地，应用于该信号的滤波器为 \(h[n -2]\)。
- en: 25.2.2 Implementation
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 25.2.2 实现
- en: 'From the way I’ve described the output view, you may well think there’s not
    much to say about how to code this. It looks straightforward: Loop over the input
    vector, and compute the dot product at every prospective output position. But
    that would mean calculating many vector products, the more, the longer the input
    sequence.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 从我描述的输出视图来看，你可能认为关于如何编码这个视图没有太多可说的。看起来很简单：遍历输入向量，并在每个预期的输出位置计算点积。但那样做意味着要计算许多向量乘积，输入序列越长，计算的量就越大。
- en: 'Fortunately, there is a better way. Single-dimension (linear) convolution is
    computed by means of Toeplitz matrices, matrices that have some number of constant
    diagonals, and values of zero everywhere else. Once the filter has been formulated
    as a Toeplitz matrix, there is just a single multiplication to be carried out:
    that of the Toeplitz matrix and the input. And even though the matrix will need
    to have as many columns as the input has values (otherwise we couldn’t do the
    multiplication), computational cost is small due to the matrix’s being “nearly
    empty”.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有一种更好的方法。一维（线性）卷积是通过托普利茨矩阵来计算的，这些矩阵有一些常数对角线，其他地方都是零。一旦滤波器被表示为托普利茨矩阵，就只需要进行一次乘法运算：即托普利茨矩阵和输入的乘法。即使矩阵需要与输入值一样多的列（否则我们无法进行乘法），但由于矩阵“几乎为空”，计算成本很小。
- en: 'Here is such a Toeplitz matrix, constructed for our running example:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个为我们的示例构建的托普利茨矩阵：
- en: '[PRE9]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '*[PRE10]'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE10]'
- en: 'Let’s check that multiplication with our example input yields the expected
    result:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用我们的示例输入来检查乘法是否得到预期的结果：
- en: '[PRE11]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '*[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE12]'
- en: It does. Now, let’s move on to two dimensions. Conceptually, there is no difference,
    but actual computation (both “by hand” and using matrices) gets a lot more involved.
    Thus, we’ll content ourselves with presenting a (generalizeable) part of the manual
    calculation, and, in the computational part, don’t aim at elucidating every single
    detail.*******  ***## 25.3 Convolution in two dimensions
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 确实如此。现在，让我们继续到二维。在概念上没有区别，但实际的计算（无论是手工计算还是使用矩阵）要复杂得多。因此，我们将满足于展示手动计算的一部分（可推广的部分），在计算部分，我们不旨在阐明每一个细节。*******  ***##
    25.3 二维卷积
- en: To show how, conceptually, one-dimensional and two-dimensional convolution are
    analogous, we assume the output view.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明一维和二维卷积在概念上的相似性，我们假设输出视图。
- en: 25.3.1 How it works (output view)
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 25.3.1 工作原理（输出视图）
- en: 'This time, the example input is two-dimensional. It could look like this:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，示例输入是二维的。它可能看起来像这样：
- en: \[ \begin{bmatrix} 1 & 4 & 1\\ 2 & 5 & 3\\ \end{bmatrix} \]
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \begin{bmatrix} 1 & 4 & 1\\ 2 & 5 & 3\\ \end{bmatrix} \]
- en: 'The same goes for the filter. Here is a possible one:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对于滤波器也是如此。这里是一个可能的例子：
- en: \[ \begin{bmatrix} 1 & 1\\ 1 & -1\\ \end{bmatrix} \]
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \begin{bmatrix} 1 & 1\\ 1 & -1\\ \end{bmatrix} \]
- en: 'We take the output view, the one where the filter “slides” over the input.
    But, to keep things readable, let me just pick a single output value (“pixel”)
    for demonstration. If the input is of size `m1 x n1`, and the filter, `m2 x n2`,
    the output will have size `(m1 + m2 - 1) x (n1 + n2 - 1)`; thus, it will be `3
    x 4` in our case. I’ll pick the value at position `(0, 1)` – counting rows from
    the bottom, as is usual in image processing:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们取输出视图，即滤波器“滑动”在输入上的视图。但是，为了保持可读性，我只需挑选一个单个输出值（“像素”）进行演示。如果输入的大小是 `m1 x n1`，滤波器的大小是
    `m2 x n2`，则输出的大小将是 `(m1 + m2 - 1) x (n1 + n2 - 1)`；因此，在我们的例子中将是 `3 x 4`。我将选择位置
    `(0, 1)` 的值——像图像处理中一样从底部开始计数：
- en: \[ \begin{bmatrix} . & . & . & .\\ . & . & . & .\\ . & y_{01} & . & .\\ \end{bmatrix}
    \]
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \begin{bmatrix} . & . & . & .\\ . & . & . & .\\ . & y_{01} & . & .\\ \end{bmatrix}
    \]
- en: Here is the input, displayed in a table that will allow us to picture elements
    at non-existing (negative) positions.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输入，以表格形式显示，这将允许我们想象非现有（负数）位置上的元素。
- en: '| Position (x/y) | -1 | 0 | 1 | 2 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 位置 (x/y) | -1 | 0 | 1 | 2 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **1** |  | 1 | 4 | 1 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| **1** |  | 1 | 4 | 1 |'
- en: '| **0** |  | 2 | 5 | 3 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| **0** |  | 2 | 5 | 3 |'
- en: '| **-1** |  |  |  |  |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| **-1** |  |  |  |  |'
- en: 'And here, the filter, with values arranged correspondingly:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 而在这里，滤波器的值按照相应的顺序排列：
- en: '| Position (x/y) | -1 | 0 | 1 | 2 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 位置 (x/y) | -1 | 0 | 1 | 2 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **1** |  | 1 | 1 |  |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| **1** |  | 1 | 1 |  |'
- en: '| **0** |  | 1 | -1 |  |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| **0** |  | 1 | -1 |  |'
- en: '| **-1** |  |  |  |  |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| **-1** |  |  |  |  |'
- en: As in the one-dimensional case, the first thing to be done is flip the filter.
    Flipping here means rotation by hundred-eighty degrees.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 与一维情况一样，首先要做的是翻转滤波器。翻转在这里意味着旋转一百八十度。
- en: '| Position (x/y) | -1 | 0 | 1 | 2 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 位置 (x/y) | -1 | 0 | 1 | 2 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **1** |  |  |  |  |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| **1** |  |  |  |  |'
- en: '| **0** | -1 | 1 |  |  |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| **0** | -1 | 1 |  |  |'
- en: '| **-1** | 1 | 1 |  |  |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| **-1** | 1 | 1 |  |  |'
- en: Next, the filter is shifted to the desired output position. What we want to
    do is shift to the right by one, leaving unaffected vertical position.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，将滤波器移动到所需的输出位置。我们想要做的是向右移动一个单位，垂直位置保持不变。
- en: '| Position (x/y) | -1 | 0 | 1 | 2 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 位置 (x/y) | -1 | 0 | 1 | 2 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **1** |  |  |  |  |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| **1** |  |  |  |  |'
- en: '| **0** |  | -1 | 1 |  |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| **0** |  | -1 | 1 |  |'
- en: '| **-1** |  | 1 | 1 |  |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| **-1** |  | 1 | 1 |  |'
- en: 'Now we are all set to compute the output value at position `(0, 1)`. It’s the
    dot product of all overlapping image and filter values:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好计算位置 `(0, 1)` 的输出值。它是所有重叠图像和滤波器值的点积：
- en: '| Position (x/y) | -1 | 0 | 1 | 2 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 位置 (x/y) | -1 | 0 | 1 | 2 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **1** |  |  |  |  |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| **1** |  |  |  |  |'
- en: '| **0** |  | -1*2=-2 | 1*5=5 |  |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| **0** |  | -1*2=-2 | 1*5=5 |  |'
- en: '| **-1** |  |  |  |  |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| **-1** |  |  |  |  |'
- en: The final result, then, is `-2 + 5 = 3`.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果是 `-2 + 5 = 3`。
- en: \[ \begin{bmatrix} . & . & . & .\\ . & . & . & .\\ . & 3 & . & .\\ \end{bmatrix}
    \]
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \begin{bmatrix} . & . & . & .\\ . & . & . & .\\ . & 3 & . & .\\ \end{bmatrix}
    \]
- en: All values still missing can be computed in an analogous way. But we’ll skip
    that exercise, and take a look at how an actual computation would proceed.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 所有缺失的值都可以用类似的方式计算。但我们将跳过这个练习，看看实际的计算过程。
- en: 25.3.2 Implementation
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 25.3.2 实现方法
- en: The way two-dimensional convolution is actually implemented in code again involves
    Toeplitz matrices. Like I already said, we won’t go into why exactly every step
    takes the *exact form* it takes – the intent here is to show a working example,
    an example you could build on, if you wanted, for your own explorations.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 代码中实现二维卷积的方式再次涉及到Toeplitz矩阵。就像我之前说的，我们不会深入探讨为什么每一步都采取 *确切的形式* – 这里的目的是展示一个工作示例，如果你愿意，你可以在此基础上进行自己的探索。
- en: '25.3.2.1 Step one: Prepare filter matrix'
  id: totrans-128
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 25.3.2.1 第一步：准备滤波矩阵
- en: We start by padding the filter to the output size, `3 x 4`.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将滤波器填充到输出大小，`3 x 4`。
- en: '[PRE13]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We then create a Toeplitz matrix for every row in the filter, starting at the
    bottom.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们为滤波器中的每一行创建一个Toeplitz矩阵，从底部开始。
- en: '[PRE14]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In code, we have:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，我们有：
- en: '[PRE15]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '*Next, these three matrices are assembled so as to form a *doubly-blocked Toeplitz*
    *matrix*. Like so:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '*接下来，这三个矩阵被组装起来，形成一个 *双重块Toeplitz* 矩阵。如下所示：'
- en: '[PRE16]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'One way of coding this is to (twice) use `torch_block_diag()` to build up the
    two non-zero blocks, and concatenate them:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 编码这种方法的其中一种方式是（两次）使用 `torch_block_diag()` 来构建两个非零块，并将它们连接起来：
- en: '[PRE17]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '*[PRE18]'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE18]'
- en: 'The final matrix has two non-zero “bands”, separated by two all-zero diagonals.
    This is the final form of the filter needed for matrix multiplication.**  **####
    25.3.2.2 Step two: Prepare input'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 最终矩阵有两个非零的“带”，由两个全零对角线分隔。这是矩阵乘法所需的滤波器的最终形式。**  **#### 25.3.2.2 第二步：准备输入
- en: To be multiplicable with this `12 x 6` matrix, the input needs to be flattened
    into a vector. Again, we proceed row-by-row, starting from the bottom here as
    well.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 为了与这个 `12 x 6` 矩阵相乘，输入需要被展平成一个向量。同样，我们也是逐行进行，从底部开始。
- en: '[PRE19]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '*[PRE20]*  *#### 25.3.2.3 Step three: Multiply'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE20]*  *#### 25.3.2.3 第三步：相乘'
- en: 'By now, convolution has morphed into straightforward matrix multiplication:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，卷积已经变成了直接的矩阵乘法：
- en: '[PRE21]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '*[PRE22]'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE22]'
- en: 'All that remains to be done is reshape the output into the correct two-dimensional
    structure. Building up the rows in order (again, bottom-first) we obtain:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的只是将输出重塑为正确的二维结构。按照顺序构建行（再次，从底部开始）我们得到：
- en: \[ \begin{bmatrix} 1 & 5 & 5 & 1\\ 3 & 10 & 5 & 2\\ 2 & 3 & -2 & -3\\ \end{bmatrix}
    \]
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \begin{bmatrix} 1 & 5 & 5 & 1\\ 3 & 10 & 5 & 2\\ 2 & 3 & -2 & -3\\ \end{bmatrix}
    \]
- en: Looking at element `(0, 1)`, we see that the computation confirms our manual
    calculation.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 查看元素 `(0, 1)`，我们看到计算结果证实了我们的手动计算。
- en: Herewith, we conclude the topic of matrix computations with `torch`. But, as
    we move on to our next topic, the Fourier transform, we won’t actually stray that
    far away. Remember how, above, we said that time-domain convolution corresponds
    to frequency-domain multiplication?
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以上内容，我们使用 `torch` 完成了矩阵计算的话题。但是，当我们转向下一个话题，傅里叶变换时，我们实际上并不会走得太远。记得我们之前说过，时域卷积对应于频域乘法？
- en: 'This correspondence is often used to speed up computation: The input data are
    Fourier-transformed, the result is multiplied by the filter, and the filtered
    frequency-domain representation is transformed back again. Just have a look at
    the documentation for R’s `convolve()`. It directly starts out stating:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这种对应关系通常用于加速计算：输入数据被傅里叶变换，结果与滤波器相乘，然后过滤后的频域表示再次变换回来。只需查看R的 `convolve()` 文档。它直接开始声明：
- en: Use the Fast Fourier Transform to compute the several kinds of convolutions
    of two sequences.
  id: totrans-152
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用快速傅里叶变换来计算两个序列的几种卷积。
- en: On to the Fourier Transform, then!****  **** * *
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是傅里叶变换！****  **** * *
- en: The argument `type = "open"` is passed to request linear, not circular, convolution.[↩︎](#fnref1)
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `type = "open"` 作为参数传递，以请求线性而不是循环卷积。[↩︎](#fnref1)
- en: The Kronecker delta, \(\delta_{ij}\), evaluates to one if \(i\) equals \(j\),
    and to zero, otherwise.[↩︎](#fnref2)
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 克罗内克δ，\(\delta_{ij}\)，当 \(i\) 等于 \(j\) 时评估为1，否则为0。[↩︎](#fnref2)
- en: I’m using \(n\), instead of \(t\), to index into different positions, because
    the signal – like any digitized one – only “exists” at discrete points in time
    (or space). In some contexts, this reads a bit awkward, but it at least is consistent.[↩︎](#fnref3)
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我使用 \(n\) 而不是 \(t\) 来索引不同的位置，因为信号 – 就像任何数字化的信号一样 – 只在离散的时间（或空间）点上“存在”。在某些上下文中，这读起来有点不自然，但至少是一致的。[↩︎](#fnref3)
- en: Like everywhere in the chapter, when I talk of filters, I think of linear time-invariant
    systems only. The restriction to time-invariant systems is immanent in the convolution
    operation.[↩︎](#fnref4)******
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 就像本章的每个地方一样，当我提到滤波器时，我只考虑线性时不变系统。卷积操作中限制为时不变系统是固有的。[↩︎](#fnref4)******
