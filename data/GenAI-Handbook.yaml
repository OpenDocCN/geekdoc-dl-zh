- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2026-01-04 09:39:43'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2026-01-04 09:39:43
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: GenAI Handbook
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GenAI手册
- en: 来源：[https://genai-handbook.github.io/](https://genai-handbook.github.io/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://genai-handbook.github.io/](https://genai-handbook.github.io/)
- en: William Brown
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 威廉·布朗
- en: '[@willccbb](https://x.com/willccbb) | [willcb.com](https://willcb.com)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[@willccbb](https://x.com/willccbb) | [willcb.com](https://willcb.com)'
- en: '[v0.1](https://github.com/genai-handbook/genai-handbook.github.io) (June 5,
    2024)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[v0.1](https://github.com/genai-handbook/genai-handbook.github.io)（2024年6月5日）'
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: This document aims to serve as a handbook for learning the key concepts underlying
    modern artificial intelligence systems. Given the speed of recent development
    in AI, there really isn’t a good textbook-style source for getting up-to-speed
    on the latest-and-greatest innovations in LLMs or other generative models, yet
    there is an abundance of great explainer resources (blog posts, videos, etc.)
    for these topics scattered across the internet. My goal is to organize the “best”
    of these resources into a textbook-style presentation, which can serve as a roadmap
    for filling in the prerequisites towards individual AI-related learning goals.
    My hope is that this will be a “living document”, to be updated as new innovations
    and paradigms inevitably emerge, and ideally also a document that can benefit
    from community input and contribution. This guide is aimed at those with a technical
    background of some kind, who are interested in diving into AI either out of curiosity
    or for a potential career. I’ll assume that you have some experience with coding
    and high-school level math, but otherwise will provide pointers for filling in
    any other prerequisites. Please let me know if there’s anything you think should
    be added!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文档旨在作为学习现代人工智能系统关键概念的指南。鉴于人工智能近期发展的速度，确实没有一本好的教科书式资料可以用来了解最新的LLMs或其他生成模型创新，然而，关于这些主题的优质解释资源（博客文章、视频等）散布在互联网的各个角落。我的目标是把这些“最佳”资源组织成教科书式的展示，这可以作为实现个人人工智能相关学习目标的路线图。我希望这将成为一份“活文档”，随着新创新和范例不可避免地出现而更新，并且理想情况下也能从社区输入和贡献中受益。本指南面向那些有一定技术背景、出于好奇心或潜在职业兴趣而想要深入研究人工智能的人。我将假设您有一些编程和高中水平数学的经验，但否则将提供填补任何其他先决条件的指南。如果您认为应该添加任何内容，请告诉我！
- en: The AI Landscape
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人工智能领域
- en: 'As of June 2024, it’s been about 18 months since [ChatGPT](http://chat.openai.com)
    was released by [OpenAI](https://openai.com/) and the world started talking a
    lot more about artificial intelligence. Much has happened since: tech giants like
    [Meta](https://llama.meta.com/) and [Google](https://gemini.google.com/) have
    released large language models of their own, newer organizations like [Mistral](https://mistral.ai/)
    and [Anthropic](https://www.anthropic.com/) have proven to be serious contenders
    as well, innumerable startups have begun building on top of their APIs, everyone
    is [scrambling](https://finance.yahoo.com/news/customer-demand-nvidia-chips-far-013826675.html)
    for powerful Nvidia GPUs, papers appear on [ArXiv](https://arxiv.org/list/cs.AI/recent)
    at a breakneck pace, demos circulate of [physical robots](https://www.figure.ai/)
    and [artificial programmers](https://www.cognition-labs.com/introducing-devin)
    powered by LLMs, and it seems like [chatbots](https://www.businessinsider.com/chat-gpt-effect-will-likely-mean-more-ai-chatbots-apps-2023-2)
    are finding their way into all aspects of online life (to varying degrees of success).
    In parallel to the LLM race, there’s been rapid development in image generation
    via diffusion models; [DALL-E](https://openai.com/dall-e-3) and [Midjourney](https://www.midjourney.com/showcase)
    are displaying increasingly impressive results that often stump humans on social
    media, and with the progress from [Sora](https://openai.com/sora), [Runway](https://runwayml.com/),
    and [Pika](https://pika.art/home), it seems like high-quality video generation
    is right around the corner as well. There are ongoing debates about when “AGI”
    will arrive, what “AGI” even means, the merits of open vs. closed models, value
    alignment, superintelligence, existential risk, fake news, and the future of the
    economy. Many are concerned about jobs being lost to automation, or excited about
    the progress that automation might drive. And the world keeps moving: chips get
    faster, data centers get bigger, models get smarter, contexts get longer, abilities
    are augmented with tools and vision, and it’s not totally clear where this is
    all headed. If you’re following “AI news” in 2024, it can often feel like there’s
    some kind of big new breakthrough happening on a near-daily basis. It’s a lot
    to keep up with, especially if you’re just tuning in.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 截至2024年6月，自[ChatGPT](http://chat.openai.com)由[OpenAI](https://openai.com/)发布以来，已经有大约18个月的时间，世界开始更多地讨论人工智能。自那时以来发生了很多事情：像[Meta](https://llama.meta.com/)和[Google](https://gemini.google.com/)这样的科技巨头发布了他们自己的大型语言模型，像[Mistral](https://mistral.ai/)和[Anthropic](https://www.anthropic.com/)这样的新组织也证明他们是严肃的竞争者，无数初创公司开始基于他们的API构建，每个人都[争相](https://finance.yahoo.com/news/customer-demand-nvidia-chips-far-013826675.html)寻找强大的Nvidia
    GPU，论文以惊人的速度出现在[ArXiv](https://arxiv.org/list/cs.AI/recent)上，由LLM驱动的[物理机器人](https://www.figure.ai/)和[人工程序员](https://www.cognition-labs.com/introducing-devin)的演示在社交媒体上引起了人们的关注，而且似乎[聊天机器人](https://www.businessinsider.com/chat-gpt-effect-will-likely-mean-more-ai-chatbots-apps-2023-2)正在以不同的成功程度进入在线生活的各个方面。在LLM竞赛的同时，通过扩散模型进行图像生成的技术也得到了快速发展；[DALL-E](https://openai.com/dall-e-3)和[Midjourney](https://www.midjourney.com/showcase)展示的结果越来越令人印象深刻，常常让社交媒体上的人类感到困惑，随着[Sora](https://openai.com/sora)、[Runway](https://runwayml.com/)和[Pika](https://pika.art/home)的进步，高质量视频生成似乎也即将到来。关于“AGI”何时到来，AGI究竟是什么，开放模型与封闭模型的优点，价值对齐，超级智能，存在风险，虚假新闻以及经济未来的争论正在持续。许多人担心自动化会导致失业，或者对自动化可能带来的进步感到兴奋。而世界仍在前进：芯片速度更快，数据中心更大，模型更智能，上下文更长，能力通过工具和视觉得到增强，而且并不完全清楚这一切将走向何方。如果你在2024年关注“AI新闻”，常常会感觉几乎每天都有某种重大新突破发生。这需要跟上，尤其是如果你是刚开始关注的话。
- en: With progress happening so quickly, a natural inclination by those seeking to
    “get in on the action” is to pick up the latest-and-greatest available tools (likely
    [GPT-4o](https://openai.com/index/hello-gpt-4o/), [Gemini 1.5 Pro](https://deepmind.google/technologies/gemini/pro/),
    or [Claude 3 Opus](https://www.anthropic.com/news/claude-3-family) as of this
    writing, depending on who you ask) and try to build a website or application on
    top of them. There’s certainly a lot of room for this, but these tools will change
    quickly, and having a solid understanding of the underlying fundamentals will
    make it much easier to get the most out of your tools, pick up new tools quickly
    as they’re introduced, and evaluate tradeoffs for things like cost, performance,
    speed, modularity, and flexibility. Further, innovation isn’t only happening at
    the application layer, and companies like [Hugging Face](https://huggingface.co/),
    [Scale AI](https://scale.com/), and [Together AI](https://www.together.ai/) have
    gained footholds by focusing on inference, training, and tooling for open-weights
    models (among other things). Whether you’re looking to get involved in open-source
    development, work on fundamental research, or leverage LLMs in settings where
    costs or privacy concerns preclude outside API usage, it helps to understand how
    these things work under the hood in order to debug or modify them as needed. From
    a broader career perspective, a lot of current “AI/ML Engineer” roles will value
    nuts-and-bolts knowledge in addition to high-level frameworks, just as “Data Scientist”
    roles have typically sought a strong grasp on theory and fundamentals over proficiency
    in the ML framework *du jour*. Diving deep is the harder path, but I think it’s
    a worthwhile one. But with the pace at which innovation has occurred over the
    past few years, where should you start? Which topics are essential, what order
    should you learn them in, and which ones can you skim or skip?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 由于进步发生得如此之快，那些寻求“加入行动”的人自然倾向于选择最新的最佳工具（根据写作时的信息，可能是 [GPT-4o](https://openai.com/index/hello-gpt-4o/)，[Gemini
    1.5 Pro](https://deepmind.google/technologies/gemini/pro/)，或 [Claude 3 Opus](https://www.anthropic.com/news/claude-3-family)，具体取决于你询问的对象）并尝试在这些工具之上构建网站或应用程序。当然，这里有很大的发展空间，但这些工具会迅速变化，对底层基础有扎实的理解将使你更容易充分利用你的工具，快速掌握新工具，并评估成本、性能、速度、模块化和灵活性等方面的权衡。此外，创新不仅仅发生在应用层，像
    [Hugging Face](https://huggingface.co/)，[Scale AI](https://scale.com/) 和 [Together
    AI](https://www.together.ai/) 这样的公司通过专注于开放权重模型的推理、训练和工具（以及其他方面）已经获得了立足点。无论你是想参与开源开发，从事基础研究，还是在成本或隐私问题排除外部API使用的环境中利用LLM，了解这些事物的工作原理以供调试或修改是很有帮助的。从更广泛的职业角度来看，许多当前的“AI/ML工程师”职位将重视实际知识，除了高级框架之外，就像“数据科学家”职位通常寻求对理论和基础知识的深刻理解，而不仅仅是ML框架
    *du jour* 的熟练度。深入研究是一条更难的路径，但我认为这是值得的。但是，考虑到过去几年创新发生的速度，你应该从哪里开始？哪些主题是必不可少的，你应该按什么顺序学习它们，哪些可以略读或跳过？
- en: The Content Landscape
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内容景观
- en: Textbooks are great for providing a high-level roadmap of fields where the set
    of “key ideas” is more stable, but as far as I can tell, there really isn’t a
    publicly available post-ChatGPT “guide to AI” with textbook-style comprehensiveness
    or organization. It’s not clear that it would even make sense for someone to write
    a traditional textbook covering the current state of AI right now; many key ideas
    (e.g. QLoRA, DPO, vLLM) are no more than a year old, and the field will likely
    have changed dramatically by the time it’d get to print. The oft-referenced [Deep
    Learning](https://www.deeplearningbook.org/) book (Goodfellow et al.) is almost
    a decade old at this point, and has only a cursory mention of language modeling
    via RNNs. The newer [Dive into Deep Learning](http://d2l.ai) book includes coverage
    up to Transformer architectures and fine-tuning for BERT models, but topics like
    RLHF and RAG (which are “old” by the standards of some of the more bleeding-edge
    topics we’ll touch on) are missing. The upcoming [“Hands-On Large Language Models”](https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/)
    book might be nice, but it’s not officially published yet (available online behind
    a paywall now) and presumably won’t be free when it is. The Stanford [CS224n](https://web.stanford.edu/class/cs224n/index.html#coursework)
    course seems great if you’re a student there, but without a login you’re limited
    to slide-decks and a reading list consisting mostly of dense academic papers.
    Microsoft’s [“Generative AI for Beginners”](https://microsoft.github.io/generative-ai-for-beginners/#/)
    guide is fairly solid for getting your hands dirty with popular frameworks, but
    it’s more focused on applications rather than understanding the fundamentals.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 教科书对于提供“关键思想”较为稳定的领域的宏观路线图非常有用，但据我所知，目前并没有一本公开可用的、具有教科书式全面性和组织的 ChatGPT 后“人工智能指南”。现在写一本涵盖当前人工智能状态的常规教科书似乎也没有太多意义；许多关键思想（例如
    QLoRA、DPO、vLLM）不过一年左右，而到印刷时，该领域可能已经发生了巨大的变化。《深度学习》（Goodfellow 等人著）这本书已经近十年历史，对通过
    RNNs 的语言建模只是简略提及。较新的《深入浅出深度学习》（http://d2l.ai）一书涵盖了 Transformer 架构和 BERT 模型的微调，但像
    RLHF 和 RAG（按照一些更前沿主题的标准来说已经“过时”）这样的主题并未涉及。《“动手实践大型语言模型”》（https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/）这本书可能不错，但它尚未正式出版（目前在线上付费墙后提供），而且预计出版时也不会免费。如果你是斯坦福大学的学生，那么
    [CS224n](https://web.stanford.edu/class/cs224n/index.html#coursework) 课程看起来很棒，但如果没有登录，你只能访问幻灯片和主要由密集学术论文组成的阅读清单。微软的《“初学者生成式人工智能指南”》（https://microsoft.github.io/generative-ai-for-beginners/#/）对于使用流行框架进行实践相当可靠，但它更侧重于应用而不是理解基础原理。
- en: The closest resource I’m aware of to what I have in mind is Maxime Labonne’s
    [LLM Course](https://github.com/mlabonne/llm-course) on Github. It features many
    interactive code notebooks, as well as links to sources for learning the underlying
    concepts, several of which overlap with what I’ll be including here. I’d recommend
    it as a primary companion guide while working through this handbook, especially
    if you’re interested in applications; this document doesn’t include notebooks,
    but the scope of topics I’m covering is a bit broader, including some research
    threads which aren’t quite “standard” as well as multimodal models.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我所知道的与心中所想最接近的资源是 Maxime Labonne 在 Github 上的 [LLM 课程](https://github.com/mlabonne/llm-course)。该课程包含许多交互式代码笔记本，以及学习底层概念的资源链接，其中一些与我在这里将要包括的内容有重叠。我建议将其作为阅读此手册时的主要辅助指南，尤其是如果你对应用感兴趣；这份文档不包括笔记本，但我所涵盖的主题范围更广，包括一些不太“标准”的研究线索，以及多模态模型。
- en: Still, there’s an abundance of other high-quality and accessible content which
    covers the latest advances in AI — it’s just not all organized. The best resources
    for quickly learning about new innovations are often one-off blog posts or YouTube
    videos (as well as Twitter/X threads, Discord servers, and discussions on Reddit
    and LessWrong). My goal with this document is to give a roadmap for navigating
    all of this content, organized into a textbook-style presentation without reinventing
    the wheel on individual explainers. Throughout, I’ll include multiple styles of
    content where possible (e.g. videos, blogs, and papers), as well as my opinions
    on goal-dependent knowledge prioritization and notes on “mental models” I found
    useful when first encountering these topics.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，还有大量其他高质量且易于获取的内容涵盖了AI的最新进展——只是没有很好地组织起来。快速了解新创新的最佳资源通常是单独的博客文章或YouTube视频（以及Twitter/X帖子、Discord服务器和Reddit、LessWrong上的讨论）。我的目标是提供一份路线图，以导航所有这些内容，以教科书式的展示方式组织，而不在个别解释器上重新发明轮子。在整个过程中，我将尽可能包括多种内容形式（例如视频、博客和论文），以及我对目标相关知识优先级的看法和关于“心智模型”的笔记，这些模型在我首次接触这些主题时发现很有用。
- en: I’m creating this document **not** as a “generative AI expert”, but rather as
    someone who’s recently had the experience of ramping up on many of these topics
    in a short time frame. While I’ve been working in and around AI since 2016 or
    so (if we count an internship project running evaluations for vision models as
    the “start”), I only started paying close attention to LLM developments 18 months
    ago, with the release of ChatGPT. I first started working with open-weights LLMs
    around 12 months ago. As such, I’ve spent a lot of the past year sifting through
    blog posts and papers and videos in search of the gems; this document is hopefully
    a more direct version of that path. It also serves as a distillation of many conversations
    I’ve had with friends, where we’ve tried to find and share useful intuitions for
    grokking complex topics in order to expedite each other’s learning. Compiling
    this has been a great forcing function for filling in gaps in my own understanding
    as well; I didn’t know how FlashAttention worked until a couple weeks ago, and
    I still don’t think that I really understand state-space models that well. But
    I know a lot more than when I started.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我创建这份文档**不是**作为一个“生成式AI专家”，而是一个最近在短时间内快速学习了许多这些主题的人。虽然我从2016年左右就开始在AI领域工作（如果我们把为视觉模型进行评估的实习项目算作“开始”），但我直到18个月前才开始密切关注LLM的发展，随着ChatGPT的发布。我大约在12个月前开始使用开放权重LLM。因此，我花了很多时间在过去一年里筛选博客文章、论文和视频，寻找精华；这份文档希望是那条路径的一个更直接的版本。它也作为我许多与朋友交谈的总结，我们试图找到并分享有用的直觉，以便加快彼此的学习。整理这份资料也极大地促进了我对自身理解的补充；直到几周前，我还不了解FlashAttention是如何工作的，而且我仍然认为自己对状态空间模型的理解还不够深入。但我知道的比开始时多了很多。
- en: Resources
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: 'Some of the sources we’ll draw from are:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从中汲取的一些资料包括：
- en: 'Blogs:'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 博客：
- en: '[Hugging Face](https://huggingface.co/blog) blog posts'
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Hugging Face](https://huggingface.co/blog)博客文章'
- en: '[Chip Huyen](https://huyenchip.com/blog/)’s blog'
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Chip Huyen](https://huyenchip.com/blog/)的博客'
- en: '[Lilian Weng](https://lilianweng.github.io/)’s blog'
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Lilian Weng](https://lilianweng.github.io/)的博客'
- en: '[Tim Dettmers](https://timdettmers.com/)’ blog'
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Tim Dettmers](https://timdettmers.com/)的博客'
- en: '[Towards Data Science](https://towardsdatascience.com/)'
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[走向数据科学](https://towardsdatascience.com/)'
- en: '[Andrej Karpathy](https://karpathy.github.io/)’s blog'
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Andrej Karpathy](https://karpathy.github.io/)的博客'
- en: Sebastian Raschka’s [“Ahead of AI”](https://magazine.sebastianraschka.com/)
    blog
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sebastian Raschka的[“AI前沿”](https://magazine.sebastianraschka.com/)博客
- en: 'YouTube:'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: YouTube：
- en: Andrej Karpathy’s [“Zero to Hero”](https://karpathy.ai/zero-to-hero.html) videos
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andrej Karpathy的[“从零到英雄”](https://karpathy.ai/zero-to-hero.html)视频
- en: '[3Blue1Brown](https://www.youtube.com/c/3blue1brown) videos'
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3Blue1Brown](https://www.youtube.com/c/3blue1brown)视频'
- en: Mutual Information
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 互信息
- en: StatQuest
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: StatQuest
- en: Textbooks
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 教科书
- en: The [d2l.ai](http://d2l.ai) interactive textbook
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[d2l.ai](http://d2l.ai)互动教科书'
- en: The [Deep Learning](https://www.deeplearningbook.org/) textbook
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度学习](https://www.deeplearningbook.org/)教科书'
- en: 'Web courses:'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络课程：
- en: Maxime Labonne’s [LLM Course](https://github.com/mlabonne/llm-course)
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maxime Labonne的[LLM 课程](https://github.com/mlabonne/llm-course)
- en: Microsoft’s [“Generative AI for Beginners”](https://microsoft.github.io/generative-ai-for-beginners/#/)
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微软的[“生成式AI入门”](https://microsoft.github.io/generative-ai-for-beginners/#/)
- en: Fast.AI’s [“Practical Deep Learning for Coders”](https://course.fast.ai/)
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fast.AI的[“为程序员设计的实用深度学习”](https://course.fast.ai/)
- en: Assorted university lecture notes
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各式各样的大学讲义
- en: Original research papers (sparingly)
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始研究论文（较少使用）
- en: I’ll often make reference to the original papers for key ideas throughout, but
    our emphasis will be on expository content which is more concise and conceptual,
    aimed at students or practitioners rather than experienced AI researchers (although
    hopefully the prospect of doing AI research will become less daunting as you progress
    through these sources). Pointers to multiple resources and media formats will
    be given when possible, along with some discussion on their relative merits.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我会经常引用原始论文中的关键思想，但我们的重点将放在更简洁、更概念化的解释性内容上，面向学生或从业者，而不是经验丰富的AI研究人员（尽管希望随着你通过这些资源的学习，进行AI研究的可能性会变得不那么令人畏惧）。在可能的情况下，将提供多个资源和媒体格式的指针，并对其相对优点进行一些讨论。
- en: Preliminaries
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前期准备
- en: Math
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数学
- en: 'Calculus and linear algebra are pretty much unavoidable if you want to understand
    modern deep learning, which is largely driven by matrix multiplication and backpropagation
    of gradients. Many technical people end their formal math educations around multivariable
    calculus or introductory linear algebra, and it seems common to be left with a
    sour taste in your mouth from having to memorize a suite of unintuitive identities
    or manually invert matrices, which can be discouraging towards the prospect of
    going deeper in one’s math education. Fortunately, we don’t need to do these calculations
    ourselves — programming libraries will handle them for us — and it’ll instead
    be more important to have a working knowledge of concepts such as:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要理解现代深度学习，微积分和线性代数几乎是不可避免的，因为深度学习主要是由矩阵乘法和梯度回传驱动的。许多技术人员在多元微积分或初等线性代数结束他们的正式数学教育，并且似乎很常见的是，人们会因不得不记忆一系列不直观的恒等式或手动求逆矩阵而留下苦涩的印象，这可能会让人对进一步深化数学教育的前景感到气馁。幸运的是，我们不需要自己进行这些计算——编程库会为我们处理——而且更重要的是，我们需要对以下概念有实际了解：
- en: Gradients and their relation to local minima/maxima
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度和它们与局部极小值/极大值的关系
- en: The chain rule for differentiation
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微分链式法则
- en: Matrices as linear transformations for vectors
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵作为向量的线性变换
- en: Notions of basis/rank/span/independence/etc.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础概念，如基/秩/张量积/独立性等。
- en: 'Good visualizations can really help these ideas sink in, and I don’t think
    there’s a better source for this than these two YouTube series from [3Blue1Brown](https://www.youtube.com/@3blue1brown/playlists):'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 优秀的可视化确实可以帮助这些概念深入人心，我认为没有比这两个来自[3Blue1Brown](https://www.youtube.com/@3blue1brown/playlists)的YouTube系列更好的资源了：
- en: '[Essence of calculus](https://www.youtube.com/watch?v=WUvTyaaNkzM&list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr&pp=iAQB)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[微积分的本质](https://www.youtube.com/watch?v=WUvTyaaNkzM&list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr&pp=iAQB)'
- en: '[Essence of linear algebra](https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&pp=iAQB)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[线性代数的本质](https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&pp=iAQB)'
- en: If your math is rusty, I’d certainly encourage (re)watching these before diving
    in deeper. To test your understanding, or as a preview of where we’re headed,
    the shorter [Neural networks](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)
    video series on the same channel is excellent as well, and the latest couple videos
    in the series give a great overview of Transformer networks for language modeling.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的数学基础薄弱，我当然会鼓励你在深入研究之前（重新）观看这些视频。为了测试你的理解，或者作为我们即将前往的方向的预览，该频道上较短的[神经网络](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)视频系列也非常出色，该系列最新几集对语言建模中的Transformer网络进行了很好的概述。
- en: These [lecture notes](https://links.uwaterloo.ca/math227docs/set4.pdf) from
    Waterloo give some useful coverage of multivariable calculus as it relates to
    optimization, and [“Linear Algebra Done Right”](https://linear.axler.net/LADR4e.pdf)
    by Sheldon Axler is a nice reference text for linear algebra. [“Convex Optimization”](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)
    by Boyd and Vandenberghe shows how these topics lay the foundations for the kinds
    of optimization problems faced in machine learning, but note that it does get
    fairly technical, and may not be essential if you’re mostly interested in applications.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 水loo大学提供的这些[讲义](https://links.uwaterloo.ca/math227docs/set4.pdf)涵盖了与优化相关的多元微积分的一些有用内容，Sheldon
    Axler的《“线性代数正确完成”》（[“Linear Algebra Done Right”](https://linear.axler.net/LADR4e.pdf)）是线性代数的一个很好的参考文本。[“凸优化”](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)由Boyd和Vandenberghe所著，展示了这些主题如何为机器学习中遇到的优化问题奠定基础，但请注意，它相当技术性，如果你主要对应用感兴趣，可能不是必需的。
- en: Linear programming is certainly worth understanding, and is basically the simplest
    kind of high-dimensional optimization problem you’ll encounter (but still quite
    practical); this illustrated [video](https://www.youtube.com/watch?v=E72DWgKP_1Y)
    should give you most of the core ideas, and Ryan O’Donnell’s [videos](https://www.youtube.com/watch?v=DYAIIUuAaGA&list=PLm3J0oaFux3ZYpFLwwrlv_EHH9wtH6pnX&index=66)
    (17a-19c in this series, depending on how deep you want to go) are excellent if
    you want to go deeper into the math. These lectures ([#10](https://www.youtube.com/watch?v=v-chgwlwqTk),
    [#11](https://www.youtube.com/watch?v=IWzErYm8Lyk)) from Tim Roughgarden also
    show some fascinating connections between linear programming and the “online learning”
    methods we’ll look at [later](#online-learning-and-regret-minimization), which
    will form the conceptual basis for [GANs](#generative-adversarial-nets) (among
    many other things).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 线性规划当然值得理解，它基本上是你将遇到的最简单的高维优化问题（但仍然非常实用）；这个[视频](https://www.youtube.com/watch?v=E72DWgKP_1Y)应该会给你大部分的核心思想，如果你想要更深入地了解数学，Ryan
    O’Donnell的[视频](https://www.youtube.com/watch?v=DYAIIUuAaGA&list=PLm3J0oaFux3ZYpFLwwrlv_EHH9wtH6pnX&index=66)（系列中的17a-19c，取决于你想要深入到什么程度）是极好的。Tim
    Roughgarden的这些讲座（[#10](https://www.youtube.com/watch?v=v-chgwlwqTk), [#11](https://www.youtube.com/watch?v=IWzErYm8Lyk)）也展示了线性规划与我们将要探讨的“在线学习”方法之间的某些迷人联系，这将为[生成对抗网络](#generative-adversarial-nets)（以及其他许多事物）的概念基础形成。
- en: Programming
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编程
- en: Most machine learning code is written in Python nowadays, and some of the references
    here will include Python examples for illustrating the discussed topics. If you’re
    unfamiliar with Python, or programming in general, I’ve heard good things about
    Replit’s [100 Days of Python](https://replit.com/learn/100-days-of-python) course
    for getting started. Some systems-level topics will also touch on implementations
    in C++ or CUDA — I’m admittedly not much of an expert in either of these, and
    will focus more on higher-level abstractions which can be accessed through Python
    libraries, but I’ll include potentially useful references for these languages
    in the relevant sections nonetheless.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，大多数机器学习代码都是用Python编写的，这里的一些参考资料将包括用于说明讨论主题的Python示例。如果你对Python不熟悉，或者对编程一无所知，我听说Replit的[100天Python课程](https://replit.com/learn/100-days-of-python)对于入门来说是个不错的选择。一些系统级主题也会涉及到C++或CUDA的实现——我承认我对这两者都不是很精通，并将更多地关注可以通过Python库访问的高级抽象，但无论如何，我仍会在相关部分包含这些语言的潜在有用参考资料。
- en: Organization
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 组织
- en: This document is organized into several sections and chapters, as listed below
    and in the sidebar. You are encouraged to jump around to whichever parts seem
    most useful for your personal learning goals. Overall, I’d recommend first skimming
    many of the linked resources rather than reading (or watching) word-for-word.
    This should hopefully at least give you a sense of where your knowledge gaps are
    in terms of dependencies for any particular learning goals, which will help guide
    a more focused second pass.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 本文档分为几个部分和章节，如下所示，并在侧边栏中列出。我们鼓励你跳转到对你个人学习目标最有用的部分。总的来说，我建议首先快速浏览许多链接资源，而不是逐字阅读（或观看）。这应该至少能让你对任何特定学习目标的依赖性方面的知识差距有一个感觉，这将有助于指导更专注的第二遍阅读。
- en: 'Section I: Foundations of Sequential Prediction'
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一部分：顺序预测的基础
- en: '**Goal:** Recap machine learning basics + survey (non-DL) methods for tasks
    under the umbrella of “sequential prediction”.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**目标：** 回顾机器学习基础知识 + 调查“顺序预测”范畴下的（非深度学习）方法。'
- en: Our focus in this section will be on quickly overviewing classical topics in
    statistical prediction and reinforcement learning, which we’ll make direct reference
    to in later sections, as well as highlighting some topics that I think are very
    useful as conceptual models for understanding LLMs, yet which are often omitted
    from deep learning crash courses – notably time-series analysis, regret minimization,
    and Markov models.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将快速概述统计预测和强化学习的经典主题，我们将在后续章节中直接引用这些主题，同时突出一些我认为对于理解大型语言模型（LLMs）非常有用的概念模型，而这些模型通常被深度学习速成课程所忽略——特别是时间序列分析、后悔最小化和马尔可夫模型。
- en: Statistical Prediction and Supervised Learning
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 统计预测与监督学习
- en: 'Before getting to deep learning and large language models, it’ll be useful
    to have a solid grasp on some foundational concepts in probability theory and
    machine learning. In particular, it helps to understand:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究深度学习和大型语言模型之前，对概率理论和机器学习的一些基础概念有一个扎实的掌握将非常有用。特别是，了解以下内容很有帮助：
- en: Random variables, expectations, and variance
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机变量、期望和方差
- en: Supervised vs. unsupervised learning
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习与无监督学习
- en: Regression vs. classification
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归与分类
- en: Linear models and regularization
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性模型和正则化
- en: Empirical risk minimization
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实验风险最小化
- en: Hypothesis classes and bias-variance tradeoffs
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设类和偏差-方差权衡
- en: For general probability theory, having a solid understanding of how the Central
    Limit Theorem works is perhaps a reasonable litmus test for how much you’ll need
    to know about random variables before tackling some of the later topics we’ll
    cover. This beautifully-animated 3Blue1Brown [video](https://www.youtube.com/watch?v=zeJD6dqJ5lo)
    is a great starting point, and there are a couple other good probability videos
    to check out on the channel if you’d like. This set of [course notes](https://blogs.ubc.ca/math105/discrete-random-variables/)
    from UBC covers the basics of random variables. If you’re into blackboard lectures,
    I’m a big fan of many of Ryan O’Donnell’s CMU courses on YouTube, and this [video](https://www.youtube.com/watch?v=r9S2fMQiP2E&list=PLm3J0oaFux3ZYpFLwwrlv_EHH9wtH6pnX&index=13)
    on random variables and the Central Limit Theorem (from the excellent “CS Theory
    Toolkit” course) is a nice overview.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一般概率理论，对中心极限定理如何工作的扎实理解可能是你在处理我们稍后将要讨论的一些主题之前需要了解多少随机变量的一个合理试金石。这个由3Blue1Brown制作的精美动画[视频](https://www.youtube.com/watch?v=zeJD6dqJ5lo)是一个很好的起点，如果你愿意，该频道上还有几个其他很好的概率视频可以查看。UBC的这套[课程笔记](https://blogs.ubc.ca/math105/discrete-random-variables/)涵盖了随机变量的基础知识。如果你喜欢黑板讲座，我非常喜欢YouTube上Ryan
    O’Donnell的CMU课程，特别是关于随机变量和中心极限定理的[视频](https://www.youtube.com/watch?v=r9S2fMQiP2E&list=PLm3J0oaFux3ZYpFLwwrlv_EHH9wtH6pnX&index=13)（来自优秀的“CS理论工具包”课程）是一个很好的概述。
- en: For understanding linear models and other key machine learning principles, the
    first two chapters of Hastie’s [Elements of Statistical Learning](https://hastie.su.domains/ElemStatLearn/printings/ESLII_print12_toc.pdf)
    (”Introduction” and “Overview of Supervised Learning”) should be enough to get
    started.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解线性模型和其他关键机器学习原理，Hastie的《统计学习基础》（[Elements of Statistical Learning](https://hastie.su.domains/ElemStatLearn/printings/ESLII_print12_toc.pdf)）的前两章（“引言”和“监督学习概述”）应该足以开始学习。
- en: Once you’re familiar with the basics, this [blog post](https://ryxcommar.com/2019/09/06/some-things-you-maybe-didnt-know-about-linear-regression/)
    by anonymous Twitter/X user [@ryxcommar](https://twitter.com/ryxcommar) does a
    nice job discussing some common pitfalls and misconceptions related to linear
    regression. [StatQuest](https://www.youtube.com/@statquest/playlists) on YouTube
    has a number of videos that might also be helpful.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你熟悉了基础知识，Twitter/X用户[@ryxcommar](https://twitter.com/ryxcommar)的这篇[博客文章](https://ryxcommar.com/2019/09/06/some-things-you-maybe-didnt-know-about-linear-regression/)就很好地讨论了一些与线性回归相关的常见陷阱和误解。[StatQuest](https://www.youtube.com/@statquest/playlists)在YouTube上有一些可能也很有帮助的视频。
- en: 'Introductions to machine learning tend to emphasize linear models, and for
    good reason. Many phenomena in the real world are modeled quite well by linear
    equations — the average temperature over past 7 days is likely a solid guess for
    the temperature tomorrow, barring any other information about weather pattern
    forecasts. Linear systems and models are a lot easier to study, interpret, and
    optimize than their nonlinear counterparts. For more complex and high-dimensional
    problems with potential nonlinear dependencies between features, it’s often useful
    to ask:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的介绍往往强调线性模型，这有很好的理由。现实世界中的许多现象都可以用线性方程很好地建模——过去7天的平均气温很可能是对明天气温的一个合理猜测，除非有关于天气模式预测的其他信息。与非线性对应物相比，线性系统和模型更容易研究、解释和优化。对于具有特征之间潜在非线性依赖关系的更复杂和高维问题，通常很有用去问：
- en: What’s a linear model for the problem?
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于这个问题，线性模型是什么？
- en: Why does the linear model fail?
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么线性模型会失败？
- en: What’s the best way to add nonlinearity, given the semantic structure of the
    problem?
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在给定问题的语义结构下，最好的添加非线性方法是什么？
- en: In particular, this framing will be helpful for motivating some of the model
    architectures we’ll look at later (e.g. LSTMs and Transformers).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是这种框架将有助于激发我们稍后将要探讨的一些模型架构（例如LSTMs和Transformers）。
- en: Time-Series Analysis
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间序列分析
- en: How much do you need to know about time-series analysis in order to understand
    the mechanics of more complex generative AI methods?
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解更复杂的生成式AI方法的机制，你需要了解多少关于时间序列分析的知识？
- en: '**Short answer:** just a tiny bit for LLMs, a good bit more for diffusion.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**简短回答：**对于LLM来说，只需要一点点，而对于扩散模型来说，则需要更多。'
- en: 'For modern Transformer-based LLMs, it’ll be useful to know:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于现代Transformer的LLM，了解以下内容将很有用：
- en: The basic setup for sequential prediction problems
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列预测问题的基本设置
- en: The notion of an autoregressive model
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自回归模型的概念
- en: There’s not really a coherent way to “visualize” the full mechanics of a multi-billion-parameter
    model in your head, but much simpler autoregressive models (like ARIMA) can serve
    as a nice mental model to extrapolate from.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在脑海中“可视化”一个多亿参数模型的全套机制实际上并没有一个连贯的方法，但更简单的自回归模型（如ARIMA）可以作为很好的心理模型来外推。
- en: When we get to neural [state-space models](#structured-state-space-models),
    a working knowledge of linear time-invariant systems and control theory (which
    have many connections to classical time-series analysis) will be helpful for intuition,
    but [diffusion](#diffusion-models) is really where it’s most essential to dive
    deeper into into stochastic differential equations to get the full picture. But
    we can table that for now.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们到达神经[状态空间模型](#structured-state-space-models)时，对线性时不变系统和控制理论（它们与经典时间序列分析有许多联系）的工作知识将有助于直觉，但[扩散](#diffusion-models)确实是深入到随机微分方程以获得全面了解的地方。但我们可以暂时搁置这个问题。
- en: This blog post ([Forecasting with Stochastic Models](https://towardsdatascience.com/forecasting-with-stochastic-models-abf2e85c9679))
    from Towards Data Science is concise and introduces the basic concepts along with
    some standard autoregressive models and code examples.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 来自Towards Data Science的这篇博客文章([使用随机模型进行预测](https://towardsdatascience.com/forecasting-with-stochastic-models-abf2e85c9679))简明扼要，介绍了基本概念，以及一些标准的自回归模型和代码示例。
- en: This set of [notes](https://sites.ualberta.ca/~kashlak/data/stat479.pdf) from
    UAlberta’s “Time Series Analysis” course is nice if you want to go a bit deeper
    on the math.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要对数学有更深入的了解，那么来自UAlberta“时间序列分析”课程的这套[笔记](https://sites.ualberta.ca/~kashlak/data/stat479.pdf)很不错。
- en: Online Learning and Regret Minimization
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在线学习和遗憾最小化
- en: 'It’s debatable how important it is to have a strong grasp on regret minimization,
    but I think a basic familiarity is useful. The basic setting here is similar to
    supervised learning, but:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 对于是否需要牢固掌握遗憾最小化的重要性，存在争议，但我认为基本的熟悉度是有用的。这里的设置与监督学习类似，但：
- en: Points arrive one-at-a-time in an arbitrary order
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点以任意顺序逐个到达
- en: We want low average error across this sequence
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们希望在整个序列中保持低平均误差
- en: If you squint and tilt your head, most of the algorithms designed for these
    problems look basically like gradient descent, often with delicate choices of
    regularizers and learning rates require for the math to work out. But there’s
    a lot of satisfying math here. I have a soft spot for it, as it relates to a lot
    of the research I worked on during my PhD. I think it’s conceptually fascinating.
    Like the previous section on time-series analysis, online learning is technically
    “sequential prediction” but you don’t really need it to understand LLMs.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你眯起眼睛并倾斜你的头部，大多数为这些问题设计的算法基本上看起来像梯度下降，通常需要精心选择正则化器和学习率，以便数学计算能够顺利进行。但这里有很多令人满意的数学。我对它有很深的感情，因为它与我博士期间研究的大多数研究相关。我认为它从概念上来说是迷人的。就像之前关于时间序列分析的部分一样，在线学习在技术上属于“序列预测”，但你实际上并不需要它来理解LLM。
- en: 'The most direct connection to it that we’ll consider is when we look at [GANs](#generative-adversarial-nets)
    in Section VIII. There are many deep connections between regret minimization and
    equilibria in games, and GANs work basically by having two neural networks play
    a game against each other. Practical gradient-based optimization algorithms like
    Adam have their roots in this field as well, following the introduction of the
    AdaGrad algorithm, which was first analyzed for online and adversarial settings.
    In terms of other insights, one takeaway I find useful is the following: If you’re
    doing gradient-based optimization with a sensible learning rate schedule, then
    the order in which you process data points doesn’t actually matter much. Gradient
    descent can handle it.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将考虑的最直接的联系是在第八节中查看[生成对抗网络](#generative-adversarial-nets)。在游戏中的后悔最小化和均衡之间存在许多深度联系，而生成对抗网络基本上是通过让两个神经网络相互玩游戏来工作的。实用的基于梯度的优化算法，如Adam，也源于这个领域，这要归功于AdaGrad算法的引入，该算法最初是在在线和对抗性设置中进行分析的。在其他见解方面，我发现以下观点很有用：如果你使用合理的学习率计划进行基于梯度的优化，那么处理数据点的顺序实际上并不重要。梯度下降可以处理它。
- en: I’d encourage you to at least skim Chapter 1 of [“Introduction to Online Convex
    Optimization”](https://arxiv.org/pdf/1909.05207.pdf) by Elad Hazan to get a feel
    for the goal of regret minimization. I’ve spent a lot of time with this book and
    I think it’s excellent.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我鼓励你至少浏览一下Elad Hazan的《“在线凸优化导论”》（[“Introduction to Online Convex Optimization”](https://arxiv.org/pdf/1909.05207.pdf)）的第一章，以了解后悔最小化的目标。我在这本书上花了很多时间，我认为它非常出色。
- en: Reinforcement Learning
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强化学习
- en: Reinforcement Learning (RL) will come up most directly when we look at finetuning
    methods in [Section IV](#finetuning-methods-for-llms), and may also be a useful
    mental model for thinking about “agent” [applications](#tool-use-and-agents) and
    some of the “control theory” notions which come up for [state-space models](#structured-state-space-models).
    Like a lot of the topics discussed in this document, you can go quite deep down
    many different RL-related threads if you’d like; as it relates to language modeling
    and alignment, it’ll be most important to be comfortable with the basic problem
    setup for Markov decision processes, notion of policies and trajectories, and
    high-level understanding of standard iterative + gradient-based optimization methods
    for RL.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在[第四节](#finetuning-methods-for-llms)中查看微调方法时，强化学习（RL）将最直接地出现，并且也可能是一个思考“代理”[应用](#tool-use-and-agents)和一些针对[状态空间模型](#structured-state-space-models)出现的“控制理论”概念的有用心理模型。像这份文档中讨论的许多主题一样，如果你愿意，你可以深入探索许多不同的RL相关线索；就语言建模和对齐而言，最重要的是要熟悉马尔可夫决策过程的基本问题设置、策略和轨迹的概念，以及RL的标准迭代+基于梯度的优化方法的高级理解。
- en: This [blog post](https://lilianweng.github.io/posts/2018-02-19-rl-overview/)
    from Lilian Weng is a great starting point, and is quite dense with important
    RL ideas despite its relative conciseness. It also touches on connections to AlphaGo
    and gameplay, which you might find interesting as well.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Lilian Weng的这篇[博客文章](https://lilianweng.github.io/posts/2018-02-19-rl-overview/)是一个很好的起点，尽管它相对简洁，但其中充满了重要的RL思想。它还涉及到了与AlphaGo和游戏玩法之间的联系，这可能会引起你的兴趣。
- en: 'The textbook [“Reinforcement Learning: An Introduction”](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)
    by Sutton and Barto is generally considered the classic reference text for the
    area, at least for “non-deep” methods. This was my primary guide when I was first
    learning about RL, and it gives a more in-depth exploration of many of the topics
    touched on in Lilian’s blog post.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 'Sutton和Barto合著的教科书《“强化学习：导论”》（[“Reinforcement Learning: An Introduction”](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)）通常被认为是该领域的经典参考文本，至少对于“非深度”方法来说是这样。这是我最初学习强化学习时的主要指南，它对Lilian博客文章中提到的许多主题进行了更深入的探讨。'
- en: If you want to jump ahead to some more neural-flavored content, Andrej Karpathy
    has a nice [blog post](https://karpathy.github.io/2016/05/31/rl/) on deep RL;
    this [manuscript](https://arxiv.org/pdf/1810.06339) by Yuxi Li and this [textbook](https://arxiv.org/pdf/2201.02135)
    by Aske Plaat may be useful for further deep dives.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要跳到一些更具神经风格的内容，Andrej Karpathy有一个关于深度强化学习的优秀[博客文章](https://karpathy.github.io/2016/05/31/rl/)；Yuxi
    Li的这篇[手稿](https://arxiv.org/pdf/1810.06339)和Aske Plaat的这篇[教科书](https://arxiv.org/pdf/2201.02135)可能对进一步深入研究有所帮助。
- en: If you like 3Blue1Brown-style animated videos, the series [“Reinforcement Learning
    By the Book”](https://www.youtube.com/playlist?list=PLzvYlJMoZ02Dxtwe-MmH4nOB5jYlMGBjr)
    is a great alternative option, and conveys a lot of content from Sutton and Barto,
    along with some deep RL, using engaging visualizations.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢3Blue1Brown风格的动画视频，系列[“通过书籍学习强化学习”（“Reinforcement Learning By the Book”）](https://www.youtube.com/playlist?list=PLzvYlJMoZ02Dxtwe-MmH4nOB5jYlMGBjr)
    是一个很好的替代选择，它传达了Sutton和Barto的大量内容，以及一些深度强化学习，并使用引人入胜的视觉化呈现。
- en: Markov Models
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 马尔可夫模型
- en: Running a fixed policy in a Markov decision process yields a Markov chain; processes
    resembling this kind of setup are fairly abundant, and many branches of machine
    learning involve modeling systems under Markovian assumptions (i.e. lack of path-dependence,
    given the current state). This [blog post](https://thagomizer.com/blog/2017/11/07/markov-models.html)
    from Aja Hammerly makes a nice case for thinking about language models via Markov
    processes, and this [post](https://ericmjl.github.io/essays-on-data-science/machine-learning/markov-models/)
    from “Essays on Data Science” has examples and code building up towards auto-regressive
    Hidden Markov Models, which will start to vaguely resemble some of the neural
    network architectures we’ll look at later on.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在马尔可夫决策过程中运行固定策略会产生一个马尔可夫链；类似这种设置的流程相当普遍，许多机器学习的分支都涉及在马尔可夫假设下（即给定当前状态，缺乏路径依赖性）对系统进行建模。Aja
    Hammerly的这篇[博客文章](https://thagomizer.com/blog/2017/11/07/markov-models.html)很好地说明了通过马尔可夫过程思考语言模型，而“数据科学论文集”中的这篇[文章](https://ericmjl.github.io/essays-on-data-science/machine-learning/markov-models/)则提供了构建自回归隐马尔可夫模型的例子和代码，这些模型将开始与我们稍后将要查看的一些神经网络架构有模糊相似之处。
- en: This [blog post](https://www.tweag.io/blog/2019-10-25-mcmc-intro1/) from Simeon
    Carstens gives a nice coverage of Markov chain Monte Carlo methods, which are
    powerful and widely-used techniques for sampling from implicitly-represented distributions,
    and are helpful for thinking about probabilistic topics ranging from stochastic
    gradient descent to diffusion.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Simeon Carstens的这篇[博客文章](https://www.tweag.io/blog/2019-10-25-mcmc-intro1/)对马尔可夫链蒙特卡洛方法进行了很好的介绍，这是一种强大的、广泛使用的从隐式表示的分布中进行采样的技术，对于从随机梯度下降到扩散等概率主题的思考非常有帮助。
- en: Markov models are also at the heart of many Bayesian methods. See this [tutorial](https://mlg.eng.cam.ac.uk/zoubin/papers/ijprai.pdf)
    from Zoubin Ghahramani for a nice overview, the textbook [“Pattern Recognition
    and Machine Learning”](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)
    for Bayesian angles on many machine learning topics (as well as a more-involved
    HMM presentation), and this [chapter](https://www.deeplearningbook.org/contents/graphical_models.html)
    of the Goodfellow et al. “Deep Learning” textbook for some connections to deep
    learning.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫模型也是许多贝叶斯方法的核心。参见Zoubin Ghahramani的这篇[教程](https://mlg.eng.cam.ac.uk/zoubin/papers/ijprai.pdf)，它提供了一个很好的概述，教科书[“模式识别与机器学习”（“Pattern
    Recognition and Machine Learning”）](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)从贝叶斯的角度探讨了多个机器学习主题（以及一个更深入的HMM介绍），以及Goodfellow等人“深度学习”教科书的这一[章节](https://www.deeplearningbook.org/contents/graphical_models.html)，其中探讨了与深度学习的联系。
- en: 'Section II: Neural Sequential Prediction'
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二部分：神经网络序列预测
- en: '**Goal:** Survey deep learning methods + applications to sequential and language
    modeling, up to basic Transformers.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**目标：** 调查深度学习方法及其在序列和语言建模中的应用，直至基本的Transformer。'
- en: Here, more than any other section of the handbook, I’ll defer mostly wholesale
    to existing sequences of instructional resources. This material has been covered
    quite well by many people in a variety of formats, and there’s no need to reinvent
    the wheel.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，与手册中的其他任何部分相比，我将主要借鉴现有的教学资源序列。许多人在各种格式下已经很好地覆盖了这一材料，没有必要重新发明轮子。
- en: There are a couple different routes you can take from the basics of neural networks
    towards Transformers (the dominant architecture for most frontier LLMs in 2024).
    Once we cover the basics, I’ll mostly focus on “deep sequence learning” methods
    like RNNs. Many deep learning books and courses will more heavily emphasize convolutional
    neural nets (CNNs), which are quite important for image-related applications and
    historically were one of the first areas where “scaling” was particularly successful,
    but technically they’re fairly disconnected from Transformers. They’ll make an
    appearance when we discuss [state-space models](#structured-state-space-models)
    and are definitely important for vision applications, but you’ll mostly be okay
    skipping them for now. However, if you’re in a rush and just want to get to the
    new stuff, you could consider diving right into decoder-only Transformers once
    you’re comfortable with feed-forward neural nets — this the approach taken by
    the excellent [“Let’s build GPT”](https://www.youtube.com/watch?v=kCc8FmEb1nY)
    video from Andrej Karpathy, casting them as an extension of neural n-gram models
    for next-token prediction. That’s probably your single best bet for speedrunning
    Transformers in under 2 hours. But if you’ve got a little more time, understanding
    the history of RNNs, LSTMs, and encoder-decoder Transformers is certainly worthwhile.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 从神经网络的基本知识到Transformers（2024年大多数前沿LLMs的主导架构）有几种不同的路径。一旦我们覆盖了基础知识，我主要会关注“深度序列学习”方法，如RNNs。许多深度学习书籍和课程会更加强调卷积神经网络（CNNs），这对于图像相关应用非常重要，并且在历史上是“扩展”特别成功的第一个领域之一，但技术上它们与Transformers相当脱节。它们将在我们讨论[状态空间模型](#structured-state-space-models)时出现，并且对于视觉应用肯定很重要，但你现在可以放心地跳过它们。然而，如果你急于想要快速进入新内容，一旦你对前馈神经网络感到舒适，你可以直接深入研究只包含解码器的Transformers——这是Andrej
    Karpathy的出色视频[“让我们构建GPT”（https://www.youtube.com/watch?v=kCc8FmEb1nY）]所采取的方法，将其视为神经网络n-gram模型预测下一个标记的扩展。这可能是你以不到2小时的速度运行Transformers的最佳选择。但如果你有更多时间，了解RNNs、LSTMs和编码器-解码器Transformers的历史无疑是值得的。
- en: 'This section is mostly composed of signposts to content from the following
    sources (along with some blog posts):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这一节主要包含指向以下来源的内容的指示（以及一些博客文章）：
- en: The [“Dive Into Deep Learning” (d2l.ai)](http://d2l.ai) interactive textbook
    (nice graphics, in-line code, some theory)
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“深入浅出深度学习”（d2l.ai）](http://d2l.ai)互动式教科书（图形优美，内联代码，一些理论）'
- en: 3Blue1Brown’s [“Neural networks”](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)
    video series (lots of animations)
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3Blue1Brown的[“神经网络”](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)视频系列（大量动画）
- en: Andrej Karpathy’s [“Zero to Hero”](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)
    video series (live coding + great intuitions)
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andrej Karpathy的[“从零到英雄”（https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ）]视频系列（现场编码+很好的直觉）
- en: '[“StatQuest with Josh Starmer”](https://www.youtube.com/@statquest) videos'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“StatQuest with Josh Starmer”](https://www.youtube.com/@statquest)视频'
- en: The Goodfellow et al. [“Deep Learning”](https://www.deeplearningbook.org/) textbook
    (theory-focused, no Transformers)
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow等人所著的[“深度学习”](https://www.deeplearningbook.org/)教科书（理论导向，无Transformers）
- en: If your focus is on applications, you might find the interactive [“Machine Learning
    with PyTorch and Scikit-Learn”](https://github.com/rasbt/machine-learning-book/tree/main)
    book useful, but I’m not as familiar with it personally.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的重点是应用，你可能会发现互动式书籍[“使用PyTorch和Scikit-Learn进行机器学习”（https://github.com/rasbt/machine-learning-book/tree/main）]很有用，但我在个人层面上并不那么熟悉它。
- en: For these topics, you can also probably get away with asking conceptual questions
    to your preferred LLM chat interface. This likely won’t be true for later sections
    — some of those topics were introduced after the knowledge cutoff dates for many
    current LLMs, and there’s also just a lot less text on the internet about them,
    so you end up with more “hallucinations”.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些主题，你也许可以通过向你偏好的LLM聊天界面提出概念性问题来应对。这很可能在后面的章节中不成立——其中一些主题是在许多当前LLMs的知识截止日期之后引入的，而且关于它们的网络文本也相对较少，因此你可能会遇到更多的“幻觉”。
- en: Statistical Prediction with Neural Networks
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用神经网络进行统计预测
- en: I’m not actually sure where I first learned about neural nets — they’re pervasive
    enough in technical discussions and general online media that I’d assume you’ve
    picked up a good bit through osmosis even if you haven’t studied them formally.
    Nonetheless, there are many worthwhile explainers out there, and I’ll highlight
    some of my favorites.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我实际上并不确定我第一次是从哪里了解到神经网络的——它们在技术讨论和一般在线媒体中如此普遍，以至于即使你没有正式学习，我也假设你已经通过渗透学到了很多。尽管如此，那里有很多有价值的解释者，我会突出一些我最喜欢的。
- en: The first 4 videos in 3Blue1Brown’s [“Neural networks”](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)
    series will take you from basic definitions up through the mechanics of backpropagation.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3Blue1Brown的[“神经网络”](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)系列的前4个视频将带你从基本定义到反向传播的机制。
- en: This [blog post](https://karpathy.github.io/neuralnets/) from Andrej Karpathy
    (back when he was a PhD student) is a solid crash-course, well-accompanied by
    his [video](https://www.youtube.com/watch?v=VMj-3S1tku0) on building backprop
    from scratch.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这篇来自Andrej Karpathy（当时他还是一名博士生）的[博客文章](https://karpathy.github.io/neuralnets/)是一个扎实的速成课程，配有他关于从头开始构建反向传播的[视频](https://www.youtube.com/watch?v=VMj-3S1tku0)。
- en: This [blog post](https://colah.github.io/posts/2015-08-Backprop/) from Chris
    Olah has a nice and concise walk-through of the math behind backprop for neural
    nets.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这篇来自Chris Olah的[博客文章](https://colah.github.io/posts/2015-08-Backprop/)对神经网络反向传播背后的数学进行了简洁的概述。
- en: Chapters 3-5 of the [d2l.ai](http://d2l.ai) book are great as a “classic textbook”
    presentation of deep nets for regression + classification, with code examples
    and visualizations throughout.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[d2l.ai](http://d2l.ai)书籍的第3-5章作为“经典教科书”形式的深度网络回归+分类的展示非常出色，其中包含了代码示例和可视化。'
- en: Recurrent Neural Networks
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: RNNs are where we start adding “state” to our models (as we process increasingly
    long sequences), and there are some high-level similarities to hidden Markov models.
    This blog post from [Andrej Karpathy](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)
    is a good starting point. [Chapter 9](https://d2l.ai/chapter_recurrent-neural-networks/index.html)
    of the [d2l.ai](http://d2l.ai) book is great for main ideas and code; check out
    [Chapter 10](https://www.deeplearningbook.org/contents/rnn.html) of “Deep Learning”
    if you want more theory.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs是我们开始给模型添加“状态”的地方（随着我们处理越来越长的序列），并且与隐藏马尔可夫模型有一些高级相似性。这篇来自[Andrej Karpathy](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)的博客文章是一个很好的起点。[d2l.ai](http://d2l.ai)书籍的第9章对主要思想和代码都很有帮助；如果你想了解更多理论，可以查看“深度学习”的第10章[Chapter
    10](https://www.deeplearningbook.org/contents/rnn.html)。
- en: For videos, [here](https://www.youtube.com/watch?v=AsNTP8Kwu80)’s a nice one
    from StatQuest.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 对于视频，[这里](https://www.youtube.com/watch?v=AsNTP8Kwu80)有一个来自StatQuest的不错视频。
- en: LSTMs and GRUs
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LSTM和GRU
- en: Long Short-Term Memory (LSTM) networks and Gated Recurrent Unit (GRU) networks
    build upon RNNs with more specialized mechanisms for state representation (with
    semantic inspirations like “memory”, “forgetting”, and “resetting”), which have
    been useful for improving performance in more challenging data domains (like language).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 长短期记忆（LSTM）网络和门控循环单元（GRU）网络在RNN的基础上增加了更专业的状态表示机制（具有“记忆”、“遗忘”和“重置”等语义灵感），这对于在更具挑战性的数据域（如语言）中提高性能非常有用。
- en: '[Chapter 10](https://d2l.ai/chapter_recurrent-modern/index.html) of [d2l.ai](http://d2l.ai)
    covers both of these quite well (up through 10.3). The [“Understanding LSTM Networks”](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
    blog post from Chris Olah is also excellent. This [video](https://www.youtube.com/watch?v=8HyCNIVRbSU)
    from “The AI Hacker” gives solid high-level coverage of both; StatQuest also has
    a video on [LSTMs](https://www.youtube.com/watch?v=YCzL96nL7j0), but not GRUs.
    GRUs are essentially a simplified alternative to LSTMs with the same basic objective,
    and it’s up to you if you want to cover them specifically.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[d2l.ai](http://d2l.ai)的第10章对这两者都进行了很好的介绍（直到10.3）。Chris Olah的[“理解LSTM网络”](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)博客文章也非常出色。来自“The
    AI Hacker”的这个[视频](https://www.youtube.com/watch?v=8HyCNIVRbSU)对两者都进行了坚实的高级概述；StatQuest也有关于LSTMs的[视频](https://www.youtube.com/watch?v=YCzL96nL7j0)，但没有GRUs。GRUs基本上是LSTMs的一个简化替代品，具有相同的基本目标，是否具体涵盖它们取决于你。'
- en: Neither LSTMs or GRUs are really prerequisites for Transformers, which are “stateless”,
    but they’re useful for understanding the general challenges neural sequence of
    neural sequence and contextualizing the Transformer design choices. They’ll also
    help motivate some of the approaches towards addressing the “quadratic scaling
    problem” in [Section VII](#s7).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM或GRU并不是Transformer的真正先决条件，因为Transformer是“无状态的”，但它们对于理解神经网络序列的一般挑战和Transformer设计选择是有用的，它们还将有助于激发[第七节](#s7)中解决“二次扩展问题”的一些方法。
- en: Embeddings and Topic Modeling
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入和主题建模
- en: Before digesting Transformers, it’s worth first establishing a couple concepts
    which will be useful for reasoning about what’s going on under the hood inside
    large language models. While deep learning has led to a large wave of progress
    in NLP, it’s definitely a bit harder to reason about than some of the “old school”
    methods which deal with word frequencies and n-gram overlaps; however, even though
    these methods don’t always scale to more complex tasks, they’re useful mental
    models for the kinds of “features” that neural nets might be learning. For example,
    it’s certainly worth knowing about Latent Dirichlet Allocation for topic modeling
    ([blog post](https://towardsdatascience.com/latent-dirichlet-allocation-lda-9d1cd064ffa2))
    and [tf-idf](https://jaketae.github.io/study/tf-idf/) to get a feel for what numerical
    similarity or relevance scores can represent for language.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在消化Transformer之前，首先建立一些概念是值得的，这些概念将有助于推理大型语言模型内部底层的运作情况。虽然深度学习在NLP领域带来了巨大的进步浪潮，但与处理词频和n-gram重叠的“老式”方法相比，推理起来确实要困难一些；然而，尽管这些方法并不总是适用于更复杂的任务，但它们对于神经网络可能学习的“特征”类型是有用的心智模型。例如，了解潜在狄利克雷分配（Latent
    Dirichlet Allocation）用于主题建模([博客文章](https://towardsdatascience.com/latent-dirichlet-allocation-lda-9d1cd064ffa2))和[tf-idf](https://jaketae.github.io/study/tf-idf/)对于理解数值相似度或相关性评分对语言的意义是非常有价值的。
- en: Thinking about words (or tokens) as high-dimensional “meaning” vectors is quite
    useful, and the Word2Vec embedding method illustrates this quite well — you may
    have seen the classic “King - Man + Woman = Queen” example referenced before.
    [“The Illustrated Word2Vec”](https://jalammar.github.io/illustrated-word2vec/)
    from Jay Alammar is great for building up this intuition, and these [course notes](https://web.stanford.edu/class/cs224n/readings/cs224n_winter2023_lecture1_notes_draft.pdf)
    from Stanford’s CS224n are excellent as well. Here’s also a nice [video](https://www.youtube.com/watch?v=f7o8aDNxf7k)
    on Word2Vec from ritvikmath, and another fun one [video](https://www.youtube.com/watch?v=gQddtTdmG_8)
    on neural word embeddings from Computerphile.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 将单词（或标记）视为高维的“意义”向量是非常有用的，Word2Vec嵌入方法很好地说明了这一点——你可能之前已经看到了经典的“国王 - 男人 + 女人
    = 女王”示例。Jay Alammar的[《图解Word2Vec》](https://jalammar.github.io/illustrated-word2vec/)对于建立这种直觉非常有帮助，斯坦福大学CS224n的这些[课程笔记](https://web.stanford.edu/class/cs224n/readings/cs224n_winter2023_lecture1_notes_draft.pdf)也非常优秀。这里还有一个ritvikmath的关于Word2Vec的[视频](https://www.youtube.com/watch?v=f7o8aDNxf7k)，以及一个来自Computerphile的有趣[视频](https://www.youtube.com/watch?v=gQddtTdmG_8)关于神经词嵌入。
- en: Beyond being a useful intuition and an element of larger language models, standalone
    neural embedding models are also widely used today. Often these are encoder-only
    Transformers, trained via “contrastive loss” to construct high-quality vector
    representations of text inputs which are useful for retrieval tasks (like [RAG](#retrieval-augmented-generation)).
    See this [post+video](https://docs.cohere.com/docs/text-embeddings) from Cohere
    for a brief overview, and this [blog post](https://lilianweng.github.io/posts/2021-05-31-contrastive/)
    from Lilian Weng for more of a deep dive.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 除了作为一个有用的直觉和更大语言模型的一个元素之外，独立的神经网络嵌入模型今天也被广泛使用。通常这些是仅包含编码器的Transformer，通过“对比损失”训练来构建文本输入的高质量向量表示，这对于检索任务（如[RAG](#retrieval-augmented-generation)）非常有用。参见Cohere的这篇[文章+视频](https://docs.cohere.com/docs/text-embeddings)以获得简要概述，以及Lilian
    Weng的这篇[博客文章](https://lilianweng.github.io/posts/2021-05-31-contrastive/)以深入了解。
- en: Encoders and Decoders
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码器和解码器
- en: Up until now we’ve been pretty agnostic as to what the inputs to our networks
    are — numbers, characters, words — as long as it can be converted to a vector
    representation somehow. Recurrent models can be configured to both input and output
    either a single object (e.g. a vector) or an entire sequence. This observation
    enables the sequence-to-sequence encoder-decoder architecture, which rose to prominence
    for machine translation, and was the original design for the Transformer in the
    famed [“Attention is All You Need”](https://arxiv.org/abs/1706.03762) paper. Here,
    the goal is to take an input sequence (e.g. an English sentence), “encode” it
    into a vector object which captures its “meaning”, and then “decode” that object
    into another sequence (e.g. a French sentence). [Chapter 10](https://d2l.ai/chapter_recurrent-modern/index.html)
    in [d2l.ai](http://d2l.ai) (10.6-10.8) covers this setup as well, which sets the
    stage for the encoder-decoder formulation of Transformers in [Chapter 11](https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html)
    (up through 11.7). For historical purposes you should certainly at least skim
    the original paper, though you might get a bit more out of the presentation of
    its contents via [“The Annotated Transformer”](https://nlp.seas.harvard.edu/annotated-transformer/),
    or perhaps [“The Illustrated Transformer”](https://jalammar.github.io/illustrated-transformer/)
    if you want more visualizations. These [notes](https://web.stanford.edu/class/cs224n/readings/cs224n-self-attention-transformers-2023_draft.pdf)
    from Stanford’s CS224n are great as well.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们对网络的输入并没有太多的偏见——无论是数字、字符还是单词——只要它能以某种方式转换为向量表示。循环模型可以配置为输入和输出单个对象（例如，一个向量）或整个序列。这个观察结果使得序列到序列的编码器-解码器架构得以兴起，该架构因机器翻译而闻名，并且是著名论文“[注意力即一切](https://arxiv.org/abs/1706.03762)”中Transformer的原设计。在这里，目标是取一个输入序列（例如，一个英文句子），“编码”成捕获其“意义”的向量对象，然后“解码”该对象成另一个序列（例如，一个法语文句）。[d2l.ai](http://d2l.ai)的[第10章](https://d2l.ai/chapter_recurrent-modern/index.html)（10.6-10.8）也涵盖了这一设置，为[第11章](https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html)（直到11.7）中Transformer的编码器-解码器公式的介绍奠定了基础。出于历史原因，你当然应该至少浏览一下原始论文，尽管通过[“The
    Annotated Transformer”](https://nlp.seas.harvard.edu/annotated-transformer/)或如果你想要更多可视化，可能通过[“The
    Illustrated Transformer”](https://jalammar.github.io/illustrated-transformer/)的内容展示，你可能会得到更多收获。斯坦福CS224n的这些[笔记](https://web.stanford.edu/class/cs224n/readings/cs224n-self-attention-transformers-2023_draft.pdf)也非常好。
- en: There are videos on [encoder-decoder](https://www.youtube.com/watch?v=L8HKweZIOmg)
    architectures and [Attention](https://www.youtube.com/watch?v=PSs6nxngL6k) from
    StatQuest, a full walkthrough of the original Transformer by [The AI Hacker](https://www.youtube.com/watch?v=4Bdc55j80l8).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: StatQuest上有关于[编码器-解码器](https://www.youtube.com/watch?v=L8HKweZIOmg)架构和[注意力](https://www.youtube.com/watch?v=PSs6nxngL6k)的视频，[The
    AI Hacker](https://www.youtube.com/watch?v=4Bdc55j80l8)对原始Transformer的全面讲解。
- en: However, note that these encoder-decoder Transformers differ from most modern
    LLMs, which are typically “decoder-only” – if you’re pressed for time, you may
    be okay jumping right to these models and skipping the history lesson.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请注意，这些编码器-解码器Transformer与大多数现代LLMs不同，后者通常是“仅解码器”的——如果你时间紧迫，直接跳到这些模型并跳过历史课程可能没问题。
- en: Decoder-Only Transformers
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 仅解码器Transformer
- en: 'There’s a lot of moving pieces inside of Transformers — multi-head attention,
    skip connections, positional encoding, etc. — and it can be tough to appreciate
    it all the first time you see it. Building up intuitions for why some of these
    choices are made helps a lot, and here I’ll recommend to pretty much anyone that
    you watch a video or two about them (even if you’re normally a textbook learner),
    largely because there are a few videos which are really excellent:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer内部有很多移动部件——多头注意力、跳跃连接、位置编码等——第一次看到时可能很难全部理解。对为什么做出某些选择建立直觉有很大帮助，因此我建议几乎每个人都要看一些关于它们的视频（即使你通常是一个教科书学习者），主要是因为有一些视频真的很出色：
- en: '| 3Blue1Brown’s [“But what is a GPT?”](https://www.youtube.com/watch?v=wjZofJX0v4M)
    and [“Attention in transformers, explained visually”](Attention in transformers,
    visually explained | Chapter 6, Deep Learning) – beautiful animations + discussions,
    supposedly a 3rd video is on the way |'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_TB
  zh: '| 3Blue1Brown的[“But what is a GPT?”](https://www.youtube.com/watch?v=wjZofJX0v4M)和[“Attention
    in transformers, explained visually”](Attention in transformers, visually explained
    | 第6章，深度学习) – 美丽的动画+讨论，据说还有第三个视频在路上 |'
- en: Andrej Karpathy’s [“Let’s build GPT”](https://www.youtube.com/watch?v=kCc8FmEb1nY)
    video – live coding and excellent explanations, really helped some things “click”
    for me
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andrej Karpathy 的[“让我们构建 GPT”](https://www.youtube.com/watch?v=kCc8FmEb1nY)视频——现场编码和出色的解释，对我的一些理解起到了很大的帮助。
- en: Here’s a [blog post](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)
    from Cameron Wolfe walking through the decoder-only architecture in a similar
    style to the Illustrated/Annotated Transformer posts. There’s also a nice section
    in d2l.ai ([11.9](https://d2l.ai/chapter_attention-mechanisms-and-transformers/large-pretraining-transformers.html))
    covering the relationships between encoder-only, encoder-decoder, and decoder-only
    Transformers.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一篇由 Cameron Wolfe 撰写的[博客文章](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)，以类似《图解/注释
    Transformer》文章的风格介绍了仅解码器架构。在 d2l.ai 中也有一个很好的部分（[11.9](https://d2l.ai/chapter_attention-mechanisms-and-transformers/large-pretraining-transformers.html)）涵盖了仅编码器、编码器-解码器和仅解码器
    Transformer 之间的关系。
- en: 'Section III: Foundations for Modern Language Modeling'
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三部分：现代语言模型的基础
- en: '**Goal:** Survey central topics related to training LLMs, with an emphasis
    on conceptual primitives.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**目标：** 调查与训练 LLM 相关的核心主题，重点在于概念原语。'
- en: In this section, we’ll explore a number of concepts which will take us from
    the decoder-only Transformer architecture towards understanding the implementation
    choices and tradeoffs behind many of today’s frontier LLMs. If you first want
    a birds-eye view the of topics in section and some of the following ones, the
    post [“Understanding Large Language Models”](https://magazine.sebastianraschka.com/p/understanding-large-language-models)
    by Sebastian Raschka is a nice summary of what the LLM landscape looks like (at
    least up through mid-2023).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨一系列概念，这些概念将引导我们从仅解码器的 Transformer 架构过渡到理解当今许多前沿大型语言模型（LLM）背后的实现选择和权衡。如果你首先想对章节中的主题以及一些后续章节有一个鸟瞰图，Sebastian
    Raschka 的文章《[理解大型语言模型](https://magazine.sebastianraschka.com/p/understanding-large-language-models)》是一个很好的总结，概述了
    LLM 环境的概况（至少到 2023 年中为止）。
- en: Tokenization
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分词
- en: Character-level tokenization (like in several of the Karpathy videos) tends
    to be inefficient for large-scale Transformers vs. word-level tokenization, yet
    naively picking a fixed “dictionary” (e.g. Merriam-Webster) of full words runs
    the risk of encountering unseen words or misspellings at inference time. Instead,
    the typical approach is to use subword-level tokenization to “cover” the space
    of possible inputs, while maintaining the efficiency gains which come from a larger
    token pool, using algorithms like Byte-Pair Encoding (BPE) to select the appropriate
    set of tokens. If you’ve ever seen Huffman coding in an introductory algorithms
    class I think it’s a somewhat useful analogy for BPE here, although the input-output
    format is notably different, as we don’t know the set of “tokens” in advance.
    I’d recommend watching Andrej Karpathy’s [video](https://www.youtube.com/watch?v=zduSFxRajkE)
    on tokenization and checking out this tokenization [guide](https://blog.octanove.org/guide-to-subword-tokenization/)
    from Masato Hagiwara.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 字符级别的分词（如 Karpathy 的几个视频中所示）对于大规模 Transformer 来说通常效率较低，与词级别的分词相比，而天真地选择一个固定的“字典”（例如
    Merriam-Webster）的全词可能会在推理时遇到未见过的词或拼写错误。相反，典型的做法是使用子词级别的分词来“覆盖”可能的输入空间，同时保持来自更大分词池的效率提升，使用字节对编码（BPE）等算法来选择合适的标记集。如果你在入门算法课程中见过霍夫曼编码，我认为它在这里是一个有用的类比，尽管输入输出格式明显不同，因为我们事先不知道“标记”的集合。我建议观看
    Andrej Karpathy 的[视频](https://www.youtube.com/watch?v=zduSFxRajkE)关于分词，并查看 Masato
    Hagiwara 的这个分词[指南](https://blog.octanove.org/guide-to-subword-tokenization/)。
- en: Positional Encoding
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 位置编码
- en: As we saw in the past section, Transformers don’t natively have the same notion
    of adjacency or position within a context windows (in contrast to RNNs), and position
    must instead represented with some kind of vector encoding. While this could be
    done naively with something like one-hot encoding, this is impractical for context-scaling
    and suboptimal for learnability, as it throws away notions of ordinality. Originally,
    this was done with sinusoidal positional encodings, which may feel reminiscent
    of Fourier features if you’re familiar; the most popular implementation of this
    type of approach nowadays is likely Rotary Positional Encoding, or RoPE, which
    tends to be more stable and faster to learn during training.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一节中看到的，Transformer 并没有在上下文窗口内具有相同的邻接或位置概念（与 RNN 相比），位置必须用某种类型的向量编码来表示。虽然这可以通过类似
    one-hot 编码的简单方法来完成，但这对于上下文扩展来说是不切实际的，并且对于可学习性来说是不理想的，因为它丢弃了序数概念。最初，这是通过正弦位置编码来完成的，如果你熟悉傅里叶特征，可能会觉得有些熟悉；如今，这种类型方法最流行的实现可能是旋转位置编码（RoPE），它在训练期间通常更稳定且学习速度更快。
- en: 'Resources:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 资源：
- en: '[blog post](https://harrisonpim.com/blog/understanding-positional-embeddings-in-transformer-models)
    by Harrison Pim on intution for positional encodings'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Harrison Pim 的关于位置编码直觉的[博客文章](https://harrisonpim.com/blog/understanding-positional-embeddings-in-transformer-models)
- en: '[blog post](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/)
    by Mehreen Saeed on the original Transformer positional encodings'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mehreen Saeed 的关于原始 Transformer 位置编码的[博客文章](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/)
- en: '[blog post](https://blog.eleuther.ai/rotary-embeddings/) on RoPE from Eleuther
    AI original Transformer: https://kazemnejad.com/blog/transformer_architecture_positional_encoding/
    https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自 Eleuther AI 的关于 RoPE 的博客文章：[博客文章](https://blog.eleuther.ai/rotary-embeddings/)，原始
    Transformer：https://kazemnejad.com/blog/transformer_architecture_positional_encoding/
    https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/
- en: '[animated video](https://www.youtube.com/watch?v=GQPOtyITy54) from DeepLearning
    Hero'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeepLearning Hero 的[动画视频](https://www.youtube.com/watch?v=GQPOtyITy54)
- en: Pretraining Recipes
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预训练配方
- en: 'Once you’ve committed to pretraining a LLM of a certain general size on a particular
    corpus of data (e.g Common Crawl, FineWeb), there are still a number of choices
    to make before you’re ready to go:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你决定在特定数据集（例如 Common Crawl、FineWeb）上预训练一个特定大小的一般性大型语言模型（LLM），在你准备开始之前，仍有许多选择需要做出：
- en: Attention mechanisms (multi-head, multi-query, grouped-query)
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力机制（多头、多查询、分组查询）
- en: Activations (ReLU, GeLU, SwiGLU)
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数（ReLU, GeLU, SwiGLU）
- en: Optimizers, learning rates, and schedulers (AdamW, warmup, cosine decay)
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化器、学习率和调度器（AdamW、预热、余弦衰减）
- en: Dropout?
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dropout？
- en: Hyperparameter choices and search strategies
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数选择和搜索策略
- en: Batching, parallelization strategies, gradient accumulation
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批处理、并行化策略、梯度累积
- en: How long to train for, how often to repeat data
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练多长时间，多久重复一次数据
- en: …and many other axes of variation
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: …以及许多其他变体轴
- en: 'As far as I can tell, there’s not a one-size-fits-all rule book for how to
    go about this, but I’ll share a handful of worthwhile resources to consider, depending
    on your interests:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 就我所能了解的，并没有一个适用于所有情况的规则手册来指导如何进行这项工作，但我将分享一些值得考虑的有价值资源，具体取决于你的兴趣：
- en: While it predates the LLM era, the blog post [“A Recipe for Training Neural
    Networks”](https://karpathy.github.io/2019/04/25/recipe/) is a great starting
    point for framing this problem, as many of these questions are relevant throughout
    deep learning.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管这篇博客文章早于 LLM 时代，但[“训练神经网络的配方”](https://karpathy.github.io/2019/04/25/recipe/)是一篇很好的起点，因为这些问题在深度学习中的许多方面都是相关的。
- en: '[“The Novice’s LLM Training Guide”](https://rentry.org/llm-training) by Alpin
    Dale, discussing hyperparameter choices in practice, as well as the finetuning
    techniques we’ll see in future sections.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alpin Dale 的[“新手大型语言模型训练指南”](https://rentry.org/llm-training)，讨论了实践中的超参数选择，以及我们将在未来章节中看到的微调技术。
- en: '[“How to train your own Large Language Models”](https://blog.replit.com/llm-training)
    from Replit has some nice discussions on data pipelines and evaluations for training.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Replit 的[“如何训练你自己的大型语言模型”](https://blog.replit.com/llm-training)有一些关于数据管道和评估的精彩讨论。
- en: 'For understanding attention mechanism tradeoffs, see the post [“Navigating
    the Attention Landscape: MHA, MQA, and GQA Decoded”](https://iamshobhitagarwal.medium.com/navigating-the-attention-landscape-mha-mqa-and-gqa-decoded-288217d0a7d1)
    by Shobhit Agarwal.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了理解注意力机制权衡，请参阅Shobhit Agarwal的这篇[帖子“导航注意力景观：MHA，MQA和GQA解码”](https://iamshobhitagarwal.medium.com/navigating-the-attention-landscape-mha-mqa-and-gqa-decoded-288217d0a7d1)。
- en: 'For discussion of “popular defaults”, see the post [“The Evolution of the Modern
    Transformer: From ‘Attention Is All You Need’ to GQA, SwiGLU, and RoPE”](https://deci.ai/blog/evolution-of-modern-transformer-swiglu-rope-gqa-attention-is-all-you-need/)
    from Deci AI.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于“流行默认值”的讨论，请参阅Deci AI的这篇[帖子“现代Transformer的演变：从‘Attention Is All You Need’到GQA，SwiGLU和RoPE”](https://deci.ai/blog/evolution-of-modern-transformer-swiglu-rope-gqa-attention-is-all-you-need/)。
- en: For details on learning rate scheduling, see [Chapter 12.11](https://d2l.ai/chapter_optimization/lr-scheduler.html)
    from the d2l.ai book.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于学习率调度的详细信息，请参阅d2l.ai书籍中的第12.11章[链接](https://d2l.ai/chapter_optimization/lr-scheduler.html)。
- en: For discussion of some controversy surrounding reporting of “best practices”,
    see this [post](https://blog.eleuther.ai/nyt-yi-34b-response/) from Eleuther AI.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于围绕“最佳实践”报告的一些争议的讨论，请参阅Eleuther AI的这篇[帖子](https://blog.eleuther.ai/nyt-yi-34b-response/)。
- en: Distributed Training and FSDP
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布式训练和FSDP
- en: There are a number of additional challenges associated with training models
    which are too large to fit on individual GPUs (or even multi-GPU machines), typically
    necessitating the use of distributed training protocols like Fully Sharded Data
    Parallelism (FSDP), in which models can be co-located across machines during training.
    It’s probably worth also understanding its precursor Distributed Data Parallelism
    (DDP), which is covered in the first post linked below.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 与训练无法适应单个GPU（甚至多GPU机器）的模型相关联的许多其他挑战，通常需要使用分布式训练协议，如完全分片数据并行（FSDP），在训练期间模型可以在机器之间协同定位。也许也值得了解其前身分布式数据并行（DDP），这在下面链接的第一篇帖子中有介绍。
- en: 'Resources:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 资源：
- en: official FSDP [blog post](https://engineering.fb.com/2021/07/15/open-source/fsdp/)
    from Meta (who pioneered the method) https://sumanthrh.com/post/distributed-and-efficient-finetuning/
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meta的官方FSDP [博客文章](https://engineering.fb.com/2021/07/15/open-source/fsdp/)（该方法的开创者）
    https://sumanthrh.com/post/distributed-and-efficient-finetuning/
- en: '[blog post](https://blog.clika.io/fsdp-1/) on FSDP by Bar Rozenman, featuring
    many excellent visualizations'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bar Rozenman关于FSDP的[博客文章](https://blog.clika.io/fsdp-1/)，其中包含许多优秀的可视化
- en: '[report](https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness)
    from Yi Tai on the challenges of pretraining a model in a startup environment'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yi Tai关于在初创环境中预训练模型挑战的[报告](https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness)
- en: '[technical blog](https://www.answer.ai/posts/2024-03-14-fsdp-qlora-deep-dive.html)
    from Answer.AI on combining FSDP with parameter-efficient finetuning techniques
    for use on consumer GPUs'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Answer.AI关于将FSDP与参数高效微调技术结合使用以在消费级GPU上使用的[技术博客](https://www.answer.ai/posts/2024-03-14-fsdp-qlora-deep-dive.html)。
- en: Scaling Laws
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缩放定律
- en: It’s useful to know about scaling laws as a meta-topic which comes up a lot
    in discussions of LLMs (most prominently in reference to the “Chinchilla” [paper](https://arxiv.org/abs/2203.15556)),
    more so than any particular empirical finding or technique. In short, the performance
    which will result from scaling up the model, data, and compute used for training
    a language model results in fairly reliable predictions for model loss. This then
    enables calibration of optimal hyperparameter settings without needing to run
    expensive grid searches.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 了解缩放定律作为元主题是有用的，这在LLM的讨论中经常出现（最显著的是在“Chinchilla”[论文](https://arxiv.org/abs/2203.15556)的参考中），比任何特定的经验发现或技术都多。简而言之，通过扩展用于训练语言模型的模型、数据和计算，将产生对模型损失的相当可靠的预测。这然后使得无需运行昂贵的网格搜索即可校准最佳超参数设置。
- en: 'Resources:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 资源：
- en: '[Chinchilla Scaling Laws for Large Language Models](https://medium.com/@raniahossam/chinchilla-scaling-laws-for-large-language-models-llms-40c434e4e1c1)
    (blog overview by Rania Hossam)'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Chinchilla大型语言模型缩放定律](https://medium.com/@raniahossam/chinchilla-scaling-laws-for-large-language-models-llms-40c434e4e1c1)（Rania
    Hossam的博客概述）'
- en: '[New Scaling Laws for LLMs](https://www.lesswrong.com/posts/midXmMb2Xg37F2Kgn/new-scaling-laws-for-large-language-models)
    (discussion on LessWrong)'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[大型语言模型的新缩放定律](https://www.lesswrong.com/posts/midXmMb2Xg37F2Kgn/new-scaling-laws-for-large-language-models)（LessWrong上的讨论）'
- en: '[Chinchilla’s Wild Implications](https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications)
    (post on LessWrong)'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Chinchilla 的深远影响](https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications)（LessWrong上的帖子）'
- en: '[Chinchilla Scaling: A Replication Attempt](https://epochai.org/blog/chinchilla-scaling-a-replication-attempt)
    (potential issues with Chinchilla findings)'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Chinchilla 缩放：复制尝试](https://epochai.org/blog/chinchilla-scaling-a-replication-attempt)（Chinchilla
    发现的潜在问题）'
- en: '[Scaling Laws and Emergent Properties](https://cthiriet.com/blog/scaling-laws)
    (blog post by Clément Thiriet)'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[缩放定律和涌现特性](https://cthiriet.com/blog/scaling-laws)（Clément Thiriet的博客帖子）'
- en: '[“Scaling Language Models”](https://www.youtube.com/watch?v=UFem7xa3Q2Q) (video
    lecture, Stanford CS224n)'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“语言模型的缩放”](https://www.youtube.com/watch?v=UFem7xa3Q2Q)（斯坦福CS224n的视频讲座）'
- en: Mixture-of-Experts
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混合专家
- en: While many of the prominent LLMs (such as Llama3) used today are “dense” models
    (i.e. without enforced sparsification), Mixture-of-Experts (MoE) architectures
    are becoming increasingly popular for navigating tradeoffs between “knowledge”
    and efficiency, used perhaps most notably in the open-weights world by Mistral
    AI’s “Mixtral” models (8x7B and 8x22B), and [rumored](https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/)
    to be used for GPT-4\. In MoE models, only a fraction of the parameters are “active”
    for each step of inference, with trained router modules for selecting the parallel
    “experts” to use at each layer. This allows models to grow in size (and perhaps
    “knowlege” or “intelligence”) while remaining efficient for training or inference
    compared to a comparably-sized dense model.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然今天使用的许多知名LLM（如Llama3）都是“密集”模型（即没有强制稀疏化），但混合专家（MoE）架构在权衡“知识”和效率方面越来越受欢迎，最著名的是Mistral
    AI的“Mixtral”模型（8x7B和8x22B）在开放权重世界中的应用，以及[传闻](https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/)将用于GPT-4。在MoE模型中，只有一小部分参数在每个推理步骤中是“活跃”的，有训练好的路由模块用于在每个层选择并行的“专家”。这使得模型在大小（以及可能“知识”或“智能”）上可以增长，同时与同等规模的密集模型相比，在训练或推理上保持高效。
- en: See this [blog post](https://huggingface.co/blog/moe) from Hugging Face for
    a technical overview, and this [video](https://www.youtube.com/watch?v=0U_65fLoTq0)
    from Trelis Research for a visualized explainer.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅Hugging Face的这篇[博客文章](https://huggingface.co/blog/moe)以获取技术概述，以及Trelis Research的这篇[视频](https://www.youtube.com/watch?v=0U_65fLoTq0)以获取可视化解释。
- en: 'Section IV: Finetuning Methods for LLMs'
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四部分：LLM的微调方法
- en: '**Goal:** Survey techniques used for improving and "aligning" the quality of
    LLM outputs after pretraining.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '**目标：** 调查用于在预训练后改进和“对齐”LLM输出质量的技巧。'
- en: In pre-training, the goal is basically “predict the next token on random internet
    text”. While the resulting “base” models are still useful in some contexts, their
    outputs are often chaotic or “unaligned”, and they may not respect the format
    of a back-and-forth conversation. Here we’ll look at a set of techniques for going
    from these base models to ones resembling the friendly chatbots and assistants
    we’re more familiar with. A great companion resource, especially for this section,
    is Maxime Labonne’s interactive [LLM course](https://github.com/mlabonne/llm-course?tab=readme-ov-file)
    on Github.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练中，目标基本上是“在随机互联网文本上预测下一个标记”。虽然结果“基础”模型在某些情况下仍然有用，但它们的输出通常是混乱的或“未对齐”的，并且可能不尊重双向对话的格式。在这里，我们将探讨一系列从这些基础模型到更熟悉的友好聊天机器人和助手的技巧。一个很好的配套资源，特别是对于这一部分，是Maxime
    Labonne在GitHub上的交互式[LLM课程](https://github.com/mlabonne/llm-course?tab=readme-ov-file)。
- en: Instruct Fine-Tuning
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 指令微调
- en: Instruct fine-tuning (or “instruction tuning”, or “supervised finetuning”, or
    “chat tuning” – the boundaries here are a bit fuzzy) is the primary technique
    used (at least initially) for coaxing LLMs to conform to a particular style or
    format. Here, data is presented as a sequence of (input, output) pairs where the
    input is a user question to answer, and the model’s goal is to predict the output
    – typically this also involves adding special “start”/”stop”/”role” tokens and
    other masking techniques, enabling the model to “understand” the difference between
    the user’s input and its own outputs. This technique is also widely used for task-specific
    finetuning on datasets with a particular kind of problem structure (e.g. translation,
    math, general question-answering).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 指令微调（或称为“指令调整”、“监督微调”或“聊天调整”——这里的界限有些模糊）是用于引导大型语言模型（LLMs）遵循特定风格或格式的首要技术（至少最初是这样）。在这里，数据以一系列（输入，输出）对的形式呈现，其中输入是用户需要回答的问题，而模型的目标是预测输出——通常这也涉及到添加特殊的“开始”/“停止”/“角色”标记和其他掩码技术，使模型能够“理解”用户输入与其自身输出的区别。这种技术也广泛应用于具有特定问题结构的特定任务微调数据集（例如翻译、数学、通用问答）。
- en: See this [blog post](https://newsletter.ruder.io/p/instruction-tuning-vol-1)
    from Sebastian Ruder or this [video](https://www.youtube.com/watch?v=YoVek79LFe0)
    from Shayne Longpre for short overviews.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解简短概述，请参阅Sebastian Ruder的这篇[博客文章](https://newsletter.ruder.io/p/instruction-tuning-vol-1)或Shayne
    Longpre的这篇[视频](https://www.youtube.com/watch?v=YoVek79LFe0)。
- en: Low-Rank Adapters (LoRA)
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 低秩适配器（LoRA）
- en: While pre-training (and “full finetuning”) requires applying gradient updates
    to all parameters of a model, this is typically impractical on consumer GPUs or
    home setups; fortunately, it’s often possible to significantly reduce the compute
    requirements by using parameter-efficient finetuning (PEFT) techniques like Low-Rank
    Adapters (LoRA). This can enable competitive performance even with relatively
    small datasets, particularly for application-specific use cases. The main idea
    behind LoRA is to train each weight matrix in a low-rank space by “freezing” the
    base matrix and training a factored representation with much smaller inner dimension,
    which is then added to the base matrix.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然预训练（和“完全微调”）需要更新模型的所有参数的梯度，但在消费级GPU或家庭设置中通常不切实际；幸运的是，通过使用参数高效的微调（PEFT）技术，如低秩适配器（LoRA），通常可以显著降低计算需求。这可以在相对较小的数据集上实现有竞争力的性能，尤其是在特定应用场景中。LoRA背后的主要思想是通过“冻结”基矩阵并在具有更小内部维度的因子表示中训练来在低秩空间中训练每个权重矩阵，然后将该表示添加到基矩阵中。
- en: 'Resources:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 资源：
- en: LoRA paper walkthrough [(video, part 1)](https://youtu.be/dA-NhCtrrVE?si=TpJkPfYxngQQ0iGj)
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LoRA论文讲解 [(视频，第一部分)](https://youtu.be/dA-NhCtrrVE?si=TpJkPfYxngQQ0iGj)
- en: LoRA code demo [(video, part 2)](https://youtu.be/iYr1xZn26R8?si=aG0F8ws9XslpZ4ur)
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LoRA代码演示 [(视频，第二部分)](https://youtu.be/iYr1xZn26R8?si=aG0F8ws9XslpZ4ur)
- en: '[“Parameter-Efficient LLM Finetuning With Low-Rank Adaptation”](https://sebastianraschka.com/blog/2023/llm-finetuning-lora.html)
    by Sebastian Raschka'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“使用低秩适配器进行参数高效的LLM微调”](https://sebastianraschka.com/blog/2023/llm-finetuning-lora.html)
    by Sebastian Raschka'
- en: '[“Practical Tips for Finetuning LLMs Using LoRA”](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms)
    by Sebastian Raschka'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“使用LoRA微调LLMs的实用技巧”](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms)
    by Sebastian Raschka'
- en: Additionally, an “decomposed” LoRA variant called DoRA has been gaining popularity
    in recent months, often yielding performance improvements; see this [post](https://magazine.sebastianraschka.com/p/lora-and-dora-from-scratch)
    from Sebastian Raschka for more details.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一种称为DoRA的“分解”LoRA变体在最近几个月中越来越受欢迎，通常能带来性能提升；有关更多详细信息，请参阅Sebastian Raschka的这篇[文章](https://magazine.sebastianraschka.com/p/lora-and-dora-from-scratch)。
- en: Reward Models and RLHF
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 奖励模型和强化学习与人类反馈（RLHF）
- en: One of the most prominent techniques for “aligning” a language model is Reinforcement
    Learning from Human Feedback (RLHF); here, we typically assume that an LLM has
    already been instruction-tuned to respect a chat style, and that we additionally
    have a “reward model” which has been trained on human preferences. Given pairs
    of differing outputs to an input, where a preferred output has been chosen by
    a human, the learning objective of the reward model is to predict the preferred
    output, which involves implicitly learning preference “scores”. This allows bootstrapping
    a general representation of human preferences (at least with respect to the dataset
    of output pairs), which can be used as a “reward simulator” for continual training
    of a LLM using RL policy gradient techniques like PPO.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 将语言模型“对齐”的最突出技术之一是从人类反馈中进行强化学习（RLHF）；在这里，我们通常假设一个LLM已经被指令调整以尊重聊天风格，并且我们还有一个“奖励模型”，它已经在人类偏好上进行了训练。对于给定的输入的不同输出对，其中人类已经选择了一个首选输出，奖励模型的学习目标是预测首选输出，这涉及到隐式地学习偏好“分数”。这允许启动一个关于人类偏好的通用表示（至少就输出对的数据集而言），这可以用作使用RL策略梯度技术（如PPO）对LLM进行持续训练的“奖励模拟器”。
- en: For overviews, see the posts [“Illustrating Reinforcement Learning from Human
    Feedback (RLHF)”](https://huggingface.co/blog/rlhf) from Hugging Face and [“Reinforcement
    Learning from Human Feedback”](https://huyenchip.com/2023/05/02/rlhf.html) from
    Chip Huyen, and/or this [RLHF talk](https://www.youtube.com/watch?v=2MBJOuVq380)
    by Nathan Lambert. Further, this [post](https://sebastianraschka.com/blog/2024/research-papers-in-march-2024.html)
    from Sebastian Raschka dives into RewardBench, and how reward models themselves
    can be evaluated against each other by leveraging ideas from Direct Preference
    Optimization, another prominent approach for aligning LLMs with human preference
    data.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 对于概述，请参阅Hugging Face的帖子“从人类反馈中展示强化学习（RLHF）”和Chip Huyen的“从人类反馈中进行强化学习”，以及Nathan
    Lambert的[RLHF演讲](https://www.youtube.com/watch?v=2MBJOuVq380)。此外，Sebastian Raschka的这篇[文章](https://sebastianraschka.com/blog/2024/research-papers-in-march-2024.html)深入探讨了RewardBench，以及如何通过利用直接偏好优化（Direct
    Preference Optimization）的想法来评估奖励模型本身，这是另一种将LLMs与人类偏好数据对齐的突出方法。
- en: Direct Preference Optimization Methods
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 直接偏好优化方法
- en: The space of alignment algorithms seems to be following a similar trajectory
    as we saw with stochastic optimization algorithms a decade ago. In this an analogy,
    RLHF is like SGD — it works, it’s the original, and it’s also become kind of a
    generic “catch-all” term for the class of algorithms that have followed it. Perhaps
    DPO is AdaGrad, and in the year since its release there’s been a rapid wave of
    further algorithmic developments along the same lines (KTO, IPO, ORPO, etc.),
    whose relative merits are still under active debate. Maybe a year from now, everyone
    will have settled on a standard approach which will become the “Adam” of alignment.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 对齐算法的空间似乎正在遵循与我们在十年前看到的随机优化算法相似的轨迹。在这个类比中，RLHF就像SGD——它有效，它是原始的，它也已经成为一个通用的“万能”术语，用于指代随后出现的算法类别。也许DPO是AdaGrad，自从其发布以来，沿着相同方向的算法发展迅速（KTO、IPO、ORPO等），它们的相对优势仍在积极讨论中。也许一年后，每个人都会选择一个标准方法，这将成为对齐的“Adam”。
- en: For an overview of the theory behind DPO see this [blog post](https://towardsdatascience.com/understanding-the-implications-of-direct-preference-optimization-a4bbd2d85841)
    Matthew Gunton; this [blog post](https://huggingface.co/blog/dpo-trl) from Hugging
    Face features some code and demonstrates how to make use of DPO in practice. Another
    [blog post](https://huggingface.co/blog/pref-tuning) from Hugging Face also discusses
    tradeoffs between a number of the DPO-flavored methods which have emerged in recent
    months.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 关于DPO背后的理论的概述，请参阅Matthew Gunton的这篇[博客文章](https://towardsdatascience.com/understanding-the-implications-of-direct-preference-optimization-a4bbd2d85841)；Hugging
    Face的这篇[博客文章](https://huggingface.co/blog/dpo-trl)包含一些代码，并演示了如何在实践中使用DPO。Hugging
    Face的另一篇[博客文章](https://huggingface.co/blog/pref-tuning)也讨论了最近几个月出现的几种DPO风格的方法的权衡。
- en: Context Scaling
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 上下文缩放
- en: Beyond task specification or alignment, another common goal of finetuning is
    to increase the effective context length of a model, either via additional training,
    adjusting parameters for positional encodings, or both. Even if adding more tokens
    to a model’s context can “type-check”, training on additional longer examples
    is generally necessary if the model may not have seen such long sequences during
    pretraining.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 除了任务指定或对齐之外，微调的另一个常见目标是增加模型的有效上下文长度，无论是通过额外训练、调整位置编码参数，还是两者兼而有之。即使向模型上下文中添加更多标记可以进行“类型检查”，但如果模型在预训练期间可能没有看到如此长的序列，那么在额外的更长的示例上进行训练通常是必要的。
- en: 'Resources:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 资源：
- en: '[“Scaling Rotational Embeddings for Long-Context Language Models”](https://gradient.ai/blog/scaling-rotational-embeddings-for-long-context-language-models)
    by Gradient AI'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“扩展旋转嵌入以用于长上下文语言模型”](https://gradient.ai/blog/scaling-rotational-embeddings-for-long-context-language-models)
    by Gradient AI'
- en: '[“Extending the RoPE”](https://blog.eleuther.ai/yarn/) by Eleuther AI, introducing
    the YaRN method for increased context via attention temperature scaling'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“扩展 RoPE”](https://blog.eleuther.ai/yarn/) by Eleuther AI，介绍通过注意力温度缩放增加上下文的方法
    YaRN'
- en: '[“Everything About Long Context Fine-tuning”](https://huggingface.co/blog/wenbopan/long-context-fine-tuning)
    by Wenbo Pan'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“关于长上下文微调的一切”](https://huggingface.co/blog/wenbopan/long-context-fine-tuning)
    by Wenbo Pan'
- en: Distillation and Merging
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 离心蒸馏和合并
- en: Here we’ll look at two very different methods of consolidating knowledge across
    LLMs — distillation and merging. Distillation was first popularized for BERT models,
    where the goal is to “distill” the knowledge and performance of a larger model
    into a smaller one (at least for some tasks) by having it serve as a “teacher”
    during the smaller model’s training, bypassing the need for large quantities of
    human-labeled data.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将探讨两种在 LLM 之间巩固知识的不同方法——蒸馏和合并。蒸馏最初是为 BERT 模型推广的，其目标是“蒸馏”较大模型的知识和性能，将其转化为较小的模型（至少对于某些任务），在较小模型训练期间充当“教师”，从而绕过需要大量人工标注数据的需求。
- en: 'Some resources on distillation:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 一些关于蒸馏的资源：
- en: '[“Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version
    of BERT”](https://medium.com/huggingface/distilbert-8cf3380435b5) from Hugging
    Face'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“更小、更快、更便宜、更轻：介绍 DistilBERT，BERT 的蒸馏版本”](https://medium.com/huggingface/distilbert-8cf3380435b5)
    from Hugging Face'
- en: '[“LLM distillation demystified: a complete guide”](https://snorkel.ai/llm-distillation-demystified-a-complete-guide/)
    from Snorkel AI'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“LLM 蒸馏揭秘：完整指南”](https://snorkel.ai/llm-distillation-demystified-a-complete-guide/)
    from Snorkel AI'
- en: '[“Distilling Step by Step” blog](https://blog.research.google/2023/09/distilling-step-by-step-outperforming.html)
    from Google Research'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“逐步蒸馏”博客](https://blog.research.google/2023/09/distilling-step-by-step-outperforming.html)
    from Google Research'
- en: Merging, on the other hand, is much more of a “wild west” technique, largely
    used by open-source engineers who want to combine the strengths of multiple finetuning
    efforts. It’s kind of wild to me that it works at all, and perhaps grants some
    credence to “linear representation hypotheses” (which will appear in the next
    section when we discuss interpretability). The idea is basically to take two different
    finetunes of the same base model and just average their weights. No training required.
    Technically, it’s usually “spherical interpolation” (or “slerp”), but this is
    pretty much just fancy averaging with a normalization step. For more details,
    see the post [Merge Large Language Models with mergekit](https://huggingface.co/blog/mlabonne/merge-models)
    by Maxime Labonne.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，合并是一种更接近“狂野西部”的技术，主要被开源工程师使用，他们希望结合多个微调工作的优势。对我来说，它竟然能工作，也许为“线性表示假设”（将在下一节讨论可解释性时出现）提供了一些可信度。基本想法是取两个相同基础模型的不同的微调版本，并简单地平均它们的权重。无需训练。技术上，这通常是“球面插值”（或“slerp”），但这基本上只是带有归一化步骤的复杂平均。更多详情，请参阅
    Maxime Labonne 的文章 [使用 mergekit 合并大型语言模型](https://huggingface.co/blog/mlabonne/merge-models)。
- en: 'Section V: LLM Evaluations and Applications'
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五节：LLM 评估和应用
- en: '**Goal:** Survey how LLMs are used and evaluated in practice, beyond just "chatbots".'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '**目标：** 调查 LLM 在实践中如何被使用和评估，而不仅仅是“聊天机器人”。'
- en: Here we’ll be looking at a handful of topics related to improving or modifying
    the performance of language models without additional training, as well as techniques
    for measuring and understanding their performance.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将探讨一些与改进或修改语言模型性能（无需额外训练）相关的话题，以及衡量和理解其性能的技术。
- en: 'Before diving into the individual chapters, I’d recommend these two high-level
    overviews, which touch on many of the topics we’ll examine here:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入各个章节之前，我推荐这两篇高级概述，它们涉及了我们在这里将要探讨的许多主题：
- en: '[“Building LLM applications for production”](https://huyenchip.com/2023/04/11/llm-engineering.html)
    by Chip Huyen'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chip Huyen的[“为生产构建LLM应用”](https://huyenchip.com/2023/04/11/llm-engineering.html)
- en: '[“What We Learned from a Year of Building with LLMs” Part 1](https://www.oreilly.com/radar/what-we-learned-from-a-year-of-building-with-llms-part-i/)
    and [Part 2](https://www.oreilly.com/radar/what-we-learned-from-a-year-of-building-with-llms-part-ii/)
    from O’Reilly (several authors)'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: O’Reilly的[“从一年使用LLM构建中学到的经验”第一部分](https://www.oreilly.com/radar/what-we-learned-from-a-year-of-building-with-llms-part-i/)和[第二部分](https://www.oreilly.com/radar/what-we-learned-from-a-year-of-building-with-llms-part-ii/)
- en: 'These web courses also have a lot of relevant interactive materials:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 这些网络课程也有很多相关的互动材料：
- en: '[“Large Language Model Course”](https://github.com/mlabonne/llm-course) from
    Maxime Labonne'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自Maxime Labonne的[“大型语言模型课程”](https://github.com/mlabonne/llm-course)
- en: '[“Generative AI for Beginners”](https://microsoft.github.io/generative-ai-for-beginners/)
    from Microsoft'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自微软的[“面向初学者的生成式AI”](https://microsoft.github.io/generative-ai-for-beginners/)
- en: Benchmarking
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基准测试
- en: 'Beyond the standard numerical performance measures used during LLM training
    like cross-entropy loss and [perplexity](https://medium.com/@priyankads/perplexity-of-language-models-41160427ed72),
    the true performance of frontier LLMs is more commonly judged according to a range
    of benchmarks, or “evals”. Common types of these are:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在LLM训练期间使用的标准数值性能指标（如交叉熵损失和[困惑度](https://medium.com/@priyankads/perplexity-of-language-models-41160427ed72)）之外，前沿LLM的真实性能更常见的是根据一系列基准或“评估”来判断。这些常见的类型包括：
- en: Human-evaluated outputs (e.g. [LMSYS Chatbot Arena](https://chat.lmsys.org/))
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工评估输出（例如 [LMSYS Chatbot Arena](https://chat.lmsys.org/)）
- en: AI-evaluated outputs (as used in [RLAIF](https://argilla.io/blog/mantisnlp-rlhf-part-4/))
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI评估输出（如[RLAIF](https://argilla.io/blog/mantisnlp-rlhf-part-4/)中使用）
- en: Challenge question sets (e.g. those in HuggingFace’s [LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard))
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 挑战性问题集（例如HuggingFace的[LLM排行榜](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)中的）
- en: See the [slides](https://web.stanford.edu/class/cs224n/slides/cs224n-spr2024-lecture11-evaluation-yann.pdf)
    from Stanford’s CS224n for an overview. This [blog post](https://www.jasonwei.net/blog/evals)
    by Jason Wei and [this one](https://humanloop.com/blog/evaluating-llm-apps?utm_source=newsletter&utm_medium=sequence&utm_campaign=)
    by Peter Hayes do a nice job discussing the challenges and tradeoffs associated
    with designing good evaluations, and highlighting a number of the most prominent
    ones used today. The documentation for the open source framework [inspect-ai](https://ukgovernmentbeis.github.io/inspect_ai/)
    also features some useful discussion around designing benchmarks and reliable
    evaluation pipelines.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 查看斯坦福大学CS224n的[幻灯片](https://web.stanford.edu/class/cs224n/slides/cs224n-spr2024-lecture11-evaluation-yann.pdf)，以了解概述。Jason
    Wei的这篇[博客文章](https://www.jasonwei.net/blog/evals)和Peter Hayes的这篇[文章](https://humanloop.com/blog/evaluating-llm-apps?utm_source=newsletter&utm_medium=sequence&utm_campaign=)很好地讨论了设计良好评估的挑战和权衡，并突出了今天使用的一些最突出的评估方法。开源框架[inspect-ai](https://ukgovernmentbeis.github.io/inspect_ai/)的文档也包含了一些关于设计基准和可靠评估管道的有用讨论。
- en: Sampling and Structured Outputs
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 样本和结构化输出
- en: While typical LLM inference samples tokens one at a time, there are number of
    parameters controlling the token distribution (temperature, top_p, top_k) which
    can be modified to control the variety of responses, as well as non-greedy decoding
    strategies that allow some degree of “lookahead”. This [blog post](https://towardsdatascience.com/decoding-strategies-in-large-language-models-9733a8f70539)
    by Maxime Labonne does a nice job discussing several of them.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 当典型的LLM推理一次处理一个标记时，有一些参数（如温度、top_p、top_k）控制标记分布，可以修改以控制响应的多样性，以及允许一定程度的“前瞻”的非贪婪解码策略。Maxime
    Labonne的这篇[博客文章](https://towardsdatascience.com/decoding-strategies-in-large-language-models-9733a8f70539)很好地讨论了其中的一些。
- en: 'Sometimes we also want our outputs to follow a particular structure, particularly
    if we are using LLMs as a component of a larger system rather than as just a chat
    interface. Few-shot prompting works okay, but not all the time, particularly as
    output schemas become more complicated. For schema types like JSON, Pydantic and
    Outlines are popular tools for constraining the output structure from LLMs. Some
    useful resources:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们还想让我们的输出遵循特定的结构，尤其是如果我们将LLMs作为更大系统的一部分而不是仅仅作为聊天界面来使用时。少样本提示效果不错，但并不总是如此，尤其是当输出模式变得更加复杂时。对于JSON、Pydantic和Outlines等模式类型，是限制LLMs输出结构的流行工具。一些有用的资源：
- en: '[Pydantic Concepts](https://docs.pydantic.dev/latest/concepts/models/)'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Pydantic概念](https://docs.pydantic.dev/latest/concepts/models/)'
- en: '[Outlines for JSON](https://outlines-dev.github.io/outlines/reference/json/)'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[JSON大纲](https://outlines-dev.github.io/outlines/reference/json/)'
- en: '[Outlines review](https://michaelwornow.net/2023/12/29/outlines-demo) by Michael
    Wornow'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[大纲评论](https://michaelwornow.net/2023/12/29/outlines-demo) by Michael Wornow'
- en: Prompting Techniques
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示技术
- en: 'There are many prompting techniques, and many more prompt engineering guides
    out there, featuring methods for coaxing more desirable outputs from LLMs. Some
    of the classics:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多提示技术，还有更多提示工程指南，它们展示了如何从LLMs中诱导出更令人满意的输出。其中一些经典方法：
- en: Few-Shot Examples
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 少样本示例
- en: Chain-of-Thought
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 思维链
- en: Retrieval-Augmented Generation (RAG)
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）
- en: ReAct
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReAct
- en: This [blog post](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)
    by Lilian Weng discusses several of the most dominant approaches, [guide](https://www.promptingguide.ai/techniques)
    has decent coverage and examples for a wider range of the prominent techniques
    used today. Keyword-searching on Twitter/X or LinkedIn will give you plenty more.
    We’ll also dig deeper into RAG and agent methods in later chapters.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: Lilian Weng的这篇[博客文章](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)讨论了几种最占主导地位的方法，[指南](https://www.promptingguide.ai/techniques)对今天广泛使用的突出技术有很好的覆盖和示例。在Twitter/X或LinkedIn上进行关键词搜索将为您提供更多内容。我们还将深入探讨RAG和代理方法，在后面的章节中。
- en: Vector Databases and Reranking
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量数据库和重排序
- en: RAG systems require the ability to quickly retrieve relevant documents from
    large corpuses. Relevancy is typically determined by similarity measures for semantic
    [embedding](#embeddings-and-topic-modeling) vectors of both queries and documents,
    such as cosine similarity or Euclidean distance. If we have just a handful of
    documents, this can be computed between a query and each document, but this quickly
    becomes intractable when the number of documents grows large. This is the problem
    addressed by vector databases, which allow retrieval of the *approximate* top-K
    matches (significantly faster than checking all pairs) by maintaining high-dimensional
    indices over vectors which efficiently encode their geometric structure. These
    [docs](https://www.pinecone.io/learn/series/faiss/) from Pinecone do a nice job
    walking through a few different methods for vector storage, like Locality-Sensitive
    Hashing and Hierarchical Navigable Small Worlds, which can be implemented with
    the popular FAISS open-source library. This [talk](https://www.youtube.com/watch?v=W-i8bcxkXok)
    by Alexander Chatzizacharias gives a nice overview as well.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: RAG系统需要能够快速从大型语料库中检索相关文档的能力。相关性通常由查询和文档的语义[嵌入](#embeddings-and-topic-modeling)向量的相似度度量来确定，例如余弦相似度或欧几里得距离。如果我们只有少量文档，这可以通过查询与每个文档之间的计算来完成，但当文档数量增加时，这很快就会变得难以处理。这是向量数据库解决的问题，它通过在向量上维护高维索引来允许检索*近似*的top-K匹配（比检查所有对要快得多），这些向量有效地编码了它们的几何结构。请参阅Pinecone的这些[文档](https://www.pinecone.io/learn/series/faiss/)，它们很好地介绍了几种不同的向量存储方法，如局部敏感哈希和分层可导航小世界，这些方法可以使用流行的FAISS开源库实现。Alexander
    Chatzizacharias的这篇[演讲](https://www.youtube.com/watch?v=W-i8bcxkXok)也提供了一个很好的概述。
- en: Another related application of vector retrieval is the “reranking” problem,
    wherein a model can optimize for other metrics beyond query similarity, such as
    diversity within retrieved results. See these [docs](https://www.pinecone.io/learn/series/rag/rerankers/)
    from Pinecone for an overview. We’ll see more about how retrieved results are
    actually used by LLMs in the next chapter.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 向量检索的另一个相关应用是“重排序”问题，其中模型可以优化除查询相似度之外的指标，例如检索结果中的多样性。请参阅Pinecone的这些[文档](https://www.pinecone.io/learn/series/rag/rerankers/)以获取概述。我们将在下一章中了解更多关于LLMs实际如何使用检索结果的信息。
- en: Retrieval-Augmented Generation
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检索增强生成
- en: One of the most buzzed-about uses of LLMs over the past year, retrieval-augmented
    generation (RAG) is how you can “chat with a PDF” (if larger than a model’s context)
    and how applications like Perplexity and Arc Search can “ground” their outputs
    using web sources. This retrieval is generally powered by embedding each document
    for storage in a vector database + querying with the relevant section of a user’s
    input.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 过去一年中最受热议的LLMs应用之一，检索增强生成（RAG），就是如何“与PDF聊天”（如果内容大于模型上下文），以及像Perplexity和Arc Search这样的应用如何使用网络资源“定位”它们的输出。这种检索通常是通过将每个文档嵌入到向量数据库中进行存储，然后使用用户输入的相关部分进行查询来实现的。
- en: 'Some overviews:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 一些概述：
- en: '[“Deconstructing RAG”](https://blog.langchain.dev/deconstructing-rag/) from
    Langchain'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“RAG解构”](https://blog.langchain.dev/deconstructing-rag/) from Langchain'
- en: '[“Building RAG with Open-Source and Custom AI Models”](https://www.bentoml.com/blog/building-rag-with-open-source-and-custom-ai-models)
    from Chaoyu Yang'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“使用开源和自定义AI模型构建RAG”](https://www.bentoml.com/blog/building-rag-with-open-source-and-custom-ai-models)
    from Chaoyu Yang'
- en: The [Advanced RAG](https://learn.deeplearning.ai/courses/building-evaluating-advanced-rag/lesson/1/introduction)
    video course from DeepLearning.AI may also be useful for exploring variants on
    the standard setup.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: DeepLearning.AI的[高级RAG](https://learn.deeplearning.ai/courses/building-evaluating-advanced-rag/lesson/1/introduction)视频课程也可能有助于探索标准设置的变体。
- en: Tool Use and "Agents"
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工具使用和“代理”
- en: The other big application buzzwords you’ve most likely encountered in some form
    are “tool use” and “agents”, or “agentic programming”. This typically starts with
    the ReAct framework we saw in the prompting section, then gets extended to elicit
    increasingly complex behaviors like software engineering (see the much-buzzed-about
    “Devin” system from Cognition, and several related open-source efforts like Devon/OpenDevin/SWE-Agent).
    There are many programming frameworks for building agent systems on top of LLMs,
    with Langchain and LlamaIndex being two of the most popular. There also seems
    to be some value in having LLMs rewrite their own prompts + evaluate their own
    partial outputs; this observation is at the heart of the DSPy framework (for “compiling”
    a program’s prompts, against a reference set of instructions or desired outputs)
    which has recently been seeing a lot of attention.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能以某种形式遇到的其他大型应用热门词汇是“工具使用”和“代理”，或“代理编程”。这通常从我们在提示部分看到的ReAct框架开始，然后扩展到引发越来越复杂的行为，如软件工程（参见备受瞩目的“Devin”系统，来自Cognition，以及几个相关的开源项目，如Devon/OpenDevin/SWE-Agent）。有许多编程框架可以用于在LLMs之上构建代理系统，其中Langchain和LlamaIndex是最受欢迎的两个。似乎还有价值让LLMs重写它们自己的提示并评估它们自己的部分输出；这一观察是DSPy框架（用于“编译”程序的提示，与一组参考指令或期望输出相对）的核心，该框架最近受到了很多关注。
- en: 'Resources:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 资源：
- en: '[“LLM Powered Autonomous Agents” (post)](https://lilianweng.github.io/posts/2023-06-23-agent/)
    from Lilian Weng'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“LLM驱动的自主代理” (文章)](https://lilianweng.github.io/posts/2023-06-23-agent/) from
    Lilian Weng'
- en: '[“A Guide to LLM Abstractions” (post)](https://www.twosigma.com/articles/a-guide-to-large-language-model-abstractions/)
    from Two Sigma'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“LLM抽象指南” (文章)](https://www.twosigma.com/articles/a-guide-to-large-language-model-abstractions/)
    from Two Sigma'
- en: '[“DSPy Explained! (video)”](https://www.youtube.com/watch?v=41EfOY0Ldkc) by
    Connor Shorten'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“DSPy Explained! (视频)”](https://www.youtube.com/watch?v=41EfOY0Ldkc) by Connor
    Shorten'
- en: Also relevant are more narrowly-tailored (but perhaps more practical) applications
    related to databases — see these two [blog](https://neo4j.com/developer-blog/knowledge-graphs-llms-multi-hop-question-answering/)
    [posts](https://neo4j.com/blog/unifying-llm-knowledge-graph/) from Neo4J for discussion
    on applying LLMs to analyzing or constructing knowledge graphs, or this [blog
    post](https://numbersstation.ai/data-wrangling-with-fms-2/) from Numbers Station
    about applying LLMs to data wrangling tasks like entity matching.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 也相关的是一些更具体（但可能更实用）的应用，这些应用与数据库相关——参见Neo4J的这两篇[博客](https://neo4j.com/developer-blog/knowledge-graphs-llms-multi-hop-question-answering/)
    [文章](https://neo4j.com/blog/unifying-llm-knowledge-graph/)，讨论了将LLMs应用于分析或构建知识图谱，或者查看Numbers
    Station的这篇[博客文章](https://numbersstation.ai/data-wrangling-with-fms-2/)，关于将LLMs应用于实体匹配等数据整理任务。
- en: LLMs for Synthetic Data
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 合成数据的LLMs
- en: An increasing number of applications are making use of LLM-generated data for
    training or evaluations, including distillation, dataset augmentation, AI-assisted
    evaluation and labeling, self-critique, and more. This [post](https://www.promptingguide.ai/applications/synthetic_rag)
    demonstrates how to construct such a synthetic dataset (in a RAG context), and
    this [post](https://argilla.io/blog/mantisnlp-rlhf-part-4/) from Argilla gives
    an overview of RLAIF, which is often a popular alternative to RLHF, given the
    challenges associated with gathering pairwise human preference data. AI-assisted
    feedback is also a central component of the “Constitutional AI” alignment method
    pioneered by Anthropic (see their [blog](https://www.anthropic.com/news/claudes-constitution)
    for an overview).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 越来越多的应用正在利用LLM生成的数据进行训练或评估，包括蒸馏、数据集增强、AI辅助评估和标记、自我批评等。这篇[文章](https://www.promptingguide.ai/applications/synthetic_rag)展示了如何在RAG环境中构建这样的合成数据集，以及来自Argilla的这篇[文章](https://argilla.io/blog/mantisnlp-rlhf-part-4/)概述了RLAIF，鉴于收集成对人类偏好数据所面临的挑战，RLAIF通常是RLHF的一个流行替代方案。AI辅助反馈也是Anthropic开创的“宪法AI”对齐方法的一个核心组成部分（请参阅他们的[博客](https://www.anthropic.com/news/claudes-constitution)以获得概述）。
- en: Representation Engineering
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 表示工程
- en: Representation Engineering is a new and promising technique for fine-grained
    steering of language model outputs via “control vectors”. Somewhat similar to
    LoRA adapters, it has the effect of adding low-rank biases to the weights of a
    network which can elicit particular response styles (e.g. “humorous”, “verbose”,
    “creative”, “honest”), yet is much more computationally efficient and can be implemented
    without any training required. Instead, the method simply looks at differences
    in activations for pairs of inputs which vary along the axis of interest (e.g.
    honesty), which can be generated synthetically, and then performs dimensionality
    reduction.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 表示工程是一种通过“控制向量”对语言模型输出进行细粒度引导的新兴且具有前景的技术。它在某种程度上类似于LoRA适配器，它通过向网络权重添加低秩偏差来引发特定的响应风格（例如“幽默”、“冗长”、“创新”、“诚实”），但计算效率更高，并且可以在不进行任何训练的情况下实现。相反，该方法只是观察沿着感兴趣轴（例如诚实）变化的输入对激活的差异，这些差异可以人工生成，然后进行降维。
- en: See this short [blog post](https://www.safe.ai/blog/representation-engineering-a-new-way-of-understanding-models)
    from Center for AI Safety (who pioneered the method) for a brief overview, and
    this [post](https://vgel.me/posts/representation-engineering/) from Theia Vogel
    for a technical deep-dive with code examples. Theia also walks through the method
    in this [podcast episode](https://www.youtube.com/watch?v=PkA4DskA-6M).
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅来自AI安全中心（该方法的开创者）的这篇简短[博客文章](https://www.safe.ai/blog/representation-engineering-a-new-way-of-understanding-models)，以获得简要概述，以及来自Theia
    Vogel的这篇[文章](https://vgel.me/posts/representation-engineering/)，其中包含带有代码示例的技术深入探讨。Theia还在这个[podcast
    episode](https://www.youtube.com/watch?v=PkA4DskA-6M)中介绍了该方法。
- en: Mechanistic Interpretability
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机制可解释性
- en: 'Mechanistic Interpretability (MI) is the dominant paradigm for understanding
    the inner workings of LLMs by identifying sparse representations of “features”
    or “circuits” encoded in model weights. Beyond enabling potential modification
    or explanation of LLM outputs, MI is often viewed as an important step towards
    potentially “aligning” increasingly powerful systems. Most of the references here
    will come from [Neel Nanda](https://www.neelnanda.io), a leading researcher in
    the field who’s created a large number of useful educational resources about MI
    across a range of formats:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 机制可解释性（MI）是通过识别编码在模型权重中的“特征”或“电路”的稀疏表示来理解LLM内部工作原理的主导范式。除了使潜在修改或解释LLM输出成为可能之外，MI通常被视为向可能“对齐”日益强大的系统的重要一步。这里的大部分参考文献将来自[Neel
    Nanda](https://www.neelnanda.io)，他是该领域的领先研究人员，在多种格式下创建了大量关于MI的有用教育资源：
- en: '[“A Comprehensive Mechanistic Interpretability Explainer & Glossary”](https://www.neelnanda.io/mechanistic-interpretability/glossary)'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“全面机制可解释性解释与术语表”](https://www.neelnanda.io/mechanistic-interpretability/glossary)'
- en: '[“An Extremely Opinionated Annotated List of My Favourite Mechanistic Interpretability
    Papers”](https://www.neelnanda.io/mechanistic-interpretability/favourite-papers)'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“我最喜欢的机制可解释性论文的极有见地的注释列表”](https://www.neelnanda.io/mechanistic-interpretability/favourite-papers)'
- en: '[“Mechanistic Interpretability Quickstart Guide”](https://www.lesswrong.com/posts/jLAvJt8wuSFySN975/mechanistic-interpretability-quickstart-guide)
    (Neel Nanda on LessWrong)'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“机制可解释性快速入门指南”](https://www.lesswrong.com/posts/jLAvJt8wuSFySN975/mechanistic-interpretability-quickstart-guide)
    (Neel Nanda on LessWrong)'
- en: '[“How useful is mechanistic interpretability?”](https://www.lesswrong.com/posts/tEPHGZAb63dfq2v8n/how-useful-is-mechanistic-interpretability)
    (Neel and others, discussion on LessWrong)'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“机制可解释性有多有用？”](https://www.lesswrong.com/posts/tEPHGZAb63dfq2v8n/how-useful-is-mechanistic-interpretability)
    (Neel等人，在LessWrong上的讨论)'
- en: '[“200 Concrete Problems In Interpretability”](https://docs.google.com/spreadsheets/d/1oOdrQ80jDK-aGn-EVdDt3dg65GhmzrvBWzJ6MUZB8n4/edit#gid=0)
    (Annotated spreadsheet of open problems from Neel)'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“200个可解释性问题”](https://docs.google.com/spreadsheets/d/1oOdrQ80jDK-aGn-EVdDt3dg65GhmzrvBWzJ6MUZB8n4/edit#gid=0)
    (Neel的开放问题标注电子表格)'
- en: 'Additionally, the articles [“Toy Models of Superposition”](https://transformer-circuits.pub/2022/toy_model/index.html)
    and [“Scaling Monosemanticity: Extracting Interpretable Features from Claude 3
    Sonnet”](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)
    from Anthropic are on the longer side, but feature a number of great visualizations
    and demonstrations of these concepts.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Anthropic的文章[“叠加的玩具模型”](https://transformer-circuits.pub/2022/toy_model/index.html)和[“扩展单义性：从Claude
    3 Sonnet中提取可解释特征”](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)篇幅较长，但包含了许多这些概念的优秀可视化演示。
- en: Linear Representation Hypotheses
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性表示假设
- en: An emerging theme from several lines of interpretability research has been the
    observation that internal representations of features in Transformers are often
    “linear” in high-dimensional space (a la Word2Vec). On one hand this may appear
    initially surprising, but it’s also essentially an implicit assumption for techniques
    like similarity-based retrieval, merging, and the key-value similarity scores
    used by attention. See this [blog post](https://www.beren.io/2023-04-04-DL-models-are-secretly-linear/)
    by Beren Millidge, this [talk](https://www.youtube.com/watch?v=ko1xVcyDt8w) from
    Kiho Park, and perhaps at least skim the paper [“Language Models Represent Space
    and Time”](https://arxiv.org/pdf/2310.02207) for its figures.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 从几条可解释性研究线路中涌现的一个主题是观察，Transformer中的特征内部表示在高维空间中通常是“线性”的（类似于Word2Vec）。一方面，这可能会一开始看起来令人惊讶，但这也是像基于相似度的检索、合并以及注意力中使用的键值相似度分数等技术的一个基本隐含假设。参见Beren
    Millidge的这篇[博客文章](https://www.beren.io/2023-04-04-DL-models-are-secretly-linear/)，Kiho
    Park的这篇[演讲](https://www.youtube.com/watch?v=ko1xVcyDt8w)，以及至少浏览一下这篇论文[“语言模型表示空间和时间”](https://arxiv.org/pdf/2310.02207)的图表。
- en: 'Section VI: Performance Optimizations for Efficient Inference'
  id: totrans-286
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第VI节：高效推理的性能优化
- en: '**Goal:** Survey architecture choices and lower-level techniques for improving
    resource utilization (time, compute, memory).'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '**目标：** 调查架构选择和底层技术，以改善资源利用（时间、计算、内存）。'
- en: Here we’ll look at a handful of techniques for improving the speed and efficiency
    of inference from pre-trained Transformer language models, most of which are fairly
    widely used in practice. It’s worth first reading this short Nvidia [blog post](https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/)
    for a crash course in several of the topics we’ll look at (and a number of others).
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将探讨一些提高从预训练Transformer语言模型中进行推理的速度和效率的技术，其中大多数在实践中都相当广泛地使用。首先阅读这篇Nvidia的[博客文章](https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/)，以了解我们将要探讨的几个主题（以及许多其他主题）的快速入门课程是值得的。
- en: Parameter Quantization
  id: totrans-289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参数量化
- en: With the rapid increase in parameter counts for leading LLMs and difficulties
    (both in cost and availability) in acquiring GPUs to run models on, there’s been
    a growing interest in quantizing LLM weights to use fewer bits each, which can
    often yield comparable output quality with a 50-75% (or more) reduction in required
    memory. Typically this shouldn’t be done naively; Tim Dettmers, one of the pioneers
    of several modern quantization methods (LLM.int8(), QLoRA, bitsandbytes) has a
    great [blog post](https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/)
    for understanding quantization principles, and the need for mixed-precision quantization
    as it relates to emergent features in large-model training. Other popular methods
    and formats are GGUF (for llama.cpp), AWQ, HQQ, and GPTQ; see this [post](https://www.tensorops.ai/post/what-are-quantized-llms)
    from TensorOps for an overview, and this [post](https://www.maartengrootendorst.com/blog/quantization/)
    from Maarten Grootendorst for a discussion of their tradeoffs.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 随着领先大型语言模型参数数量的快速增加，以及获取GPU以运行模型在成本和可用性方面的困难，对量化LLM权重以使用更少的位来使用越来越感兴趣，这通常可以在所需内存减少50-75%（或更多）的情况下产生可比较的输出质量。通常，这不应该天真地去做；Tim
    Dettmers是现代几种量化方法（LLM.int8(), QLoRA, bitsandbytes）的先驱之一，他有一篇很好的[博客文章](https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/)，介绍了量化原理，以及与大型模型训练中的涌现特征相关的混合精度量化的必要性。其他流行的方法和格式包括GGUF（用于llama.cpp）、AWQ、HQQ和GPTQ；请参阅TensorOps的这篇[文章](https://www.tensorops.ai/post/what-are-quantized-llms)以获取概述，以及Maarten
    Grootendorst的这篇[文章](https://www.maartengrootendorst.com/blog/quantization/)，讨论了它们的权衡。
- en: In addition to enabling inference on smaller machines, quantization is also
    popular for parameter-efficient training; in QLoRA, most weights are quantized
    to 4-bit precision and frozen, while active LoRA adapters are trained in 16-bit
    precision. See this [talk](https://www.youtube.com/watch?v=fQirE9N5q_Y) from Tim
    Dettmers, or this [blog](https://huggingface.co/blog/4bit-transformers-bitsandbytes)
    from Hugging Face for overviews. This [blog post](https://www.answer.ai/posts/2024-03-14-fsdp-qlora-deep-dive.html)
    from Answer.AI also shows how to combine QLoRA with FSDP for efficient finetuning
    of 70B+ parameter models on consumer GPUs.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 除了能够在较小的机器上进行推理之外，量化在参数高效的训练中也非常流行；在QLoRA中，大多数权重被量化到4位精度并冻结，而活跃的LoRA适配器以16位精度进行训练。请参阅Tim
    Dettmers的这篇[演讲](https://www.youtube.com/watch?v=fQirE9N5q_Y)，或Hugging Face的这篇[博客](https://huggingface.co/blog/4bit-transformers-bitsandbytes)以获取概述。Answer.AI的这篇[博客文章](https://www.answer.ai/posts/2024-03-14-fsdp-qlora-deep-dive.html)还展示了如何将QLoRA与FSDP结合，以在消费级GPU上高效微调70B+参数模型。
- en: Speculative Decoding
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推测性解码
- en: The basic idea behind speculative decoding is to speed up inference from a larger
    model by primarily sampling tokens from a much smaller model and occasionally
    applying corrections (e.g. every *N* tokens) from the larger model whenever the
    output distributions diverge. These batched consistency checks tend to be much
    faster than sampling *N* tokens directly, and so there can be large overall speedups
    if the token sequences from smaller model only diverge periodically.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 推测性解码背后的基本思想是通过主要从远小于原始模型中采样标记，并在输出分布发生偏差时偶尔应用来自较大模型的校正（例如，每*N*个标记），来加速从较大模型的推理。这些批处理一致性检查通常比直接采样*N*个标记要快得多，因此如果较小模型的标记序列只是偶尔发生偏差，整体速度可以大幅提升。
- en: See this [blog post](https://jaykmody.com/blog/speculative-sampling/) from Jay
    Mody for a walkthrough of the original paper, and this PyTorch [article](https://pytorch.org/blog/hitchhikers-guide-speculative-decoding/)
    for some evaluation results. There’s a nice [video](https://www.youtube.com/watch?v=hm7VEgxhOvk)
    overview from Trelis Research as well.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅Jay Mody的这篇[博客文章](https://jaykmody.com/blog/speculative-sampling/)，了解原始论文的概述，以及这篇PyTorch[文章](https://pytorch.org/blog/hitchhikers-guide-speculative-decoding/)中的一些评估结果。Trelis
    Research还提供了一个很好的[视频概述](https://www.youtube.com/watch?v=hm7VEgxhOvk)。
- en: FlashAttention
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FlashAttention
- en: Computing attention matrices tends to be a primary bottleneck in inference and
    training for Transformers, and FlashAttention has become one of the most widely-used
    techniques for speeding it up. In contrast to some of the techniques we’ll see
    in [Section 7](#s7) which *approximate* attention with a more concise representation
    (occurring some representation error as a result), FlashAttention is an *exact*
    representation whose speedup comes from hardware-aware impleemntation. It applies
    a few tricks — namely, tiling and recomputation — to decompose the expression
    of attention matrices, enabling significantly reduced memory I/O and faster wall-clock
    performance (even with slightly increasing the required FLOPS).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 计算注意力矩阵通常是在 Transformers 的推理和训练中遇到的主要瓶颈，FlashAttention 已经成为加速这一过程最广泛使用的技巧之一。与我们在[第七节](#s7)中将要看到的一些技术不同，这些技术通过更简洁的表示来*近似*注意力（因此会产生一些表示误差），FlashAttention
    是一种*精确*的表示，其加速来自于对硬件感知的实现。它应用了一些技巧——即分块和重新计算——来分解注意力矩阵的表达式，从而显著减少内存 I/O 并提高墙钟性能（即使略微增加了所需的
    FLOPS）。
- en: 'Resources:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 资源：
- en: '[Talk](https://www.youtube.com/watch?v=gMOAud7hZg4) by Tri Dao (author of FlashAttention)'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Tri Dao 的演讲](https://www.youtube.com/watch?v=gMOAud7hZg4) (FlashAttention
    的作者)'
- en: '[ELI5: FlashAttention](https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad)
    by Aleksa Gordić'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ELI5: FlashAttention](https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad)
    by Aleksa Gordić'
- en: Key-Value Caching and Paged Attention
  id: totrans-300
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 键值缓存和分页注意力
- en: As noted in the [NVIDIA blog](https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/)
    referenced above, key-value caching is fairly standard in Transformer implementation
    matrices to avoid redundant recomputation of attention. This enables a tradeoff
    between speed and resource utilization, as these matrices are kept in GPU VRAM.
    While managing this is fairly straightforward for a single “thread” of inference,
    a number of complexities arise when considering parallel inference or multiple
    users for a single hosted model instance. How can you avoid recomputing values
    for system prompts and few-shot examples? When should you evict cache elements
    for a user who may or may not want to continue a chat session? PagedAttention
    and its popular implementation [vLLM](https://docs.vllm.ai/en/stable/) addresses
    this by leveraging ideas from classical paging in operating systems, and has become
    a standard for self-hosted multi-user inference servers.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述的[NVIDIA 博客](https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/)中提到的，键值缓存在
    Transformer 实现矩阵中相当标准，以避免重复计算注意力。这允许在速度和资源利用之间进行权衡，因为这些矩阵保存在 GPU VRAM 中。虽然对于单个“线程”的推理来说管理这一点相当直接，但当考虑到并行推理或单个托管模型实例的多个用户时，会出现许多复杂性。你如何避免重新计算系统提示和少样本示例的值？对于可能或可能不想继续聊天会话的用户，何时应该驱逐缓存元素？PagedAttention
    及其流行的实现 [vLLM](https://docs.vllm.ai/en/stable/) 通过利用操作系统中经典的分页思想来解决这一问题，并已成为自托管多用户推理服务器的标准。
- en: 'Resources:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 资源：
- en: '[The KV Cache: Memory Usage in Transformers](https://www.youtube.com/watch?v=80bIUggRJf4)
    (video, Efficient NLP)'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KV 缓存：Transformers 中的内存使用](https://www.youtube.com/watch?v=80bIUggRJf4) (视频，高效自然语言处理)'
- en: '[Fast LLM Serving with vLLM and PagedAttention](https://www.youtube.com/watch?v=5ZlavKF_98U)
    (video, Anyscale)'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 vLLM 和分页注意力快速服务 LLM](https://www.youtube.com/watch?v=5ZlavKF_98U) (视频，Anyscale)'
- en: vLLM [blog post](https://blog.vllm.ai/2023/06/20/vllm.html)
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: vLLM [博客文章](https://blog.vllm.ai/2023/06/20/vllm.html)
- en: CPU Offloading
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CPU 负载
- en: The primary method used for running LLMs either partially or entirely on CPU
    (vs. GPU) is llama.cpp. See [here](https://www.datacamp.com/tutorial/llama-cpp-tutorial)
    for a high-level overview; llama.cpp serves as the backend for a number of popular
    self-hosted LLM tools/frameworks like LMStudio and Ollama. Here’s a [blog post](https://justine.lol/matmul/)
    with some technical details about CPU performance improvements.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CPU（与 GPU 相比）上部分或全部运行 LLM 的主要方法是 llama.cpp。有关高级概述，请参阅[这里](https://www.datacamp.com/tutorial/llama-cpp-tutorial)；llama.cpp
    作为 LMStudio 和 Ollama 等许多流行的自托管 LLM 工具/框架的后端。以下是一篇[博客文章](https://justine.lol/matmul/)，其中包含一些关于
    CPU 性能改进的技术细节。
- en: 'Section VII: Sub-Quadratic Context Scaling'
  id: totrans-308
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第七节：亚二次上下文缩放
- en: '**Goal:** Survey approaches for avoiding the "quadratic scaling problem" faced
    by self-attention in Transformers.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '**目标：调查避免 Transformers 中自注意力面临的“二次缩放问题”的方法。'
- en: A major bottleneck in scaling both the size and context length of Transformers
    is the quadratic nature of attention, in which all pairs of token interactions
    are considered. Here we’ll look at a number of approaches for circumventing this,
    ranging from those which are currently widely used to those which are more exploratory
    (but promising) research directions.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在扩展Transformer的大小和上下文长度时，一个主要的瓶颈是注意力的二次性质，其中考虑了所有标记对的交互。在这里，我们将探讨多种绕过这一问题的方法，从目前广泛使用的方法到更具探索性（但很有希望）的研究方向。
- en: Sliding Window Attention
  id: totrans-311
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 滑动窗口注意力
- en: Introduced in the “Longformer” [paper](https://arxiv.org/abs/2004.05150), sliding
    window attention acts as a sub-quadratic drop-in replacement for standard attention
    which allows attending only to a sliding window (shocking, right?) of recent tokens/states
    rather than the entire context window, under the pretense that vectors for these
    states have already attended to earlier ones and thus have sufficient representational
    power to encode relevant pieces of early context. Due to its simplicity, it’s
    become one of the more widely adopted approaches towards sub-quadratic scaling,
    and is used in Mistral’s popular Mixtral-8x7B model (among others).
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在“Longformer”[论文](https://arxiv.org/abs/2004.05150)中引入的滑动窗口注意力，作为标准注意力的亚二次替代品，允许只关注最近标记/状态的滑动窗口（令人震惊，对吧？），而不是整个上下文窗口，前提是这些状态的向量已经关注了早期的那些，因此具有足够的表示能力来编码相关的早期上下文片段。由于其简单性，它已成为亚二次扩展中更广泛采用的方法之一，并被用于Mistral流行的Mixtral-8x7B模型（以及其他模型）。
- en: 'Resources:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 资源：
- en: '[“What is Sliding Window Attention?”](https://klu.ai/glossary/sliding-window-attention)
    (blog post by Stephen M. Walker)'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“什么是滑动窗口注意力？”（https://klu.ai/glossary/sliding-window-attention）](blog post
    by Stephen M. Walker)'
- en: '[“Sliding Window Attention”](https://medium.com/@manojkumal/sliding-window-attention-565f963a1ffd)
    (blog post by Manoj Kumal)'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“滑动窗口注意力”（https://medium.com/@manojkumal/sliding-window-attention-565f963a1ffd）](blog
    post by Manoj Kumal)'
- en: '[“Longformer: The Long-Document Transformer”](https://www.youtube.com/watch?v=_8KNb5iqblE)
    (video by Yannic Kilcher)'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“Longformer：长文档Transformer”（https://www.youtube.com/watch?v=_8KNb5iqblE）](video
    by Yannic Kilcher)'
- en: Ring Attention
  id: totrans-317
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 环形注意力
- en: Another modification to standard attention mechanisms, Ring Attention enables
    sub-quadratic full-context interaction via incremental computation with a “message-passing”
    structure, wherein “blocks” of context communicate with each other over a series
    of steps rather than all at once. Within each block, the technique is essentially
    classical attention. While largely a research direction rather than standard technique
    at least within the open-weights world, Google’s Gemini is [rumored](https://www.reddit.com/r/MachineLearning/comments/1arj2j8/d_gemini_1m10m_token_context_window_how/)
    to possibly be using Ring Attention in order to enable its million-plus-token
    context.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种对标准注意力机制的修改，环形注意力通过具有“消息传递”结构的增量计算实现亚二次全上下文交互，其中“块”的上下文通过一系列步骤相互通信，而不是一次性全部通信。在每个块内，该技术本质上就是经典注意力。尽管在开放权重世界中它更多地是一个研究方向而不是标准技术，但据传言谷歌的Gemini可能正在使用环形注意力来启用其百万以上的标记上下文。
- en: 'Resources:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 资源：
- en: '[“Breaking the Boundaries: Understanding Context Window Limitations and the
    idea of Ring Attention”](https://medium.com/@tanuj22july/breaking-the-boundaries-understanding-context-window-limitations-and-the-idea-of-ring-attention-170e522d44b2)
    (blog post, Tanuj Sharma)'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“打破界限：理解上下文窗口限制和环形注意力的概念”（https://medium.com/@tanuj22july/breaking-the-boundaries-understanding-context-window-limitations-and-the-idea-of-ring-attention-170e522d44b2）](blog
    post, Tanuj Sharma)'
- en: '[“Understanding Ring Attention: Building Transformers With Near-Infinite Context”](https://www.e2enetworks.com/blog/understanding-ring-attention-building-transformers-with-near-infinite-context)
    (blog post, E2E Networks)'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“理解环形注意力：构建具有近乎无限上下文的Transformer”（https://www.e2enetworks.com/blog/understanding-ring-attention-building-transformers-with-near-infinite-context）](blog
    post, E2E Networks)'
- en: '[“Ring Attention Explained”](https://www.youtube.com/watch?v=jTJcP8iyoOM) (video)'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“环形注意力解释”（https://www.youtube.com/watch?v=jTJcP8iyoOM）](video)'
- en: Linear Attention (RWKV)
  id: totrans-323
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性注意力（RWKV）
- en: The Receptance-Weighted Key Value (RWKV) architecture is a return to the general
    structure of RNN models (e.g LSTMs), with modifications to enable increased scaling
    and a *linear* attention-style mechanism which supports recurrent “unrolling”
    of its representation (allowing constant computation per output token as context
    length scales).
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: Receptance-Weighted Key Value（RWKV）架构回归到RNN模型（例如LSTMs）的一般结构，并进行修改以实现更大的扩展和一种*线性*的注意力机制，该机制支持其表示的循环“展开”（允许每个输出标记的恒定计算，随着上下文长度的增加）。
- en: 'Resources:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 资源：
- en: (Huggingface blog)
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （Huggingface博客）
- en: '[“The RWKV language model: An RNN with the advantages of a transformer” - Pt.
    1](https://johanwind.github.io/2023/03/23/rwkv_overview.html) (blog post, Johan
    Wind)'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“RWKV语言模型：具有Transformer优势的RNN” - 第一部分](https://johanwind.github.io/2023/03/23/rwkv_overview.html)（博客文章，Johan
    Wind）'
- en: '[“How the RWKV language model works” - Pt. 2](https://johanwind.github.io/2023/03/23/rwkv_details.html)'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“RWKV语言模型的工作原理” - 第二部分](https://johanwind.github.io/2023/03/23/rwkv_details.html)'
- en: '[“RWKV: Reinventing RNNs for the Transformer Era (Paper Explained)”](https://www.youtube.com/watch?v=x8pW19wKfXQ)
    (video, Yannic Kilcher)'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“RWKV：为Transformer时代重新发明RNN（论文解释）”](https://www.youtube.com/watch?v=x8pW19wKfXQ)（视频，Yannic
    Kilcher）'
- en: Structured State Space Models
  id: totrans-330
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结构化状态空间模型
- en: Structured State Space Models (SSMs) have become one of the most popular alternatives
    to Transformers in terms of current research focus, with several notable variants
    (S4, Hyena, Mamba/S6, Jamba, Mamba-2), but are somewhat notorious for their complexity.
    The architecture draws inspiration from classical control theory and linear time-invariant
    systems, with a number of optimizations to translate from continuous to discrete
    time, and to avoid dense representations of large matrices. They support both
    recurrent and convolutional representations, which allows efficiency gains both
    for training and at inference, and many variants require carefully-conditioned
    “hidden state matrix” representations to support “memorization” of context without
    needing all-pairs attention. SSMs also seem to be becoming more practical at scale,
    and have recently resulted in breakthrough speed improvements for high-quality
    text to speech (via [Cartesia AI](https://www.cartesia.ai/), founded by the inventors
    of SSMs).
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化状态空间模型（SSMs）在当前研究焦点方面已成为Transformer最受欢迎的替代方案之一，拥有几个显著的变体（S4、Hyena、Mamba/S6、Jamba、Mamba-2），但它们因复杂性而有些臭名昭著。该架构从经典控制理论和线性时不变系统中汲取灵感，进行了一系列优化，以将连续时间转换为离散时间，并避免大矩阵的密集表示。它们支持循环和卷积表示，这既提高了训练效率，也提高了推理效率，许多变体需要仔细条件化的“隐藏状态矩阵”表示，以支持“记忆”上下文而不需要所有成对注意力。SSMs似乎在规模上也变得更加实用，最近在高质量文本到语音转换（通过[Cartesia
    AI](https://www.cartesia.ai/)，由SSMs的发明者创立）方面取得了突破性的速度提升。
- en: The best explainer out there is likely [“The Annotated S4”](https://srush.github.io/annotated-s4/),
    focused on the S4 paper from which SSMs originated. The post [“A Visual Guide
    to Mamba and State Space Models”](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state)
    is great for intuitions and visuals with slightly less math, and Yannic Kilcher
    has a nice [video](https://www.youtube.com/watch?v=9dSkvxS2EB0) on SSMs as well.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 目前最好的解释可能是“《标注的S4》”，它专注于SSMs起源的S4论文。帖子“《Mamba和状态空间模型的视觉指南》”非常适合直观和视觉理解，数学内容较少，Yannic
    Kilcher还有一个关于SSMs的[视频](https://www.youtube.com/watch?v=9dSkvxS2EB0)。
- en: 'Recently, the Mamba authors released their follow-up “Mamba 2” paper, and their
    accompanying series of blog posts discusses some newly-uncovered connections between
    SSM representations and linear attention which may be interesting:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Mamba的作者发布了他们的后续论文“Mamba 2”，以及他们的一系列配套博客文章讨论了SSM表示和线性注意力之间的一些新发现的联系，这可能很有趣：
- en: '[State Space Duality (Mamba-2) Part I - The Model](https://tridao.me/blog/2024/mamba2-part1-model/)'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[状态空间对偶性（Mamba-2）第一部分 - 模型](https://tridao.me/blog/2024/mamba2-part1-model/)'
- en: '[State Space Duality (Mamba-2) Part II - The Theory](https://tridao.me/blog/2024/mamba2-part2-theory/)'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[状态空间对偶性（Mamba-2）第二部分 - 理论](https://tridao.me/blog/2024/mamba2-part2-theory/)'
- en: '[State Space Duality (Mamba-2) Part III - The Algorithm](https://tridao.me/blog/2024/mamba2-part3-algorithm/)'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[状态空间对偶性（Mamba-2）第三部分 - 算法](https://tridao.me/blog/2024/mamba2-part3-algorithm/)'
- en: '[State Space Duality (Mamba-2) Part IV - The Systems](https://tridao.me/blog/2024/mamba2-part4-systems/)'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[状态空间对偶性（Mamba-2）第四部分 - 系统](https://tridao.me/blog/2024/mamba2-part4-systems/)'
- en: HyperAttention
  id: totrans-338
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超注意力
- en: Somewhat similar to RWKV and SSMs, HyperAttention is another proposal for achieving
    near-linear scaling for attention-like mechanisms, relying on locality-sensitive
    hashing (think vector DBs) rather than recurrent representations. I don’t see
    it discussed as much as the others, but it may be worth being aware of nonetheless.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 与RWKV和SSMs有些相似，HyperAttention是另一种实现类似注意力机制近线性缩放的提议，它依赖于局部敏感哈希（想想向量数据库）而不是循环表示。我没有看到它像其他方法那样被广泛讨论，但无论如何，它可能值得关注。
- en: For an overview, see this [blog post](https://medium.com/@yousra.aoudi/linear-time-magic-how-hyperattention-optimizes-large-language-models-b691c0e2c2b0)
    by Yousra Aoudi and short explainer [video](https://www.youtube.com/watch?v=uvix7XwAjOg)
    by Tony Shin.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 概览请参阅Yousra Aoudi的这篇[博客文章](https://medium.com/@yousra.aoudi/linear-time-magic-how-hyperattention-optimizes-large-language-models-b691c0e2c2b0)和Tony
    Shin的简短[视频](https://www.youtube.com/watch?v=uvix7XwAjOg)。
- en: 'Section VIII: Generative Modeling Beyond Sequences'
  id: totrans-341
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第八节：序列之外的生成建模
- en: '**Goal:** Survey topics building towards generation of non-sequential content
    like images, from GANs to diffusion models.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '**目标：** 调查构建非顺序内容生成（如图像）的主题，从GANs到扩散模型。'
- en: So far, everything we’ve looked has been focused on text and sequence prediction
    with language models, but many other “generative AI” techniques require learning
    distributions with less of a sequential structure (e.g. images). Here we’ll examine
    a number of non-Transformer architectures for generative modeling, starting from
    simple mixture models and culminating with diffusion.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所看的一切都集中在文本和序列预测上，使用语言模型，但许多其他“生成式AI”技术需要学习具有较少顺序结构的分布（例如图像）。在这里，我们将检查用于生成建模的几种非Transformer架构，从简单的混合模型开始，以扩散模型结束。
- en: Distribution Modeling
  id: totrans-344
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布建模
- en: Recalling our first glimpse of language models as simple bigram distributions,
    the most basic thing you can do in distributional modeling is just count co-occurrence
    probabilities in your dataset and repeat them as ground truth. This idea can be
    extended to conditional sampling or classification as “Naive Bayes” ([blog post](https://mitesh1612.github.io/blog/2020/08/30/naive-bayes)
    [video](https://www.youtube.com/watch?v=O2L2Uv9pdDA)), often one of the simplest
    algorithms covered in introductory machine learning courses.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 回想我们第一次看到语言模型作为简单的二元分布，在分布建模中最基本的事情就是只是计算你的数据集中的共现概率，并重复它们作为真实情况。这个想法可以扩展到条件采样或分类，称为“朴素贝叶斯”([博客文章](https://mitesh1612.github.io/blog/2020/08/30/naive-bayes)
    [视频](https://www.youtube.com/watch?v=O2L2Uv9pdDA))，通常是在机器学习入门课程中涵盖的最简单算法之一。
- en: The next generative model students are often taught is the Gaussian Mixture
    Model and its Expectation-Maximization algorithm; Gaussian Mixture Models + Expectation-Maximization
    algorithm. This [blog post](https://mpatacchiola.github.io/blog/2020/07/31/gaussian-mixture-models.html)
    and this [video](https://www.youtube.com/watch?v=DODphRRL79c) give decent overviews;
    the core idea here is assuming that data distributions can be approximated as
    a mixture of multivariate Gaussian distributions. GMMs can also be used for clustering
    if individual groups can be assumed to be approximately Gaussian.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 学生通常接下来学习的是高斯混合模型及其期望最大化算法；高斯混合模型 + 期望最大化算法。这篇[博客文章](https://mpatacchiola.github.io/blog/2020/07/31/gaussian-mixture-models.html)和这篇[视频](https://www.youtube.com/watch?v=DODphRRL79c)提供了相当好的概述；这里的核心思想是假设数据分布可以被近似为多元高斯分布的混合。如果可以假设单个组是近似高斯分布的，GMMs也可以用于聚类。
- en: While these methods aren’t very effective at representing complex structures
    like images or language, related ideas will appear as components of some of the
    more advanced methods we’ll see.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些方法在表示复杂结构如图像或语言方面不是很有效，但相关想法将作为我们接下来将要看到的一些更先进方法的部分出现。
- en: Variational Auto-Encoders
  id: totrans-348
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变分自编码器
- en: 'Auto-encoders and variational auto-encoders are widely used for learning compressed
    representations of data distributions, and can also be useful for “denoising”
    inputs, which will come into play when we discuss diffusion. Some nice resources:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器和变分自编码器被广泛用于学习数据分布的压缩表示，并且对于“去噪”输入也很有用，这在讨论扩散时将发挥作用。一些不错的资源：
- en: '[“Autoencoders”](https://www.deeplearningbook.org/contents/autoencoders.html)
    chapter in the “Deep Learning” book'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “深度学习”书籍中的[“自编码器”](https://www.deeplearningbook.org/contents/autoencoders.html)章节
- en: '[blog post]([https://lilianweng.github.io/posts/2018-08-12-vae/]) from Lilian
    Weng'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[博客文章](https://lilianweng.github.io/posts/2018-08-12-vae/) 来自李莲英'
- en: '[video](https://www.youtube.com/watch?v=9zKuYvjFFS8) from Arxiv Insights'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arxiv Insights的[视频](https://www.youtube.com/watch?v=9zKuYvjFFS8)
- en: '[blog post](https://towardsdatascience.com/deep-generative-models-25ab2821afd3)
    from Prakash Pandey on both VAEs and GANs'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Prakash Pandey关于VAEs和GANs的[博客文章](https://towardsdatascience.com/deep-generative-models-25ab2821afd3)
- en: Generative Adversarial Nets
  id: totrans-354
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成对抗网络
- en: The basic idea behind Generative Adversarial Networks (GANs) is to simulate
    a “game” between two neural nets — the Generator wants to create samples which
    are indistinguishable from real data by the Discriminator, who wants to identify
    the generated samples, and both nets are trained continuously until an equilibrium
    (or desired sample quality) is reached. Following from von Neumann’s minimax theorem
    for zero-sum games, you basically get a “theorem” promising that GANs succeed
    at learning distributions, if you assume that gradient descent finds global minimizers
    and allow both networks to grow arbitrarily large. Granted, neither of these are
    literally true in practice, but GANs do tend to be quite effective (although they’ve
    fallen out of favor somewhat in recent years, partly due to the instabilities
    of simultaneous training).
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络（GANs）背后的基本思想是模拟两个神经网络之间的“游戏”——生成器希望创建的样本能够被判别器识别为与真实数据不可区分，而判别器则希望识别生成的样本，并且两个网络会持续训练，直到达到平衡（或所需的样本质量）。根据冯·诺伊曼的零和博弈最小-最大定理，如果你假设梯度下降可以找到全局最小值，并且允许两个网络任意增长，你基本上会得到一个“定理”，它承诺GANs在学习分布方面是成功的。诚然，在实际情况中这两者都不是字面意义上的真实，但GANs确实往往非常有效（尽管近年来它们在一定程度上已经不再受欢迎，部分原因是同时训练的不稳定性）。
- en: 'Resources:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 资源：
- en: '[“Complete Guide to Generative Adversarial Networks”](https://blog.paperspace.com/complete-guide-to-gans/)
    from Paperspace'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paperspace的[“生成对抗网络（GANs）完整指南”](https://blog.paperspace.com/complete-guide-to-gans/)
- en: '[“Generative Adversarial Networks (GANs): End-to-End Introduction”](https://www.analyticsvidhya.com/blog/2021/10/an-end-to-end-introduction-to-generative-adversarial-networksgans/)
    by'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“生成对抗网络（GANs）：端到端介绍”](https://www.analyticsvidhya.com/blog/2021/10/an-end-to-end-introduction-to-generative-adversarial-networksgans/)
    by'
- en: '[Deep Learning, Ch. 20 - Generative Models](https://www.deeplearningbook.org/contents/generative_models.html)
    (theory-focused)'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度学习，第20章 - 生成模型](https://www.deeplearningbook.org/contents/generative_models.html)（理论导向）'
- en: Conditional GANs
  id: totrans-360
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 条件GANs
- en: Conditional GANs are where we’ll start going from vanilla “distribution learning”
    to something which more closely resembles interactive generative tools like DALL-E
    and Midjourney, incorporating text-image multimodality. A key idea is to learn
    “representations” (in the sense of text embeddings or autoencoders) which are
    more abstract and can be applied to either text or image inputs. For example,
    you could imagine training a vanilla GAN on (image, caption) pairs by embedding
    the text and concatenating it with an image, which could then learn this joint
    distribution over images and captions. Note that this implicitly involves learning
    conditional distributions if part of the input (image or caption) is fixed, and
    this can be extended to enable automatic captioning (given an image) or image
    generation (given a caption). There a number of variants on this setup with differing
    bells and whistles. The VQGAN+CLIP architecture is worth knowing about, as it
    was a major popular source of early “AI art” generated from input text.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 条件GANs是我们从传统的“分布学习”开始，走向更接近交互式生成工具（如DALL-E和Midjourney）的地方，它结合了文本-图像的多模态。一个关键的想法是学习“表示”（在文本嵌入或自编码器的意义上）更抽象，并且可以应用于文本或图像输入。例如，你可以想象通过嵌入文本并将其与图像连接起来，在（图像，标题）对上训练一个传统的GAN，这样它就可以学习图像和标题的联合分布。请注意，如果输入的一部分（图像或标题）是固定的，这隐式地涉及到学习条件分布，并且这可以扩展以实现自动配图（给定一个图像）或图像生成（给定一个标题）。这个设置有许多变体，各有不同的功能和特性。VQGAN+CLIP架构值得了解，因为它曾是早期从输入文本生成“AI艺术”的主要流行来源之一。
- en: 'Resources:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 资源：
- en: '[“Implementing Conditional Generative Adversarial Networks”](https://blog.paperspace.com/conditional-generative-adversarial-networks/)
    blog from Paperspace'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paperspace的[“实现条件生成对抗网络”](https://blog.paperspace.com/conditional-generative-adversarial-networks/)博客文章
- en: '[“Conditional Generative Adversarial Network — How to Gain Control Over GAN
    Outputs”](https://towardsdatascience.com/cgan-conditional-generative-adversarial-network-how-to-gain-control-over-gan-outputs-b30620bd0cc8)
    by Saul Dobilas'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saul Dobilas的[“条件生成对抗网络——如何控制GAN输出”](https://towardsdatascience.com/cgan-conditional-generative-adversarial-network-how-to-gain-control-over-gan-outputs-b30620bd0cc8)
- en: '[“The Illustrated VQGAN”](https://ljvmiranda921.github.io/notebook/2021/08/08/clip-vqgan/)
    by LJ Miranda'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“The Illustrated VQGAN”](https://ljvmiranda921.github.io/notebook/2021/08/08/clip-vqgan/)
    by LJ Miranda'
- en: '[“Using Deep Learning to Generate Artwork with VQGAN-CLIP”](https://www.youtube.com/watch?v=Ih4qOakCZD4)
    talk from Paperspace'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“Using Deep Learning to Generate Artwork with VQGAN-CLIP”](https://www.youtube.com/watch?v=Ih4qOakCZD4)
    from Paperspace'
- en: Normalizing Flows
  id: totrans-367
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正态化流
- en: The aim of normalizing flows is to learn a series of invertible transformations
    between Gaussian noise and an output distribution, avoiding the need for “simultaneous
    training” in GANs, and have been popular for generative modeling in a number of
    domains. Here I’ll just recommend [“Flow-based Deep Generative Models”](https://lilianweng.github.io/posts/2018-10-13-flow-models/)
    from Lilian Weng as an overview — I haven’t personally gone very deep on normalizing
    flows, but they come up enough that they’re probably worth being aware of.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 正态化流的目的是学习一系列可逆变换，将高斯噪声与输出分布之间建立联系，避免在GANs中需要“同时训练”，并且在许多领域的生成建模中已经变得非常流行。在这里，我仅推荐Lilian
    Weng的[“Flow-based Deep Generative Models”](https://lilianweng.github.io/posts/2018-10-13-flow-models/)作为概述——我本人并没有深入研究正态化流，但它们出现的频率足够高，可能值得注意。
- en: Diffusion Models
  id: totrans-369
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩散模型
- en: One of the central ideas behind diffusion models (like StableDiffusion) is iterative
    guided application of denoising operations, refining random noise into something
    that increasingly resembles an image. Diffusion originates from the worlds of
    stochastic differential equations and statistical physics — relating to the “Schrodinger
    bridge” problem and optimal transport for probability distributions — and a fair
    amount of math is basically unavoidable if you want to understand the whole picture.
    For a relatively soft introduction, see [“A friendly Introduction to Denoising
    Diffusion Probabilistic Models”](https://medium.com/@gitau_am/a-friendly-introduction-to-denoising-diffusion-probabilistic-models-cc76b8abef25)
    by Antony Gitau. If you’re up for some more math, check out [“What are Diffusion
    Models?”](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/) for
    more of a deep dive. If you’re more interested in code and pictures (but still
    some math), see [“The Annotated Diffusion Model”](https://huggingface.co/blog/annotated-diffusion)
    from Hugging Face, as well as this Hugging Face [blog post](https://huggingface.co/blog/lora)
    on LoRA finetuning for diffusion models.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型（如StableDiffusion）背后的一个核心思想是迭代引导应用去噪操作，将随机噪声逐步细化成越来越像图像的东西。扩散起源于随机微分方程和统计物理的世界——与“薛定谔桥”问题和概率分布的最优传输相关——如果你想要全面理解，那么基本上不可避免地需要大量的数学知识。对于相对轻松的介绍，可以参考Antony
    Gitau的[“A friendly Introduction to Denoising Diffusion Probabilistic Models”](https://medium.com/@gitau_am/a-friendly-introduction-to-denoising-diffusion-probabilistic-models-cc76b8abef25)。如果你想要更多数学内容，可以查看Lilian
    Weng的[“What are Diffusion Models?”](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)，以深入了解。如果你对代码和图片（但仍然需要一些数学知识）更感兴趣，可以查看Hugging
    Face的[“The Annotated Diffusion Model”](https://huggingface.co/blog/annotated-diffusion)，以及关于LoRA微调扩散模型的这篇Hugging
    Face[博客文章](https://huggingface.co/blog/lora)。
- en: 'Section IX: Multimodal Models'
  id: totrans-371
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第IX节：多模态模型
- en: '**Goal:** Survey how models can use multiple modalities of input and output
    (text, audio, images) simultaneously.'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '**目标：** 调查模型如何同时使用输入和输出的多种模态（文本、音频、图像）。'
- en: 'Note: This is the set of topics with which I’m the least familiar, but wanted
    to include for completeness. I’ll be lighter on commentary and recommendations
    here, and will return to add more when I think I have a tighter story to tell.
    The post [“Multimodality and Large Multimodal Models (LMMs)”](https://huyenchip.com/2023/10/10/multimodal.html)
    by Chip Huyen is a nice broad overview (or [“How Multimodal LLMs Work”](https://www.determined.ai/blog/multimodal-llms)
    by Kevin Musgrave for a more concise one).'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '注意：这是我最不熟悉的一组主题，但为了完整性，我还是想包括在内。在这里，我将减少评论和建议，并在我认为自己有更紧密的故事要讲述时再回来补充。Chip
    Huyen的博客文章[“Multimodality and Large Multimodal Models (LMMs)”](https://huyenchip.com/2023/10/10/multimodal.html)提供了一个很好的广泛概述（或者Kevin
    Musgrave的[“How Multimodal LLMs Work”](https://www.determined.ai/blog/multimodal-llms)提供了一个更简洁的概述）。 '
- en: Tokenization Beyond Text
  id: totrans-374
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词汇化超越文本
- en: The idea of tokenization isn’t only relevant to text; audio, images, and video
    can also be “tokenized” for use in Transformer-style archictectures, and there
    a range of tradeoffs to consider between tokenization and other methods like convolution.
    The next two sections will look more into visual inputs; this [blog post](https://www.assemblyai.com/blog/recent-developments-in-generative-ai-for-audio/)
    from AssemblyAI touches on a number of relevant topics for audio tokenization
    and representation in sequence models, for applications like audio generation,
    text-to-speech, and speech-to-text.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 标记化（tokenization）的概念不仅与文本相关；音频、图像和视频也可以“标记化”以用于 Transformer 风格的架构，并且需要在标记化和其他方法（如卷积）之间考虑一系列权衡。接下来的两个部分将更深入地探讨视觉输入；AssemblyAI
    的这篇 [博客文章](https://www.assemblyai.com/blog/recent-developments-in-generative-ai-for-audio/)
    讨论了音频标记化和序列模型中的表示，适用于音频生成、文本到语音和语音到文本等应用。
- en: VQ-VAE
  id: totrans-376
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VQ-VAE
- en: The VQ-VAE architecture has become quite popular for image generation in recent
    years, and underlies at least the earlier versions of DALL-E.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，VQ-VAE 架构在图像生成方面变得非常流行，并且至少是 DALL-E 的早期版本的基础。
- en: 'Resources:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 资源：
- en: '[“Understanding VQ-VAE (DALL-E Explained Pt. 1)”](https://mlberkeley.substack.com/p/vq-vae)
    from the Machine Learning @ Berkeley blog'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“理解 VQ-VAE（DALL-E 解释第 1 部分）”（来自 Machine Learning @ Berkeley 博客）](https://mlberkeley.substack.com/p/vq-vae)'
- en: '[“How is it so good ? (DALL-E Explained Pt. 2)”](https://mlberkeley.substack.com/p/dalle2)'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“它为什么这么好？（DALL-E 解释第 2 部分）”（来自 Machine Learning @ Berkeley 博客）](https://mlberkeley.substack.com/p/dalle2)'
- en: '[“Understanding Vector Quantized Variational Autoencoders (VQ-VAE)”](https://shashank7-iitd.medium.com/understanding-vector-quantized-variational-autoencoders-vq-vae-323d710a888a)
    by Shashank Yadav'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“理解向量量化变分自编码器（VQ-VAE）”（Shashank Yadav 撰写）](https://shashank7-iitd.medium.com/understanding-vector-quantized-variational-autoencoders-vq-vae-323d710a888a)'
- en: Vision Transformers
  id: totrans-382
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 视觉 Transformer
- en: Vision Transformers extend the Transformer architecture to domains like image
    and video, and have become popular for applications like self-driving cars as
    well as for multimodal LLMs. There’s a nice [section](https://d2l.ai/chapter_attention-mechanisms-and-transformers/vision-transformer.html)
    in the d2l.ai book about how they work.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉 Transformer 将 Transformer 架构扩展到图像和视频等领域，并且因为自动驾驶汽车以及多模态 LLMs 的应用而变得流行。d2l.ai
    书籍中有一个很好的 [部分](https://d2l.ai/chapter_attention-mechanisms-and-transformers/vision-transformer.html)，介绍了它们的工作原理。
- en: '[“Generalized Visual Language Models”](https://lilianweng.github.io/posts/2022-06-09-vlm/)
    by Lilian Weng discusses a range of different approaches and for training multimodal
    Transformer-style models.'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '[“广义视觉语言模型”（Lilian Weng 撰写）](https://lilianweng.github.io/posts/2022-06-09-vlm/)
    讨论了多种不同的方法，用于训练多模态 Transformer 风格的模型。'
- en: The post [“Guide to Vision Language Models”](https://encord.com/blog/vision-language-models-guide/)
    from Encord’s blog overviews several architectures for mixing text and vision.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: Encord 博客上的文章 [“视觉语言模型指南”](https://encord.com/blog/vision-language-models-guide/)
    概述了多种混合文本和视觉的架构。
- en: If you’re interested in practical large-scale training advice for Vision Transformers,
    the MM1 [paper](https://arxiv.org/abs/2403.09611) from Apple examines several
    architecture and data tradeoffs with experimental evidence.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对于视觉 Transformer 的实际大规模训练建议感兴趣，苹果公司的 MM1 [论文](https://arxiv.org/abs/2403.09611)
    分析了几个架构和数据权衡，并提供了实验证据。
- en: '[“Multimodal Neurons in Artificial Neural Networks”](https://distill.pub/2021/multimodal-neurons/)
    from Distill.pub has some very fun visualizations of concept representations in
    multimodal networks.'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '[“人工神经网络中的多模态神经元”（来自 Distill.pub）](https://distill.pub/2021/multimodal-neurons/)
    有一些非常有趣的多模态网络概念表示的视觉化。'
- en: Citation
  id: totrans-388
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 引用
- en: If you’re making reference to any individual piece of content featured here,
    please just cite that directly. However, if you wish to cite this as a broad survey,
    you can use the BibTeX citation below.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你引用了这里展示的任何特定内容，请直接引用。然而，如果你希望将其作为一项广泛的调查引用，可以使用下面的 BibTeX 引用。
- en: '[PRE0]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
