- en: '2'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '2'
- en: Semantic Search with LLMs
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用LLM的语义搜索
- en: Introduction
  id: totrans-2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 简介
- en: 'In the last chapter, we explored the inner workings of language models and
    the impact that modern LLMs have had on NLP tasks like text classification, generation,
    and machine translation. There is another powerful application of LLMs that has
    been gaining traction in recent years: semantic search.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们探讨了语言模型的内部工作原理以及现代LLM对NLP任务如文本分类、生成和机器翻译的影响。近年来，LLM的另一个强大应用正在获得关注：语义搜索。
- en: Now you might be thinking that it’s time to finally learn the best ways to talk
    to ChatGPT and GPT-4 to get the optimal results, and we will start to do that
    in the next chapter, I promise. In the meantime, I want to show you what else
    we can build on top of this novel transformer architecture. While text-to-text
    generative models like GPT are extremely impressive in their own right, one of
    the most versatile solutions that AI companies offer is the ability to generate
    text embeddings based on powerful LLMs.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可能认为，是时候最终学习与ChatGPT和GPT-4的最佳沟通方式以获得最佳结果了，我保证我们将在下一章开始这样做。与此同时，我想向你展示我们还可以在这个新颖的Transformer架构之上构建什么。虽然像GPT这样的文本到文本生成模型本身非常令人印象深刻，但AI公司提供的最灵活的解决方案之一是能够基于强大的LLM生成文本嵌入。
- en: Text embeddings are a way to represent words or phrases as vectors in a high-dimensional
    space based on their contextual meaning within a corpus of text data. The idea
    is that if two phrases are similar (we will explore that word in more detail later
    on in this chapter) then the vectors that represent those phrases should be close
    together and vice versa. [Figure 2.1](ch02.html#ch02fig01) shows an example of
    a simple search algorithm. When a user searches for an item to buy – say a magic
    the gathering trading card they might simply search for “a vintage magic card”.
    The system should then embed the query such that if two text embeddings that are
    near each other should indicate that the phrases that were used to generate them
    are similar.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 文本嵌入是一种将单词或短语表示为高维空间中的向量，基于它们在文本数据语料库中的上下文意义。其理念是，如果两个短语相似（我们将在本章后面更详细地探讨这个单词），那么代表这些短语的向量应该彼此靠近，反之亦然。[图2.1](ch02.html#ch02fig01)展示了简单搜索算法的一个示例。当用户搜索要购买的商品时——比如说一张魔法集换卡，他们可能会简单地搜索“一张复古魔法卡”。系统应该将查询嵌入，使得如果两个彼此靠近的文本嵌入，应该表明用于生成它们的短语是相似的。
- en: '![Images](graphics/02fig01.jpg)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/02fig01.jpg)'
- en: '**Figure 2.1** *Vectors that represent similar phrases should be close together
    and those that represent dissimilar phrases should be far apart. In this case,
    if a user wants a trading card they might ask for “a vintage magic card”. A proper
    semantic search system should embed the query in such a way that it ends up near
    relevant results (like “magic card”) and far apart from non relevant items (like
    “a vintage magic kit”) even if they share certain keywords.*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**图2.1** 表示相似短语的向量应该彼此靠近，而表示不相似短语的向量应该彼此远离。在这种情况下，如果用户想要一张交易卡，他们可能会要求“一张复古魔法卡”。一个合适的语义搜索系统应该将查询嵌入，使其最终靠近相关结果（如“魔法卡”），远离非相关项目（如“一张复古魔法套件”），即使它们共享某些关键词。'
- en: This map from text to vectors can be thought of as a kind of hash with meaning.
    We can’t really reverse vectors back to text but rather they are a representation
    of the text that has the added benefit of carrying the ability to compare points
    while in their encoded state.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这种从文本到向量的映射可以被视为一种具有意义的哈希。我们实际上无法将向量反转回文本，而是它们是文本的表示，具有在编码状态下比较点的附加优势。
- en: LLM-enabled text embeddings allow us to capture the semantic value of words
    and phrases beyond just their surface-level syntax or spelling. We can rely on
    the pre-training and fine-tuning of LLMs to build virtually unlimited applications
    on top of them by leveraging this rich source of information about language use.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: LLM赋能的文本嵌入使我们能够捕捉到单词和短语语义价值，而不仅仅是它们表面层面的句法或拼写。我们可以依赖LLM的预训练和微调来构建在它们之上几乎无限的应用程序，通过利用关于语言使用的丰富信息来源。
- en: This chapter introduces us to the world of semantic search using LLMs to explore
    how they can be used to create powerful tools for information retrieval and analysis.
    In the next chapter, we will build a chatbot on top of GPT-4 that leverages a
    fully realized semantic search system that we will build in this chapter.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了使用LLM探索语义搜索世界的知识，了解它们如何被用来创建强大的信息检索和分析工具。在下一章中，我们将构建一个基于GPT-4的聊天机器人，该聊天机器人利用我们在本章中构建的完整语义搜索系统。
- en: Without further ado, let’s get right into it, shall we?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 不再拖延，让我们直接进入正题，好吗？
- en: The Task
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 任务
- en: A traditional search engine would generally take what you type in and then give
    you a bunch of links to websites or items that contain those words or permutations
    of the characters that you typed in. So if you typed in “Vintage Magic the Gathering
    Cards” on a marketplace, you would get items with a title/description that contains
    combinations of those words. That’s a pretty standard way to search, but it’s
    not always the best way. For example I might get vintage magic sets to help me
    learn how to pull a rabbit out of a hat. Fun but not what I asked for.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一个传统的搜索引擎通常会接收你输入的内容，然后给你一串包含那些单词或你输入字符排列的网站或项目链接。所以如果你在市场上输入“复古万智牌卡牌”，你会得到标题/描述中包含这些单词组合的项目。这是一种相当标准的搜索方式，但并不总是最好的方式。例如，我可能会得到一些复古魔法集来帮助我学习如何从帽子里变出兔子。很有趣，但不是我想要的。
- en: 'The terms you input into a search engine may not always align with the *exact*
    words used in the items you want to see. It could be that the words in the query
    are too general, resulting in a slew of unrelated findings. This issue often extends
    beyond just differing words in the results; the same words might carry different
    meanings than what was searched for. This is where semantic search comes into
    play, as exemplified by the earlier-mentioned Magic: The Gathering cards scenario.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你输入搜索引擎的术语可能并不总是与你想看到的物品中使用的**确切**词语一致。可能是查询中的词语过于笼统，导致一系列无关的发现。这个问题通常不仅限于结果中不同的词语；相同的词语可能具有与搜索内容不同的含义。这就是语义搜索发挥作用的地方，正如前面提到的万智牌卡牌场景所展示的。
- en: Asymmetric Semantic Search
  id: totrans-15
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 非对称语义搜索
- en: A **semantic search** system can understand the meaning and context of your
    search query and match it against the meaning and context of the documents that
    are available to retrieve. This kind of system can find relevant results in a
    database without having to rely on exact keyword or n-gram matching but rather
    rely on a pre-trained LLM to understand the nuance of the query and the documents
    ([Figure 2.2](ch02.html#ch02fig02)).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一个**语义搜索**系统可以理解你的搜索查询的意义和上下文，并将其与可检索文档的意义和上下文相匹配。这种系统可以在数据库中找到相关结果，而无需依赖于精确的关键词或n-gram匹配，而是依赖于预训练的LLM来理解查询和文档的细微差别（[图2.2](ch02.html#ch02fig02)）。
- en: '![Images](graphics/02fig02.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/02fig02.jpg)'
- en: '**Figure 2.2** *A traditional keyword-based search might rank a vintage magic
    kit with the same weight as the item we actually want whereas a semantic search
    system can understand the actual concept we are searching for*'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**图2.2** *传统的基于关键词的搜索可能会将复古魔法套件与我们所想要的物品的权重相同，而语义搜索系统可以理解我们实际搜索的概念*'
- en: The **asymmetric** part of asymmetric semantic search refers to the fact that
    there is generally an imbalance between the semantic information (basically the
    size) of the input query and the documents/information that the search system
    has to retrieve. For example, the search system is trying to match “magic the
    gathering card” to paragraphs of item descriptions on a marketplace. The four-word
    search query has much less information than the paragraphs but nonetheless it
    is what we are comparing.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 非对称语义搜索中的**非对称**部分指的是输入查询的语义信息（基本上是大小）与搜索系统必须检索的文档/信息之间通常存在不平衡。例如，搜索系统正在尝试将“万智牌卡”与市场上项目描述的段落相匹配。四个单词的搜索查询比段落包含的信息少得多，但无论如何，这是我们比较的内容。
- en: Asymmetric semantic search systems can get very accurate and relevant search
    results, even if you don’t use the exact right words in your search. They rely
    on the learnings of LLMs rather than the user being able to know exactly what
    needle to search for in the haystack.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 非对称语义搜索系统可以非常精确和相关性地检索搜索结果，即使你搜索时没有使用完全正确的词语。它们依赖于LLM的学习，而不是用户能够确切知道在稻草堆中搜索哪根针。
- en: 'I am of course, vastly oversimplifying the traditional method. There are many
    ways to make them more performant without switching to a more complex LLM approach
    and pure semantic search systems are not always the answer. They are not simply
    “the better way to do search”. Semantic algorithms have their own deficiencies
    like:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我对传统方法进行了极大的简化。有许多方法可以使它们更高效，而无需切换到更复杂的LLM方法，纯语义搜索系统并不总是答案。它们不仅仅是“更好的搜索方式”。语义算法有其自身的缺陷，例如：
- en: '![Images](graphics/square.jpg) They can be overly sensitive to small variations
    in text, such as differences in capitalization or punctuation.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](graphics/square.jpg) 它们可能对文本中的微小变化过于敏感，例如大小写或标点符号的差异。'
- en: '![Images](graphics/square.jpg) They struggle with nuanced concepts, such as
    sarcasm or irony that rely on localized cultural knowledge.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](graphics/square.jpg) 它们难以处理细微的概念，如讽刺或讽刺，这些概念依赖于局部的文化知识。'
- en: '![Images](graphics/square.jpg) They can be more computationally expensive to
    implement and maintain than the traditional method, especially when launching
    a home-grown system with many open-source components.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](graphics/square.jpg) 它们可能比传统方法更昂贵，在实现和维护方面，尤其是在启动包含许多开源组件的自建系统时。'
- en: Semantic search systems can be a valuable tool in certain contexts, so let’s
    jump right into how we will architect our solution.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 语义搜索系统在某些情境下可以成为一个有价值的工具，所以让我们直接进入如何构建我们的解决方案。
- en: Solution Overview
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解决方案概述
- en: 'The general flow of our asymmetric semantic search system will follow these
    steps:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们非对称语义搜索系统的一般流程将遵循以下步骤：
- en: '![Images](graphics/square.jpg) PART I - Ingesting documents ([Figure 2.3](ch02.html#ch02fig03))'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/square.jpg) 第一部分 - 摄入文档（[图2.3](ch02.html#ch02fig03)）'
- en: 1\. Collect documents for embedding
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 收集文档以进行嵌入
- en: 2\. Create text embeddings to encode semantic information
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 创建文本嵌入以编码语义信息
- en: 3\. Store embeddings in a database for later retrieval given a query
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 将嵌入存储在数据库中，以便在查询时检索
- en: '![Images](graphics/02fig03.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/02fig03.jpg)'
- en: '**Figure 2.3** *Zooming in on Part I, storing documents will consist of doing
    some pre-processing on our documents, embedding them, and then storing them in
    some database*'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**图2.3** *聚焦于第一部分，存储文档将包括对我们文档进行一些预处理，嵌入它们，然后将它们存储在某个数据库中*'
- en: '![Images](graphics/square.jpg) PART II - Retrieving documents ([Figure 2.4](ch02.html#ch02fig04))'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/square.jpg) 第二部分 - 检索文档（[图2.4](ch02.html#ch02fig04)）'
- en: 1\. User has a query which may be pre-processed and cleaned
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 用户有一个查询，可能需要预处理和清理
- en: 2\. Retrieve candidate documents
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 检索候选文档
- en: 3\. Re-rank the candidate documents if necessary
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 如有必要，重新排序候选文档
- en: 4\. Return the final search results
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 返回最终搜索结果
- en: '![Images](graphics/02fig04.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/02fig04.jpg)'
- en: '**Figure 2.4** *Zooming in on Part II, when retrieving documents we will have
    to embed our query using the same embedding scheme as we used for the documents
    and then compare them against the previously stored documents and return the best
    (closest) document*'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**图2.4** *聚焦于第二部分，在检索文档时，我们将使用与文档相同的嵌入方案嵌入我们的查询，然后与先前存储的文档进行比较，并返回最佳（最接近）的文档*'
- en: The Components
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 组件
- en: Let’s go over each of our components in more detail to understand the choices
    we’re making and what considerations we need to take into account.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地了解我们的每个组件，以了解我们做出的选择以及我们需要考虑的因素。
- en: Text Embedder
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 文本嵌入器
- en: As we now know, at the heart of any semantic search system is the text embedder.
    This is the component that takes in a text document, or a single word or phrase,
    and converts it into a vector. The vector is unique to that text and should capture
    the contextual meaning of the phrase.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所知，任何语义搜索系统的核心是文本嵌入器。这是将文本文档、单个单词或短语作为输入，并将其转换为向量的组件。这个向量是唯一的，应该捕捉到短语的语言环境意义。
- en: The choice of the text embedder is critical as it determines the quality of
    the vector representation of the text. We have many options in how we vectorize
    with LLMs, both open and closed source. To get off of the ground quicker, we are
    going to use OpenAI’s closed-source “Embeddings” product. In a later section,
    I’ll go over some open-source options.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 文本嵌入器的选择至关重要，因为它决定了文本向量表示的质量。我们在使用LLMs进行向量化的方式上有很多选择，既有开源也有闭源。为了更快地上手，我们将使用OpenAI的闭源“嵌入”产品。在后面的章节中，我会介绍一些开源选项。
- en: OpenAI’s “Embeddings” is a powerful tool that can quickly provide high-quality
    vectors, but it is a closed-source product, which means we have limited control
    over its implementation and potential biases. It’s important to keep in mind that
    when using closed-source products, we may not have access to the underlying algorithms,
    which can make it difficult to troubleshoot any issues that may arise.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI的“嵌入”是一个强大的工具，可以快速提供高质量的向量，但它是一个封闭源产品，这意味着我们对其实施和潜在偏差的控制有限。重要的是要记住，在使用封闭源产品时，我们可能无法访问底层算法，这可能会使解决可能出现的任何问题变得困难。
- en: What makes pieces of text “similar”
  id: totrans-47
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 什么使文本片段“相似”
- en: Once we convert our text into vectors, we have to find a mathematical representation
    of figuring out if pieces of text are “similar” or not. Cosine similarity is a
    way to measure how similar two things are. It looks at the angle between two vectors
    and gives a score based on how close they are in direction. If the vectors point
    in exactly the same direction, the cosine similarity is 1\. If they’re perpendicular
    (90 degrees apart), it’s 0\. And if they point in opposite directions, it’s -1\.
    The size of the vectors doesn’t matter, only their orientation does.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们将文本转换为向量，我们就必须找到确定文本片段是否“相似”的数学表示。余弦相似度是一种衡量两个事物相似度的方法。它查看两个向量之间的角度，并根据它们在方向上的接近程度给出分数。如果向量指向完全相同的方向，余弦相似度为1。如果它们垂直（90度），则为0。如果它们指向相反方向，则为-1。向量的尺寸无关紧要，只有它们的取向才重要。
- en: '[Figure 2.5](ch02.html#ch02fig05) shows how the cosine similarity would help
    us retrieve documents given a query.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.5](ch02.html#ch02fig05) 展示了余弦相似度如何帮助我们根据查询检索文档。'
- en: '![Images](graphics/02fig05.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图像](graphics/02fig05.jpg)'
- en: '**Figure 2.5** *In an ideal semantic search scenario, the Cosine Similarity
    (formula given at the top) gives us a computationally efficient way to compare
    pieces of text at scale, given that embeddings are tuned to place semantically
    similar pieces of text near each other (bottom). We start by embedding all items
    – including the query (bottom left) and then checking the angle between them.
    The smaller the angle, the larger the cosine similarity (bottom right)*'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**图2.5** *在理想的语义搜索场景中，余弦相似度（公式见顶部）为我们提供了一个计算效率高的方法来比较大量文本片段，前提是嵌入被调整以将语义相似的文本片段放置在一起（底部）。我们首先嵌入所有项目，包括查询（底部左），然后检查它们之间的角度。角度越小，余弦相似度（底部右）越大*。'
- en: 'We could also turn to other similarity metrics like the dot product or the
    Euclidean distance but OpenAI embeddings have a special property. The magnitudes
    (lengths) of their vectors are normalized to length 1, which basically means that
    we benefit mathematically on two fronts:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以转向其他相似度度量，如点积或欧几里得距离，但OpenAI嵌入有一个特殊属性。它们的向量的模长（长度）被归一化到长度1，这基本上意味着我们在数学上有两个方面的好处：
- en: '![Images](graphics/square.jpg) Cosine similarity is identical to the dot product'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '![图像](graphics/square.jpg) 余弦相似度等同于点积'
- en: '![Images](graphics/square.jpg) Cosine similarity and Euclidean distance will
    result in the identical rankings'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '![图像](graphics/square.jpg) 余弦相似度和欧几里得距离将导致相同的排名'
- en: 'TL;DR: Having normalized vectors (all having a magnitude of 1) is great because
    we can use a cheap cosine calculation to see how close two vectors are and therefore
    how close two phrases are semantically via the cosine similarity.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: TL;DR：拥有归一化向量（所有向量的模长均为1）是非常好的，因为我们可以使用廉价的余弦计算来查看两个向量有多接近，因此可以通过余弦相似度来查看两个短语在语义上的接近程度。
- en: OpenAI’s embedding
  id: totrans-56
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: OpenAI的嵌入
- en: Getting embeddings from OpenAI is as simple as a few lines of code ([Listing
    2.1](ch02.html#list2_1)). As mentioned previously, this entire system relies on
    an embedding mechanism that places semantically similar items near each other
    so that the cosine similiarty is large when the items are actually similar. There
    are multiple methods we could use to create these embeddings, but we will for
    now rely on OpenAI’s embedding **engines** to do this work for us. Engines are
    different embedding mechanism that OpenAI offer. We will use their most recent
    engine that they recommend for most use-cases.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 从OpenAI获取嵌入就像几行代码（[列表2.1](ch02.html#list2_1)）那么简单。如前所述，整个系统依赖于一个嵌入机制，该机制将语义相似的项目放置在一起，以便当项目实际上相似时，余弦相似度很大。我们可以使用多种方法来创建这些嵌入，但我们将现在依赖OpenAI的嵌入**引擎**为我们完成这项工作。引擎是OpenAI提供的不同嵌入机制。我们将使用他们推荐的最新引擎，该引擎适用于大多数用例。
- en: '**Listing 2.1** *Getting text embeddings from OpenAI*'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**列表2.1** *从OpenAI获取文本嵌入*'
- en: '[PRE0]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: It’s worth noting that OpenAI provides several engine options that can be used
    for text embedding. Each engine may provide different levels of accuracy and may
    be optimized for different types of text data. At the time of writing, the engine
    used in the code block is the most recent and the one they recommend using.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，OpenAI 提供了几个可用于文本嵌入的引擎选项。每个引擎可能提供不同级别的准确性，并且可能针对不同类型的文本数据进行优化。在撰写本文时，代码块中使用的引擎是最新的，也是他们推荐的使用的引擎。
- en: Additionally, it is possible to pass in multiple pieces of text at once to the
    “get_embeddings” function, which can generate embeddings for all of them in a
    single API call. This can be more efficient than calling “get_embedding” multiple
    times for each individual text. We will see an example of this later on.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还可以一次性将多个文本块传递给“get_embeddings”函数，该函数可以在单个API调用中为所有这些文本生成嵌入。这比多次调用“get_embedding”为每个单独的文本更有效率。我们将在稍后看到这个示例。
- en: Open-source Embedding Alternatives
  id: totrans-62
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 开源嵌入替代方案
- en: While OpenAI and other companies provide powerful text embedding products, there
    are also several open-source alternatives available for text embedding. A popular
    one is the bi-encoder with BERT, a powerful deep learning-based algorithm that
    has been shown to produce state-of-the-art results on a range of natural language
    processing tasks. We can find pre-trained bi-encoders in many open source repositories,
    including the **Sentence Transformers** library, which provides pre-trained models
    for a variety of natural language processing tasks to use off the shelf.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然OpenAI和其他公司提供了强大的文本嵌入产品，但还有几个开源的文本嵌入替代方案可用。其中一个流行的是使用BERT的双编码器，BERT是一种强大的基于深度学习的算法，已经在各种自然语言处理任务上显示出最先进的结果。我们可以在许多开源仓库中找到预训练的双编码器，包括**Sentence
    Transformers**库，该库为各种自然语言处理任务提供了现成的模型以供使用。
- en: A bi-encoder involves training two BERT models, one to encode the input text
    and the other to encode the output text ([Figure 2.6](ch02.html#ch02fig06)). The
    two models are trained simultaneously on a large corpus of text data, with the
    goal of maximizing the similarity between corresponding pairs of input and output
    text. The resulting embeddings capture the semantic relationship between the input
    and output text.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 双编码器涉及训练两个BERT模型，一个用于编码输入文本，另一个用于编码输出文本（[图2.6](ch02.html#ch02fig06)）。这两个模型在大量文本数据集上同时训练，目标是最大化输入和输出文本对应对的相似性。生成的嵌入捕捉了输入和输出文本之间的语义关系。
- en: '![Images](graphics/02fig06.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/02fig06.jpg)'
- en: '**Figure 2.6** *A bi-encoder is trained in a unique way with two clones of
    a single LLM trained in parallel to learn similarities between documents. For
    example, a bi-encoder can learn to associate questions to paragraphs so they appear
    near each other in a vector space*'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**图2.6** *双编码器以一种独特的方式训练，使用单个LLM的两个克隆并行训练，以学习文档之间的相似性。例如，双编码器可以学会将问题与段落关联起来，以便它们在向量空间中彼此靠近*'
- en: '[Listing 2.2](ch02.html#list2_2) is an example of embedding text with a pre-trained
    bi-encoder with the “sentence_transformer” package:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表2.2](ch02.html#list2_2) 是使用“sentence_transformer”包嵌入文本的示例：'
- en: '**Listing 2.2** *Getting text embeddings from a pre-trained open source bi-encoder*'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码列表2.2** *从预训练的开源双编码器获取文本嵌入*'
- en: '[PRE1]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This code creates an instance of the ‘SentenceTransformer’ class, which is initialized
    with the pre-trained model ‘multi-qa-mpnet-base-cos-v1’. This model is designed
    for multi-task learning, specifically for tasks such as question-answering and
    text classification. This one in particular was pre-trained using asymmetric data
    so we know it can handle both short queries and long documents and be able to
    compare them well. We use the ‘encode’ function from the SentenceTransformer class
    to generate vector embeddings for the documents, with the resulting embeddings
    stored in the ‘doc_emb’ variable.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码创建了一个“SentenceTransformer”类的实例，该实例使用预训练模型“multi-qa-mpnet-base-cos-v1”进行初始化。该模型旨在进行多任务学习，特别是针对问答和文本分类等任务。这个特定的模型是使用非对称数据进行预训练的，因此我们知道它可以处理短查询和长文档，并且能够很好地比较它们。我们使用“SentenceTransformer”类的“encode”函数生成文档的向量嵌入，并将生成的嵌入存储在“doc_emb”变量中。
- en: Different algorithms may perform better on different types of text data and
    will have different vector sizes. The choice of algorithm can have a significant
    impact on the quality of the resulting embeddings. Additionally, open-source alternatives
    may require more customization and fine-tuning than closed-source products, but
    they also provide greater flexibility and control over the embedding process.
    For more examples of using open-source bi-encoders to embed text, check out the
    code portion of this book!
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的算法可能在不同的文本数据类型上表现更好，并且会有不同的向量大小。算法的选择可能会对结果嵌入的质量产生重大影响。此外，开源替代品可能需要比闭源产品更多的定制和微调，但它们也提供了对嵌入过程的更大灵活性和控制。有关使用开源双编码器嵌入文本的更多示例，请参阅本书的代码部分！
- en: Document Chunker
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 文档块分割器
- en: Once we have our text embedding engine set up, we need to consider the challenge
    of embedding large documents. It is often not practical to embed entire documents
    as a single vector, particularly when dealing with long documents such as books
    or research papers. One solution to this problem is to use document chunking,
    which involves dividing a large document into smaller, more manageable chunks
    for embedding.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们设置了文本嵌入引擎，我们需要考虑嵌入大型文档的挑战。通常，将整个文档作为一个单一向量嵌入并不实用，尤其是在处理书籍或研究论文等长文档时。解决这个问题的一个方法就是使用文档块分割，这涉及将大型文档分割成更小、更易于管理的块以进行嵌入。
- en: Max Token Window Chunking
  id: totrans-74
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 最大标记窗口分割
- en: One approach to document chunking is max token window chunking. This is one
    of the easiest methods to implement and involves splitting the document into chunks
    of a given max size. So if we set a token window to be 500, then we’d expect each
    chunk to be just below 500 tokens. Having our chunks all be around the same size
    will also help make our system more consistent.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 文档块分割的一种方法是最大标记窗口分割。这是最容易实现的方法之一，涉及将文档分割成给定最大大小的块。所以如果我们设置标记窗口为500，那么我们预计每个块将接近500个标记。让我们的块大小大致相同也将有助于使我们的系统更加一致。
- en: One common concern of this method is that we might accidentally cut off some
    important text between chunks, splitting up the context. To mitigate this, we
    can set overlapping windows with a specified amount of tokens to overlap so we
    have tokens shared between chunks. This of course introduces a sense of redundancy
    but this is often fine in service of higher accuracy and latency.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的一个常见担忧是我们可能会意外地切断块之间的某些重要文本，从而分割上下文。为了减轻这种情况，我们可以设置重叠窗口，并指定重叠的标记数量，以便块之间共享标记。当然，这会引入一定的冗余，但为了更高的准确性和延迟，这通常是可接受的。
- en: Let’s see an example of overlapping window chunking with some sample text ([Listing
    2.3](ch02.html#list2_3)). Let’s begin by ingesting a large document. How about
    a recent book I wrote with over 400 pages?
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一些示例文本（[列表2.3](ch02.html#list2_3)）来看一下重叠窗口块分割的例子。让我们首先摄入一个大型文档。比如我最近写的一本超过400页的书？
- en: '**Listing 2.3** *Ingesting an entire textbook*'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**列表2.3** *摄入整本教科书*'
- en: '[PRE2]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: And now let’s chunk this document by getting chunks of at most a certain token
    size ([Listing 2.4](ch02.html#list2_4)).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们通过获取最多一定标记大小的块来分割这个文档（[列表2.4](ch02.html#list2_4)）。
- en: '**Listing 2.4** *Chunking the textbook with and without overlap*'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**列表2.4** *带有和不带有重叠的教科书块分割*'
- en: '[PRE3]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: With overlap, we see an increase in the number of document chunks but around
    the same size. The higher the overlapping factor, the more redundancy we introduce
    into the system. The max token window method does not take into account the natural
    structure of the document and may result in information being split up between
    chunks or chunks with overlapping information, confusing the retrieval system.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 当存在重叠时，我们可以看到文档块的数量增加，但大小大致相同。重叠因子越高，我们向系统中引入的冗余就越多。最大标记窗口方法没有考虑到文档的自然结构，可能会导致信息在块之间或重叠信息的块之间被分割，从而混淆检索系统。
- en: Finding Custom Delimiters
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 寻找自定义分隔符
- en: To help aid our chunking method, we could search for custom natural delimiters.
    We would identify natural white spaces within the text and use them to create
    more meaningful units of text that will end up in document chunks that will eventually
    get embedded ([Figure 2.7](ch02.html#ch02fig07)).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助辅助我们的块分割方法，我们可以搜索自定义的自然分隔符。我们会在文本中识别自然空白，并使用它们来创建更有意义的文本单元，这些单元最终将成为文档块，并最终被嵌入（[图2.7](ch02.html#ch02fig07)）。
- en: '![Images](graphics/02fig07.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/02fig07.jpg)'
- en: '**Figure 2.7** *Max-token chunking (on the left) and natural whitespace chunking
    (on the right) can be done with or without overlap. The natural whitespace chunking
    tends to end up with non-uniform chunk sizes.*'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**图2.7** *最大标记分块（左侧）和自然空白分块（右侧）可以重叠或不重叠进行。自然空白分块往往会导致分块大小不均匀*'
- en: Let’s look for common whitespaces in the textbook ([Listing 2.5](ch02.html#list2_5)).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在教科书中寻找常见的空白（[列表2.5](ch02.html#list2_5)）。
- en: '**Listing 2.5** *Chunking the textbook with natural whitespace*'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**列表2.5** *使用自然空白分块教科书*'
- en: '[PRE4]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The most common double white space is two newline characters in a row which
    is actually how I earlier distinguished between pages which makes sense. The most
    natural whitespace in a book is by page. In other cases, we may have found natural
    whitespace between paragraphs as well. This method is very hands-on and requires
    a good amount of familiarity and knowledge of the source documents.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的双空格是连续的两个换行符，这实际上是我之前区分页面的一种方式，这是有意义的。一本书中最自然的空白是按页码划分。在其他情况下，我们可能在段落之间也发现了自然空白。这种方法非常实际，需要大量熟悉和了解源文档的知识。
- en: We can also turn to more machine learning to get slightly more creative with
    how we architect document chunks.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以转向更高级的机器学习，以更巧妙地构建文档分块。
- en: Using Clustering to Create Semantic Documents
  id: totrans-93
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用聚类创建语义文档
- en: Another approach to document chunking is to use clustering to create semantic
    documents. This approach involves creating new documents by combining small chunks
    of information that are semantically similar ([Figure 2.8](ch02.html#ch02fig08)).
    This approach requires some creativity, as any modifications to the document chunks
    will alter the resulting vector. We could use an instance of Agglomerative clustering
    from scikit-learn, for example, where similar sentences or paragraphs are grouped
    together to form new documents.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种文档分块的方法是使用聚类来创建语义文档。这种方法涉及通过组合语义相似的小块信息来创建新文档（[图2.8](ch02.html#ch02fig08)）。这种方法需要一些创造性，因为对文档分块的任何修改都会改变结果向量。例如，我们可以使用scikit-learn中的层次聚类的一个实例，将相似的句子或段落分组在一起形成新文档。
- en: '![Images](graphics/02fig08.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/02fig08.jpg)'
- en: '**Figure 2.8** *We can group any kinds of document chunks together by using
    some separate semantic clustering system (shown on the right) to create brand
    new documents with chunks of information in them that are similar to each other.*'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**图2.8** *我们可以使用一些独立的语义聚类系统（如图右侧所示）将任何类型的文档分块组合在一起，创建包含相似信息分块的新文档*'
- en: Let’s try to cluster together those chunks we found from the textbook in our
    last section ([Listing 2.6](ch02.html#list2_6)).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试将我们在上一节中从教科书中找到的这些分块聚类在一起（[列表2.6](ch02.html#list2_6)）。
- en: '**Listing 2.6** *Clustering pages of the document by semantic similarity*'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**列表2.6** *通过语义相似性聚类文档页面*'
- en: '[PRE5]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This approach tends to yield chunks that are more cohesive semantically but
    suffer from pieces of content being out of context with surrounding text. This
    approach works well when the chunks you start with are known to not necessarily
    relate to each other i.e. chunks are more independent of one another.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法往往会产生语义上更连贯的分块，但内容片段可能与周围文本脱节。当起始分块已知不一定相互关联时，即分块彼此更独立时，这种方法效果良好。
- en: Use Entire Documents Without Chunking
  id: totrans-101
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 无需分块使用整个文档
- en: Alternatively, it is possible to use entire documents without chunking. This
    approach is probably the easiest option overall but will have drawbacks when documents
    are far too long and we hit a context window limit when we embed the text. We
    also might fall victim to the documents being filled with extraneous disparate
    context points and the resulting embeddings may be trying to encode too much and
    may suffer in quality. These drawbacks compound for very large (multi-page) documents.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，也可以不进行分块而使用整个文档。这种方法可能是所有选项中最简单的，但当文档非常长时，我们可能会遇到上下文窗口限制，在嵌入文本时。我们也可能成为那些充满无关分散上下文点的文档的受害者，结果嵌入可能试图编码过多的内容，并且可能质量受损。对于非常大的（多页）文档，这些缺点会叠加。
- en: It is important to consider the trade-offs between chunking and using entire
    documents when selecting an approach for document embedding ([Table 2.1](ch02.html#ch02tab01)).
    Once we decide how we want to chunk our documents, we need a home for the embeddings
    we create. Locally, we can rely on matrix operations for quick retrieval, but
    we are building for the cloud here, so let’s look at our database options.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择文档嵌入方法时，考虑分块与使用整个文档之间的权衡很重要（[表2.1](ch02.html#ch02tab01)）。一旦我们决定如何分块我们的文档，我们需要为创建的嵌入找到一个家。在本地，我们可以依赖矩阵操作进行快速检索，但我们在构建云服务，所以让我们来看看我们的数据库选项。
- en: '**Table 2.1** *Outlining different document chunking methods with pros and
    cons*'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**表2.1** *概述不同的文档分块方法及其优缺点*'
- en: '![Images](graphics/02tab01.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/02tab01.jpg)'
- en: Vector Databases
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 向量数据库
- en: A **vector database** is a data storage system that is specifically designed
    to both store and retrieve vectors quickly. This type of database is useful for
    storing embeddings generated by an LLM which encode and store the semantic meaning
    of our documents or chunks of documents. By storing embeddings in a vector database,
    we can efficiently perform nearest-neighbor searches to retrieve similar pieces
    of text based on their semantic meaning.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**向量数据库**是一种专门设计用于快速存储和检索向量的数据存储系统。此类数据库对于存储由LLM生成的嵌入很有用，这些嵌入编码并存储了我们文档或文档片段的语义意义。通过在向量数据库中存储嵌入，我们可以高效地执行最近邻搜索，根据其语义意义检索相似文本。'
- en: Pinecone
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Pinecone
- en: Pinecone is a vector database that is designed for small to medium-sized datasets
    (usually ideal for less than 1 million entries). It is easy to get started with
    Pinecone for free, but it also has a pricing plan that provides additional features
    and increased scalability. Pinecone is optimized for fast vector search and retrieval,
    making it a great choice for applications that require low-latency search, such
    as recommendation systems, search engines, and chatbots.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Pinecone是一个为小型到中型数据集（通常理想情况下少于100万条记录）设计的向量数据库。使用Pinecone开始是免费的，但它也提供了一种定价计划，该计划提供额外功能和更高的可扩展性。Pinecone针对快速向量搜索和检索进行了优化，使其成为需要低延迟搜索的应用程序（如推荐系统、搜索引擎和聊天机器人）的理想选择。
- en: Open-source Alternatives
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 开源替代方案
- en: There are several open-source alternatives to Pinecone that can be used to build
    a vector database for LLM embeddings. One such alternative is Pgvector, a PostgreSQL
    extension that adds support for vector data types and provides fast vector operations.
    Another option is Weaviate, a cloud-native, open-source vector database that is
    designed for machine learning applications. Weaviate provides support for semantic
    search and can be integrated with other machine learning tools such as TensorFlow
    and PyTorch. ANNOY is an open-source library for approximate nearest neighbor
    search that is optimized for large-scale datasets. It can be used to build a custom
    vector database that is tailored to specific use cases.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Pinecone，存在几种开源替代方案，可用于构建用于LLM嵌入的向量数据库。其中一种替代方案是Pgvector，这是一个PostgreSQL扩展，它增加了对向量数据类型的支持，并提供了快速的向量操作。另一个选项是Weaviate，这是一个专为机器学习应用设计的云原生、开源向量数据库。Weaviate提供了对语义搜索的支持，可以与其他机器学习工具如TensorFlow和PyTorch集成。ANNOY是一个用于近似最近邻搜索的开源库，针对大规模数据集进行了优化。它可以用来构建针对特定用例定制的向量数据库。
- en: Re-ranking the Retrieved Results
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 重排序检索结果
- en: After retrieving potential results from a vector database given a query using
    a similarity like cosine similarity, it is often useful to re-rank them to ensure
    that the most relevant results are presented to the user ([Figure 2.9](ch02.html#ch02fig09)).
    One way to re-rank results is by using a cross-encoder, which is a type of transformer
    model that takes pairs of input sequences and predicts a score indicating how
    relevant the second sequence is to the first. By using a cross-encoder to re-rank
    search results, we can take into account the entire query context rather than
    just individual keywords. This of course will add some overhead and worsen our
    latency but it could help us in terms of performance. I will take the time to
    outline some results in a later section to compare and contrast using and not
    using a cross-encoder.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用余弦相似度等相似度度量从向量数据库中检索给定查询的潜在结果后，通常很有用重新排序它们，以确保向用户展示最相关的结果（[图2.9](ch02.html#ch02fig09)）。重新排序结果的一种方法是通过使用跨编码器，这是一种接受成对输入序列并预测一个分数，表示第二个序列对第一个序列的相关性的Transformer模型。通过使用跨编码器重新排序搜索结果，我们可以考虑整个查询上下文，而不仅仅是单个关键词。当然，这会增加一些开销并降低我们的延迟，但它可能在性能方面有所帮助。我将在稍后的部分花时间概述一些结果，以比较和对比使用和不使用跨编码器的情况。
- en: '![Images](graphics/02fig09.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/02fig09.jpg)'
- en: '**Figure 2.9** *A cross-encoder (left) takes in two pieces of text and outputs
    a similarity score without returning a vectorized format of the text. A bi-encoder
    (right), on the other hand, embeds a bunch of pieces of text into vectors up front
    and then retrieves them later in real time given a query (e.g. looking up “I’m
    a Data Scientist”)*'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**图2.9** *一个跨编码器（左）接受两段文本并输出一个相似度分数，而不返回文本的向量格式。另一方面，双编码器（右）首先将大量文本嵌入到向量中，然后在给定查询的情况下实时检索它们（例如，查找“我是一个数据科学家”)*'
- en: One popular source of cross-encoder models is the Sentence Transformers library,
    which is where we found our bi-encoders earlier. We can also fine-tune a pre-trained
    cross-encoder model on our task-specific dataset to improve the relevance of search
    results and provide more accurate recommendations.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 跨编码器模型的一个流行来源是Sentence Transformers库，我们之前就是在这里找到了我们的双编码器。我们还可以在我们的特定任务数据集上微调预训练的跨编码器模型，以提高搜索结果的相关性并提供更准确的推荐。
- en: Another option for re-ranking search results is by using a traditional retrieval
    model like BM25, which ranks results by the frequency of query terms in the document
    and takes into account term proximity and inverse document frequency. While BM25
    does not take into account the entire query context, it can still be a useful
    way to re-rank search results and improve the overall relevance of the results.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种重新排序搜索结果的方法是使用传统的检索模型，如BM25，它根据文档中查询词的频率对结果进行排序，并考虑词的邻近性和逆文档频率。虽然BM25没有考虑整个查询上下文，但它仍然可以是一种重新排序搜索结果并提高结果整体相关性的有用方法。
- en: API
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: API
- en: We now need a place to put all of these components so that users can access
    the documents in a fast, secure, and easy way. To do this, let’s create an API.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要一个地方来放置所有这些组件，以便用户可以快速、安全、方便地访问文档。为此，让我们创建一个API。
- en: FastAPI
  id: totrans-120
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: FastAPI
- en: '**FastAPI** is a web framework for building APIs with Python quickly. It is
    designed to be both fast and easy to set up, making it an excellent choice for
    our semantic search API. FastAPI uses the Pydantic data validation library to
    validate request and response data and uses the high-performance ASGI server,
    uvicorn.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**FastAPI**是一个用于使用Python快速构建API的Web框架。它旨在既快速又易于设置，因此是构建语义搜索API的一个优秀选择。FastAPI使用Pydantic数据验证库来验证请求和响应数据，并使用高性能的ASGI服务器uvicorn。'
- en: Setting up a FastAPI project is straightforward and requires minimal configuration.
    FastAPI provides automatic documentation generation with the OpenAPI standard,
    which makes it easy to build API documentation and client libraries. [Listing
    2.7](ch02.html#list2_7) is a skeleton of what that file would look like.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 设置FastAPI项目非常简单，只需要最小配置。FastAPI使用OpenAPI标准自动生成文档，这使得构建API文档和客户端库变得容易。[列表2.7](ch02.html#list2_7)是这个文件的外观骨架。
- en: '**Listing 2.7** *FastAPI skeleton code*'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**列表2.7** *FastAPI骨架代码*'
- en: '[PRE6]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: For the full file, be sure to check out the code repository for this book!
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 对于完整的文件，请务必查看本书的代码仓库！
- en: Putting It All Together
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将所有这些组件放在一起
- en: We now have a solution for all of our components. Let’s take a look at where
    we are in our solution. Items in bold are new from the last time we outlined this
    solution.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在为所有组件都找到了解决方案。让我们看看我们的解决方案进展如何。加粗的项目是上次我们概述此解决方案时新增的。
- en: '![Images](graphics/square.jpg) PART I - Ingesting documents'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](graphics/square.jpg) 第一部分 - 摄入文档'
- en: 1\. Collect documents for embedding - **Chunk them**
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 收集文档进行嵌入 - **分块**
- en: 2\. Create text embeddings to encode semantic information - **OpenAI’s Embedding**
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 创建文本嵌入以编码语义信息 - **OpenAI 的嵌入**
- en: 3\. Store embeddings in a database for later retrieval given a query - **Pinecone**
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 将嵌入存储在数据库中，以便在给定查询时检索 - **Pinecone**
- en: '![Images](graphics/square.jpg) PART II - Retrieving documents'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](graphics/square.jpg) 第二部分 - 检索文档'
- en: 1\. User has a query which may be pre-processed and cleaned - **FastAPI**
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 用户有一个查询，可能需要预处理和清理 - **FastAPI**
- en: 2\. Retrieve candidate documents - **OpenAI’s Embedding + Pinecone**
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 检索候选文档 - **OpenAI 的嵌入 + Pinecone**
- en: 3\. Re-rank the candidate documents if necessary - **Cross-Encoder**
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 如果需要，重新排名候选文档 - **交叉编码器**
- en: 4\. Return the final search results - **FastAPI**
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 返回最终搜索结果 - **FastAPI**
- en: With all of these moving parts, let’s take a look at our final system architecture
    in [Figure 2.10](ch02.html#ch02fig010).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些组件中，让我们看看我们的最终系统架构在[图 2.10](ch02.html#ch02fig010)中。
- en: '![Images](graphics/02fig10.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/02fig10.jpg)'
- en: '**Figure 2.10** *Our complete semantic search architecture using two closed-source
    systems (OpenAI and Pinecone) and an open source API framework (FastAPI)*'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.10** *我们使用两个闭源系统（OpenAI 和 Pinecone）和一个开源 API 框架（FastAPI）构建的完整语义搜索架构*'
- en: We now have a complete end to end solution for our semantic search. Let’s see
    how well the system performs against a validation set.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有一个完整的端到端解决方案用于我们的语义搜索。让我们看看系统在验证集上的表现如何。
- en: Performance
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 性能
- en: 'I’ve outlined a solution to the problem of semantic search, but I want to also
    talk about how to test how these different components work together. For this,
    let’s use a well-known dataset to run against: the **BoolQ** dataset - a question
    answering dataset for yes/no questions containing nearly 16K examples. This dataset
    has pairs of (question, passage) that indicate for a given question, that passage
    would be the best passage to answer the question.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我概述了解决语义搜索问题的方案，但我也想谈谈如何测试这些不同组件如何协同工作。为此，让我们使用一个知名的数据集来运行：**BoolQ** 数据集 - 一个包含近
    16K 个示例的 yes/no 问题问答数据集。此数据集包含（问题，段落）对，表示对于给定问题，该段落将是回答问题的最佳段落。
- en: '[Table 2.2](ch02.html#ch02tab02) outlines a few trials I ran and coded up in
    the code for this book. I use combinations of embedders, re-ranking solutions,
    and a bit of fine-tuning to try and see how well the system performs on two fronts:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 2.2](ch02.html#ch02tab02) 列出了我在本书代码中运行并编码的一些试验。我使用嵌入器、重新排名解决方案和一点微调的组合来尝试查看系统在两个方面的表现：'
- en: 1\. Performance - as indicated by the **top result accuracy**. For each known
    pair of (question, passage) in our BoolQ validation set - 3,270 examples, we will
    test if the system’s top result is the intended passage. This is not the only
    metric we could have used. The sentence_transformers library has other metrics
    including ranking evaluation, correlation evaluation, and more
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 性能 - 如**顶部结果准确率**所示。对于我们的 BoolQ 验证集中每个已知的（问题，段落）对 - 3,270 个示例，我们将测试系统的顶部结果是否是预期的段落。这并非我们能够使用的唯一指标。sentence_transformers
    库还有其他指标，包括排名评估、相关性评估等。
- en: 2\. Latency - I want to see how long it takes to run through these examples
    using Pinecone, so for each embedder, I reset the index and uploaded new vectors
    and used cross-encoders in my laptop’s memory to keep things simple and standardized.
    I will measure latency in **minutes** it took to run against the validation set
    of the BoolQ dataset
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 延迟 - 我想看看使用 Pinecone 运行这些示例需要多长时间，因此对于每个嵌入器，我重置了索引，上传了新的向量，并在笔记本电脑的内存中使用交叉编码器以保持简单和标准化。我将测量运行对
    BoolQ 数据集验证集的延迟，以**分钟**为单位。
- en: '**Table 2.2** *Performance results from various combinations against the BoolQ
    validation set*'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 2.2** *各种组合对 BoolQ 验证集的性能结果*'
- en: '![Images](graphics/02tab02.jpg)![Images](graphics/02tab02a.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/02tab02.jpg)![图片](graphics/02tab02a.jpg)'
- en: 'Some experiments I didn’t try include the following:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我没有尝试的一些实验包括以下内容：
- en: 1\. Fine-tuning the cross-encoder for more epochs and spending more time finding
    optimal learning parameters (e.g. weight decay, learning rate scheduler, etc)
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 为了更多轮次和更多时间寻找最佳学习参数（例如，权重衰减、学习率调度器等）微调交叉编码器
- en: 2\. Using other OpenAI embedding engines
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 使用其他 OpenAI 嵌入引擎
- en: 3\. Fine-tuning an open-source bi-encoder on the training set
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 在训练集上微调开源双编码器
- en: Note that the models I used for the cross-encoder and the bi-encoder were both
    specifically pre-trained on data that is similar to asymmetric semantic search.
    This is important because we want the embedder to produce vectors for both short
    queries and long documents and place them near each other when they are related.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我用于交叉编码器和双编码器的模型都是在类似于非对称语义搜索的数据上专门预训练的。这很重要，因为我们希望嵌入器为短查询和长文档生成向量，并在它们相关时将它们放置在一起。
- en: Let’s assume we want to keep things simple to get things off of the ground and
    use only the OpenAI embedder and do no re-ranking (row 1) in our application.
    Let’s consider the costs associated with using FastAPI, Pinecone, and OpenAI for
    text embeddings.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要保持简单，以便让事情起步，只使用 OpenAI 嵌入器，并在我们的应用程序中不进行重新排序（第 1 行）。让我们考虑使用 FastAPI、Pinecone
    和 OpenAI 进行文本嵌入的相关成本。
- en: The Cost of Closed-Source
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 封闭源的成本
- en: We have a few components in play and not all of them are free. Fortunately FastAPI
    is an open-source framework and does not require any licensing fees. Our cost
    with FastAPI is hosting which could be on a free tier depending on what service
    we use. I like Render which has a free tier but also pricing starts at $7/month
    for 100% uptime. At the time of writing, Pinecone offers a free tier with a limit
    of 100,000 embeddings and up to 3 indexes, but beyond that, they charge based
    on the number of embeddings and indexes used. Their Standard plan charges $49/month
    for up to 1 million embeddings and 10 indexes.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有几个组件在运行，但并非所有都是免费的。幸运的是，FastAPI 是一个开源框架，并且不需要任何许可费用。我们使用 FastAPI 的成本是托管费用，这取决于我们使用的服务，可能是免费层。我喜欢
    Render，它有一个免费层，但价格从每月 7 美元开始，保证 100% 的正常运行时间。在撰写本文时，Pinecone 提供了一个免费层，限制为 100,000
    个嵌入和最多 3 个索引，但超出这个范围，他们根据使用的嵌入和索引数量收费。他们的标准计划每月收费 49 美元，最多支持 1,000 万个嵌入和 10 个索引。
- en: OpenAI offers a free tier of their text embedding service, but it is limited
    to 100,000 requests per month. Beyond that, they charge $0.0004 per 1,000 tokens
    for the embedding engine we used - Ada-002\. If we assume an average of 500 tokens
    per document, the cost per document would be $0.0002\. For example, if we wanted
    to embed 1 million documents, it would cost approximately $200.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 提供了其文本嵌入服务的免费层，但每月限制为 100,000 个请求。超出这个范围，他们根据我们使用的嵌入引擎 - Ada-002 - 收费
    $0.0004 每千个标记。如果我们假设每份文档平均有 500 个标记，那么每份文档的成本将是 $0.0002。例如，如果我们想要嵌入 1,000 万份文档，那么成本大约是
    $200。
- en: 'If we want to build a system with 1 million embeddings, and we expect to update
    the index once a month with totally fresh embeddings, the total cost per month
    would be:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要构建一个包含 1,000 万个嵌入的系统，并且我们预计每月更新一次索引，使用全新的嵌入，那么每月的总成本将是：
- en: Pinecone Cost = $49
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Pinecone 成本 = $49
- en: OpenAI Cost = $200
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 成本 = $200
- en: FastAPI Cost = $7
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: FastAPI 成本 = $7
- en: Total Cost = $49 + $200 + $7 = **$256/month**
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 总成本 = $49 + $200 + $7 = **$256/month**
- en: A nice binary number :) Not intended but still fun.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很好的二进制数 :) 虽然不是故意为之，但仍然很有趣。
- en: These costs can quickly add up as the system scales, and it may be worth exploring
    open-source alternatives or other strategies to reduce costs - like using open-source
    bi-encoders for embedding or Pgvector as your vector database.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 随着系统的扩展，这些成本可能会迅速增加，因此探索开源替代方案或其他降低成本的战略可能值得考虑 - 例如，使用开源双编码器进行嵌入或使用 Pgvector
    作为您的向量数据库。
- en: Summary
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: With all of these components accounted for, our pennies added up, and alternatives
    available at every step of the way, I’ll leave you all to it. Enjoy setting up
    your new semantic search system and be sure to check out the complete code for
    this - including a fully working FastAPI app with instructions on how to deploy
    it - on the book’s code repository and experiment to your heart’s content to try
    and make this work as well as possible for your domain-specific data.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到所有这些组件，我们的成本逐渐增加，并且每一步都有可用的替代方案，我将把你们留给它。享受设置您的新语义搜索系统，并确保查看书籍的代码仓库中的完整代码
    - 包括一个完全工作的 FastAPI 应用程序以及如何部署它的说明 - 并尽情实验，以尝试使它尽可能适合您的特定领域数据。
- en: Stay tuned for our next chapter where we will build on this API with a chatbot
    built using GPT-4 and our retrieval system.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 请继续关注我们的下一章节，我们将在这个API的基础上，使用GPT-4构建的聊天机器人和我们的检索系统来进一步开发。
