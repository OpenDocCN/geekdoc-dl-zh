- en: Reinforcement Learning (i.e.Â Policy Gradient Algorithms)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ ï¼ˆå³ç­–ç•¥æ¢¯åº¦ç®—æ³•ï¼‰
- en: In the RLHF process, the reinforcement learning algorithm slowly updates the
    modelâ€™s weights with respect to feedback from a reward model. The policy â€“ the
    model being trained â€“ generates completions to prompts in the training set, then
    the reward model scores them, and then the reinforcement learning optimizer takes
    gradient steps based on this information. This chapter explains the mathematics
    and trade-offs across various algorithms used to learn from the signal the reward
    model gives to on-policy data. These algorithms are run for a period of many epochs,
    often thousands or millions of batches across a larger set of prompts, with gradient
    updates in between each of them.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨RLHFè¿‡ç¨‹ä¸­ï¼Œå¼ºåŒ–å­¦ä¹ ç®—æ³•æ ¹æ®å¥–åŠ±æ¨¡å‹çš„åé¦ˆç¼“æ…¢åœ°æ›´æ–°æ¨¡å‹çš„æƒé‡ã€‚ç­–ç•¥â€”â€”æ­£åœ¨è®­ç»ƒçš„æ¨¡å‹â€”â€”ç”Ÿæˆè®­ç»ƒé›†ä¸­çš„æç¤ºçš„å®Œæˆå†…å®¹ï¼Œç„¶åå¥–åŠ±æ¨¡å‹å¯¹å…¶è¿›è¡Œè¯„åˆ†ï¼Œç„¶åå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å™¨æ ¹æ®è¿™äº›ä¿¡æ¯è¿›è¡Œæ¢¯åº¦æ­¥éª¤ã€‚æœ¬ç« è§£é‡Šäº†ç”¨äºä»å¥–åŠ±æ¨¡å‹å¯¹åœ¨çº¿ç­–ç•¥æ•°æ®æä¾›çš„ä¿¡å·ä¸­å­¦ä¹ çš„å„ç§ç®—æ³•çš„æ•°å­¦å’Œæƒè¡¡ã€‚è¿™äº›ç®—æ³•åœ¨è®¸å¤šä¸ªepochæœŸé—´è¿è¡Œï¼Œé€šå¸¸åœ¨æ›´å¤§çš„æç¤ºé›†ä¸Šè·¨è¶Šæ•°åƒæˆ–æ•°ç™¾ä¸‡ä¸ªæ‰¹æ¬¡ï¼Œå¹¶åœ¨å®ƒä»¬ä¹‹é—´è¿›è¡Œæ¢¯åº¦æ›´æ–°ã€‚
- en: The algorithms that popularized RLHF for language models were policy-gradient
    reinforcement learning algorithms. These algorithms, such as Proximal Policy Optimization
    (PPO), Group Relative Policy Optimization (GRPO), and REINFORCE, use recently
    generated samples to update their model (rather than storing scores in a replay
    buffer like algorithms, e.g.Â Deep Q-Networks, DQN, used in popular projects such
    as AlphaGo). In this section we will cover the fundamentals of the policy gradient
    algorithms and how they are used in the modern RLHF framework.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿RLHFåœ¨è¯­è¨€æ¨¡å‹ä¸­æµè¡Œçš„ç®—æ³•æ˜¯ç­–ç•¥æ¢¯åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚è¿™äº›ç®—æ³•ï¼Œå¦‚è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆProximal Policy Optimizationï¼ŒPPOï¼‰ã€ç¾¤ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGroup
    Relative Policy Optimizationï¼ŒGRPOï¼‰å’ŒREINFORCEï¼Œä½¿ç”¨æœ€è¿‘ç”Ÿæˆçš„æ ·æœ¬æ¥æ›´æ–°å®ƒä»¬çš„æ¨¡å‹ï¼ˆè€Œä¸æ˜¯åƒåœ¨AlphaGoç­‰æµè¡Œé¡¹ç›®ä¸­ä½¿ç”¨çš„Deep
    Q-Networksï¼ŒDQNç­‰ç®—æ³•é‚£æ ·åœ¨é‡æ”¾ç¼“å†²åŒºä¸­å­˜å‚¨åˆ†æ•°ï¼‰ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»ç­–ç•¥æ¢¯åº¦ç®—æ³•çš„åŸºæœ¬åŸç†ä»¥åŠå®ƒä»¬åœ¨ç°ä»£RLHFæ¡†æ¶ä¸­çš„åº”ç”¨ã€‚
- en: At a machine learning level, this section is the subject with the highest complexity
    in the RLHF process. Though, as with most modern AI models, the largest determining
    factor on its success is the data provided as inputs to the process.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœºå™¨å­¦ä¹ å±‚é¢ï¼Œæœ¬èŠ‚æ˜¯RLHFè¿‡ç¨‹ä¸­æœ€å¤æ‚çš„ä¸»é¢˜ã€‚å°½ç®¡å¦‚æ­¤ï¼Œä¸å¤§å¤šæ•°ç°ä»£AIæ¨¡å‹ä¸€æ ·ï¼Œå…¶æˆåŠŸæœ€å¤§çš„å†³å®šå› ç´ æ˜¯æä¾›ç»™è¿‡ç¨‹çš„è¾“å…¥æ•°æ®ã€‚
- en: When RLHF came onto the scene with ChatGPT, it was largely known that they used
    a variant of PPO, and many initial efforts were built upon that. Over time, multiple
    research projects showed the promise of REINFORCE-style algorithms [[178]](ch021.xhtml#ref-ahmadian2024back)
    [[111]](ch021.xhtml#ref-wang2024helpsteer2p), touted for its simplicity over PPO
    without a reward model (saves memory and therefore the number of GPUs required)
    and with simpler value estimation (no Generalized Advantage Estimation, GAE, which
    is a method to compute advantages used for variance reduction in policy gradient
    algorithms). More algorithms have emerged, including Group Relative Policy Optimization,
    which is particularly popular with reasoning tasks, but in general many of these
    algorithms can be tuned to fit a specific task. In this chapter, we cover the
    core policy gradient setup and the three algorithms mentioned above due to their
    central role in the establishment of a canonical RLHF literature.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ChatGPTå¼•å…¥RLHFï¼ˆåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼‰æ—¶ï¼Œäººä»¬æ™®éçŸ¥é“å®ƒä»¬ä½¿ç”¨äº†PPOï¼ˆç­–ç•¥æ¢¯åº¦ä¼˜åŒ–ï¼‰çš„ä¸€ä¸ªå˜ä½“ï¼Œå¹¶ä¸”è®¸å¤šåˆæ­¥å·¥ä½œéƒ½æ˜¯åŸºäºè¿™ä¸ªå˜ä½“æ„å»ºçš„ã€‚éšç€æ—¶é—´çš„æ¨ç§»ï¼Œå¤šä¸ªç ”ç©¶é¡¹ç›®å±•ç¤ºäº†REINFORCEé£æ ¼ç®—æ³•çš„æ½œåŠ›
    [[178]](ch021.xhtml#ref-ahmadian2024back) [[111]](ch021.xhtml#ref-wang2024helpsteer2p)ï¼Œè¿™ç§ç®—æ³•å› å…¶ç›¸å¯¹äºPPOçš„ç®€å•æ€§è€Œå—åˆ°èµèª‰ï¼ˆæ— éœ€å¥–åŠ±æ¨¡å‹ï¼ŒèŠ‚çœå†…å­˜ï¼Œå› æ­¤æ‰€éœ€çš„GPUæ•°é‡å‡å°‘ï¼‰ä»¥åŠæ›´ç®€å•çš„ä»·å€¼ä¼°è®¡ï¼ˆæ²¡æœ‰å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡GAEï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºåœ¨ç­–ç•¥æ¢¯åº¦ç®—æ³•ä¸­è®¡ç®—ä¼˜åŠ¿ä»¥å‡å°‘æ–¹å·®çš„æ–¹æ³•ï¼‰ã€‚å‡ºç°äº†æ›´å¤šç®—æ³•ï¼ŒåŒ…æ‹¬ç¾¤ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGroup
    Relative Policy Optimizationï¼‰ï¼Œå®ƒåœ¨æ¨ç†ä»»åŠ¡ä¸­ç‰¹åˆ«å—æ¬¢è¿ï¼Œä½†æ€»çš„æ¥è¯´ï¼Œè®¸å¤šè¿™äº›ç®—æ³•éƒ½å¯ä»¥è°ƒæ•´ä»¥é€‚åº”ç‰¹å®šä»»åŠ¡ã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»æ ¸å¿ƒç­–ç•¥æ¢¯åº¦è®¾ç½®ä»¥åŠä¸Šè¿°æåˆ°çš„ä¸‰ä¸ªç®—æ³•ï¼Œå› ä¸ºå®ƒä»¬åœ¨å»ºç«‹æ ‡å‡†RLHFæ–‡çŒ®ä¸­çš„æ ¸å¿ƒä½œç”¨ã€‚
- en: For definitions of symbols, see the problem setup chapter.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºç¬¦å·çš„å®šä¹‰ï¼Œè¯·å‚é˜…é—®é¢˜è®¾ç½®ç« èŠ‚ã€‚
- en: '*This chapter uses <semantics><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(s,
    a)</annotation></semantics> notation from the reinforcement learning literature,
    where <semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics>
    denotes states and <semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics>
    denotes actions. In the language model context, you will often see <semantics><mrow><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(x, y)</annotation></semantics>
    instead, where <semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics>
    is the prompt and <semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics>
    is the completion. The <semantics><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(s,
    a)</annotation></semantics> framing is more generalâ€”these algorithms were designed
    for sequential decision problems where actions are taken at each timestep. However,
    many RLHF implementations treat the entire completion as a single action, making
    the <semantics><mrow><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(x,
    y)</annotation></semantics> notation equally valid.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*æœ¬ç« é‡‡ç”¨äº†å¼ºåŒ–å­¦ä¹ æ–‡çŒ®ä¸­çš„<semantics><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(s,
    a)</annotation></semantics>ç¬¦å·ï¼Œå…¶ä¸­<semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics>è¡¨ç¤ºçŠ¶æ€ï¼Œ<semantics><mi>a</mi><annotation
    encoding="application/x-tex">a</annotation></semantics>è¡¨ç¤ºåŠ¨ä½œã€‚åœ¨è¯­è¨€æ¨¡å‹è¯­å¢ƒä¸­ï¼Œä½ ç»å¸¸ä¼šçœ‹åˆ°<semantics><mrow><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(x, y)</annotation></semantics>ï¼Œå…¶ä¸­<semantics><mi>x</mi><annotation
    encoding="application/x-tex">x</annotation></semantics>æ˜¯æç¤ºï¼Œ<semantics><mi>y</mi><annotation
    encoding="application/x-tex">y</annotation></semantics>æ˜¯å®Œæˆã€‚è¿™ç§<semantics><mrow><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(s, a)</annotation></semantics>çš„æ¡†æ¶æ›´ä¸ºé€šç”¨â€”â€”è¿™äº›ç®—æ³•æ˜¯ä¸ºæ¯ä¸ªæ—¶é—´æ­¥é•¿é‡‡å–åŠ¨ä½œçš„é¡ºåºå†³ç­–é—®é¢˜è€Œè®¾è®¡çš„ã€‚ç„¶è€Œï¼Œè®¸å¤šRLHFå®ç°å°†æ•´ä¸ªå®Œæˆè§†ä¸ºå•ä¸€åŠ¨ä½œï¼Œä½¿å¾—<semantics><mrow><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(x, y)</annotation></semantics>ç¬¦å·åŒæ ·æœ‰æ•ˆã€‚*'
- en: Policy Gradient Algorithms
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ”¿ç­–æ¢¯åº¦ç®—æ³•
- en: 'Reinforcement learning algorithms are designed to maximize the future, discounted
    reward across a trajectory of states, <semantics><mrow><mi>s</mi><mo>âˆˆ</mo><mi>ğ’®</mi></mrow><annotation
    encoding="application/x-tex">s \in \mathcal{S}</annotation></semantics>, and actions,
    <semantics><mrow><mi>a</mi><mo>âˆˆ</mo><mi>ğ’œ</mi></mrow><annotation encoding="application/x-tex">a
    \in \mathcal{A}</annotation></semantics> (for more notation, see Chapter 3, Definitions).
    The objective of the agent, often called the *return*, is the sum of discounted
    future rewards (where <semantics><mrow><mi>Î³</mi><mo>âˆˆ</mo><mo stretchy="false"
    form="prefix">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false" form="postfix">]</mo></mrow><annotation
    encoding="application/x-tex">\gamma\in [0,1]</annotation></semantics> is a factor
    that prioritizes near-term rewards) at a given time <semantics><mi>t</mi><annotation
    encoding="application/x-tex">t</annotation></semantics>:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¼ºåŒ–å­¦ä¹ ç®—æ³•æ—¨åœ¨æœ€å¤§åŒ–è½¨è¿¹ä¸ŠçŠ¶æ€å’ŒåŠ¨ä½œçš„æœªæ¥æŠ˜ç°å¥–åŠ±ï¼Œå…¶ä¸­<semantics><mrow><mi>s</mi><mo>âˆˆ</mo><mi>ğ’®</mi></mrow><annotation
    encoding="application/x-tex">s \in \mathcal{S}</annotation></semantics>è¡¨ç¤ºçŠ¶æ€ï¼Œ<semantics><mrow><mi>a</mi><mo>âˆˆ</mo><mi>ğ’œ</mi></mrow><annotation
    encoding="application/x-tex">a \in \mathcal{A}</annotation></semantics>è¡¨ç¤ºåŠ¨ä½œï¼ˆæ›´å¤šç¬¦å·ï¼Œè§ç¬¬3ç« ï¼Œå®šä¹‰ï¼‰ã€‚ä»£ç†çš„ç›®æ ‡ï¼Œé€šå¸¸ç§°ä¸º*å›æŠ¥*ï¼Œæ˜¯åœ¨ç»™å®šæ—¶é—´<semantics><mi>t</mi><annotation
    encoding="application/x-tex">t</annotation></semantics>çš„æŠ˜ç°æœªæ¥å¥–åŠ±ä¹‹å’Œï¼ˆå…¶ä¸­<semantics><mrow><mi>Î³</mi><mo>âˆˆ</mo><mo
    stretchy="false" form="prefix">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false"
    form="postfix">]</mo></mrow><annotation encoding="application/x-tex">\gamma\in
    [0,1]</annotation></semantics>æ˜¯ä¸€ä¸ªä¼˜å…ˆè€ƒè™‘è¿‘æœŸå¥–åŠ±çš„å› å­ï¼‰:'
- en: <semantics><mrow><msub><mi>G</mi><mi>t</mi></msub><mo>=</mo><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>Î³</mi><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>2</mn></mrow></msub><mo>+</mo><mi>â‹¯</mi><mo>=</mo><munderover><mo>âˆ‘</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mo
    accent="false">âˆ</mo></munderover><msup><mi>Î³</mi><mi>k</mi></msup><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mi>.</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>33</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">G_t = R_{t+1} + \gamma R_{t+2} + \cdots = \sum_{k=0}^\infty
    \gamma^k R_{t+k+1}.\qquad{(33)}</annotation></semantics>
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><msub><mi>G</mi><mi>t</mi></msub><mo>=</mo><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>Î³</mi><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>2</mn></mrow></msub><mo>+</mo><mi>â‹¯</mi><mo>=</mo><munderover><mo>âˆ‘</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mo
    accent="false">âˆ</mo></munderover><msup><mi>Î³</mi><mi>k</mi></msup><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mi>.</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>33</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">G_t = R_{t+1} + \gamma R_{t+2} + \cdots = \sum_{k=0}^\infty
    \gamma^k R_{t+k+1}.\qquad{(33)}</annotation></semantics>
- en: 'The return definition can also be estimated as: <semantics><mrow><msub><mi>G</mi><mi>t</mi></msub><mo>=</mo><mi>Î³</mi><msub><mi>G</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mi>.</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>34</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">G_{t} = \gamma{G_{t+1}} + R_{t+1}.\qquad{(34)}</annotation></semantics>'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›å®šä¹‰ä¹Ÿå¯ä»¥ä¼°è®¡ä¸ºï¼š<semantics><mrow><msub><mi>G</mi><mi>t</mi></msub><mo>=</mo><mi>Î³</mi><msub><mi>G</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mi>.</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>34</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">G_{t} = \gamma{G_{t+1}} + R_{t+1}.\qquad{(34)}</annotation></semantics>
- en: 'This return is the basis for learning a value function <semantics><mrow><mi>V</mi><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">V(s)</annotation></semantics> that is the estimated
    future return given a current state:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå›æŠ¥æ˜¯å­¦ä¹ ä»·å€¼å‡½æ•° <semantics><mrow><mi>V</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">V(s)</annotation></semantics>
    çš„åŸºç¡€ï¼Œè¯¥å‡½æ•°æ˜¯åœ¨ç»™å®šå½“å‰çŠ¶æ€ä¸‹çš„ä¼°è®¡æœªæ¥å›æŠ¥ï¼š
- en: <semantics><mrow><mi>V</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mi>ğ”¼</mi><mo minsize="120%" maxsize="120%"
    stretchy="true" form="prefix">[</mo><msub><mi>G</mi><mi>t</mi></msub><mo stretchy="false"
    form="prefix">|</mo><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi><mo minsize="120%"
    maxsize="120%" stretchy="true" form="postfix">]</mo><mi>.</mi><mrow><mo stretchy="false"
    form="prefix">(</mo><mn>35</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">V(s) = \mathbb{E}\big[G_t | S_t = s \big].\qquad{(35)}</annotation></semantics>
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><mi>V</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mi>ğ”¼</mi><mo minsize="120%" maxsize="120%"
    stretchy="true" form="prefix">[</mo><msub><mi>G</mi><mi>t</mi></msub><mo stretchy="false"
    form="prefix">|</mo><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi><mo minsize="120%"
    maxsize="120%" stretchy="true" form="postfix">]</mo><mi>.</mi><mrow><mo stretchy="false"
    form="prefix">(</mo><mn>35</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">V(s) = \mathbb{E}\big[G_t | S_t = s \big].\qquad{(35)}</annotation></semantics>
- en: All policy gradient algorithms optimize a policy <semantics><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>a</mi><mo>âˆ£</mo><mi>s</mi><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\pi_\theta(a\mid
    s)</annotation></semantics> to maximize expected return; this objective can be
    expressed using the induced value function <semantics><mrow><msup><mi>V</mi><msub><mi>Ï€</mi><mi>Î¸</mi></msub></msup><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">V^{\pi_\theta}(s)</annotation></semantics>.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰ç­–ç•¥æ¢¯åº¦ç®—æ³•éƒ½ä¼˜åŒ–ä¸€ä¸ªç­–ç•¥ <semantics><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><mi>a</mi><mo>âˆ£</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">\pi_\theta(a\mid s)</annotation></semantics> ä»¥æœ€å¤§åŒ–æœŸæœ›å›æŠ¥ï¼›è¿™ä¸ªç›®æ ‡å¯ä»¥ç”¨è¯±å¯¼çš„ä»·å€¼å‡½æ•°
    <semantics><mrow><msup><mi>V</mi><msub><mi>Ï€</mi><mi>Î¸</mi></msub></msup><mo stretchy="false"
    form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">V^{\pi_\theta}(s)</annotation></semantics> æ¥è¡¨è¾¾ã€‚
- en: 'Where <semantics><mrow><msup><mi>d</mi><msub><mi>Ï€</mi><mi>Î¸</mi></msub></msup><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">d^{\pi_\theta}(s)</annotation></semantics> is the
    state-visitation distribution induced by policy <semantics><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>a</mi><mo>âˆ£</mo><mi>s</mi><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\pi_\theta(a
    \mid s)</annotation></semantics>, the objective we maximize can be written as:
    <semantics><mrow><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><munder><mo>âˆ‘</mo><mi>s</mi></munder><msup><mi>d</mi><msub><mi>Ï€</mi><mi>Î¸</mi></msub></msup><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo><msup><mi>V</mi><msub><mi>Ï€</mi><mi>Î¸</mi></msub></msup><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo><mo>,</mo><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>36</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">J(\theta) \;=\; \sum_{s} d^{\pi_\theta}(s)
    V^{\pi_\theta}(s), \qquad{(36)}</annotation></semantics>'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ <semantics><mrow><msup><mi>d</mi><msub><mi>Ï€</mi><mi>Î¸</mi></msub></msup><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">d^{\pi_\theta}(s)</annotation></semantics> æ˜¯ç”±ç­–ç•¥ <semantics><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>a</mi><mo>âˆ£</mo><mi>s</mi><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\pi_\theta(a
    \mid s)</annotation></semantics> è¯±å¯¼çš„çŠ¶æ€è®¿é—®åˆ†å¸ƒï¼Œæˆ‘ä»¬æœ€å¤§åŒ–çš„ç›®æ ‡å¯ä»¥è¡¨ç¤ºä¸ºï¼š<semantics><mrow><mi>J</mi><mo
    stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><munder><mo>âˆ‘</mo><mi>s</mi></munder><msup><mi>d</mi><msub><mi>Ï€</mi><mi>Î¸</mi></msub></msup><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo><msup><mi>V</mi><msub><mi>Ï€</mi><mi>Î¸</mi></msub></msup><mo
    stretchy="false" form="postfix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo><mo>,</mo><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>36</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">J(\theta) \;=\; \sum_{s} d^{\pi_\theta}(s)
    V^{\pi_\theta}(s), \qquad{(36)}</annotation></semantics>
- en: 'In a finite MDP this is a sum over all states, but in practice we never compute
    it exactly. Instead, we estimate it from data by sampling rollouts from the current
    policy. In RLHF this typically means sampling prompts <semantics><msub><mi>x</mi><mi>i</mi></msub><annotation
    encoding="application/x-tex">x_i</annotation></semantics> from a dataset and generating
    completions <semantics><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>âˆ¼</mo><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>â‹…</mi><mo>âˆ£</mo><msub><mi>x</mi><mi>i</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">y_i
    \sim \pi_\theta(\cdot\mid x_i)</annotation></semantics>, then taking an empirical
    average such as:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ‰é™é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ä¸­ï¼Œè¿™ç›¸å½“äºå¯¹æ‰€æœ‰çŠ¶æ€çš„æ€»å’Œï¼Œä½†åœ¨å®é™…æ“ä½œä¸­æˆ‘ä»¬ä»æœªç²¾ç¡®åœ°è®¡ç®—å®ƒã€‚ç›¸åï¼Œæˆ‘ä»¬é€šè¿‡ä»å½“å‰ç­–ç•¥ä¸­é‡‡æ ·å›æ»šæ•°æ®æ¥ä¼°è®¡å®ƒã€‚åœ¨å¼ºåŒ–å­¦ä¹ ä¸äººç±»åé¦ˆï¼ˆRLHFï¼‰ä¸­ï¼Œè¿™é€šå¸¸æ„å‘³ç€ä»æ•°æ®é›†ä¸­é‡‡æ ·æç¤º
    <semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding="application/x-tex">x_i</annotation></semantics>
    å¹¶ç”Ÿæˆå®Œæˆ <semantics><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>âˆ¼</mo><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>â‹…</mi><mo>âˆ£</mo><msub><mi>x</mi><mi>i</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">y_i
    \sim \pi_\theta(\cdot\mid x_i)</annotation></semantics>ï¼Œç„¶åå–ä¸€ä¸ªç»éªŒå¹³å‡å€¼ï¼Œä¾‹å¦‚ï¼š
- en: <semantics><mrow><mover><mi>J</mi><mo accent="true">Ì‚</mo></mover><mo stretchy="false"
    form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>B</mi></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><mi>R</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo>,</mo></mrow> <annotation encoding="application/x-tex">\hat{J}(\theta)
    = \frac{1}{B}\sum_{i=1}^{B} R(x_i, y_i),</annotation></semantics>
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><mover><mi>J</mi><mo accent="true">Ì‚</mo></mover><mo stretchy="false"
    form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>B</mi></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><mi>R</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo>,</mo></mrow> <annotation encoding="application/x-tex">\hat{J}(\theta)
    = \frac{1}{B}\sum_{i=1}^{B} R(x_i, y_i),</annotation></semantics>
- en: or, in an MDP view with per-step rewards,
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…ï¼Œåœ¨å…·æœ‰æ¯æ­¥å¥–åŠ±çš„MDPè§†å›¾ä¸­ï¼Œ
- en: <semantics><mrow><mover><mi>J</mi><mo accent="true">Ì‚</mo></mover><mo stretchy="false"
    form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>B</mi></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><munderover><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><msub><mi>T</mi><mi>i</mi></msub></munderover><msup><mi>Î³</mi><mi>t</mi></msup><msub><mi>r</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mi>.</mi></mrow>
    <annotation encoding="application/x-tex">\hat{J}(\theta) = \frac{1}{B}\sum_{i=1}^{B}
    \sum_{t=0}^{T_i} \gamma^t r_{i,t}.</annotation></semantics>
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><mover><mi>J</mi><mo accent="true">Ì‚</mo></mover><mo stretchy="false"
    form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>B</mi></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><munderover><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><msub><mi>T</mi><mi>i</mi></msub></munderover><munderover><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><msub><mi>T</mi><mi>i</mi></msub></munderover><msup><mi>Î³</mi><mi>t</mi></msup><msub><mi>r</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mi>.</mi></mrow>
    <annotation encoding="application/x-tex">\hat{J}(\theta) = \frac{1}{B}\sum_{i=1}^{B}
    \sum_{t=0}^{T_i} \gamma^t r_{i,t}.</annotation></semantics>
- en: 'The core of policy gradient algorithms is computing the gradient with respect
    to the finite-time expected return over the current policy. With this expected
    return, <semantics><mi>J</mi><annotation encoding="application/x-tex">J</annotation></semantics>,
    the parameter update can be computed as follows, where <semantics><mi>Î±</mi><annotation
    encoding="application/x-tex">\alpha</annotation></semantics> is the learning rate:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ç­–ç•¥æ¢¯åº¦ç®—æ³•çš„æ ¸å¿ƒæ˜¯è®¡ç®—ç›¸å¯¹äºå½“å‰ç­–ç•¥çš„æœ‰é™æ—¶é—´æœŸæœ›å›æŠ¥çš„æ¢¯åº¦ã€‚æœ‰äº†è¿™ä¸ªæœŸæœ›å›æŠ¥ï¼Œ<semantics><mi>J</mi><annotation encoding="application/x-tex">J</annotation></semantics>ï¼Œå‚æ•°æ›´æ–°å¯ä»¥æŒ‰ä»¥ä¸‹æ–¹å¼è®¡ç®—ï¼Œå…¶ä¸­
    <semantics><mi>Î±</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics>
    æ˜¯å­¦ä¹ ç‡ï¼š
- en: <semantics><mrow><mi>Î¸</mi><mo>â†</mo><mi>Î¸</mi><mo>+</mo><mi>Î±</mi><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi>J</mi><mo
    stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>37</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)\qquad{(37)}</annotation></semantics>
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><mi>Î¸</mi><mo>â†</mo><mi>Î¸</mi><mo>+</mo><mi>Î±</mi><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi>J</mi><mo
    stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>37</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)\qquad{(37)}</annotation></semantics>
- en: The core implementation detail is how to compute said gradient.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¸å¿ƒçš„å®ç°ç»†èŠ‚æ˜¯å¦‚ä½•è®¡ç®—è¿™ä¸ªæ¢¯åº¦ã€‚
- en: 'Another way to pose the RL objective we want to maximize is as follows: <semantics><mrow><mi>J</mi><mo
    stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ğ”¼</mi><mrow><mi>Ï„</mi><mo>âˆ¼</mo><msub><mi>Ï€</mi><mi>Î¸</mi></msub></mrow></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow><mo>,</mo><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>38</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}
    \left[ R(\tau) \right], \qquad{(38)}</annotation></semantics>'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ç§è¡¨è¿°æˆ‘ä»¬æƒ³è¦æœ€å¤§åŒ–çš„å¼ºåŒ–å­¦ä¹ ç›®æ ‡çš„æ–¹æ³•å¦‚ä¸‹ï¼š<semantics><mrow><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ğ”¼</mi><mrow><mi>Ï„</mi><mo>âˆ¼</mo><msub><mi>Ï€</mi><mi>Î¸</mi></msub></mrow></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow><mo>,</mo><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>38</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}
    \left[ R(\tau) \right], \qquad{(38)}</annotation></semantics>
- en: 'where <semantics><mrow><mi>Ï„</mi><mo>=</mo><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo>,</mo><msub><mi>a</mi><mn>0</mn></msub><mo>,</mo><msub><mi>s</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><mi>â€¦</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\tau
    = (s_0, a_0, s_1, a_1, \ldots)</annotation></semantics> is a trajectory and <semantics><mrow><mi>R</mi><mo
    stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msubsup><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>âˆ</mi></msubsup><msub><mi>r</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">R(\tau) = \sum_{t=0}^\infty r_t</annotation></semantics>
    is the total reward of the trajectory. Alternatively, we can write the expectation
    as an integral over all possible trajectories: <semantics><mrow><mi>J</mi><mo
    stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mo>âˆ«</mo><mi>Ï„</mi></msub><msub><mi>p</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo stretchy="false" form="postfix">)</mo><mi>R</mi><mo
    stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo stretchy="false" form="postfix">)</mo><mi>d</mi><mi>Ï„</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>39</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">J(\theta) = \int_\tau p_\theta (\tau)
    R(\tau) d\tau \qquad{(39)}</annotation></semantics>'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ <semantics><mrow><mi>Ï„</mi><mo>=</mo><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo>,</mo><msub><mi>a</mi><mn>0</mn></msub><mo>,</mo><msub><mi>s</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><mi>â€¦</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\tau
    = (s_0, a_0, s_1, a_1, \ldots)</annotation></semantics> è¡¨ç¤ºä¸€æ¡è½¨è¿¹ï¼Œè€Œ <semantics><mrow><mi>R</mi><mo
    stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msubsup><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>âˆ</mi></msubsup><msub><mi>r</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">R(\tau) = \sum_{t=0}^\infty r_t</annotation></semantics>
    æ˜¯è¯¥è½¨è¿¹çš„æ€»å¥–åŠ±ã€‚æˆ–è€…ï¼Œæˆ‘ä»¬å¯ä»¥å°†æœŸæœ›å€¼è¡¨ç¤ºä¸ºå¯¹æ‰€æœ‰å¯èƒ½è½¨è¿¹çš„ç§¯åˆ†ï¼š<semantics><mrow><mi>J</mi><mo stretchy="false"
    form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mo>âˆ«</mo><mi>Ï„</mi></msub><msub><mi>p</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo stretchy="false" form="postfix">)</mo><mi>R</mi><mo
    stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo stretchy="false" form="postfix">)</mo><mi>d</mi><mi>Ï„</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>39</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">J(\theta) = \int_\tau p_\theta (\tau)
    R(\tau) d\tau \qquad{(39)}</annotation></semantics>
- en: 'Notice that we can express the trajectory probability as follows, where <semantics><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false"
    form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mi>p</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo
    stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\pi_\theta(a_t|s_t)
    p(s_{t+1}|s_t, a_t)</annotation></semantics> is the transition probability to
    a group of next states from one state and action: <semantics><mrow><msub><mi>p</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>p</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo stretchy="false"
    form="postfix">)</mo><munderover><mo>âˆ</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mo
    accent="false">âˆ</mo></munderover><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mi>p</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo
    stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo>,</mo><mrow><mo stretchy="false" form="prefix">(</mo><mn>40</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">p_\theta
    (\tau) = p(s_0) \prod_{t=0}^\infty \pi_\theta(a_t|s_t) p(s_{t+1}|s_t, a_t), \qquad{(40)}</annotation></semantics>'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œæˆ‘ä»¬å¯ä»¥å°†è½¨è¿¹æ¦‚ç‡è¡¨ç¤ºå¦‚ä¸‹ï¼Œå…¶ä¸­ <semantics><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mi>p</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo
    stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\pi_\theta(a_t|s_t)
    p(s_{t+1}|s_t, a_t)</annotation></semantics> æ˜¯ä»ä¸€ä¸ªçŠ¶æ€å’ŒåŠ¨ä½œåˆ°ä¸€ç»„ä¸‹ä¸€ä¸ªçŠ¶æ€çš„è½¬ç§»æ¦‚ç‡ï¼š<semantics><mrow><msub><mi>p</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>p</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo stretchy="false"
    form="postfix">)</mo><munderover><mo>âˆ</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mo
    accent="false">âˆ</mo></munderover><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mi>p</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo
    stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo>,</mo><mrow><mo stretchy="false" form="prefix">(</mo><mn>40</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">p_\theta
    (\tau) = p(s_0) \prod_{t=0}^\infty \pi_\theta(a_t|s_t) p(s_{t+1}|s_t, a_t), \qquad{(40)}</annotation></semantics>
- en: 'If we take the gradient of the objective (eq.Â [38](ch011.xhtml#eq:policy_objective_expectation))
    with respect to the policy parameters <semantics><mi>Î¸</mi><annotation encoding="application/x-tex">\theta</annotation></semantics>:
    <semantics><mrow><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi>J</mi><mo stretchy="false"
    form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mo>âˆ«</mo><mi>Ï„</mi></msub><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><msub><mi>p</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo stretchy="false" form="postfix">)</mo><mi>R</mi><mo
    stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo stretchy="false" form="postfix">)</mo><mi>d</mi><mi>Ï„</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>41</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">\nabla_\theta J(\theta) = \int_\tau \nabla_\theta
    p_\theta (\tau) R(\tau) d\tau \qquad{(41)}</annotation></semantics>'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬å¯¹ç›®æ ‡å‡½æ•°ï¼ˆæ–¹ç¨‹[38](ch011.xhtml#eq:policy_objective_expectation)ï¼‰ç›¸å¯¹äºç­–ç•¥å‚æ•° <semantics><mi>Î¸</mi><annotation
    encoding="application/x-tex">\theta</annotation></semantics> æ±‚æ¢¯åº¦ï¼š<semantics><mrow><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi>J</mi><mo
    stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mo>âˆ«</mo><mi>Ï„</mi></msub><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><msub><mi>p</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo stretchy="false" form="postfix">)</mo><mi>R</mi><mo
    stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo stretchy="false" form="postfix">)</mo><mi>d</mi><mi>Ï„</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>41</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">\nabla_\theta J(\theta) = \int_\tau \nabla_\theta
    p_\theta (\tau) R(\tau) d\tau \qquad{(41)}</annotation></semantics>
- en: 'Notice that we can use the [log-derivative trick](https://andrewcharlesjones.github.io/journal/log-derivative.html)
    in order to rewrite the gradient of the integral as an expectation: <semantics><mrow><mtable><mtr><mtd
    columnalign="right" style="text-align: right"><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mrow><mi
    mathvariant="normal">log</mi><mo>â¡</mo></mrow><msub><mi>p</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo stretchy="false" form="postfix">)</mo></mtd><mtd
    columnalign="left" style="text-align: left"><mo>=</mo><mfrac><mrow><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><msub><mi>p</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>p</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac></mtd><mtd
    columnalign="right" style="text-align: right"><mtext mathvariant="normal">(from
    chain rule)</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align:
    right"><mo>âŸ¹</mo><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><msub><mi>p</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo stretchy="false" form="postfix">)</mo></mtd><mtd
    columnalign="left" style="text-align: left"><mo>=</mo><msub><mi>p</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mrow><mi
    mathvariant="normal">log</mi><mo>â¡</mo></mrow><msub><mi>p</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo stretchy="false" form="postfix">)</mo></mtd><mtd
    columnalign="right" style="text-align: right"><mtext mathvariant="normal">(rearranging)</mtext></mtd></mtr></mtable><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>42</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">\begin{aligned} \nabla_\theta \log p_\theta(\tau)
    &= \frac{\nabla_\theta p_\theta(\tau)}{p_\theta(\tau)} &\text{(from chain rule)}
    \\ \implies \nabla_\theta p_\theta(\tau) &= p_\theta(\tau) \nabla_\theta \log
    p_\theta(\tau) &\text{(rearranging)} \end{aligned} \qquad{(42)}</annotation></semantics>'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'Using this log-derivative trick: <semantics><mrow><mtable><mtr><mtd columnalign="right"
    style="text-align: right; padding-right: 0"><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi>J</mi><mo
    stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo></mtd><mtd
    columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><msub><mo>âˆ«</mo><mi>Ï„</mi></msub><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><msub><mi>p</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo stretchy="false" form="postfix">)</mo><mi>R</mi><mo
    stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo stretchy="false" form="postfix">)</mo><mi>d</mi><mi>Ï„</mi></mtd></mtr><mtr><mtd
    columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><msub><mo>âˆ«</mo><mi>Ï„</mi></msub><msub><mi>p</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mrow><mi
    mathvariant="normal">log</mi><mo>â¡</mo></mrow><msub><mi>p</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo stretchy="false" form="postfix">)</mo><mi>R</mi><mo
    stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo stretchy="false" form="postfix">)</mo><mi>d</mi><mi>Ï„</mi></mtd></mtr><mtr><mtd
    columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><msub><mi>ğ”¼</mi><mrow><mi>Ï„</mi><mo>âˆ¼</mo><msub><mi>Ï€</mi><mi>Î¸</mi></msub></mrow></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi mathvariant="normal">log</mi><msub><mi>p</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo stretchy="false" form="postfix">)</mo><mi>R</mi><mo
    stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo stretchy="false" form="postfix">)</mo><mo
    stretchy="true" form="postfix">]</mo></mrow></mtd></mtr></mtable><mrow><mo stretchy="false"
    form="prefix">(</mo><mn>43</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">\begin{aligned} \nabla_\theta J(\theta)
    &= \int_\tau \nabla_\theta p_\theta (\tau) R(\tau) d\tau \\ &= \int_\tau p_\theta
    (\tau) \nabla_\theta \log p_\theta (\tau) R(\tau) d\tau \\ &= \mathbb{E}_{\tau
    \sim \pi_\theta} \left[ \nabla_\theta \log p_\theta (\tau) R(\tau) \right] \end{aligned}
    \qquad{(43)}</annotation></semantics>'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'Where the final step uses the definition of an expectation under the trajectory
    distribution <semantics><mrow><msub><mi>p</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><mi>Ï„</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">p_\theta(\tau)</annotation></semantics>: for any
    function <semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics>,
    <semantics><mrow><msub><mi>ğ”¼</mi><mrow><mi>Ï„</mi><mo>âˆ¼</mo><msub><mi>p</mi><mi>Î¸</mi></msub></mrow></msub><mo
    stretchy="false" form="prefix">[</mo><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">]</mo><mo>=</mo><msub><mo>âˆ«</mo><mi>Ï„</mi></msub><mi>f</mi><mo
    stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>p</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo stretchy="false" form="postfix">)</mo><mi>d</mi><mi>Ï„</mi></mrow><annotation
    encoding="application/x-tex">\mathbb{E}_{\tau \sim p_\theta}[f(\tau)] = \int_\tau
    f(\tau)\,p_\theta(\tau)\,d\tau</annotation></semantics> (or a sum in the discrete
    case). Writing it as an expectation is useful because we can approximate it with
    Monte Carlo rollouts, e.g., <semantics><mrow><mfrac><mn>1</mn><mi>B</mi></mfrac><msubsup><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></msubsup><mi>f</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>Ï„</mi><mi>i</mi></msub><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\frac{1}{B}\sum_{i=1}^{B}
    f(\tau_i)</annotation></semantics> for trajectories <semantics><mrow><msub><mi>Ï„</mi><mi>i</mi></msub><mo>âˆ¼</mo><msub><mi>Ï€</mi><mi>Î¸</mi></msub></mrow><annotation
    encoding="application/x-tex">\tau_i \sim \pi_\theta</annotation></semantics>.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­æœ€åä¸€æ­¥ä½¿ç”¨äº†è½¨è¿¹åˆ†å¸ƒä¸‹æœŸæœ›çš„å®šä¹‰ <semantics><mrow><msub><mi>p</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><mi>Ï„</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">p_\theta(\tau)</annotation></semantics>ï¼šå¯¹äºä»»ä½•å‡½æ•° <semantics><mi>f</mi><annotation
    encoding="application/x-tex">f</annotation></semantics>ï¼Œ<semantics><mrow><msub><mi>ğ”¼</mi><mrow><mi>Ï„</mi><mo>âˆ¼</mo><msub><mi>p</mi><mi>Î¸</mi></msub></mrow></msub><mo
    stretchy="false" form="prefix">[</mo><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mo>âˆ«</mo><mi>Ï„</mi></msub><mi>f</mi><mo
    stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>p</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo stretchy="false" form="postfix">)</mo><mi>d</mi><mi>Ï„</mi></mrow><annotation
    encoding="application/x-tex">\mathbb{E}_{\tau \sim p_\theta}[f(\tau)] = \int_\tau
    f(\tau)\,p_\theta(\tau)\,d\tau</annotation></semantics>ï¼ˆæˆ–åœ¨ç¦»æ•£æƒ…å†µä¸‹çš„å’Œï¼‰ã€‚å°†å…¶å†™ä½œæœŸæœ›æ˜¯æœ‰ç”¨çš„ï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥ç”¨è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿæ¥è¿‘ä¼¼å®ƒï¼Œä¾‹å¦‚
    <semantics><mrow><mfrac><mn>1</mn><mi>B</mi></mfrac><msubsup><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></msubsup><mi>f</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>Ï„</mi><mi>i</mi></msub><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\frac{1}{B}\sum_{i=1}^{B}
    f(\tau_i)</annotation></semantics> å¯¹äºè½¨è¿¹ <semantics><mrow><msub><mi>Ï„</mi><mi>i</mi></msub><mo>âˆ¼</mo><msub><mi>Ï€</mi><mi>Î¸</mi></msub></mrow><annotation
    encoding="application/x-tex">\tau_i \sim \pi_\theta</annotation></semantics>ã€‚
- en: 'Back to the derivation, expanding the log probability of the trajectory:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å›åˆ°æ¨å¯¼è¿‡ç¨‹ï¼Œå±•å¼€è½¨è¿¹çš„å¯¹æ•°æ¦‚ç‡ï¼š
- en: <semantics><mrow><mrow><mi mathvariant="normal">log</mi><mo>â¡</mo></mrow><msub><mi>p</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mrow><mi
    mathvariant="normal">log</mi><mo>â¡</mo></mrow><mi>p</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo
    stretchy="false" form="postfix">)</mo><mo>+</mo><munderover><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mo
    accent="false">âˆ</mo></munderover><mrow><mi mathvariant="normal">log</mi><mo>â¡</mo></mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false"
    form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>+</mo><munderover><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mo
    accent="false">âˆ</mo></munderover><mrow><mi mathvariant="normal">log</mi><mo>â¡</mo></mrow><mi>p</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo
    stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow> <annotation encoding="application/x-tex">\log
    p_\theta (\tau) = \log p(s_0) + \sum_{t=0}^\infty \log \pi_\theta(a_t|s_t) + \sum_{t=0}^\infty
    \log p(s_{t+1}|s_t, a_t)</annotation></semantics>
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><mrow><mi mathvariant="normal">log</mi><mo>â¡</mo></mrow><msub><mi>p</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mrow><mi
    mathvariant="normal">log</mi><mo>â¡</mo></mrow><mi>p</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo
    stretchy="false" form="postfix">)</mo><mo>+</mo><munderover><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mo
    accent="false">âˆ</mo></munderover><mrow><mi mathvariant="normal">log</mi><mo>â¡</mo></mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false"
    form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>+</mo><munderover><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mo
    accent="false">âˆ</mo></munderover><mrow><mi mathvariant="normal">log</mi><mo>â¡</mo></mrow><mi>p</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo
    stretchy="false" form="postfix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow> <annotation encoding="application/x-tex">\log
    p_\theta (\tau) = \log p(s_0) + \sum_{t=0}^\infty \log \pi_\theta(a_t|s_t) + \sum_{t=0}^\infty
    \log p(s_{t+1}|s_t, a_t)</annotation></semantics>
- en: 'Now, if we take the gradient of the above, we get:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œå¦‚æœæˆ‘ä»¬å¯¹ä¸Šè¿°å…¬å¼æ±‚æ¢¯åº¦ï¼Œæˆ‘ä»¬å¾—åˆ°ï¼š
- en: <semantics><mrow><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mrow><mi mathvariant="normal">log</mi><mo>â¡</mo></mrow><mi>p</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo stretchy="false"
    form="postfix">)</mo><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\nabla_\theta
    \log p(s_0) = 0</annotation></semantics> (initial state doesnâ€™t depend on <semantics><mi>Î¸</mi><annotation
    encoding="application/x-tex">\theta</annotation></semantics>)
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <semantics><mrow><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mrow><mi mathvariant="normal">log</mi><mo>â¡</mo></mrow><mi>p</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo stretchy="false"
    form="postfix">)</mo><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\nabla_\theta
    \log p(s_0) = 0</annotation></semantics> (åˆå§‹çŠ¶æ€ä¸ä¾èµ–äº <semantics><mi>Î¸</mi><annotation
    encoding="application/x-tex">\theta</annotation></semantics>)
- en: <semantics><mrow><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mrow><mi mathvariant="normal">log</mi><mo>â¡</mo></mrow><mi>p</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo
    stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\nabla_\theta
    \log p(s_{t+1}|s_t, a_t) = 0</annotation></semantics> (environment transition
    dynamics donâ€™t depend on <semantics><mi>Î¸</mi><annotation encoding="application/x-tex">\theta</annotation></semantics>)
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <semantics><mrow><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mrow><mi mathvariant="normal">log</mi><mo>â¡</mo></mrow><mi>p</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo
    stretchy="false" form="postfix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\nabla_\theta
    \log p(s_{t+1}|s_t, a_t) = 0</annotation></semantics> (ç¯å¢ƒè½¬æ¢åŠ¨æ€ä¸ä¾èµ–äº <semantics><mi>Î¸</mi><annotation
    encoding="application/x-tex">\theta</annotation></semantics>)
- en: only <semantics><mrow><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mrow><mi mathvariant="normal">log</mi><mo>â¡</mo></mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false"
    form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">\nabla_\theta \log \pi_\theta(a_t|s_t)</annotation></semantics>
    survives
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åªæœ‰ <semantics><mrow><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mrow><mi mathvariant="normal">log</mi><mo>â¡</mo></mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false"
    form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">\nabla_\theta \log \pi_\theta(a_t|s_t)</annotation></semantics>
    ä¿ç•™
- en: 'Therefore, the gradient of the log probability of the trajectory simplifies
    to: <semantics><mrow><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mrow><mi mathvariant="normal">log</mi><mo>â¡</mo></mrow><msub><mi>p</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><munderover><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mo
    accent="false">âˆ</mo></munderover><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mrow><mi mathvariant="normal">log</mi><mo>â¡</mo></mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false"
    form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow>
    <annotation encoding="application/x-tex">\nabla_\theta \log p_\theta (\tau) =
    \sum_{t=0}^\infty \nabla_\theta \log \pi_\theta(a_t|s_t)</annotation></semantics>'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œè½¨è¿¹å¯¹æ•°æ¦‚ç‡çš„æ¢¯åº¦ç®€åŒ–ä¸ºï¼š<semantics><mrow><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mrow><mi
    mathvariant="normal">log</mi><mo>â¡</mo></mrow><msub><mi>p</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><munderover><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mo
    accent="false">âˆ</mo></munderover><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mrow><mi mathvariant="normal">log</mi><mo>â¡</mo></mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false"
    form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow>
    <annotation encoding="application/x-tex">\nabla_\theta \log p_\theta (\tau) =
    \sum_{t=0}^\infty \nabla_\theta \log \pi_\theta(a_t|s_t)</annotation></semantics>
- en: 'Substituting this back in eq.Â [43](ch011.xhtml#eq:policy_gradient_expectation),
    we get: <semantics><mrow><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi>J</mi><mo stretchy="false"
    form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ğ”¼</mi><mrow><mi>Ï„</mi><mo>âˆ¼</mo><msub><mi>Ï€</mi><mi>Î¸</mi></msub></mrow></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><munderover><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mo
    accent="false">âˆ</mo></munderover><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi mathvariant="normal">log</mi><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false"
    form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mi>R</mi><mo
    stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo stretchy="false" form="postfix">)</mo><mo
    stretchy="true" form="postfix">]</mo></mrow></mrow> <annotation encoding="application/x-tex">\nabla_\theta
    J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^\infty \nabla_\theta
    \log \pi_\theta(a_t|s_t) R(\tau) \right]</annotation></semantics>'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ­¤ä»£å›æ–¹ç¨‹ [43](ch011.xhtml#eq:policy_gradient_expectation)ï¼Œæˆ‘ä»¬å¾—åˆ°ï¼š<semantics><mrow><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi>J</mi><mo
    stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ğ”¼</mi><mrow><mi>Ï„</mi><mo>âˆ¼</mo><msub><mi>Ï€</mi><mi>Î¸</mi></msub></mrow></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><munderover><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mo
    accent="false">âˆ</mo></munderover><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi mathvariant="normal">log</mi><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false"
    form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mi>R</mi><mo
    stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo stretchy="false" form="postfix">)</mo><mo
    stretchy="true" form="postfix">]</mo></mrow></mrow> <annotation encoding="application/x-tex">\nabla_\theta
    J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^\infty \nabla_\theta
    \log \pi_\theta(a_t|s_t) R(\tau) \right]</annotation></semantics>
- en: 'Quite often, people use a more general formulation of the policy gradient:
    <semantics><mrow><mi>g</mi><mo>=</mo><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi>J</mi><mo
    stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ğ”¼</mi><mrow><mi>Ï„</mi><mo>âˆ¼</mo><msub><mi>Ï€</mi><mi>Î¸</mi></msub></mrow></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><munderover><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mo
    accent="false">âˆ</mo></munderover><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi mathvariant="normal">log</mi><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false"
    form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><msub><mi
    mathvariant="normal">Î¨</mi><mi>t</mi></msub><mo stretchy="true" form="postfix">]</mo></mrow><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>44</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">g = \nabla_\theta J(\theta) = \mathbb{E}_{\tau
    \sim \pi_\theta} \left[ \sum_{t=0}^\infty \nabla_\theta \log \pi_\theta(a_t|s_t)
    \Psi_t \right] \qquad{(44)}</annotation></semantics>'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆå¸¸è§ï¼Œäººä»¬ä½¿ç”¨ç­–ç•¥æ¢¯åº¦çš„æ›´ä¸€èˆ¬å½¢å¼ï¼š<semantics><mrow><mi>g</mi><mo>=</mo><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi>J</mi><mo
    stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ğ”¼</mi><mrow><mi>Ï„</mi><mo>âˆ¼</mo><msub><mi>Ï€</mi><mi>Î¸</mi></msub></mrow></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><munderover><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mo
    accent="false">âˆ</mo></munderover><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi mathvariant="normal">log</mi><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false"
    form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><msub><mi
    mathvariant="normal">Î¨</mi><mi>t</mi></msub><mo stretchy="true" form="postfix">]</mo></mrow><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>44</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">g = \nabla_\theta J(\theta) = \mathbb{E}_{\tau
    \sim \pi_\theta} \left[ \sum_{t=0}^\infty \nabla_\theta \log \pi_\theta(a_t|s_t)
    \Psi_t \right] \qquad{(44)}</annotation></semantics>
- en: 'Where <semantics><msub><mi mathvariant="normal">Î¨</mi><mi>t</mi></msub><annotation
    encoding="application/x-tex">\Psi_t</annotation></semantics> can be the following
    (where the rewards can also often be discounted by <semantics><mi>Î³</mi><annotation
    encoding="application/x-tex">\gamma</annotation></semantics>), a taxonomy adopted
    from Schulman et al.Â 2015 [[179]](ch021.xhtml#ref-schulman2015high):'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 'å…¶ä¸­ <semantics><msub><mi mathvariant="normal">Î¨</mi><mi>t</mi></msub><annotation
    encoding="application/x-tex">\Psi_t</annotation></semantics> å¯ä»¥æ˜¯ä»¥ä¸‹å½¢å¼ï¼ˆå…¶ä¸­å¥–åŠ±ä¹Ÿå¯ä»¥ç»å¸¸é€šè¿‡
    <semantics><mi>Î³</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics>
    è¿›è¡ŒæŠ˜ç°ï¼‰ï¼Œè¿™æ˜¯ Schulman ç­‰äººäº 2015 å¹´é‡‡ç”¨çš„ä¸€ä¸ªåˆ†ç±»æ³• [[179]](ch021.xhtml#ref-schulman2015high):'
- en: '<semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><msubsup><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>âˆ</mi></msubsup><msub><mi>r</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">R(\tau) = \sum_{t=0}^{\infty} r_t</annotation></semantics>:
    total reward of the trajectory.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><msubsup><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>âˆ</mi></msubsup><msub><mi>r</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">R(\tau) = \sum_{t=0}^{\infty} r_t</annotation></semantics>ï¼šè½¨è¿¹çš„æ€»å¥–åŠ±ã€‚
- en: '<semantics><mrow><msubsup><mo>âˆ‘</mo><mrow><msup><mi>t</mi><mo>â€²</mo></msup><mo>=</mo><mi>t</mi></mrow><mi>âˆ</mi></msubsup><msub><mi>r</mi><msup><mi>t</mi><mo>â€²</mo></msup></msub></mrow><annotation
    encoding="application/x-tex">\sum_{t''=t}^{\infty} r_{t''}</annotation></semantics>:
    reward following action <semantics><msub><mi>a</mi><mi>t</mi></msub><annotation
    encoding="application/x-tex">a_t</annotation></semantics>, also described as the
    return, <semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics>.'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <semantics><mrow><msubsup><mo>âˆ‘</mo><mrow><msup><mi>t</mi><mo>â€²</mo></msup><mo>=</mo><mi>t</mi></mrow><mi>âˆ</mi></msubsup><msub><mi>r</mi><msup><mi>t</mi><mo>â€²</mo></msup></msub></mrow><annotation
    encoding="application/x-tex">\sum_{t'=t}^{\infty} r_{t'}</annotation></semantics>ï¼šè·ŸéšåŠ¨ä½œ
    <semantics><msub><mi>a</mi><mi>t</mi></msub><annotation encoding="application/x-tex">a_t</annotation></semantics>
    çš„å¥–åŠ±ï¼Œä¹Ÿç§°ä¸ºå›æŠ¥ï¼Œ<semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics>ã€‚
- en: '<semantics><mrow><msubsup><mo>âˆ‘</mo><mrow><msup><mi>t</mi><mo>â€²</mo></msup><mo>=</mo><mi>t</mi></mrow><mi>âˆ</mi></msubsup><msub><mi>r</mi><msup><mi>t</mi><mo>â€²</mo></msup></msub><mo>âˆ’</mo><mi>b</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\sum_{t''=t}^{\infty}
    r_{t''} - b(s_t)</annotation></semantics>: baselined version of previous formula.'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '<semantics><mrow><msubsup><mo>âˆ‘</mo><mrow><msup><mi>t</mi><mo>â€²</mo></msup><mo>=</mo><mi>t</mi></mrow><mi>âˆ</mi></msubsup><msub><mi>r</mi><msup><mi>t</mi><mo>â€²</mo></msup></msub><mo>âˆ’</mo><mi>b</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\sum_{t''=t}^{\infty}
    r_{t''} - b(s_t)</annotation></semantics>: åŸºå‡†åŒ–åçš„å‰ä¸€ä¸ªå…¬å¼ã€‚'
- en: '<semantics><mrow><msup><mi>Q</mi><mi>Ï€</mi></msup><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">Q^{\pi}(s_t,
    a_t)</annotation></semantics>: state-action value function.'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '<semantics><mrow><msup><mi>Q</mi><mi>Ï€</mi></msup><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">Q^{\pi}(s_t,
    a_t)</annotation></semantics>: çŠ¶æ€-åŠ¨ä½œå€¼å‡½æ•°ã€‚'
- en: '<semantics><mrow><msup><mi>A</mi><mi>Ï€</mi></msup><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">A^{\pi}(s_t,
    a_t)</annotation></semantics>: advantage function, which yields the lowest possible
    theoretical variance if it can be computed accurately.'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '<semantics><mrow><msup><mi>A</mi><mi>Ï€</mi></msup><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">A^{\pi}(s_t,
    a_t)</annotation></semantics>: ä¼˜åŠ¿å‡½æ•°ï¼Œå¦‚æœèƒ½å¤Ÿå‡†ç¡®è®¡ç®—ï¼Œå°†äº§ç”Ÿæœ€ä½å¯èƒ½çš„ç†è®ºæ–¹å·®ã€‚'
- en: '<semantics><mrow><msub><mi>r</mi><mi>t</mi></msub><mo>+</mo><msup><mi>V</mi><mi>Ï€</mi></msup><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo
    stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><msup><mi>V</mi><mi>Ï€</mi></msup><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">r_t + V^{\pi}(s_{t+1})
    - V^{\pi}(s_t)</annotation></semantics>: Temporal Difference (TD) residual.'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '<semantics><mrow><msub><mi>r</mi><mi>t</mi></msub><mo>+</mo><msup><mi>V</mi><mi>Ï€</mi></msup><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo
    stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><msup><mi>V</mi><mi>Ï€</mi></msup><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">r_t + V^{\pi}(s_{t+1})
    - V^{\pi}(s_t)</annotation></semantics>: æ—¶é—´å·®åˆ†ï¼ˆTDï¼‰æ®‹å·®ã€‚'
- en: The *baseline* is a value used to reduce variance of policy updates (more on
    this below).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*åŸºå‡†çº¿* æ˜¯ä¸€ä¸ªç”¨äºå‡å°‘ç­–ç•¥æ›´æ–°æ–¹å·®çš„å€¼ï¼ˆå…³äºè¿™ä¸€ç‚¹ä¸‹é¢ä¼šè¯¦ç»†è¯´æ˜ï¼‰ã€‚'
- en: 'For language models, some of these concepts do not make as much sense. For
    example, for a deterministic policy <semantics><mi>Ï€</mi><annotation encoding="application/x-tex">\pi</annotation></semantics>
    the state value is <semantics><mrow><msup><mi>V</mi><mi>Ï€</mi></msup><mo stretchy="false"
    form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msup><mi>Q</mi><mi>Ï€</mi></msup><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>Ï€</mi><mo stretchy="false"
    form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">V^{\pi}(s)
    = Q^{\pi}(s, \pi(s))</annotation></semantics> (and for the optimal value function
    one has <semantics><mrow><msup><mi>V</mi><mo>*</mo></msup><mo stretchy="false"
    form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mrow><mi
    mathvariant="normal">max</mi><mo>â¡</mo></mrow><mi>a</mi></msub><msup><mi>Q</mi><mo>*</mo></msup><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">V^*(s)=\max_a
    Q^*(s,a)</annotation></semantics>). For a stochastic policy, the analogous identity
    is <semantics><mrow><msup><mi>V</mi><mi>Ï€</mi></msup><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ğ”¼</mi><mrow><mi>a</mi><mo>âˆ¼</mo><mi>Ï€</mi><mo
    stretchy="false" form="prefix">(</mo><mi>â‹…</mi><mo>âˆ£</mo><mi>s</mi><mo stretchy="false"
    form="postfix">)</mo></mrow></msub><mo stretchy="false" form="prefix">[</mo><msup><mi>Q</mi><mi>Ï€</mi></msup><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false"
    form="postfix">)</mo><mo stretchy="false" form="postfix">]</mo></mrow><annotation
    encoding="application/x-tex">V^{\pi}(s) = \mathbb{E}_{a \sim \pi(\cdot\mid s)}[Q^{\pi}(s,a)]</annotation></semantics>.
    If we define <semantics><mrow><mi>s</mi><mo>+</mo><mi>a</mi></mrow><annotation
    encoding="application/x-tex">s+a</annotation></semantics> as the continuation
    <semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics>
    to the prompt <semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics>,
    then <semantics><mrow><mi>Q</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mi>V</mi><mo stretchy="false"
    form="prefix">(</mo><mi>s</mi><mo>+</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">Q(s, a) = V(s+a)</annotation></semantics>, which
    gives a different advantage trick:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¯­è¨€æ¨¡å‹æ¥è¯´ï¼Œè¿™äº›æ¦‚å¿µä¸­çš„ä¸€äº›å¹¶ä¸é‚£ä¹ˆæœ‰æ„ä¹‰ã€‚ä¾‹å¦‚ï¼Œå¯¹äºä¸€ä¸ªç¡®å®šæ€§çš„ç­–ç•¥ <semantics><mi>Ï€</mi><annotation encoding="application/x-tex">\pi</annotation></semantics>ï¼ŒçŠ¶æ€å€¼æ˜¯
    <semantics><mrow><msup><mi>V</mi><mi>Ï€</mi></msup><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><msup><mi>Q</mi><mi>Ï€</mi></msup><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>Ï€</mi><mo stretchy="false"
    form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">V^{\pi}(s)
    = Q^{\pi}(s, \pi(s))</annotation></semantics>ï¼ˆè€Œå¯¹äºæœ€ä¼˜å€¼å‡½æ•°ï¼Œæœ‰ <semantics><mrow><msup><mi>V</mi><mo>*</mo></msup><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mrow><mi
    mathvariant="normal">max</mi><mo>â¡</mo></mrow><mi>a</mi></msub><msup><mi>Q</mi><mo>*</mo></msup><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">V^*(s)=\max_a
    Q^*(s,a)</annotation></semantics>ï¼‰ã€‚å¯¹äºéšæœºç­–ç•¥ï¼Œç±»ä¼¼çš„ç­‰å¼æ˜¯ <semantics><mrow><msup><mi>V</mi><mi>Ï€</mi></msup><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ğ”¼</mi><mrow><mi>a</mi><mo>âˆ¼</mo><mi>Ï€</mi><mo
    stretchy="false" form="prefix">(</mo><mi>â‹…</mi><mo>âˆ£</mo><mi>s</mi><mo stretchy="false"
    form="postfix">)</mo></mrow></msub><mo stretchy="false" form="prefix">[</mo><msup><mi>Q</mi><mi>Ï€</mi></msup><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false"
    form="postfix">)</mo><mo stretchy="false" form="postfix">]</mo></mrow><annotation
    encoding="application/x-tex">V^{\pi}(s) = \mathbb{E}_{a \sim \pi(\cdot\mid s)}[Q^{\pi}(s,a)]</annotation></semantics>ã€‚å¦‚æœæˆ‘ä»¬å®šä¹‰
    <semantics><mrow><mi>s</mi><mo>+</mo><mi>a</mi></mrow><annotation encoding="application/x-tex">s+a</annotation></semantics>
    ä¸ºå¯¹æç¤º <semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics>
    çš„å»¶ç»­ <semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics>ï¼Œé‚£ä¹ˆ
    <semantics><mrow><mi>Q</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mi>V</mi><mo stretchy="false"
    form="prefix">(</mo><mi>s</mi><mo>+</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">Q(s, a) = V(s+a)</annotation></semantics>ï¼Œè¿™ç»™å‡ºäº†ä¸€ç§ä¸åŒçš„ä¼˜åŠ¿æŠ€å·§ï¼š
- en: <semantics><mrow><mi>A</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mi>Q</mi><mo stretchy="false"
    form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><mi>V</mi><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>V</mi><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo>+</mo><mi>a</mi><mo stretchy="false"
    form="postfix">)</mo><mo>âˆ’</mo><mi>V</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mi>r</mi><mo>+</mo><mi>Î³</mi><mi>V</mi><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo>+</mo><mi>a</mi><mo stretchy="false"
    form="postfix">)</mo><mo>âˆ’</mo><mi>V</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo
    stretchy="false" form="postfix">)</mo><mrow><mo stretchy="false" form="prefix">(</mo><mn>45</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">A(s,a)
    = Q(s,a) - V(s) = V(s + a) - V(s) = r + \gamma V(s + a) - V(s)\qquad{(45)}</annotation></semantics>
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><mi>A</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mi>Q</mi><mo stretchy="false"
    form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><mi>V</mi><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>V</mi><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo>+</mo><mi>a</mi><mo stretchy="false"
    form="postfix">)</mo><mo>âˆ’</mo><mi>V</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mi>r</mi><mo>+</mo><mi>Î³</mi><mi>V</mi><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo>+</mo><mi>a</mi><mo stretchy="false"
    form="postfix">)</mo><mo>âˆ’</mo><mi>V</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo
    stretchy="false" form="postfix">)</mo><mrow><mo stretchy="false" form="prefix">(</mo><mn>45</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">A(s,a)
    = Q(s,a) - V(s) = V(s + a) - V(s) = r + \gamma V(s + a) - V(s)\qquad{(45)}</annotation></semantics>
- en: Which is a combination of the reward, the value of the prompt, and the discounted
    value of the entire utterance.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ç§å¥–åŠ±ã€æç¤ºçš„ä»·å€¼ä»¥åŠæ•´ä¸ªè¯è¯­çš„æŠ˜ç°ä»·å€¼çš„ç»„åˆã€‚
- en: Vanilla Policy Gradient
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é¦™è‰ç­–ç•¥æ¢¯åº¦
- en: 'The vanilla policy gradient implementation optimizes the above expression for
    <semantics><mrow><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">J(\theta)</annotation></semantics>
    by differentiating with respect to the policy parameters. A simple version, with
    respect to the overall return, is:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: é¦™è‰ç­–ç•¥æ¢¯åº¦çš„å®ç°é€šè¿‡ç›¸å¯¹äºç­–ç•¥å‚æ•°çš„å¾®åˆ†æ¥ä¼˜åŒ–ä¸Šè¿°è¡¨è¾¾å¼ <semantics><mrow><mi>J</mi><mo stretchy="false"
    form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">J(\theta)</annotation></semantics>ã€‚ä¸€ä¸ªç®€å•çš„ç‰ˆæœ¬ï¼Œç›¸å¯¹äºæ•´ä½“å›æŠ¥ï¼Œæ˜¯ï¼š
- en: <semantics><mrow><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi>J</mi><mo stretchy="false"
    form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ğ”¼</mi><mi>Ï„</mi></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><munderover><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>T</mi></munderover><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi
    mathvariant="normal">log</mi><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo><msub><mi>R</mi><mi>t</mi></msub><mo stretchy="true"
    form="postfix">]</mo></mrow><mrow><mo stretchy="false" form="prefix">(</mo><mn>46</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\nabla_\theta
    J(\theta) = \mathbb{E}_\tau \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t)
    R_t \right]\qquad{(46)}</annotation></semantics>
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi>J</mi><mo stretchy="false"
    form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ğ”¼</mi><mi>Ï„</mi></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><munderover><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>T</mi></munderover><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi
    mathvariant="normal">log</mi><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo><msub><mi>R</mi><mi>t</mi></msub><mo stretchy="true"
    form="postfix">]</mo></mrow><mrow><mo stretchy="false" form="prefix">(</mo><mn>46</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\nabla_\theta
    J(\theta) = \mathbb{E}_\tau \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t)
    R_t \right]\qquad{(46)}</annotation></semantics>
- en: A common problem with vanilla policy gradient algorithms is the high variance
    in gradient updates, which can be mitigated in multiple ways. The high variance
    comes from the gradient updates being computed from estimating the return <semantics><mi>G</mi><annotation
    encoding="application/x-tex">G</annotation></semantics> from an often small set
    of rollouts in the environment that tend to be susceptible to noise (e.g.Â the
    stochastic nature of generating from language models with temperature <semantics><mrow><mo>></mo><mn>0</mn></mrow><annotation
    encoding="application/x-tex">>0</annotation></semantics>). The variance across
    return estimates is higher in domains with sparse rewards, as more of the samples
    are 0 or 1, rather than closely clustered. In order to alleviate this, various
    techniques are used to normalize the value estimation, called *baselines*. Baselines
    accomplish this in multiple ways, effectively normalizing by the value of the
    state relative to the downstream action (e.g.Â in the case of Advantage, which
    is the difference between the Q value and the value). The simplest baselines are
    averages over the batch of rewards or a moving average. Even these baselines can
    de-bias the gradients so <semantics><mrow><msub><mi>ğ”¼</mi><mrow><mi>a</mi><mo>âˆ¼</mo><mi>Ï€</mi><mo
    stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="prefix">|</mo><mi>s</mi><mo
    stretchy="false" form="postfix">)</mo></mrow></msub><mo stretchy="false" form="prefix">[</mo><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mrow><mi
    mathvariant="normal">log</mi><mo>â¡</mo></mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="prefix">|</mo><mi>s</mi><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">]</mo><mo>=</mo><mn>0</mn></mrow><annotation
    encoding="application/x-tex">\mathbb{E}_{a \sim \pi(a|s)}[\nabla_\theta \log \pi_\theta(a|s)]
    = 0</annotation></semantics>, improving the learning signal substantially.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: èŒƒä¾‹ç­–ç•¥æ¢¯åº¦ç®—æ³•çš„ä¸€ä¸ªå¸¸è§é—®é¢˜æ˜¯æ¢¯åº¦æ›´æ–°çš„é«˜æ–¹å·®ï¼Œè¿™å¯ä»¥é€šè¿‡å¤šç§æ–¹å¼æ¥ç¼“è§£ã€‚é«˜æ–¹å·®æ¥æºäºä»ç¯å¢ƒä¸­é€šå¸¸è¾ƒå°çš„ä¸€ç»„å›æ”¾ä¸­ä¼°è®¡å›æŠ¥ <semantics><mi>G</mi><annotation
    encoding="application/x-tex">G</annotation></semantics>ï¼Œè¿™äº›å›æ”¾å¾€å¾€å®¹æ˜“å—åˆ°å™ªå£°çš„å½±å“ï¼ˆä¾‹å¦‚ï¼Œä»å…·æœ‰æ¸©åº¦
    <semantics><mrow><mo>></mo><mn>0</mn></mrow><annotation encoding="application/x-tex">>0</annotation></semantics>
    çš„è¯­è¨€æ¨¡å‹ç”Ÿæˆæ—¶çš„éšæœºæ€§ï¼‰ã€‚åœ¨å¥–åŠ±ç¨€ç–çš„é¢†åŸŸä¸­ï¼Œå›æŠ¥ä¼°è®¡çš„æ–¹å·®æ›´é«˜ï¼Œå› ä¸ºæ›´å¤šçš„æ ·æœ¬æ˜¯ 0 æˆ– 1ï¼Œè€Œä¸æ˜¯ç´§å¯†èšé›†ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€ç‚¹ï¼Œä½¿ç”¨äº†å„ç§æŠ€æœ¯æ¥å½’ä¸€åŒ–ä»·å€¼ä¼°è®¡ï¼Œç§°ä¸º
    *åŸºçº¿*ã€‚åŸºçº¿é€šè¿‡å¤šç§æ–¹å¼å®ç°è¿™ä¸€ç‚¹ï¼Œæœ‰æ•ˆåœ°é€šè¿‡çŠ¶æ€ç›¸å¯¹äºä¸‹æ¸¸åŠ¨ä½œçš„ä»·å€¼æ¥å½’ä¸€åŒ–ï¼ˆä¾‹å¦‚ï¼Œåœ¨ä¼˜åŠ¿çš„æƒ…å†µä¸‹ï¼Œè¿™æ˜¯ Q å€¼å’Œä»·å€¼çš„å·®å¼‚ï¼‰ã€‚æœ€ç®€å•çš„åŸºçº¿æ˜¯å¥–åŠ±æ‰¹æ¬¡çš„å¹³å‡å€¼æˆ–ç§»åŠ¨å¹³å‡å€¼ã€‚å³ä½¿è¿™äº›åŸºçº¿ä¹Ÿå¯ä»¥å»åæ¢¯åº¦ï¼Œä½¿å¾—
    <semantics><mrow><msub><mi>ğ”¼</mi><mrow><mi>a</mi><mo>âˆ¼</mo><mi>Ï€</mi><mo stretchy="false"
    form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="prefix">|</mo><mi>s</mi><mo
    stretchy="false" form="postfix">)</mo></mrow></msub><mo stretchy="false" form="prefix">[</mo><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mrow><mi
    mathvariant="normal">log</mi><mo>â¡</mo></mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="prefix">|</mo><mi>s</mi><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">]</mo><mo>=</mo><mn>0</mn></mrow><annotation
    encoding="application/x-tex">\mathbb{E}_{a \sim \pi(a|s)}[\nabla_\theta \log \pi_\theta(a|s)]
    = 0</annotation></semantics>ï¼Œä»è€Œæ˜¾è‘—æé«˜å­¦ä¹ ä¿¡å·ã€‚
- en: 'Many of the policy gradient algorithms discussed in this chapter build on the
    advantage formulation of policy gradient:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« è®¨è®ºçš„è®¸å¤šç­–ç•¥æ¢¯åº¦ç®—æ³•å»ºç«‹åœ¨ç­–ç•¥æ¢¯åº¦çš„ä¼˜åŠ¿å…¬å¼ä¹‹ä¸Šï¼š
- en: <semantics><mrow><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi>J</mi><mo stretchy="false"
    form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ğ”¼</mi><mi>Ï„</mi></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><munderover><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>T</mi></munderover><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi
    mathvariant="normal">log</mi><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo><msup><mi>A</mi><msub><mi>Ï€</mi><mi>Î¸</mi></msub></msup><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>47</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">\nabla_\theta J(\theta) = \mathbb{E}_\tau \left[
    \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) A^{\pi_\theta}(s_t, a_t) \right]\qquad{(47)}</annotation></semantics>
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><msub><mi mathvariant="normal">âˆ‡</mi><mi>Î¸</mi></msub><mi>J</mi><mo
    stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ğ”¼</mi><mi>Ï„</mi></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><munderover><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>T</mi></munderover><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi
    mathvariant="normal">log</mi><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo><msup><mi>A</mi><msub><mi>Ï€</mi><mi>Î¸</mi></msub></msup><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>47</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">\nabla_\theta J(\theta) = \mathbb{E}_\tau \left[
    \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) A^{\pi_\theta}(s_t, a_t) \right]\qquad{(47)}</annotation></semantics>
- en: REINFORCE
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: REINFORCE
- en: 'The algorithm REINFORCE is likely a backronym, but the components of the algorithm
    it represents are quite relevant for modern reinforcement learning algorithms.
    Defined in the seminal paper *Simple statistical gradient-following algorithms
    for connectionist reinforcement learning* [[180]](ch021.xhtml#ref-williams1992simple):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ç®—æ³•REINFORCEå¯èƒ½æ˜¯ä¸€ä¸ªç¼©å†™ï¼Œä½†è¯¥ç®—æ³•æ‰€ä»£è¡¨çš„ç»„æˆéƒ¨åˆ†ä¸ç°ä»£å¼ºåŒ–å­¦ä¹ ç®—æ³•éå¸¸ç›¸å…³ã€‚åœ¨å¼€åˆ›æ€§çš„è®ºæ–‡ã€ŠSimple statistical gradient-following
    algorithms for connectionist reinforcement learningã€‹[[180]](ch021.xhtml#ref-williams1992simple)ä¸­å®šä¹‰ï¼š
- en: The name is an acronym for â€œREward Increment = Nonnegative Factor X Offset Reinforcement
    X Characteristic Eligibility.â€
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: è¯¥åç§°æ˜¯â€œå¥–åŠ±å¢é‡ = éè´Ÿå› å­ X åç§»å¼ºåŒ– X ç‰¹å¾èµ„æ ¼â€çš„ç¼©å†™ã€‚
- en: 'The three components of this are how to do the *reward increment*, a.k.a. the
    policy gradient step. It has three pieces to the update rule:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè¿‡ç¨‹çš„ä¸‰ä¸ªç»„æˆéƒ¨åˆ†æ˜¯å¦‚ä½•è¿›è¡Œ**å¥–åŠ±å¢é‡**ï¼Œä¹Ÿå°±æ˜¯æ‰€è°“çš„ç­–ç•¥æ¢¯åº¦æ­¥éª¤ã€‚æ›´æ–°è§„åˆ™æœ‰ä¸‰ä¸ªéƒ¨åˆ†ï¼š
- en: 'Nonnegative factor: This is the learning rate (step size) that must be a positive
    number, e.g.Â <semantics><mi>Î±</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics>
    below.'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: éè´Ÿå› å­ï¼šè¿™æ˜¯å¿…é¡»ä¸ºæ­£æ•°çš„**å­¦ä¹ ç‡ï¼ˆæ­¥é•¿ï¼‰**ï¼Œä¾‹å¦‚ä»¥ä¸‹ã€‚
- en: 'Offset Reinforcement: This is a baseline <semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics>
    or other normalizing factor of the reward to improve stability.'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åç§»å¼ºåŒ–ï¼šè¿™æ˜¯å¥–åŠ±çš„**åŸºå‡†** <semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics>
    æˆ–å…¶ä»–å½’ä¸€åŒ–å› å­ï¼Œä»¥æé«˜ç¨³å®šæ€§ã€‚
- en: 'Characteristic Eligibility: This is how the learning becomes attributed per
    token. It can be a general value, <semantics><mi>e</mi><annotation encoding="application/x-tex">e</annotation></semantics>
    per parameter, but is often log probabilities of the policy in modern equations.'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç‰¹å¾èµ„æ ¼ï¼šè¿™æ˜¯å­¦ä¹ å¦‚ä½•æŒ‰æ ‡è®°åˆ†é…çš„ã€‚å®ƒå¯ä»¥æ˜¯ä¸€ä¸ªä¸€èˆ¬å€¼ï¼Œ<semantics><mi>e</mi><annotation encoding="application/x-tex">e</annotation></semantics>
    æ¯ä¸ªå‚æ•°ï¼Œä½†é€šå¸¸æ˜¯ç°ä»£æ–¹ç¨‹ä¸­ç­–ç•¥çš„å¯¹æ•°æ¦‚ç‡ã€‚
- en: 'Thus, the form looks quite familiar:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œå½¢å¼çœ‹èµ·æ¥ç›¸å½“ç†Ÿæ‚‰ï¼š
- en: <semantics><mrow><msub><mi mathvariant="normal">Î”</mi><mi>Î¸</mi></msub><mo>=</mo><mi>Î±</mi><mo
    stretchy="false" form="prefix">(</mo><mi>r</mi><mo>âˆ’</mo><mi>b</mi><mo stretchy="false"
    form="postfix">)</mo><mi>e</mi><mrow><mo stretchy="false" form="prefix">(</mo><mn>48</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\Delta_\theta
    = \alpha(r - b)e \qquad{(48)}</annotation></semantics>
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><msub><mi mathvariant="normal">Î”</mi><mi>Î¸</mi></msub><mo>=</mo><mi>Î±</mi><mo
    stretchy="false" form="prefix">(</mo><mi>r</mi><mo>âˆ’</mo><mi>b</mi><mo stretchy="false"
    form="postfix">)</mo><mi>e</mi><mrow><mo stretchy="false" form="prefix">(</mo><mn>48</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\Delta_\theta
    = \alpha(r - b)e \qquad{(48)}</annotation></semantics>
- en: 'With more modern notation and the generalized return <semantics><mi>G</mi><annotation
    encoding="application/x-tex">G</annotation></semantics>, the REINFORCE operator
    appears as:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ›´ç°ä»£çš„ç¬¦å·å’Œå¹¿ä¹‰å›æŠ¥ <semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics>ï¼ŒREINFORCEç®—å­è¡¨ç°ä¸ºï¼š
- en: <semantics><mrow><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi>J</mi><mo stretchy="false"
    form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ğ”¼</mi><mrow><mi>Ï„</mi><mo>âˆ¼</mo><msub><mi>Ï€</mi><mi>Î¸</mi></msub></mrow></msub><mo
    minsize="180%" maxsize="180%" stretchy="true" form="prefix">[</mo><munderover><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>T</mi></munderover><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mrow><mi
    mathvariant="normal">log</mi><mo>â¡</mo></mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>âˆ£</mo><msub><mi>s</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="false" form="prefix">(</mo><msub><mi>G</mi><mi>t</mi></msub><mo>âˆ’</mo><mi>b</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false"
    form="postfix">)</mo><mo stretchy="false" form="postfix">)</mo><mo minsize="180%"
    maxsize="180%" stretchy="true" form="postfix">]</mo><mo>,</mo><mrow><mo stretchy="false"
    form="prefix">(</mo><mn>49</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">\nabla_{\theta}\,J(\theta) \;=\; \mathbb{E}_{\tau
    \sim \pi_{\theta}}\!\Big[ \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t
    \mid s_t)\,(G_t - b(s_t)) \Big], \qquad{(49)}</annotation></semantics>
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi>J</mi><mo stretchy="false"
    form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ğ”¼</mi><mrow><mi>Ï„</mi><mo>âˆ¼</mo><msub><mi>Ï€</mi><mi>Î¸</mi></msub></mrow></msub><mo
    minsize="180%" maxsize="180%" stretchy="true" form="prefix">[</mo><munderover><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>T</mi></munderover><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mrow><mi
    mathvariant="normal">log</mi><mo>â¡</mo></mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>âˆ£</mo><msub><mi>s</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="false" form="prefix">(</mo><msub><mi>G</mi><mi>t</mi></msub><mo>âˆ’</mo><mi>b</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false"
    form="postfix">)</mo><mo stretchy="false" form="postfix">)</mo><mo minsize="180%"
    maxsize="180%" stretchy="true" form="postfix">]</mo><mo>,</mo><mrow><mo stretchy="false"
    form="prefix">(</mo><mn>49</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">\nabla_{\theta}\,J(\theta) \;=\; \mathbb{E}_{\tau
    \sim \pi_{\theta}}\!\Big[ \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t
    \mid s_t)\,(G_t - b(s_t)) \Big], \qquad{(49)}</annotation></semantics>
- en: 'Here, the value <semantics><mrow><msub><mi>G</mi><mi>t</mi></msub><mo>âˆ’</mo><mi>b</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">G_t - b(s_t)</annotation></semantics>
    is the *advantage* of the policy at the current state, so we can reformulate the
    policy gradient in a form that we continue later with the advantage, <semantics><mi>A</mi><annotation
    encoding="application/x-tex">A</annotation></semantics>:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨è¿™é‡Œï¼Œå€¼ <semantics><mrow><msub><mi>G</mi><mi>t</mi></msub><mo>âˆ’</mo><mi>b</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">G_t - b(s_t)</annotation></semantics>
    æ˜¯å½“å‰çŠ¶æ€çš„ç­–ç•¥ä¼˜åŠ¿ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥å°†ç­–ç•¥æ¢¯åº¦é‡æ–°è¡¨è¿°ä¸ºä¸€ç§å½¢å¼ï¼Œæˆ‘ä»¬ç¨åä¼šç»§ç»­ä½¿ç”¨ä¼˜åŠ¿ï¼Œ<semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics>:'
- en: <semantics><mrow><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi>J</mi><mo stretchy="false"
    form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ğ”¼</mi><mrow><mi>Ï„</mi><mo>âˆ¼</mo><msub><mi>Ï€</mi><mi>Î¸</mi></msub></mrow></msub><mo
    minsize="180%" maxsize="180%" stretchy="true" form="prefix">[</mo><munderover><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>T</mi></munderover><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mrow><mi
    mathvariant="normal">log</mi><mo>â¡</mo></mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>âˆ£</mo><msub><mi>s</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub><mo minsize="180%"
    maxsize="180%" stretchy="true" form="postfix">]</mo><mo>,</mo><mrow><mo stretchy="false"
    form="prefix">(</mo><mn>50</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">\nabla_{\theta}\,J(\theta) \;=\; \mathbb{E}_{\tau
    \sim \pi_{\theta}}\!\Big[ \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t
    \mid s_t)\,A_t \Big], \qquad{(50)}</annotation></semantics>
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi>J</mi><mo stretchy="false"
    form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ğ”¼</mi><mrow><mi>Ï„</mi><mo>âˆ¼</mo><msub><mi>Ï€</mi><mi>Î¸</mi></msub></mrow></msub><mo
    minsize="180%" maxsize="180%" stretchy="true" form="prefix">[</mo><munderover><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>T</mi></munderover><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mrow><mi
    mathvariant="normal">log</mi><mo>â¡</mo></mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>âˆ£</mo><msub><mi>s</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub><mo minsize="180%"
    maxsize="180%" stretchy="true" form="postfix">]</mo><mo>,</mo><mrow><mo stretchy="false"
    form="prefix">(</mo><mn>50</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">\nabla_{\theta}\,J(\theta) \;=\; \mathbb{E}_{\tau
    \sim \pi_{\theta}}\!\Big[ \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t
    \mid s_t)\,A_t \Big], \qquad{(50)}</annotation></semantics>
- en: REINFORCE is a specific implementation of vanilla policy gradient that uses
    a Monte Carlo estimator of the gradient.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: REINFORCEæ˜¯vanillaç­–ç•¥æ¢¯åº¦çš„ç‰¹å®šå®ç°ï¼Œå®ƒä½¿ç”¨æ¢¯åº¦è’™ç‰¹å¡æ´›ä¼°è®¡å™¨ã€‚
- en: REINFORCE Leave One Out (RLOO)
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: REINFORCE Leave One Out (RLOO)
- en: The core implementation detail of REINFORCE Leave One Out versus standard REINFORCE
    is that it takes the average reward of the *other* samples in the batch to compute
    the baseline â€“ rather than averaging over all rewards in the batch [[181]](ch021.xhtml#ref-huang2024putting),
    [[178]](ch021.xhtml#ref-ahmadian2024back), [[182]](ch021.xhtml#ref-kool2019buy).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸æ ‡å‡†REINFORCEç›¸æ¯”ï¼ŒREINFORCE Leave One Outçš„æ ¸å¿ƒå®ç°ç»†èŠ‚åœ¨äºå®ƒä½¿ç”¨æ‰¹ä¸­*å…¶ä»–*æ ·æœ¬çš„å¹³å‡å¥–åŠ±æ¥è®¡ç®—åŸºçº¿â€”â€”è€Œä¸æ˜¯å¹³å‡æ‰¹ä¸­çš„æ‰€æœ‰å¥–åŠ±
    [[181]](ch021.xhtml#ref-huang2024putting), [[178]](ch021.xhtml#ref-ahmadian2024back),
    [[182]](ch021.xhtml#ref-kool2019buy).
- en: Crucially, this only works when generating multiple trajectories (completions)
    per state (prompt), which is common practice in multiple domains of finetuning
    language models with RL.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å…³é”®çš„æ˜¯ï¼Œè¿™ä»…åœ¨ä¸ºæ¯ä¸ªçŠ¶æ€ï¼ˆæç¤ºï¼‰ç”Ÿæˆå¤šä¸ªè½¨è¿¹ï¼ˆå®Œæˆï¼‰æ—¶æ‰æœ‰æ•ˆï¼Œè¿™åœ¨ä½¿ç”¨RLå¾®è°ƒè¯­è¨€æ¨¡å‹çš„å¤šä¸ªé¢†åŸŸä¸­æ˜¯å¸¸è§åšæ³•ã€‚
- en: 'Specifically, for the REINFORCE Leave-One-Out (RLOO) baseline, given <semantics><mi>K</mi><annotation
    encoding="application/x-tex">K</annotation></semantics> sampled trajectories (actions
    taken conditioned on a prompt) <semantics><mrow><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><mi>â€¦</mi><mo>,</mo><msub><mi>a</mi><mi>K</mi></msub></mrow><annotation
    encoding="application/x-tex">a_1, \dots, a_K</annotation></semantics>, to a given
    prompt <semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics>
    we define the baseline explicitly as the following *per-prompt*:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: å…·ä½“æ¥è¯´ï¼Œå¯¹äºREINFORCE Leave-One-Out (RLOO)åŸºçº¿ï¼Œç»™å®š<semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics>ä¸ªé‡‡æ ·çš„è½¨è¿¹ï¼ˆåœ¨æç¤ºä¸‹é‡‡å–çš„åŠ¨ä½œï¼‰<semantics><mrow><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><mi>â€¦</mi><mo>,</mo><msub><mi>a</mi><mi>K</mi></msub></mrow><annotation
    encoding="application/x-tex">a_1, \dots, a_K</annotation></semantics>ï¼Œé’ˆå¯¹ç»™å®šçš„æç¤º<semantics><mi>s</mi><annotation
    encoding="application/x-tex">s</annotation></semantics>ï¼Œæˆ‘ä»¬æ˜ç¡®åœ°å°†åŸºçº¿å®šä¹‰ä¸ºä»¥ä¸‹*æ¯ä¸ªæç¤º*ï¼š
- en: <semantics><mrow><mi>b</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><msub><mi>a</mi><mi>k</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mi>K</mi><mo>âˆ’</mo><mn>1</mn></mrow></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>i</mi><mo>â‰ </mo><mi>k</mi></mrow><mi>K</mi></munderover><mi>R</mi><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><msub><mi>a</mi><mi>i</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo>,</mo><mrow><mo stretchy="false" form="prefix">(</mo><mn>51</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">b(s,
    a_k) = \frac{1}{K-1}\sum_{i=1, i\neq k}^{K} R(s, a_i), \qquad{(51)}</annotation></semantics>
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mi>b</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><msub><mi>a</mi><mi>k</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mi>K</mi><mo>âˆ’</mo><mn>1</mn></mrow></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>i</mi><mo>â‰ </mo><mi>k</mi></mrow><mi>K</mi></munderover><mi>R</mi><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><msub><mi>a</mi><mi>i</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo>,</mo><mrow><mo stretchy="false" form="prefix">(</mo><mn>51</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">b(s,
    a_k) = \frac{1}{K-1}\sum_{i=1, i\neq k}^{K} R(s, a_i), \qquad{(51)}</annotation></semantics>
- en: 'resulting in the advantage:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœäº§ç”Ÿä¼˜åŠ¿ï¼š
- en: <semantics><mrow><mi>A</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><msub><mi>a</mi><mi>k</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mi>R</mi><mo stretchy="false"
    form="prefix">(</mo><mi>s</mi><mo>,</mo><msub><mi>a</mi><mi>k</mi></msub><mo stretchy="false"
    form="postfix">)</mo><mo>âˆ’</mo><mi>b</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><msub><mi>a</mi><mi>k</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mi>.</mi><mrow><mo stretchy="false" form="prefix">(</mo><mn>52</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">A(s,
    a_k) = R(s, a_k) - b(s, a_k). \qquad{(52)}</annotation></semantics>
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><mi>A</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><msub><mi>a</mi><mi>k</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mi>R</mi><mo stretchy="false"
    form="prefix">(</mo><mi>s</mi><mo>,</mo><msub><mi>a</mi><mi>k</mi></msub><mo stretchy="false"
    form="postfix">)</mo><mo>âˆ’</mo><mi>b</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><msub><mi>a</mi><mi>k</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mi>.</mi><mrow><mo stretchy="false" form="prefix">(</mo><mn>52</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">A(s,
    a_k) = R(s, a_k) - b(s, a_k). \qquad{(52)}</annotation></semantics>
- en: 'Equivalently, this can be expressed as:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ç­‰ä»·åœ°ï¼Œè¿™å¯ä»¥è¡¨ç¤ºä¸ºï¼š
- en: <semantics><mrow><mi>A</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><msub><mi>a</mi><mi>k</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mi>K</mi><mrow><mi>K</mi><mo>âˆ’</mo><mn>1</mn></mrow></mfrac><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><msub><mi>a</mi><mi>k</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><mfrac><mn>1</mn><mi>K</mi></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mi>R</mi><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><msub><mi>a</mi><mi>i</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>53</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">A(s, a_k) = \frac{K}{K - 1}\left(R(s,
    a_k) - \frac{1}{K}\sum_{i=1}^{K} R(s, a_i)\right). \qquad{(53)}</annotation></semantics>
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><mi>A</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><msub><mi>a</mi><mi>k</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mi>K</mi><mrow><mi>K</mi><mo>âˆ’</mo><mn>1</mn></mrow></mfrac><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><msub><mi>a</mi><mi>k</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><mfrac><mn>1</mn><mi>K</mi></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mi>R</mi><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><msub><mi>a</mi><mi>i</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>53</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">A(s, a_k) = \frac{K}{K - 1}\left(R(s,
    a_k) - \frac{1}{K}\sum_{i=1}^{K} R(s, a_i)\right). \qquad{(53)}</annotation></semantics>
- en: This is a simple, low-variance *per-prompt* advantage estimate that is closely
    related to the group-relative advantage used in Group Relative Policy Optimization,
    GRPO (discussed shortly, after Proximal Policy Optimization, PPO). In practice,
    GRPO-style training mainly differs in how it applies the KL regularizer (as an
    explicit loss term vs.Â folded into the reward) and whether it uses PPO-style ratio
    clipping. To be specific, the canonical GRPO implementation applies the KL penalty
    at the loss level, where the derivation for RLOO or traditional policy-gradients
    apply the KL penalty to the reward itself. With the transition from RLHF to reasoning
    and reinforcement learning with verifiable rewards (RLVR), the prevalence of KL
    penalties has decreased overall, with many reasoning adaptations of RLHF code
    turning them off entirely. Still, the advantage from RLOO could be combined with
    the clipping of PPO, showing how similar many of these algorithms are.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªç®€å•ã€ä½å˜å·®çš„**æ¯æç¤º**ä¼˜åŠ¿ä¼°è®¡ï¼Œå®ƒä¸åœ¨ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ä¸­ä½¿ç”¨çš„ç»„ç›¸å¯¹ä¼˜åŠ¿å¯†åˆ‡ç›¸å…³ï¼ˆåœ¨æ¥è¿‘ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ä¹‹åå°†ç®€è¦è®¨è®ºï¼‰ã€‚åœ¨å®è·µä¸­ï¼ŒGRPOé£æ ¼çš„è®­ç»ƒä¸»è¦åŒºåˆ«åœ¨äºå®ƒå¦‚ä½•åº”ç”¨KLæ­£åˆ™åŒ–å™¨ï¼ˆä½œä¸ºä¸€ä¸ªæ˜¾å¼çš„æŸå¤±é¡¹ä¸æŠ˜å åˆ°å¥–åŠ±ä¸­ï¼‰ä»¥åŠæ˜¯å¦ä½¿ç”¨PPOé£æ ¼çš„æ¯”ç‡å‰ªè¾‘ã€‚å…·ä½“æ¥è¯´ï¼Œæ ‡å‡†çš„GRPOå®ç°å°†KLæƒ©ç½šåº”ç”¨äºæŸå¤±çº§åˆ«ï¼Œè€ŒRLOOæˆ–ä¼ ç»Ÿçš„ç­–ç•¥æ¢¯åº¦åœ¨å¥–åŠ±æœ¬èº«ä¸Šåº”ç”¨KLæƒ©ç½šã€‚éšç€ä»RLHFåˆ°å…·æœ‰å¯éªŒè¯å¥–åŠ±çš„æ¨ç†å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰çš„è½¬å˜ï¼ŒKLæƒ©ç½šçš„æ™®éæ€§æ€»ä½“ä¸Šæœ‰æ‰€ä¸‹é™ï¼Œè®¸å¤šRLHFä»£ç çš„æ¨ç†é€‚é…å°†å…¶å®Œå…¨å…³é—­ã€‚å°½ç®¡å¦‚æ­¤ï¼ŒRLOOçš„ä¼˜åŠ¿å¯ä»¥ä¸PPOçš„å‰ªè¾‘ç›¸ç»“åˆï¼Œå±•ç¤ºäº†è¿™äº›ç®—æ³•çš„ç›¸ä¼¼æ€§ã€‚
- en: RLOO and other algorithms that do not use a value network â€“ an additional model
    copy (a critic) that predicts a scalar value <semantics><mrow><mi>V</mi><mo stretchy="false"
    form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">V(s_t)</annotation></semantics> per token â€“ assign
    the same sequence-level advantage (or reward) to every token when computing the
    loss. Algorithms that use a learned value network, such as PPO, assign a different
    value to every token individually, discounting from the final reward achieved
    at the EOS token. With a KL distance penalty, RLOO aggregates the per-token KL
    over the completion and folds that scalar into the sequence reward, so the resulting
    advantage is broadcast to all tokens. PPO subtracts a per-token KL from the per-token
    reward before computing <semantics><msub><mi>A</mi><mi>t</mi></msub><annotation
    encoding="application/x-tex">A_t</annotation></semantics>, giving token-level
    credit assignment. GRPO typically retains a sequence-level advantage but adds
    a separate per-token term to the loss, rather than subtracting it from the reward.
    These details and trade-offs are discussed later in the chapter.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: RLOOå’Œå…¶ä»–ä¸ä½¿ç”¨ä»·å€¼ç½‘ç»œï¼ˆä¸€ä¸ªé¢å¤–çš„æ¨¡å‹å‰¯æœ¬ï¼Œå³è¯„è®ºå®¶ï¼Œé¢„æµ‹æ¯ä¸ªæ ‡è®°çš„æ ‡é‡å€¼<semantics><mrow><mi>V</mi><mo stretchy="false"
    form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">V(s_t)</annotation></semantics>ï¼‰çš„ç®—æ³•å°†ç›¸åŒçš„åºåˆ—çº§ä¼˜åŠ¿ï¼ˆæˆ–å¥–åŠ±ï¼‰åˆ†é…ç»™æ¯ä¸ªæ ‡è®°ï¼Œå½“è®¡ç®—æŸå¤±æ—¶ã€‚ä½¿ç”¨å­¦ä¹ ä»·å€¼ç½‘ç»œï¼ˆå¦‚PPOï¼‰çš„ç®—æ³•ä¸ºæ¯ä¸ªæ ‡è®°åˆ†é…ä¸åŒçš„å€¼ï¼Œä»EOSæ ‡è®°æœ€ç»ˆè·å¾—çš„å¥–åŠ±ä¸­è¿›è¡ŒæŠ˜æ‰£ã€‚å…·æœ‰KLè·ç¦»æƒ©ç½šçš„RLOOå°†æ¯ä¸ªæ ‡è®°çš„KLèšåˆåˆ°å®Œæˆå’ŒæŠ˜å ä¸­ï¼Œå°†è¯¥æ ‡é‡æŠ˜å åˆ°åºåˆ—å¥–åŠ±ä¸­ï¼Œå› æ­¤äº§ç”Ÿçš„ä¼˜åŠ¿å¹¿æ’­åˆ°æ‰€æœ‰æ ‡è®°ã€‚PPOåœ¨è®¡ç®—<semantics><msub><mi>A</mi><mi>t</mi></msub><annotation
    encoding="application/x-tex">A_t</annotation></semantics>ä¹‹å‰ä»æ¯ä¸ªæ ‡è®°çš„å¥–åŠ±ä¸­å‡å»ä¸€ä¸ªæ¯ä¸ªæ ‡è®°çš„KLï¼Œä»è€Œè¿›è¡Œæ ‡è®°çº§ä¿¡ç”¨åˆ†é…ã€‚GRPOé€šå¸¸ä¿ç•™åºåˆ—çº§ä¼˜åŠ¿ï¼Œä½†å°†ä¸€ä¸ªå•ç‹¬çš„æ¯ä¸ªæ ‡è®°é¡¹æ·»åŠ åˆ°æŸå¤±ä¸­ï¼Œè€Œä¸æ˜¯ä»å¥–åŠ±ä¸­å‡å»å®ƒã€‚è¿™äº›ç»†èŠ‚å’Œæƒè¡¡å°†åœ¨æœ¬ç« åé¢è®¨è®ºã€‚
- en: Proximal Policy Optimization
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–
- en: 'Proximal Policy Optimization (PPO) [[183]](ch021.xhtml#ref-schulman2017proximal)
    is one of the foundational algorithms behind Deep RLâ€™s successes (such as OpenAIâ€™s
    Five, which mastered DOTA 2 [[184]](ch021.xhtml#ref-berner2019dota) and large
    amounts of research). The objective that PPO maximizes, with respect to the advantages
    and the policy probabilities, is as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰[[183]](ch021.xhtml#ref-schulman2017proximal)æ˜¯æ·±åº¦å¼ºåŒ–å­¦ä¹ æˆåŠŸèƒŒåçš„åŸºç¡€ç®—æ³•ä¹‹ä¸€ï¼ˆä¾‹å¦‚ï¼ŒOpenAIçš„FiveæŒæ¡äº†DOTA
    2 [[184]](ch021.xhtml#ref-berner2019dota)ä»¥åŠå¤§é‡çš„ç ”ç©¶ï¼‰ã€‚PPOç›¸å¯¹äºä¼˜åŠ¿å’Œç­–ç•¥æ¦‚ç‡æœ€å¤§åŒ–çš„ç›®æ ‡æ˜¯ä»¥ä¸‹å†…å®¹ï¼š
- en: <semantics><mrow><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mrow><mi mathvariant="normal">min</mi><mo>â¡</mo></mrow><mrow><mo
    stretchy="true" form="prefix">(</mo><mfrac><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="prefix">|</mo><mi>s</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>Ï€</mi><msub><mi>Î¸</mi><mtext
    mathvariant="normal">old</mtext></msub></msub><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo
    stretchy="false" form="prefix">|</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mi>A</mi><mo>,</mo><mtext
    mathvariant="normal">clip</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="prefix">|</mo><mi>s</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>Ï€</mi><msub><mi>Î¸</mi><mtext
    mathvariant="normal">old</mtext></msub></msub><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo
    stretchy="false" form="prefix">|</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo>,</mo><mn>1</mn><mo>âˆ’</mo><mi>Îµ</mi><mo>,</mo><mn>1</mn><mo>+</mo><mi>Îµ</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>54</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">J(\theta) = \min\left(\frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)}A,
    \text{clip} \left( \frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)}, 1-\varepsilon,
    1+\varepsilon \right) A \right).\qquad{(54)}</annotation></semantics>
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mrow><mi mathvariant="normal">min</mi><mo>â¡</mo></mrow><mrow><mo
    stretchy="true" form="prefix">(</mo><mfrac><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="prefix">|</mo><mi>s</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>Ï€</mi><msub><mi>Î¸</mi><mtext
    mathvariant="normal">old</mtext></msub></msub><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo
    stretchy="false" form="prefix">|</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mi>A</mi><mo>,</mo><mtext
    mathvariant="normal">clip</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="prefix">|</mo><mi>s</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>Ï€</mi><msub><mi>Î¸</mi><mtext
    mathvariant="normal">old</mtext></msub></msub><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo
    stretchy="false" form="prefix">|</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo>,</mo><mn>1</mn><mo>âˆ’</mo><mi>Îµ</mi><mo>,</mo><mn>1</mn><mo>+</mo><mi>Îµ</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>54</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">J(\theta) = \min\left(\frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)}A,
    \text{clip} \left( \frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)}, 1-\varepsilon,
    1+\varepsilon \right) A \right).\qquad{(54)}</annotation></semantics>
- en: Here, <semantics><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="prefix">|</mo><mi>s</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\pi_\theta(a|s)</annotation></semantics>
    is the current policy being optimized and <semantics><mrow><msub><mi>Ï€</mi><msub><mi>Î¸</mi><mtext
    mathvariant="normal">old</mtext></msub></msub><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo
    stretchy="false" form="prefix">|</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">\pi_{\theta_{\text{old}}}(a|s)</annotation></semantics>
    is the policy that was used to collect the training data (i.e., the policy from
    the previous iteration). The ratio between these two policies emerges from *importance
    sampling*, which allows us to reuse data collected under an old policy to estimate
    gradients for a new policy.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œ<semantics><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo
    stretchy="false" form="prefix">|</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">\pi_\theta(a|s)</annotation></semantics> æ˜¯æ­£åœ¨ä¼˜åŒ–çš„å½“å‰ç­–ç•¥ï¼Œè€Œ
    <semantics><mrow><msub><mi>Ï€</mi><msub><mi>Î¸</mi><mtext mathvariant="normal">old</mtext></msub></msub><mo
    stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="prefix">|</mo><mi>s</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\pi_{\theta_{\text{old}}}(a|s)</annotation></semantics>
    æ˜¯ç”¨äºæ”¶é›†è®­ç»ƒæ•°æ®çš„ç­–ç•¥ï¼ˆå³å‰ä¸€æ¬¡è¿­ä»£çš„ç­–ç•¥ï¼‰ã€‚è¿™ä¸¤ä¸ªç­–ç•¥ä¹‹é—´çš„æ¯”ç‡æ¥è‡ª *é‡è¦æ€§é‡‡æ ·*ï¼Œè¿™å…è®¸æˆ‘ä»¬é‡ç”¨æ ¹æ®æ—§ç­–ç•¥æ”¶é›†çš„æ•°æ®æ¥ä¼°è®¡æ–°ç­–ç•¥çš„æ¢¯åº¦ã€‚
- en: 'Recall from the advantage formulation of the policy gradient (eq.Â [47](ch011.xhtml#eq:advantage_policy_gradient))
    that we have: <semantics><mrow><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi>J</mi><mo
    stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ğ”¼</mi><mrow><mi>Ï„</mi><mo>âˆ¼</mo><msub><mi>Ï€</mi><mi>Î¸</mi></msub></mrow></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><munderover><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>T</mi></munderover><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi
    mathvariant="normal">log</mi><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo><msup><mi>A</mi><msub><mi>Ï€</mi><mi>Î¸</mi></msub></msup><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow><mi>.</mi></mrow><annotation
    encoding="application/x-tex">\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}
    \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) A^{\pi_\theta}(s_t,
    a_t) \right].</annotation></semantics>'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: This expectation is taken over trajectories sampled from <semantics><msub><mi>Ï€</mi><mi>Î¸</mi></msub><annotation
    encoding="application/x-tex">\pi_\theta</annotation></semantics>, but in practice
    we want to take multiple gradient steps on a batch of data that was collected
    from a fixed policy <semantics><msub><mi>Ï€</mi><msub><mi>Î¸</mi><mtext mathvariant="normal">old</mtext></msub></msub><annotation
    encoding="application/x-tex">\pi_{\theta_{\text{old}}}</annotation></semantics>.
    To correct for this distribution mismatch, we multiply by the importance weight
    <semantics><mfrac><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="prefix">|</mo><mi>s</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>Ï€</mi><msub><mi>Î¸</mi><mtext
    mathvariant="normal">old</mtext></msub></msub><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo
    stretchy="false" form="prefix">|</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><annotation
    encoding="application/x-tex">\frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)}</annotation></semantics>,
    which reweights samples to account for how much more or less likely they are under
    the current policy versus the data-collection policy. Without constraints, optimizing
    this importance-weighted objective can lead to destructively large policy updates
    when the ratio diverges far from 1\. PPO addresses this by clipping the ratio
    to the range <semantics><mrow><mo stretchy="false" form="prefix">[</mo><mn>1</mn><mo>âˆ’</mo><mi>Îµ</mi><mo>,</mo><mn>1</mn><mo>+</mo><mi>Îµ</mi><mo
    stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[1-\varepsilon,
    1+\varepsilon]</annotation></semantics>, ensuring that the policy cannot change
    too drastically in a single update.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æœŸæœ›æ˜¯ä»ä» <semantics><msub><mi>Ï€</mi><mi>Î¸</mi></msub><annotation encoding="application/x-tex">\pi_\theta</annotation></semantics>
    ä¸­é‡‡æ ·çš„è½¨è¿¹ä¸­è·å¾—çš„ï¼Œä½†åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›åœ¨ä»å›ºå®šç­–ç•¥ <semantics><msub><mi>Ï€</mi><msub><mi>Î¸</mi><mtext
    mathvariant="normal">old</mtext></msub></msub><annotation encoding="application/x-tex">\pi_{\theta_{\text{old}}}</annotation></semantics>
    æ”¶é›†çš„ä¸€æ‰¹æ•°æ®ä¸Šæ‰§è¡Œå¤šä¸ªæ¢¯åº¦æ­¥éª¤ã€‚ä¸ºäº†çº æ­£è¿™ç§åˆ†å¸ƒä¸åŒ¹é…ï¼Œæˆ‘ä»¬ä¹˜ä»¥é‡è¦æ€§æƒé‡ <semantics><mfrac><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="prefix">|</mo><mi>s</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>Ï€</mi><msub><mi>Î¸</mi><mtext
    mathvariant="normal">old</mtext></msub></msub><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo
    stretchy="false" form="prefix">|</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><annotation
    encoding="application/x-tex">\frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)}</annotation></semantics>ï¼Œè¿™ä¼šé‡æ–°åŠ æƒæ ·æœ¬ï¼Œä»¥è€ƒè™‘å®ƒä»¬åœ¨å½“å‰ç­–ç•¥ä¸æ•°æ®æ”¶é›†ç­–ç•¥ä¸‹æ›´æœ‰å¯èƒ½æˆ–ä¸å¤ªå¯èƒ½çš„æƒ…å†µã€‚å¦‚æœæ²¡æœ‰çº¦æŸï¼Œå½“æ¯”ç‡è¿œç¦»1æ—¶ï¼Œä¼˜åŒ–è¿™ä¸ªé‡è¦æ€§åŠ æƒç›®æ ‡å¯èƒ½å¯¼è‡´ç­–ç•¥æ›´æ–°è¿‡å¤§ã€‚PPOé€šè¿‡å°†æ¯”ç‡å‰ªè¾‘åˆ°èŒƒå›´
    <semantics><mrow><mo stretchy="false" form="prefix">[</mo><mn>1</mn><mo>âˆ’</mo><mi>Îµ</mi><mo>]</mo><mn>1</mn><mo>+</mo><mi>Îµ</mi><mo
    stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[1-\varepsilon,
    1+\varepsilon]</annotation></semantics> å†…æ¥è§£å†³æ­¤é—®é¢˜ï¼Œç¡®ä¿ç­–ç•¥åœ¨å•ä¸ªæ›´æ–°ä¸­ä¸ä¼šå‘ç”Ÿå¤ªå¤§å˜åŒ–ã€‚
- en: 'For completeness, PPO is typically written as an *expected* clipped surrogate
    objective over timesteps:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å®Œæ•´æ€§ï¼ŒPPOé€šå¸¸è¢«å†™æˆæ—¶é—´æ­¥é•¿ä¸Šçš„ *æœŸæœ›* å‰ªè¾‘ä»£ç†ç›®æ ‡ï¼š
- en: <semantics><mrow><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ğ”¼</mi><mi>t</mi></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><mi mathvariant="normal">min</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>r</mi><mi>t</mi></msub><mo stretchy="false"
    form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub><mo>,</mo><mtext
    mathvariant="normal">clip</mtext><mo stretchy="false" form="prefix">(</mo><msub><mi>r</mi><mi>t</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>,</mo><mn>1</mn><mo>âˆ’</mo><mi>Îµ</mi><mo>,</mo><mn>1</mn><mo>+</mo><mi>Îµ</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub><mo stretchy="true"
    form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mo>,</mo><msub><mi>R</mi><mi>t</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>âˆ£</mo><msub><mi>s</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>Ï€</mi><msub><mi>Î¸</mi><mtext
    mathvariant="normal">old</mtext></msub></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>âˆ£</mo><msub><mi>s</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow></mfrac><mi>.</mi><mrow><mo stretchy="false"
    form="prefix">(</mo><mn>55</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">J(\theta) = \mathbb{E}_{t}\left[ \min\left(r_t(\theta)A_t,\
    \text{clip}(r_t(\theta),1-\varepsilon,1+\varepsilon)A_t\right) \right], \qquad
    R_t(\theta)=\frac{\pi_\theta(a_t\mid s_t)}{\pi_{\theta_{\text{old}}}(a_t\mid s_t)}.
    \qquad{(55)}</annotation></semantics>
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ğ”¼</mi><mi>t</mi></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><mi mathvariant="normal">min</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>r</mi><mi>t</mi></msub><mo stretchy="false"
    form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub><mo>,</mo><mtext
    mathvariant="normal">clip</mtext><mo stretchy="false" form="prefix">(</mo><msub><mi>r</mi><mi>t</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>,</mo><mn>1</mn><mo>âˆ’</mo><mi>Îµ</mi><mo>,</mo><mn>1</mn><mo>+</mo><mi>Îµ</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub><mo stretchy="true"
    form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mo>,</mo><msub><mi>R</mi><mi>t</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>âˆ£</mo><msub><mi>s</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>Ï€</mi><msub><mi>Î¸</mi><mtext
    mathvariant="normal">old</mtext></msub></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>âˆ£</mo><msub><mi>s</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow></mfrac><mi>.</mi><mrow><mo stretchy="false"
    form="prefix">(</mo><mn>55</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">J(\theta) = \mathbb{E}_{t}\left[ \min\left(r_t(\theta)A_t,\
    \text{clip}(r_t(\theta),1-\varepsilon,1+\varepsilon)A_t\right) \right], \qquad
    R_t(\theta)=\frac{\pi_\theta(a_t\mid s_t)}{\pi_{\theta_{\text{old}}}(a_t\mid s_t)}.
    \qquad{(55)}</annotation></semantics>
- en: The objective is often converted into a loss function by simply adding a negative
    sign, which makes the optimizer seek to make it as negative as possible.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®æ ‡é€šå¸¸é€šè¿‡ç®€å•åœ°æ·»åŠ ä¸€ä¸ªè´Ÿå·è½¬æ¢ä¸ºæŸå¤±å‡½æ•°ï¼Œè¿™ä½¿å¾—ä¼˜åŒ–å™¨å¯»æ±‚ä½¿å…¶å°½å¯èƒ½è´Ÿã€‚
- en: For language models, the objective (or loss) is computed per token, which intuitively
    can be grounded in how one would compute the probability of the entire sequence
    of autoregressive predictions â€“ by a product of probabilities. From there, the
    common implementation is with *log-probabilities* that make the computation simpler
    to perform in modern language modeling frameworks.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¯­è¨€æ¨¡å‹ï¼Œç›®æ ‡ï¼ˆæˆ–æŸå¤±ï¼‰æ˜¯æŒ‰æ¯ä¸ªæ ‡è®°è®¡ç®—çš„ï¼Œç›´è§‚ä¸Šå¯ä»¥åŸºäºå¦‚ä½•è®¡ç®—è‡ªå›å½’é¢„æµ‹æ•´ä¸ªåºåˆ—çš„æ¦‚ç‡â€”â€”é€šè¿‡æ¦‚ç‡çš„ä¹˜ç§¯ã€‚ä»è¿™é‡Œï¼Œå¸¸è§çš„å®ç°æ–¹å¼æ˜¯ä½¿ç”¨*å¯¹æ•°æ¦‚ç‡*ï¼Œè¿™ä½¿å¾—åœ¨ç°ä»£è¯­è¨€å»ºæ¨¡æ¡†æ¶ä¸­æ‰§è¡Œè®¡ç®—æ›´åŠ ç®€å•ã€‚
- en: <semantics><mrow><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mo stretchy="false"
    form="prefix">|</mo><mi>a</mi><mo stretchy="false" form="prefix">|</mo></mrow></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mrow><mo
    stretchy="false" form="prefix">|</mo><mi>a</mi><mo stretchy="false" form="prefix">|</mo></mrow></munderover><mrow><mi
    mathvariant="normal">min</mi><mo>â¡</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false"
    form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>Ï€</mi><msub><mi>Î¸</mi><mtext
    mathvariant="normal">old</mtext></msub></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false"
    form="postfix">)</mo></mrow></mfrac><msub><mi>A</mi><mi>t</mi></msub><mo>,</mo><mtext
    mathvariant="normal">clip</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false"
    form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>Ï€</mi><msub><mi>Î¸</mi><mtext
    mathvariant="normal">old</mtext></msub></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false"
    form="postfix">)</mo></mrow></mfrac><mo>,</mo><mn>1</mn><mo>âˆ’</mo><mi>Îµ</mi><mo>,</mo><mn>1</mn><mo>+</mo><mi>Îµ</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><msub><mi>A</mi><mi>t</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow><mi>.</mi><mrow><mo stretchy="false"
    form="prefix">(</mo><mn>56</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">J(\theta) = \frac{1}{|a|} \sum_{t=0}^{|a|}
    \min\left(\frac{\pi_\theta(a_{t}|s_t)}{\pi_{\theta_{\text{old}}}(a_{t}|s_t)}A_{t},
    \text{clip} \left( \frac{\pi_\theta(a_{t}|s_t)}{\pi_{\theta_{\text{old}}}(a_{t}|s_t)},
    1-\varepsilon, 1+\varepsilon \right) A_{t} \right). \qquad{(56)}</annotation></semantics>
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: This is the per-token version of PPO, which also applies to other policy-gradient
    methods, but is explored further later in the implementation section of this chapter.
    Here, the term for averaging by the number of tokens in the action, <semantics><mfrac><mn>1</mn><mrow><mo
    stretchy="false" form="prefix">|</mo><mi>a</mi><mo stretchy="false" form="prefix">|</mo></mrow></mfrac><annotation
    encoding="application/x-tex">\frac{1}{|a|}</annotation></semantics>, comes from
    common implementation practices, but is not in a formal derivation of the loss
    (shown in [[185]](ch021.xhtml#ref-liu2025understanding)).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯PPOçš„æŒ‰ä»¤ç‰Œç‰ˆæœ¬ï¼Œä¹Ÿé€‚ç”¨äºå…¶ä»–ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼Œä½†å°†åœ¨æœ¬ç« çš„å®ç°éƒ¨åˆ†è¿›ä¸€æ­¥æ¢è®¨ã€‚åœ¨è¿™é‡Œï¼Œå¹³å‡åŠ¨ä½œä¸­ä»¤ç‰Œæ•°é‡çš„æœ¯è¯­<semantics><mfrac><mn>1</mn><mrow><mo
    stretchy="false" form="prefix">|</mo><mi>a</mi><mo stretchy="false" form="prefix">|</mo></mrow></mfrac><annotation
    encoding="application/x-tex">\frac{1}{|a|}</annotation></semantics>æ¥è‡ªå¸¸è§çš„å®ç°å®è·µï¼Œä½†å¹¶ä¸åœ¨æŸå¤±å‡½æ•°çš„æ­£å¼æ¨å¯¼ï¼ˆè§[[185]](ch021.xhtml#ref-liu2025understanding)ï¼‰ä¸­ã€‚
- en: 'Here we will explain the different cases this loss function triggers given
    various advantages and policy ratios. At an implementation level, the inner computations
    for PPO involve two main terms: 1) a standard policy gradient with a learned advantage
    and 2) a clipped policy gradient based on a maximum step size.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†è§£é‡Šè¿™ä¸ªæŸå¤±å‡½æ•°æ ¹æ®ä¸åŒçš„ä¼˜åŠ¿å’Œç­–ç•¥æ¯”ç‡è§¦å‘çš„æƒ…å†µã€‚åœ¨å®ç°å±‚é¢ï¼ŒPPOçš„å†…éƒ¨è®¡ç®—æ¶‰åŠä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼š1) å¸¦æœ‰å­¦ä¹ ä¼˜åŠ¿çš„æ ‡å‡†ç­–ç•¥æ¢¯åº¦ï¼›2)
    åŸºäºæœ€å¤§æ­¥é•¿çš„è£å‰ªç­–ç•¥æ¢¯åº¦ã€‚
- en: 'To understand how different situations emerge, we can define the policy ratio
    as:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç†è§£ä¸åŒæƒ…å†µçš„å‡ºç°ï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰ç­–ç•¥æ¯”ç‡ä¸ºï¼š
- en: <semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="prefix">|</mo><mi>s</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>Ï€</mi><msub><mi>Î¸</mi><mtext
    mathvariant="normal">old</mtext></msub></msub><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo
    stretchy="false" form="prefix">|</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>57</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">R(\theta) = \frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)}\qquad{(57)}</annotation></semantics>
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="prefix">|</mo><mi>s</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>Ï€</mi><msub><mi>Î¸</mi><mtext
    mathvariant="normal">old</mtext></msub></msub><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo
    stretchy="false" form="prefix">|</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>57</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">R(\theta) = \frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)}\qquad{(57)}</annotation></semantics>
- en: The policy ratio is a centerpiece of PPO and related algorithms. It emerges
    from computing the gradient of a policy and controls the parameter updates in
    a very intuitive way. For any batch of data, the policy ratio starts at 1 for
    the first gradient step for that batch, since <semantics><msub><mi>Ï€</mi><mi>Î¸</mi></msub><annotation
    encoding="application/x-tex">\pi_{\theta}</annotation></semantics> is the same
    as <semantics><msub><mi>Ï€</mi><msub><mi>Î¸</mi><mtext mathvariant="normal">old</mtext></msub></msub><annotation
    encoding="application/x-tex">\pi_{\theta_{\text{old}}}</annotation></semantics>
    at this point. Then, in the next gradient step, the policy ratio will be above
    one if that gradient step increased the likelihood of certain tokens with an associated
    positive advantage, or less than one for the other case. A common practice is
    to take 1-4 gradient steps per batch with policy gradient algorithms before updating
    <semantics><msub><mi>Ï€</mi><msub><mi>Î¸</mi><mtext mathvariant="normal">old</mtext></msub></msub><annotation
    encoding="application/x-tex">\pi_{\theta_{\text{old}}}</annotation></semantics>.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ç­–ç•¥æ¯”ç‡æ˜¯PPOå’Œç›¸å…³ç®—æ³•çš„æ ¸å¿ƒã€‚å®ƒä»è®¡ç®—ç­–ç•¥çš„æ¢¯åº¦ä¸­äº§ç”Ÿï¼Œå¹¶ä»¥éå¸¸ç›´è§‚çš„æ–¹å¼æ§åˆ¶å‚æ•°æ›´æ–°ã€‚å¯¹äºä»»ä½•ä¸€æ‰¹æ•°æ®ï¼Œç­–ç•¥æ¯”ç‡åœ¨ç¬¬ä¸€æ‰¹æ¬¡çš„ç¬¬ä¸€ä¸ªæ¢¯åº¦æ­¥éª¤ä¸­ä»1å¼€å§‹ï¼Œå› ä¸ºåœ¨è¿™ä¸ªç‚¹ä¸Š<semantics><msub><mi>Ï€</mi><mi>Î¸</mi></msub><annotation
    encoding="application/x-tex">\pi_{\theta}</annotation></semantics>ä¸<semantics><msub><mi>Ï€</mi><msub><mi>Î¸</mi><mtext
    mathvariant="normal">old</mtext></msub></msub><annotation encoding="application/x-tex">\pi_{\theta_{\text{old}}}</annotation></semantics>ç›¸åŒã€‚ç„¶åï¼Œåœ¨ä¸‹ä¸€ä¸ªæ¢¯åº¦æ­¥éª¤ä¸­ï¼Œå¦‚æœè¯¥æ¢¯åº¦æ­¥éª¤å¢åŠ äº†ä¸ç›¸å…³æ­£ä¼˜åŠ¿çš„æŸäº›ä»¤ç‰Œçš„å¯èƒ½æ€§ï¼Œç­–ç•¥æ¯”ç‡å°†å¤§äº1ï¼›å¯¹äºå…¶ä»–æƒ…å†µï¼Œç­–ç•¥æ¯”ç‡å°†å°äº1ã€‚å¸¸è§çš„åšæ³•æ˜¯åœ¨æ›´æ–°<semantics><msub><mi>Ï€</mi><msub><mi>Î¸</mi><mtext
    mathvariant="normal">old</mtext></msub></msub><annotation encoding="application/x-tex">\pi_{\theta_{\text{old}}}</annotation></semantics>ä¹‹å‰ï¼Œä½¿ç”¨ç­–ç•¥æ¢¯åº¦ç®—æ³•å¯¹æ¯ä¸ªæ‰¹æ¬¡è¿›è¡Œ1-4ä¸ªæ¢¯åº¦æ­¥éª¤ã€‚
- en: Understanding the PPO Objective
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç†è§£PPOç›®æ ‡
- en: Overall, the PPO objective can be visualized by two lines of a plot of objective
    versus policy ratio, which is shown in fig.Â [15](#fig:ppo-obj). The PPO objective
    is maximized by changing the probability of the sampled actions. Numerically,
    the objective controls for both positive and negative advantage cases by clever
    use of the minimum operation, making it so the update is at most pushed by an
    epsilon distance away from a policy ratio of 1.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä½“è€Œè¨€ï¼ŒPPOç›®æ ‡å‡½æ•°å¯ä»¥é€šè¿‡ç›®æ ‡å‡½æ•°ä¸ç­–ç•¥æ¯”ç‡å›¾çš„ä¸¤æ¡çº¿æ¥å¯è§†åŒ–ï¼Œå¦‚å›¾[15](#fig:ppo-obj)æ‰€ç¤ºã€‚é€šè¿‡æ”¹å˜é‡‡æ ·åŠ¨ä½œçš„æ¦‚ç‡ï¼ŒPPOç›®æ ‡å‡½æ•°é€šè¿‡æœ€å¤§åŒ–æ¥æ§åˆ¶æ­£ä¼˜åŠ¿å’Œè´Ÿä¼˜åŠ¿æƒ…å†µã€‚åœ¨æ•°å€¼ä¸Šï¼Œé€šè¿‡å·§å¦™åœ°ä½¿ç”¨æœ€å°æ“ä½œï¼Œç›®æ ‡å‡½æ•°ç¡®ä¿æ›´æ–°æœ€å¤šè¢«æ¨ç¦»ç­–ç•¥æ¯”ç‡1çš„Îµè·ç¦»ã€‚
- en: Within the trust region, PPO operates the same as other policy gradient algorithms.
    This is by design! The trust region is a concept used to cap the maximum step
    size of PPO and its peer algorithms for stability of updates. The core of the
    PPO algorithm, the clip and min/max functions, is to define this region. The objective
    becomes flat outside of it.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¿¡ä»»åŒºåŸŸå†…ï¼ŒPPOçš„æ“ä½œä¸å…¶ä»–ç­–ç•¥æ¢¯åº¦ç®—æ³•ç›¸åŒã€‚è¿™æ˜¯è®¾è®¡ä¸Šçš„è¦æ±‚ï¼ä¿¡ä»»åŒºåŸŸæ˜¯ä¸€ä¸ªç”¨äºé™åˆ¶PPOåŠå…¶åŒç±»ç®—æ³•æœ€å¤§æ­¥é•¿ä»¥ä¿æŒæ›´æ–°ç¨³å®šæ€§çš„æ¦‚å¿µã€‚PPOç®—æ³•çš„æ ¸å¿ƒï¼Œå³clipå’Œmin/maxå‡½æ•°ï¼Œæ˜¯ç”¨æ¥å®šä¹‰è¿™ä¸ªåŒºåŸŸçš„ã€‚ç›®æ ‡å‡½æ•°åœ¨å…¶å¤–éƒ¨å˜å¾—å¹³å¦ã€‚
- en: The idea of a â€œtrust regionâ€ comes from the numerical optimization literature
    [[186]](ch021.xhtml#ref-nocedal2006numerical), but was popularized within Deep
    RL from the algorithm Trust Region Policy Optimization (TRPO), which is accepted
    as the predecessor to PPO [[187]](ch021.xhtml#ref-schulman2015trust). The trust
    region is the area where the full policy-gradient steps are applied, as the updates
    are not â€œclippedâ€ by the max/min operations of the PPO objective.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: â€œä¿¡ä»»åŒºåŸŸâ€è¿™ä¸€æ¦‚å¿µæºè‡ªæ•°å€¼ä¼˜åŒ–æ–‡çŒ®[[186]](ch021.xhtml#ref-nocedal2006numerical)ï¼Œä½†å®ƒåœ¨æ·±åº¦å¼ºåŒ–å­¦ä¹ é¢†åŸŸè¢«å¹¿æ³›æ¨å¹¿ï¼Œå§‹äºä¿¡ä»»åŒºåŸŸç­–ç•¥ä¼˜åŒ–ï¼ˆTRPOï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•è¢«è§†ä¸ºPPO
    [[187]](ch021.xhtml#ref-schulman2015trust)çš„å‰èº«ã€‚ä¿¡ä»»åŒºåŸŸæ˜¯åº”ç”¨å®Œæ•´ç­–ç•¥æ¢¯åº¦æ­¥éª¤çš„åŒºåŸŸï¼Œå› ä¸ºæ›´æ–°ä¸ä¼šè¢«PPOç›®æ ‡å‡½æ•°çš„æœ€å¤§/æœ€å°æ“ä½œâ€œå‰ªè¾‘â€ã€‚
- en: '![Figure 15: Visualization of the different regions of the PPO objective for
    a hypothetical advantage. The â€œtrust regionâ€ would be described as the region
    where the log-ratio is within 1\pm\varepsilon.](../media/file13.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾15ï¼šå‡è®¾ä¼˜åŠ¿çš„PPOç›®æ ‡å‡½æ•°ä¸åŒåŒºåŸŸçš„å¯è§†åŒ–ã€‚ä¿¡ä»»åŒºåŸŸå¯ä»¥æè¿°ä¸ºå¯¹æ•°æ¯”ç‡åœ¨1Â±ÎµèŒƒå›´å†…çš„åŒºåŸŸã€‚](../media/file13.png)'
- en: 'Figure 15: Visualization of the different regions of the PPO objective for
    a hypothetical advantage. The â€œtrust regionâ€ would be described as the region
    where the log-ratio is within <semantics><mrow><mn>1</mn><mo>Â±</mo><mi>Îµ</mi></mrow><annotation
    encoding="application/x-tex">1\pm\varepsilon</annotation></semantics>.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾15ï¼šå‡è®¾ä¼˜åŠ¿çš„PPOç›®æ ‡å‡½æ•°ä¸åŒåŒºåŸŸçš„å¯è§†åŒ–ã€‚ä¿¡ä»»åŒºåŸŸå¯ä»¥æè¿°ä¸ºå¯¹æ•°æ¯”ç‡åœ¨<semantics><mrow><mn>1</mn><mo>Â±</mo><mi>Îµ</mi></mrow><annotation
    encoding="application/x-tex">1\pm\varepsilon</annotation></semantics>èŒƒå›´å†…çš„åŒºåŸŸã€‚
- en: 'The policy ratio and advantage together can occur in a few different configurations.
    We will split the cases into two groups: positive and negative advantage.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: æ”¿ç­–æ¯”ç‡å’Œä¼˜åŠ¿å¯ä»¥ä»¥å‡ ç§ä¸åŒçš„é…ç½®åŒæ—¶å‡ºç°ã€‚æˆ‘ä»¬å°†æ¡ˆä¾‹åˆ†ä¸ºä¸¤ç»„ï¼šæ­£ä¼˜åŠ¿å’Œè´Ÿä¼˜åŠ¿ã€‚
- en: '**Positive Advantage (<semantics><mrow><msub><mi>A</mi><mi>t</mi></msub><mo>></mo><mn>0</mn></mrow><annotation
    encoding="application/x-tex">A_t > 0</annotation></semantics>)**'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­£ä¼˜åŠ¿ (<semantics><mrow><msub><mi>A</mi><mi>t</mi></msub><mo>></mo><mn>0</mn></mrow><annotation
    encoding="application/x-tex">A_t > 0</annotation></semantics>)**'
- en: 'This means that the action taken was beneficial according to the value function,
    and we want to increase the likelihood of taking that action in the future. Now,
    letâ€™s look at different cases for the policy ratio <semantics><mrow><mi>R</mi><mo
    stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">R(\theta)</annotation></semantics>:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€é‡‡å–çš„è¡ŒåŠ¨æ ¹æ®ä»·å€¼å‡½æ•°æ˜¯æœ‰ç›Šçš„ï¼Œæˆ‘ä»¬å¸Œæœ›å¢åŠ åœ¨æœªæ¥é‡‡å–è¯¥è¡ŒåŠ¨çš„å¯èƒ½æ€§ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹ç­–ç•¥æ¯”ç‡<semantics><mrow><mi>R</mi><mo
    stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">R(\theta)</annotation></semantics>çš„ä¸åŒæƒ…å†µï¼š
- en: '<semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><mo><</mo><mn>1</mn><mo>âˆ’</mo><mi>Îµ</mi></mrow><annotation
    encoding="application/x-tex">R(\theta) < 1 - \varepsilon</annotation></semantics>:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '<semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><mo><</mo><mn>1</mn><mo>âˆ’</mo><mi>Îµ</mi></mrow><annotation
    encoding="application/x-tex">R(\theta) < 1 - \varepsilon</annotation></semantics>:'
- en: '**Interpretation**: Action is less likely with the new policy than the old
    policy'
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è§£é‡Š**ï¼šä¸æ—§æ”¿ç­–ç›¸æ¯”ï¼Œæ–°æ”¿ç­–é‡‡å–è¡ŒåŠ¨çš„å¯èƒ½æ€§æ›´ä½ã€‚'
- en: '**Unclipped Term**: <semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">R(\theta) A_t</annotation></semantics>'
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æœªè£å‰ªé¡¹**ï¼š<semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">R(\theta) A_t</annotation></semantics>'
- en: '**Clipped Term**: <semantics><mrow><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>âˆ’</mo><mi>Îµ</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">(1 - \varepsilon) A_t</annotation></semantics>'
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è£å‰ªé¡¹**ï¼š<semantics><mrow><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>âˆ’</mo><mi>Îµ</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">(1 - \varepsilon) A_t</annotation></semantics>'
- en: '**Objective**: <semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">R(\theta) A_t</annotation></semantics>'
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç›®æ ‡**ï¼š<semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">R(\theta) A_t</annotation></semantics>'
- en: '**Gradient**: <semantics><mrow><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi>R</mi><mo
    stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub><mo>â‰ </mo><mn>0</mn></mrow><annotation
    encoding="application/x-tex">\nabla_\theta R(\theta) A_t \neq 0</annotation></semantics>'
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ¢¯åº¦**ï¼š<semantics><mrow><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi>R</mi><mo stretchy="false"
    form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub><mo>â‰ </mo><mn>0</mn></mrow><annotation
    encoding="application/x-tex">\nabla_\theta R(\theta) A_t \neq 0</annotation></semantics>'
- en: '**What happens**: Normal policy-gradient update - increase likelihood of action'
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å‘ç”Ÿçš„æƒ…å†µ**ï¼šæ­£å¸¸çš„ç­–ç•¥æ¢¯åº¦æ›´æ–° - å¢åŠ è¡ŒåŠ¨çš„å¯èƒ½æ€§'
- en: '<semantics><mrow><mn>1</mn><mo>âˆ’</mo><mi>Îµ</mi><mo>â‰¤</mo><mi>R</mi><mo stretchy="false"
    form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>â‰¤</mo><mn>1</mn><mo>+</mo><mi>Îµ</mi></mrow><annotation
    encoding="application/x-tex">1 - \varepsilon \leq R(\theta) \leq 1 + \varepsilon</annotation></semantics>:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '<semantics><mrow><mn>1</mn><mo>âˆ’</mo><mi>Îµ</mi><mo>â‰¤</mo><mi>R</mi><mo stretchy="false"
    form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>â‰¤</mo><mn>1</mn><mo>+</mo><mi>Îµ</mi></mrow><annotation
    encoding="application/x-tex">1 - \varepsilon \leq R(\theta) \leq 1 + \varepsilon</annotation></semantics>:'
- en: '**Interpretation**: Action is almost equally likely with the new policy as
    the old policy'
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è§£é‡Š**ï¼šåœ¨æ–°çš„æ”¿ç­–å’Œæ—§çš„æ”¿ç­–ä¸‹ï¼Œè¡ŒåŠ¨çš„å¯èƒ½æ€§å‡ ä¹ç›¸ç­‰'
- en: '**Unclipped Term**: <semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">R(\theta) A_t</annotation></semantics>'
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æœªè£å‰ªé¡¹**ï¼š<semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">R(\theta) A_t</annotation></semantics>'
- en: '**Clipped Term**: <semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">R(\theta) A_t</annotation></semantics>'
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è£å‰ªé¡¹**ï¼š<semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">R(\theta) A_t</annotation></semantics>'
- en: '**Objective**: <semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">R(\theta) A_t</annotation></semantics>'
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç›®æ ‡**ï¼š<semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">R(\theta) A_t</annotation></semantics>'
- en: '**Gradient**: <semantics><mrow><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi>R</mi><mo
    stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub><mo>â‰ </mo><mn>0</mn></mrow><annotation
    encoding="application/x-tex">\nabla_\theta R(\theta) A_t \neq 0</annotation></semantics>'
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ¢¯åº¦**ï¼š<semantics><mrow><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi>R</mi><mo stretchy="false"
    form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub><mo>â‰ </mo><mn>0</mn></mrow><annotation
    encoding="application/x-tex">\nabla_\theta R(\theta) A_t \neq 0</annotation></semantics>'
- en: '**What happens**: Normal policy-gradient update - increase likelihood of action'
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å‘ç”Ÿçš„æƒ…å†µ**ï¼šæ­£å¸¸çš„ç­–ç•¥æ¢¯åº¦æ›´æ–° - å¢åŠ è¡ŒåŠ¨çš„å¯èƒ½æ€§'
- en: '<semantics><mrow><mn>1</mn><mo>+</mo><mi>Îµ</mi><mo><</mo><mi>R</mi><mo stretchy="false"
    form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">1 + \varepsilon < R(\theta)</annotation></semantics>:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '<semantics><mrow><mn>1</mn><mo>+</mo><mi>Îµ</mi><mo><</mo><mi>R</mi><mo stretchy="false"
    form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">1 + \varepsilon < R(\theta)</annotation></semantics>:'
- en: '**Interpretation**: Action is more likely with the new policy than the old
    policy'
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è§£é‡Š**: ä¸æ—§ç­–ç•¥ç›¸æ¯”ï¼Œæ–°ç­–ç•¥ä¸‹åŠ¨ä½œçš„æ¦‚ç‡æ›´é«˜'
- en: '**Unclipped Term**: <semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">R(\theta) A_t</annotation></semantics>'
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æœªå‰ªè£é¡¹**: <semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">R(\theta) A_t</annotation></semantics>'
- en: '**Clipped Term**: <semantics><mrow><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>+</mo><mi>Îµ</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">(1 + \varepsilon) A_t</annotation></semantics>'
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å‰ªè£é¡¹**: <semantics><mrow><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>+</mo><mi>Îµ</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">(1 + \varepsilon) A_t</annotation></semantics>'
- en: '**Objective**: <semantics><mrow><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>+</mo><mi>Îµ</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">(1 + \varepsilon) A_t</annotation></semantics>'
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç›®æ ‡**: <semantics><mrow><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>+</mo><mi>Îµ</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">(1 + \varepsilon) A_t</annotation></semantics>'
- en: '**Gradient**: <semantics><mrow><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><mn>1</mn><mo>+</mo><mi>Îµ</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mn>0</mn></mrow><annotation
    encoding="application/x-tex">\nabla_\theta (1 + \varepsilon) A_t = 0</annotation></semantics>'
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ¢¯åº¦**: <semantics><mrow><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><mn>1</mn><mo>+</mo><mi>Îµ</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mn>0</mn></mrow><annotation
    encoding="application/x-tex">\nabla_\theta (1 + \varepsilon) A_t = 0</annotation></semantics>'
- en: '**What happens**: NO UPDATE - action is already more likely under the new policy'
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å‘ç”Ÿæƒ…å†µ**: æ— æ›´æ–° - åœ¨æ–°ç­–ç•¥ä¸‹åŠ¨ä½œçš„æ¦‚ç‡å·²ç»æ›´é«˜'
- en: 'To summarize, when the advantage is positive (<semantics><mrow><msub><mi>A</mi><mi>t</mi></msub><mo>></mo><mn>0</mn></mrow><annotation
    encoding="application/x-tex">A_t>0</annotation></semantics>), we want to boost
    the probability of the action. Therefore:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“æ¥è¯´ï¼Œå½“ä¼˜åŠ¿ä¸ºæ­£ï¼ˆ<semantics><mrow><msub><mi>A</mi><mi>t</mi></msub><mo>></mo><mn>0</mn></mrow><annotation
    encoding="application/x-tex">A_t>0</annotation></semantics>ï¼‰æ—¶ï¼Œæˆ‘ä»¬å¸Œæœ›æé«˜åŠ¨ä½œçš„æ¦‚ç‡ã€‚å› æ­¤ï¼š
- en: We perform gradient steps only in the case when <semantics><mrow><msub><mi>Ï€</mi><mtext
    mathvariant="normal">new</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo
    stretchy="false" form="postfix">)</mo><mo>â‰¤</mo><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>+</mo><mi>Îµ</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>Ï€</mi><mtext mathvariant="normal">old</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">\pi_{\text{new}}(a) \leq (1+\varepsilon) \pi_{\text{old}}(a)</annotation></semantics>.
    Intuitively, we want to boost the probability of the action, since the advantage
    was positive, but not boost it so much that we have made it substantially more
    likely.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åªåœ¨ä»¥ä¸‹æƒ…å†µä¸‹æ‰§è¡Œæ¢¯åº¦æ­¥éª¤ï¼š<semantics><mrow><msub><mi>Ï€</mi><mtext mathvariant="normal">new</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo><mo>â‰¤</mo><mo
    stretchy="false" form="prefix">(</mo><mn>1</mn><mo>+</mo><mi>Îµ</mi><mo stretchy="false"
    form="postfix">)</mo><msub><mi>Ï€</mi><mtext mathvariant="normal">old</mtext></msub><mo
    stretchy="false" form="postfix">(</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">\pi_{\text{new}}(a) \leq (1+\varepsilon) \pi_{\text{old}}(a)</annotation></semantics>.
    ç›´è§‚ä¸Šï¼Œæˆ‘ä»¬å¸Œæœ›æé«˜åŠ¨ä½œçš„æ¦‚ç‡ï¼Œå› ä¸ºä¼˜åŠ¿æ˜¯æ­£çš„ï¼Œä½†æˆ‘ä»¬ä¸å¸Œæœ›æé«˜å¾—å¤ªå¤šï¼Œä»¥è‡³äºæˆ‘ä»¬ä½¿å®ƒå˜å¾—å®è´¨ä¸Šæ›´å¯èƒ½ã€‚
- en: Crucially, when <semantics><mrow><msub><mi>Ï€</mi><mtext mathvariant="normal">new</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo><mo>></mo><mo
    stretchy="false" form="prefix">(</mo><mn>1</mn><mo>+</mo><mi>Îµ</mi><mo stretchy="false"
    form="postfix">)</mo><msub><mi>Ï€</mi><mtext mathvariant="normal">old</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">\pi_{\text{new}}(a) > (1+\varepsilon) \pi_{\text{old}}(a)</annotation></semantics>,
    then we donâ€™t perform any update, and the gradient of the clipped objective is
    <semantics><mn>0</mn><annotation encoding="application/x-tex">0</annotation></semantics>.
    Intuitively, the action is already more expressed with the new policy, so we donâ€™t
    want to over-reinforce it.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å…³é”®çš„æ˜¯ï¼Œå½“<semantics><mrow><msub><mi>Ï€</mi><mtext mathvariant="normal">new</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo><mo>></mo><mo
    stretchy="false" form="prefix">(</mo><mn>1</mn><mo>+</mo><mi>Îµ</mi><mo stretchy="false"
    form="postfix">)</mo><msub><mi>Ï€</mi><mtext mathvariant="normal">old</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">\pi_{\text{new}}(a) > (1+\varepsilon) \pi_{\text{old}}(a)</annotation></semantics>ï¼Œåˆ™æˆ‘ä»¬ä¸æ‰§è¡Œä»»ä½•æ›´æ–°ï¼Œå‰ªè£ç›®æ ‡å‡½æ•°çš„æ¢¯åº¦æ˜¯<semantics><mn>0</mn><annotation
    encoding="application/x-tex">0</annotation></semantics>ã€‚ç›´è§‚ä¸Šï¼Œæ–°ç­–ç•¥å·²ç»æ›´å……åˆ†åœ°è¡¨è¾¾äº†è¯¥è¡ŒåŠ¨ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸å¸Œæœ›è¿‡åº¦å¼ºåŒ–å®ƒã€‚
- en: '**Negative Advantage (<semantics><mrow><msub><mi>A</mi><mi>t</mi></msub><mo><</mo><mn>0</mn></mrow><annotation
    encoding="application/x-tex">A_t < 0</annotation></semantics>)**'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**è´Ÿä¼˜åŠ¿(<semantics><mrow><msub><mi>A</mi><mi>t</mi></msub><mo><</mo><mn>0</mn></mrow><annotation
    encoding="application/x-tex">A_t < 0</annotation></semantics>)**'
- en: 'This means that the action taken was detrimental according to the value function,
    and we want to decrease the likelihood of taking that action in the future. Now,
    letâ€™s look at different cases for the policy ratio <semantics><mrow><mi>R</mi><mo
    stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">R(\theta)</annotation></semantics>:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€æ ¹æ®ä»·å€¼å‡½æ•°ï¼Œæ‰€é‡‡å–çš„è¡ŒåŠ¨æ˜¯æœ‰å®³çš„ï¼Œæˆ‘ä»¬å¸Œæœ›é™ä½æœªæ¥é‡‡å–è¯¥è¡ŒåŠ¨çš„å¯èƒ½æ€§ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹ç­–ç•¥æ¯”ç‡<semantics><mrow><mi>R</mi><mo
    stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">R(\theta)</annotation></semantics>çš„ä¸åŒæƒ…å†µï¼š
- en: '<semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><mo><</mo><mn>1</mn><mo>âˆ’</mo><mi>Îµ</mi></mrow><annotation
    encoding="application/x-tex">R(\theta) < 1 - \varepsilon</annotation></semantics>:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '<semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><mo><</mo><mn>1</mn><mo>âˆ’</mo><mi>Îµ</mi></mrow><annotation
    encoding="application/x-tex">R(\theta) < 1 - \varepsilon</annotation></semantics>:'
- en: '**Interpretation**: Action is less likely with the new policy than the old
    policy'
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è§£é‡Š**: æ–°ç­–ç•¥ä¸‹é‡‡å–è¡ŒåŠ¨çš„å¯èƒ½æ€§ä½äºæ—§ç­–ç•¥'
- en: '**Unclipped Term**: <semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">R(\theta) A_t</annotation></semantics>'
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æœªå‰ªè£é¡¹**: <semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">R(\theta) A_t</annotation></semantics>'
- en: '**Clipped Term**: <semantics><mrow><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>âˆ’</mo><mi>Îµ</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">(1 - \varepsilon) A_t</annotation></semantics>'
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å‰ªè£é¡¹**: <semantics><mrow><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>âˆ’</mo><mi>Îµ</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">(1 - \varepsilon) A_t</annotation></semantics>'
- en: '**Objective**: <semantics><mrow><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>âˆ’</mo><mi>Îµ</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">(1 - \varepsilon) A_t</annotation></semantics>'
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç›®æ ‡**: <semantics><mrow><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>âˆ’</mo><mi>Îµ</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">(1 - \varepsilon) A_t</annotation></semantics>'
- en: '**Gradient**: <semantics><mrow><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><mn>1</mn><mo>âˆ’</mo><mi>Îµ</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mn>0</mn></mrow><annotation
    encoding="application/x-tex">\nabla_\theta (1 - \varepsilon) A_t = 0</annotation></semantics>'
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ¢¯åº¦**: <semantics><mrow><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><mn>1</mn><mo>âˆ’</mo><mi>Îµ</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mn>0</mn></mrow><annotation
    encoding="application/x-tex">\nabla_\theta (1 - \varepsilon) A_t = 0</annotation></semantics>'
- en: '**What happens**: NO UPDATE - action is already less likely under the new policy'
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å‘ç”Ÿæƒ…å†µ**: æ— æ›´æ–° - åœ¨æ–°ç­–ç•¥ä¸‹åŠ¨ä½œçš„å¯èƒ½æ€§å·²ç»è¾ƒä½'
- en: '<semantics><mrow><mn>1</mn><mo>âˆ’</mo><mi>Îµ</mi><mo>â‰¤</mo><mi>R</mi><mo stretchy="false"
    form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>â‰¤</mo><mn>1</mn><mo>+</mo><mi>Îµ</mi></mrow><annotation
    encoding="application/x-tex">1 - \varepsilon \leq R(\theta) \leq 1 + \varepsilon</annotation></semantics>:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '<semantics><mrow><mn>1</mn><mo>âˆ’</mo><mi>Îµ</mi><mo>â‰¤</mo><mi>R</mi><mo stretchy="false"
    form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>â‰¤</mo><mn>1</mn><mo>+</mo><mi>Îµ</mi></mrow><annotation
    encoding="application/x-tex">1 - \varepsilon \leq R(\theta) \leq 1 + \varepsilon</annotation></semantics>:'
- en: '**Interpretation**: Action is almost equally likely with the new policy as
    the old policy'
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è§£é‡Š**: åœ¨æ–°ç­–ç•¥ä¸‹ï¼ŒåŠ¨ä½œçš„å¯èƒ½æ€§å‡ ä¹ä¸æ—§ç­–ç•¥ç›¸åŒ'
- en: '**Unclipped Term**: <semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">R(\theta) A_t</annotation></semantics>'
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æœªè£å‰ªé¡¹**: <semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">R(\theta) A_t</annotation></semantics>'
- en: '**Clipped Term**: <semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">R(\theta) A_t</annotation></semantics>'
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è£å‰ªé¡¹**: <semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">R(\theta) A_t</annotation></semantics>'
- en: '**Objective**: <semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">R(\theta) A_t</annotation></semantics>'
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç›®æ ‡**: <semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">R(\theta) A_t</annotation></semantics>'
- en: '**Gradient**: <semantics><mrow><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi>R</mi><mo
    stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub><mo>â‰ </mo><mn>0</mn></mrow><annotation
    encoding="application/x-tex">\nabla_\theta R(\theta) A_t \neq 0</annotation></semantics>'
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ¢¯åº¦**: <semantics><mrow><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi>R</mi><mo stretchy="false"
    form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub><mo>â‰ </mo><mn>0</mn></mrow><annotation
    encoding="application/x-tex">\nabla_\theta R(\theta) A_t \neq 0</annotation></semantics>'
- en: '**What happens**: Normal policy-gradient update - decrease likelihood of action'
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å‘ç”Ÿæƒ…å†µ**: æ­£å¸¸çš„ç­–ç•¥æ¢¯åº¦æ›´æ–° - å‡å°‘åŠ¨ä½œçš„å¯èƒ½æ€§'
- en: '<semantics><mrow><mn>1</mn><mo>+</mo><mi>Îµ</mi><mo><</mo><mi>R</mi><mo stretchy="false"
    form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">1 + \varepsilon < R(\theta)</annotation></semantics>:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '<semantics><mrow><mn>1</mn><mo>+</mo><mi>Îµ</mi><mo><</mo><mi>R</mi><mo stretchy="false"
    form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">1 + \varepsilon < R(\theta)</annotation></semantics>:'
- en: '**Interpretation**: Action is more likely with the new policy than the old
    policy'
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è§£é‡Š**: åœ¨æ–°ç­–ç•¥ä¸‹ï¼ŒåŠ¨ä½œçš„å¯èƒ½æ€§æ¯”æ—§ç­–ç•¥æ›´é«˜'
- en: '**Unclipped Term**: <semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">R(\theta) A_t</annotation></semantics>'
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æœªè£å‰ªé¡¹**: <semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">R(\theta) A_t</annotation></semantics>'
- en: '**Clipped Term**: <semantics><mrow><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>+</mo><mi>Îµ</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">(1 + \varepsilon) A_t</annotation></semantics>'
  id: totrans-148
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è£å‰ªé¡¹**: <semantics><mrow><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>+</mo><mi>Îµ</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">(1 + \varepsilon) A_t</annotation></semantics>'
- en: '**Objective**: <semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">R(\theta) A_t</annotation></semantics>'
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç›®æ ‡**: <semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">R(\theta) A_t</annotation></semantics>'
- en: '**Gradient**: <semantics><mrow><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi>R</mi><mo
    stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub><mo>â‰ </mo><mn>0</mn></mrow><annotation
    encoding="application/x-tex">\nabla_\theta R(\theta) A_t \neq 0</annotation></semantics>'
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ¢¯åº¦**: <semantics><mrow><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi>R</mi><mo stretchy="false"
    form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub><mo>â‰ </mo><mn>0</mn></mrow><annotation
    encoding="application/x-tex">\nabla_\theta R(\theta) A_t \neq 0</annotation></semantics>'
- en: '**What happens**: Normal policy-gradient update - decrease likelihood of action'
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å‘ç”Ÿçš„æƒ…å†µ**ï¼šæ­£å¸¸çš„ç­–ç•¥æ¢¯åº¦æ›´æ–° - é™ä½åŠ¨ä½œçš„å¯èƒ½æ€§'
- en: 'To summarize, when the advantage is negative (<semantics><mrow><msub><mi>A</mi><mi>t</mi></msub><mo><</mo><mn>0</mn></mrow><annotation
    encoding="application/x-tex">A_t < 0</annotation></semantics>), we want to decrease
    the probability of the action. Therefore:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“æ¥è¯´ï¼Œå½“ä¼˜åŠ¿ä¸ºè´Ÿï¼ˆ<semantics><mrow><msub><mi>A</mi><mi>t</mi></msub><mo><</mo><mn>0</mn></mrow><annotation
    encoding="application/x-tex">A_t < 0</annotation></semantics>ï¼‰æ—¶ï¼Œæˆ‘ä»¬å¸Œæœ›é™ä½åŠ¨ä½œçš„æ¦‚ç‡ã€‚å› æ­¤ï¼š
- en: We perform gradient steps only in the case when <semantics><mrow><msub><mi>Ï€</mi><mtext
    mathvariant="normal">new</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo
    stretchy="false" form="postfix">)</mo><mo>â‰¥</mo><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>âˆ’</mo><mi>Îµ</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>Ï€</mi><mtext mathvariant="normal">old</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">\pi_{\text{new}}(a) \geq (1-\varepsilon) \pi_{\text{old}}(a)</annotation></semantics>.
    Intuitively, we want to decrease the probability of the action, since the advantage
    was negative, and we do so proportional to the advantage.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åªåœ¨ä»¥ä¸‹æƒ…å†µä¸‹æ‰§è¡Œæ¢¯åº¦æ­¥éª¤ï¼š<semantics><mrow><msub><mi>Ï€</mi><mtext mathvariant="normal">new</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo><mo>â‰¥</mo><mo
    stretchy="false" form="prefix">(</mo><mn>1</mn><mo>âˆ’</mo><mi>Îµ</mi><mo stretchy="false"
    form="postfix">)</mo><msub><mi>Ï€</mi><mtext mathvariant="normal">old</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">\pi_{\text{new}}(a) \geq (1-\varepsilon) \pi_{\text{old}}(a)</annotation></semantics>ã€‚ç›´è§‚ä¸Šï¼Œæˆ‘ä»¬å¸Œæœ›é™ä½åŠ¨ä½œçš„æ¦‚ç‡ï¼Œå› ä¸ºä¼˜åŠ¿æ˜¯è´Ÿçš„ï¼Œæˆ‘ä»¬è¿™æ ·åšæ˜¯æŒ‰ä¼˜åŠ¿çš„æ¯”ä¾‹è¿›è¡Œçš„ã€‚
- en: Crucially, when <semantics><mrow><msub><mi>Ï€</mi><mtext mathvariant="normal">new</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo><mo><</mo><mo
    stretchy="false" form="prefix">(</mo><mn>1</mn><mo>âˆ’</mo><mi>Îµ</mi><mo stretchy="false"
    form="postfix">)</mo><msub><mi>Ï€</mi><mtext mathvariant="normal">old</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">\pi_{\text{new}}(a) < (1-\varepsilon) \pi_{\text{old}}(a)</annotation></semantics>,
    then we donâ€™t perform any update, and the gradient of the clipped objective is
    <semantics><mn>0</mn><annotation encoding="application/x-tex">0</annotation></semantics>.
    Intuitively, the action is already less likely under the new policy, so we donâ€™t
    want to over-suppress it.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å…³é”®çš„æ˜¯ï¼Œå½“ <semantics><mrow><msub><mi>Ï€</mi><mtext mathvariant="normal">new</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo><mo><</mo><mo
    stretchy="false" form="prefix">(</mo><mn>1</mn><mo>âˆ’</mo><mi>Îµ</mi><mo stretchy="false"
    form="postfix">)</mo><msub><mi>Ï€</mi><mtext mathvariant="normal">old</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">\pi_{\text{new}}(a) < (1-\varepsilon) \pi_{\text{old}}(a)</annotation></semantics>ï¼Œåˆ™æˆ‘ä»¬ä¸æ‰§è¡Œä»»ä½•æ›´æ–°ï¼Œå‰ªè£ç›®æ ‡å‡½æ•°çš„æ¢¯åº¦ä¸º
    <semantics><mn>0</mn><annotation encoding="application/x-tex">0</annotation></semantics>ã€‚ç›´è§‚ä¸Šï¼Œåœ¨æ–°çš„ç­–ç•¥ä¸‹ï¼ŒåŠ¨ä½œçš„å¯èƒ½æ€§å·²ç»é™ä½ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸æƒ³è¿‡åº¦æŠ‘åˆ¶å®ƒã€‚
- en: It is crucial to remember that PPO within the trust region is roughly the same
    as standard forms of policy gradient.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: è®°ä½ï¼ŒPPOåœ¨ä¿¡ä»»åŒºåŸŸå†…å¤§è‡´ç­‰åŒäºæ ‡å‡†çš„ç­–ç•¥æ¢¯åº¦å½¢å¼ã€‚
- en: Value Functions and PPO
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ä»·å€¼å‡½æ•°å’ŒPPO
- en: The value function within PPO is an additional copy of the model that is used
    to predict the value per token. The value of a token (or state) in traditional
    RL is predicting the future return from that moment, often with discounting. This
    value in PPO is used as a learned baseline, representing an evolution of the simple
    Monte Carlo version used with REINFORCE (which doesnâ€™t need the learned value
    network). This highlights how PPO is an evolution of REINFORCE and vanilla policy-gradient
    in multiple forms, across the optimization form, baseline, etc. In practice, with
    PPO and other algorithms used for language models, this is predicting the return
    of each token after the deduction of KL penalties (the per-token loss includes
    the KL from the reward traditionally, as discussed).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨PPOä¸­ï¼Œå€¼å‡½æ•°æ˜¯æ¨¡å‹çš„ä¸€ä¸ªé¢å¤–å‰¯æœ¬ï¼Œç”¨äºé¢„æµ‹æ¯ä¸ªæ ‡è®°çš„ä»·å€¼ã€‚åœ¨ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæ ‡è®°ï¼ˆæˆ–çŠ¶æ€ï¼‰çš„ä»·å€¼æ˜¯é¢„æµ‹ä»é‚£ä¸€åˆ»èµ·æœªæ¥çš„å›æŠ¥ï¼Œé€šå¸¸å¸¦æœ‰æŠ˜æ‰£ã€‚åœ¨PPOä¸­ï¼Œè¿™ä¸ªä»·å€¼è¢«ç”¨ä½œä¸€ä¸ªå­¦ä¹ åˆ°çš„åŸºçº¿ï¼Œä»£è¡¨ä¸REINFORCEï¼ˆä¸éœ€è¦å­¦ä¹ åˆ°çš„å€¼ç½‘ç»œï¼‰ä¸€èµ·ä½¿ç”¨çš„ç®€å•è’™ç‰¹å¡æ´›ç‰ˆæœ¬çš„å‘å±•ã€‚è¿™çªå‡ºäº†PPOæ˜¯å¦‚ä½•åœ¨ä¼˜åŒ–å½¢å¼ã€åŸºçº¿ç­‰å¤šä¸ªæ–¹é¢æ˜¯REINFORCEå’Œvanillaç­–ç•¥æ¢¯åº¦çš„æ¼”åŒ–çš„ã€‚åœ¨å®è·µä¸­ï¼Œä½¿ç”¨PPOå’Œå…¶ä»–ç”¨äºè¯­è¨€æ¨¡å‹çš„ç®—æ³•æ—¶ï¼Œè¿™æ˜¯åœ¨æ‰£é™¤KLæƒ©ç½šï¼ˆæ¯ä¸ªæ ‡è®°çš„æŸå¤±åŒ…æ‹¬ä¼ ç»Ÿä¸Šä»å¥–åŠ±ä¸­æ¥çš„KLï¼Œå¦‚å‰æ‰€è¿°ï¼‰åé¢„æµ‹æ¯ä¸ªæ ‡è®°çš„å›æŠ¥ã€‚
- en: There are a few different methods (or targets) used to learn the value functions.
    Generalized Advantage Estimation (GAE) is considered the state-of-the-art and
    canonical implementation in modern systems, but it carries more complexity by
    computing the value prediction error over multiple steps â€“ see the later section
    on GAE in this chapter. A value function can also be learned with Monte Carlo
    estimates from the rollouts used to update the policy. PPO has two losses â€“ one
    to learn the value function and another to use that value function to update the
    policy.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å‡ ç§ä¸åŒçš„æ–¹æ³•ï¼ˆæˆ–ç›®æ ‡ï¼‰ç”¨äºå­¦ä¹ å€¼å‡½æ•°ã€‚å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼ˆGAEï¼‰è¢«è®¤ä¸ºæ˜¯ç°ä»£ç³»ç»Ÿä¸­çš„æœ€å…ˆè¿›å’Œæ ‡å‡†å®ç°ï¼Œä½†å®ƒé€šè¿‡åœ¨å¤šä¸ªæ­¥éª¤ä¸­è®¡ç®—å€¼é¢„æµ‹è¯¯å·®è€Œå¢åŠ äº†å¤æ‚æ€§â€”â€”è¯·å‚é˜…æœ¬ç« åé¢çš„GAEéƒ¨åˆ†ã€‚å€¼å‡½æ•°ä¹Ÿå¯ä»¥é€šè¿‡ä»ç”¨äºæ›´æ–°ç­–ç•¥çš„rolloutsä¸­ä½¿ç”¨çš„è’™ç‰¹å¡æ´›ä¼°è®¡æ¥å­¦ä¹ ã€‚PPOæœ‰ä¸¤ä¸ªæŸå¤±â€”â€”ä¸€ä¸ªç”¨äºå­¦ä¹ å€¼å‡½æ•°ï¼Œå¦ä¸€ä¸ªç”¨äºä½¿ç”¨è¯¥å€¼å‡½æ•°æ¥æ›´æ–°ç­–ç•¥ã€‚
- en: A simple example implementation of a value network loss is shown below.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢å±•ç¤ºäº†å€¼ç½‘ç»œæŸå¤±çš„ç®€å•ç¤ºä¾‹å®ç°ã€‚
- en: '[PRE0]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Group Relative Policy Optimization
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–
- en: 'Group Relative Policy Optimization (GRPO) is introduced in DeepSeekMath [[188]](ch021.xhtml#ref-shao2024deepseekmath),
    and used in other DeepSeek works, e.g.Â DeepSeek-V3 [[189]](ch021.xhtml#ref-liu2024deepseek)
    and DeepSeek-R1 [[61]](ch021.xhtml#ref-guo2025deepseek). GRPO can be viewed as
    a PPO-inspired algorithm with a very similar surrogate loss, but it avoids learning
    a value function with another copy of the original policy language model (or another
    checkpoint for initialization). This brings two posited benefits:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰åœ¨DeepSeekMath [[188]](ch021.xhtml#ref-shao2024deepseekmath)ä¸­è¢«å¼•å…¥ï¼Œå¹¶åœ¨å…¶ä»–DeepSeekä½œå“ä¸­ä½¿ç”¨ï¼Œä¾‹å¦‚DeepSeek-V3
    [[189]](ch021.xhtml#ref-liu2024deepseek)å’ŒDeepSeek-R1 [[61]](ch021.xhtml#ref-guo2025deepseek)ã€‚GRPOå¯ä»¥è¢«è§†ä¸ºä¸€ä¸ªå—PPOå¯å‘çš„ç®—æ³•ï¼Œå…·æœ‰éå¸¸ç›¸ä¼¼çš„ä»£ç†æŸå¤±ï¼Œä½†å®ƒé¿å…äº†ä½¿ç”¨åŸå§‹ç­–ç•¥è¯­è¨€æ¨¡å‹ï¼ˆæˆ–å¦ä¸€ä¸ªç”¨äºåˆå§‹åŒ–çš„æ£€æŸ¥ç‚¹ï¼‰çš„å¦ä¸€ä¸ªå‰¯æœ¬æ¥å­¦ä¹ å€¼å‡½æ•°ã€‚è¿™å¸¦æ¥äº†ä¸¤ä¸ªå‡è®¾çš„å¥½å¤„ï¼š
- en: Avoiding the challenge of learning a value function from a LM backbone, where
    research hasnâ€™t established best practices.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é¿å…ä»LMéª¨å¹²ä¸­å­¦ä¹ å€¼å‡½æ•°çš„æŒ‘æˆ˜ï¼Œåœ¨æ­¤é¢†åŸŸç ”ç©¶å°šæœªç¡®ç«‹æœ€ä½³å®è·µã€‚
- en: Saves memory by not needing to keep the extra set of model weights in memory
    (going from needing the current policy, the reference policy, and a value function,
    to just the first two copies).
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€šè¿‡ä¸éœ€è¦åœ¨å†…å­˜ä¸­ä¿ç•™é¢å¤–çš„æ¨¡å‹æƒé‡é›†æ¥èŠ‚çœå†…å­˜ï¼ˆä»éœ€è¦å½“å‰ç­–ç•¥ã€å‚è€ƒç­–ç•¥å’Œå€¼å‡½æ•°ï¼Œåˆ°åªéœ€å‰ä¸¤ä¸ªå‰¯æœ¬ï¼‰ã€‚
- en: GRPO does this by simplifying the value estimation and assigning the same value
    to every token in the episode (i.e.Â in the completion to a prompt, each token
    gets assigned the same value rather than discounted rewards in a standard value
    function) by estimating the advantage or baseline. The estimate is done by collecting
    multiple completions (<semantics><msub><mi>a</mi><mi>i</mi></msub><annotation
    encoding="application/x-tex">a_i</annotation></semantics>) and rewards (<semantics><msub><mi>r</mi><mi>i</mi></msub><annotation
    encoding="application/x-tex">r_i</annotation></semantics>), i.e.Â a Monte Carlo
    estimate, from the same initial state / prompt (<semantics><mi>s</mi><annotation
    encoding="application/x-tex">s</annotation></semantics>).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: GRPOé€šè¿‡ç®€åŒ–ä»·å€¼ä¼°è®¡å¹¶å°†åŒä¸€ä»·å€¼åˆ†é…ç»™åœºæ™¯ä¸­çš„æ¯ä¸ªæ ‡è®°ï¼ˆå³ï¼Œåœ¨æç¤ºçš„å®Œæˆä¸­ï¼Œæ¯ä¸ªæ ‡è®°è¢«åˆ†é…ç›¸åŒçš„ä»·å€¼ï¼Œè€Œä¸æ˜¯æ ‡å‡†ä»·å€¼å‡½æ•°ä¸­çš„æŠ˜æ‰£å¥–åŠ±ï¼‰æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œé€šè¿‡ä¼°è®¡ä¼˜åŠ¿æˆ–åŸºçº¿ã€‚ä¼°è®¡æ˜¯é€šè¿‡æ”¶é›†æ¥è‡ªç›¸åŒåˆå§‹çŠ¶æ€/æç¤ºï¼ˆ<semantics><mi>s</mi><annotation
    encoding="application/x-tex">s</annotation></semantics>ï¼‰çš„å¤šä¸ªå®Œæˆï¼ˆ<semantics><msub><mi>a</mi><mi>i</mi></msub><annotation
    encoding="application/x-tex">a_i</annotation></semantics>ï¼‰å’Œå¥–åŠ±ï¼ˆ<semantics><msub><mi>r</mi><mi>i</mi></msub><annotation
    encoding="application/x-tex">r_i</annotation></semantics>ï¼‰ï¼Œå³è’™ç‰¹å¡æ´›ä¼°è®¡æ¥å®Œæˆçš„ã€‚
- en: 'To state this formally, the GRPO objective is very similar to the PPO objective
    above. For GRPO, the objective (or loss) is accumulated over a group of completions
    <semantics><mrow><mo stretchy="false" form="prefix">{</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi><mo>,</mo><msub><mi>a</mi><mi>G</mi></msub><mo
    stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\{a_1,
    a_2, ..., a_G\}</annotation></semantics> to a given prompt <semantics><mi>s</mi><annotation
    encoding="application/x-tex">s</annotation></semantics>. Here, we show the GRPO
    objective:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ­£å¼è¡¨è¿°ï¼ŒGRPOçš„ç›®æ ‡ä¸ä¸Šé¢æåˆ°çš„PPOç›®æ ‡éå¸¸ç›¸ä¼¼ã€‚å¯¹äºGRPOï¼Œç›®æ ‡ï¼ˆæˆ–æŸå¤±ï¼‰æ˜¯åœ¨ä¸€ç»„å®Œæˆ<semantics><mrow><mo stretchy="false"
    form="prefix">{</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi><mo>,</mo><msub><mi>a</mi><mi>G</mi></msub><mo
    stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\{a_1,
    a_2, ..., a_G\}</annotation></semantics>åˆ°ä¸€ä¸ªç»™å®šçš„æç¤º<semantics><mi>s</mi><annotation
    encoding="application/x-tex">s</annotation></semantics>ä¸Šç´¯ç§¯çš„ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å±•ç¤ºäº†GRPOçš„ç›®æ ‡ï¼š
- en: <semantics><mrow><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>G</mi></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>G</mi></munderover><mrow><mo
    stretchy="true" form="prefix">(</mo><mi mathvariant="normal">min</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mfrac><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy="false"
    form="prefix">|</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>Ï€</mi><msub><mi>Î¸</mi><mtext
    mathvariant="normal">old</mtext></msub></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo
    stretchy="false" form="prefix">|</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><msub><mi>A</mi><mi>i</mi></msub><mo>,</mo><mtext
    mathvariant="normal">clip</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy="false"
    form="prefix">|</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>Ï€</mi><msub><mi>Î¸</mi><mtext
    mathvariant="normal">old</mtext></msub></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo
    stretchy="false" form="prefix">|</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo>,</mo><mn>1</mn><mo>âˆ’</mo><mi>Îµ</mi><mo>,</mo><mn>1</mn><mo>+</mo><mi>Îµ</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><msub><mi>A</mi><mi>i</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>âˆ’</mo><mi>Î²</mi><msub><mi>ğ’Ÿ</mi><mtext
    mathvariant="normal">KL</mtext></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">|</mo><mo stretchy="false" form="prefix">|</mo><msub><mi>Ï€</mi><mtext
    mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="postfix">)</mo><mo
    stretchy="true" form="postfix">)</mo></mrow><mi>.</mi><mrow><mo stretchy="false"
    form="prefix">(</mo><mn>58</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">J(\theta) = \frac{1}{G}\sum_{i=1}^G \left(\min\left(\frac{\pi_\theta(a_i|s)}{\pi_{\theta_{\text{old}}}(a_i|s)}A_i,
    \text{clip} \left( \frac{\pi_\theta(a_i|s)}{\pi_{\theta_{\text{old}}}(a_i|s)},
    1-\varepsilon, 1+\varepsilon \right) A_i \right) - \beta \mathcal{D}_{\text{KL}}(\pi_\theta||\pi_{\text{ref}})\right).\qquad{(58)}</annotation></semantics>
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>G</mi></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>G</mi></munderover><mrow><mo
    stretchy="true" form="prefix">(</mo><mi mathvariant="normal">min</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mfrac><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy="false"
    form="prefix">|</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>Ï€</mi><msub><mi>Î¸</mi><mtext
    mathvariant="normal">old</mtext></msub></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo
    stretchy="false" form="prefix">|</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><msub><mi>A</mi><mi>i</mi></msub><mo>,</mo><mtext
    mathvariant="normal">clip</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy="false"
    form="prefix">|</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>Ï€</mi><msub><mi>Î¸</mi><mtext
    mathvariant="normal">old</mtext></msub></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo
    stretchy="false" form="prefix">|</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo>,</mo><mn>1</mn><mo>âˆ’</mo><mi>Îµ</mi><mo>,</mo><mn>1</mn><mo>+</mo><mi>Îµ</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><msub><mi>A</mi><mi>i</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>âˆ’</mo><mi>Î²</mi><msub><mi>ğ’Ÿ</mi><mtext
    mathvariant="normal">KL</mtext></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">|</mo><mo stretchy="false" form="prefix">|</mo><msub><mi>Ï€</mi><mtext
    mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="postfix">)</mo><mo
    stretchy="true" form="postfix">)</mo></mrow><mi>.</mi><mrow><mo stretchy="false"
    form="prefix">(</mo><mn>58</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">J(\theta) = \frac{1}{G}\sum_{i=1}^G \left(\min\left(\frac{\pi_\theta(a_i|s)}{\pi_{\theta_{\text{old}}}(a_i|s)}A_i,
    \text{clip} \left( \frac{\pi_\theta(a_i|s)}{\pi_{\theta_{\text{old}}}(a_i|s)},
    1-\varepsilon, 1+\varepsilon \right) A_i \right) - \beta \mathcal{D}_{\text{KL}}(\pi_\theta||\pi_{\text{ref}})\right).\qquad{(58)}</annotation></semantics>
- en: 'Note that relative to PPO, the standard implementation of GRPO includes the
    KL distance in the loss. As above, we can expand this into a per-token computation:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œä¸PPOç›¸æ¯”ï¼ŒGRPOçš„æ ‡å‡†å®ç°å°†KLè·ç¦»åŒ…å«åœ¨æŸå¤±ä¸­ã€‚å¦‚ä¸Šæ‰€è¿°ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶æ‰©å±•ä¸ºæ¯ä¸ªæ ‡è®°çš„è®¡ç®—ï¼š
- en: <semantics><mrow><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>G</mi></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>G</mi></munderover><mfrac><mn>1</mn><mrow><mo
    stretchy="false" form="prefix">|</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy="false"
    form="prefix">|</mo></mrow></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mo
    stretchy="false" form="prefix">|</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy="false"
    form="prefix">|</mo></mrow></munderover><mrow><mo stretchy="true" form="prefix">(</mo><mi
    mathvariant="normal">min</mi><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo
    stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>i</mi></msub><mo stretchy="false"
    form="postfix">)</mo></mrow><mrow><msub><mi>Ï€</mi><msub><mi>Î¸</mi><mtext mathvariant="normal">old</mtext></msub></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo
    stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>i</mi></msub><mo stretchy="false"
    form="postfix">)</mo></mrow></mfrac><msub><mi>A</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>,</mo><mtext
    mathvariant="normal">clip</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo
    stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>i</mi></msub><mo stretchy="false"
    form="postfix">)</mo></mrow><mrow><msub><mi>Ï€</mi><msub><mi>Î¸</mi><mtext mathvariant="normal">old</mtext></msub></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo
    stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>i</mi></msub><mo stretchy="false"
    form="postfix">)</mo></mrow></mfrac><mo>,</mo><mn>1</mn><mo>âˆ’</mo><mi>Îµ</mi><mo>,</mo><mn>1</mn><mo>+</mo><mi>Îµ</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><msub><mi>A</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>âˆ’</mo><mi>Î²</mi><msub><mi>ğ’Ÿ</mi><mtext
    mathvariant="normal">KL</mtext></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>â‹…</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>i</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="false" form="prefix">|</mo><mo
    stretchy="false" form="prefix">|</mo><msub><mi>Ï€</mi><mtext mathvariant="normal">ref</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>â‹…</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>i</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">)</mo><mo
    stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="false" form="prefix">(</mo><mn>59</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">J(\theta)
    = \frac{1}{G}\sum_{i=1}^G \frac{1}{|a_i|} \sum_{t=1}^{|a_i|} \left( \min\left(\frac{\pi_\theta(a_{i,t}|s_{i})}{\pi_{\theta_{\text{old}}}(a_{i,t}|s_{i})}A_{i,t},
    \text{clip} \left( \frac{\pi_\theta(a_{i,t}|s_{i})}{\pi_{\theta_{\text{old}}}(a_{i,t}|s_{i})},
    1-\varepsilon, 1+\varepsilon \right) A_{i,t} \right) - \beta \mathcal{D}_{\text{KL}}(\pi_\theta(\cdot|s_{i})||\pi_{\text{ref}}(\cdot|s_{i}))
    \right) \qquad{(59)}</annotation></semantics>
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'With the advantage computation for the completion index <semantics><mi>i</mi><annotation
    encoding="application/x-tex">i</annotation></semantics>:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå®Œæˆç´¢å¼•<semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics>çš„ä¼˜åŠ¿è®¡ç®—ï¼š
- en: <semantics><mrow><msub><mi>A</mi><mi>i</mi></msub><mo>=</mo><mfrac><mrow><msub><mi>r</mi><mi>i</mi></msub><mo>âˆ’</mo><mtext
    mathvariant="normal">mean</mtext><mo stretchy="false" form="prefix">(</mo><mrow><msub><mi>r</mi><mn>1</mn></msub><mo>,</mo><msub><mi>r</mi><mn>2</mn></msub><mo>,</mo><mi>â‹¯</mi><mo>,</mo><msub><mi>r</mi><mi>G</mi></msub></mrow><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><mtext mathvariant="normal">std</mtext><mo
    stretchy="false" form="prefix">(</mo><mrow><msub><mi>r</mi><mn>1</mn></msub><mo>,</mo><msub><mi>r</mi><mn>2</mn></msub><mo>,</mo><mi>â‹¯</mi><mo>,</mo><msub><mi>r</mi><mi>G</mi></msub></mrow><mo
    stretchy="false" form="postfix">)</mo></mrow></mfrac><mi>.</mi><mrow><mo stretchy="false"
    form="prefix">(</mo><mn>60</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">A_i = \frac{r_i - \text{mean}({r_1, r_2, \cdots,
    r_G})}{\text{std}({r_1, r_2, \cdots, r_G})}.\qquad{(60)}</annotation></semantics>
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><msub><mi>A</mi><mi>i</mi></msub><mo>=</mo><mfrac><mrow><msub><mi>r</mi><mi>i</mi></msub><mo>âˆ’</mo><mtext
    mathvariant="normal">å‡å€¼</mtext><mo stretchy="false" form="prefix">(</mo><mrow><msub><mi>r</mi><mn>1</mn></msub><mo>,</mo><msub><mi>r</mi><mn>2</mn></msub><mo>,</mo><mi>â‹¯</mi><mo>,</mo><msub><mi>r</mi><mi>G</mi></msub></mrow><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><mtext mathvariant="normal">æ ‡å‡†å·®</mtext><mo
    stretchy="false" form="prefix">(</mo><mrow><msub><mi>r</mi><mn>1</mn></msub><mo>,</mo><msub><mi>r</mi><mn>2</mn></msub><mo>,</mo><mi>â‹¯</mi><mo>,</mo><msub><mi>r</mi><mi>G</mi></msub></mrow><mo
    stretchy="false" form="postfix">)</mo></mrow></mfrac><mi>.</mi><mrow><mo stretchy="false"
    form="prefix">(</mo><mn>60</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">A_i = \frac{r_i - \text{å‡å€¼}({r_1, r_2, \cdots, r_G})}{\text{æ ‡å‡†å·®}({r_1,
    r_2, \cdots, r_G})}.\qquad{(60)}</annotation></semantics>
- en: Intuitively, the GRPO update is comparing multiple answers to a single question
    within a batch. The model learns to become more like the answers marked as correct
    and less like the others. This is a very simple way to compute the advantage,
    which is the measure of how much better a specific action is than the average
    at a given state. Relative to PPO, REINFORCE, and broadly RLHF performed with
    a reward model rating (relative to output reward), GRPO is often run with a far
    higher number of samples per prompt because the advantage is entirely about the
    relative value of a completion to its peers from that prompt. Here, the current
    policy generates multiple responses to a given prompt, and the group-wise GRPO
    advantage estimate is given valuable context. PPO and vanilla policy-gradient
    algorithms were designed to accurately estimate the reward of every completion
    (in fact, more completions can do little to improve the value estimate in some
    cases). GRPO and its variants are particularly well-suited to modern language
    model tools, where multiple completions to a given prompt is very natural (especially
    when compared to, e.g., multiple actions from a set environment state in a robotic
    task).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ç›´è§‚ä¸Šï¼ŒGRPOçš„æ›´æ–°æ˜¯åœ¨ä¸€ä¸ªæ‰¹æ¬¡å†…æ¯”è¾ƒå•ä¸ªé—®é¢˜çš„å¤šä¸ªç­”æ¡ˆã€‚æ¨¡å‹å­¦ä¹ å˜å¾—æ›´åŠ æ¥è¿‘æ ‡è®°ä¸ºæ­£ç¡®çš„ç­”æ¡ˆï¼Œè€Œè¿œç¦»å…¶ä»–ç­”æ¡ˆã€‚è¿™æ˜¯ä¸€ç§è®¡ç®—ä¼˜åŠ¿çš„éå¸¸ç®€å•çš„æ–¹æ³•ï¼Œä¼˜åŠ¿æ˜¯è¡¡é‡ç‰¹å®šåŠ¨ä½œåœ¨ç»™å®šçŠ¶æ€ä¸‹æ¯”å¹³å‡è¡¨ç°å¥½å¤šå°‘çš„åº¦é‡ã€‚ä¸PPOã€REINFORCEä»¥åŠå¹¿æ³›ä½¿ç”¨çš„RLHFï¼ˆåŸºäºå¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ä¸äººç±»åé¦ˆï¼‰ç›¸æ¯”ï¼ŒGRPOé€šå¸¸ä½¿ç”¨æ¯ä¸ªæç¤ºçš„æ ·æœ¬æ•°é‡è¦é«˜å¾—å¤šï¼Œå› ä¸ºä¼˜åŠ¿å®Œå…¨å…³ä¹äºä¸€ä¸ªå®Œæˆé¡¹ç›¸å¯¹äºåŒä¸€æç¤ºä¸­å…¶ä»–å®Œæˆé¡¹çš„ç›¸å¯¹ä»·å€¼ã€‚åœ¨è¿™é‡Œï¼Œå½“å‰ç­–ç•¥ä¸ºç»™å®šçš„æç¤ºç”Ÿæˆå¤šä¸ªå“åº”ï¼Œå¹¶ä¸”ç»„å†…GRPOä¼˜åŠ¿ä¼°è®¡æä¾›äº†æœ‰ä»·å€¼çš„ä¸Šä¸‹æ–‡ã€‚PPOå’Œvanillaç­–ç•¥æ¢¯åº¦ç®—æ³•æ—¨åœ¨å‡†ç¡®ä¼°è®¡æ¯ä¸ªå®Œæˆé¡¹çš„å¥–åŠ±ï¼ˆå®é™…ä¸Šï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæ›´å¤šçš„å®Œæˆé¡¹å¯¹ä»·å€¼ä¼°è®¡çš„æ”¹å–„ä½œç”¨å¾ˆå°ï¼‰ã€‚GRPOåŠå…¶å˜ä½“ç‰¹åˆ«é€‚åˆäºç°ä»£è¯­è¨€æ¨¡å‹å·¥å…·ï¼Œåœ¨è¿™äº›å·¥å…·ä¸­ï¼Œé’ˆå¯¹ç»™å®šæç¤ºçš„å¤šä¸ªå®Œæˆé¡¹æ˜¯éå¸¸è‡ªç„¶çš„ï¼ˆå°¤å…¶æ˜¯ä¸ï¼Œä¾‹å¦‚ï¼Œåœ¨æœºå™¨äººä»»åŠ¡ä¸­ä»ä¸€ç»„ç¯å¢ƒçŠ¶æ€ä¸­è·å¾—çš„å¤šä¸ªåŠ¨ä½œç›¸æ¯”ï¼‰ã€‚
- en: The advantage computation for GRPO has trade-offs in its biases. The normalization
    by standard deviation is rewarding questions in a batch that have a low variation
    in answer correctness. For questions with either nearly all correct or all incorrect
    answers, the standard deviation will be lower and the advantage will be higher.
    [[185]](ch021.xhtml#ref-liu2025understanding) proposes removing the standard deviation
    term given this bias, but this comes at the cost of down-weighing questions that
    were all incorrect with a few correct answers, which could be seen as valuable
    learning signal for the model. Those high-variance prompts can be exactly the
    hardest cases, where only a few sampled completions find the correct answer and
    provide a strong training signal.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: GRPOçš„ä¼˜åŠ¿è®¡ç®—åœ¨åå·®æ–¹é¢å­˜åœ¨æƒè¡¡ã€‚é€šè¿‡æ ‡å‡†å·®è¿›è¡Œå½’ä¸€åŒ–ä¼šå¥–åŠ±é‚£äº›åœ¨ç­”æ¡ˆæ­£ç¡®æ€§ä¸Šå˜åŒ–è¾ƒå°çš„æ‰¹æ¬¡ä¸­çš„é—®é¢˜ã€‚å¯¹äºç­”æ¡ˆå‡ ä¹å…¨éƒ¨æ­£ç¡®æˆ–å…¨éƒ¨é”™è¯¯çš„é—®é¢˜ï¼Œæ ‡å‡†å·®ä¼šè¾ƒä½ï¼Œä¼˜åŠ¿ä¼šè¾ƒé«˜ã€‚[185](ch021.xhtml#ref-liu2025understanding)æå‡ºï¼Œé‰´äºè¿™ç§åå·®ï¼Œå¯ä»¥ç§»é™¤æ ‡å‡†å·®é¡¹ï¼Œä½†è¿™ä¼šä»¥é™ä½æ‰€æœ‰ç­”æ¡ˆéƒ½é”™è¯¯ä½†æœ‰å‡ ä¸ªæ­£ç¡®ç­”æ¡ˆçš„é—®é¢˜çš„æƒé‡ä¸ºä»£ä»·ï¼Œè¿™å¯èƒ½ä¼šè¢«è§†ä¸ºæ¨¡å‹çš„æœ‰ä»·å€¼çš„å­¦ä¹ ä¿¡å·ã€‚è¿™äº›é«˜æ–¹å·®æç¤ºå¯èƒ½æ˜¯æœ€å›°éš¾çš„æ¡ˆä¾‹ï¼Œåªæœ‰å°‘æ•°æ ·æœ¬çš„å®Œæˆæ‰¾åˆ°äº†æ­£ç¡®ç­”æ¡ˆå¹¶æä¾›äº†å¼ºå¤§çš„è®­ç»ƒä¿¡å·ã€‚
- en: eq.Â [60](ch011.xhtml#eq:GRPO_ADV) is the implementation of GRPO when working
    with outcome supervision (either a standard reward model or a single verifiable
    reward) and a different implementation is needed with process supervision. In
    this case, GRPO computes the advantage as the sum of the normalized rewards for
    the following reasoning steps.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ç­‰å¼[60](ch011.xhtml#eq:GRPO_ADV)æ˜¯å½“ä½¿ç”¨ç»“æœç›‘ç£ï¼ˆæ— è®ºæ˜¯æ ‡å‡†å¥–åŠ±æ¨¡å‹è¿˜æ˜¯å•ä¸ªå¯éªŒè¯å¥–åŠ±ï¼‰æ—¶GRPOçš„å®ç°ï¼Œè€Œåœ¨è¿‡ç¨‹ç›‘ç£ä¸‹åˆ™éœ€è¦ä¸åŒçš„å®ç°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒGRPOé€šè¿‡è®¡ç®—åç»­æ¨ç†æ­¥éª¤çš„å½’ä¸€åŒ–å¥–åŠ±ä¹‹å’Œæ¥è®¡ç®—ä¼˜åŠ¿ã€‚
- en: 'Finally, GRPOâ€™s advantage estimation can also be applied without the PPO clipping
    to more vanilla versions of policy gradient (e.g.Â REINFORCE), but it is not the
    canonical form. As an example of how these algorithms are intertwined, we can
    show that the advantage estimation in a variant of GRPO, Dr.Â GRPO (GRPO Done Right)
    [[185]](ch021.xhtml#ref-liu2025understanding), is equivalent to the RLOO estimation
    (which uses the average reward of other samples as its baseline) up to a constant
    scaling factor (which normally does not matter due to implementation details to
    normalize the advantage). Dr.Â GRPO removes the standard deviation normalization
    term from eq.Â [60](ch011.xhtml#eq:GRPO_ADV) â€“ note that this also scales the advantage
    *up*, which is equivalent to increasing the GRPO learning rate on samples with
    a variance in answer scores. This addresses a bias towards questions with low
    reward variance â€“ i.e.Â almost all the answers are right or wrong â€“ but comes at
    a potential cost where problems where just one sample gets the answer right are
    important to learn from. The Dr.Â GRPO advantage for completion <semantics><mi>i</mi><annotation
    encoding="application/x-tex">i</annotation></semantics> within a group of size
    <semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics>
    is defined as:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼ŒGRPOçš„ä¼˜åŠ¿ä¼°è®¡ä¹Ÿå¯ä»¥åº”ç”¨äºæ²¡æœ‰PPOè£å‰ªçš„æ›´ä¼ ç»Ÿçš„ç­–ç•¥æ¢¯åº¦ç‰ˆæœ¬ï¼ˆä¾‹å¦‚REINFORCEï¼‰ï¼Œä½†å®ƒä¸æ˜¯è§„èŒƒå½¢å¼ã€‚ä½œä¸ºè¿™äº›ç®—æ³•ç›¸äº’äº¤ç»‡çš„ä¸€ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬å¯ä»¥è¡¨æ˜ï¼ŒGRPOçš„ä¸€ä¸ªå˜ä½“ï¼Œå³Dr.
    GRPOï¼ˆGRPO Done Rightï¼‰[185](ch021.xhtml#ref-liu2025understanding)çš„ä¼˜åŠ¿ä¼°è®¡ï¼Œåœ¨å¸¸æ•°ç¼©æ”¾å› å­ï¼ˆé€šå¸¸ç”±äºå®ç°ç»†èŠ‚æ¥å½’ä¸€åŒ–ä¼˜åŠ¿è€Œä¸ä¼šäº§ç”Ÿå½±å“ï¼‰ä¸Šç­‰åŒäºRLOOä¼°è®¡ï¼ˆå®ƒä½¿ç”¨å…¶ä»–æ ·æœ¬çš„å¹³å‡å¥–åŠ±ä½œä¸ºå…¶åŸºçº¿ï¼‰ã€‚Dr.
    GRPOä»ç­‰å¼[60](ch011.xhtml#eq:GRPO_ADV)ä¸­ç§»é™¤äº†æ ‡å‡†å·®å½’ä¸€åŒ–é¡¹â€”â€”è¯·æ³¨æ„ï¼Œè¿™ä¹Ÿæ”¾å¤§äº†ä¼˜åŠ¿ï¼Œè¿™ç›¸å½“äºå¢åŠ äº†å…·æœ‰ç­”æ¡ˆåˆ†æ•°å˜åŒ–çš„æ ·æœ¬çš„GRPOå­¦ä¹ ç‡ã€‚è¿™è§£å†³äº†å€¾å‘äºå¥–åŠ±æ–¹å·®ä½çš„é—®é¢˜â€”â€”å³å‡ ä¹æ‰€æœ‰ç­”æ¡ˆéƒ½æ˜¯æ­£ç¡®æˆ–é”™è¯¯çš„â€”â€”ä½†å¯èƒ½ä¼šå¸¦æ¥æ½œåœ¨çš„ä»£ä»·ï¼Œå³å¯¹äºåªæœ‰ä¸€ä¸ªæ ·æœ¬æ­£ç¡®å›ç­”çš„é—®é¢˜ï¼Œè¿™äº›æ˜¯é‡è¦çš„å­¦ä¹ å†…å®¹ã€‚åœ¨å¤§å°ä¸º<semantics><mi>G</mi><annotation
    encoding="application/x-tex">G</annotation></semantics>çš„ç»„å†…ï¼ŒDr. GRPOçš„ä¼˜åŠ¿å¯¹äºå®Œæˆ<semantics><mi>i</mi><annotation
    encoding="application/x-tex">i</annotation></semantics>è¢«å®šä¹‰ä¸ºï¼š
- en: <semantics><mrow><msub><mover><mi>A</mi><mo accent="true">Ìƒ</mo></mover><mi>i</mi></msub><mo>=</mo><msub><mi>r</mi><mi>i</mi></msub><mo>âˆ’</mo><mtext
    mathvariant="normal">mean</mtext><mo stretchy="false" form="prefix">(</mo><mrow><msub><mi>r</mi><mn>1</mn></msub><mo>,</mo><msub><mi>r</mi><mn>2</mn></msub><mo>,</mo><mi>â‹¯</mi><mo>,</mo><msub><mi>r</mi><mi>G</mi></msub></mrow><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>r</mi><mi>i</mi></msub><mo>âˆ’</mo><mfrac><mn>1</mn><mi>G</mi></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>G</mi></munderover><msub><mi>r</mi><mi>j</mi></msub><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>61</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">\tilde{A}_i = r_i - \text{mean}({r_1,
    r_2, \cdots, r_G}) = r_i - \frac{1}{G}\sum_{j=1}^G r_j \qquad{(61)}</annotation></semantics>
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><msub><mover><mi>A</mi><mo accent="true">Ìƒ</mo></mover><mi>i</mi></msub><mo>=</mo><msub><mi>r</mi><mi>i</mi></msub><mo>âˆ’</mo><mtext
    mathvariant="normal">mean</mtext><mo stretchy="false" form="prefix">(</mo><mrow><msub><mi>r</mi><mn>1</mn></msub><mo>,</mo><msub><mi>r</mi><mn>2</mn></msub><mo>,</mo><mi>â‹¯</mi><mo>,</mo><msub><mi>r</mi><mi>G</mi></msub></mrow><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>r</mi><mi>i</mi></msub><mo>âˆ’</mo><mfrac><mn>1</mn><mi>G</mi></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>G</mi></munderover><msub><mi>r</mi><mi>j</mi></msub><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>61</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">\tilde{A}_i = r_i - \text{mean}({r_1,
    r_2, \cdots, r_G}) = r_i - \frac{1}{G}\sum_{j=1}^G r_j \qquad{(61)}</annotation></semantics>
- en: 'Here, in the same notation, we can recall the RLOO advantage estimation as:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œä½¿ç”¨ç›¸åŒçš„ç¬¦å·ï¼Œæˆ‘ä»¬å¯ä»¥å›å¿†èµ·RLOOä¼˜åŠ¿ä¼°è®¡ä¸ºï¼š
- en: <semantics><mrow><msubsup><mi>A</mi><mi>i</mi><mtext mathvariant="normal">RLOO</mtext></msubsup><mo>=</mo><msub><mi>r</mi><mi>i</mi></msub><mo>âˆ’</mo><mfrac><mn>1</mn><mrow><mi>G</mi><mo>âˆ’</mo><mn>1</mn></mrow></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>i</mi><mo>â‰ </mo><mi>j</mi></mrow><mi>G</mi></munderover><msub><mi>r</mi><mi>j</mi></msub><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>62</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">A_i^\text{RLOO} = r_i - \frac{1}{G-1}\sum_{j=1,
    i\neq j}^G r_j \qquad{(62)}</annotation></semantics>
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><msubsup><mi>A</mi><mi>i</mi><mtext mathvariant="normal">RLOO</mtext></msubsup><mo>=</mo><msub><mi>r</mi><mi>i</mi></msub><mo>âˆ’</mo><mfrac><mn>1</mn><mrow><mi>G</mi><mo>âˆ’</mo><mn>1</mn></mrow></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>i</mi><mo>â‰ </mo><mi>j</mi></mrow><mi>G</mi></munderover><msub><mi>r</mi><mi>j</mi></msub><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>62</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">A_i^\text{RLOO} = r_i - \frac{1}{G-1}\sum_{j=1,
    i\neq j}^G r_j \qquad{(62)}</annotation></semantics>
- en: 'Thus, if we multiply the Dr.Â GRPO advantage definition by <semantics><mfrac><mi>G</mi><mrow><mi>G</mi><mo>âˆ’</mo><mn>1</mn></mrow></mfrac><annotation
    encoding="application/x-tex">\frac{G}{G-1}</annotation></semantics> we can see
    a scaled equivalence:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬æŠŠDr. GRPOä¼˜åŠ¿å®šä¹‰ä¹˜ä»¥<semantics><mfrac><mi>G</mi><mrow><mi>G</mi><mo>âˆ’</mo><mn>1</mn></mrow></mfrac><annotation
    encoding="application/x-tex">\frac{G}{G-1}</annotation></semantics>ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸€ä¸ªç¼©æ”¾ç­‰ä»·æ€§ï¼š
- en: '<semantics><mrow><mtable><mtr><mtd columnalign="right" style="text-align: right;
    padding-right: 0"><mfrac><mi>G</mi><mrow><mi>G</mi><mo>âˆ’</mo><mn>1</mn></mrow></mfrac><msub><mover><mi>A</mi><mo
    accent="true">Ìƒ</mo></mover><mi>i</mi></msub></mtd><mtd columnalign="left" style="text-align:
    left; padding-left: 0"><mo>=</mo><mfrac><mi>G</mi><mrow><mi>G</mi><mo>âˆ’</mo><mn>1</mn></mrow></mfrac><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>r</mi><mi>i</mi></msub><mo>âˆ’</mo><mfrac><mn>1</mn><mi>G</mi></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>G</mi></munderover><msub><mi>r</mi><mi>j</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="left"
    style="text-align: left; padding-left: 0"><mo>=</mo><mfrac><mi>G</mi><mrow><mi>G</mi><mo>âˆ’</mo><mn>1</mn></mrow></mfrac><msub><mi>r</mi><mi>i</mi></msub><mo>âˆ’</mo><mfrac><mn>1</mn><mrow><mi>G</mi><mo>âˆ’</mo><mn>1</mn></mrow></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>G</mi></munderover><msub><mi>r</mi><mi>j</mi></msub></mtd></mtr><mtr><mtd
    columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><mfrac><mi>G</mi><mrow><mi>G</mi><mo>âˆ’</mo><mn>1</mn></mrow></mfrac><msub><mi>r</mi><mi>i</mi></msub><mo>âˆ’</mo><mfrac><mn>1</mn><mrow><mi>G</mi><mo>âˆ’</mo><mn>1</mn></mrow></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>j</mi><mo>â‰ </mo><mi>i</mi></mrow><mi>G</mi></munderover><msub><mi>r</mi><mi>j</mi></msub><mo>âˆ’</mo><mfrac><mn>1</mn><mrow><mi>G</mi><mo>âˆ’</mo><mn>1</mn></mrow></mfrac><msub><mi>r</mi><mi>i</mi></msub></mtd></mtr><mtr><mtd
    columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><msub><mi>r</mi><mi>i</mi></msub><mrow><mo
    stretchy="true" form="prefix">(</mo><mfrac><mi>G</mi><mrow><mi>G</mi><mo>âˆ’</mo><mn>1</mn></mrow></mfrac><mo>âˆ’</mo><mfrac><mn>1</mn><mrow><mi>G</mi><mo>âˆ’</mo><mn>1</mn></mrow></mfrac><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>âˆ’</mo><mfrac><mn>1</mn><mrow><mi>G</mi><mo>âˆ’</mo><mn>1</mn></mrow></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>j</mi><mo>â‰ </mo><mi>i</mi></mrow><mi>G</mi></munderover><msub><mi>r</mi><mi>j</mi></msub></mtd></mtr><mtr><mtd
    columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><msub><mi>r</mi><mi>i</mi></msub><mo>âˆ’</mo><mfrac><mn>1</mn><mrow><mi>G</mi><mo>âˆ’</mo><mn>1</mn></mrow></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>j</mi><mo>â‰ </mo><mi>i</mi></mrow><mi>G</mi></munderover><msub><mi>r</mi><mi>j</mi></msub></mtd></mtr><mtr><mtd
    columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><msubsup><mi>A</mi><mi>i</mi><mtext
    mathvariant="normal">RLOO</mtext></msubsup></mtd></mtr></mtable><mrow><mo stretchy="false"
    form="prefix">(</mo><mn>63</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">\begin{aligned} \frac{G}{G-1} \tilde{A}_i
    &= \frac{G}{G-1} \left( r_i - \frac{1}{G}\sum_{j=1}^G r_j \right) \\ &= \frac{G}{G-1}
    r_i - \frac{1}{G-1} \sum_{j=1}^G r_j \\ &= \frac{G}{G-1} r_i - \frac{1}{G-1} \sum_{j=1,
    j\neq i}^G r_j - \frac{1}{G-1} r_i \\ &= r_i \left( \frac{G}{G-1} - \frac{1}{G-1}
    \right) - \frac{1}{G-1} \sum_{j=1, j\neq i}^G r_j \\ &= r_i - \frac{1}{G-1} \sum_{j=1,
    j\neq i}^G r_j \\ &= A_i^{\text{RLOO}} \end{aligned} \qquad{(63)}</annotation></semantics>'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®ç°
- en: Compared to the original Deep RL literature where many of these algorithms were
    developed, implementing RL for optimizing language models or other large AI models
    requires many small implementation details. In this section, we highlight some
    key factors that differentiate the implementations of popular algorithms.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è®¸å¤šè¿™äº›ç®—æ³•åœ¨åŸå§‹æ·±åº¦å¼ºåŒ–å­¦ä¹ æ–‡çŒ®ä¸­å¼€å‘ç›¸æ¯”ï¼Œä¸ºä¼˜åŒ–è¯­è¨€æ¨¡å‹æˆ–å…¶ä»–å¤§å‹AIæ¨¡å‹å®ç°RLéœ€è¦è®¸å¤šå°çš„å®ç°ç»†èŠ‚ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å¼ºè°ƒäº†ä¸€äº›åŒºåˆ†æµè¡Œç®—æ³•å®ç°çš„å…³é”®å› ç´ ã€‚
- en: There are many other small details that go into this training. For example,
    when doing RLHF with language models a crucial step is generating text that will
    then be rated by the reward model. Under normal circumstances, the model should
    generate an end-of-sequence (EOS) token indicating it finished generating, but
    a common practice is to put a hard cap on generation length to efficiently utilize
    infrastructure. A failure mode of RLHF is that the model is regularly truncated
    in its answers, driving the ratings from the reward model out of distribution
    and to unpredictable scores. The solution to this is to *only* run reward model
    scoring on the `eos_token`, and to otherwise assign a penalty to the model for
    generating too long.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªè®­ç»ƒä¸­è¿˜æœ‰å¾ˆå¤šå…¶ä»–çš„å°ç»†èŠ‚ã€‚ä¾‹å¦‚ï¼Œåœ¨è¿›è¡ŒåŸºäºè¯­è¨€æ¨¡å‹çš„RLHFæ—¶ï¼Œä¸€ä¸ªå…³é”®æ­¥éª¤æ˜¯ç”Ÿæˆå°†è¢«å¥–åŠ±æ¨¡å‹è¯„åˆ†çš„æ–‡æœ¬ã€‚åœ¨æ­£å¸¸æƒ…å†µä¸‹ï¼Œæ¨¡å‹åº”è¯¥ç”Ÿæˆä¸€ä¸ªåºåˆ—ç»“æŸï¼ˆEOSï¼‰æ ‡è®°æ¥æŒ‡ç¤ºå®ƒå·²ç»å®Œæˆç”Ÿæˆï¼Œä½†å¸¸è§çš„åšæ³•æ˜¯è®¾ç½®ä¸€ä¸ªç”Ÿæˆé•¿åº¦çš„ä¸Šé™ï¼Œä»¥æœ‰æ•ˆåœ°åˆ©ç”¨åŸºç¡€è®¾æ–½ã€‚RLHFçš„ä¸€ä¸ªå¤±è´¥æ¨¡å¼æ˜¯æ¨¡å‹åœ¨å›ç­”ä¸­ç»å¸¸è¢«æˆªæ–­ï¼Œå¯¼è‡´å¥–åŠ±æ¨¡å‹çš„è¯„åˆ†åç¦»åˆ†å¸ƒå¹¶å˜å¾—ä¸å¯é¢„æµ‹ã€‚è§£å†³è¿™ä¸ªé—®é¢˜çš„æ–¹æ³•æ˜¯*ä»…*å¯¹`eos_token`è¿è¡Œå¥–åŠ±æ¨¡å‹è¯„åˆ†ï¼Œå¦åˆ™å¯¹ç”Ÿæˆè¿‡é•¿çš„æ¨¡å‹è¿›è¡Œæƒ©ç½šã€‚
- en: 'The popular open-source tools for RLHF have a large variance in implementation
    details across the algorithms (see table 10 in [[190]](ch021.xhtml#ref-ivison2024unpacking)).
    Some decisions not covered here include:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: é€‚ç”¨äºRLHFçš„æµè¡Œå¼€æºå·¥å…·åœ¨ç®—æ³•çš„å®ç°ç»†èŠ‚ä¸Šå­˜åœ¨å¾ˆå¤§çš„å·®å¼‚ï¼ˆè§ç¬¬10è¡¨[[190]](ch021.xhtml#ref-ivison2024unpacking)ï¼‰ã€‚è¿™é‡Œæ²¡æœ‰æ¶µç›–çš„ä¸€äº›å†³ç­–åŒ…æ‹¬ï¼š
- en: '**Value network initialization**: The internal learned value network used by
    PPO and other similar algorithms can be started from a different model of the
    same architecture or randomly selected weights. This can have a large impact on
    performance. The standard established in InstructGPT [[3]](ch021.xhtml#ref-ouyang2022training)
    (and re-used in TÃ¼lu 3 for its work on RLVR [[6]](ch021.xhtml#ref-lambert2024t))
    is to initialize the value network from the reward model used during RLHF. Others
    have used the previous checkpoint to RLHF training (normally an SFT model) with
    a value head appened randomly initialized, or fully re-initialized language models
    (less common as it will take longer for RLHF to converge, but possible).'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä»·å€¼ç½‘ç»œåˆå§‹åŒ–**ï¼šPPOå’Œå…¶ä»–ç±»ä¼¼ç®—æ³•ä½¿ç”¨çš„å†…éƒ¨å­¦ä¹ ä»·å€¼ç½‘ç»œå¯ä»¥ä»å…·æœ‰ç›¸åŒæ¶æ„çš„ä¸åŒæ¨¡å‹æˆ–éšæœºé€‰æ‹©çš„æƒé‡å¼€å§‹ã€‚è¿™å¯èƒ½ä¼šå¯¹æ€§èƒ½äº§ç”Ÿé‡å¤§å½±å“ã€‚InstructGPT
    [[3]](ch021.xhtml#ref-ouyang2022training)ï¼ˆå¹¶åœ¨TÃ¼lu 3çš„RLVR [[6]](ch021.xhtml#ref-lambert2024t)å·¥ä½œä¸­é‡ç”¨ï¼‰ä¸­å»ºç«‹çš„æ ‡å‡†æ˜¯ä»RLHFæœŸé—´ä½¿ç”¨çš„å¥–åŠ±æ¨¡å‹åˆå§‹åŒ–ä»·å€¼ç½‘ç»œã€‚å…¶ä»–äººåˆ™ä½¿ç”¨äº†ä¹‹å‰çš„æ£€æŸ¥ç‚¹æ¥RLHFè®­ç»ƒï¼ˆé€šå¸¸æ˜¯ä¸€ä¸ªSFTæ¨¡å‹ï¼‰ï¼Œå¹¶é™„åŠ ä¸€ä¸ªéšæœºåˆå§‹åŒ–çš„ä»·å€¼å¤´ï¼Œæˆ–è€…å®Œå…¨é‡æ–°åˆå§‹åŒ–è¯­è¨€æ¨¡å‹ï¼ˆè¾ƒå°‘è§ï¼Œå› ä¸ºRLHFæ”¶æ•›éœ€è¦æ›´é•¿çš„æ—¶é—´ï¼Œä½†å¯è¡Œï¼‰ã€‚'
- en: '**Reward normalization, reward whitening, and/or advantage whitening**: Normalization
    bounds all the values from the RM (or environment) to be between 0 and 1, which
    can help with learning stability. [Whitening](https://en.wikipedia.org/wiki/Whitening_transformation)
    goes further by transforming rewards or advantage estimates to have zero mean
    and unit variance, providing an even stronger boost to stability.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¥–åŠ±å½’ä¸€åŒ–ã€å¥–åŠ±ç™½åŒ–ä»¥åŠ/æˆ–ä¼˜åŠ¿ç™½åŒ–**ï¼šå½’ä¸€åŒ–å°†RMï¼ˆæˆ–ç¯å¢ƒï¼‰ä¸­çš„æ‰€æœ‰å€¼é™åˆ¶åœ¨0åˆ°1ä¹‹é—´ï¼Œè¿™æœ‰åŠ©äºæé«˜å­¦ä¹ ç¨³å®šæ€§ã€‚[ç™½åŒ–](https://en.wikipedia.org/wiki/Whitening_transformation)é€šè¿‡å°†å¥–åŠ±æˆ–ä¼˜åŠ¿ä¼°è®¡è½¬æ¢ä¸ºå…·æœ‰é›¶å‡å€¼å’Œå•ä½æ–¹å·®æ¥è¿›ä¸€æ­¥æ‰©å±•ï¼Œä»è€Œä¸ºç¨³å®šæ€§æä¾›æ›´å¼ºçš„æå‡ã€‚'
- en: '**Different KL estimators**: With complex language models, precisely computing
    the KL divergence between models can be complex, so multiple approximations are
    used to substitute for an exact calculation [[164]](ch021.xhtml#ref-schulman2016klapprox).'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä¸åŒçš„KLä¼°è®¡å™¨**ï¼šå¯¹äºå¤æ‚çš„è¯­è¨€æ¨¡å‹ï¼Œç²¾ç¡®è®¡ç®—æ¨¡å‹ä¹‹é—´çš„KLæ•£åº¦å¯èƒ½å¾ˆå¤æ‚ï¼Œå› æ­¤ä½¿ç”¨äº†å¤šä¸ªè¿‘ä¼¼æ¥ä»£æ›¿ç²¾ç¡®è®¡ç®—[[164]](ch021.xhtml#ref-schulman2016klapprox)ã€‚'
- en: '**KL controllers**: Original implementations of PPO and related algorithms
    had dynamic controllers that targeted specific KLs and changed the penalty based
    on recent measurements. Most modern RLHF implementations use static KL penalties,
    but this can also vary.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**KLæ§åˆ¶å™¨**ï¼šPPOå’Œç›¸å…³ç®—æ³•çš„åŸå§‹å®ç°å…·æœ‰é’ˆå¯¹ç‰¹å®šKLçš„åŠ¨æ€æ§åˆ¶å™¨ï¼Œå¹¶æ ¹æ®æœ€è¿‘æµ‹é‡ç»“æœæ”¹å˜æƒ©ç½šã€‚å¤§å¤šæ•°ç°ä»£RLHFå®ç°ä½¿ç”¨é™æ€KLæƒ©ç½šï¼Œä½†è¿™ä¹Ÿå¯ä»¥å˜åŒ–ã€‚'
- en: For more details on implementation details for RLHF, see [[191]](ch021.xhtml#ref-huang2024n).
    For further information on the algorithms, see [[192]](ch021.xhtml#ref-weng2018PG).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´å¤šå…³äºRLHFå®ç°ç»†èŠ‚çš„ä¿¡æ¯ï¼Œè¯·å‚é˜…[[191]](ch021.xhtml#ref-huang2024n)ã€‚æœ‰å…³ç®—æ³•çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[[192]](ch021.xhtml#ref-weng2018PG)ã€‚
- en: Policy Gradient Basics
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç­–ç•¥æ¢¯åº¦åŸºç¡€
- en: 'A simple implementation of policy gradient, using advantages to estimate the
    gradient to prepare for advanced algorithms such as PPO and GRPO follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ç­–ç•¥æ¢¯åº¦çš„ç®€å•å®ç°ï¼Œä½¿ç”¨ä¼˜åŠ¿æ¥ä¼°è®¡æ¢¯åº¦ï¼Œä¸ºé«˜çº§ç®—æ³•å¦‚PPOå’ŒGRPOåšå‡†å¤‡å¦‚ä¸‹ï¼š
- en: '[PRE1]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Ratio here is the (per-token) probability ratio (often computed from a log-probability
    difference) of the new policy model probabilities relative to the reference model.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œçš„æ¯”ç‡æ˜¯æ–°ç­–ç•¥æ¨¡å‹æ¦‚ç‡ç›¸å¯¹äºå‚è€ƒæ¨¡å‹æ¦‚ç‡çš„ï¼ˆæ¯ä¸ªæ ‡è®°çš„ï¼‰æ¦‚ç‡æ¯”ç‡ï¼ˆé€šå¸¸ä»logæ¦‚ç‡å·®è®¡ç®—å¾—å‡ºï¼‰ã€‚
- en: In order to understand this equation, it is good to understand different cases
    that can fall within a batch of updates. Remember that we want the loss to *decrease*
    as the model gets better at the task.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç†è§£è¿™ä¸ªæ–¹ç¨‹ï¼Œäº†è§£å¯èƒ½å‡ºç°åœ¨ä¸€æ‰¹æ›´æ–°ä¸­çš„ä¸åŒæƒ…å†µæ˜¯æœ‰å¸®åŠ©çš„ã€‚è®°ä½ï¼Œæˆ‘ä»¬å¸Œæœ›éšç€æ¨¡å‹åœ¨ä»»åŠ¡ä¸Šå˜å¾—æ›´å¥½ï¼ŒæŸå¤±ä¼š*å‡å°‘*ã€‚
- en: 'Case 1: Positive advantage, so the action was better than the expected value
    of the state. We want to reinforce this. In this case, the model will make this
    more likely with the negative sign. To do so, itâ€™ll increase the logratio. A positive
    logratio, or sum of log probabilities of the tokens, means that the model is more
    likely to generate those tokens.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: æƒ…å†µ1ï¼šæ­£ä¼˜åŠ¿ï¼Œå› æ­¤åŠ¨ä½œæ¯”çŠ¶æ€çš„é¢„æœŸå€¼æ›´å¥½ã€‚æˆ‘ä»¬æƒ³è¦åŠ å¼ºè¿™ä¸€ç‚¹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¨¡å‹å°†é€šè¿‡è´Ÿå·ä½¿è¿™ç§æƒ…å†µæ›´æœ‰å¯èƒ½ã€‚ä¸ºæ­¤ï¼Œå®ƒå°†å¢åŠ logratioã€‚æ­£logratioï¼Œæˆ–æ ‡è®°çš„logæ¦‚ç‡ä¹‹å’Œï¼Œæ„å‘³ç€æ¨¡å‹æ›´æœ‰å¯èƒ½ç”Ÿæˆè¿™äº›æ ‡è®°ã€‚
- en: 'Case 2: Negative advantage, so the action was worse than the expected value
    of the state. This follows very similarly. Here, the loss will be positive if
    the new model was more likely, so the model will try to make it so the policy
    parameters make this completion less likely.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: æƒ…å†µ2ï¼šè´Ÿä¼˜åŠ¿ï¼Œå› æ­¤åŠ¨ä½œæ¯”çŠ¶æ€çš„é¢„æœŸå€¼æ›´å·®ã€‚è¿™éå¸¸ç›¸ä¼¼ã€‚åœ¨è¿™é‡Œï¼Œå¦‚æœæ–°æ¨¡å‹æ›´æœ‰å¯èƒ½ï¼ŒæŸå¤±å°†æ˜¯æ­£çš„ï¼Œå› æ­¤æ¨¡å‹å°†å°è¯•ä½¿ç­–ç•¥å‚æ•°ä½¿è¿™ç§å®Œæˆä¸å¤ªå¯èƒ½ã€‚
- en: 'Case 3: Zero advantage, so no update is needed. The loss is zero, donâ€™t change
    the policy model.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: æƒ…å†µ3ï¼šé›¶ä¼˜åŠ¿ï¼Œå› æ­¤ä¸éœ€è¦æ›´æ–°ã€‚æŸå¤±ä¸ºé›¶ï¼Œä¸è¦æ”¹å˜ç­–ç•¥æ¨¡å‹ã€‚
- en: Loss Aggregation
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æŸå¤±èšåˆ
- en: 'The question when implementing any policy gradient algorithm with language
    models is: How do you aggregate per-token losses into a final scalar loss? Given
    per-token losses <semantics><msub><mi>â„“</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><annotation
    encoding="application/x-tex">\ell_{i,t}</annotation></semantics> for sample <semantics><mi>i</mi><annotation
    encoding="application/x-tex">i</annotation></semantics> at token <semantics><mi>t</mi><annotation
    encoding="application/x-tex">t</annotation></semantics>, with completion lengths
    <semantics><mrow><mo stretchy="false" form="prefix">|</mo><msub><mi>a</mi><mi>i</mi></msub><mo
    stretchy="false" form="prefix">|</mo></mrow><annotation encoding="application/x-tex">|a_i|</annotation></semantics>
    and batch size <semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics>,
    there are three main strategies:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®ç°ä»»ä½•å¸¦æœ‰è¯­è¨€æ¨¡å‹çš„ç­–ç•¥æ¢¯åº¦ç®—æ³•æ—¶ï¼Œé—®é¢˜æ˜¯ï¼šä½ å¦‚ä½•å°†æ¯ä¸ªæ ‡è®°çš„æŸå¤±èšåˆæˆä¸€ä¸ªæœ€ç»ˆçš„æ ‡é‡æŸå¤±ï¼Ÿç»™å®šæ ·æœ¬ <semantics><mi>i</mi><annotation
    encoding="application/x-tex">i</annotation></semantics> åœ¨æ ‡è®° <semantics><mi>t</mi><annotation
    encoding="application/x-tex">t</annotation></semantics> å¤„çš„æ¯ä¸ªæ ‡è®°æŸå¤± <semantics><msub><mi>â„“</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><annotation
    encoding="application/x-tex">\ell_{i,t}</annotation></semantics>ï¼Œä»¥åŠå®Œæˆé•¿åº¦ <semantics><mrow><mo
    stretchy="false" form="prefix">|</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy="false"
    form="prefix">|</mo></mrow><annotation encoding="application/x-tex">|a_i|</annotation></semantics>
    å’Œæ‰¹é‡å¤§å° <semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics>ï¼Œæœ‰ä¸‰ç§ä¸»è¦ç­–ç•¥ï¼š
- en: '**Strategy 1: Per-sequence normalization** (standard GRPO; also used in some
    PPO implementations)'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç­–ç•¥1ï¼šæŒ‰åºåˆ—å½’ä¸€åŒ–**ï¼ˆæ ‡å‡†GRPOï¼›ä¹Ÿç”¨äºä¸€äº›PPOå®ç°ï¼‰'
- en: <semantics><mrow><mi>L</mi><mo>=</mo><mfrac><mn>1</mn><mi>B</mi></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><mfrac><mn>1</mn><mrow><mo
    stretchy="false" form="prefix">|</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy="false"
    form="prefix">|</mo></mrow></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mo
    stretchy="false" form="prefix">|</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy="false"
    form="prefix">|</mo></mrow></munderover><msub><mi>â„“</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub></mrow><annotation
    encoding="application/x-tex">L = \frac{1}{B} \sum_{i=1}^{B} \frac{1}{|a_i|} \sum_{t=1}^{|a_i|}
    \ell_{i,t}</annotation></semantics>
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><mi>L</mi><mo>=</mo><mfrac><mn>1</mn><mi>B</mi></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><mfrac><mn>1</mn><mrow><mo
    stretchy="false" form="prefix">|</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy="false"
    form="prefix">|</mo></mrow></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mo
    stretchy="false" form="prefix">|</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy="false"
    form="prefix">|</mo></mrow></munderover><msub><mi>â„“</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub></mrow><annotation
    encoding="application/x-tex">L = \frac{1}{B} \sum_{i=1}^{B} \frac{1}{|a_i|} \sum_{t=1}^{|a_i|}
    \ell_{i,t}</annotation></semantics>
- en: 'Each sequence contributes equally to the batch loss, regardless of length.
    In code:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªåºåˆ—å¯¹æ‰¹æŸå¤±çš„è´¡çŒ®æ˜¯ç›¸ç­‰çš„ï¼Œæ— è®ºå…¶é•¿åº¦å¦‚ä½•ã€‚åœ¨ä»£ç ä¸­ï¼š
- en: '[PRE2]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Strategy 2: Per-token normalization** (DAPO [[193]](ch021.xhtml#ref-yu2025dapo))'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç­–ç•¥ 2ï¼šæ¯ä¸ªæ ‡è®°å½’ä¸€åŒ–**ï¼ˆDAPO [[193]](ch021.xhtml#ref-yu2025dapo))'
- en: <semantics><mrow><mi>L</mi><mo>=</mo><mfrac><mrow><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><munderover><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mo
    stretchy="false" form="prefix">|</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy="false"
    form="prefix">|</mo></mrow></munderover><msub><mi>â„“</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub></mrow><mrow><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><mo
    stretchy="false" form="prefix">|</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy="false"
    form="prefix">|</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">L
    = \frac{\sum_{i=1}^{B} \sum_{t=1}^{|a_i|} \ell_{i,t}}{\sum_{i=1}^{B} |a_i|}</annotation></semantics>
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><mi>L</mi><mo>=</mo><mfrac><mrow><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><munderover><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mo
    stretchy="false" form="prefix">|</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy="false"
    form="prefix">|</mo></mrow></munderover><msub><mi>â„“</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub></mrow><mrow><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><mo
    stretchy="false" form="prefix">|</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy="false"
    form="prefix">|</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">L
    = \frac{\sum_{i=1}^{B} \sum_{t=1}^{|a_i|} \ell_{i,t}}{\sum_{i=1}^{B} |a_i|}</annotation></semantics>
- en: 'Each token contributes equally; longer sequences have proportionally more influence
    on the gradient. In code:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªæ ‡è®°çš„è´¡çŒ®æ˜¯ç›¸ç­‰çš„ï¼›è¾ƒé•¿çš„åºåˆ—åœ¨æ¢¯åº¦ä¸Šå…·æœ‰æˆæ¯”ä¾‹æ›´å¤§çš„å½±å“ã€‚åœ¨ä»£ç ä¸­ï¼š
- en: '[PRE3]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Strategy 3: Fixed-length normalization** (Dr.Â GRPO [[185]](ch021.xhtml#ref-liu2025understanding))'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç­–ç•¥ 3ï¼šå›ºå®šé•¿åº¦å½’ä¸€åŒ–**ï¼ˆDr.Â GRPO [[185]](ch021.xhtml#ref-liu2025understanding))'
- en: <semantics><mrow><mi>L</mi><mo>=</mo><mfrac><mn>1</mn><mi>B</mi></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><mfrac><mn>1</mn><msub><mi>L</mi><mi
    mathvariant="normal">max</mi></msub></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mo
    stretchy="false" form="prefix">|</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy="false"
    form="prefix">|</mo></mrow></munderover><msub><mi>â„“</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub></mrow><annotation
    encoding="application/x-tex">L = \frac{1}{B} \sum_{i=1}^{B} \frac{1}{L_{\max}}
    \sum_{t=1}^{|a_i|} \ell_{i,t}</annotation></semantics>
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><mi>L</mi><mo>=</mo><mfrac><mn>1</mn><mi>B</mi></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><mfrac><mn>1</mn><msub><mi>L</mi><mi
    mathvariant="normal">max</mi></msub></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mo
    stretchy="false" form="prefix">|</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy="false"
    form="prefix">|</mo></mrow></munderover><msub><mi>â„“</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub></mrow><annotation
    encoding="application/x-tex">L = \frac{1}{B} \sum_{i=1}^{B} \frac{1}{L_{\max}}
    \sum_{t=1}^{|a_i|} \ell_{i,t}</annotation></semantics>
- en: Normalizes by max sequence length <semantics><msub><mi>L</mi><mi mathvariant="normal">max</mi></msub><annotation
    encoding="application/x-tex">L_{\max}</annotation></semantics>, equalizing the
    per-token scale across sequences while still letting longer sequences contribute
    more total gradient because they contain more active tokens.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡æœ€å¤§åºåˆ—é•¿åº¦è¿›è¡Œå½’ä¸€åŒ–<semantics><msub><mi>L</mi><mi mathvariant="normal">max</mi></msub><annotation
    encoding="application/x-tex">L_{\max}</annotation></semantics>ï¼Œåœ¨åºåˆ—ä¹‹é—´å‡è¡¡æ¯ä¸ªæ ‡è®°çš„è§„æ¨¡ï¼ŒåŒæ—¶ä»ç„¶å…è®¸è¾ƒé•¿çš„åºåˆ—å¯¹æ€»æ¢¯åº¦æœ‰æ›´å¤§çš„è´¡çŒ®ï¼Œå› ä¸ºå®ƒä»¬åŒ…å«æ›´å¤šçš„æ´»è·ƒæ ‡è®°ã€‚
- en: Note that `completion_mask` in the code above is a matrix of 1s and 0s, where
    the prompt tokens are masked out (0s) because we donâ€™t want the model to learn
    from predicting prompt tokens.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œä¸Šè¿°ä»£ç ä¸­çš„`completion_mask`æ˜¯ä¸€ä¸ªç”± 1 å’Œ 0 ç»„æˆçš„çŸ©é˜µï¼Œå…¶ä¸­æç¤ºæ ‡è®°è¢«å±è”½ï¼ˆ0sï¼‰ï¼Œå› ä¸ºæˆ‘ä»¬ä¸å¸Œæœ›æ¨¡å‹ä»é¢„æµ‹æç¤ºæ ‡è®°ä¸­å­¦ä¹ ã€‚
- en: Why does this matter?
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è¿™ä¸ºä»€ä¹ˆå¾ˆé‡è¦ï¼Ÿ
- en: 'Intuitively, per-sequence normalization (Strategy 1) seems best since we care
    about *outcomes*, not individual tokens. However, this introduces subtle biases
    based on sequence length, which can cause the model to overthink of down-weight
    strategies that naturally need to use more tokens, depending on the direction
    of the bias. Consider two sequences of different lengths with per-token losses:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ç›´è§‚æ¥çœ‹ï¼ŒæŒ‰åºåˆ—å½’ä¸€åŒ–ï¼ˆç­–ç•¥ 1ï¼‰ä¼¼ä¹æœ€å¥½ï¼Œå› ä¸ºæˆ‘ä»¬å…³æ³¨çš„æ˜¯*ç»“æœ*ï¼Œè€Œä¸æ˜¯å•ä¸ªæ ‡è®°ã€‚ç„¶è€Œï¼Œè¿™ä¼šå¼•å…¥åŸºäºåºåˆ—é•¿åº¦çš„å¾®å¦™åå·®ï¼Œå¯èƒ½å¯¼è‡´æ¨¡å‹è¿‡åº¦è€ƒè™‘éœ€è¦ä½¿ç”¨æ›´å¤šæ ‡è®°çš„è‡ªç„¶é™æƒç­–ç•¥ï¼Œè¿™å–å†³äºåå·®çš„æ–¹å‘ã€‚è€ƒè™‘ä¸¤ä¸ªä¸åŒé•¿åº¦çš„åºåˆ—åŠå…¶æ¯æ ‡è®°æŸå¤±ï¼š
- en: '[PRE4]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'With **Strategy 1** (per-sequence): The batch loss is <semantics><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>2.8</mn><mo>+</mo><mn>1.9</mn><mo stretchy="false"
    form="postfix">)</mo><mi>/</mi><mn>2</mn><mo>=</mo><mn>2.35</mn></mrow><annotation
    encoding="application/x-tex">(2.8 + 1.9)/2 = 2.35</annotation></semantics>, and
    crucially, each token in the short sequence receives a larger gradient than tokens
    in the long sequence.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨**ç­–ç•¥ 1**ï¼ˆæŒ‰åºåˆ—ï¼‰ï¼šæ‰¹æŸå¤±ä¸º<semantics><mrow><mo stretchy="false" form="prefix">(</mo><mn>2.8</mn><mo>+</mo><mn>1.9</mn><mo
    stretchy="false" form="postfix">)</mo><mi>/</mi><mn>2</mn><mo>=</mo><mn>2.35</mn></mrow><annotation
    encoding="application/x-tex">(2.8 + 1.9)/2 = 2.35</annotation></semantics>ï¼Œå¹¶ä¸”å…³é”®çš„æ˜¯ï¼ŒçŸ­åºåˆ—ä¸­çš„æ¯ä¸ªæ ‡è®°æ¯”é•¿åºåˆ—ä¸­çš„æ ‡è®°æ¥æ”¶åˆ°çš„æ¢¯åº¦æ›´å¤§ã€‚
- en: 'With **Strategy 2** (per-token): The batch loss is <semantics><mrow><mo stretchy="false"
    form="prefix">(</mo><mn>14</mn><mo>+</mo><mn>19</mn><mo stretchy="false" form="postfix">)</mo><mi>/</mi><mn>15</mn><mo>=</mo><mn>2.2</mn></mrow><annotation
    encoding="application/x-tex">(14 + 19)/15 = 2.2</annotation></semantics>, and
    all tokens receive equal gradient magnitude.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨**ç­–ç•¥ 2**ï¼ˆæŒ‰æ ‡è®°ï¼‰ï¼šæ‰¹æŸå¤±ä¸º<semantics><mrow><mo stretchy="false" form="prefix">(</mo><mn>14</mn><mo>+</mo><mn>19</mn><mo
    stretchy="false" form="postfix">)</mo><mi>/</mi><mn>15</mn><mo>=</mo><mn>2.2</mn></mrow><annotation
    encoding="application/x-tex">(14 + 19)/15 = 2.2</annotation></semantics>ï¼Œå¹¶ä¸”æ‰€æœ‰æ ‡è®°éƒ½æ¥æ”¶åˆ°äº†ç›¸åŒçš„æ¢¯åº¦å¹…åº¦ã€‚
- en: 'With **Strategy 3** (fixed-length with <semantics><mrow><msub><mi>L</mi><mi
    mathvariant="normal">max</mi></msub><mo>=</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">L_{\max}=10</annotation></semantics>):
    The short sequence contributes <semantics><mn>1.4</mn><annotation encoding="application/x-tex">1.4</annotation></semantics>
    and the long sequence contributes <semantics><mn>1.9</mn><annotation encoding="application/x-tex">1.9</annotation></semantics>,
    balancing per-token gradients while still weighting by sequence.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨**ç­–ç•¥ 3**ï¼ˆå›ºå®šé•¿åº¦ï¼Œ<semantics><mrow><msub><mi>L</mi><mi mathvariant="normal">max</mi></msub><mo>=</mo><mn>10</mn></mrow><annotation
    encoding="application/x-tex">L_{\max}=10</annotation></semantics>ï¼‰ï¼šçŸ­åºåˆ—è´¡çŒ®äº†<semantics><mn>1.4</mn><annotation
    encoding="application/x-tex">1.4</annotation></semantics>ï¼Œé•¿åºåˆ—è´¡çŒ®äº†<semantics><mn>1.9</mn><annotation
    encoding="application/x-tex">1.9</annotation></semantics>ï¼Œåœ¨å¹³è¡¡æ¯è¯æ¢¯åº¦çš„åŒæ—¶ä»ç„¶æŒ‰åºåˆ—è¿›è¡ŒåŠ æƒã€‚
- en: For a more complete example showing how these strategies affect gradients, see
    the script below.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: æƒ³è¦æ›´å®Œæ•´åœ°äº†è§£è¿™äº›ç­–ç•¥å¦‚ä½•å½±å“æ¢¯åº¦ï¼Œè¯·å‚é˜…ä¸‹é¢çš„è„šæœ¬ã€‚
- en: '[PRE5]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The output shows that with Strategy 1 (`masked_mean`), the short sequence has
    larger per-token gradients (0.25) than the long sequence (0.14). Strategies 2
    and 3 equalize the per-token gradients across sequences. Note that these results
    can vary substantially if gradient accumulation is used, where the gradients are
    summed across multiple minibatches before taking a backward stepâ€”in this case,
    the balance between shorter and longer sequences can flip.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºæ˜¾ç¤ºï¼Œä½¿ç”¨ç­–ç•¥ 1ï¼ˆ`masked_mean`ï¼‰ï¼ŒçŸ­åºåˆ—çš„æ¯è¯æ¢¯åº¦ï¼ˆ0.25ï¼‰æ¯”é•¿åºåˆ—ï¼ˆ0.14ï¼‰å¤§ã€‚ç­–ç•¥ 2 å’Œ 3 åœ¨åºåˆ—é—´å‡ç­‰åŒ–äº†æ¯è¯æ¢¯åº¦ã€‚è¯·æ³¨æ„ï¼Œå¦‚æœä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼Œè¿™äº›ç»“æœå¯èƒ½ä¼šæœ‰å¾ˆå¤§å·®å¼‚ï¼Œå…¶ä¸­æ¢¯åº¦åœ¨åå‘æ­¥éª¤ä¹‹å‰è·¨å¤šä¸ªå°æ‰¹é‡æ±‚å’Œâ€”â€”åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒçŸ­åºåˆ—å’Œé•¿åºåˆ—ä¹‹é—´çš„å¹³è¡¡å¯èƒ½ä¼šç¿»è½¬ã€‚
- en: In practice, the best strategy depends on the specific training setup. Often
    in RLHF the method with the best numerical stability or the least variance in
    loss is preferred.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®è·µä¸­ï¼Œæœ€ä½³ç­–ç•¥å–å†³äºå…·ä½“çš„è®­ç»ƒè®¾ç½®ã€‚åœ¨ RLHF ä¸­ï¼Œé€šå¸¸æ›´å€¾å‘äºå…·æœ‰æœ€ä½³æ•°å€¼ç¨³å®šæ€§æˆ–æŸå¤±æ–¹å·®æœ€å°çš„æ–¹æ³•ã€‚
- en: 'Related: MDP vs Bandit Framing'
  id: totrans-222
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç›¸å…³ï¼šMDP ä¸ Bandit æ¡†æ¶
- en: The choice of loss aggregation connects to a deeper distinction in how we frame
    the RL problem. The **MDP (token-level)** view treats each token <semantics><msub><mi>a</mi><mi>t</mi></msub><annotation
    encoding="application/x-tex">a_t</annotation></semantics> as an action with state
    <semantics><msub><mi>s</mi><mi>t</mi></msub><annotation encoding="application/x-tex">s_t</annotation></semantics>
    being the running prefix. In practice, this is the framing used when we compute
    token-level advantages with a learned value function <semantics><mrow><mi>V</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">V(s_t)</annotation></semantics>
    (e.g., GAE [[179]](ch021.xhtml#ref-schulman2015high)) and apply KL penalties per
    token. PPO with a learned value network is the canonical example [[183]](ch021.xhtml#ref-schulman2017proximal).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: æŸå¤±èšåˆçš„é€‰æ‹©ä¸æˆ‘ä»¬å¯¹å¼ºåŒ–å­¦ä¹ é—®é¢˜æ¡†æ¶çš„æ›´æ·±å±‚æ¬¡åŒºåˆ«ç›¸å…³ã€‚**MDPï¼ˆæ ‡è®°çº§ï¼‰** è§†å›¾å°†æ¯ä¸ªæ ‡è®° <semantics><msub><mi>a</mi><mi>t</mi></msub><annotation
    encoding="application/x-tex">a_t</annotation></semantics> è§†ä¸ºä¸€ä¸ªåŠ¨ä½œï¼ŒçŠ¶æ€ <semantics><msub><mi>s</mi><mi>t</mi></msub><annotation
    encoding="application/x-tex">s_t</annotation></semantics> æ˜¯è¿è¡Œçš„å‰ç¼€ã€‚åœ¨å®è·µä¸­ï¼Œå½“æˆ‘ä»¬ä½¿ç”¨å­¦ä¹ åˆ°çš„å€¼å‡½æ•°
    <semantics><mrow><mi>V</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">V(s_t)</annotation></semantics>ï¼ˆä¾‹å¦‚ï¼ŒGAE
    [[179]](ch021.xhtml#ref-schulman2015high)ï¼‰è®¡ç®—æ ‡è®°çº§ä¼˜åŠ¿å¹¶æŒ‰æ ‡è®°åº”ç”¨KLæƒ©ç½šæ—¶ï¼Œä½¿ç”¨çš„æ˜¯è¿™ç§æ¡†æ¶ã€‚PPOä½¿ç”¨å­¦ä¹ åˆ°çš„å€¼ç½‘ç»œæ˜¯å…¸å‹çš„ä¾‹å­
    [[183]](ch021.xhtml#ref-schulman2017proximal)ã€‚
- en: In contrast, the **bandit (sequence-level)** view treats the whole completion
    as a single action with one scalar reward <semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics>.
    In code, this means computing a sequence-level advantage <semantics><msub><mi>A</mi><mtext
    mathvariant="normal">seq</mtext></msub><annotation encoding="application/x-tex">A_{\text{seq}}</annotation></semantics>
    and broadcasting it to all tokens. RLOO and GRPO-style advantages are often used
    in this bandit-style setting [[182]](ch021.xhtml#ref-kool2019buy) [[178]](ch021.xhtml#ref-ahmadian2024back)
    [[188]](ch021.xhtml#ref-shao2024deepseekmath). Direct alignment methods like DPO
    and A-LoL also define sequence-level objectives, although they are not policy-gradient
    estimators [[194]](ch021.xhtml#ref-baheti2023leftover).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸æ¯”ä¹‹ä¸‹ï¼Œ**å¸¦å¼ï¼ˆåºåˆ—çº§ï¼‰** è§†å›¾å°†æ•´ä¸ªå®Œæˆè§†ä¸ºä¸€ä¸ªå•ä¸€çš„åŠ¨ä½œï¼Œå…·æœ‰ä¸€ä¸ªæ ‡é‡å¥–åŠ± <semantics><mi>R</mi><annotation
    encoding="application/x-tex">R</annotation></semantics>ã€‚åœ¨ä»£ç ä¸­ï¼Œè¿™æ„å‘³ç€è®¡ç®—åºåˆ—çº§ä¼˜åŠ¿ <semantics><msub><mi>A</mi><mtext
    mathvariant="normal">seq</mtext></msub><annotation encoding="application/x-tex">A_{\text{seq}}</annotation></semantics>
    å¹¶å°†å…¶å¹¿æ’­åˆ°æ‰€æœ‰æ ‡è®°ã€‚RLOOå’ŒGRPOé£æ ¼çš„ä¼˜ç‚¹é€šå¸¸ç”¨äºè¿™ç§å¸¦å¼è®¾ç½® [[182]](ch021.xhtml#ref-kool2019buy) [[178]](ch021.xhtml#ref-ahmadian2024back)
    [[188]](ch021.xhtml#ref-shao2024deepseekmath)ã€‚ç›´æ¥å¯¹é½æ–¹æ³•å¦‚DPOå’ŒA-LoLä¹Ÿå®šä¹‰åºåˆ—çº§ç›®æ ‡ï¼Œå°½ç®¡å®ƒä»¬ä¸æ˜¯ç­–ç•¥æ¢¯åº¦ä¼°è®¡å™¨
    [[194]](ch021.xhtml#ref-baheti2023leftover)ã€‚
- en: Note that many GRPO implementations use a bandit-style advantage *and* add a
    separate per-token KL term in the loss, while many PPO/RLOO implementations fold
    KL into the reward before computing advantages; both conventions exist in practice.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œè®¸å¤šGRPOå®ç°ä½¿ç”¨å¸¦å¼ä¼˜åŠ¿ *å¹¶ä¸”* åœ¨æŸå¤±ä¸­æ·»åŠ ä¸€ä¸ªå•ç‹¬çš„æŒ‰æ ‡è®°KLé¡¹ï¼Œè€Œè®¸å¤šPPO/RLOOå®ç°åˆ™åœ¨è®¡ç®—ä¼˜åŠ¿ä¹‹å‰å°†KLæŠ˜å åˆ°å¥–åŠ±ä¸­ï¼›è¿™ä¸¤ç§æƒ¯ä¾‹åœ¨å®è·µä¸­éƒ½å­˜åœ¨ã€‚
- en: Asynchronicity
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¼‚æ­¥æ€§
- en: The default implementation for policy-gradient algorithms is what is called
    **on-policy** execution, where the actions (generations) taken by the agent (language
    model) are scored before updating the model. The theoretical derivations of policy-gradient
    rely on all actions being exactly on-policy where the model is always up to date
    with the results from the latest trials/roll-outs. In practice, maintaining exact
    on-policy execution substantially slows training [[195]](ch021.xhtml#ref-noukhovitch2024asynchronous)â€”and
    perfect synchronization is technically impossible regardless. Therefore, all of
    the recent empirical results with language models tend to be slightly outside
    of the theoretical proofs. What happens in practice is designing the algorithms
    and systems for what actually works.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: æ”¿ç­–æ¢¯åº¦ç®—æ³•çš„é»˜è®¤å®ç°è¢«ç§°ä¸º**æŒ‰ç­–ç•¥**æ‰§è¡Œï¼Œå…¶ä¸­åœ¨æ›´æ–°æ¨¡å‹ä¹‹å‰ï¼Œå¯¹ä»£ç†ï¼ˆè¯­è¨€æ¨¡å‹ï¼‰é‡‡å–çš„åŠ¨ä½œï¼ˆç”Ÿæˆï¼‰è¿›è¡Œè¯„åˆ†ã€‚æ”¿ç­–æ¢¯åº¦çš„ç†è®ºæ¨å¯¼ä¾èµ–äºæ‰€æœ‰åŠ¨ä½œéƒ½å®Œå…¨æŒ‰ç­–ç•¥è¿›è¡Œï¼Œå³æ¨¡å‹å§‹ç»ˆä¸æœ€æ–°è¯•éªŒ/å±•å¼€çš„ç»“æœä¿æŒæœ€æ–°ã€‚åœ¨å®è·µä¸­ï¼Œä¿æŒå®Œå…¨æŒ‰ç­–ç•¥æ‰§è¡Œä¼šæ˜¾è‘—å‡æ…¢è®­ç»ƒ
    [[195]](ch021.xhtml#ref-noukhovitch2024asynchronous)â€”â€”è€Œä¸”å®Œç¾çš„åŒæ­¥åœ¨æŠ€æœ¯ä¸Šæ˜¯ä¸å¯èƒ½çš„ã€‚å› æ­¤ï¼Œæ‰€æœ‰æœ€è¿‘å…³äºè¯­è¨€æ¨¡å‹çš„å®è¯ç»“æœéƒ½ç•¥åç¦»ç†è®ºè¯æ˜ã€‚åœ¨å®è·µä¸­å‘ç”Ÿçš„æ˜¯è®¾è®¡å®é™…å·¥ä½œçš„ç®—æ³•å’Œç³»ç»Ÿã€‚
- en: '![Figure 16: A comparison of the generation-update phases for synchronous or
    asynchronous RL training following Noukhovitch et al.Â 2024.](../media/file14.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾16ï¼šæ ¹æ®Noukhovitchç­‰äºº2024å¹´çš„ç ”ç©¶ï¼ŒåŒæ­¥æˆ–å¼‚æ­¥RLè®­ç»ƒçš„ç”Ÿæˆ-æ›´æ–°é˜¶æ®µçš„æ¯”è¾ƒ](../media/file14.png)'
- en: 'Figure 16: A comparison of the generation-update phases for synchronous or
    asynchronous RL training following Noukhovitch et al.Â 2024.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾16ï¼šæ ¹æ®Noukhovitchç­‰äºº2024å¹´çš„ç ”ç©¶ï¼ŒåŒæ­¥æˆ–å¼‚æ­¥å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„ç”Ÿæˆå’Œæ›´æ–°é˜¶æ®µçš„æ¯”è¾ƒã€‚
- en: The common solution used is to constantly run inference and training on separate
    GPU nodes with software designed to efficiently run both, as shown in the bottom
    of fig.Â [16](#fig:async). Common practice in popular open-source RL tools for
    language models is to use a distributed process management library such as Ray
    to hand information off between the policy-gradient learning loop and the inference
    loop using an efficient inference engine, e.g., VLLM. In these setups, the GPUs
    dedicated to taking the RL steps are called the â€œleanersâ€ and the GPUs dedicated
    to sampling from the language model are called the â€œactorsâ€ The primary challenges
    faced when making training more asynchronous are keeping training stable and maintaining
    learning signal.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: å¸¸ç”¨çš„è§£å†³æ–¹æ¡ˆæ˜¯åœ¨ä¸åŒçš„GPUèŠ‚ç‚¹ä¸Šä¸æ–­è¿è¡Œæ¨ç†å’Œè®­ç»ƒï¼Œä½¿ç”¨è½¯ä»¶æ¥é«˜æ•ˆåœ°è¿è¡Œä¸¤è€…ï¼Œå¦‚å›¾[16](#fig:async)åº•éƒ¨æ‰€ç¤ºã€‚åœ¨æµè¡Œçš„å¼€æºå¼ºåŒ–å­¦ä¹ å·¥å…·ä¸­ï¼Œå¯¹äºè¯­è¨€æ¨¡å‹ï¼Œå¸¸è§çš„åšæ³•æ˜¯ä½¿ç”¨åˆ†å¸ƒå¼è¿›ç¨‹ç®¡ç†åº“ï¼Œå¦‚Rayï¼Œé€šè¿‡é«˜æ•ˆçš„æ¨ç†å¼•æ“ï¼ˆä¾‹å¦‚VLLMï¼‰åœ¨ç­–ç•¥æ¢¯åº¦å­¦ä¹ å¾ªç¯å’Œæ¨ç†å¾ªç¯ä¹‹é—´ä¼ é€’ä¿¡æ¯ã€‚åœ¨è¿™äº›è®¾ç½®ä¸­ï¼Œä¸“é—¨ç”¨äºæ‰§è¡Œå¼ºåŒ–å­¦ä¹ æ­¥éª¤çš„GPUè¢«ç§°ä¸ºâ€œå­¦ä¹ è€…â€ï¼Œè€Œä¸“é—¨ç”¨äºä»è¯­è¨€æ¨¡å‹ä¸­é‡‡æ ·çš„GPUè¢«ç§°ä¸ºâ€œæ¼”å‘˜â€ã€‚åœ¨ä½¿è®­ç»ƒæ›´åŠ å¼‚æ­¥æ—¶é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯ä¿æŒè®­ç»ƒçš„ç¨³å®šæ€§å¹¶ç»´æŠ¤å­¦ä¹ ä¿¡å·ã€‚
- en: '![Figure 17: An example distributed RL system, where two queues are managed
    to pass data to the learner and actor GPUs, which can both be synchonized with
    a distributed computing library such as Ray. Olmo Team 2025, license CC-BY.](../media/file15.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾17ï¼šä¸€ä¸ªç¤ºä¾‹åˆ†å¸ƒå¼å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿï¼Œå…¶ä¸­ä¸¤ä¸ªé˜Ÿåˆ—è¢«ç®¡ç†ä»¥å°†æ•°æ®ä¼ é€’ç»™å­¦ä¹ è€…å’Œæ¼”å‘˜GPUï¼Œå®ƒä»¬éƒ½å¯ä»¥ä¸åˆ†å¸ƒå¼è®¡ç®—åº“ï¼ˆå¦‚Rayï¼‰åŒæ­¥ã€‚Olmoå›¢é˜Ÿ2025ï¼Œè®¸å¯CC-BYã€‚](../media/file15.png)'
- en: 'Figure 17: An example distributed RL system, where two queues are managed to
    pass data to the learner and actor GPUs, which can both be synchonized with a
    distributed computing library such as Ray. Olmo Team 2025, license CC-BY.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾17ï¼šä¸€ä¸ªç¤ºä¾‹åˆ†å¸ƒå¼å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿï¼Œå…¶ä¸­ä¸¤ä¸ªé˜Ÿåˆ—è¢«ç®¡ç†ä»¥å°†æ•°æ®ä¼ é€’ç»™å­¦ä¹ è€…å’Œæ¼”å‘˜GPUï¼Œå®ƒä»¬éƒ½å¯ä»¥ä¸åˆ†å¸ƒå¼è®¡ç®—åº“ï¼ˆå¦‚Rayï¼‰åŒæ­¥ã€‚Olmoå›¢é˜Ÿ2025ï¼Œè®¸å¯CC-BYã€‚
- en: These systems are designed and implemented with the presumption that nearly
    on-policy data is good enough for stable learning. Here, the generation and update
    phases can easily be synced to avoid idle compute on either piece of the training
    system, which would be passing model weights from the leaners to the actors in
    fig.Â [17](#fig:async_system). With reasoning models, the extremely long inference
    characteristics of problems requiring 10K to 100K+ tokens per answer makes the
    generation of roll-outs a far stronger bottleneck. A common problem when training
    reasoning models on more synchronous RL infrastructure is that an answer to one
    prompt in the batch can take substantially more time to generate (either through
    more tokens or more tool calls), resulting in the majority of the allocated compute
    being idle until it completes. A second solution to this length mismatch issue,
    called sequence-level packing, is to stack shorter samples within a batch with
    clever masking to enable continued roll-outs from the model and better distribute
    length normalization across samples within a batch. The full complexity of distributed
    RL infrastructure is out of scope for this book, as it can cause many other subtle
    issues that slow down training or cause instability.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›ç³»ç»Ÿè¢«è®¾è®¡å’Œå®ç°çš„å‰ææ˜¯ï¼Œå‡ ä¹ç¬¦åˆç­–ç•¥çš„æ•°æ®å¯¹äºç¨³å®šå­¦ä¹ æ¥è¯´å·²ç»è¶³å¤Ÿå¥½ã€‚åœ¨è¿™é‡Œï¼Œç”Ÿæˆå’Œæ›´æ–°é˜¶æ®µå¯ä»¥è½»æ¾åŒæ­¥ï¼Œä»¥é¿å…è®­ç»ƒç³»ç»Ÿä»»ä¸€éƒ¨åˆ†çš„ç©ºé—²è®¡ç®—ï¼Œå¦‚å›¾[17](#fig:async_system)ä¸­æ‰€ç¤ºï¼Œå³ä»å­¦ä¹ è€…å‘æ¼”å‘˜ä¼ é€’æ¨¡å‹æƒé‡ã€‚åœ¨éœ€è¦æ¯ä¸ªç­”æ¡ˆ10Kåˆ°100K+ä¸ªtokençš„é—®é¢˜ä¸­ï¼Œæ¨ç†çš„æé•¿ç‰¹æ€§ä½¿å¾—ç”Ÿæˆroll-outæˆä¸ºä¸€ä¸ªè¿œæ›´å¼ºçš„ç“¶é¢ˆã€‚åœ¨æ›´åŒæ­¥çš„å¼ºåŒ–å­¦ä¹ åŸºç¡€è®¾æ–½ä¸Šè®­ç»ƒæ¨ç†æ¨¡å‹æ—¶é‡åˆ°çš„ä¸€ä¸ªå¸¸è§é—®é¢˜æ˜¯ï¼Œæ‰¹å¤„ç†ä¸­çš„ä¸€ä¸ªæç¤ºçš„å›ç­”å¯èƒ½éœ€è¦æ›´å¤šçš„æ—¶é—´æ¥ç”Ÿæˆï¼ˆæ— è®ºæ˜¯é€šè¿‡æ›´å¤šçš„tokenè¿˜æ˜¯æ›´å¤šçš„å·¥å…·è°ƒç”¨ï¼‰ï¼Œå¯¼è‡´å¤§éƒ¨åˆ†åˆ†é…çš„è®¡ç®—èµ„æºåœ¨å®Œæˆä¹‹å‰éƒ½æ˜¯ç©ºé—²çš„ã€‚è§£å†³è¿™ç§é•¿åº¦ä¸åŒ¹é…é—®é¢˜çš„ç¬¬äºŒç§è§£å†³æ–¹æ¡ˆï¼Œç§°ä¸ºåºåˆ—çº§æ‰“åŒ…ï¼Œæ˜¯é€šè¿‡å·§å¦™çš„æ©ç åœ¨æ‰¹æ¬¡ä¸­å †å è¾ƒçŸ­çš„æ ·æœ¬ï¼Œä»¥ä½¿æ¨¡å‹èƒ½å¤Ÿç»§ç»­ç”Ÿæˆroll-outï¼Œå¹¶åœ¨æ‰¹æ¬¡å†…çš„æ ·æœ¬ä¹‹é—´æ›´å¥½åœ°åˆ†é…é•¿åº¦å½’ä¸€åŒ–ã€‚åˆ†å¸ƒå¼å¼ºåŒ–å­¦ä¹ åŸºç¡€è®¾æ–½çš„å®Œæ•´å¤æ‚æ€§è¶…å‡ºäº†æœ¬ä¹¦çš„èŒƒå›´ï¼Œå› ä¸ºå®ƒå¯èƒ½å¯¼è‡´è®¸å¤šå…¶ä»–å¾®å¦™çš„é—®é¢˜ï¼Œè¿™äº›é—®é¢˜ä¼šå‡æ…¢è®­ç»ƒé€Ÿåº¦æˆ–å¯¼è‡´ä¸ç¨³å®šã€‚
- en: Following the emergence of these reasoning models, further interest has been
    taken to make the training and inference loops fully off-policy, where training
    batches for the policy gradient updates are filled with the most recently completed
    roll-outs across multiple instances generating answers [[196]](ch021.xhtml#ref-wu2025llamarl)
    [[197]](ch021.xhtml#ref-fu2025areal). Fully asynchronous training would also enable
    scaling RL training runs across multiple datacenters more easily due to the option
    of increasing the time between weight syncs between the learner node (taking policy
    gradient steps) and the actor (trying to solve problems) [[198]](ch021.xhtml#ref-primeintellectteam2025intellect2reasoningmodeltrained).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€è¿™äº›æ¨ç†æ¨¡å‹çš„å‡ºç°ï¼Œè¿›ä¸€æ­¥çš„å…´è¶£åœ¨äºä½¿è®­ç»ƒå’Œæ¨ç†å¾ªç¯å®Œå…¨ç¦»çº¿ï¼Œå…¶ä¸­ç­–ç•¥æ¢¯åº¦æ›´æ–°çš„è®­ç»ƒæ‰¹æ¬¡å¡«å……äº†å¤šä¸ªå®ä¾‹ç”Ÿæˆç­”æ¡ˆçš„æœ€æ–°å®Œæˆçš„å›æ»š [[196]](ch021.xhtml#ref-wu2025llamarl)
    [[197]](ch021.xhtml#ref-fu2025areal)ã€‚å®Œå…¨å¼‚æ­¥è®­ç»ƒè¿˜å¯ä»¥é€šè¿‡å¢åŠ å­¦ä¹ èŠ‚ç‚¹ï¼ˆæ‰§è¡Œç­–ç•¥æ¢¯åº¦æ­¥éª¤ï¼‰å’Œæ¼”å‘˜ï¼ˆè¯•å›¾è§£å†³é—®é¢˜ï¼‰ä¹‹é—´æƒé‡åŒæ­¥çš„æ—¶é—´æ¥æ›´å®¹æ˜“åœ°æ‰©å±•
    RL è®­ç»ƒè¿è¡Œ [[198]](ch021.xhtml#ref-primeintellectteam2025intellect2reasoningmodeltrained)ã€‚
- en: Related methods are exploring fully off-policy policy gradient algorithms [[199]](ch021.xhtml#ref-roux2025tapered).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸å…³æ–¹æ³•æ­£åœ¨æ¢ç´¢å®Œå…¨ç¦»çº¿çš„ç­–ç•¥æ¢¯åº¦ç®—æ³• [[199]](ch021.xhtml#ref-roux2025tapered)ã€‚
- en: Proximal Policy Optimization
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–
- en: There are many, many implementations of PPO available. The core *loss* computation
    is shown below. Crucial to stable performance is also the *value* computation,
    where multiple options exist (including multiple options for the *value model*
    loss).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: PPO æœ‰è®¸å¤šå®ç°ã€‚æ ¸å¿ƒ *æŸå¤±* è®¡ç®—å¦‚ä¸‹æ‰€ç¤ºã€‚å¯¹äºç¨³å®šæ€§èƒ½è‡³å…³é‡è¦çš„æ˜¯ *ä»·å€¼* è®¡ç®—ï¼Œå…¶ä¸­å­˜åœ¨å¤šç§é€‰æ‹©ï¼ˆåŒ…æ‹¬ *ä»·å€¼æ¨¡å‹* æŸå¤±çš„å¤šç§é€‰æ‹©ï¼‰ã€‚
- en: Note that the reference policy (or old logprobs) here are from the time the
    generations were sampled and not necessarily the reference policy. The reference
    policy is only used for the KL distance constraint/penalty.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œè¿™é‡Œçš„å‚è€ƒç­–ç•¥ï¼ˆæˆ–æ—§çš„ logprobsï¼‰æ˜¯ä»ç”Ÿæˆæ ·æœ¬çš„æ—¶é—´ç‚¹æ¥çš„ï¼Œå¹¶ä¸ä¸€å®šæ˜¯å‚è€ƒç­–ç•¥ã€‚å‚è€ƒç­–ç•¥ä»…ç”¨äº KL è·ç¦»çº¦æŸ/æƒ©ç½šã€‚
- en: '[PRE6]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The core piece to understand with PPO is how the policy gradient loss is updated.
    Focus on these three lines:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: ç†è§£ PPO çš„æ ¸å¿ƒæ˜¯ç­–ç•¥æ¢¯åº¦æŸå¤±å¦‚ä½•æ›´æ–°ã€‚å…³æ³¨ä»¥ä¸‹ä¸‰è¡Œï¼š
- en: '[PRE7]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '`pg_losses1` is the vanilla advantage-weighted policy gradient loss. `pg_losses2`
    applies the same formula but with the probability ratio clamped to the range <semantics><mrow><mo
    stretchy="false" form="prefix">[</mo><mn>1</mn><mo>âˆ’</mo><mi>Îµ</mi><mo>,</mo><mn>1</mn><mo>+</mo><mi>Îµ</mi><mo
    stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[1-\varepsilon,
    1+\varepsilon]</annotation></semantics>, limiting how much the policy can change
    in a single update.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '`pg_losses1` æ˜¯åŸºæœ¬çš„åŠ æƒç­–ç•¥æ¢¯åº¦æŸå¤±ã€‚`pg_losses2` åº”ç”¨ç›¸åŒçš„å…¬å¼ï¼Œä½†å°†æ¦‚ç‡æ¯”é™åˆ¶åœ¨ <semantics><mrow><mo
    stretchy="false" form="prefix">[</mo><mn>1</mn><mo>âˆ’</mo><mi>Îµ</mi><mo>,</mo><mn>1</mn><mo>+</mo><mi>Îµ</mi><mo
    stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[1-\varepsilon,
    1+\varepsilon]</annotation></semantics> èŒƒå›´å†…ï¼Œé™åˆ¶ç­–ç•¥åœ¨å•ä¸ªæ›´æ–°ä¸­å¯ä»¥æ”¹å˜çš„ç¨‹åº¦ã€‚'
- en: The key insight is taking `torch.max` of the two losses. Because weâ€™re minimizing
    a *negative* loss (recall the negative sign in front of advantages), taking the
    maximum selects the more pessimistic gradientâ€”the one that produces a smaller
    policy update. When the advantage is positive (good action), clipping prevents
    the policy from increasing that actionâ€™s probability too aggressively. When the
    advantage is negative (bad action), clipping prevents over-correction in the other
    direction.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: å…³é”®çš„æ´å¯Ÿæ˜¯å–ä¸¤ä¸ªæŸå¤±çš„ `torch.max`ã€‚å› ä¸ºæˆ‘ä»¬æœ€å°åŒ–çš„æ˜¯ *è´Ÿ* æŸå¤±ï¼ˆå›æƒ³ä¸€ä¸‹ä¼˜åŠ¿å‰çš„è´Ÿå·ï¼‰ï¼Œå–æœ€å¤§å€¼ä¼šé€‰æ‹©æ›´æ‚²è§‚çš„æ¢¯åº¦â€”â€”äº§ç”Ÿè¾ƒå°ç­–ç•¥æ›´æ–°çš„é‚£ä¸ªã€‚å½“ä¼˜åŠ¿ä¸ºæ­£ï¼ˆå¥½çš„åŠ¨ä½œï¼‰æ—¶ï¼Œè£å‰ªå¯ä»¥é˜²æ­¢ç­–ç•¥è¿‡äºæ¿€è¿›åœ°å¢åŠ è¯¥åŠ¨ä½œçš„æ¦‚ç‡ã€‚å½“ä¼˜åŠ¿ä¸ºè´Ÿï¼ˆåçš„åŠ¨ä½œï¼‰æ—¶ï¼Œè£å‰ªå¯ä»¥é˜²æ­¢è¿‡åº¦çº æ­£ã€‚
- en: By clamping the log-probability ratio, PPO bounds how far the policy can drift
    from the version that generated the training data, stabilizing learning without
    requiring an explicit trust region computation.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è£å‰ªå¯¹æ•°æ¦‚ç‡æ¯”ï¼ŒPPO é™åˆ¶äº†ç­–ç•¥å¯ä»¥ä»ç”Ÿæˆè®­ç»ƒæ•°æ®çš„ç‰ˆæœ¬åç¦»å¤šè¿œï¼Œç¨³å®šå­¦ä¹ è€Œä¸éœ€è¦æ˜¾å¼åœ°è®¡ç®—ä¿¡ä»»åŒºåŸŸã€‚
- en: The code above also shows PPO learning a value function alongside the policy,
    which adds implementation complexity, but the clipped objective is the core mechanism.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°ä»£ç è¿˜æ˜¾ç¤ºäº† PPO åœ¨ç­–ç•¥çš„åŒæ—¶å­¦ä¹ ä»·å€¼å‡½æ•°ï¼Œè¿™å¢åŠ äº†å®ç°å¤æ‚æ€§ï¼Œä½†è£å‰ªçš„ç›®æ ‡æ˜¯æ ¸å¿ƒæœºåˆ¶ã€‚
- en: PPO/GRPO simplification with 1 gradient step per sample (no clipping)
  id: totrans-246
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ¯ä¸ªæ ·æœ¬ä½¿ç”¨ 1 ä¸ªæ¢¯åº¦æ­¥éª¤çš„ PPO/GRPO ç®€åŒ–ï¼ˆæ— è£å‰ªï¼‰
- en: 'PPO (and GRPO) implementations can be handled much more elegantly if the hyperparameter
    â€œnumber of gradient steps per sampleâ€ is equal to 1\. Many typical values for
    this are from 2-4 or higher. In the main PPO or GRPO equations, see eq.Â [54](ch011.xhtml#eq:PPO_EQN),
    the â€œreferenceâ€ policy is the previous parameters â€“ those used to generate the
    completions or actions. Thus, if only one gradient step is taken, <semantics><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo>=</mo><msub><mi>Ï€</mi><msub><mi>Î¸</mi><mtext
    mathvariant="normal">old</mtext></msub></msub></mrow><annotation encoding="application/x-tex">\pi_\theta
    = \pi_{\theta_{\text{old}}}</annotation></semantics>, and the update rule reduces
    to the following (the notation <semantics><mrow><mo stretchy="false" form="prefix">[</mo><msub><mo
    stretchy="false" form="postfix">]</mo><mi>âˆ‡</mi></msub></mrow><annotation encoding="application/x-tex">[]_\nabla</annotation></semantics>
    indicates a stop gradient):'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœè¶…å‚æ•°â€œæ¯ä¸ªæ ·æœ¬çš„æ¢¯åº¦æ­¥æ•°â€ç­‰äº 1ï¼Œé‚£ä¹ˆ PPOï¼ˆå’Œ GRPOï¼‰çš„å®ç°å¯ä»¥å¤„ç†å¾—æ›´åŠ ä¼˜é›…ã€‚è®¸å¤šå…¸å‹çš„å€¼èŒƒå›´ä» 2-4 æˆ–æ›´é«˜ã€‚åœ¨ä¸»è¦çš„ PPO æˆ–
    GRPO æ–¹ç¨‹ä¸­ï¼Œå‚è§æ–¹ç¨‹ [54](ch011.xhtml#eq:PPO_EQN)ï¼Œâ€œå‚è€ƒâ€ç­–ç•¥æ˜¯å…ˆå‰çš„å‚æ•°â€”â€”ç”¨äºç”Ÿæˆè¡¥å…¨æˆ–åŠ¨ä½œçš„å‚æ•°ã€‚å› æ­¤ï¼Œå¦‚æœåªè¿›è¡Œä¸€æ­¥æ¢¯åº¦æ›´æ–°ï¼Œ<semantics><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo>=</mo><msub><mi>Ï€</mi><msub><mi>Î¸</mi><mtext
    mathvariant="normal">old</mtext></msub></msub></mrow><annotation encoding="application/x-tex">\pi_\theta
    = \pi_{\theta_{\text{old}}}</annotation></semantics>ï¼Œæ›´æ–°è§„åˆ™ç®€åŒ–ä¸ºä»¥ä¸‹å†…å®¹ï¼ˆç¬¦å· <semantics><mrow><mo
    stretchy="false" form="prefix">[</mo><msub><mo stretchy="false" form="postfix">]</mo><mi>âˆ‡</mi></msub></mrow><annotation
    encoding="application/x-tex">[]_\nabla</annotation></semantics> è¡¨ç¤ºåœæ­¢æ¢¯åº¦ï¼‰ï¼š
- en: <semantics><mrow><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>G</mi></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>G</mi></munderover><mrow><mo
    stretchy="true" form="prefix">(</mo><mfrac><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy="false"
    form="prefix">|</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><msub><mrow><mo
    stretchy="true" form="prefix">[</mo><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">|</mo><mi>s</mi><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow><mi>âˆ‡</mi></msub></mfrac><msub><mi>A</mi><mi>i</mi></msub><mo>âˆ’</mo><mi>Î²</mi><msub><mi>ğ’Ÿ</mi><mtext
    mathvariant="normal">KL</mtext></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">|</mo><mo stretchy="false" form="prefix">|</mo><msub><mi>Ï€</mi><mtext
    mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="postfix">)</mo><mo
    stretchy="true" form="postfix">)</mo></mrow><mi>.</mi><mrow><mo stretchy="false"
    form="prefix">(</mo><mn>64</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">J(\theta) = \frac{1}{G}\sum_{i=1}^G \left(\frac{\pi_\theta(a_i|s)}{\left[\pi_{\theta}(a_i|s)\right]_\nabla}A_i
    - \beta \mathcal{D}_{\text{KL}}(\pi_\theta||\pi_{\text{ref}})\right). \qquad{(64)}</annotation></semantics>
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>G</mi></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>G</mi></munderover><mrow><mo
    stretchy="true" form="prefix">(</mo><mfrac><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy="false"
    form="prefix">|</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><msub><mrow><mo
    stretchy="true" form="prefix">[</mo><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">|</mo><mi>s</mi><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow><mi>âˆ‡</mi></msub></mfrac><msub><mi>A</mi><mi>i</mi></msub><mo>âˆ’</mo><mi>Î²</mi><msub><mi>ğ’Ÿ</mi><mtext
    mathvariant="normal">KL</mtext></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">|</mo><mo stretchy="false" form="prefix">|</mo><msub><mi>Ï€</mi><mtext
    mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="postfix">)</mo><mo
    stretchy="true" form="postfix">)</mo></mrow><mi>.</mi><mrow><mo stretchy="false"
    form="prefix">(</mo><mn>64</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">J(\theta) = \frac{1}{G}\sum_{i=1}^G \left(\frac{\pi_\theta(a_i|s)}{\left[\pi_{\theta}(a_i|s)\right]_\nabla}A_i
    - \beta \mathcal{D}_{\text{KL}}(\pi_\theta||\pi_{\text{ref}})\right). \qquad{(64)}</annotation></semantics>
- en: This leads to PPO or GRPO implementations where the second policy gradient and
    clipping logic can be omitted, making the optimizer far closer to standard policy
    gradient.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯¼è‡´äº† PPO æˆ– GRPO çš„å®ç°ï¼Œå…¶ä¸­å¯ä»¥çœç•¥ç¬¬äºŒä¸ªç­–ç•¥æ¢¯åº¦å’Œè£å‰ªé€»è¾‘ï¼Œä½¿å¾—ä¼˜åŒ–å™¨æ›´æ¥è¿‘æ ‡å‡†çš„ç­–ç•¥æ¢¯åº¦ã€‚
- en: Group Relative Policy Optimization
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç¾¤ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–
- en: 'The DeepSeekMath paper describes some implementation details of GRPO that differ
    from PPO [[188]](ch021.xhtml#ref-shao2024deepseekmath), especially if comparing
    to a standard application of PPO from Deep RL rather than language models. For
    example, the KL penalty within the RLHF optimization (recall the KL penalty is
    also used when training reasoning models on verifiable rewards without a reward
    model) is applied directly in the loss update rather than to the reward function.
    Where the standard KL penalty application for RLHF is applied as <semantics><mrow><mi>r</mi><mo>=</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo>âˆ’</mo><mi>Î²</mi><msub><mi>ğ’Ÿ</mi><mtext
    mathvariant="normal">KL</mtext></msub></mrow><annotation encoding="application/x-tex">r=r_\theta
    - \beta \mathcal{D}_{\text{KL}}</annotation></semantics>, the GRPO implementation
    is along the lines of:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: DeepSeekMath è®ºæ–‡æè¿°äº† GRPO ä¸ PPO ä¸åŒçš„æŸäº›å®ç°ç»†èŠ‚ï¼Œç‰¹åˆ«æ˜¯ä¸ Deep RL ä¸­çš„æ ‡å‡† PPO åº”ç”¨ç›¸æ¯”ï¼Œè€Œä¸æ˜¯ä¸è¯­è¨€æ¨¡å‹ç›¸æ¯”ã€‚ä¾‹å¦‚ï¼Œåœ¨
    RLHF ä¼˜åŒ–ä¸­çš„ KL æƒ©ç½šï¼ˆå›æƒ³ä¸€ä¸‹ï¼Œå½“åœ¨æ²¡æœ‰å¥–åŠ±æ¨¡å‹çš„æƒ…å†µä¸‹åœ¨å¯éªŒè¯å¥–åŠ±ä¸Šè®­ç»ƒæ¨ç†æ¨¡å‹æ—¶ï¼Œä¹Ÿä½¿ç”¨äº† KL æƒ©ç½šï¼‰ç›´æ¥åº”ç”¨äºæŸå¤±æ›´æ–°ï¼Œè€Œä¸æ˜¯å¥–åŠ±å‡½æ•°ã€‚æ ‡å‡†
    RLHF çš„ KL æƒ©ç½šåº”ç”¨å¦‚ä¸‹ <semantics><mrow><mi>r</mi><mo>=</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo>âˆ’</mo><mi>Î²</mi><msub><mi>ğ’Ÿ</mi><mtext
    mathvariant="normal">KL</mtext></msub></mrow><annotation encoding="application/x-tex">r=r_\theta
    - \beta \mathcal{D}_{\text{KL}}</annotation></semantics>ï¼ŒGRPO çš„å®ç°æ–¹å¼å¦‚ä¸‹ï¼š
- en: <semantics><mrow><mi>L</mi><mo>=</mo><msub><mi>L</mi><mtext mathvariant="normal">policy
    gradient</mtext></msub><mo>+</mo><mi>Î²</mi><mo>*</mo><msub><mi>ğ’Ÿ</mi><mtext mathvariant="normal">KL</mtext></msub></mrow>
    <annotation encoding="application/x-tex">L = L_{\text{policy gradient}} + \beta
    * \mathcal{D}_{\text{KL}}</annotation></semantics>
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><mi>L</mi><mo>=</mo><msub><mi>L</mi><mtext mathvariant="normal">ç­–ç•¥æ¢¯åº¦</mtext></msub><mo>+</mo><mi>Î²</mi><mo>*</mo><msub><mi>ğ’Ÿ</mi><mtext
    mathvariant="normal">KL</mtext></msub></mrow> <annotation encoding="application/x-tex">L
    = L_{\text{ç­–ç•¥æ¢¯åº¦}} + \beta * \mathcal{D}_{\text{KL}}</annotation></semantics>
- en: Though, there are multiple ways to implement this. Traditionally, the KL distance
    is computed with respect to each token in the completion to a prompt <semantics><mi>s</mi><annotation
    encoding="application/x-tex">s</annotation></semantics>. For reasoning training,
    multiple completions are sampled from one prompt, and there are multiple prompts
    in one batch, so the KL distance will have a shape of [B, L, N], where B is the
    batch size, L is the sequence length, and N is the number of completions per prompt.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œæœ‰å¤šç§å®ç°æ–¹å¼ã€‚ä¼ ç»Ÿä¸Šï¼ŒKL è·ç¦»æ˜¯é’ˆå¯¹æ¯ä¸ªæç¤ºçš„å®Œæˆä¸­çš„æ¯ä¸ªæ ‡è®°è®¡ç®—çš„ã€‚å¯¹äºæ¨ç†è®­ç»ƒï¼Œä»ä¸€ä¸ªæç¤ºä¸­é‡‡æ ·å¤šä¸ªå®Œæˆï¼Œä¸€ä¸ªæ‰¹æ¬¡ä¸­æœ‰å¤šä¸ªæç¤ºï¼Œå› æ­¤ KL
    è·ç¦»çš„å½¢çŠ¶å°†æ˜¯ [B, L, N]ï¼Œå…¶ä¸­ B æ˜¯æ‰¹æ¬¡å¤§å°ï¼ŒL æ˜¯åºåˆ—é•¿åº¦ï¼ŒN æ˜¯æ¯ä¸ªæç¤ºçš„å®Œæˆæ•°é‡ã€‚
- en: Putting it together, using the first loss accumulation, the pseudocode can be
    written as below.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: å°†å…¶ç»„åˆï¼Œä½¿ç”¨ç¬¬ä¸€æ¬¡æŸå¤±ç´¯ç§¯ï¼Œä¼ªä»£ç å¯ä»¥å†™æˆå¦‚ä¸‹ã€‚
- en: '[PRE8]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'For more details on how to interpret this code, see the PPO section above.
    The core differences from the PPO example are:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºå¦‚ä½•è§£é‡Šæ­¤ä»£ç çš„æ›´å¤šç»†èŠ‚ï¼Œè¯·å‚é˜…ä¸Šé¢çš„ PPO éƒ¨åˆ†ã€‚ä¸ PPO ç¤ºä¾‹çš„æ ¸å¿ƒåŒºåˆ«æ˜¯ï¼š
- en: '**Advantage computation**: GRPO normalizes rewards relative to the group (mean
    and std across generations for the same prompt) rather than using a learned value
    function as baseline.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è®¡ç®—ä¼˜åŠ¿**: GRPOç›¸å¯¹äºç¾¤ä½“ï¼ˆå¯¹äºç›¸åŒæç¤ºçš„è·¨ä»£å‡å€¼å’Œæ ‡å‡†å·®ï¼‰å¯¹å¥–åŠ±è¿›è¡Œå½’ä¸€åŒ–ï¼Œè€Œä¸æ˜¯ä½¿ç”¨å­¦ä¹ åˆ°çš„ä»·å€¼å‡½æ•°ä½œä¸ºåŸºçº¿ã€‚'
- en: '**No value network**: GRPO removes the value model entirely, eliminating `vf_loss`
    and the associated complexity.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ— ä»·å€¼ç½‘ç»œ**: GRPOå®Œå…¨ç§»é™¤äº†ä»·å€¼æ¨¡å‹ï¼Œæ¶ˆé™¤äº†`vf_loss`åŠå…¶ç›¸å…³å¤æ‚æ€§ã€‚'
- en: '**KL penalty placement**: GRPO adds the KL penalty directly to the loss rather
    than subtracting it from the reward (this is the standard implementation, but
    more versions exist on how the KL is applied).'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**KL æƒ©ç½šæ”¾ç½®**: GRPOç›´æ¥å°† KL æƒ©ç½šæ·»åŠ åˆ°æŸå¤±ä¸­ï¼Œè€Œä¸æ˜¯ä»å¥–åŠ±ä¸­å‡å»ï¼ˆè¿™æ˜¯æ ‡å‡†å®ç°ï¼Œä½†å­˜åœ¨æ›´å¤šå…³äºå¦‚ä½•åº”ç”¨ KL çš„ç‰ˆæœ¬ï¼‰ã€‚'
- en: RLOO vs.Â GRPO
  id: totrans-260
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: RLOO ä¸ GRPO
- en: 'The advantage updates for RLOO follow very closely to GRPO, highlighting the
    conceptual similarity of the algorithm when taken separately from the PPO style
    clipping and KL penalty details. Specifically, for RLOO, the advantage is computed
    relative to a baseline that is extremely similar to that of GRPO â€“ the completion
    reward relative to the others for that same question. Concisely, the RLOO advantage
    estimate follows as (expanded from [TRL](https://github.com/huggingface/trl/blob/bfe20756082488350091352d1cdc19c172e42cd8/trl/trainer/rloo_trainer.py#L433)â€™s
    implementation):'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: RLOO çš„ä¼˜åŠ¿æ›´æ–°ä¸ GRPO éå¸¸ç›¸ä¼¼ï¼Œçªå‡ºäº†ç®—æ³•åœ¨å•ç‹¬è€ƒè™‘æ—¶ä¸ PPO é£æ ¼çš„å‰ªè¾‘å’Œ KL æƒ©ç½šç»†èŠ‚çš„æ¦‚å¿µç›¸ä¼¼æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äº RLOOï¼Œä¼˜åŠ¿æ˜¯ç›¸å¯¹äºä¸€ä¸ªä¸
    GRPO æå…¶ç›¸ä¼¼çš„åŸºç¡€å€¼æ¥è®¡ç®—çš„â€”â€”å³å¯¹äºç›¸åŒé—®é¢˜çš„å®Œæˆå¥–åŠ±ç›¸å¯¹äºå…¶ä»–å¥–åŠ±ã€‚ç®€è€Œè¨€ä¹‹ï¼ŒRLOO çš„ä¼˜åŠ¿ä¼°è®¡å¦‚ä¸‹ï¼ˆä» [TRL](https://github.com/huggingface/trl/blob/bfe20756082488350091352d1cdc19c172e42cd8/trl/trainer/rloo_trainer.py#L433)
    çš„å®ç°æ‰©å±•è€Œæ¥ï¼‰ï¼š
- en: '[PRE9]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The rest of the implementation details for RLOO follow the other trade-offs
    of implementing policy-gradient.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: RLOO çš„å…¶ä½™å®ç°ç»†èŠ‚éµå¾ªç­–ç•¥æ¢¯åº¦å®ç°çš„å…¶å®ƒæƒè¡¡ã€‚
- en: Auxiliary Topics
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¾…åŠ©ä¸»é¢˜
- en: In order to master the application of policy-gradient algorithms, there are
    countless other considerations. Here we consider some of the long-tail of complexities
    in successfully deploying a policy-gradient RL algorithm.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æŒæ¡ç­–ç•¥æ¢¯åº¦ç®—æ³•çš„åº”ç”¨ï¼Œè¿˜æœ‰æ— æ•°å…¶ä»–çš„è€ƒè™‘å› ç´ ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è€ƒè™‘äº†æˆåŠŸéƒ¨ç½²ç­–ç•¥æ¢¯åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„ä¸€äº›å¤æ‚æ€§é•¿å°¾ã€‚
- en: Comparing Algorithms
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç®—æ³•æ¯”è¾ƒ
- en: Hereâ€™s a summary of some of the discussed material (and foreshadowing to coming
    material on Direct Preference Optimization) when applied to RLHF. Here, on- or
    off-policy indicates the derivation (where most are applied slightly off-policy
    in practice). A reference policy here indicates if it is required for the optimization
    itself, rather than for a KL penalty.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯å¯¹ä¸€äº›è®¨è®ºææ–™ï¼ˆä»¥åŠå¯¹ç›´æ¥åå¥½ä¼˜åŒ–å³å°†åˆ°æ¥çš„ææ–™çš„é¢„ç¤ºï¼‰çš„æ€»ç»“ï¼Œå½“åº”ç”¨äºå¼ºåŒ–å­¦ä¹ ä¸äººç±»åé¦ˆï¼ˆRLHFï¼‰æ—¶ã€‚åœ¨è¿™é‡Œï¼Œåœ¨çº¿æˆ–ç¦»çº¿è¡¨ç¤ºå¯¼å‡ºçš„æ–¹å¼ï¼ˆåœ¨å®è·µä¸­å¤§å¤šæ•°éƒ½æ˜¯ç¨å¾®ç¦»çº¿åº”ç”¨ï¼‰ã€‚è¿™é‡Œçš„å‚è€ƒç­–ç•¥è¡¨ç¤ºå®ƒæ˜¯å¦æ˜¯ä¼˜åŒ–æœ¬èº«æ‰€å¿…éœ€çš„ï¼Œè€Œä¸æ˜¯ç”¨äº
    KL æƒ©ç½šã€‚
- en: 'Table 5: Comparing policy gradient algorithms (and friends).'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 5ï¼šæ¯”è¾ƒç­–ç•¥æ¢¯åº¦ç®—æ³•ï¼ˆåŠå…¶ç›¸å…³ç®—æ³•ï¼‰ã€‚
- en: '| Method | Type | Reward Model | Value Function | Reference Policy | Core Loss
    <semantics><mrow><mi>â„’</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{L}(\theta)</annotation></semantics>
    |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| æ–¹æ³• | ç±»å‹ | å¥–åŠ±æ¨¡å‹ | ä»·å€¼å‡½æ•° | å‚è€ƒç­–ç•¥ | æ ¸å¿ƒæŸå¤± <semantics><mrow><mi>â„’</mi><mo stretchy="false"
    form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">\mathcal{L}(\theta)</annotation></semantics> |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| **REINFORCE** | On-policy | Yes | No | No | <semantics><mrow><mi>âˆ’</mi><mfrac><mn>1</mn><mi>T</mi></mfrac><msubsup><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></msubsup><mrow><mi
    mathvariant="normal">log</mi><mo>â¡</mo></mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>âˆ£</mo><msub><mi>s</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo minsize="120%" maxsize="120%" stretchy="true"
    form="prefix">(</mo><msub><mi>G</mi><mi>t</mi></msub><mo>âˆ’</mo><mi>b</mi><mo stretchy="false"
    form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mo
    minsize="120%" maxsize="120%" stretchy="true" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">-\frac{1}{T}\sum_{t=1}^{T}\log \pi_\theta(a_t\mid
    s_t)\,\big(G_t - b(s_t)\big)</annotation></semantics> |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| **REINFORCE** | On-policy | æ˜¯ | å¦ | å¦ | <semantics><mrow><mi>âˆ’</mi><mfrac><mn>1</mn><mi>T</mi></mfrac><msubsup><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></msubsup><mrow><mi
    mathvariant="normal">log</mi><mo>â¡</mo></mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>âˆ£</mo><msub><mi>s</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="false" form="prefix">(</mo><msub><mi>G</mi><mi>t</mi></msub><mo>âˆ’</mo><mi>b</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false"
    form="postfix">)</mo><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">-\frac{1}{T}\sum_{t=1}^{T}\log \pi_\theta(a_t\mid
    s_t)\,\big(G_t - b(s_t)\big)</annotation></semantics> |'
- en: '| **RLOO** | On-policy | Yes | No | No | <semantics><mrow><mi>âˆ’</mi><mfrac><mn>1</mn><mi>K</mi></mfrac><msubsup><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><msub><mo>âˆ‘</mo><mi>t</mi></msub><mrow><mi
    mathvariant="normal">log</mi><mo>â¡</mo></mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>âˆ£</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo
    stretchy="false" form="postfix">)</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>R</mi><mi>i</mi></msub><mo>âˆ’</mo><mfrac><mn>1</mn><mrow><mi>K</mi><mo>âˆ’</mo><mn>1</mn></mrow></mfrac><msub><mo>âˆ‘</mo><mrow><mi>j</mi><mo>â‰ </mo><mi>i</mi></mrow></msub><msub><mi>R</mi><mi>j</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">-\frac{1}{K}\sum_{i=1}^{K}\sum_t
    \log \pi_\theta(a_{i,t}\mid s_{i,t})\left(R_i-\frac{1}{K-1}\sum_{j\neq i}R_j\right)</annotation></semantics>
    |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| **RLOO** | On-policy | æ˜¯ | å¦ | å¦ | <semantics><mrow><mi>âˆ’</mi><mfrac><mn>1</mn><mi>K</mi></mfrac><msubsup><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><msub><mo>âˆ‘</mo><mi>t</mi></msub><mrow><mi
    mathvariant="normal">log</mi><mo>â¡</mo></mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>âˆ£</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo
    stretchy="false" form="postfix">)</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>R</mi><mi>i</mi></msub><mo>âˆ’</mo><mfrac><mn>1</mn><mrow><mi>K</mi><mo>âˆ’</mo><mn>1</mn></mrow></mfrac><msub><mo>âˆ‘</mo><mrow><mi>j</mi><mo>â‰ </mo><mi>i</mi></mrow></msub><msub><mi>R</mi><mi>j</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">-\frac{1}{K}\sum_{i=1}^{K}\sum_t
    \log \pi_\theta(a_{i,t}\mid s_{i,t})\left(R_i-\frac{1}{K-1}\sum_{j\neq i}R_j\right)</annotation></semantics>
    |'
- en: '| **PPO** | On-policy | Yes | Yes | Yes | <semantics><mrow><mi>âˆ’</mi><mfrac><mn>1</mn><mi>T</mi></mfrac><msubsup><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></msubsup><mrow><mi
    mathvariant="normal">min</mi><mo>â¡</mo></mrow><mo minsize="120%" maxsize="120%"
    stretchy="true" form="prefix">(</mo><msub><mi>Ï</mi><mi>t</mi></msub><msub><mi>A</mi><mi>t</mi></msub><mo>,</mo><mrow><mi
    mathvariant="normal">c</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">i</mi><mi
    mathvariant="normal">p</mi></mrow><mo stretchy="false" form="prefix">(</mo><msub><mi>Ï</mi><mi>t</mi></msub><mo>,</mo><mn>1</mn><mo>âˆ’</mo><mi>Îµ</mi><mo>,</mo><mn>1</mn><mo>+</mo><mi>Îµ</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub><mo minsize="120%"
    maxsize="120%" stretchy="true" form="postfix">)</mo><mo>;</mo><msub><mi>Ï</mi><mi>t</mi></msub><mo>=</mo><mfrac><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>âˆ£</mo><msub><mi>s</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>Ï€</mi><msub><mi>Î¸</mi><mtext
    mathvariant="normal">old</mtext></msub></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>âˆ£</mo><msub><mi>s</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">-\frac{1}{T}\sum_{t=1}^{T}\min\!\big(\rho_t
    A_t,\ \mathrm{clip}(\rho_t,1-\varepsilon,1+\varepsilon) A_t\big);\ \rho_t = \frac{\pi_\theta(a_t\mid
    s_t)}{\pi_{\theta_{\text{old}}}(a_t\mid s_t)}</annotation></semantics> |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| **PPO** | On-policy | æ˜¯ | æ˜¯ | æ˜¯ | <semantics><mrow><mi>âˆ’</mi><mfrac><mn>1</mn><mi>T</mi></mfrac><msubsup><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></msubsup><mrow><mi
    mathvariant="normal">min</mi><mo>â¡</mo></mrow><mo minsize="120%" maxsize="120%"
    stretchy="true" form="prefix">(</mo><msub><mi>Ï</mi><mi>t</mi></msub><msub><mi>A</mi><mi>t</mi></msub><mo>,</mo><mrow><mi
    mathvariant="normal">c</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">i</mi><mi
    mathvariant="normal">p</mi></mrow><mo stretchy="false" form="prefix">(</mo><msub><mi>Ï</mi><mi>t</mi></msub><mo>,</mo><mn>1</mn><mo>âˆ’</mo><mi>Îµ</mi><mo>,</mo><mn>1</mn><mo>+</mo><mi>Îµ</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>t</mi></msub><mo minsize="120%"
    maxsize="120%" stretchy="true" form="postfix">)</mo><mo>;</mo><msub><mi>Ï</mi><mi>t</mi></msub><mo>=</mo><mfrac><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>âˆ£</mo><msub><mi>s</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>Ï€</mi><msub><mi>Î¸</mi><mtext
    mathvariant="normal">old</mtext></msub></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>âˆ£</mo><msub><mi>s</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">-\frac{1}{T}\sum_{t=1}^{T}\min\!\big(\rho_t
    A_t,\ \mathrm{clip}(\rho_t,1-\varepsilon,1+\varepsilon) A_t\big);\ \rho_t = \frac{\pi_\theta(a_t\mid
    s_t)}{\pi_{\theta_{\text{old}}}(a_t\mid s_t)}</annotation></semantics> |'
- en: '| **GRPO** | On-policy | Yes | No | Yes | <semantics><mrow><mi>âˆ’</mi><mfrac><mn>1</mn><mi>G</mi></mfrac><msubsup><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>G</mi></msubsup><mrow><mi
    mathvariant="normal">min</mi><mo>â¡</mo></mrow><mo minsize="120%" maxsize="120%"
    stretchy="true" form="prefix">(</mo><msub><mi>Ï</mi><mi>i</mi></msub><msub><mi>A</mi><mi>i</mi></msub><mo>,</mo><mrow><mi
    mathvariant="normal">c</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">i</mi><mi
    mathvariant="normal">p</mi></mrow><mo stretchy="false" form="prefix">(</mo><msub><mi>Ï</mi><mi>i</mi></msub><mo>,</mo><mn>1</mn><mo>âˆ’</mo><mi>Îµ</mi><mo>,</mo><mn>1</mn><mo>+</mo><mi>Îµ</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>i</mi></msub><mo minsize="120%"
    maxsize="120%" stretchy="true" form="postfix">)</mo><mo>;</mo><msub><mi>Ï</mi><mi>i</mi></msub><mo>=</mo><mfrac><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo>âˆ£</mo><mi>s</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>Ï€</mi><msub><mi>Î¸</mi><mtext
    mathvariant="normal">old</mtext></msub></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo>âˆ£</mo><mi>s</mi><mo
    stretchy="false" form="postfix">)</mo></mrow></mfrac><mo>,</mo><msub><mi>A</mi><mi>i</mi></msub><mo>=</mo><mfrac><mrow><msub><mi>r</mi><mi>i</mi></msub><mo>âˆ’</mo><mrow><mi
    mathvariant="normal">m</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">a</mi><mi
    mathvariant="normal">n</mi></mrow><mo stretchy="false" form="prefix">(</mo><msub><mi>r</mi><mrow><mn>1</mn><mo>:</mo><mi>G</mi></mrow></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><mrow><mi mathvariant="normal">s</mi><mi
    mathvariant="normal">t</mi><mi mathvariant="normal">d</mi></mrow><mo stretchy="false"
    form="prefix">(</mo><msub><mi>r</mi><mrow><mn>1</mn><mo>:</mo><mi>G</mi></mrow></msub><mo
    stretchy="false" form="postfix">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">-\frac{1}{G}\sum_{i=1}^{G}\min\!\big(\rho_i
    A_i,\ \mathrm{clip}(\rho_i,1-\varepsilon,1+\varepsilon) A_i\big);\ \rho_i = \frac{\pi_\theta(a_i\mid
    s)}{\pi_{\theta_{\text{old}}}(a_i\mid s)},\ A_i = \frac{r_i-\mathrm{mean}(r_{1:G})}{\mathrm{std}(r_{1:G})}</annotation></semantics>
    |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| **GRPO** | On-policy | æ˜¯ | å¦ | æ˜¯ | <semantics><mrow><mi>âˆ’</mi><mfrac><mn>1</mn><mi>G</mi></mfrac><msubsup><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>G</mi></msubsup><mrow><mi
    mathvariant="normal">min</mi><mo>â¡</mo></mrow><mo minsize="120%" maxsize="120%"
    stretchy="true" form="prefix">(</mo><msub><mi>Ï</mi><mi>i</mi></msub><msub><mi>A</mi><mi>i</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mrow><mi mathvariant="normal">c</mi><mi
    mathvariant="normal">l</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">p</mi></mrow><mo
    stretchy="false" form="prefix">(</mo><msub><mi>Ï</mi><mi>i</mi></msub><mo>âˆ’</mo><mn>1</mn><mo>âˆ’</mo><mi>Îµ</mi><mo>+</mo><mn>1</mn><mo>+</mo><mi>Îµ</mi><mo
    stretchy="false" form="postfix">)</mo><msub><mi>A</mi><mi>i</mi></msub><mo stretchy="false"
    form="postfix">)</mo><mo>;</mo><msub><mi>Ï</mi><mi>i</mi></msub><mo>=</mo><mfrac><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo>âˆ£</mo><mi>s</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>Ï€</mi><msub><mi>Î¸</mi><mtext
    mathvariant="normal">old</mtext></msub></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo>âˆ£</mo><mi>s</mi><mo
    stretchy="false" form="postfix">)</mo></mrow></mfrac><mo>;</mo><msub><mi>A</mi><mi>i</mi></msub><mo>=</mo><mfrac><mrow><msub><mi>r</mi><mi>i</mi></msub><mo>âˆ’</mo><mrow><mi
    mathvariant="normal">m</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">a</mi><mi
    mathvariant="normal">n</mi></mrow><mo stretchy="false" form="prefix">(</mo><msub><mi>r</mi><mrow><mn>1</mn><mo>:</mo><mi>G</mi></mrow></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><mrow><mi mathvariant="normal">s</mi><mi
    mathvariant="normal">t</mi><mi mathvariant="normal">d</mi></mrow><mo stretchy="false"
    form="prefix">(</mo><msub><mi>r</mi><mrow><mn>1</mn><mo>:</mo><mi>G</mi></mrow></msub><mo
    stretchy="false" form="postfix">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">-\frac{1}{G}\sum_{i=1}^{G}\min\!\big(\rho_i
    A_i,\ \mathrm{clip}(\rho_i,1-\varepsilon,1+\varepsilon) A_i\big);\ \rho_i = \frac{\pi_\theta(a_i\mid
    s)}{\pi_{\theta_{\text{old}}}(a_i\mid s)},\ A_i = \frac{r_i-\mathrm{mean}(r_{1:G})}{\mathrm{std}(r_{1:G})}</annotation></semantics>
    |'
- en: '| **DPO** | Off-policy | No | No | Yes | <semantics><mrow><mi>âˆ’</mi><msub><mi>ğ”¼</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><msup><mi>y</mi><mi>w</mi></msup><mo>,</mo><msup><mi>y</mi><mi>l</mi></msup><mo
    stretchy="false" form="postfix">)</mo></mrow></msub><mrow><mo stretchy="true"
    form="prefix">[</mo><mi mathvariant="normal">log</mi><mi>Ïƒ</mi><mo minsize="120%"
    maxsize="120%" stretchy="true" form="prefix">(</mo><mi>Î²</mi><mo stretchy="false"
    form="prefix">[</mo><mi mathvariant="normal">Î”</mi><mi mathvariant="normal">log</mi><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><mi
    mathvariant="normal">Î”</mi><mi mathvariant="normal">log</mi><msub><mi>Ï€</mi><mrow><mi
    mathvariant="normal">r</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">f</mi></mrow></msub><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo
    stretchy="false" form="postfix">]</mo><mo minsize="120%" maxsize="120%" stretchy="true"
    form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation
    encoding="application/x-tex">-\mathbb{E}_{(x,y^{w},y^{l})}\!\left[\log \sigma\!\big(\beta[\Delta\log
    \pi_\theta(x)-\Delta\log \pi_{\mathrm{ref}}(x)]\big)\right]</annotation></semantics>
    |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| **DPO** | Off-policy | No | No | Yes | <semantics><mrow><mi>âˆ’</mi><msub><mi>ğ”¼</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><msup><mi>y</mi><mi>w</mi></msup><mo>,</mo><msup><mi>y</mi><mi>l</mi></msup><mo
    stretchy="false" form="postfix">)</mo></mrow></msub><mrow><mo stretchy="true"
    form="prefix">[</mo><mi mathvariant="normal">log</mi><mi>Ïƒ</mi><mo minsize="120%"
    maxsize="120%" stretchy="true" form="prefix">(</mo><mi>Î²</mi><mo stretchy="false"
    form="prefix">[</mo><mi mathvariant="normal">Î”</mi><mi mathvariant="normal">log</mi><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><mi
    mathvariant="normal">Î”</mi><mi mathvariant="normal">log</mi><msub><mi>Ï€</mi><mrow><mi
    mathvariant="normal">r</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">f</mi></mrow></msub><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo
    stretchy="false" form="postfix">]</mo><mo minsize="120%" maxsize="120%" stretchy="true"
    form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation
    encoding="application/x-tex">-\mathbb{E}_{(x,y^{w},y^{l})}\!\left[\log \sigma\!\big(\beta[\Delta\log
    \pi_\theta(x)-\Delta\log \pi_{\mathrm{ref}}(x)]\big)\right]</annotation></semantics>
    |'
- en: Generalized Advantage Estimation (GAE)
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ (GAE)
- en: Generalized Advantage Estimation (GAE) is an alternate method to compute the
    advantage for policy gradient algorithms [[179]](ch021.xhtml#ref-schulman2015high)
    that better balances the bias-variance tradeoff. Traditional single-step advantage
    estimates can introduce too much bias, while using complete trajectories can suffer
    from high variance. GAE computes an exponentially-weighted average of multi-step
    advantage estimates, where the <semantics><mi>Î»</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics>
    hyperparameter controls the bias-variance tradeoffâ€”ranging from single-step TD
    (<semantics><mrow><mi>Î»</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\lambda=0</annotation></semantics>)
    to full trajectory returns (<semantics><mrow><mi>Î»</mi><mo>=</mo><mn>1</mn></mrow><annotation
    encoding="application/x-tex">\lambda=1</annotation></semantics>).
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ (GAE) æ˜¯ä¸€ç§ç”¨äºç­–ç•¥æ¢¯åº¦ç®—æ³•çš„ä¼˜åŠ¿è®¡ç®—æ›¿ä»£æ–¹æ³• [[179]](ch021.xhtml#ref-schulman2015high)ï¼Œå®ƒæ›´å¥½åœ°å¹³è¡¡äº†åå·®-æ–¹å·®æƒè¡¡ã€‚ä¼ ç»Ÿçš„å•æ­¥ä¼˜åŠ¿ä¼°è®¡å¯èƒ½ä¼šå¼•å…¥è¿‡å¤šçš„åå·®ï¼Œè€Œä½¿ç”¨å®Œæ•´è½¨è¿¹å¯èƒ½ä¼šé­å—é«˜æ–¹å·®ã€‚GAE
    è®¡ç®—å¤šæ­¥ä¼˜åŠ¿ä¼°è®¡çš„æŒ‡æ•°åŠ æƒå¹³å‡å€¼ï¼Œå…¶ä¸­ <semantics><mi>Î»</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics>
    è¶…å‚æ•°æ§åˆ¶åå·®-æ–¹å·®æƒè¡¡â€”â€”ä»å•æ­¥ TD (<semantics><mrow><mi>Î»</mi><mo>=</mo><mn>0</mn></mrow><annotation
    encoding="application/x-tex">\lambda=0</annotation></semantics>) åˆ°å®Œæ•´è½¨è¿¹å›æŠ¥ (<semantics><mrow><mi>Î»</mi><mo>=</mo><mn>1</mn></mrow><annotation
    encoding="application/x-tex">\lambda=1</annotation></semantics>)ã€‚
- en: 'Advantage estimates can take many forms, but we can define a <semantics><mi>n</mi><annotation
    encoding="application/x-tex">n</annotation></semantics> step advantage estimator
    (similar to the TD residual at the beginning of the chapter) as follows:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼˜åŠ¿ä¼°è®¡å¯ä»¥æœ‰å¤šç§å½¢å¼ï¼Œä½†æˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸€ä¸ª <semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics>
    æ­¥ä¼˜åŠ¿ä¼°è®¡å™¨ï¼ˆç±»ä¼¼äºæœ¬ç« å¼€å¤´çš„ TD å‰©ä½™ï¼‰ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '<semantics><mrow><msubsup><mover><mi>A</mi><mo accent="true">Ì‚</mo></mover><mi>t</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow></msubsup><mo>=</mo><mrow><mo
    stretchy="true" form="prefix">{</mo><mtable><mtr><mtd columnalign="left" style="text-align:
    left"><msub><mi>r</mi><mi>t</mi></msub><mo>+</mo><mi>Î³</mi><mi>V</mi><mo stretchy="false"
    form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo
    stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><mi>V</mi><mo stretchy="false"
    form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>,</mo></mtd><mtd
    columnalign="left" style="text-align: left"><mi>n</mi><mo>=</mo><mn>1</mn></mtd></mtr><mtr><mtd
    columnalign="left" style="text-align: left"><msub><mi>r</mi><mi>t</mi></msub><mo>+</mo><mi>Î³</mi><msub><mi>r</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><msup><mi>Î³</mi><mn>2</mn></msup><mi>V</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>2</mn></mrow></msub><mo
    stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><mi>V</mi><mo stretchy="false"
    form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>,</mo></mtd><mtd
    columnalign="left" style="text-align: left"><mi>n</mi><mo>=</mo><mn>2</mn></mtd></mtr><mtr><mtd
    columnalign="left" style="text-align: left"><mi>â‹®</mi></mtd></mtr><mtr><mtd columnalign="left"
    style="text-align: left"><msub><mi>r</mi><mi>t</mi></msub><mo>+</mo><mi>Î³</mi><msub><mi>r</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><msup><mi>Î³</mi><mn>2</mn></msup><msub><mi>r</mi><mrow><mi>t</mi><mo>+</mo><mn>2</mn></mrow></msub><mo>+</mo><mi>â‹¯</mi><mo>âˆ’</mo><mi>V</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false"
    form="postfix">)</mo><mo>,</mo></mtd><mtd columnalign="left" style="text-align:
    left"><mi>n</mi><mo>=</mo><mi>âˆ</mi></mtd></mtr></mtable></mrow><mrow><mo stretchy="false"
    form="prefix">(</mo><mn>65</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">\hat{A}_t^{(n)} = \begin{cases} r_t +
    \gamma V(s_{t+1}) - V(s_t), & n = 1 \\ r_t + \gamma r_{t+1} + \gamma^2 V(s_{t+2})
    - V(s_t), & n = 2 \\ \vdots \\ r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \cdots
    - V(s_t), & n = \infty \end{cases} \qquad{(65)}</annotation></semantics>'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '<semantics><mrow><msubsup><mover><mi>A</mi><mo accent="true">Ì‚</mo></mover><mi>t</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow></msubsup><mo>=</mo><mrow><mo
    stretchy="true" form="prefix">{</mo><mtable><mtr><mtd columnalign="left" style="text-align:
    left"><msub><mi>r</mi><mi>t</mi></msub><mo>+</mo><mi>Î³</mi><mi>V</mi><mi>V</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo
    stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><mi>V</mi><mi>V</mi><mo stretchy="false"
    form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>,</mo></mtd><mtd
    columnalign="left" style="text-align: left"><mi>n</mi><mo>=</mo><mn>1</mn></mtd></mtr><mtr><mtd
    columnalign="left" style="text-align: left"><msub><mi>r</mi><mi>t</mi></msub><mo>+</mo><mi>Î³</mi><msub><mi>r</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><msup><mi>Î³</mi><mn>2</mn></msup><mi>V</mi><mi>V</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>2</mn></mrow></msub><mo
    stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><mi>V</mi><mi>V</mi><mo stretchy="false"
    form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>,</mo></mtd><mtd
    columnalign="left" style="text-align: left"><mi>n</mi><mo>=</mo><mn>2</mn></mtd></mtr><mtr><mtd
    columnalign="left" style="text-align: left"><mi>â‹®</mi></mtd></mtr><mtr><mtd columnalign="left"
    style="text-align: left"><msub><mi>r</mi><mi>t</mi></msub><mo>+</mo><mi>Î³</mi><msub><mi>r</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><msup><mi>Î³</mi><mn>2</mn></msup><msub><mi>r</mi><mrow><mi>t</mi><mo>+</mo><mn>2</mn></mrow></msub><mo>+</mo><mi>â‹¯</mi><mo>âˆ’</mo><mi>V</mi><mi>V</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false"
    form="postfix">)</mo><mo>,</mo></mtd><mtd columnalign="left" style="text-align:
    left"><mi>n</mi><mo>=</mo><mi>âˆ</mi></mtd></mtr></mtable></mrow><mrow><mo stretchy="false"
    form="prefix">(</mo><mn>65</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">\hat{A}_t^{(n)} = \begin{cases} r_t +
    \gamma V(s_{t+1}) - V(s_t), & n = 1 \\ r_t + \gamma r_{t+1} + \gamma^2 V(s_{t+2})
    - V(s_t), & n = 2 \\ \vdots \\ r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \cdots
    - V(s_t), & n = \infty \end{cases} \qquad{(65)}</annotation></semantics>'
- en: Here a shorter <semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics>
    will have lower variance but higher bias as we are attributing more learning power
    to each trajectory â€“ it can overfit. GAE attempts to generalize this formulation
    into a weighted multi-step average instead of a specific <semantics><mi>n</mi><annotation
    encoding="application/x-tex">n</annotation></semantics>. To start, we must define
    the temporal difference (TD) residual of predicted value.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œè¾ƒçŸ­çš„ <semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics>
    å°†ä¼šæœ‰æ›´ä½çš„æ–¹å·®ä½†æ›´é«˜çš„åå·®ï¼Œå› ä¸ºæˆ‘ä»¬æ­£åœ¨å°†æ›´å¤šçš„å­¦ä¹ èƒ½åŠ›åˆ†é…ç»™æ¯ä¸ªè½¨è¿¹â€”â€”å®ƒå¯èƒ½ä¼šè¿‡æ‹Ÿåˆã€‚GAE å°è¯•å°†è¿™ç§å…¬å¼æ¨å¹¿åˆ°åŠ æƒå¤šæ­¥å¹³å‡è€Œä¸æ˜¯ç‰¹å®šçš„ <semantics><mi>n</mi><annotation
    encoding="application/x-tex">n</annotation></semantics>ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¿…é¡»å®šä¹‰é¢„æµ‹å€¼çš„æ—¶å·®ï¼ˆTDï¼‰æ®‹å·®ã€‚
- en: <semantics><mrow><msubsup><mi>Î´</mi><mi>t</mi><mi>V</mi></msubsup><mo>=</mo><msub><mi>r</mi><mi>t</mi></msub><mo>+</mo><mi>Î³</mi><mi>V</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo
    stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><mi>V</mi><mo stretchy="false"
    form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>66</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">\delta_t^V = r_t + \gamma V(s_{t+1})
    - V(s_t) \qquad{(66)}</annotation></semantics>
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><msubsup><mi>Î´</mi><mi>t</mi><mi>V</mi></msubsup><mo>=</mo><msub><mi>r</mi><mi>t</mi></msub><mo>+</mo><mi>Î³</mi><mi>V</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo
    stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><mi>V</mi><mo stretchy="false"
    form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>66</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">\delta_t^V = r_t + \gamma V(s_{t+1})
    - V(s_t) \qquad{(66)}</annotation></semantics>
- en: 'To utilize this, we introduce another variable <semantics><mi>Î»</mi><annotation
    encoding="application/x-tex">\lambda</annotation></semantics> as the GAE mixing
    parameter. This folds into an exponential decay of future advantages we wish to
    estimate:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†åˆ©ç”¨è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥å¦ä¸€ä¸ªå˜é‡ <semantics><mi>Î»</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics>
    ä½œä¸º GAE æ··åˆå‚æ•°ã€‚è¿™ä¼šæŠ˜å æˆæˆ‘ä»¬æƒ³è¦ä¼°è®¡çš„æœªæ¥ä¼˜åŠ¿çš„æŒ‡æ•°è¡°å‡ï¼š
- en: '<semantics><mrow><mtable><mtr><mtd columnalign="left" style="text-align: left"><msubsup><mover><mi>A</mi><mo
    accent="true">Ì‚</mo></mover><mi>t</mi><mrow><mi>G</mi><mi>A</mi><mi>E</mi><mo
    stretchy="false" form="prefix">(</mo><mi>Î³</mi><mo>,</mo><mi>Î»</mi><mo stretchy="false"
    form="postfix">)</mo></mrow></msubsup><mo>=</mo><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>âˆ’</mo><mi>Î»</mi><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="false" form="prefix">(</mo><msubsup><mover><mi>A</mi><mo
    accent="true">Ì‚</mo></mover><mi>t</mi><mrow><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></msubsup><mo>+</mo><mi>Î»</mi><msubsup><mover><mi>A</mi><mo
    accent="true">Ì‚</mo></mover><mi>t</mi><mrow><mo stretchy="false" form="prefix">(</mo><mn>2</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></msubsup><mo>+</mo><msup><mi>Î»</mi><mn>2</mn></msup><msubsup><mover><mi>A</mi><mo
    accent="true">Ì‚</mo></mover><mi>t</mi><mrow><mo stretchy="false" form="prefix">(</mo><mn>3</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></msubsup><mo>+</mo><mi>â‹¯</mi><mo
    stretchy="false" form="postfix">)</mo></mtd></mtr><mtr><mtd columnalign="left"
    style="text-align: left"><mo>=</mo><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>âˆ’</mo><mi>Î»</mi><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="false" form="prefix">(</mo><msubsup><mi>Î´</mi><mi>t</mi><mi>V</mi></msubsup><mo>+</mo><mi>Î»</mi><mo
    stretchy="false" form="prefix">(</mo><msubsup><mi>Î´</mi><mi>t</mi><mi>V</mi></msubsup><mo>+</mo><mi>Î³</mi><msubsup><mi>Î´</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mi>V</mi></msubsup><mo
    stretchy="false" form="postfix">)</mo><mo>+</mo><msup><mi>Î»</mi><mn>2</mn></msup><mo
    stretchy="false" form="prefix">(</mo><msubsup><mi>Î´</mi><mi>t</mi><mi>V</mi></msubsup><mo>+</mo><mi>Î³</mi><msubsup><mi>Î´</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mi>V</mi></msubsup><mo>+</mo><msup><mi>Î³</mi><mn>2</mn></msup><msubsup><mi>Î´</mi><mrow><mi>t</mi><mo>+</mo><mn>2</mn></mrow><mi>V</mi></msubsup><mo
    stretchy="false" form="postfix">)</mo><mo>+</mo><mi>â‹¯</mi><mo stretchy="false"
    form="postfix">)</mo></mtd></mtr><mtr><mtd columnalign="left" style="text-align:
    left"><mo>=</mo><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>âˆ’</mo><mi>Î»</mi><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="false" form="prefix">(</mo><msubsup><mi>Î´</mi><mi>t</mi><mi>V</mi></msubsup><mo
    stretchy="false" form="prefix">(</mo><mn>1</mn><mo>+</mo><mi>Î»</mi><mo>+</mo><msup><mi>Î»</mi><mn>2</mn></msup><mo>+</mo><mi>â‹¯</mi><mo
    stretchy="false" form="postfix">)</mo><mo>+</mo><mi>Î³</mi><msubsup><mi>Î´</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mi>V</mi></msubsup><mo
    stretchy="false" form="prefix">(</mo><mi>Î»</mi><mo>+</mo><msup><mi>Î»</mi><mn>2</mn></msup><mo>+</mo><mi>â‹¯</mi><mo
    stretchy="false" form="postfix">)</mo><mo>+</mo><mi>â‹¯</mi><mo stretchy="false"
    form="postfix">)</mo></mtd></mtr><mtr><mtd columnalign="left" style="text-align:
    left"><mo>=</mo><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>âˆ’</mo><mi>Î»</mi><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="false" form="prefix">(</mo><msubsup><mi>Î´</mi><mi>t</mi><mi>V</mi></msubsup><mfrac><mn>1</mn><mrow><mn>1</mn><mo>âˆ’</mo><mi>Î»</mi></mrow></mfrac><mo>+</mo><mi>Î³</mi><msubsup><mi>Î´</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mi>V</mi></msubsup><mfrac><mi>Î»</mi><mrow><mn>1</mn><mo>âˆ’</mo><mi>Î»</mi></mrow></mfrac><mo>+</mo><mi>â‹¯</mi><mo
    stretchy="false" form="postfix">)</mo></mtd></mtr><mtr><mtd columnalign="left"
    style="text-align: left"><mo>=</mo><munderover><mo>âˆ‘</mo><mrow><mi>l</mi><mo>=</mo><mn>0</mn></mrow><mo
    accent="false">âˆ</mo></munderover><mo stretchy="false" form="prefix">(</mo><mi>Î³</mi><mi>Î»</mi><msup><mo
    stretchy="false" form="postfix">)</mo><mi>l</mi></msup><msubsup><mi>Î´</mi><mrow><mi>t</mi><mo>+</mo><mi>l</mi></mrow><mi>V</mi></msubsup></mtd></mtr></mtable><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>67</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">\begin{array}{l} \hat{A}_t^{GAE(\gamma,\lambda)}
    = (1-\lambda)(\hat{A}_t^{(1)} + \lambda\hat{A}_t^{(2)} + \lambda^2\hat{A}_t^{(3)}
    + \cdots) \\ = (1-\lambda)(\delta_t^V + \lambda(\delta_t^V + \gamma\delta_{t+1}^V)
    + \lambda^2(\delta_t^V + \gamma\delta_{t+1}^V + \gamma^2\delta_{t+2}^V) + \cdots)
    \\ = (1-\lambda)(\delta_t^V(1 + \lambda + \lambda^2 + \cdots) + \gamma\delta_{t+1}^V(\lambda
    + \lambda^2 + \cdots) + \cdots) \\ = (1-\lambda)(\delta_t^V\frac{1}{1-\lambda}
    + \gamma\delta_{t+1}^V\frac{\lambda}{1-\lambda} + \cdots) \\ = \sum_{l=0}^{\infty}(\gamma\lambda)^l\delta_{t+l}^V
    \end{array} \qquad{(67)}</annotation></semantics>'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: 'Intuitively, this can be used to average multi-step estimates of Advantage
    in an elegant fashion. An example implementation is shown below:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: ç›´è§‚åœ°è¯´ï¼Œè¿™å¯ä»¥ä¼˜é›…åœ°å¹³å‡å¤šæ­¥ä¼˜åŠ¿ä¼°è®¡ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºä¾‹å®ç°ï¼š
- en: '[PRE10]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '*For further reading, see [[200]](ch021.xhtml#ref-seita2017gae).*'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '*æœ‰å…³è¿›ä¸€æ­¥é˜…è¯»ï¼Œè¯·å‚é˜…[[200]](ch021.xhtml#ref-seita2017gae)ã€‚*'
- en: Double Regularization
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åŒé‡æ­£åˆ™åŒ–
- en: Weâ€™ve seen in this chapter two types of regularization. One is built into algorithms
    like PPO with step-size constraints, and the other is a KL divergence based distance
    penalty relative to the start of the optimization.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†ä¸¤ç§ç±»å‹çš„æ­£åˆ™åŒ–ã€‚ä¸€ç§æ˜¯åƒPPOè¿™æ ·çš„ç®—æ³•ä¸­å†…ç½®çš„ï¼Œå…·æœ‰æ­¥é•¿çº¦æŸï¼›å¦ä¸€ç§æ˜¯åŸºäºKLæ•£åº¦çš„è·ç¦»æƒ©ç½šï¼Œç›¸å¯¹äºä¼˜åŒ–çš„å¼€å§‹ã€‚
- en: Many popular policy gradient algorithms from Deep Reinforcement Learning, including
    PPO and its predecessors, originated due to the need to control the learning process
    of the agent. In RLHF, as discussed extensively in Chapter 8 on Regularization
    and in Chapter 4 on Problem Formulation, there is a built-in regularization term
    via the distance penalty relative to the original policy one is finetuning. In
    this view, a large part of the difference between algorithms like PPO (which have
    internal step-size regularization) and REINFORCE (which is simpler, and to which
    PPO reduces under certain hyperparameters) is far less meaningful for finetuning
    language models than training agents from scratch.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: è®¸å¤šæ¥è‡ªæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„æµè¡Œç­–ç•¥æ¢¯åº¦ç®—æ³•ï¼ŒåŒ…æ‹¬PPOåŠå…¶å‰èº«ï¼Œéƒ½æ˜¯ç”±äºéœ€è¦æ§åˆ¶æ™ºèƒ½ä½“çš„å­¦ä¹ è¿‡ç¨‹è€Œå‡ºç°çš„ã€‚åœ¨RLHFä¸­ï¼Œæ­£å¦‚ç¬¬8ç« å…³äºæ­£åˆ™åŒ–å’Œç¬¬4ç« å…³äºé—®é¢˜è¡¨è¿°ä¸­å¹¿æ³›è®¨è®ºçš„é‚£æ ·ï¼Œé€šè¿‡ç›¸å¯¹äºåŸå§‹ç­–ç•¥çš„è·ç¦»æƒ©ç½šå†…ç½®äº†ä¸€ä¸ªæ­£åˆ™åŒ–é¡¹ã€‚ä»è¿™ä¸ªè§’åº¦æ¥çœ‹ï¼ŒPPOï¼ˆå…·æœ‰å†…éƒ¨æ­¥é•¿æ­£åˆ™åŒ–ï¼‰å’ŒREINFORCEï¼ˆæ›´ç®€å•ï¼Œä¸”åœ¨ç‰¹å®šè¶…å‚æ•°ä¸‹PPOä¼šç®€åŒ–ä¸ºå®ƒï¼‰ä¹‹é—´çš„å¾ˆå¤§ä¸€éƒ¨åˆ†å·®å¼‚ï¼Œå¯¹äºå¾®è°ƒè¯­è¨€æ¨¡å‹æ¥è¯´ï¼Œä¸ä»å¤´å¼€å§‹è®­ç»ƒæ™ºèƒ½ä½“ç›¸æ¯”ï¼Œæ„ä¹‰è¦å°å¾—å¤šã€‚
- en: In PPO, the objective that handles capping the step-size of the update is known
    as the [surrogate objective](https://huggingface.co/blog/deep-rl-ppo#introducing-the-clipped-surrogate-objective).
    To monitor how much the PPO regularization is impacting updates in RLHF, one can
    look at the clip fraction variable in many popular implementations, which is the
    percentage of samples in the batch where the gradients are clipped by this regularizer
    in PPO. These gradients are *reduced* to a maximum value.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨PPOä¸­ï¼Œå¤„ç†æ›´æ–°æ­¥é•¿å°é¡¶çš„ç›®æ ‡è¢«ç§°ä¸º[ä»£ç†ç›®æ ‡](https://huggingface.co/blog/deep-rl-ppo#introducing-the-clipped-surrogate-objective)ã€‚ä¸ºäº†ç›‘æ§PPOæ­£åˆ™åŒ–å¯¹RLHFä¸­æ›´æ–°çš„å½±å“ï¼Œå¯ä»¥æŸ¥çœ‹è®¸å¤šæµè¡Œå®ç°ä¸­çš„clip
    fractionå˜é‡ï¼Œè¿™æ˜¯æ‰¹æ ·æœ¬ä¸­æ¢¯åº¦è¢«æ­¤æ­£åˆ™åŒ–å™¨å‰ªè£çš„ç™¾åˆ†æ¯”ã€‚è¿™äº›æ¢¯åº¦è¢«*é™ä½*åˆ°æœ€å¤§å€¼ã€‚
- en: In practice with language models, algorithms like PPO and GRPO are run with
    only one gradient step per batch, which means that the PPO-native regularization
    is never applied (as clipping can only occur within a batch when the policy changes
    substantially) and the KL distances penalties predominate.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¯­è¨€æ¨¡å‹çš„å®é™…åº”ç”¨ä¸­ï¼ŒåƒPPOå’ŒGRPOè¿™æ ·çš„ç®—æ³•æ¯æ‰¹åªè¿è¡Œä¸€ä¸ªæ¢¯åº¦æ­¥ï¼Œè¿™æ„å‘³ç€PPOåŸç”Ÿçš„æ­£åˆ™åŒ–ä»æœªè¢«åº”ç”¨ï¼ˆå› ä¸ºå‰ªè£åªèƒ½åœ¨ç­–ç•¥æœ‰æ˜¾è‘—å˜åŒ–æ—¶åœ¨æ‰¹å†…å‘ç”Ÿï¼‰ï¼Œè€ŒKLè·ç¦»æƒ©ç½šå ä¸»å¯¼åœ°ä½ã€‚
- en: Further Reading
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è¿›ä¸€æ­¥é˜…è¯»
- en: 'As RLHF has cemented itself at the center of modern post-training, other policy-gradient
    RL algorithms and RL algorithms generally have been proposed to improve the training
    process, but they have not had a central role in governing best practices. Examples
    for further reading include:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€RLHFåœ¨ç°ä»£åè®­ç»ƒä¸­çš„åœ°ä½ç¨³å›ºï¼Œå…¶ä»–ç­–ç•¥æ¢¯åº¦RLç®—æ³•å’ŒRLç®—æ³•é€šå¸¸è¢«æå‡ºä»¥æ”¹è¿›è®­ç»ƒè¿‡ç¨‹ï¼Œä½†å®ƒä»¬åœ¨æŒ‡å¯¼æœ€ä½³å®è·µæ–¹é¢å¹¶æ²¡æœ‰æ‰®æ¼”æ ¸å¿ƒè§’è‰²ã€‚è¿›ä¸€æ­¥é˜…è¯»çš„ä¾‹å­åŒ…æ‹¬ï¼š
- en: '**Pairwise Proximal Policy Optimization (P3O)** [[201]](ch021.xhtml#ref-wu2023pairwise)
    uses pairwise data directly in a PPO-style policy update without learning an intermediate
    reward model.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æˆå¯¹è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆP3Oï¼‰** [[201]](ch021.xhtml#ref-wu2023pairwise)ç›´æ¥åœ¨PPOé£æ ¼çš„ç­–ç•¥æ›´æ–°ä¸­ä½¿ç”¨æˆå¯¹æ•°æ®ï¼Œè€Œä¸å­¦ä¹ ä¸­é—´å¥–åŠ±æ¨¡å‹ã€‚'
- en: Off-policy policy-gradient algorithms could enable further asynchronous training,
    such as **Contrastive Policy Gradient (CoPG)** [[202]](ch021.xhtml#ref-flet2024contrastive)
    (a generalization of the direct alignment algorithm IPO and vanilla policy gradient),
    which was used by Cohere for their Command A model [[58]](ch021.xhtml#ref-cohere2025command).
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¦»ç­–ç•¥ç­–ç•¥æ¢¯åº¦ç®—æ³•å¯ä»¥è¿›ä¸€æ­¥å®ç°å¼‚æ­¥è®­ç»ƒï¼Œä¾‹å¦‚**å¯¹æ¯”ç­–ç•¥æ¢¯åº¦ï¼ˆCoPGï¼‰** [[202]](ch021.xhtml#ref-flet2024contrastive)ï¼ˆç›´æ¥å¯¹é½ç®—æ³•IPOå’Œæ™®é€šç­–ç•¥æ¢¯åº¦çš„æ¨å¹¿ï¼‰ï¼Œè¯¥ç®—æ³•è¢«Cohereç”¨äºä»–ä»¬çš„Command
    Aæ¨¡å‹ [[58]](ch021.xhtml#ref-cohere2025command)ã€‚
- en: Other implementations of REINFORCE algorithms have been designed for language
    models, such as **ReMax** [[203]](ch021.xhtml#ref-li2023remax), which implements
    a baseline normalization designed specifically to accommodate the sources of uncertainty
    from reward model inference.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å…¶ä»–å®ç°REINFORCEç®—æ³•çš„è¯­è¨€æ¨¡å‹ï¼Œä¾‹å¦‚**ReMax** [[203]](ch021.xhtml#ref-li2023remax)ï¼Œå®ƒå®ç°äº†ä¸€ç§ä¸“é—¨ä¸ºé€‚åº”å¥–åŠ±æ¨¡å‹æ¨ç†çš„ä¸ç¡®å®šæ€§æ¥æºè€Œè®¾è®¡çš„åŸºçº¿å½’ä¸€åŒ–ã€‚
- en: Some foundation models, such as Apple Intelligence Foundation Models [[204]](ch021.xhtml#ref-gunter2024apple)
    or Kimi k1.5 reasoning model [[205]](ch021.xhtml#ref-team2025kimi), have used
    variants of **Mirror Descent Policy Optimization (MDPO)** [[206]](ch021.xhtml#ref-tomar2020mirror).
    Research is still developing further on the fundamentals here [[207]](ch021.xhtml#ref-zhang2025improving),
    but Mirror Descent is an optimization method rather than directly a policy gradient
    algorithm. What is important here is that it is substituted in very similarly
    to existing RL infrastructure.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€äº›åŸºç¡€æ¨¡å‹ï¼Œå¦‚è‹¹æœæ™ºèƒ½åŸºç¡€æ¨¡å‹ [[204]](ch021.xhtml#ref-gunter2024apple) æˆ– Kimi k1.5æ¨ç†æ¨¡å‹ [[205]](ch021.xhtml#ref-team2025kimi)ï¼Œå·²ç»ä½¿ç”¨äº†**é•œåƒä¸‹é™ç­–ç•¥ä¼˜åŒ–ï¼ˆMDPOï¼‰**
    [[206]](ch021.xhtml#ref-tomar2020mirror) çš„å˜ä½“ã€‚åœ¨æ­¤åŸºç¡€ä¸Šçš„ç ”ç©¶ä»åœ¨è¿›ä¸€æ­¥å‘å±• [[207]](ch021.xhtml#ref-zhang2025improving)ï¼Œä½†é•œåƒä¸‹é™æ˜¯ä¸€ç§ä¼˜åŒ–æ–¹æ³•ï¼Œè€Œä¸æ˜¯ç›´æ¥çš„æ”¿ç­–æ¢¯åº¦ç®—æ³•ã€‚è¿™é‡Œé‡è¦çš„æ˜¯ï¼Œå®ƒéå¸¸ç±»ä¼¼äºç°æœ‰çš„å¼ºåŒ–å­¦ä¹ åŸºç¡€è®¾æ–½ä¸­è¢«æ›¿æ¢ã€‚
- en: '**Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO)** proposes
    4 modifications to GRPO to better suit reasoning language models, where long traces
    are needed and new, underutilized tokens need to be increased in probability [[193]](ch021.xhtml#ref-yu2025dapo).
    The changes are: 1, have two different clip hyperparameters, <semantics><msub><mi>Îµ</mi><mtext
    mathvariant="normal">low</mtext></msub><annotation encoding="application/x-tex">\varepsilon_\text{low}</annotation></semantics>
    and <semantics><msub><mi>Îµ</mi><mtext mathvariant="normal">high</mtext></msub><annotation
    encoding="application/x-tex">\varepsilon_\text{high}</annotation></semantics>,
    so clipping on the positive side of the logratio can take bigger steps for better
    exploration; 2, dynamic sampling, which removes all samples with reward = 0 or
    reward = 1 for all samples in the batch (no learning signal); 3, use the per token
    loss as discussed above in Implementation: GRPO; and 4, a soft penalty on samples
    that are too long to avoid trying to learn from truncated answers.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è§£è€¦å‰ªè¾‘å’ŒåŠ¨æ€é‡‡æ ·ç­–ç•¥ä¼˜åŒ–ï¼ˆDAPOï¼‰** æå‡ºäº†å¯¹GRPOçš„4é¡¹ä¿®æ”¹ï¼Œä»¥æ›´å¥½åœ°é€‚åº”éœ€è¦é•¿åºåˆ—å’Œå¢åŠ æ–°ã€æœªå……åˆ†åˆ©ç”¨çš„æ ‡è®°æ¦‚ç‡çš„æ¨ç†è¯­è¨€æ¨¡å‹ [[193]](ch021.xhtml#ref-yu2025dapo)ã€‚è¿™äº›ä¿®æ”¹åŒ…æ‹¬ï¼š1ï¼Œæœ‰ä¸¤ä¸ªä¸åŒçš„å‰ªè¾‘è¶…å‚æ•°ï¼Œ<semantics><msub><mi>Îµ</mi><mtext
    mathvariant="normal">low</mtext></msub><annotation encoding="application/x-tex">\varepsilon_\text{low}</annotation></semantics>
    å’Œ <semantics><msub><mi>Îµ</mi><mtext mathvariant="normal">high</mtext></msub><annotation
    encoding="application/x-tex">\varepsilon_\text{high}</annotation></semantics>ï¼Œå› æ­¤å¯ä»¥åœ¨æ­£å¯¹æ•°æ¯”ç‡çš„æ­£ä¾§é‡‡å–æ›´å¤§çš„æ­¥é•¿ä»¥è¿›è¡Œæ›´å¥½çš„æ¢ç´¢ï¼›2ï¼ŒåŠ¨æ€é‡‡æ ·ï¼Œç§»é™¤æ‰€æœ‰å¥–åŠ±ä¸º0æˆ–å¥–åŠ±ä¸º1çš„æ ·æœ¬ï¼ˆæ²¡æœ‰å­¦ä¹ ä¿¡å·ï¼‰ï¼›3ï¼Œä½¿ç”¨ä¸Šè¿°å®ç°ä¸­è®¨è®ºçš„æ¯ä¸ªæ ‡è®°æŸå¤±ï¼›4ï¼Œå¯¹è¿‡é•¿æ ·æœ¬æ–½åŠ è½¯æƒ©ç½šï¼Œä»¥é¿å…å°è¯•ä»æˆªæ–­ç­”æ¡ˆä¸­å­¦ä¹ ã€‚'
- en: '**Value-based Augmented Proximal Policy Optimization (VAPO)** [[208]](ch021.xhtml#ref-yuan2025vapo)
    combines optimizations from DAPO (including clip-higher, token level policy-gradient,
    and different length normalization) with insights from Value-Calibrated PPO [[209]](ch021.xhtml#ref-yuan2025s)
    to pretrain the value function and length-adaptive GAE to show the promise of
    value base methods relative to GRPO.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**åŸºäºä»·å€¼çš„å¢å¼ºè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆVAPOï¼‰** [[208]](ch021.xhtml#ref-yuan2025vapo) ç»“åˆäº†DAPOï¼ˆåŒ…æ‹¬æ›´é«˜çš„å‰ªè¾‘ã€æ ‡è®°çº§ç­–ç•¥æ¢¯åº¦ä»¥åŠä¸åŒé•¿åº¦çš„å½’ä¸€åŒ–ï¼‰å’Œæ¥è‡ªä»·å€¼æ ¡å‡†PPO
    [[209]](ch021.xhtml#ref-yuan2025s) çš„è§è§£ï¼Œä»¥é¢„è®­ç»ƒä»·å€¼å‡½æ•°å’Œé•¿åº¦è‡ªé€‚åº”GAEï¼Œä»¥å±•ç¤ºç›¸å¯¹äºGRPOçš„ä»·å€¼åŸºç¡€æ–¹æ³•çš„å‰æ™¯ã€‚'
