- en: '**JAX & Flax ebook**'
  id: totrans-0
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '**JAX & Flax ç”µå­ä¹¦**'
- en: '![](`../images/00001.jpeg`)'
  id: totrans-1
  prefs: []
  stylish: true
  type: TYPE_IMG
  zh: '![](`../images/00001.jpeg`)'
- en: Download all notebooks JAX (What it is and how to use it in Python) What is
    XLA? Installing JAX **Setting up TPUs on Google Colab Data types in JAX Ways to
    create JAXÂ arrays Generating random numbers with JAX Pure functions JAX NumPy
    operations JAX arrays are immutable Out-of-Bounds Indexing Data placement on devices
    in JAX How fast is JAX? Using jit() to speed up functions How JIT works Taking
    derivatives with grad() Auto-vectorization with vmap Parallelization with pmap
    Debugging NANs in JAX Double (64bit) precision What is a pytree? Handling state
    in JAX Loading datasets with JAX Building neural networks with JAX Final thoughts**
    Optimizers in JAX and Flax **Adaptive vs stochastic gradient descent (SGD) optimizers
    AdaBelief AdaGrad Adam â€“ Adaptive moment estimation AdamW RAdam â€“ Rectified Adam
    optimizer AdaFactor Fromage Lamb â€“ Layerwise adaptive large batch optimization
    Lars â€“ Layer-wise Adaptive Rate Scaling SM3 - Square-root of Minima of Sums of
    Maxima of Squared-gradients Method SGDâ€“ Stochastic Gradient Descent Noisy SGD
    Optimistic GD Differentially Private SGD RMSProp Yogi Final thoughts JAX loss
    functions What is a loss function? Creating custom loss functions in JAX Which
    loss functions are available in JAX? Sigmoid binary cross entropy Softmax cross
    entropy Cosine distance Cosine similarity Huber loss l2 loss log cosh Smooth labels
    Computing loss with JAX Metrics How to monitor JAX loss functions Why JAX loss
    nan happens Final thoughts Activation functions in JAX and Flax ReLU â€“ Rectified
    linear unit PReLUâ€“ Parametric Rectified Linear Unit Sigmoid Log sigmoid Softmax
    Log softmax ELU â€“ Exponential linear unit activation CELU â€“ Continuously-differentiable
    exponential linear unit GELUâ€“ Gaussian error linear unit activation GLU â€“ Gated
    linear unit activation Soft sign Softplus Swishâ€“SigmoidÂ Linear Unit(Â SiLU) Custom
    activation functions in JAX and Flax Final thoughts How to load datasets in JAX
    with TensorFlow How to load text data in JAX Clean the text data Label encode
    the sentiment column Text preprocessing with TensorFlow How to load image data
    in JAX How to load CSV data in JAX Final thoughts Image classification with JAX
    & Flax Loading the dataset Define Convolution Neural Network with Flax Define
    loss Compute metrics Create training state Define training step Define evaluation
    step Training function Evaluate the model Train and evaluate the model Model performance
    Final thoughts Distributed training with JAX & Flax Perform standard imports Setup
    TPUs on Colab Download the dataset Load the dataset Define the model with Flax
    Create training state Apply the model Training function Train the model Model
    evaluation Final thoughts How to use TensorBoard in JAX & Flax How to use TensorBoard
    How to install TensorBoard Using TensorBoard with Jupyter notebooks and Google
    Colab How to launch TensorBoard Tensorboard dashboards How to use TensorBoard
    with Flax How to log images with TensorBoard in Flax How to log text with TensorBoard
    in Flax Track model training in JAX using TensorBoard How to profile JAX programs
    with TensorBoard Programmatic profiling Manual profiling with TensorBoard How
    to profile JAX program on a remote machine Share TensorBoard dashboards Final
    thoughts Handling state in JAX & Flax (BatchNorm and DropOut layers) Perform standard
    imports Download the dataset Loading datasets in JAX Data processing with PyTorch
    Define Flax model with BatchNorm and DropOut Create loss function Compute metrics
    Create custom Flax training state Training step Evaluation step Train Flax model
    Set up TensorBoard in Flax Train model Save Flax model Load Flax model Evaluate
    Flax model Visualize Flax model performance Final thoughts LSTM in JAX & Flax
    Dataset download Data processing with NLTK Text vectorization with Keras Create
    tf.data dataset Define LSTM model in Flax Compute metrics in Flax Create training
    state Define training step Evaluate the Flax model Create training function Train
    LSTM model in Flax Visualize LSTM model performance in Flax Save LSTM model Final
    thoughts Flax vs. TensorFlow Random number generation in TensorFlow and Flax Model
    definition in Flax and TensorFlow Activations in Flax and TensorFlow Optimizers
    in Flax and TensorFlow Metrics in Flax and TensorFlow Computing gradients in Flax
    and TensorFlow Loading datasets in Flax and TensorFlow Training model in Flax
    vs. TensorFlow Distributed training in Flax and TensorFlow Working with TPU accelerators
    Model evaluation Visualize model performance Final thoughts Train ResNet in Flax
    from scratch(Distributed ResNet training) Install Flax models Perform standard
    imports Download dataset Loading dataset in Flax Data transformation in Flax Instantiate
    Flax ResNet model Compute metrics Create Flax model training state Apply model
    function TensorBoard in Flax Train Flax ResNet model Evaluate model with TensorBoard
    Visualize Flax model performance Save Flax ResNet model Load Flax RestNet model
    Final thoughts Transfer learning with JAX & Flax Install JAX ResNet Download dataset
    Data loading in JAX Data processing ResNet model definition Create head network
    Combine ResNet backbone with head Load pre-trained ResNet 50 Get model and variables
    Zero gradients Define Flax optimizer Define Flax loss function Compute Flax metrics
    Create Flax training state Training step Evaluation step Train ResNet modelÂ in
    Flax Set up TensorBoard in Flax Train model Save Flax model Load saved Flax model
    Evaluate Flax ResNet model Visualize model performance Final thoughts Elegy(High-level
    API for deep learning in JAX & Flax) Data pre-processing Model definition in Elegy
    Elegy model summary Distributed training in Elegy Keras-like callbacks in Flax
    Train Elegy models Evaluate Elegy models Visualize Elegy model with TensorBoard
    Plot model performance with Matplotlib Making predictions with Elegy models Saving
    and loading Elegy models Final thoughts** Appendix Disclaimer Copyright Other
    things to learn
  id: totrans-2
  prefs:
  - PREF_H3
  stylish: true
  type: TYPE_NORMAL
  zh: ä¸‹è½½æ‰€æœ‰ JAX ç¬”è®°æœ¬ï¼ˆå®ƒæ˜¯ä»€ä¹ˆä»¥åŠå¦‚ä½•åœ¨ Python ä¸­ä½¿ç”¨å®ƒï¼‰ä»€ä¹ˆæ˜¯ XLAï¼Ÿå®‰è£… JAX **åœ¨ Google Colab ä¸Šè®¾ç½® TPU JAX
    ä¸­çš„æ•°æ®ç±»å‹ ä½¿ç”¨ JAX åˆ›å»ºæ•°ç»„ ç”¨ JAX ç”Ÿæˆéšæœºæ•° çº¯å‡½æ•° JAX NumPy æ“ä½œ JAX æ•°ç»„æ˜¯ä¸å¯å˜çš„ è¶…å‡ºè¾¹ç•Œç´¢å¼• JAX ä¸­çš„æ•°æ®æ”¾ç½®
    JAX æœ‰å¤šå¿«ï¼Ÿ ä½¿ç”¨ jit() åŠ é€Ÿå‡½æ•° JIT å¦‚ä½•å·¥ä½œ ç”¨ grad() æ±‚å¯¼ è‡ªåŠ¨å‘é‡åŒ–ä¸ vmap ä½¿ç”¨ pmap å¹¶è¡ŒåŒ– åœ¨ JAX ä¸­è°ƒè¯•
    NANs åŒç²¾åº¦ï¼ˆ64ä½ï¼‰æ˜¯ä»€ä¹ˆï¼Ÿ ä»€ä¹ˆæ˜¯ pytreeï¼Ÿ åœ¨ JAX ä¸­å¤„ç†çŠ¶æ€ ä½¿ç”¨ JAX åŠ è½½æ•°æ®é›† ä½¿ç”¨ JAX æ„å»ºç¥ç»ç½‘ç»œ æ€»ç»“** JAX
    å’Œ Flax ä¸­çš„ä¼˜åŒ–å™¨ ** è‡ªé€‚åº”ä¸éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰ä¼˜åŒ–å™¨ AdaBelief AdaGrad Adam â€“ è‡ªé€‚åº”çŸ©ä¼°è®¡ AdamW RAdam
    â€“ ä¿®æ­£çš„ Adam ä¼˜åŒ–å™¨ AdaFactor Fromage Lamb â€“ å±‚æ¬¡è‡ªé€‚åº”å¤§æ‰¹é‡ä¼˜åŒ– Lars â€“ å±‚æ¬¡è‡ªé€‚åº”é€Ÿç‡ç¼©æ”¾ SM3 - å¹³æ–¹æ ¹æœ€å°å€¼ä¹‹å’Œçš„æœ€å¤§å€¼çš„å¹³æ–¹æ¢¯åº¦æ³•
    SGD â€“ éšæœºæ¢¯åº¦ä¸‹é™ Noisy SGD ä¹è§‚çš„ GD å·®åˆ†éšç§ SGD RMSProp Yogi æ€»ç»“ JAX æŸå¤±å‡½æ•° ä»€ä¹ˆæ˜¯æŸå¤±å‡½æ•°ï¼Ÿ åœ¨ JAX
    ä¸­åˆ›å»ºè‡ªå®šä¹‰æŸå¤±å‡½æ•° JAX ä¸­å¯ç”¨çš„æŸå¤±å‡½æ•°æœ‰å“ªäº›ï¼Ÿ Sigmoid äºŒå…ƒäº¤å‰ç†µ Softmax äº¤å‰ç†µ ä½™å¼¦è·ç¦» ä½™å¼¦ç›¸ä¼¼åº¦ Huber æŸå¤± l2 æŸå¤±
    log cosh å¹³æ»‘æ ‡ç­¾ ä½¿ç”¨ JAX è®¡ç®—æŸå¤±å‡½æ•° æŒ‡æ ‡ å¦‚ä½•ç›‘æ§ JAX æŸå¤±å‡½æ•° JAX æŸå¤±ä¸º NAN çš„åŸå›  æ€»ç»“ JAX å’Œ Flax ä¸­çš„æ¿€æ´»å‡½æ•°
    ReLU â€“ çŸ«æ­£çº¿æ€§å•å…ƒ PReLUâ€“ å‚æ•°åŒ–çŸ«æ­£çº¿æ€§å•å…ƒ Sigmoid Log sigmoid Softmax Log softmax ELU â€“ æŒ‡æ•°çº¿æ€§å•å…ƒ
    CELU â€“ è¿ç»­å¯å¾®æŒ‡æ•°çº¿æ€§å•å…ƒ GELUâ€“ é«˜æ–¯è¯¯å·®çº¿æ€§å•å…ƒ GLU â€“ é—¨æ§çº¿æ€§å•å…ƒ Soft sign Softplus Swishâ€“SigmoidÂ Linear
    Unit(Â SiLU) JAX å’Œ Flax ä¸­çš„è‡ªå®šä¹‰æ¿€æ´»å‡½æ•° æ€»ç»“ å¦‚ä½•åœ¨ JAX ä¸­ä½¿ç”¨ TensorFlow åŠ è½½æ•°æ®é›† å¦‚ä½•åœ¨ JAX ä¸­åŠ è½½æ–‡æœ¬æ•°æ®
    æ¸…ç†æ–‡æœ¬æ•°æ® æ ‡ç­¾ç¼–ç æƒ…æ„Ÿåˆ— ä½¿ç”¨ TensorFlow è¿›è¡Œæ–‡æœ¬é¢„å¤„ç† å¦‚ä½•åœ¨ JAX ä¸­åŠ è½½å›¾åƒæ•°æ® å¦‚ä½•åœ¨ JAX ä¸­åŠ è½½ CSV æ•°æ® æ€»ç»“ JAX
    å’Œ Flax ä¸­çš„å›¾åƒåˆ†ç±» å®šä¹‰ Convolution Neural Network ä½¿ç”¨ Flax å®šä¹‰æŸå¤± è®¡ç®—æŒ‡æ ‡ åˆ›å»ºè®­ç»ƒçŠ¶æ€ å®šä¹‰è®­ç»ƒæ­¥éª¤ å®šä¹‰è¯„ä¼°æ­¥éª¤
    è®­ç»ƒå‡½æ•° è¯„ä¼°æ¨¡å‹ è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹ æ¨¡å‹æ€§èƒ½ æ€»ç»“ JAX å’Œ Flax ä¸­çš„åˆ†å¸ƒå¼è®­ç»ƒ æ‰§è¡Œæ ‡å‡†å¯¼å…¥ åœ¨ Colab ä¸Šè®¾ç½® TPUs ä¸‹è½½æ•°æ®é›† åŠ è½½æ•°æ®é›†
    ä½¿ç”¨ Flax å®šä¹‰æ¨¡å‹ åˆ›å»ºè®­ç»ƒçŠ¶æ€ åº”ç”¨æ¨¡å‹ è®­ç»ƒå‡½æ•° è®­ç»ƒæ¨¡å‹ æ¨¡å‹è¯„ä¼° æ€»ç»“ å¦‚ä½•åœ¨ JAX å’Œ Flax ä¸­ä½¿ç”¨ TensorBoard å¦‚ä½•ä½¿ç”¨
    TensorBoard å¦‚ä½•å®‰è£… TensorBoard åœ¨ Jupyter notebooks å’Œ Google Colab ä¸­ä½¿ç”¨ TensorBoard
    å¦‚ä½•å¯åŠ¨ TensorBoard Tensorboard ä»ªè¡¨æ¿ å¦‚ä½•åœ¨ Flax ä¸­ä½¿ç”¨ TensorBoard å¦‚ä½•è®°å½•å›¾åƒåˆ° Flax çš„ TensorBoard
    å¦‚ä½•è®°å½•æ–‡æœ¬åˆ° Flax çš„ TensorBoard ä½¿ç”¨ TensorBoard åœ¨ JAX ä¸­è·Ÿè¸ªæ¨¡å‹è®­ç»ƒ å¦‚ä½•ä½¿ç”¨ TensorBoard å¯¹ JAX
    ç¨‹åºè¿›è¡Œæ€§èƒ½åˆ†æ é€šè¿‡ TensorBoard è¿›è¡Œç¼–ç¨‹æ€§èƒ½åˆ†æ æ‰‹åŠ¨ä½¿ç”¨ TensorBoard è¿›è¡Œæ€§èƒ½åˆ†æ å¦‚ä½•åœ¨è¿œç¨‹æœºå™¨ä¸Šå¯¹ JAX ç¨‹åºè¿›è¡Œæ€§èƒ½åˆ†æ
    åˆ†äº« TensorBoard ä»ªè¡¨æ¿ æ€»ç»“ JAX å’Œ Flax ä¸­çš„çŠ¶æ€å¤„ç†ï¼ˆBatchNorm å’Œ DropOut å±‚ï¼‰ æ‰§è¡Œæ ‡å‡†å¯¼å…¥ ä¸‹è½½æ•°æ®é›† åœ¨
    JAX ä¸­åŠ è½½æ•°æ®é›† ä½¿ç”¨ PyTorch å¤„ç†æ•°æ® ä½¿ç”¨ BatchNorm å’Œ DropOut å±‚å®šä¹‰ Flax æ¨¡å‹ åˆ›å»ºæŸå¤±å‡½æ•° è®¡ç®—æŒ‡æ ‡ åˆ›å»ºè‡ªå®šä¹‰
    Flax è®­ç»ƒçŠ¶æ€ è®­ç»ƒæ­¥éª¤ è¯„ä¼°æ­¥éª¤ è®­ç»ƒ Flax æ¨¡å‹ åœ¨ Flax ä¸­è®¾ç½® TensorBoard è®­ç»ƒæ¨¡å‹ ä¿å­˜ Flax æ¨¡å‹ åŠ è½½ Flax
    æ¨¡å‹ è¯„ä¼° Flax æ¨¡å‹ å¯è§†åŒ– Flax æ¨¡å‹æ€§èƒ½ æ€»ç»“ JAX å’Œ Flax ä¸­çš„ LSTM æ•°æ®é›†ä¸‹è½½ ä½¿ç”¨ NLTK å¤„ç†æ–‡æœ¬ æ•°æ®å‘é‡åŒ– ä½¿ç”¨
    Keras åˆ›å»º tf.data æ•°æ®é›† åœ¨ Flax ä¸­å®šä¹‰ LSTM æ¨¡å‹ è®¡ç®— Flax ä¸­çš„æŒ‡æ ‡ åˆ›å»ºè®­ç»ƒçŠ¶æ€ å®šä¹‰è®­ç»ƒæ­¥éª¤ è¯„ä¼° Flax æ¨¡å‹
    åˆ›å»ºè®­ç»ƒå‡½æ•° åœ¨ Flax ä¸­è®­ç»ƒ LSTM æ¨¡å‹ å¯è§†åŒ– Flax ä¸­çš„ LSTM æ¨¡å‹æ€§èƒ½ ä¿å­˜ LSTM æ¨¡å‹ æ€»ç»“ Flax vs. TensorFlow
    TensorFlow å’Œ Flax ä¸­çš„éšæœºæ•°ç”Ÿæˆ åœ¨ Flax å’Œ TensorFlow ä¸­å®šä¹‰æ¨¡å‹ åœ¨ Flax å’Œ TensorFlow ä¸­çš„æ¿€æ´»å‡½æ•°
    åœ¨ Flax å’Œ TensorFlow ä¸­çš„ä¼˜åŒ–å™¨ åœ¨ Flax å’Œ TensorFlow ä¸­çš„æŒ‡æ ‡ åœ¨ Flax å’Œ TensorFlow ä¸­è®¡ç®—æ¢¯åº¦ åœ¨
    Flax å’Œ TensorFlow ä¸­åŠ è½½æ•°æ®é›† åœ¨ Flax å’Œ TensorFlow ä¸­è®­ç»ƒæ¨¡å‹ åœ¨ Flax å’Œ TensorFlow ä¸­çš„åˆ†å¸ƒå¼è®­ç»ƒ
    ä½¿ç”¨ TPU åŠ é€Ÿå™¨ æ¨¡å‹è¯„ä¼° å¯è§†åŒ–æ¨¡å‹æ€§èƒ½ æ€»ç»“ ä»å¤´å¼€å§‹åœ¨ Flax ä¸­è®­ç»ƒ ResNetï¼ˆåˆ†å¸ƒå¼ ResNet è®­ç»ƒï¼‰ å®‰è£… Flax æ¨¡å‹ æ‰§è¡Œæ ‡å‡†å¯¼å…¥
    ä¸‹è½½æ•°æ®é›† åœ¨ Flax ä¸­åŠ è½½æ•°æ®é›† æ•°æ®è½¬æ¢ åœ¨ Flax ä¸­å®ä¾‹åŒ– ResNet æ¨¡å‹ è®¡ç®—æŒ‡æ ‡ åˆ›å»º Flax æ¨¡å‹è®­ç»ƒçŠ¶æ€ åº”ç”¨æ¨¡å‹å‡½æ•° Flax
    ä¸­çš„ TensorBoard è®­ç»ƒ Flax ResNet æ¨¡å‹ è¯„ä¼°å¸¦æœ‰ TensorBoard çš„æ¨¡å‹ å¯è§†åŒ– Flax æ¨¡å‹æ€§èƒ½ ä¿å­˜ Flax ResNet
    æ¨¡å‹ åŠ è½½ Flax ResNet æ¨¡å‹ æ€»ç»“ JAX å’Œ Flax ä¸­çš„è¿ç§»å­¦ä¹  å®‰è£… JAX ResNet ä¸‹è½½æ•°æ®é›† åœ¨ JAX ä¸­åŠ è½½æ•°æ® å¤„ç†æ•°æ®
    åœ¨ Flax ä¸­å®šä¹‰ ResNet æ¨¡å‹ è®¡ç®— Flax ä¸­çš„æŒ‡æ ‡ åˆ›å»º Flax ä¼˜åŒ–å™¨ å®šä¹‰ Flax æŸå¤±å‡½æ•° è®¡ç®— Flax æŒ‡æ ‡ åˆ›å»º Flax
    è®­ç»ƒçŠ¶æ€ è®­ç»ƒæ­¥éª¤ è¯„ä¼°æ­¥éª¤ åœ¨ Flax ä¸­è®­ç»ƒ ResNet æ¨¡å‹ è®¾ç½® Flax ä¸­çš„ TensorBoard è®­ç»ƒæ¨¡å‹ ä¿å­˜ Flax æ¨¡å‹ åŠ è½½ä¿å­˜çš„
    Flax æ¨¡å‹ è¯„ä¼° Flax ResNet æ¨¡å‹ å¯è§†åŒ–æ¨¡å‹æ€§èƒ½ æ€»ç»“ Elegyï¼ˆJAX å’Œ Flax ä¸­çš„é«˜çº§ APIï¼‰ æ•°æ®é¢„å¤„ç† åœ¨ Elegy
    ä¸­å®šä¹‰æ¨¡å‹ Elegy æ¨¡å‹æ‘˜è¦ Elegy ä¸­çš„åˆ†å¸ƒå¼è®­ç»ƒ Flax ä¸­çš„ Keras å›è°ƒ åœ¨ Flax ä¸­è®­ç»ƒ Elegy æ¨¡å‹ è¯„ä¼° Elegy æ¨¡å‹
    ä½¿ç”¨ TensorBoard å¯è§†åŒ– Elegy æ¨¡å‹ ç”¨ Matplotlib ç»˜åˆ¶æ¨¡å‹æ€§èƒ½ ä½¿ç”¨ Elegy æ¨¡å‹è¿›è¡Œé¢„æµ‹ ä¿å­˜å’ŒåŠ è½½ Elegy æ¨¡å‹
    æ€»ç»“** é™„å½• å…è´£å£°æ˜ ç‰ˆæƒ å…¶ä»–è¦å­¦ä¹ çš„äº‹é¡¹
- en: Download all notebooks
  id: totrans-3
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: ä¸‹è½½æ‰€æœ‰ç¬”è®°æœ¬
- en: Link to download all notebooks. The password for the ZIP file is FDCPJx0D5A6SO#%Qsg
  id: totrans-4
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä¸‹è½½æ‰€æœ‰ç¬”è®°æœ¬çš„é“¾æ¥ã€‚ZIP æ–‡ä»¶çš„å¯†ç æ˜¯ FDCPJx0D5A6SO#%Qsg
- en: JAX (What it is and how to use it in Python)
  id: totrans-5
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: JAXï¼ˆå®ƒæ˜¯ä»€ä¹ˆä»¥åŠå¦‚ä½•åœ¨ Python ä¸­ä½¿ç”¨å®ƒï¼‰
- en: 'JAX is a Python library offering high performance in machine learning with
    XLA and Just In Time (JIT) compilation. Its API is similar to NumPy''s with a
    few differences. JAX ships with functionalities that aim to improve and increase
    speed in machine learning research. These functionalities include:'
  id: totrans-6
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAX æ˜¯ä¸€ä¸ª Python åº“ï¼Œåˆ©ç”¨ XLA å’Œå³æ—¶ï¼ˆJITï¼‰ç¼–è¯‘æä¾›é«˜æ€§èƒ½æœºå™¨å­¦ä¹ ã€‚å…¶ API ç±»ä¼¼äº NumPyï¼Œä½†æœ‰ä¸€äº›ä¸åŒä¹‹å¤„ã€‚JAX é…å¤‡äº†æ—¨åœ¨æ”¹å–„å’Œæé«˜æœºå™¨å­¦ä¹ ç ”ç©¶é€Ÿåº¦çš„åŠŸèƒ½ã€‚è¿™äº›åŠŸèƒ½åŒ…æ‹¬ï¼š
- en: Automatic differentiation
  id: totrans-7
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è‡ªåŠ¨å¾®åˆ†
- en: Vectorization
  id: totrans-8
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å‘é‡åŒ–
- en: JIT compilation
  id: totrans-9
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JIT ç¼–è¯‘
- en: This article will cover these functionalities and other JAX concepts. Let's
    get started.
  id: totrans-10
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æœ¬æ–‡å°†æ¶µç›–è¿™äº›åŠŸèƒ½å’Œå…¶ä»– JAX æ¦‚å¿µã€‚è®©æˆ‘ä»¬å¼€å§‹å§ã€‚
- en: What is XLA?
  id: totrans-11
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯ XLAï¼Ÿ
- en: XLA (Accelerated Linear Algebra) is a linear algebra compiler for accelerating
    machine learning models. It leads to an increase in the speed of model execution
    and reduced memory usage. XLA programs can be generated by JAX, PyTorch, Julia,
    and NX.
  id: totrans-12
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: XLAï¼ˆåŠ é€Ÿçº¿æ€§ä»£æ•°ï¼‰æ˜¯ç”¨äºåŠ é€Ÿæœºå™¨å­¦ä¹ æ¨¡å‹çš„çº¿æ€§ä»£æ•°ç¼–è¯‘å™¨ã€‚å®ƒå¯¼è‡´æ¨¡å‹æ‰§è¡Œé€Ÿåº¦çš„æé«˜å’Œå†…å­˜ä½¿ç”¨é‡çš„å‡å°‘ã€‚JAXã€PyTorchã€Julia å’Œ NX
    éƒ½å¯ä»¥ç”Ÿæˆ XLA ç¨‹åºã€‚
- en: Installing JAX
  id: totrans-13
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: å®‰è£… JAX
- en: 'JAX can be installed from the Python Package Index using: `pip install jax`
    JAX is pre-installed on Google Colab. See the link below for other installation
    options.'
  id: totrans-14
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAX å¯ä»¥é€šè¿‡ Python åŒ…ç´¢å¼•è¿›è¡Œå®‰è£…ï¼š `pip install jax` JAX å·²é¢„è£…åœ¨ Google Colab ä¸Šã€‚æŸ¥çœ‹ä¸‹é¢çš„é“¾æ¥è·å–å…¶ä»–å®‰è£…é€‰é¡¹ã€‚
- en: Setting up TPUs on Google Colab
  id: totrans-15
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨ Google Colab ä¸Šè®¾ç½® TPUs
- en: You need to set up JAX to use TPUs on Colab. That is done by executing the following
    code. Ensure that you have changed the runtime to TPU by going to Runtime-> Change
    Runtime Type. If no accelerator is available, JAX will use the CPU.
  id: totrans-16
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ‚¨éœ€è¦è®¾ç½® JAX ä»¥åœ¨ Colab ä¸Šä½¿ç”¨ TPUsã€‚é€šè¿‡æ‰§è¡Œä»¥ä¸‹ä»£ç æ¥å®Œæˆã€‚ç¡®ä¿æ‚¨å·²ç»å°†è¿è¡Œæ—¶æ›´æ”¹ä¸º TPUï¼Œæ–¹æ³•æ˜¯è½¬åˆ°è¿è¡Œæ—¶->æ›´æ”¹è¿è¡Œæ—¶ç±»å‹ã€‚å¦‚æœæ²¡æœ‰å¯ç”¨çš„åŠ é€Ÿå™¨ï¼ŒJAX
    å°†ä½¿ç”¨ CPUã€‚
- en: '`import jax.tools.colab_tpu jax.tools.colab_tpu.setup_tpu() jax.devices()`'
  id: totrans-17
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import jax.tools.colab_tpu jax.tools.colab_tpu.setup_tpu() jax.devices()`'
- en: Data types in JAX
  id: totrans-18
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: JAX ä¸­çš„æ•°æ®ç±»å‹
- en: The data types in NumPy are similar to those in JAX arrays. For instance, here
    is how you can create float and int data in JAX.
  id: totrans-19
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: NumPy ä¸­çš„æ•°æ®ç±»å‹ä¸ JAX æ•°ç»„ä¸­çš„ç±»ä¼¼ã€‚ä¾‹å¦‚ï¼Œä»¥ä¸‹æ˜¯å¦‚ä½•åœ¨ JAX ä¸­åˆ›å»ºæµ®ç‚¹æ•°å’Œæ•´æ•°æ•°æ®çš„æ–¹å¼ã€‚
- en: '`import jax.numpy as jnp x = jnp.float32(1.25844) x = jnp.int32(45.25844)`'
  id: totrans-20
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import jax.numpy as jnp x = jnp.float32(1.25844) x = jnp.int32(45.25844)`'
- en: When you check the type of the data, you will see that it's
  id: totrans-21
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å½“æ‚¨æ£€æŸ¥æ•°æ®ç±»å‹æ—¶ï¼Œæ‚¨ä¼šå‘ç°å®ƒæ˜¯
- en: a `DeviceArray.DeviceArray` in JAX is the equivalent of `numpy.ndarray` in NumPy.
  id: totrans-22
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAX ä¸­çš„ `DeviceArray.DeviceArray` ç›¸å½“äº NumPy ä¸­çš„ `numpy.ndarray`ã€‚
- en: '`jax.numpy` provides an interface similar to NumPy''s. However, JAX also provides
    `jax.lax` a low-level API that is more powerful and stricter. For example, with
    `[jax.numpy]` you can add numbers that have mixed types but `[jax.lax]` will not
    allow this.'
  id: totrans-23
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jax.numpy` æä¾›äº†ä¸ NumPy ç±»ä¼¼çš„æ¥å£ã€‚ä½†æ˜¯ï¼ŒJAX è¿˜æä¾›äº† `jax.lax`ï¼Œè¿™æ˜¯ä¸€ä¸ªæ›´å¼ºå¤§å’Œæ›´ä¸¥æ ¼çš„ä½çº§ APIã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨
    `[jax.numpy]` å¯ä»¥æ·»åŠ å…·æœ‰æ··åˆç±»å‹çš„æ•°å­—ï¼Œä½† `[jax.lax]` ä¸å…è®¸è¿™æ ·åšã€‚'
- en: Ways to create JAX arrays
  id: totrans-24
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: åˆ›å»º JAX æ•°ç»„çš„æ–¹æ³•
- en: You can create JAX arrays like you would in NumPy. For example, can use:arange
  id: totrans-25
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åƒåœ¨ NumPy ä¸­ä¸€æ ·åˆ›å»º JAX æ•°ç»„ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨ï¼šarange
- en: linspacePython lists.ones.zeros.identity.
  id: totrans-26
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: linspacePython lists.ones.zeros.identity.
- en: '`jnp.arange(10)`'
  id: totrans-27
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jnp.arange(10)`'
- en: '`jnp.arange(0,10)`'
  id: totrans-28
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jnp.arange(0,10)`'
- en: '`scores = [50,60,70,30,25,70] scores_array = jnp.array(scores) jnp.zeros(5)`'
  id: totrans-29
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`scores = [50,60,70,30,25,70] scores_array = jnp.array(scores) jnp.zeros(5)`'
- en: '`jnp.ones(5)`'
  id: totrans-30
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jnp.ones(5)`'
- en: '`jnp.eye(5)`'
  id: totrans-31
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jnp.eye(5)`'
- en: '`jnp.identity(5)`'
  id: totrans-32
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jnp.identity(5)`'
- en: '`![](../images/00002.jpeg)`'
  id: totrans-33
  prefs: []
  stylish: true
  type: TYPE_IMG
  zh: '`![](../images/00002.jpeg)`'
- en: Generating random numbers with JAX
  id: totrans-34
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ JAX ç”Ÿæˆéšæœºæ•°
- en: Random number generation is one main difference between JAX and NumPy. JAX is
    meant to be used with functional programs. JAX expects these functions to be pure.
    A **pure function** has no side effects and expects the output to only come from
    its inputs. JAX transformation functions expect pure functions.
  id: totrans-35
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: éšæœºæ•°ç”Ÿæˆæ˜¯ JAX ä¸ NumPy çš„ä¸€ä¸ªä¸»è¦åŒºåˆ«ã€‚JAX æ—¨åœ¨ä¸åŠŸèƒ½ç¨‹åºä¸€èµ·ä½¿ç”¨ã€‚JAX æœŸæœ›è¿™äº›å‡½æ•°æ˜¯çº¯å‡½æ•°ã€‚**çº¯å‡½æ•°** æ²¡æœ‰å‰¯ä½œç”¨ï¼Œå¹¶æœŸæœ›è¾“å‡ºä»…æ¥è‡ªå…¶è¾“å…¥ã€‚JAX
    è½¬æ¢å‡½æ•°æœŸæœ›çº¯å‡½æ•°ã€‚
- en: Therefore, when working with JAX, all input should be passed through function
    parameters, while all output should come from the function results. Hence, something
    like Python's print function is not pure.
  id: totrans-36
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œåœ¨ä½¿ç”¨ JAX æ—¶ï¼Œæ‰€æœ‰è¾“å…¥éƒ½åº”é€šè¿‡å‡½æ•°å‚æ•°ä¼ é€’ï¼Œè€Œæ‰€æœ‰è¾“å‡ºéƒ½åº”æ¥è‡ªå‡½æ•°ç»“æœã€‚å› æ­¤ï¼Œç±»ä¼¼äº Python çš„æ‰“å°å‡½æ•°ä¸æ˜¯çº¯å‡½æ•°ã€‚
- en: A pure function returns the same results when called with the same inputs. This
    is not possible withÂ [np.random.random()]Â because it is stateful and returns different
    results when called several times.
  id: totrans-37
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: çº¯å‡½æ•°åœ¨ä½¿ç”¨ç›¸åŒçš„è¾“å…¥è°ƒç”¨æ—¶è¿”å›ç›¸åŒçš„ç»“æœã€‚è¿™å¯¹äº[np.random.random()]æ˜¯ä¸å¯èƒ½çš„ï¼Œå› ä¸ºå®ƒæ˜¯æœ‰çŠ¶æ€çš„ï¼Œåœ¨å¤šæ¬¡è°ƒç”¨æ—¶è¿”å›ä¸åŒçš„ç»“æœã€‚
- en: '`print(np.random.random()) print(np.random.random()) print(np.random.random())`'
  id: totrans-38
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`print(np.random.random()) print(np.random.random()) print(np.random.random())`'
- en: '`![](../images/00003.jpeg)`'
  id: totrans-39
  prefs: []
  stylish: true
  type: TYPE_IMG
  zh: '`![](../images/00003.jpeg)`'
- en: JAX implements random number generation using a random state. This random state
    is referred to as aÂ `keyğŸ”‘`. JAX generates pseudorandom numbers from the pseudorandom
    number generator (PRNGs) Â state.
  id: totrans-40
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAXä½¿ç”¨éšæœºçŠ¶æ€æ¥å®ç°éšæœºæ•°ç”Ÿæˆã€‚è¿™ä¸ªéšæœºçŠ¶æ€è¢«ç§°ä¸º`keyğŸ”‘`ã€‚JAXä»ä¼ªéšæœºæ•°ç”Ÿæˆå™¨ï¼ˆPRNGsï¼‰çŠ¶æ€ä¸­ç”Ÿæˆä¼ªéšæœºæ•°ã€‚
- en: '`seed = 98`'
  id: totrans-41
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`seed = 98`'
- en: '`key = jax.random.PRNGKey(seed) jax.random.uniform(key)`'
  id: totrans-42
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`key = jax.random.PRNGKey(seed) jax.random.uniform(key)`'
- en: You should, therefore, not reuse the same state. Instead, you should split the
    PRNG to obtain as many sub keys as you need.`key, subkey = jax.random.split(key)
    Using the same key will always generate the same output. ![](../images/00004.gif)`
  id: totrans-43
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œä½ ä¸åº”è¯¥é‡ç”¨ç›¸åŒçš„çŠ¶æ€ã€‚ç›¸åï¼Œä½ åº”è¯¥åˆ†å‰²PRNGä»¥è·å¾—æ‰€éœ€æ•°é‡çš„å­é”®ã€‚`key, subkey = jax.random.split(key) ä½¿ç”¨ç›¸åŒçš„é”®å°†å§‹ç»ˆç”Ÿæˆç›¸åŒçš„è¾“å‡ºã€‚![](../images/00004.gif)`
- en: Pure functions
  id: totrans-44
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: çº¯å‡½æ•°
- en: We have mentioned that the output of a pure function should only come from the
    result of the function. Therefore, something like Python'sÂ [print]Â function introduces
    impurity. This can be demonstrated using this function.
  id: totrans-45
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»æåˆ°çº¯å‡½æ•°çš„è¾“å‡ºåº”è¯¥åªæ¥è‡ªå‡½æ•°çš„ç»“æœã€‚å› æ­¤ï¼ŒåƒPythonçš„[print]å‡½æ•°è¿™æ ·çš„ä¸œè¥¿ä¼šå¼•å…¥ä¸çº¯æ€§ã€‚è¿™å¯ä»¥é€šè¿‡è¿™ä¸ªå‡½æ•°æ¥æ¼”ç¤ºã€‚
- en: '`def impure_print_side_effect(x):`'
  id: totrans-46
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def impure_print_side_effect(x):`'
- en: '`print("Executing function")` # This is a side-effect return x'
  id: totrans-47
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`print("Executing function")` # è¿™æ˜¯ä¸€ä¸ªå‰¯ä½œç”¨è¿”å›x'
- en: 'The side-effects appear during the first run`print ("First call: ", jax.jit(impure_print_side_effect)(4.))`'
  id: totrans-48
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: 'å‰¯ä½œç”¨å‡ºç°åœ¨ç¬¬ä¸€æ¬¡è¿è¡Œæ—¶`print ("First call: ", jax.jit(impure_print_side_effect)(4.))`'
- en: Subsequent runs with parameters of same type and shape may no t show the side-effect
  id: totrans-49
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: ç›¸åŒç±»å‹å’Œå½¢çŠ¶çš„å‚æ•°è¿›è¡Œåç»­è¿è¡Œå¯èƒ½ä¸ä¼šæ˜¾ç¤ºå‰¯ä½œç”¨ã€‚
- en: This is because JAX now invokes a cached compilation of the f unction
  id: totrans-50
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å› ä¸ºJAXç°åœ¨è°ƒç”¨äº†å‡½æ•°çš„ç¼“å­˜ç¼–è¯‘
- en: '`print ("Second call: ", jax.jit(impure_print_side_effect)(5.))`'
  id: totrans-51
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`print ("Second call: ", jax.jit(impure_print_side_effect)(5.))`'
- en: JAX re-runs the Python function when the type or shape of the argument changes
  id: totrans-52
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: å½“å‚æ•°çš„ç±»å‹æˆ–å½¢çŠ¶å‘ç”Ÿå˜åŒ–æ—¶ï¼ŒJAXä¼šé‡æ–°è¿è¡ŒPythonå‡½æ•°
- en: '`print ("Third call, different type: ", jax.jit(impure_print_sid e_effect)(jnp.array([5.])))`'
  id: totrans-53
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`print ("Third call, different type: ", jax.jit(impure_print_sid e_effect)(jnp.array([5.])))`'
- en: '`![](../images/00005.gif)`'
  id: totrans-54
  prefs: []
  stylish: true
  type: TYPE_IMG
  zh: '`![](../images/00005.gif)`'
- en: We can see the printed statement the first time the function is executed. However,
    we don't see that print statement in consecutive runs because it is cached. We
    only see the statement again after changing the data's shape, which forces JAX
    to recompile the function. More onÂ jax.jitÂ in a moment.
  id: totrans-55
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€æ¬¡æ‰§è¡Œå‡½æ•°æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ‰“å°çš„è¯­å¥ã€‚ç„¶è€Œï¼Œåœ¨è¿ç»­è¿è¡Œä¸­æˆ‘ä»¬çœ‹ä¸åˆ°è¿™ä¸ªæ‰“å°è¯­å¥ï¼Œå› ä¸ºå®ƒè¢«ç¼“å­˜äº†ã€‚åªæœ‰åœ¨æ”¹å˜æ•°æ®å½¢çŠ¶åï¼Œå¼ºåˆ¶JAXé‡æ–°ç¼–è¯‘å‡½æ•°æ—¶ï¼Œæˆ‘ä»¬æ‰ä¼šå†æ¬¡çœ‹åˆ°è¿™ä¸ªè¯­å¥ã€‚ç¨åæˆ‘ä»¬ä¼šè¯¦ç»†è®¨è®º`jax.jit`ã€‚
- en: JAX NumPy operations
  id: totrans-56
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: JAX NumPyæ“ä½œ
- en: Operations on JAX arrays are similar to operations withÂ NumPyÂ arrays. For example,
    you canÂ [max],Â [argmax], andÂ [sum]Â like inÂ NumPy.
  id: totrans-57
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å¯¹JAXæ•°ç»„çš„æ“ä½œç±»ä¼¼äºå¯¹NumPyæ•°ç»„çš„æ“ä½œã€‚ä¾‹å¦‚ï¼Œä½ å¯ä»¥åƒåœ¨NumPyä¸­ä¸€æ ·ä½¿ç”¨[max]ã€[argmax]å’Œ[sum]ã€‚
- en: '`matrix = matrix.reshape(4,4) jnp.max(matrix)`'
  id: totrans-58
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`matrix = matrix.reshape(4,4) jnp.max(matrix)`'
- en: '`jnp.argmax(matrix)`'
  id: totrans-59
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jnp.argmax(matrix)`'
- en: '`jnp.min(matrix)`'
  id: totrans-60
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jnp.min(matrix)`'
- en: '`jnp.argmin(matrix)`'
  id: totrans-61
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jnp.argmin(matrix)`'
- en: '`jnp.sum(matrix)`'
  id: totrans-62
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jnp.sum(matrix)`'
- en: '`jnp.sqrt(matrix)`'
  id: totrans-63
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jnp.sqrt(matrix)`'
- en: '`matrix.transpose()`'
  id: totrans-64
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`matrix.transpose()`'
- en: '`![](../images/00006.jpeg)`However, JAX doesn''t allow operations on non-array
    input likeÂ NumPy. For example, passingÂ Python listsÂ or tuples will lead to an
    error.'
  id: totrans-65
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`![](../images/00006.jpeg)`ç„¶è€Œï¼ŒJAXä¸å…è®¸å¯¹éæ•°ç»„è¾“å…¥ï¼ˆå¦‚NumPyä¸­çš„Pythonåˆ—è¡¨æˆ–å…ƒç»„ï¼‰è¿›è¡Œæ“ä½œï¼Œè¿™ä¼šå¯¼è‡´é”™è¯¯ã€‚'
- en: '`try:`'
  id: totrans-66
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`try:`'
- en: '`jnp.sum([1, 2, 3])`'
  id: totrans-67
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jnp.sum([1, 2, 3])`'
- en: '`except TypeError as e:`'
  id: totrans-68
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`except TypeError as e:`'
- en: '`print(f"TypeError: {e}")`'
  id: totrans-69
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`print(f"TypeError: {e}")`'
- en: '`TypeError: sum requires ndarray or scalar arguments, got <c`'
  id: totrans-70
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`TypeError: sum requires ndarray or scalar arguments, got <c`'
- en: '`lass ''list''> at position 0.`'
  id: totrans-71
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`lass ''list''> at position 0.`'
- en: JAX arrays are immutable
  id: totrans-72
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: JAXæ•°ç»„æ˜¯ä¸å¯å˜çš„
- en: Unlike inÂ NumPy, JAX arrays can not be modified in place. This is because JAX
    expects pure functions.
  id: totrans-73
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä¸NumPyä¸åŒï¼ŒJAXæ•°ç»„ä¸èƒ½å°±åœ°ä¿®æ”¹ã€‚è¿™æ˜¯å› ä¸ºJAXæœŸæœ›çº¯å‡½æ•°ã€‚
- en: '`scores = [50,60,70,30,25]`'
  id: totrans-74
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`scores = [50,60,70,30,25]`'
- en: '`scores_array = jnp.array(scores)`'
  id: totrans-75
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`scores_array = jnp.array(scores)`'
- en: '`scores_array[0:3] = [20,40,90]`'
  id: totrans-76
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`scores_array[0:3] = [20,40,90]`'
- en: '`TypeError: ''<class ''jaxlib.xla_extension.DeviceArray''>'' objec t does not
    support item assignment.`'
  id: totrans-77
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`TypeError: ''<class ''jaxlib.xla_extension.DeviceArray''>'' objec t does not
    support item assignment.`'
- en: JAX arrays are immutable. Instead of ``x[idx] = y``, use ``x
  id: totrans-78
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: JAXæ•°ç»„æ˜¯ä¸å¯å˜çš„ã€‚è€Œä¸æ˜¯``x[idx] = y``ï¼Œè¯·ä½¿ç”¨``x
- en: '`= x.at[idx].set(y)`` or another `.at[]`'
  id: totrans-79
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`= x.at[idx].set(y)`` æˆ–è€…å¦ä¸€ä¸ª `.at[]`'
- en: 'method: `https://jax.readthedocs.io/en/latest/_autosummary/ja x.numpy.ndarray.at.html`'
  id: totrans-80
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: æ–¹æ³•ï¼š`https://jax.readthedocs.io/en/latest/_autosummary/ja x.numpy.ndarray.at.html`
- en: Array updates in JAX are performed using `[x.at[idx].set(y)]`. This returns
    a new array while the old array stays unaltered.
  id: totrans-81
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAXä¸­çš„æ•°ç»„æ›´æ–°ä½¿ç”¨`[x.at[idx].set(y)]`è¿›è¡Œã€‚è¿™å°†è¿”å›ä¸€ä¸ªæ–°æ•°ç»„ï¼Œè€Œæ—§æ•°ç»„ä¿æŒä¸å˜ã€‚
- en: '`try:`'
  id: totrans-82
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`try:`'
- en: '`jnp.sum([1, 2, 3])`'
  id: totrans-83
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jnp.sum([1, 2, 3])`'
- en: '`except TypeError as e:`'
  id: totrans-84
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`except TypeError as e:`'
- en: '`print(f"TypeError: {e}")`'
  id: totrans-85
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`print(f"TypeError: {e}")`'
- en: '`TypeError: sum requires ndarray or scalar arguments, got <c`'
  id: totrans-86
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`TypeError: suméœ€è¦ndarrayæˆ–æ ‡é‡å‚æ•°ï¼Œå¾—åˆ°<c`'
- en: '`lass ''list''> at position 0.`'
  id: totrans-87
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`åœ¨ä½ç½®0å¤„''list''åˆ—è¡¨ã€‚`'
- en: '**Out-of-Bounds Indexing**'
  id: totrans-88
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**è¶Šç•Œç´¢å¼•**'
- en: '`NumPy` usually throws an error when you try to get an item in an array that
    is out of bounds. JAX doesn''t throw any error but returns the last item in the
    array.'
  id: totrans-89
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`NumPy`é€šå¸¸åœ¨æ‚¨å°è¯•è·å–æ•°ç»„ä¸­è¶…å‡ºè¾¹ç•Œçš„é¡¹æ—¶ä¼šæŠ›å‡ºé”™è¯¯ã€‚JAXä¸ä¼šæŠ›å‡ºä»»ä½•é”™è¯¯ï¼Œè€Œæ˜¯è¿”å›æ•°ç»„ä¸­çš„æœ€åä¸€é¡¹ã€‚'
- en: '`matrix = jnp.arange(1,17) matrix[20]`'
  id: totrans-90
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`matrix = jnp.arange(1,17) matrix[20]`'
- en: '`DeviceArray(16, dtype=int32)`'
  id: totrans-91
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`DeviceArray(16, dtype=int32)`'
- en: JAX is designed like this because throwing errors in accelerators can be challenging.
  id: totrans-92
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAXè®¾è®¡å¦‚æ­¤ï¼Œå› ä¸ºåœ¨åŠ é€Ÿå™¨ä¸­æŠ›å‡ºé”™è¯¯å¯èƒ½ä¼šå¾ˆå…·æœ‰æŒ‘æˆ˜æ€§ã€‚
- en: '**Data placement on devices in JAX**'
  id: totrans-93
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**JAXä¸­çš„æ•°æ®æ”¾ç½®**'
- en: JAX arrays are placed in the first device, `[jax.devices()[0]]`that is, GPU,
    TPU, or CPU. Data can be placed on a particular device
  id: totrans-94
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAXæ•°ç»„è¢«æ”¾ç½®åœ¨ç¬¬ä¸€ä¸ªè®¾å¤‡ä¸Šï¼Œ`[jax.devices()[0]]`å³GPUã€TPUæˆ–CPUã€‚æ•°æ®å¯ä»¥è¢«æ”¾ç½®åœ¨ç‰¹å®šçš„è®¾å¤‡ä¸Š
- en: using `jax.device_put()`.
  id: totrans-95
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`jax.device_put()`ã€‚
- en: from `jax` import `device_put`
  id: totrans-96
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from jax import device_put`'
- en: import `numpy` as `np`
  id: totrans-97
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import numpy as np`'
- en: '`size = 5000`'
  id: totrans-98
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`size = 5000`'
- en: '`x = np.random.normal(size=(size, size)).astype(np.float32) x = device_put(x)`'
  id: totrans-99
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = np.random.normal(size=(size, size)).astype(np.float32) x = device_put(x)`'
- en: The data becomes committed to that device, and operations on it are also committed
    on the same device.
  id: totrans-100
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ•°æ®å°†æäº¤åˆ°è¯¥è®¾å¤‡ï¼Œå¹¶ä¸”å¯¹å…¶è¿›è¡Œçš„æ“ä½œä¹Ÿå°†æäº¤åˆ°åŒä¸€è®¾å¤‡ã€‚
- en: '**How fast is JAX?**'
  id: totrans-101
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**JAXçš„é€Ÿåº¦æœ‰å¤šå¿«ï¼Ÿ**'
- en: JAX uses `asynchronous` dispatch, meaning that it does not wait for computation
    to complete to give control back to the `Python program`. Therefore, when you
    perform an execution, JAX will return a future. JAX forces Python to wait for
    the execution when you want to print the output or if you convert the result to
    a `NumPy array`.
  id: totrans-102
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAXä½¿ç”¨`asynchronous`åˆ†å‘ï¼Œè¿™æ„å‘³ç€å®ƒä¸ä¼šç­‰å¾…è®¡ç®—å®Œæˆå°±å°†æ§åˆ¶æƒäº¤è¿˜ç»™`Pythonç¨‹åº`ã€‚å› æ­¤ï¼Œå½“æ‚¨æ‰§è¡Œä¸€ä¸ªæ“ä½œæ—¶ï¼ŒJAXä¼šè¿”å›ä¸€ä¸ªfutureã€‚å½“æ‚¨æƒ³è¦æ‰“å°è¾“å‡ºæˆ–å°†ç»“æœè½¬æ¢ä¸º`NumPyæ•°ç»„`æ—¶ï¼ŒJAXä¼šå¼ºåˆ¶Pythonç­‰å¾…æ‰§è¡Œã€‚
- en: Therefore, if you want to compute the time of execution of a program you'll
    have to convert the result to a `NumPy` array
  id: totrans-103
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œå¦‚æœæ‚¨æƒ³è®¡ç®—ç¨‹åºçš„æ‰§è¡Œæ—¶é—´ï¼Œæ‚¨å°†ä¸å¾—ä¸å°†ç»“æœè½¬æ¢ä¸º`NumPy`æ•°ç»„
- en: using `[block_until_ready()]` to wait for the execution to complete. Generally
    speaking, `NumPy` will outperform JAX on the CPU, but JAX will outperform `NumPy`
    on accelerators and when using jitted functions.
  id: totrans-104
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`[block_until_ready()]`ç­‰å¾…æ‰§è¡Œå®Œæˆã€‚ä¸€èˆ¬æ¥è¯´ï¼Œåœ¨CPUä¸Šï¼Œ`NumPy`çš„æ€§èƒ½ä¼˜äºJAXï¼Œä½†åœ¨åŠ é€Ÿå™¨å’Œä½¿ç”¨jittedå‡½æ•°æ—¶ï¼ŒJAXçš„æ€§èƒ½ä¼˜äº`NumPy`ã€‚
- en: '**Using jit() to speed up functions**'
  id: totrans-105
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**ä½¿ç”¨jit()åŠ é€Ÿå‡½æ•°**'
- en: '`[jit]` performs just-in-time compilation with XLA. `[jax.jit]` expects a pure
    function. Any side effects in the function will only be executed once. Let''s
    create a pure function and time its execution time without `jit`.'
  id: totrans-106
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[jit]`ä½¿ç”¨XLAè¿›è¡Œå³æ—¶ç¼–è¯‘ã€‚`[jax.jit]`æœŸæœ›ä¸€ä¸ªçº¯å‡½æ•°ã€‚å‡½æ•°ä¸­çš„ä»»ä½•å‰¯ä½œç”¨å°†åªæ‰§è¡Œä¸€æ¬¡ã€‚è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªçº¯å‡½æ•°ï¼Œå¹¶è®¡ç®—å…¶åœ¨æ²¡æœ‰`jit`çš„æƒ…å†µä¸‹çš„æ‰§è¡Œæ—¶é—´ã€‚'
- en: '`def test_fn(sample_rate=3000,frequency=3):`'
  id: totrans-107
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def test_fn(sample_rate=3000,frequency=3):`'
- en: '`x = jnp.arange(sample_rate)`'
  id: totrans-108
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = jnp.arange(sample_rate)`'
- en: '`y = np.sin(2*jnp.pi*frequency * (frequency/sample_rate)) return jnp.dot(x,y)`'
  id: totrans-109
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`y = np.sin(2*jnp.pi*frequency * (frequency/sample_rate)) return jnp.dot(x,y)`'
- en: '`%timeit test_fn()`'
  id: totrans-110
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`%timeit test_fn()`'
- en: '`best of 5: 76.1 Âµs per loop`'
  id: totrans-111
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`æœ€ä½³ç»“æœä¸º 5 æ¬¡ï¼šæ¯æ¬¡å¾ªç¯ 76.1 Âµs`'
- en: Let's now use `jit` and time the execution of the same function. In this case,
    we can see that using `jit` makes the execution almost 20 times faster.
  id: totrans-112
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨`jit`å¹¶è®¡ç®—ç›¸åŒå‡½æ•°çš„æ‰§è¡Œæ—¶é—´ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä½¿ç”¨`jit`ä½¿æ‰§è¡Œé€Ÿåº¦å‡ ä¹å¿«äº†20å€ã€‚
- en: '`test_fn_jit = jax.jit(test_fn)`'
  id: totrans-113
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`test_fn_jit = jax.jit(test_fn)`'
- en: '`%timeit test_fn_jit().block_until_ready()` # best of 5: 4.54 Âµs per loop'
  id: totrans-114
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`%timeit test_fn_jit().block_until_ready()` # æœ€ä½³ç»“æœä¸º 5 æ¬¡ï¼šæ¯æ¬¡å¾ªç¯ 4.54 Âµs'
- en: In the above example, `[test_fn_jit]` is the jit-compiled version of the function.
    JAX then created code that is optimized for GPU or TPU. The optimized code is
    what will be used the next time this function is called.
  id: totrans-115
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œ`[test_fn_jit]`æ˜¯å‡½æ•°çš„jitç¼–è¯‘ç‰ˆæœ¬ã€‚ç„¶åJAXåˆ›å»ºäº†é’ˆå¯¹GPUæˆ–TPUä¼˜åŒ–çš„ä»£ç ã€‚ä¼˜åŒ–åçš„ä»£ç å°†åœ¨ä¸‹æ¬¡è°ƒç”¨æ­¤å‡½æ•°æ—¶ä½¿ç”¨ã€‚
- en: '**How JIT works**'
  id: totrans-116
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**JITçš„å·¥ä½œåŸç†**'
- en: JAX works by converting Python functions into an intermediate language called
    jaxpr (JAX Expression). The `[jax.make_jaxpr]` can be used to show the jaxpr representation
    of a Python function. If the function has any side effects, they are not recorded
    by jaxpr. We saw earlier that any side effects, for example, printing, will only
    be shown during the first call.
  id: totrans-117
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAXé€šè¿‡å°†Pythonå‡½æ•°è½¬æ¢ä¸ºä¸€ç§ç§°ä¸ºjaxprï¼ˆJAXè¡¨è¾¾å¼ï¼‰çš„ä¸­é—´è¯­è¨€æ¥å·¥ä½œã€‚`[jax.make_jaxpr]`å¯ç”¨äºæ˜¾ç¤ºPythonå‡½æ•°çš„jaxprè¡¨ç¤ºã€‚å¦‚æœå‡½æ•°æœ‰ä»»ä½•å‰¯ä½œç”¨ï¼Œå®ƒä»¬ä¸ä¼šè¢«jaxprè®°å½•ã€‚æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°ï¼Œä¾‹å¦‚æ‰“å°çš„ä»»ä½•å‰¯ä½œç”¨åªä¼šåœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶æ˜¾ç¤ºå‡ºæ¥ã€‚
- en: '`def sum_logistic(x):`'
  id: totrans-118
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def sum_logistic(x):`'
- en: '`print("printed x:", x)`'
  id: totrans-119
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`print("printed x:", x)`'
- en: '`return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))`'
  id: totrans-120
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))`'
- en: '`x_small = jnp.arange(6.)print(jax.make_jaxpr(sum_logistic)(x_small))`'
  id: totrans-121
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x_small = jnp.arange(6.)print(jax.make_jaxpr(sum_logistic)(x_small))`'
- en: JAX creates the jaxpr through tracing. Each argument in the function is wrapped
    with a tracer object. The purpose of these tracers is to record all JAX operations
    performed on them when the function is called. JAX uses the tracer records to
    rebuild the function, which leads to jaxpr. Python side-effects don't show up
    in the jaxpr because the tracers do not record them.
  id: totrans-122
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAXé€šè¿‡è¿½è¸ªåˆ›å»ºjaxprã€‚å‡½æ•°ä¸­çš„æ¯ä¸ªå‚æ•°éƒ½ä¼šè¢«åŒ…è£…æˆè¿½è¸ªå™¨å¯¹è±¡ã€‚è¿™äº›è¿½è¸ªå™¨çš„ç›®çš„æ˜¯åœ¨è°ƒç”¨å‡½æ•°æ—¶è®°å½•å¯¹å®ƒä»¬æ‰§è¡Œçš„æ‰€æœ‰JAXæ“ä½œã€‚JAXä½¿ç”¨è¿½è¸ªå™¨è®°å½•é‡å»ºå‡½æ•°ï¼Œä»è€Œå¾—åˆ°jaxprã€‚Pythonçš„å‰¯ä½œç”¨ä¸ä¼šå‡ºç°åœ¨jaxprä¸­ï¼Œå› ä¸ºè¿½è¸ªå™¨ä¸è®°å½•å®ƒä»¬ã€‚
- en: JAX requires arrays shapes to be static and known at compile time. Decorating
    a function conditioned on a value with jit results in error. Therefore, not all
    code can be jit-compiled.
  id: totrans-123
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAXè¦æ±‚æ•°ç»„å½¢çŠ¶åœ¨ç¼–è¯‘æ—¶æ˜¯é™æ€å’Œå·²çŸ¥çš„ã€‚å°†å¸¦æœ‰å€¼æ¡ä»¶çš„å‡½æ•°ä½¿ç”¨jitä¿®é¥°ä¼šå¯¼è‡´é”™è¯¯ã€‚å› æ­¤ï¼Œå¹¶éæ‰€æœ‰ä»£ç éƒ½å¯ä»¥jitç¼–è¯‘ã€‚
- en: '`@jax.jitdef f(boolean, x):return -x if boolean else x`'
  id: totrans-124
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`@jax.jitdef f(boolean, x):return -x if boolean else x`'
- en: '`f(True, 1)`'
  id: totrans-125
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`f(True, 1)`'
- en: '`ConcretizationTypeError`: Abstract tracer value encountered where concrete
    value is expected: `Traced<ShapedArray(bool[], weak _type=True)>with<DynamicJaxprTrace(level=0/1)>`'
  id: totrans-126
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`ConcretizationTypeError`: é‡åˆ°æŠ½è±¡è¿½è¸ªå™¨å€¼ï¼Œä½†éœ€è¦å…·ä½“å€¼ï¼š`Traced<ShapedArray(bool[], weak
    _type=True)>with<DynamicJaxprTrace(level=0/1)>`'
- en: 'There are a couple of solutions to this problem:'
  id: totrans-127
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å¯¹äºè¿™ä¸ªé—®é¢˜æœ‰å‡ ä¸ªè§£å†³æ–¹æ¡ˆï¼š
- en: Remove conditionals on the value. Use JAX control flow operators such as `[jax.lax.cond]`.
  id: totrans-128
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ç§»é™¤å€¼æ¡ä»¶ã€‚ä½¿ç”¨JAXæ§åˆ¶æµæ“ä½œç¬¦å¦‚`[jax.lax.cond]`ã€‚
- en: Jit only a part of the function. Make parameters static.
  id: totrans-129
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä»…å¯¹å‡½æ•°çš„ä¸€éƒ¨åˆ†ä½¿ç”¨jitã€‚ä½¿å‚æ•°é™æ€åŒ–ã€‚
- en: We can implement the last option and make the boolean parameter static. This
    is done by specifying `[static_argnums]` or `[static_argnames]`. This forces JAX
    to recompile the function when the value of the static parameter changes. This
    is not a good strategy if the function will get many values for the static argument.
    You don't want to recompile the function too many times.
  id: totrans-130
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å®ç°æœ€åä¸€ç§é€‰é¡¹å¹¶ä½¿å¸ƒå°”å‚æ•°é™æ€åŒ–ã€‚è¿™å¯ä»¥é€šè¿‡æŒ‡å®š`[static_argnums]`æˆ–`[static_argnames]`æ¥å®Œæˆã€‚å½“é™æ€å‚æ•°çš„å€¼å˜åŒ–æ—¶ï¼Œè¿™ä¼šå¼ºåˆ¶JAXé‡æ–°ç¼–è¯‘å‡½æ•°ã€‚å¦‚æœå‡½æ•°ä¼šè·å¾—è®¸å¤šé™æ€å‚æ•°çš„å€¼ï¼Œè¿™ä¸æ˜¯ä¸€ä¸ªå¥½ç­–ç•¥ã€‚ä½ ä¸å¸Œæœ›å‡½æ•°é‡æ–°ç¼–è¯‘å¤ªå¤šæ¬¡ã€‚
- en: You can pass the static arguments using Pythonâ€™s `[functools.partial]`. `from
    functools import partial@partial(jax.jit, static_argnums=(0,)) def f(boolean,
    x):return -x if boolean else xf(True, 1)`
  id: totrans-131
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥ä½¿ç”¨Pythonçš„`[functools.partial]`ä¼ é€’é™æ€å‚æ•°ã€‚`from functools import partial@partial(jax.jit,
    static_argnums=(0,)) def f(boolean, x):return -x if boolean else xf(True, 1)`
- en: '**Taking derivatives with `grad()`**'
  id: totrans-132
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**ä½¿ç”¨`grad()`è®¡ç®—å¯¼æ•°**'
- en: Computing derivatives in JAX is done using `jax.grad.` `@jax.jitdef sum_logistic(x):return
    jnp.sum(1.0 / (1.0 + jnp.exp(-x)))`
  id: totrans-133
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨JAXä¸­è®¡ç®—å¯¼æ•°æ˜¯é€šè¿‡`jax.grad.`å®Œæˆçš„ã€‚`@jax.jitdef sum_logistic(x):return jnp.sum(1.0 /
    (1.0 + jnp.exp(-x)))`
- en: '`x_small = jnp.arange(6.)`'
  id: totrans-134
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x_small = jnp.arange(6.)`'
- en: '`derivative_fn = jax.grad(sum_logistic) print(derivative_fn(x_small))`'
  id: totrans-135
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`derivative_fn = jax.grad(sum_logistic) print(derivative_fn(x_small))`'
- en: The `[grad]` function has a `[has_aux]` argument that allows you to return auxiliary
    data. For example, when building machine learning models, you can use it to return
    loss and gradients.
  id: totrans-136
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[grad]`å‡½æ•°æœ‰ä¸€ä¸ª`[has_aux]`å‚æ•°ï¼Œå…è®¸ä½ è¿”å›è¾…åŠ©æ•°æ®ã€‚ä¾‹å¦‚ï¼Œåœ¨æ„å»ºæœºå™¨å­¦ä¹ æ¨¡å‹æ—¶ï¼Œä½ å¯ä»¥ç”¨å®ƒè¿”å›æŸå¤±å’Œæ¢¯åº¦ã€‚'
- en: '`@jax.jitdef sum_logistic(x):return jnp.sum(1.0 / (1.0 + jnp.exp(-x))),(x +
    1)`'
  id: totrans-137
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`@jax.jitdef sum_logistic(x):return jnp.sum(1.0 / (1.0 + jnp.exp(-x))),(x +
    1)`'
- en: '`x_small = jnp.arange(6.)`'
  id: totrans-138
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x_small = jnp.arange(6.)`'
- en: '`derivative_fn = jax.grad(sum_logistic, has_aux=True) print(derivative_fn(x_small))`'
  id: totrans-139
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`derivative_fn = jax.grad(sum_logistic, has_aux=True) print(derivative_fn(x_small))`'
- en: '`(DeviceArray([0.25 , 0.19661194, 0.10499357, 0.04517666, 0.01766271,`'
  id: totrans-140
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`(DeviceArray([0.25 , 0.19661194, 0.10499357, 0.04517666, 0.01766271,`'
- en: '`0.00664806], dtype=float32), DeviceArray([1., 2.,`'
  id: totrans-141
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`0.00664806], dtype=float32), DeviceArray([1., 2.,`'
- en: '`3., 4., 5., 6.], dtype=float32))`You can perform advanced automatic differentiation
    using **`jax.vjp()`**and **`jax.jvp()`**. **## Auto-vectorization with vmap**'
  id: totrans-142
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`3., 4., 5., 6.], dtype=float32))`ä½¿ç”¨ **`jax.vjp()`** å’Œ **`jax.jvp()`** å¯ä»¥æ‰§è¡Œé«˜çº§è‡ªåŠ¨å¾®åˆ†ã€‚
    **## ä½¿ç”¨ vmap è¿›è¡Œè‡ªåŠ¨å‘é‡åŒ–**'
- en: vmap(Vectorizing map) allows you write a function that can be applied to a single
    data and then vmap will map it to a batch of data. Without vmap the solution would
    be to loop through the batches while applying the function. Using jit with for
    loops is a little complicated and may be slower.
  id: totrans-143
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: vmapï¼ˆå‘é‡åŒ–æ˜ å°„ï¼‰å…è®¸ä½ ç¼–å†™ä¸€ä¸ªå¯ä»¥åº”ç”¨äºå•ä¸ªæ•°æ®çš„å‡½æ•°ï¼Œç„¶å vmap å°†å…¶æ˜ å°„åˆ°ä¸€æ‰¹æ•°æ®ä¸­ã€‚å¦‚æœæ²¡æœ‰ vmapï¼Œåˆ™è§£å†³æ–¹æ¡ˆå°†æ˜¯é€šè¿‡æ‰¹å¤„ç†å¾ªç¯åº”ç”¨å‡½æ•°ã€‚åœ¨ä½¿ç”¨
    jit å’Œå¾ªç¯çš„æƒ…å†µä¸‹ï¼Œè§£å†³æ–¹æ¡ˆä¼šæ›´åŠ å¤æ‚ä¸”å¯èƒ½ä¼šæ›´æ…¢ã€‚
- en: '`mat = jax.random.normal(key, (150, 100)) batched_x = jax.random.normal(key,
    (10, 100)) def apply_matrix(v):`'
  id: totrans-144
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`mat = jax.random.normal(key, (150, 100)) batched_x = jax.random.normal(key,
    (10, 100)) def apply_matrix(v):`'
- en: '`return jnp.dot(mat, v)`'
  id: totrans-145
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return jnp.dot(mat, v)`'
- en: '`@jax.jit`'
  id: totrans-146
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`@jax.jit`'
- en: '`def vmap_batched_apply_matrix(v_batched):`'
  id: totrans-147
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def vmap_batched_apply_matrix(v_batched):`'
- en: '`return jax.vmap(apply_matrix)(v_batched)`'
  id: totrans-148
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return jax.vmap(apply_matrix)(v_batched)`'
- en: '`print(''Auto-vectorized with vmap'')`'
  id: totrans-149
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`print(''ä½¿ç”¨ vmap è¿›è¡Œè‡ªåŠ¨å‘é‡åŒ–'')`'
- en: '`%timeit vmap_batched_apply_matrix(batched_x).block_until_ready ()`'
  id: totrans-150
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`%timeit vmap_batched_apply_matrix(batched_x).block_until_ready ()`'
- en: In JAX, the `[jax.vmap]` transformation is designed to generate a vectorized
    implementation of a function automatically. It does this by tracing the function
    similarly to `[jax.jit]`, and automatically adding batch axes at the beginning
    of each input. If the batch dimension is not the first, you may use the `[in_axes]`
    and `[out_axes]` arguments to specify the location of the batch dimension in inputs
    and outputs. These may be an integer if the batch axis is the same for all inputs
    and outputs, or lists, otherwise. `Matteo Hessel, JAX author.`
  id: totrans-151
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨ JAX ä¸­ï¼Œ`[jax.vmap]` è½¬æ¢è®¾è®¡ä¸ºè‡ªåŠ¨ç”Ÿæˆå‡½æ•°çš„å‘é‡åŒ–å®ç°ã€‚å®ƒé€šè¿‡ç±»ä¼¼äº `[jax.jit]` çš„è·Ÿè¸ªåŠŸèƒ½å®ç°ï¼Œè‡ªåŠ¨åœ¨æ¯ä¸ªè¾“å…¥çš„å¼€å¤´æ·»åŠ æ‰¹å¤„ç†è½´ã€‚å¦‚æœæ‰¹å¤„ç†ç»´åº¦ä¸æ˜¯ç¬¬ä¸€ä¸ªï¼Œå¯ä»¥ä½¿ç”¨
    `[in_axes]` å’Œ `[out_axes]` å‚æ•°æ¥æŒ‡å®šè¾“å…¥å’Œè¾“å‡ºä¸­æ‰¹å¤„ç†ç»´åº¦çš„ä½ç½®ã€‚å¦‚æœæ‰€æœ‰è¾“å…¥å’Œè¾“å‡ºçš„æ‰¹å¤„ç†è½´ç›¸åŒï¼Œå¯ä»¥ä½¿ç”¨æ•´æ•°ï¼Œå¦åˆ™å¯ä»¥ä½¿ç”¨åˆ—è¡¨ã€‚`Matteo
    Hesselï¼ŒJAX ä½œè€…ã€‚`
- en: Parallelization with `[pmap]`
  id: totrans-152
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ `[pmap]` è¿›è¡Œå¹¶è¡ŒåŒ–
- en: The working of `jax.pmap` is similar to `jax.vmap`. The difference is that `jax.pmap`
    is meant for parallel execution, that is, computation on multiple devices. This
    is applicable when training a machine learning model on batches of data.
  id: totrans-153
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jax.pmap` çš„å·¥ä½œæ–¹å¼ç±»ä¼¼äº `jax.vmap`ã€‚ä¸åŒä¹‹å¤„åœ¨äº `jax.pmap` ç”¨äºå¹¶è¡Œæ‰§è¡Œï¼Œå³åœ¨å¤šä¸ªè®¾å¤‡ä¸Šè¿›è¡Œè®¡ç®—ã€‚è¿™åœ¨å¯¹æ•°æ®æ‰¹æ¬¡è¿›è¡Œè®­ç»ƒæ—¶æ˜¯é€‚ç”¨çš„ã€‚'
- en: Computation on batches can occur in different devices then the results are aggregated.
    The `[pmap]`ed function returns a `[ShardedDeviceArray]`. This is because the
    arrays are split across all the devices. There is no need to decorate the function
    with jit because the function is jitcompiled by default when using `[pmap]`.
  id: totrans-154
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è®¡ç®—åœ¨ä¸åŒè®¾å¤‡ä¸Šçš„æ‰¹å¤„ç†å¯èƒ½ä¼šå‘ç”Ÿï¼Œç„¶åå°†ç»“æœèšåˆã€‚è¢« `[pmap]` çš„å‡½æ•°è¿”å›ä¸€ä¸ª `[ShardedDeviceArray]`ã€‚è¿™æ˜¯å› ä¸ºæ•°ç»„åˆ†å¸ƒåœ¨æ‰€æœ‰è®¾å¤‡ä¸Šã€‚ä¸éœ€è¦ç”¨
    jit è£…é¥°å‡½æ•°ï¼Œå› ä¸ºä½¿ç”¨ `[pmap]` æ—¶å‡½æ•°é»˜è®¤ä¸º jit ç¼–è¯‘ã€‚
- en: '`x = np.arange(5)w = np.array([2., 3., 4.])`'
  id: totrans-155
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = np.arange(5)w = np.array([2., 3., 4.])`'
- en: '`def convolve(x, w):`'
  id: totrans-156
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def convolve(x, w):`'
- en: '`output = []`'
  id: totrans-157
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`output = []`'
- en: '`for i in range(1, len(x)-1):`'
  id: totrans-158
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`for i in range(1, len(x)-1):`'
- en: '`output.append(jnp.dot(x[i-1:i+2], w)) return jnp.array(output)`'
  id: totrans-159
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`output.append(jnp.dot(x[i-1:i+2], w)) return jnp.array(output)`'
- en: '`convolve(x, w) n_devices = jax.local_device_count()`'
  id: totrans-160
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`convolve(x, w) n_devices = jax.local_device_count()`'
- en: '`xs = np.arange(5 * n_devices).reshape(-1, 5)`'
  id: totrans-161
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`xs = np.arange(5 * n_devices).reshape(-1, 5)`'
- en: '`ws = np.stack([w] * n_devices)`'
  id: totrans-162
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`ws = np.stack([w] * n_devices)`'
- en: '`jax.pmap(convolve)(xs, ws)`'
  id: totrans-163
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jax.pmap(convolve)(xs, ws)`'
- en: '`ShardedDeviceArray([[ 11., 20., 29.],`'
  id: totrans-164
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`ShardedDeviceArray([[ 11., 20., 29.],`'
- en: '`.................`'
  id: totrans-165
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`.................`'
- en: '`[326., 335., 344.]], dtype=float32)`'
  id: totrans-166
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`[326., 335., 344.]], dtype=float32)`'
- en: You may need to aggregate data using one of the `[collective operators]`, for
    example, to compute the mean of the accuracy or mean of the logits. In that case,
    you'll need to specify an `[axis_name]`. This name is important to achieve communication
    between devices.
  id: totrans-167
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½éœ€è¦ä½¿ç”¨å…¶ä¸­ä¸€ä¸ª `[collective operators]` æ¥èšåˆæ•°æ®ï¼Œä¾‹å¦‚ï¼Œè®¡ç®—å‡†ç¡®åº¦çš„å¹³å‡å€¼æˆ– logits çš„å¹³å‡å€¼ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ éœ€è¦æŒ‡å®šä¸€ä¸ª
    `[axis_name]`ã€‚è¿™ä¸ªåç§°åœ¨è®¾å¤‡ä¹‹é—´çš„é€šä¿¡ä¸­å¾ˆé‡è¦ã€‚
- en: Debugging NANs in JAX
  id: totrans-168
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨ JAX ä¸­è°ƒè¯• NANs
- en: By default, the occurrence of NANs in JAX program will not lead to an error.
    `jnp.divide(0.0,0.0)# DeviceArray(nan, dtype=float32, weak_type=True)`
  id: totrans-169
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨ JAX ç¨‹åºä¸­ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œå‡ºç° NANs ä¸ä¼šå¯¼è‡´é”™è¯¯ã€‚`jnp.divide(0.0,0.0)# DeviceArray(nan, dtype=float32,
    weak_type=True)`
- en: You can turn on the NAN checker and your program will error out at the occurrence
    of NANs. You should only use the NAN checker for debugging because it leads to
    performance issues. Also, it doesn't work with `[pmap]`, use `[vmap]` instead.
  id: totrans-170
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥æ‰“å¼€ NAN æ£€æŸ¥å™¨ï¼Œç¨‹åºå°†åœ¨å‡ºç° NAN æ—¶æŠ¥é”™ã€‚æ‚¨åº”ä»…åœ¨è°ƒè¯•æ—¶ä½¿ç”¨ NAN æ£€æŸ¥å™¨ï¼Œå› ä¸ºå®ƒä¼šå¯¼è‡´æ€§èƒ½é—®é¢˜ã€‚æ­¤å¤–ï¼Œå®ƒä¸é€‚ç”¨äº `[pmap]`ï¼Œè¯·æ”¹ç”¨
    `[vmap]`ã€‚
- en: '`from jax.config import config`'
  id: totrans-171
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from jax.config import config`'
- en: '`config.update("jax_debug_nans", True)`'
  id: totrans-172
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`config.update("jax_debug_nans", True)`'
- en: '`jnp.divide(0.0,0.0)`'
  id: totrans-173
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jnp.divide(0.0,0.0)`'
- en: '`FloatingPointError: invalid value (nan) encountered in div`'
  id: totrans-174
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`FloatingPointError: åœ¨é™¤æ³•ä¸­é‡åˆ°æ— æ•ˆå€¼ (nan)`'
- en: '**Double (64bit) precision**'
  id: totrans-175
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**åŒç²¾åº¦ï¼ˆ64 ä½ï¼‰**'
- en: JAX enforces single-precision of numbers. For example, you will get a warning
    when you create a `[float64]` number. If you check the type of the number, you
    will notice that it's `[float32]`.
  id: totrans-176
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAX å¼ºåˆ¶ä½¿ç”¨å•ç²¾åº¦æ•°ã€‚ä¾‹å¦‚ï¼Œåˆ›å»º `[float64]` æ•°å­—æ—¶ä¼šæ”¶åˆ°è­¦å‘Šã€‚å¦‚æœæ£€æŸ¥æ•°å­—ç±»å‹ï¼Œä¼šå‘ç°å®ƒæ˜¯ `[float32]`ã€‚
- en: '`x = jnp.float64(1.25844)`'
  id: totrans-177
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = jnp.float64(1.25844)`'
- en: '`/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_num py.py:1806`:
    UserWarning: Explicitly requested dtype `float64` requested in array is not available,
    and will be truncated to dtype `float32`. To enable more dtypes, set the `jax_enable_x64`
    configuration option or the `JAX_ENABLE_X64` shell environment variable. See [https://github.com/google/jax#current-gotchas](https://github.com/google/jax#current-gotchas)
    for more. `# lax_internal._check_user_dtype_supported(dtype, "array")` `# DeviceArray(1.25844,
    dtype=float32)`'
  id: totrans-178
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_num py.py:1806`:
    UserWarning: ä½ åœ¨æ•°ç»„ä¸­æ˜¾å¼è¯·æ±‚çš„ `float64` ç±»å‹ä¸å¯ç”¨ï¼Œå°†è¢«æˆªæ–­ä¸º `float32` ç±»å‹ã€‚è¦å¯ç”¨æ›´å¤šæ•°æ®ç±»å‹ï¼Œè¯·è®¾ç½® `jax_enable_x64`
    é…ç½®é€‰é¡¹æˆ– `JAX_ENABLE_X64` shell ç¯å¢ƒå˜é‡ã€‚è¯¦æƒ…è¯·è§ [https://github.com/google/jax#current-gotchas](https://github.com/google/jax#current-gotchas)ã€‚
    `# lax_internal._check_user_dtype_supported(dtype, "array")` `# DeviceArray(1.25844,
    dtype=float32)`'
- en: You can use double-precision numbers by setting that in the configuration using
    `[jax_enable_x64]`.
  id: totrans-179
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥é€šè¿‡åœ¨é…ç½®ä¸­è®¾ç½® `[jax_enable_x64]` æ¥ä½¿ç”¨åŒç²¾åº¦æ•°å­—ã€‚
- en: '`set this config at the begining of the program from jax.config import config`'
  id: totrans-180
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`åœ¨ç¨‹åºå¼€å¤´è®¾ç½®æ­¤é…ç½® from jax.config import config`'
- en: '`config.update("jax_enable_x64", True)`'
  id: totrans-181
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`config.update("jax_enable_x64", True)`'
- en: '`x = jnp.float64(1.25844)`'
  id: totrans-182
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = jnp.float64(1.25844)`'
- en: '`x`'
  id: totrans-183
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x`'
- en: '`DeviceArray(1.25844, dtype=float64)`'
  id: totrans-184
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`DeviceArray(1.25844, dtype=float64)`'
- en: '**What is a pytree?**'
  id: totrans-185
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**ä»€ä¹ˆæ˜¯ pytreeï¼Ÿ**'
- en: A pytree is a container that holds Python objects. In JAX, it can hold arrays,
    tuples, lists, dictionaries, etc. A Pytree contains leaves. For example, model
    parameters in JAX are pytrees.
  id: totrans-186
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: Pytree æ˜¯ä¸€ä¸ªå®¹å™¨ï¼Œå¯ä»¥å®¹çº³ Python å¯¹è±¡ã€‚åœ¨ JAX ä¸­ï¼Œå®ƒå¯ä»¥åŒ…å«æ•°ç»„ã€å…ƒç»„ã€åˆ—è¡¨ã€å­—å…¸ç­‰ã€‚Pytree åŒ…å«å¶å­èŠ‚ç‚¹ã€‚ä¾‹å¦‚ï¼Œåœ¨ JAX
    ä¸­ï¼Œæ¨¡å‹å‚æ•°å°±æ˜¯ pytreeã€‚
- en: '`example_trees = [`'
  id: totrans-187
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`example_trees = [`'
- en: '`[1, ''a'', object()],`'
  id: totrans-188
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[1, ''a'', object()],`'
- en: '`(1, (2, 3), ())`'
  id: totrans-189
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`(1, (2, 3), ())`'
- en: '`[1, {''k1'': 2, ''k2'': (3, 4)}, 5], {''a'': 2, ''b'': (2, 3)},`'
  id: totrans-190
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[1, {''k1'': 2, ''k2'': (3, 4)}, 5], {''a'': 2, ''b'': (2, 3)},`'
- en: '`jnp.array([1, 2, 3]),`'
  id: totrans-191
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jnp.array([1, 2, 3]),`'
- en: '`]`'
  id: totrans-192
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`]`'
- en: 'Let''s see how many leaves they have:'
  id: totrans-193
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹å®ƒä»¬å„è‡ªæœ‰å¤šå°‘ä¸ªå¶å­èŠ‚ç‚¹ï¼š
- en: '`for pytree in example_trees:`'
  id: totrans-194
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`for pytree in example_trees:`'
- en: '`leaves = jax.tree_leaves(pytree)`'
  id: totrans-195
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`leaves = jax.tree_leaves(pytree)`'
- en: '`print(f"{repr(pytree):<45} has {len(leaves)} leaves: {leave s}")`'
  id: totrans-196
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`print(f"{repr(pytree):<45} æœ‰ {len(leaves)} ä¸ªå¶å­èŠ‚ç‚¹: {leave s}")`'
- en: '`[1, ''a'', <object object at 0x7f280a01f6d0>] [1, ''a'', <object object at
    0x7f280a01f6d0>]`'
  id: totrans-197
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`[1, ''a'', <object object at 0x7f280a01f6d0>] [1, ''a'', <object object at
    0x7f280a01f6d0>]`'
- en: '`(1, (2, 3), ())`'
  id: totrans-198
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`(1, (2, 3), ())`'
- en: '`[1, 2, 3]`'
  id: totrans-199
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[1, 2, 3]`'
- en: '`[1, {''k1'': 2, ''k2'': (3, 4)}, 5]`'
  id: totrans-200
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`[1, {''k1'': 2, ''k2'': (3, 4)}, 5]`'
- en: '`[1, 2, 3, 4, 5]`'
  id: totrans-201
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[1, 2, 3, 4, 5]`'
- en: '`{''a'': 2, ''b'': (2, 3)}`'
  id: totrans-202
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`{''a'': 2, ''b'': (2, 3)}`'
- en: '`[2, 2, 3]`'
  id: totrans-203
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[2, 2, 3]`'
- en: '`DeviceArray([1, 2, 3], dtype=int64) [DeviceArray([1, 2, 3], dtype=int64)]
    has 3 leaves:`'
  id: totrans-204
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`DeviceArray([1, 2, 3], dtype=int64) [DeviceArray([1, 2, 3], dtype=int64)]
    æœ‰ 3 ä¸ªå¶å­èŠ‚ç‚¹:`'
- en: 'has 3 leaves:has 5 leaves:has 3 leaves: has 1 leaves:'
  id: totrans-205
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'æœ‰ 3 ä¸ªå¶å­èŠ‚ç‚¹: æœ‰ 5 ä¸ªå¶å­èŠ‚ç‚¹: æœ‰ 3 ä¸ªå¶å­èŠ‚ç‚¹: æœ‰ 1 ä¸ªå¶å­èŠ‚ç‚¹:'
- en: '**Handling state in JAX**'
  id: totrans-206
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**åœ¨ JAX ä¸­å¤„ç†çŠ¶æ€**'
- en: Training machine learning models will often involve state in areas such as model
    parameters, optimizer state, and stateful Layer such as BatchNorm. However, jit-compiled
    functions must have no side effects. We, therefore, need a way to track and update
    model parameters, optimizer state, and stateful layers. The solution is to define
    the state explicitly.
  id: totrans-207
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹é€šå¸¸æ¶‰åŠçŠ¶æ€ï¼Œä¾‹å¦‚æ¨¡å‹å‚æ•°ã€ä¼˜åŒ–å™¨çŠ¶æ€å’Œç±»ä¼¼ BatchNorm çš„æœ‰çŠ¶æ€å±‚ã€‚ç„¶è€Œï¼Œjit ç¼–è¯‘çš„å‡½æ•°å¿…é¡»æ²¡æœ‰å‰¯ä½œç”¨ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ç§æ–¹æ³•æ¥è·Ÿè¸ªå’Œæ›´æ–°æ¨¡å‹å‚æ•°ã€ä¼˜åŒ–å™¨çŠ¶æ€å’Œæœ‰çŠ¶æ€å±‚ã€‚è§£å†³æ–¹æ¡ˆæ˜¯æ˜¾å¼å®šä¹‰çŠ¶æ€ã€‚
- en: '**Loading datasets with JAX**'
  id: totrans-208
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**ä½¿ç”¨ JAX åŠ è½½æ•°æ®é›†**'
- en: JAX doesn't ship with any data loading tools. However, JAX recommends using
    data loaders from `PyTorch` and `TensorFlow`.
  id: totrans-209
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAX å¹¶ä¸é™„å¸¦ä»»ä½•æ•°æ®åŠ è½½å·¥å…·ã€‚ä¸è¿‡ï¼ŒJAX å»ºè®®ä½¿ç”¨æ¥è‡ª `PyTorch` å’Œ `TensorFlow` çš„æ•°æ®åŠ è½½å™¨ã€‚
- en: import `tensorflow` as `tf`
  id: totrans-210
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: import `tensorflow` as `tf`
- en: Ensure TF does not see GPU and grab all GPU memory. `tf.config.set_visible_devices([],
    device_type='GPU')`
  id: totrans-211
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: ç¡®ä¿ TF çœ‹ä¸åˆ° GPU å¹¶æŠ¢å æ‰€æœ‰ GPU å†…å­˜ã€‚`tf.config.set_visible_devices([], device_type='GPU')`
- en: import `tensorflow_datasets` as `tfdsdata_dir = '/tmp/tfds'`
  id: totrans-212
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: import `tensorflow_datasets` as `tfdsdata_dir = '/tmp/tfds'`
- en: Fetch full datasets for evaluation
  id: totrans-213
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: è·å–å®Œæ•´æ•°æ®é›†ä»¥è¿›è¡Œè¯„ä¼°
- en: '`tfds.load returns tf.Tensors (or tf.data.Datasets if batch_size != -1)`'
  id: totrans-214
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`tfds.load è¿”å› tf.Tensorsï¼ˆå¦‚æœ batch_size != -1 åˆ™ä¸º tf.data.Datasetsï¼‰`'
- en: You can convert them to NumPy arrays (or iterables of NumPy arrays) with `tfds.dataset_as_numpy`
  id: totrans-215
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ä½¿ç”¨ `tfds.dataset_as_numpy` å°†å…¶è½¬æ¢ä¸º NumPy æ•°ç»„ï¼ˆæˆ– NumPy æ•°ç»„çš„å¯è¿­ä»£å¯¹è±¡ï¼‰ã€‚
- en: '`mnist_data, info = tfds.load(name="mnist", batch_size=-1, data_dir=data_dir,
    with_info=True)`'
  id: totrans-216
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`mnist_data, info = tfds.load(name="mnist", batch_size=-1, data_dir=data_dir,
    with_info=True)`'
- en: '`mnist_data = tfds.as_numpy(mnist_data)`'
  id: totrans-217
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`mnist_data = tfds.as_numpy(mnist_data)`'
- en: '`train_data`, `test_data` = `mnist_data[''train'']`, `mnist_data[''test'']`
    `num_labels = info.features[''label''].num_classes`'
  id: totrans-218
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_data`, `test_data` = `mnist_data[''train'']`, `mnist_data[''test'']`
    `num_labels = info.features[''label''].num_classes`'
- en: '`h, w, c = info.features[''image''].shape`'
  id: totrans-219
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`h, w, c = info.features[''image''].shape`'
- en: '`num_pixels = h * w * c`'
  id: totrans-220
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`num_pixels = h * w * c`'
- en: Full train set
  id: totrans-221
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: å®Œæ•´çš„è®­ç»ƒé›†
- en: '`train_images`, `train_labels` = `train_data[''image'']`, `train_data[''l abel'']`'
  id: totrans-222
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_images`, `train_labels` = `train_data[''image'']`, `train_data[''l abel'']`'
- en: '`train_images = jnp.reshape(train_images, (len(train_images), nu m_pixels))`'
  id: totrans-223
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_images = jnp.reshape(train_images, (len(train_images), nu m_pixels))`'
- en: '`train_labels = one_hot(train_labels, num_labels)`'
  id: totrans-224
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_labels = one_hot(train_labels, num_labels)`'
- en: Full test set
  id: totrans-225
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: å®Œæ•´çš„æµ‹è¯•é›†
- en: '`test_images`, `test_labels` = `test_data[''image'']`, `test_data[''labe l'']`'
  id: totrans-226
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`test_images`, `test_labels` = `test_data[''image'']`, `test_data[''labe l'']`'
- en: '`test_images = jnp.reshape(test_images, (len(test_images), num_p ixels))`'
  id: totrans-227
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`test_images = jnp.reshape(test_images, (len(test_images), num_p ixels))`'
- en: '`test_labels = one_hot(test_labels, num_labels)`'
  id: totrans-228
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`test_labels = one_hot(test_labels, num_labels)`'
- en: '`print(''Train:'', train_images.shape, train_labels.shape)` `print(''Test:'',
    test_images.shape, test_labels.shape)`'
  id: totrans-229
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`print(''Train:'', train_images.shape, train_labels.shape)` `print(''Test:'',
    test_images.shape, test_labels.shape)`'
- en: 'Train: `(60000, 784)` `(60000, 10)` # Test: `(10000, 784)` `(10000, 10)`'
  id: totrans-230
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: 'Train: `(60000, 784)` `(60000, 10)` # Test: `(10000, 784)` `(10000, 10)`'
- en: Building neural networks with JAX
  id: totrans-231
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ JAX æ„å»ºç¥ç»ç½‘ç»œ
- en: You can build a model from scratch using JAX. However, various neural network
    libraries are built on top of JAX to make building neural networks with JAX easier.
    The Image classification with JAX & Flax article shows how to load data with PyTorch
    and build a convolutional neural network with Jax and Flax.
  id: totrans-232
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ä½¿ç”¨ JAX ä»å¤´å¼€å§‹æ„å»ºæ¨¡å‹ã€‚ç„¶è€Œï¼Œå„ç§ç¥ç»ç½‘ç»œåº“éƒ½æ˜¯åŸºäº JAX æ„å»ºçš„ï¼Œä»¥ä½¿ä½¿ç”¨ JAX æ„å»ºç¥ç»ç½‘ç»œå˜å¾—æ›´åŠ å®¹æ˜“ã€‚ã€ŠJAX å’Œ Flax
    å›¾åƒåˆ†ç±»ã€‹æ–‡ç« å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ PyTorch åŠ è½½æ•°æ®ï¼Œå¹¶ä½¿ç”¨ JAX å’Œ Flax æ„å»ºå·ç§¯ç¥ç»ç½‘ç»œã€‚
- en: Final thoughts
  id: totrans-233
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: æ€»ç»“æ€è€ƒ
- en: 'In this article, we have covered the basics of JAX. We have seen that JAX uses
    XLA and just-in-time compilation to improve the performance of Python functions.
    Specifically, we have covered:'
  id: totrans-234
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº† JAX çš„åŸºç¡€çŸ¥è¯†ã€‚æˆ‘ä»¬çœ‹åˆ° JAX ä½¿ç”¨ XLA å’Œå³æ—¶ç¼–è¯‘æ¥æé«˜ Python å‡½æ•°çš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä»‹ç»äº†ï¼š
- en: Setting up JAX to use TPUs on Google Colab.
  id: totrans-235
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨ Google Colab ä¸Šé…ç½® JAX ä»¥ä½¿ç”¨ TPUsã€‚
- en: Comparison between data types in JAX and NumPy. Creating arrays in JAX.
  id: totrans-236
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAX å’Œ NumPy ä¸­æ•°æ®ç±»å‹çš„æ¯”è¾ƒã€‚åœ¨ JAX ä¸­åˆ›å»ºæ•°ç»„ã€‚
- en: How to generate random numbers in JAX.
  id: totrans-237
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å¦‚ä½•åœ¨ JAX ä¸­ç”Ÿæˆéšæœºæ•°ã€‚
- en: Operations on JAX arrays.
  id: totrans-238
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAX æ•°ç»„ä¸Šçš„æ“ä½œã€‚
- en: Gotchas in JAX, such as using pure functions and the immutability of JAX arrays.
  id: totrans-239
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAX ä¸­çš„æ³¨æ„äº‹é¡¹ï¼Œå¦‚ä½¿ç”¨çº¯å‡½æ•°å’Œ JAX æ•°ç»„çš„ä¸å¯å˜æ€§ã€‚
- en: Placing JAX arrays in GPUs or TPUs.
  id: totrans-240
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å°† JAX æ•°ç»„æ”¾å…¥ GPU æˆ– TPUã€‚
- en: How to use JIT to speed up functions.
  id: totrans-241
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å¦‚ä½•ä½¿ç”¨ JIT åŠ é€Ÿå‡½æ•°ã€‚
- en: '...and so much more'
  id: totrans-242
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '... ä»¥åŠæ›´å¤šå†…å®¹'
- en: Optimizers in JAX and Flax
  id: totrans-243
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: JAX å’Œ Flax ä¸­çš„ä¼˜åŒ–å™¨
- en: Optimizers are applied when training neural networks to reduce the error between
    the true and predicted values. This optimization is done via gradient descent.
    Gradient descent adjusts errors in the network through a cost function. In JAX,
    optimizers are applied from the Optax library.
  id: totrans-244
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒç¥ç»ç½‘ç»œæ—¶åº”ç”¨ä¼˜åŒ–å™¨ä»¥å‡å°‘çœŸå®å€¼å’Œé¢„æµ‹å€¼ä¹‹é—´çš„è¯¯å·®ã€‚è¿™ç§ä¼˜åŒ–é€šè¿‡æ¢¯åº¦ä¸‹é™å®Œæˆã€‚æ¢¯åº¦ä¸‹é™é€šè¿‡æˆæœ¬å‡½æ•°è°ƒæ•´ç½‘ç»œä¸­çš„è¯¯å·®ã€‚åœ¨ JAX ä¸­ï¼Œä¼˜åŒ–å™¨æ¥è‡ª Optax
    åº“ã€‚
- en: 'Optimizers can be classified into two broad categories:'
  id: totrans-245
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–å™¨å¯ä»¥åˆ†ä¸ºä¸¤å¤§ç±»ï¼š
- en: Adaptive such as Adam, Adagrad, AdaDelta, and RMSProp.
  id: totrans-246
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åŒ…æ‹¬ Adamã€Adagradã€AdaDelta å’Œ RMSProp åœ¨å†…çš„è‡ªé€‚åº”ä¼˜åŒ–å™¨ã€‚
- en: Accelerated stochastic gradient descent (SGD), for example, SGD with momentum,
    heavy-ball method (HB), and Nesterov accelerated gradient (NAG).
  id: totrans-247
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åŠ é€Ÿéšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰ï¼Œä¾‹å¦‚å¸¦åŠ¨é‡çš„ SGDã€é‡çƒæ–¹æ³•ï¼ˆHBï¼‰å’Œ Nesterov åŠ é€Ÿæ¢¯åº¦ï¼ˆNAGï¼‰ã€‚
- en: Let's look at common optimizer functions used in JAX and Flax.
  id: totrans-248
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¥çœ‹çœ‹åœ¨ JAX å’Œ Flax ä¸­å¸¸ç”¨çš„ä¼˜åŒ–å™¨å‡½æ•°ã€‚
- en: Adaptive vs stochastic gradient descent (SGD) optimizers
  id: totrans-249
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: è‡ªé€‚åº” vs éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰ä¼˜åŒ–å™¨
- en: When performing optimization, adaptive optimizers start with large update steps
    but reduce the step size as they get close to the global minimum. This ensures
    that they don't miss the global minimum.
  id: totrans-250
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨æ‰§è¡Œä¼˜åŒ–æ—¶ï¼Œè‡ªé€‚åº”ä¼˜åŒ–å™¨ä»¥è¾ƒå¤§çš„æ›´æ–°æ­¥éª¤å¼€å§‹ï¼Œä½†åœ¨æ¥è¿‘å…¨å±€æœ€å°å€¼æ—¶å‡å°æ­¥é•¿ã€‚è¿™ç¡®ä¿å®ƒä»¬ä¸ä¼šé”™è¿‡å…¨å±€æœ€å°å€¼ã€‚
- en: 'Adaptive optimizers such as Adam are quite common because they converge faster,
    but they may have poor generalization. `SGD-based optimizers apply a global learning
    rate on all parameters, while adaptive optimizers calculate a learning rate for
    each parameter.` `## AdaBelief` The authors of AdaBelief introduced the optimizer
    to:'
  id: totrans-251
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è‡ªé€‚åº”ä¼˜åŒ–å™¨å¦‚Adaméå¸¸å¸¸è§ï¼Œå› ä¸ºå®ƒä»¬æ”¶æ•›é€Ÿåº¦å¿«ï¼Œä½†å¯èƒ½æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚`SGD-based optimizers apply a global learning
    rate on all parameters, while adaptive optimizers calculate a learning rate for
    each parameter.` `## AdaBelief` AdaBeliefçš„ä½œè€…å¼•å…¥äº†è¿™ä¸ªä¼˜åŒ–å™¨æ¥ï¼š
- en: Converge fast as in adaptive methods.
  id: totrans-252
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä¸è‡ªé€‚åº”æ–¹æ³•ä¸€æ ·å¿«é€Ÿæ”¶æ•›ã€‚
- en: Have good generalization like SGD.
  id: totrans-253
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å…·æœ‰åƒSGDä¸€æ ·çš„è‰¯å¥½æ³›åŒ–èƒ½åŠ›ã€‚
- en: Be stable during training.
  id: totrans-254
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒæœŸé—´ä¿æŒç¨³å®šã€‚
- en: '`AdaBelief`Â works on the concept of " **belief**" in the current gradient direction.
    If it results in good performance, then that direction is trusted, and large updates
    are applied. Otherwise, it''s distrusted and the step size is reduced.'
  id: totrans-255
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`AdaBelief`Â åŸºäºå¯¹å½“å‰æ¢¯åº¦æ–¹å‘çš„â€œ**ä¿¡å¿µ**â€å·¥ä½œã€‚å¦‚æœå®ƒå¯¼è‡´è‰¯å¥½çš„æ€§èƒ½ï¼Œåˆ™ä¿¡ä»»è¯¥æ–¹å‘ï¼Œå¹¶åº”ç”¨å¤§å¹…æ›´æ–°ã€‚å¦åˆ™ï¼Œå®ƒè¢«ä¸ä¿¡ä»»ï¼Œæ­¥é•¿è¢«å‡å°ã€‚'
- en: Let's look at a `Flax training state` that applies the `AdaBelief` optimizer.from
    `flax.training` import `train_state`
  id: totrans-256
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸ªåº”ç”¨`AdaBelief`ä¼˜åŒ–å™¨çš„`Flaxè®­ç»ƒçŠ¶æ€`ã€‚ä»`flax.training`å¯¼å…¥`train_state`
- en: 'def `create_train_state`(rng, learning_rate):'
  id: totrans-257
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def `create_train_state`(rng, learning_rate):'
- en: '"""Creates initial `TrainState`."""'
  id: totrans-258
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""åˆ›å»ºåˆå§‹`TrainState`ã€‚"""'
- en: cnn = `CNN()``
  id: totrans-259
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: cnn = `CNN()``
- en: params = `cnn.init`(rng, `jnp.ones([1, size_image, size_image,`
  id: totrans-260
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: params = `cnn.init`(rng, `jnp.ones([1, size_image, size_image,`
- en: '`3]))[''params'']`'
  id: totrans-261
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`3]))[''params'']`'
- en: tx = `optax.adabelief`(learning_rate)
  id: totrans-262
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: tx = `optax.adabelief`(learning_rate)
- en: return `train_state.TrainState.create`(
  id: totrans-263
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¿”å›`train_state.TrainState.create`(
- en: apply_fn=`cnn.apply`, params=params, tx=tx)Here's the performance of `AdaBelief`
    on various tasks as provided by itsÂ authors.![](../images/00007.jpeg)`
  id: totrans-264
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: apply_fn=`cnn.apply`, params=params, tx=tx)è¿™æ˜¯`AdaBelief`åœ¨å„ç§ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œç”±å…¶ä½œè€…æä¾›ï¼[](../images/00007.jpeg)`
- en: '**`AdaGrad`**'
  id: totrans-265
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**`AdaGrad`**'
- en: '`AdaGrad`Â works well in situations leading to sparse gradients. `Adagrad` is
    an algorithm for gradient-based optimization that anneals the learning rate for
    each parameter during trainingâ€“Â `Optax`.'
  id: totrans-266
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`AdaGrad`Â åœ¨å¯¼è‡´ç¨€ç–æ¢¯åº¦çš„æƒ…å†µä¸‹æ•ˆæœå¾ˆå¥½ã€‚`Adagrad`æ˜¯ä¸€ç§åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–ç®—æ³•ï¼Œåœ¨è®­ç»ƒæœŸé—´ä¸ºæ¯ä¸ªå‚æ•°é€€ç«å­¦ä¹ ç‡â€“Â `Optax`.'
- en: from `flax.training` import `train_state`
  id: totrans-267
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: from `flax.training` import `train_state`
- en: 'def `create_train_state`(rng, learning_rate):'
  id: totrans-268
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def `create_train_state`(rng, learning_rate):'
- en: '"""Creates initial `TrainState`."""'
  id: totrans-269
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""åˆ›å»ºåˆå§‹`TrainState`ã€‚"""'
- en: cnn = `CNN()``
  id: totrans-270
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: cnn = `CNN()``
- en: params = `cnn.init`(rng, `jnp.ones([1, size_image, size_image,`
  id: totrans-271
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: params = `cnn.init`(rng, `jnp.ones([1, size_image, size_image,`
- en: '`3]))[''params'']`'
  id: totrans-272
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`3]))[''params'']`'
- en: tx = `optax.AdaGrad`(learning_rate)
  id: totrans-273
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: tx = `optax.AdaGrad`(learning_rate)
- en: return `train_state.TrainState.create`(
  id: totrans-274
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¿”å›`train_state.TrainState.create`(
- en: apply_fn=`cnn.apply`, params=params, tx=tx)
  id: totrans-275
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: apply_fn=`cnn.apply`, params=params, tx=tx)
- en: '**`Adam â€“ Adaptive moment estimation`**'
  id: totrans-276
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**`Adam â€“ è‡ªé€‚åº”çŸ©ä¼°è®¡`**'
- en: '`Adam`Â is a common optimizer in deep learning because it gives good results
    with default parameters, is computationally inexpensive, and uses little memory.'
  id: totrans-277
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Adam`Â æ˜¯æ·±åº¦å­¦ä¹ ä¸­å¸¸è§çš„ä¼˜åŒ–å™¨ï¼Œå› ä¸ºå®ƒä½¿ç”¨é»˜è®¤å‚æ•°èƒ½å¤Ÿè·å¾—è‰¯å¥½çš„ç»“æœï¼Œè®¡ç®—æˆæœ¬ä½ï¼Œå†…å­˜ä½¿ç”¨å°‘ã€‚'
- en: from `flax.training` import `train_state`
  id: totrans-278
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: from `flax.training` import `train_state`
- en: 'def `create_train_state`(rng):'
  id: totrans-279
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def `create_train_state`(rng):'
- en: '"""Creates initial `TrainState`."""'
  id: totrans-280
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""åˆ›å»ºåˆå§‹`TrainState`ã€‚"""'
- en: model = `LSTMModel()``
  id: totrans-281
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: model = `LSTMModel()``
- en: params = `model.init`(rng, `jnp.array(X_train_padded[0]))['param`
  id: totrans-282
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: params = `model.init`(rng, `jnp.array(X_train_padded[0]))['param`
- en: '`s'']`'
  id: totrans-283
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`s'']`'
- en: tx = `optax.adam`(0.001,0.9,0.999,1e-07)
  id: totrans-284
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: tx = `optax.adam`(0.001,0.9,0.999,1e-07)
- en: return `train_state.TrainState.create`(
  id: totrans-285
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¿”å›`train_state.TrainState.create`(
- en: apply_fn=`model.apply`, params=params, tx=tx)![](../images/00008.jpeg) **`Training
    of multilayer neural networks on MNIST images. (a) Neural networks using dropout
    stochastic regularization. (b) Neural networks with deterministic cost function`**  **##
    `AdamW`**
  id: totrans-286
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: apply_fn=`model.apply`, params=params, tx=tx)![](../images/00008.jpeg) **`åœ¨MNISTå›¾åƒä¸Šè®­ç»ƒå¤šå±‚ç¥ç»ç½‘ç»œã€‚ï¼ˆaï¼‰ä½¿ç”¨dropoutéšæœºæ­£åˆ™åŒ–çš„ç¥ç»ç½‘ç»œã€‚ï¼ˆbï¼‰å…·æœ‰ç¡®å®šæ€§æˆæœ¬å‡½æ•°çš„ç¥ç»ç½‘ç»œ`**  **##
    `AdamW`**
- en: '`AdamW`Â is `Adam` with weight decay regularization. Weight decay regularization
    penalizes the cost function making the weights smaller during backpropagarion.
    It results in small weights that lead to better generalization. In some cases,
    `Adam` with decoupled weight decay leads to better results compared `Adam` with
    `L2 regularization`.'
  id: totrans-287
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`AdamWæ˜¯å¸¦æœ‰æƒé‡è¡°å‡æ­£åˆ™åŒ–çš„Adamã€‚æƒé‡è¡°å‡æ­£åˆ™åŒ–é€šè¿‡æƒ©ç½šæˆæœ¬å‡½æ•°åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ä½¿æƒé‡å˜å°ã€‚è¿™å¯¼è‡´æƒé‡å˜å°ï¼Œä»è€Œå¯¼è‡´æ›´å¥½çš„æ³›åŒ–ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä¸L2æ­£åˆ™åŒ–çš„Adamç›¸æ¯”ï¼Œä½¿ç”¨è§£è€¦æƒé‡è¡°å‡çš„Adamä¼šå¯¼è‡´æ›´å¥½çš„ç»“æœã€‚`'
- en: from `flax.training` import `train_state`
  id: totrans-288
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: from `flax.training` import `train_state`
- en: 'def `create_train_state`(rng):'
  id: totrans-289
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def `create_train_state`(rng):`'
- en: '"""Creates initial `TrainState`."""'
  id: totrans-290
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""åˆ›å»ºåˆå§‹`TrainState`ã€‚"""'
- en: model = `LSTMModel()``
  id: totrans-291
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: model = `LSTMModel()``
- en: params = `model.init`(rng, `jnp.array(X_train_padded[0]))['param`
  id: totrans-292
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: params = `model.init`(rng, `jnp.array(X_train_padded[0]))['param`
- en: '`s'']tx = optax.adamw(0.001,0.9,0.999,1e-07)return train_state.TrainState.create(apply_fn=model.apply,
    params=params, tx=tx)![](../images/00009.jpeg) **`Learning curves (top row) and
    generalization results (bottom row) obtained by a 26 2x96d ResNet trained with
    Adam and AdamW on CIFAR-10`**  **## `RAdam â€“ Rectified Adam optimizer`** `RAdam`Â aims
    to solve large variances during the early training stages when applying an adaptive
    learning rate.from `flax.training` import `train_state`'
  id: totrans-293
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`s'']tx = optax.adamw(0.001,0.9,0.999,1e-07)return train_state.TrainState.create(apply_fn=model.apply,
    params=params, tx=tx)![](../images/00009.jpeg) **`å­¦ä¹ æ›²çº¿ï¼ˆé¡¶éƒ¨è¡Œï¼‰å’Œåœ¨CIFAR-10ä¸Šä½¿ç”¨Adamå’ŒAdamWè®­ç»ƒçš„26
    2x96d ResNetçš„æ³›åŒ–ç»“æœï¼ˆåº•éƒ¨è¡Œï¼‰`**  **## `RAdam â€“ çŸ«æ­£çš„Adamä¼˜åŒ–å™¨`** `RAdamæ—¨åœ¨è§£å†³åº”ç”¨è‡ªé€‚åº”å­¦ä¹ ç‡æ—¶åœ¨æ—©æœŸè®­ç»ƒé˜¶æ®µå‡ºç°çš„å¤§æ–¹å·®ã€‚from
    `flax.training` import `train_state`'
- en: '`def create_train_state(rng, learning_rate):`'
  id: totrans-294
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def create_train_state(rng, learning_rate):`'
- en: '"""Creates initial `TrainState`."""'
  id: totrans-295
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""åˆ›å»ºåˆå§‹`TrainState`ã€‚"""'
- en: '`cnn = CNN()`'
  id: totrans-296
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`cnn = CNN()`'
- en: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`'
  id: totrans-297
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`'
- en: '`3]))[''params'']`'
  id: totrans-298
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`3]))[''params'']`'
- en: '`tx = optax.radam(learning_rate)`'
  id: totrans-299
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`tx = optax.radam(learning_rate)`'
- en: '`return train_state.TrainState.create(`'
  id: totrans-300
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return train_state.TrainState.create(`'
- en: '`apply_fn=cnn.apply, params=params, tx=tx)`'
  id: totrans-301
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`apply_fn=cnn.apply, params=params, tx=tx)`'
- en: '**AdaFactor**'
  id: totrans-302
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**AdaFactor**'
- en: '`AdaFactorÂ is used for training large neural networks because it is implemented
    to reduce memory utilization.from flax.training import train_state`'
  id: totrans-303
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`AdaFactorç”¨äºè®­ç»ƒå¤§å‹ç¥ç»ç½‘ç»œï¼Œå› ä¸ºå®ƒå®ç°äº†å‡å°‘å†…å­˜ä½¿ç”¨ã€‚from flax.training import train_state`'
- en: '`def create_train_state(rng, learning_rate):`'
  id: totrans-304
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def create_train_state(rng, learning_rate):`'
- en: '"""Creates initial `TrainState`."""'
  id: totrans-305
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""åˆ›å»ºåˆå§‹`TrainState`ã€‚"""'
- en: '`cnn = CNN()`'
  id: totrans-306
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`cnn = CNN()`'
- en: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`'
  id: totrans-307
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`'
- en: '`3]))[''params'']`'
  id: totrans-308
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`3]))[''params'']`'
- en: '`tx = optax.adafactor(learning_rate)`'
  id: totrans-309
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`tx = optax.adafactor(learning_rate)`'
- en: '`return train_state.TrainState.create(`'
  id: totrans-310
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return train_state.TrainState.create(`'
- en: '`apply_fn=cnn.apply, params=params, tx=tx)`'
  id: totrans-311
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`apply_fn=cnn.apply, params=params, tx=tx)`'
- en: '**Fromage**'
  id: totrans-312
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**Fromage**'
- en: '`FromageÂ introduces a distance function on deep neural networks called*Â **deep
    relative trust**.Â *It requires little to no learning rate tuning.from flax.training
    import train_state`'
  id: totrans-313
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Fromageå¼•å…¥äº†ä¸€ç§ç§°ä¸ºâ€œæ·±åº¦ç›¸å¯¹ä¿¡ä»»â€çš„æ·±åº¦ç¥ç»ç½‘ç»œä¸Šçš„è·ç¦»å‡½æ•°ã€‚å®ƒå‡ ä¹ä¸éœ€è¦å­¦ä¹ ç‡è°ƒæ•´ã€‚from flax.training import
    train_state`'
- en: '`def create_train_state(rng, learning_rate): """Creates initial `TrainState`."""
    cnn = CNN()`'
  id: totrans-314
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def create_train_state(rng, learning_rate): """åˆ›å»ºåˆå§‹`TrainState`ã€‚""" cnn =
    CNN()`'
- en: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`'
  id: totrans-315
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`'
- en: '`3]))[''params'']`'
  id: totrans-316
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`3]))[''params'']`'
- en: '`tx = optax.fromage(learning_rate)`'
  id: totrans-317
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`tx = optax.fromage(learning_rate)`'
- en: '`return train_state.TrainState.create(`'
  id: totrans-318
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return train_state.TrainState.create(`'
- en: '`apply_fn=cnn.apply, params=params, tx=tx)`'
  id: totrans-319
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`apply_fn=cnn.apply, params=params, tx=tx)`'
- en: '**Lamb â€“ Layerwise adaptive large batch optimization**'
  id: totrans-320
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**Lamb â€“ å±‚è‡ªé€‚åº”å¤§æ‰¹é‡ä¼˜åŒ–**'
- en: '`LambÂ aims to enable the training of deep neural networks by computing gradients
    using large mini-batches. It leads to good performance on attention-based models
    such as Transformers and ResNet-50.`'
  id: totrans-321
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Lambæ—¨åœ¨é€šè¿‡ä½¿ç”¨å¤§å‹å°æ‰¹é‡è®¡ç®—æ¢¯åº¦æ¥è®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œã€‚å®ƒåœ¨åŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹ï¼ˆå¦‚Transformerså’ŒResNet-50ï¼‰ä¸Šè¡¨ç°è‰¯å¥½ã€‚`'
- en: '`from flax.training import train_state`'
  id: totrans-322
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from flax.training import train_state`'
- en: '`def create_train_state(rng, learning_rate):`'
  id: totrans-323
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def create_train_state(rng, learning_rate):`'
- en: '"""Creates initial `TrainState`."""'
  id: totrans-324
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""åˆ›å»ºåˆå§‹`TrainState`ã€‚"""'
- en: '`cnn = CNN()`'
  id: totrans-325
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`cnn = CNN()`'
- en: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`'
  id: totrans-326
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`'
- en: '`3]))[''params'']`'
  id: totrans-327
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`3]))[''params'']`'
- en: '`tx = optax.lamb(learning_rate)`'
  id: totrans-328
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`tx = optax.lamb(learning_rate)`'
- en: '`return train_state.TrainState.create(`'
  id: totrans-329
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return train_state.TrainState.create(`'
- en: '`apply_fn=cnn.apply, params=params, tx=tx)`'
  id: totrans-330
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`apply_fn=cnn.apply, params=params, tx=tx)`'
- en: '**Lars â€“ Layer-wise Adaptive Rate Scaling**'
  id: totrans-331
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**Lars â€“ å±‚è‡ªé€‚åº”é€Ÿç‡ç¼©æ”¾**'
- en: '`LarsÂ is inspired by Lamb to scale SGD to large batch sizes. Lars has been
    used to train AlexNet with an 8K batch size and Resnet-50 with a 32K batch size
    without degrading the accuracy. from flax.training import train_state`'
  id: totrans-332
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`LarsÂ is inspired by Lamb to scale SGD to large batch sizes. Lars has been
    used to train AlexNet with an 8K batch size and Resnet-50 with a 32K batch size
    without degrading the accuracy. from flax.training import train_state`'
- en: '`def create_train_state(rng, learning_rate):`'
  id: totrans-333
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def create_train_state(rng, learning_rate):`'
- en: '"""Creates initial `TrainState`."""'
  id: totrans-334
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""Creates initial `TrainState`."""'
- en: '`cnn = CNN()`'
  id: totrans-335
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`cnn = CNN()`'
- en: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`'
  id: totrans-336
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`'
- en: '`3]))[''params'']`'
  id: totrans-337
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`3]))[''params'']`'
- en: '`tx = optax.lars(learning_rate)`'
  id: totrans-338
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`tx = optax.lars(learning_rate)`'
- en: '`return train_state.TrainState.create(`'
  id: totrans-339
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return train_state.TrainState.create(`'
- en: '`apply_fn=cnn.apply, params=params, tx=tx)![](../images/00010.jpeg)`**LARS:
    Alexnet-BN with B=8K ** **## SM3 - Square-root of Minima of Sums of Maxima of
    Squaredgradients Method**'
  id: totrans-340
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`apply_fn=cnn.apply, params=params, tx=tx)![](../images/00010.jpeg)`**LARS:
    Alexnet-BN with B=8K ** **## SM3 - Square-root of Minima of Sums of Maxima of
    Squaredgradients Method**'
- en: '`SM3Â was designed to reduce memory utilization when training very large models
    such as Transformer for machine translation, BERT for language modeling, and AmoebaNet-D
    for image classification`'
  id: totrans-341
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`SM3Â was designed to reduce memory utilization when training very large models
    such as Transformer for machine translation, BERT for language modeling, and AmoebaNet-D
    for image classification`'
- en: '`from flax.training import train_state`'
  id: totrans-342
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from flax.training import train_state`'
- en: '`def create_train_state(rng, learning_rate):`'
  id: totrans-343
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def create_train_state(rng, learning_rate):`'
- en: '"""Creates initial `TrainState`."""'
  id: totrans-344
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""Creates initial `TrainState`."""'
- en: '`cnn = CNN()`'
  id: totrans-345
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`cnn = CNN()`'
- en: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`'
  id: totrans-346
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`'
- en: '`3]))[''params'']`'
  id: totrans-347
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`3]))[''params'']`'
- en: '`tx = optax.sm3(learning_rate)`'
  id: totrans-348
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`tx = optax.sm3(learning_rate)`'
- en: '`return train_state.TrainState.create(`'
  id: totrans-349
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return train_state.TrainState.create(`'
- en: apply_fn=cnn.apply, params=params, tx=tx)![](../images/00011.jpeg)**Top-1 (left)
    and Top-5 (right) test accuracy of AmoebaNet-D on ImageNet ** **## SGDâ€“ Stochastic
    Gradient Descent**
  id: totrans-350
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: apply_fn=cnn.apply, params=params, tx=tx)![](../images/00011.jpeg)**Top-1 (left)
    and Top-5 (right) test accuracy of AmoebaNet-D on ImageNet ** **## SGDâ€“ Stochastic
    Gradient Descent**
- en: SDGÂ implements stochastic gradient descent with support for momentum and Nesterov
    acceleration. Momentum makes obtaining optimal model weights faster by accelerating
    gradient descent in a certain direction.
  id: totrans-351
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: SDGÂ implements stochastic gradient descent with support for momentum and Nesterov
    acceleration. Momentum makes obtaining optimal model weights faster by accelerating
    gradient descent in a certain direction.
- en: from flax.training import train_state
  id: totrans-352
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: from flax.training import train_state
- en: 'def create_train_state(rng, learning_rate):'
  id: totrans-353
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def create_train_state(rng, learning_rate):'
- en: '"""Creates initial `TrainState`."""'
  id: totrans-354
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""Creates initial `TrainState`."""'
- en: cnn = CNN()
  id: totrans-355
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: cnn = CNN()
- en: params = cnn.init(rng, jnp.ones([1, size_image, size_image,
  id: totrans-356
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: params = cnn.init(rng, jnp.ones([1, size_image, size_image,
- en: 3]))['params']
  id: totrans-357
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 3]))['params']
- en: tx = optax.sgd(learning_rate)
  id: totrans-358
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: tx = optax.sgd(learning_rate)
- en: return train_state.TrainState.create(
  id: totrans-359
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: return train_state.TrainState.create(
- en: apply_fn=cnn.apply, params=params, tx=tx)
  id: totrans-360
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: apply_fn=cnn.apply, params=params, tx=tx)
- en: '**Noisy SGD**'
  id: totrans-361
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**Noisy SGD**'
- en: Noisy SGDÂ is SGD with added noise. Adding noise to gradients can prevent overfitting
    and improve training error and generalization in deep architectures.
  id: totrans-362
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: Noisy SGDÂ is SGD with added noise. Adding noise to gradients can prevent overfitting
    and improve training error and generalization in deep architectures.
- en: from flax.training import train_state
  id: totrans-363
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: from flax.training import train_state
- en: 'def create_train_state(rng, learning_rate):'
  id: totrans-364
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def create_train_state(rng, learning_rate):'
- en: '"""Creates initial `TrainState`."""'
  id: totrans-365
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""Creates initial `TrainState`."""'
- en: cnn = CNN()
  id: totrans-366
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: cnn = CNN()
- en: params = cnn.init(rng, jnp.ones([1, size_image, size_image,
  id: totrans-367
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: params = cnn.init(rng, jnp.ones([1, size_image, size_image,
- en: 3]))['params']
  id: totrans-368
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 3]))['params']
- en: tx = optax.noisy_sgd(learning_rate)
  id: totrans-369
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: tx = optax.noisy_sgd(learning_rate)
- en: return train_state.TrainState.create(
  id: totrans-370
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: return train_state.TrainState.create(
- en: 'apply_fn=cnn.apply, params=params, tx=tx)![](../images/00012.jpeg) **: Noise
    vs. No Noise in our experiment with tables containing 5 columns. The models trained
    with noise generalizes almost always better**  **## Optimistic GD** An Optimistic
    Gradient Descent optimizer.'
  id: totrans-371
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'apply_fn=cnn.apply, params=params, tx=tx)![](../images/00012.jpeg) **: Noise
    vs. No Noise in our experiment with tables containing 5 columns. The models trained
    with noise generalizes almost always better**  **## Optimistic GD** An Optimistic
    Gradient Descent optimizer.'
- en: '*Optimistic gradient descent is an approximation of extra-gradient methods
    which require multiple gradient calls to compute the next update. It has strong
    formal guarantees for last-iterate convergence in min-max games, for which standard
    gradient descent can oscillate or even divergeâ€“Â Optax.*'
  id: totrans-372
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '*Optimistic gradient descent is an approximation of extra-gradient methods
    which require multiple gradient calls to compute the next update. It has strong
    formal guarantees for last-iterate convergence in min-max games, for which standard
    gradient descent can oscillate or even divergeâ€“Â Optax.*'
- en: from flax.training import train_state
  id: totrans-373
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: from flax.training import train_state
- en: 'def create_train_state(rng, learning_rate):'
  id: totrans-374
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def create_train_state(rng, learning_rate):'
- en: '"""Creates initial `TrainState`."""'
  id: totrans-375
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""Creates initial `TrainState`."""'
- en: cnn = CNN()
  id: totrans-376
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: cnn = CNN()
- en: params = cnn.init(rng, jnp.ones([1, size_image, size_image, tx = optax.optimistic_gradient_descent(learning_rate)
    return train_state.TrainState.create(
  id: totrans-377
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image, tx = optax.optimistic_gradient_descent(learning_rate)
    return train_state.TrainState.create(`'
- en: apply_fn=cnn.apply, params=params, tx=tx)
  id: totrans-378
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: apply_fn=cnn.apply, params=params, tx=tx)
- en: '**Differentially Private SGD**'
  id: totrans-379
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**å·®åˆ†éšç§SGD**'
- en: Differentially Private SGDÂ is used for training networks with sensitive data.
    It ensures that the models don't expose sensitive training data.
  id: totrans-380
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å·®åˆ†éšç§SGDç”¨äºè®­ç»ƒå…·æœ‰æ•æ„Ÿæ•°æ®çš„ç½‘ç»œã€‚ç¡®ä¿æ¨¡å‹ä¸ä¼šæ³„éœ²æ•æ„Ÿçš„è®­ç»ƒæ•°æ®ã€‚
- en: from flax.training import train_state
  id: totrans-381
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from flax.training import train_state`'
- en: 'def create_train_state(rng, learning_rate):'
  id: totrans-382
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def create_train_state(rng, learning_rate):`'
- en: '"""Creates initial `TrainState`."""'
  id: totrans-383
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""åˆ›å»ºåˆå§‹`TrainState`ã€‚"""'
- en: cnn = CNN()
  id: totrans-384
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: cnn = CNN()
- en: params = cnn.init(rng, jnp.ones([1, size_image, size_image,
  id: totrans-385
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`'
- en: 3]))['params']
  id: totrans-386
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 3]))['params']
- en: tx = optax.dpsgd(learning_rate)
  id: totrans-387
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`tx = optax.dpsgd(learning_rate)`'
- en: return train_state.TrainState.create(
  id: totrans-388
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¿”å›train_state.TrainState.create(
- en: apply_fn=cnn.apply, params=params, tx=tx)
  id: totrans-389
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: apply_fn=cnn.apply, params=params, tx=tx)
- en: '**RMSProp**'
  id: totrans-390
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**RMSProp**'
- en: RMSPropÂ works by dividing the gradient of a running average of its recent magnitudeâ€“Hinton.from
    flax.training import train_state
  id: totrans-391
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: RMSPropé€šè¿‡å°†å…¶æœ€è¿‘å¹…åº¦çš„è¿è¡Œå¹³å‡å€¼çš„æ¢¯åº¦åˆ†å‰²æ¥å·¥ä½œ - Hinton.æ¥è‡ªflax.training import train_state
- en: 'def create_train_state(rng, learning_rate):'
  id: totrans-392
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def create_train_state(rng, learning_rate):`'
- en: '"""Creates initial `TrainState`."""'
  id: totrans-393
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""åˆ›å»ºåˆå§‹`TrainState`ã€‚"""'
- en: cnn = CNN()
  id: totrans-394
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: cnn = CNN()
- en: params = cnn.init(rng, jnp.ones([1, size_image, size_image, tx = optax.rmsprop(learning_rate)
  id: totrans-395
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image, tx = optax.rmsprop(learning_rate)`'
- en: return train_state.TrainState.create(
  id: totrans-396
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_state.TrainState.create(`çš„è¿”å›'
- en: apply_fn=cnn.apply, params=params, tx=tx)
  id: totrans-397
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: apply_fn=cnn.apply, params=params, tx=tx)
- en: '**Yogi**'
  id: totrans-398
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**Yogi**'
- en: '`YogiÂ is a modified Adam optimizer for optimizing the stochastic nonconvex
    optimization problem.from flax.training import train_state`'
  id: totrans-399
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Yogiæ˜¯ç”¨äºä¼˜åŒ–éšæœºéå‡¸ä¼˜åŒ–é—®é¢˜çš„ä¿®æ”¹ç‰ˆAdamä¼˜åŒ–å™¨ã€‚æ¥è‡ªflax.training import train_state`'
- en: '`def create_train_state(rng, learning_rate):`'
  id: totrans-400
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def create_train_state(rng, learning_rate):`'
- en: '"""Creates initial `TrainState`."""'
  id: totrans-401
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""åˆ›å»ºåˆå§‹`TrainState`ã€‚"""'
- en: '`cnn = CNN()`'
  id: totrans-402
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`cnn = CNN()`'
- en: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`'
  id: totrans-403
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image,`'
- en: '`3]))[''params'']`'
  id: totrans-404
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`3]))[''params'']`'
- en: '`tx = optax.yogi(learning_rate)`'
  id: totrans-405
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`tx = optax.yogi(learning_rate)`'
- en: '`return train_state.TrainState.create(`'
  id: totrans-406
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return train_state.TrainState.create(`'
- en: '`apply_fn=cnn.apply, params=params, tx=tx)![](../images/00013.jpeg)` **Comparison
    of highly tuned RMSProp optimizer with YOGI for Inception-Resnet-v2 on Imagenet.
    First plot shows the mini-batch estimate of the loss during training, while the
    remaining two plots show top-1 and top-5 error rates on the held-out Imagenet
    validation set**  **## Final thoughts**'
  id: totrans-407
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`apply_fn=cnn.apply, params=params, tx=tx)![](../images/00013.jpeg)` **æ¯”è¾ƒé«˜åº¦è°ƒæ•´çš„RMSPropä¼˜åŒ–å™¨ä¸YOGIåœ¨Imagenetä¸Šçš„Inception-Resnet-v2ã€‚ç¬¬ä¸€å¼ å›¾æ˜¾ç¤ºäº†è®­ç»ƒæœŸé—´æŸå¤±çš„å°æ‰¹é‡ä¼°è®¡ï¼Œè€Œå…¶ä½™ä¸¤å¼ å›¾æ˜¾ç¤ºäº†åœ¨ä¿ç•™çš„ImagenetéªŒè¯é›†ä¸Šçš„top-1å’Œtop-5é”™è¯¯ç‡**  **##
    æœ€åçš„æ€è€ƒ**'
- en: 'Choosing the right optimizer function determines how long training a network
    will take. It also determines how well the model performs. Choosing the appropriate
    optimizer functions is therefore paramount. This article discusses various optimizer
    functions that you can apply to your JAX and Flax networks. In particular, you
    walk away with nuggets about these optimizers:'
  id: totrans-408
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: é€‰æ‹©æ­£ç¡®çš„ä¼˜åŒ–å™¨å‡½æ•°å†³å®šäº†è®­ç»ƒç½‘ç»œæ‰€éœ€çš„æ—¶é—´ã€‚å®ƒè¿˜å†³å®šäº†æ¨¡å‹çš„è¡¨ç°å¦‚ä½•ã€‚å› æ­¤ï¼Œåœ¨æ„å»ºJAXå’ŒFlaxç½‘ç»œæ—¶é€‰æ‹©é€‚å½“çš„ä¼˜åŒ–å™¨å‡½æ•°è‡³å…³é‡è¦ã€‚æœ¬æ–‡è®¨è®ºäº†å¯ä»¥åº”ç”¨äºæ‚¨çš„ç½‘ç»œçš„å„ç§ä¼˜åŒ–å™¨å‡½æ•°ã€‚ç‰¹åˆ«æ˜¯ï¼Œæ‚¨å°†äº†è§£ä»¥ä¸‹ä¼˜åŒ–å™¨çš„ç²¾åï¼š
- en: Adam optimizer in JAX.
  id: totrans-409
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAXä¸­çš„Adamä¼˜åŒ–å™¨ã€‚
- en: RMSProp optimizer in Flax.
  id: totrans-410
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: Flaxä¸­çš„RMSPropä¼˜åŒ–å™¨ã€‚
- en: Stochastic Gradient Descent in JAX.
  id: totrans-411
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAXä¸­çš„éšæœºæ¢¯åº¦ä¸‹é™ã€‚
- en: '`..to mention a few.`'
  id: totrans-412
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`..æåŠä¸€äº›ã€‚`'
- en: '**JAX loss functions**'
  id: totrans-413
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**JAXæŸå¤±å‡½æ•°**'
- en: Loss functions are at the core of training machine learning. They can be used
    to identify how well the model is performing on a dataset. Poor performance leads
    to a very high loss, while a well-performing model will have a lower loss. Therefore,
    the choice of a loss function is an important one when building machine learning
    models.
  id: totrans-414
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æŸå¤±å‡½æ•°æ˜¯è®­ç»ƒæœºå™¨å­¦ä¹ çš„æ ¸å¿ƒã€‚å®ƒä»¬å¯ä»¥ç”¨æ¥è¯†åˆ«æ¨¡å‹åœ¨æ•°æ®é›†ä¸Šçš„è¡¨ç°å¦‚ä½•ã€‚è¡¨ç°ä¸ä½³ä¼šå¯¼è‡´éå¸¸é«˜çš„æŸå¤±ï¼Œè€Œè¡¨ç°è‰¯å¥½çš„æ¨¡å‹å°†å…·æœ‰è¾ƒä½çš„æŸå¤±ã€‚å› æ­¤ï¼Œåœ¨æ„å»ºæœºå™¨å­¦ä¹ æ¨¡å‹æ—¶ï¼Œé€‰æ‹©æŸå¤±å‡½æ•°æ˜¯ä¸€ä¸ªé‡è¦çš„å†³å®šã€‚
- en: In this article, we'll look at the loss functions available inÂ JAXÂ and how you
    can use them.
  id: totrans-415
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹çœ‹JAXä¸­å¯ç”¨çš„æŸå¤±å‡½æ•°ä»¥åŠå¦‚ä½•ä½¿ç”¨å®ƒä»¬ã€‚
- en: '**What is a loss function?**'
  id: totrans-416
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**ä»€ä¹ˆæ˜¯æŸå¤±å‡½æ•°ï¼Ÿ**'
- en: Machine learning models learn by evaluating predictions against true values
    and adjusting the weights. The objective is to obtain the weights that minimize
    theÂ **loss function**, that is theÂ **error**. The loss function is also referred
    to as theÂ **cost function**. The choice of a loss function depends on the problem.
    The two most common problems are classification and regression problems. Each
    will require a different set of loss functions.
  id: totrans-417
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æœºå™¨å­¦ä¹ æ¨¡å‹é€šè¿‡è¯„ä¼°é¢„æµ‹ä¸çœŸå®å€¼çš„å·®å¼‚å¹¶è°ƒæ•´æƒé‡æ¥å­¦ä¹ ã€‚ç›®æ ‡æ˜¯è·å¾—èƒ½å¤Ÿæœ€å°åŒ–**æŸå¤±å‡½æ•°**ï¼ˆå³**è¯¯å·®**ï¼‰çš„æƒé‡ã€‚æŸå¤±å‡½æ•°ä¹Ÿç§°ä¸º**æˆæœ¬å‡½æ•°**ã€‚é€‰æ‹©æŸå¤±å‡½æ•°å–å†³äºé—®é¢˜çš„æ€§è´¨ã€‚æœ€å¸¸è§çš„é—®é¢˜æ˜¯åˆ†ç±»å’Œå›å½’é—®é¢˜ã€‚æ¯ä¸ªé—®é¢˜éœ€è¦ä¸åŒçš„æŸå¤±å‡½æ•°ã€‚
- en: '**Creating custom loss functions in JAX**'
  id: totrans-418
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**åœ¨JAXä¸­åˆ›å»ºè‡ªå®šä¹‰æŸå¤±å‡½æ•°**'
- en: WhenÂ training networks with JAX, you'll need to obtain the logits at the training
    stage. These logits are used for computing the loss. You'll then need to evaluate
    the loss function and its gradient. The gradient is used to update the model parameters.
    At this point, you can compute the training metrics for the model.
  id: totrans-419
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒJAXç½‘ç»œæ—¶ï¼Œæ‚¨éœ€è¦åœ¨è®­ç»ƒé˜¶æ®µè·å–logitsã€‚è¿™äº›logitsç”¨äºè®¡ç®—æŸå¤±ã€‚ç„¶åéœ€è¦è¯„ä¼°æŸå¤±å‡½æ•°åŠå…¶æ¢¯åº¦ã€‚æ¢¯åº¦ç”¨äºæ›´æ–°æ¨¡å‹å‚æ•°ã€‚æ­¤æ—¶ï¼Œæ‚¨å¯ä»¥ä¸ºæ¨¡å‹è®¡ç®—è®­ç»ƒæŒ‡æ ‡ã€‚
- en: ğŸ’¡Â **What are logits?** Logits are unnormalized log probabilities.*
  id: totrans-420
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ğŸ’¡Â **Logits æ˜¯ä»€ä¹ˆï¼Ÿ** Logits æ˜¯æœªå½’ä¸€åŒ–çš„å¯¹æ•°æ¦‚ç‡ã€‚*
- en: '`def compute_loss(params,images,labels):`'
  id: totrans-421
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def compute_loss(params,images,labels):`'
- en: '`logits = CNN().apply({''params'': params}, images) loss = cross_entropy_loss(logits=logits,
    labels=labels) return loss, logits`'
  id: totrans-422
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`logits = CNN().apply({''params'': params}, images) loss = cross_entropy_loss(logits=logits,
    labels=labels) return loss, logits`'
- en: '`@jax.jitdef train_step(state,images, labels): """Train for a single step."""
    (_, logits), grads = jax.value_and_grad(compute_loss, has_aux`'
  id: totrans-423
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`@jax.jitdef train_step(state,images, labels): """å•æ­¥è®­ç»ƒã€‚""" (_, logits), grads
    = jax.value_and_grad(compute_loss, has_aux`'
- en: '`=True)(state.params,images,labels)`'
  id: totrans-424
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`=True)(state.params,images,labels)`'
- en: '`state = state.apply_gradients(grads=grads)`'
  id: totrans-425
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`state = state.apply_gradients(grads=grads)`'
- en: metrics = compute_metrics(logits=logits, labels=labels) return state, metrics
  id: totrans-426
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`metrics = compute_metrics(logits=logits, labels=labels) return state, metrics`'
- en: You can use JAX functions such as `[log_sigmoid]` and `[log_softmax]` to build
    custom loss functions. You can even write your loss functions from scratch without
    using these functions.
  id: totrans-427
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ä½¿ç”¨JAXå‡½æ•°å¦‚`[log_sigmoid]`å’Œ`[log_softmax]`æ¥æ„å»ºè‡ªå®šä¹‰æŸå¤±å‡½æ•°ã€‚ç”šè‡³å¯ä»¥ä¸ä½¿ç”¨è¿™äº›å‡½æ•°ä»å¤´å¼€å§‹ç¼–å†™æ‚¨è‡ªå·±çš„æŸå¤±å‡½æ•°ã€‚
- en: Here is an example of computing the sigmoid binary cross entropy loss.
  id: totrans-428
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¿™æ˜¯è®¡ç®—SigmoidäºŒå…ƒäº¤å‰ç†µæŸå¤±çš„ä¸€ä¸ªä¾‹å­ã€‚
- en: '`import jax`'
  id: totrans-429
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import jax`'
- en: '`def custom_sigmoid_binary_cross_entropy(logits, labels): log_p = jax.nn.log_sigmoid(logits)`'
  id: totrans-430
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def custom_sigmoid_binary_cross_entropy(logits, labels): log_p = jax.nn.log_sigmoid(logits)`'
- en: '`log_not_p = jax.nn.log_sigmoid(-logits)`'
  id: totrans-431
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`log_not_p = jax.nn.log_sigmoid(-logits)`'
- en: '`return -labels * log_p - (1\. - labels) * log_not_p`'
  id: totrans-432
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return -labels * log_p - (1\. - labels) * log_not_p`'
- en: '`custom_sigmoid_binary_cross_entropy(0.5,0.0) # DeviceArray(0.974077, dtype=float32,
    weak_type=True)`'
  id: totrans-433
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`custom_sigmoid_binary_cross_entropy(0.5,0.0) # DeviceArray(0.974077, dtype=float32,
    weak_type=True)`'
- en: '**Which loss functions are available in JAX?**'
  id: totrans-434
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**JAXä¸­æœ‰å“ªäº›æŸå¤±å‡½æ•°å¯ç”¨ï¼Ÿ**'
- en: Building custom loss functions for your networks can introduce errors in your
    program. Furthermore, you have to take the burden of maintaining these functions.
    However, if the loss function you want is unavailable, there is a strong case
    for creating a custom loss function. Be that as it may, there is no need to reinvent
    the wheel and rewrite the already implemented loss functions.
  id: totrans-435
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä¸ºæ‚¨çš„ç½‘ç»œæ„å»ºè‡ªå®šä¹‰æŸå¤±å‡½æ•°å¯èƒ½ä¼šåœ¨ç¨‹åºä¸­å¼•å…¥é”™è¯¯ã€‚æ­¤å¤–ï¼Œæ‚¨å¿…é¡»æ‰¿æ‹…ç»´æŠ¤è¿™äº›å‡½æ•°çš„è´Ÿæ‹…ã€‚ç„¶è€Œï¼Œå¦‚æœæ‚¨éœ€è¦çš„æŸå¤±å‡½æ•°ä¸å¯ç”¨ï¼Œåˆ™æœ‰ç†ç”±åˆ›å»ºè‡ªå®šä¹‰æŸå¤±å‡½æ•°ã€‚å°½ç®¡å¦‚æ­¤ï¼Œæ²¡æœ‰å¿…è¦é‡æ–°å‘æ˜è½®å­å¹¶é‡å†™å·²ç»å®ç°çš„æŸå¤±å‡½æ•°ã€‚
- en: JAX doesn't ship with any loss functions. In JAX, we useÂ `optax`Â for defining
    loss functions. It's important to ensure that you use JAXcompatible libraries
    to take advantage of functions such asÂ `[JIT]`,Â `[vmap]`Â andÂ `[pmap]`Â that make
    your programs faster.
  id: totrans-436
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAXä¸æä¾›ä»»ä½•æŸå¤±å‡½æ•°ã€‚åœ¨JAXä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨Â `optax`Â æ¥å®šä¹‰æŸå¤±å‡½æ•°ã€‚ç¡®ä¿æ‚¨ä½¿ç”¨ä¸JAXå…¼å®¹çš„åº“ä»¥åˆ©ç”¨è¯¸å¦‚Â `[JIT]`ã€Â `[vmap]`Â å’ŒÂ `[pmap]`Â ç­‰å‡½æ•°ï¼Œè¿™äº›å‡½æ•°èƒ½å¤ŸåŠ å¿«ç¨‹åºè¿è¡Œé€Ÿåº¦ã€‚
- en: Let's take a look at some of the loss functions available inÂ `optax`.
  id: totrans-437
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹åœ¨`optax`ä¸­å¯ç”¨çš„ä¸€äº›æŸå¤±å‡½æ•°ã€‚
- en: '**Sigmoid binary cross entropy**'
  id: totrans-438
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**Sigmoid äºŒå…ƒäº¤å‰ç†µ**'
- en: The sigmoid binary cross entropy loss is computed
  id: totrans-439
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è®¡ç®—SigmoidäºŒå…ƒäº¤å‰ç†µæŸå¤±
- en: usingÂ `optax.sigmoid_binary_cross_entropy`. The function expects
  id: totrans-440
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Â `optax.sigmoid_binary_cross_entropy`ã€‚è¯¥å‡½æ•°æœŸæœ›
- en: '`logits` and `class labels`. It is used in problems where the classes are not
    mutually exclusive. For example, the model can predict that the image contains
    two objects in anÂ image classification problem.'
  id: totrans-441
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`logits` å’Œ `class labels`ã€‚å®ƒç”¨äºç±»åˆ«ä¸äº’æ–¥çš„é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œæ¨¡å‹å¯ä»¥é¢„æµ‹å›¾åƒä¸­åŒ…å«ä¸¤ä¸ªå¯¹è±¡çš„å›¾åƒåˆ†ç±»é—®é¢˜ã€‚'
- en: '`optax.sigmoid_binary_cross_entropy(0.5,0.0)# DeviceArray(0.974077, dtype=float32,
    weak_type=True)`'
  id: totrans-442
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`optax.sigmoid_binary_cross_entropy(0.5,0.0)# DeviceArray(0.974077, dtype=float32,
    weak_type=True)`'
- en: '**Softmax cross entropy**'
  id: totrans-443
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**Softmax äº¤å‰ç†µ**'
- en: The softmax cross entropy function is used where the classes are mutually exclusive.
    For example, in the MNIST dataset, each digit has exactly one label. The function
    expects an array of logits and probability distributions. The probability distribution
    sum to 1.
  id: totrans-444
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: Softmax äº¤å‰ç†µå‡½æ•°ç”¨äºç±»åˆ«äº’æ–¥çš„é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œåœ¨ MNIST æ•°æ®é›†ä¸­ï¼Œæ¯ä¸ªæ•°å­—æ­£å¥½æœ‰ä¸€ä¸ªæ ‡ç­¾ã€‚è¯¥å‡½æ•°æœŸæœ›ä¸€ä¸ª logits æ•°ç»„å’Œæ¦‚ç‡åˆ†å¸ƒã€‚æ¦‚ç‡åˆ†å¸ƒæ€»å’Œä¸º1ã€‚
- en: '`logits = jnp.array([0.50,0.60,0.70,0.30,0.25]) labels = jnp.array([0.20,0.30,0.10,0.20,0.2])
    optax.softmax_cross_entropy(logits,labels) # DeviceArray(1.6341426, dtype=float32)`'
  id: totrans-445
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`logits = jnp.array([0.50,0.60,0.70,0.30,0.25]) labels = jnp.array([0.20,0.30,0.10,0.20,0.2])
    optax.softmax_cross_entropy(logits,labels) # DeviceArray(1.6341426, dtype=float32)`'
- en: '**Cosine distance**'
  id: totrans-446
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**ä½™å¼¦è·ç¦»**'
- en: The cosine distance measures the cosine distance between targets and predictions.
  id: totrans-447
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä½™å¼¦è·ç¦»è¡¡é‡ç›®æ ‡ä¸é¢„æµ‹ä¹‹é—´çš„ä½™å¼¦è·ç¦»ã€‚
- en: '`predictions = jnp.array([0.50,0.60,0.70,0.30,0.25]) targets = jnp.array([0.20,0.30,0.10,0.20,0.2])
    optax.cosine_distance(predictions,targets,epsilon=0.7) # DeviceArray(0.4128204,
    dtype=float32)`'
  id: totrans-448
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`predictions = jnp.array([0.50,0.60,0.70,0.30,0.25]) targets = jnp.array([0.20,0.30,0.10,0.20,0.2])
    optax.cosine_distance(predictions,targets,epsilon=0.7) # DeviceArray(0.4128204,
    dtype=float32)`'
- en: '**Cosine similarity**'
  id: totrans-449
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**ä½™å¼¦ç›¸ä¼¼åº¦**'
- en: The cosine similarity loss measures the cosine similarity between the true and
    predicted values. The cosine similarity is the cosine of the angle between two
    vectors. This is obtained by the dot product of the vectors divided by the product
    of their lengths.
  id: totrans-450
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä½™å¼¦ç›¸ä¼¼åº¦æŸå¤±å‡½æ•°è¡¡é‡çœŸå®å€¼å’Œé¢„æµ‹å€¼ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚ä½™å¼¦ç›¸ä¼¼åº¦æ˜¯ä¸¤ä¸ªå‘é‡ä¹‹é—´è§’åº¦çš„ä½™å¼¦ã€‚é€šè¿‡å‘é‡çš„ç‚¹ç§¯é™¤ä»¥å®ƒä»¬é•¿åº¦çš„ä¹˜ç§¯å¾—åˆ°ã€‚
- en: The result is a number between -1 and 1\. 0 shows orthogonality, while numbers
    closer to -1 indicate similarity. Numbers close to Â 1 portray high dissimilarity.
  id: totrans-451
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ç»“æœæ˜¯-1åˆ°1ä¹‹é—´çš„æ•°ã€‚0è¡¨ç¤ºæ­£äº¤ï¼Œæ¥è¿‘-1è¡¨ç¤ºç›¸ä¼¼åº¦é«˜ã€‚æ¥è¿‘1è¡¨ç¤ºé«˜åº¦ä¸ç›¸ä¼¼ã€‚
- en: '`predictions = jnp.array([0.50,0.60,0.70,0.30,0.25]) targets = jnp.array([0.20,0.30,0.10,0.20,0.2])
    optax.cosine_similarity(predictions,targets,epsilon=0.5) # DeviceArray(0.8220514,
    dtype=float32)`'
  id: totrans-452
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`predictions = jnp.array([0.50,0.60,0.70,0.30,0.25]) targets = jnp.array([0.20,0.30,0.10,0.20,0.2])
    optax.cosine_similarity(predictions,targets,epsilon=0.5) # DeviceArray(0.8220514,
    dtype=float32)`'
- en: '**`Huber loss`**'
  id: totrans-453
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**Huber æŸå¤±**'
- en: The Huber loss is used for regression problems. It is less sensitive to outliers
    compared to the squared error loss. A variant of the Huber loss that can be used
    in classification problems exists.
  id: totrans-454
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: Huber æŸå¤±å‡½æ•°ç”¨äºå›å½’é—®é¢˜ã€‚ä¸å¹³æ–¹è¯¯å·®æŸå¤±ç›¸æ¯”ï¼Œå®ƒå¯¹å¼‚å¸¸å€¼ä¸å¤ªæ•æ„Ÿã€‚å­˜åœ¨å¯ç”¨äºåˆ†ç±»é—®é¢˜çš„ Huber æŸå¤±å‡½æ•°çš„å˜ä½“ã€‚
- en: '`logits = jnp.array([0.50,0.60,0.70,0.30,0.25])`'
  id: totrans-455
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`logits = jnp.array([0.50,0.60,0.70,0.30,0.25])`'
- en: '`labels = jnp.array([0.20,0.30,0.10,0.20,0.2])`'
  id: totrans-456
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`labels = jnp.array([0.20,0.30,0.10,0.20,0.2])`'
- en: '`optax.huber_loss(logits,labels)`'
  id: totrans-457
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`optax.huber_loss(logits,labels)`'
- en: '`DeviceArray([0.045 , 0.045 , 0.17999998, 0.005 ,`'
  id: totrans-458
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`DeviceArray([0.045 , 0.045 , 0.17999998, 0.005 ,`'
- en: '`0.00125 ], dtype=float32)`'
  id: totrans-459
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`0.00125 ], dtype=float32)`'
- en: '**`l2 loss`**'
  id: totrans-460
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**l2 æŸå¤±**'
- en: L2 loss function is the Least Square Errors. The L2 loss aims at minimizing
    the sum of the squared differences between the true and predicted values. The
    Mean Squared Error is the mean of all L2 loss values.
  id: totrans-461
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: L2 æŸå¤±å‡½æ•°æ˜¯æœ€å°äºŒä¹˜è¯¯å·®ã€‚L2 æŸå¤±æ—¨åœ¨æœ€å°åŒ–çœŸå®å€¼ä¸é¢„æµ‹å€¼ä¹‹é—´çš„å¹³æ–¹å·®çš„æ€»å’Œã€‚å‡æ–¹è¯¯å·®æ˜¯æ‰€æœ‰ L2 æŸå¤±å€¼çš„å¹³å‡å€¼ã€‚
- en: '`predictions = jnp.array([0.50,0.60,0.70,0.30,0.25]) targets = jnp.array([0.20,0.30,0.10,0.20,0.2])`'
  id: totrans-462
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`predictions = jnp.array([0.50,0.60,0.70,0.30,0.25]) targets = jnp.array([0.20,0.30,0.10,0.20,0.2])`'
- en: '`optax.l2_loss(predictions,targets)`'
  id: totrans-463
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`optax.l2_loss(predictions,targets)`'
- en: '`DeviceArray([0.045 , 0.045 , 0.17999998, 0.005 ,`'
  id: totrans-464
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`DeviceArray([0.045 , 0.045 , 0.17999998, 0.005 ,`'
- en: '`0.00125 ], dtype=float32)`'
  id: totrans-465
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`0.00125 ], dtype=float32)`'
- en: '**`log cosh`**'
  id: totrans-466
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**`log cosh`**'
- en: '`[`log_cosh`]`Â is the logarithm of the hyperbolic cosine of the prediction
    error.'
  id: totrans-467
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[`log_cosh`]`Â æ˜¯é¢„æµ‹è¯¯å·®çš„åŒæ›²ä½™å¼¦çš„å¯¹æ•°ã€‚'
- en: ğŸ’¡Â `log(cosh(x))` is approximately equal to `(x ** 2) / 2` for small `x` and
    to `abs(x) - log(2)` for large `x`. This means that 'logcosh' works mostly like
    the mean squared error, but will not be so strongly affected by the occasional
    wildly incorrect prediction.Â TensorFlow Docs*
  id: totrans-468
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ğŸ’¡Â `log(cosh(x))` is approximately equal to `(x ** 2) / 2` for small `x` and
    to `abs(x) - log(2)` for large `x`. This means that 'logcosh' works mostly like
    the mean squared error, but will not be so strongly affected by the occasional
    wildly incorrect prediction.Â TensorFlow Docs*
- en: '`predictions = jnp.array([0.50,0.60,0.70,0.30,0.25]) targets = jnp.array([0.20,0.30,0.10,0.20,0.2])`'
  id: totrans-469
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`predictions = jnp.array([0.50,0.60,0.70,0.30,0.25]) targets = jnp.array([0.20,0.30,0.10,0.20,0.2])`'
- en: '`optax.log_cosh(predictions,targets)`'
  id: totrans-470
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`optax.log_cosh(predictions,targets)`'
- en: '`DeviceArray([0.04434085, 0.04434085, 0.17013526, 0.00499171,`'
  id: totrans-471
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`DeviceArray([0.04434085, 0.04434085, 0.17013526, 0.00499171,`'
- en: '`0.00124949], dtype=float32)`'
  id: totrans-472
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`0.00124949], dtype=float32)`'
- en: '**`Smooth labels`**'
  id: totrans-473
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**`Smooth labels`**'
- en: '`[`optax.smooth_labels`]` Â is used together with a cross-entropy loss to smooth
    labels. It returns a smoothed version of the one hot input labels. Label smoothing
    has been applied in image classification, language translation, and speech recognition
    to prevent models from becoming overconfident.'
  id: totrans-474
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[`optax.smooth_labels`]` Â ä¸äº¤å‰ç†µæŸå¤±ä¸€èµ·ä½¿ç”¨ä»¥å¹³æ»‘æ ‡ç­¾ã€‚å®ƒè¿”å›è¾“å…¥æ ‡ç­¾çš„å¹³æ»‘ç‰ˆæœ¬ã€‚æ ‡ç­¾å¹³æ»‘å·²åº”ç”¨äºå›¾åƒåˆ†ç±»ã€è¯­è¨€ç¿»è¯‘å’Œè¯­éŸ³è¯†åˆ«ï¼Œä»¥é˜²æ­¢æ¨¡å‹è¿‡äºè‡ªä¿¡ã€‚'
- en: '`labels = jnp.array([0.20,0.30,0.10,0.20,0.2])`'
  id: totrans-475
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`labels = jnp.array([0.20,0.30,0.10,0.20,0.2])`'
- en: '`optax.smooth_labels(labels,alpha=0.4)`'
  id: totrans-476
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`optax.smooth_labels(labels,alpha=0.4)`'
- en: '`DeviceArray([0.2 , 0.26, 0.14, 0.2 , 0.2 ], dtype=float32)`'
  id: totrans-477
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`DeviceArray([0.2 , 0.26, 0.14, 0.2 , 0.2 ], dtype=float32)`'
- en: '**`Computing loss with JAX Metrics`**'
  id: totrans-478
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**`Computing loss with JAX Metrics`**'
- en: JAX MetricsÂ is an open-source package for computing losses and metrics in JAX.
    It provides aÂ Keras-like API for computing model loss and metrics.
  id: totrans-479
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAX Metrics æ˜¯ä¸€ä¸ªç”¨äºåœ¨ JAX ä¸­è®¡ç®—æŸå¤±å’ŒæŒ‡æ ‡çš„å¼€æºåŒ…ã€‚å®ƒæä¾›äº†ç±»ä¼¼ Keras çš„ API æ¥è®¡ç®—æ¨¡å‹çš„æŸå¤±å’ŒæŒ‡æ ‡ã€‚
- en: For example, here is how you use the library to compute the crossentropy loss.
    Similar to Keras, the losses can be computed by either instantiating theÂ [`Loss`]Â orÂ [`loss`].
  id: totrans-480
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œè¿™é‡Œæ˜¯å¦‚ä½•ä½¿ç”¨åº“æ¥è®¡ç®—äº¤å‰ç†µæŸå¤±çš„ã€‚ä¸ Keras ç±»ä¼¼ï¼Œå¯ä»¥é€šè¿‡å®ä¾‹åŒ– [`Loss`] æˆ– [`loss`] æ¥è®¡ç®—æŸå¤±ã€‚
- en: '`pip install jax_metrics import jax_metrics as jm`'
  id: totrans-481
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`pip install jax_metrics import jax_metrics as jm`'
- en: '`crossentropy = jm.losses.Crossentropy() logits = jnp.array([0.50,0.60,0.70,0.30,0.25])
    y = jnp.array([0.50,0.60,0.70,0.30,0.25]) crossentropy(target=y, preds=logits)`'
  id: totrans-482
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`crossentropy = jm.losses.Crossentropy() logits = jnp.array([0.50,0.60,0.70,0.30,0.25])
    y = jnp.array([0.50,0.60,0.70,0.30,0.25]) crossentropy(target=y, preds=logits)`'
- en: '`DeviceArray(3.668735, dtype=float32)`'
  id: totrans-483
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`DeviceArray(3.668735, dtype=float32)`'
- en: å˜é‡å`logits`èµ‹å€¼`jnp.array([0.50,0.60,0.70,0.30,0.25])` `y`èµ‹å€¼`jnp.array([0.50,0.60,0.70,0.30,0.25])`
    `jm.losses.crossentropy`(`target=y`, `preds=logits`) `# DeviceArray(3.668735,
    dtype=float32)`
  id: totrans-484
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å˜é‡å`logits`èµ‹å€¼`jnp.array([0.50,0.60,0.70,0.30,0.25])` `y`èµ‹å€¼`jnp.array([0.50,0.60,0.70,0.30,0.25])`
    `jm.losses.crossentropy`(`target=y`, `preds=logits`) `# DeviceArray(3.668735,
    dtype=float32)`
- en: '**Here is what the code would like in a JAX training step.import jax_metrics
    as jmmetric = jm.metrics.Accuracy()@jax.jitdef init_step`(`metric: jm.Metric`)
    -> `jm.Metric`: return `metric.init()`'
  id: totrans-485
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '**Here is what the code would like in a JAX training step.import jax_metrics
    as jmmetric = jm.metrics.Accuracy()@jax.jitdef init_step`(`metric: jm.Metric`)
    -> `jm.Metric`: return `metric.init()`'
- en: 'å‡½æ•°å`loss_fn`ï¼Œå‚æ•°`params`ï¼Œ`metric`ï¼Œ`x`ï¼Œ`y`: ...'
  id: totrans-486
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'å‡½æ•°å`loss_fn`ï¼Œå‚æ•°`params`ï¼Œ`metric`ï¼Œ`x`ï¼Œ`y`: ...'
- en: å˜é‡å`metric`èµ‹å€¼`metric.update`ï¼Œå‚æ•°`target=y`ï¼Œ`preds=logits`...
  id: totrans-487
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å˜é‡å`metric`èµ‹å€¼`metric.update`ï¼Œå‚æ•°`target=y`ï¼Œ`preds=logits`...
- en: return `loss`, `metric`@jax.jitdef train_step`(`params`, `metric`, `x`, `y`):grads,
    metric = jax.grad`(`loss_fn`, `has_aux=True`)(
  id: totrans-488
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: return `loss`, `metric`@jax.jitdef train_step`(`params`, `metric`, `x`, `y`):grads,
    metric = jax.grad`(`loss_fn`, `has_aux=True`)(
- en: '`params`, `metric`, `x`, `y`'
  id: totrans-489
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`params`, `metric`, `x`, `y`'
- en: )
  id: totrans-490
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: )
- en: '...'
  id: totrans-491
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '...'
- en: return `params`, `metric`
  id: totrans-492
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: return `params`, `metric`
- en: '`@jax.jitdef reset_step`(`metric: jm.Metric`) -> `jm.Metric`: return `metric.reset()`The
    losses we have seen earlier can also be computed using JAX Metrics.'
  id: totrans-493
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`@jax.jitdef reset_step`(`metric: jm.Metric`) -> `jm.Metric`: return `metric.reset()`The
    losses we have seen earlier can also be computed using JAX Metrics.'
- en: '`! pip install jax_metrics`'
  id: totrans-494
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`! pip install jax_metrics`'
- en: '`import jax_metrics as jm`'
  id: totrans-495
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import jax_metrics as jm`'
- en: å˜é‡å`target`èµ‹å€¼`jnp.array([50,60,70,30,25])` `preds`èµ‹å€¼`jnp.array([0.50,0.60,0.70,0.30,0.25])`
    `huber_loss`èµ‹å€¼`jm.losses.Huber()`
  id: totrans-496
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å˜é‡å`target`èµ‹å€¼`jnp.array([50,60,70,30,25])` `preds`èµ‹å€¼`jnp.array([0.50,0.60,0.70,0.30,0.25])`
    `huber_loss`èµ‹å€¼`jm.losses.Huber()`
- en: '`huber_loss`(`target=target`, `preds=preds`) `# DeviceArray(46.030003, dtype=float32)`'
  id: totrans-497
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`huber_loss`(`target=target`, `preds=preds`) `# DeviceArray(46.030003, dtype=float32)`'
- en: å˜é‡å`target`èµ‹å€¼`jnp.array([50,60,70,30,25])`
  id: totrans-498
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å˜é‡å`target`èµ‹å€¼`jnp.array([50,60,70,30,25])`
- en: å˜é‡å`preds`èµ‹å€¼`jnp.array([0.50,0.60,0.70,0.30,0.25])`
  id: totrans-499
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å˜é‡å`preds`èµ‹å€¼`jnp.array([0.50,0.60,0.70,0.30,0.25])`
- en: '`jm.losses.mean_absolute_error`(`target=target`, `preds=preds`) `# DeviceArray(46.530003,
    dtype=float32)`'
  id: totrans-500
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jm.losses.mean_absolute_error`(`target=target`, `preds=preds`) `# DeviceArray(46.530003,
    dtype=float32)`'
- en: '`rng`èµ‹å€¼`jax.random.PRNGKey(42)`'
  id: totrans-501
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`rng`èµ‹å€¼`jax.random.PRNGKey(42)`'
- en: å˜é‡å`target`èµ‹å€¼`jax.random.randint(rng, shape=(2, 3), minval=0, maxval =2)`
  id: totrans-502
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å˜é‡å`target`èµ‹å€¼`jax.random.randint(rng, shape=(2, 3), minval=0, maxval =2)`
- en: å˜é‡å`preds`èµ‹å€¼`jax.random.uniform(rng, shape=(2, 3))`
  id: totrans-503
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å˜é‡å`preds`èµ‹å€¼`jax.random.uniform(rng, shape=(2, 3))`
- en: '`jm.losses.cosine_similarity`(`target`, `preds`, `axis=1`)'
  id: totrans-504
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jm.losses.cosine_similarity`(`target`, `preds`, `axis=1`)'
- en: '`DeviceArray`([-0.8602638 , -0.33731455], dtype=float32) `target`èµ‹å€¼`jnp.array([50,60,70,30,25])`'
  id: totrans-505
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`DeviceArray`([-0.8602638 , -0.33731455], dtype=float32) `target`èµ‹å€¼`jnp.array([50,60,70,30,25])`'
- en: å˜é‡å`preds`èµ‹å€¼`jnp.array([0.50,0.60,0.70,0.30,0.25])`
  id: totrans-506
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å˜é‡å`preds`èµ‹å€¼`jnp.array([0.50,0.60,0.70,0.30,0.25])`
- en: '`jm.losses.mean_absolute_percentage_error`(`target=target`, `preds=p reds`)'
  id: totrans-507
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jm.losses.mean_absolute_percentage_error`(`target=target`, `preds=preds`)'
- en: '`DeviceArray(98.99999, dtype=float32)`'
  id: totrans-508
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`DeviceArray(98.99999, dtype=float32)`'
- en: å˜é‡å`target`èµ‹å€¼`jnp.array([50,60,70,30,25])`
  id: totrans-509
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å˜é‡å`target`èµ‹å€¼`jnp.array([50,60,70,30,25])`
- en: å˜é‡å`preds`èµ‹å€¼`jnp.array([0.50,0.60,0.70,0.30,0.25])`
  id: totrans-510
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å˜é‡å`preds`èµ‹å€¼`jnp.array([0.50,0.60,0.70,0.30,0.25])`
- en: '`jm.losses.mean_squared_logarithmic_error`(`target=target`, `preds=p reds`)'
  id: totrans-511
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jm.losses.mean_squared_logarithmic_error`(`target=target`, `preds=preds`)'
- en: '`DeviceArray(11.7779, dtype=float32)`'
  id: totrans-512
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`DeviceArray(11.7779, dtype=float32)`'
- en: '`target`èµ‹å€¼`jnp.array([0.50,0.60,0.70,0.30,0.25])` `preds`èµ‹å€¼`jnp.array([0.50,0.60,0.70,0.30,0.25])`
    `jm.losses.mean_squared_error`(`target=target`, `preds=preds`) `# DeviceArray(0.,
    dtype=float32)`'
  id: totrans-513
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`target`èµ‹å€¼`jnp.array([0.50,0.60,0.70,0.30,0.25])` `preds`èµ‹å€¼`jnp.array([0.50,0.60,0.70,0.30,0.25])`
    `jm.losses.mean_squared_error`(`target=target`, `preds=preds`) `# DeviceArray(0.,
    dtype=float32)`'
- en: '**How to monitor JAX loss functions**'
  id: totrans-514
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**å¦‚ä½•ç›‘æ§JAXæŸå¤±å‡½æ•°**'
- en: Monitoring the loss of your network is important because it indicates whether
    it's learning or not. A glance at the loss can tell you if there are any problems
    in the network, such as overfitting. One way to monitor the loss is to print the
    training and validation loss as the network is training.
  id: totrans-515
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ç›‘æ§ç½‘ç»œçš„æŸå¤±å¾ˆé‡è¦ï¼Œå› ä¸ºå®ƒè¡¨æ˜ç½‘ç»œæ˜¯å¦åœ¨å­¦ä¹ ã€‚æŸå¤±çš„ä¸€ç¥å¯ä»¥å‘Šè¯‰æ‚¨ç½‘ç»œä¸­æ˜¯å¦å­˜åœ¨é—®é¢˜ï¼Œæ¯”å¦‚è¿‡æ‹Ÿåˆã€‚ç›‘æ§æŸå¤±çš„ä¸€ç§æ–¹å¼æ˜¯åœ¨ç½‘ç»œè®­ç»ƒæ—¶æ‰“å°è®­ç»ƒå’ŒéªŒè¯æŸå¤±ã€‚
- en: '`![](`../images/00014.jpeg`)`'
  id: totrans-516
  prefs: []
  stylish: true
  type: TYPE_IMG
  zh: '`![](`../images/00014.jpeg`)`'
- en: You can also plot the training and validation loss to represent the training
    visually.
  id: totrans-517
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥ç»˜åˆ¶è®­ç»ƒå’ŒéªŒè¯æŸå¤±ï¼Œä»¥å¯è§†åŒ–åœ°è¡¨ç¤ºè®­ç»ƒè¿‡ç¨‹ã€‚
- en: '`![](`../images/00015.jpeg`)`'
  id: totrans-518
  prefs: []
  stylish: true
  type: TYPE_IMG
  zh: '`![](`../images/00015.jpeg`)`'
- en: '**Why JAX loss nan happens**'
  id: totrans-519
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**ä¸ºä»€ä¹ˆJAXæŸå¤±ä¼šå‡ºç°NaN**'
- en: JAX will not show errors when NANs occur in your program. This is by design
    because of the complexities involved in showing errors from accelerators. When
    debugging, you can turn on the NAN checker to show NAN errors. NANs should be
    fixed because the network stops learning when they occur.
  id: totrans-520
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å½“æ‚¨çš„ç¨‹åºä¸­å‡ºç°NANæ—¶ï¼ŒJAXä¸ä¼šæ˜¾ç¤ºé”™è¯¯ã€‚è¿™æ˜¯è®¾è®¡ä¸Šçš„è€ƒè™‘ï¼Œå› ä¸ºä»åŠ é€Ÿå™¨æ˜¾ç¤ºé”™è¯¯æ¶‰åŠå¤æ‚æ€§ã€‚åœ¨è°ƒè¯•æ—¶ï¼Œæ‚¨å¯ä»¥æ‰“å¼€NANæ£€æŸ¥å™¨ä»¥æ˜¾ç¤ºNANé”™è¯¯ã€‚NANåº”è¯¥è¢«ä¿®å¤ï¼Œå› ä¸ºå½“å®ƒä»¬å‡ºç°æ—¶ï¼Œç½‘ç»œåœæ­¢å­¦ä¹ ã€‚
- en: '`from jax.config import config`'
  id: totrans-521
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from jax.config import config`'
- en: '`config.update`(`"jax_debug_nans"`, `True`)'
  id: totrans-522
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`config.update`(`"jax_debug_nans"`, `True`)'
- en: '`jnp.divide`(`0.0`, `0.0`)'
  id: totrans-523
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jnp.divide`(`0.0`, `0.0`)'
- en: '`FloatingPointError`: invalid value (`nan`) encountered in `div`'
  id: totrans-524
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`FloatingPointError`: é‡åˆ°äº†æ— æ•ˆå€¼ (`nan`) åœ¨ `div` ä¸­'
- en: 'However, what produces NANs in a network. There are various factors, not limited
    to:'
  id: totrans-525
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œä»€ä¹ˆå¯¼è‡´ç½‘ç»œä¸­å‡ºç°NaNï¼Ÿæœ‰å„ç§å› ç´ ï¼Œä¸é™äºï¼š
- en: The dataset has not been scaled.
  id: totrans-526
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ•°æ®é›†å°šæœªç¼©æ”¾ã€‚
- en: There are NANs in the training set. The occurrence of infinite values in the
    training data.
  id: totrans-527
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è®­ç»ƒé›†ä¸­å­˜åœ¨NaNã€‚è®­ç»ƒæ•°æ®ä¸­çš„æ— é™å€¼å‡ºç°ã€‚
- en: Wrong optimizer function. Exploding gradients leading to large updates to training
    weights. Using a very large learning rate.
  id: totrans-528
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: é”™è¯¯çš„ä¼˜åŒ–å™¨å‡½æ•°ã€‚æ¢¯åº¦çˆ†ç‚¸å¯¼è‡´è®­ç»ƒæƒé‡å¤§å¹…æ›´æ–°ã€‚ä½¿ç”¨éå¸¸å¤§çš„å­¦ä¹ ç‡ã€‚
- en: '`Final thoughts`'
  id: totrans-529
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '`æœ€åçš„æ€è€ƒ`'
- en: 'In this article, we have seen that choosing the right loss function is critical
    to the learning of a network. We have also discussed various loss functions in
    JAX. More precisely, we have coved:'
  id: totrans-530
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å·²ç»çœ‹åˆ°é€‰æ‹©æ­£ç¡®çš„æŸå¤±å‡½æ•°å¯¹ç½‘ç»œçš„å­¦ä¹ è‡³å…³é‡è¦ã€‚æˆ‘ä»¬è¿˜è®¨è®ºäº†JAXä¸­çš„å„ç§æŸå¤±å‡½æ•°ã€‚æ›´å‡†ç¡®åœ°è¯´ï¼Œæˆ‘ä»¬æ¶µç›–äº†ï¼š
- en: What is a loss function?
  id: totrans-531
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯æŸå¤±å‡½æ•°ï¼Ÿ
- en: How to create custom loss functions in JAX.
  id: totrans-532
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å¦‚ä½•åœ¨JAXä¸­åˆ›å»ºè‡ªå®šä¹‰æŸå¤±å‡½æ•°ã€‚
- en: Loss functions available in JAX.
  id: totrans-533
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAXä¸­å¯ç”¨çš„æŸå¤±å‡½æ•°ã€‚
- en: Computing loss with JAX Metrics.
  id: totrans-534
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä½¿ç”¨JAXæŒ‡æ ‡è®¡ç®—æŸå¤±ã€‚
- en: Monitoring loss in JAX.
  id: totrans-535
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨JAXä¸­ç›‘æ§æŸå¤±ã€‚
- en: How to avoid NANs in JAX.
  id: totrans-536
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å¦‚ä½•åœ¨JAXä¸­é¿å…NaNã€‚
- en: '`Activation functions in JAX and Flax`'
  id: totrans-537
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '`åœ¨JAXå’ŒFlaxä¸­çš„æ¿€æ´»å‡½æ•°`'
- en: Activation functions are applied in neural networks to ensure that the network
    outputs the desired result. The activations functions cap the output within a
    specific range. For instance, when solving a binary classification problem, the
    outcome should be a number between 0 and 1\. This indicates the probability of
    an item belonging to either of the two classes.
  id: totrans-538
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ¿€æ´»å‡½æ•°è¢«åº”ç”¨åœ¨ç¥ç»ç½‘ç»œä¸­ï¼Œä»¥ç¡®ä¿ç½‘ç»œè¾“å‡ºæ‰€éœ€çš„ç»“æœã€‚æ¿€æ´»å‡½æ•°å°†è¾“å‡ºé™åˆ¶åœ¨ç‰¹å®šèŒƒå›´å†…ã€‚ä¾‹å¦‚ï¼Œåœ¨è§£å†³äºŒå…ƒåˆ†ç±»é—®é¢˜æ—¶ï¼Œç»“æœåº”è¯¥æ˜¯ä¸€ä¸ªä»‹äº0å’Œ1ä¹‹é—´çš„æ•°å­—ã€‚è¿™è¡¨ç¤ºç‰©å“å±äºä¸¤ä¸ªç±»åˆ«çš„æ¦‚ç‡ã€‚
- en: However, in a regression problem, you want the numerical prediction of a quantity,
    for example, the price of an item. You should, therefore, choose an appropriate
    activation function for the problem being solved.
  id: totrans-539
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œåœ¨å›å½’é—®é¢˜ä¸­ï¼Œæ‚¨å¸Œæœ›æ•°é‡çš„æ•°å€¼é¢„æµ‹ï¼Œä¾‹å¦‚ç‰©å“çš„ä»·æ ¼ã€‚å› æ­¤ï¼Œæ‚¨åº”è¯¥ä¸ºæ‰€è§£å†³çš„é—®é¢˜é€‰æ‹©åˆé€‚çš„æ¿€æ´»å‡½æ•°ã€‚
- en: Let's look at common activation functions in JAX and Flax.
  id: totrans-540
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¥çœ‹çœ‹JAXå’ŒFlaxä¸­å¸¸è§çš„æ¿€æ´»å‡½æ•°ã€‚
- en: '`ReLU â€“ Rectified linear unit`'
  id: totrans-541
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '`ReLU â€“ çŸ«æ­£çº¿æ€§å•å…ƒ`'
- en: TheÂ  **ReLU activation function**Â is primarily used in the hidden layers of
    neural networks to ensure non-linearity. The function caps all outputs to zero
    and above. Outputs below zero are returned as zero, while numbers above zero are
    returned as they are. This ensures that there are no negative numbers in the network.
  id: totrans-542
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '**ReLUæ¿€æ´»å‡½æ•°**ä¸»è¦ç”¨äºç¥ç»ç½‘ç»œçš„éšè—å±‚ï¼Œç¡®ä¿éçº¿æ€§ã€‚è¯¥å‡½æ•°å°†æ‰€æœ‰è¾“å‡ºé™åˆ¶åœ¨é›¶åŠä»¥ä¸Šã€‚å°äºé›¶çš„è¾“å‡ºè¢«è¿”å›ä¸ºé›¶ï¼Œè€Œå¤§äºé›¶çš„æ•°åˆ™åŸæ ·è¿”å›ã€‚è¿™ç¡®ä¿äº†ç½‘ç»œä¸­æ²¡æœ‰è´Ÿæ•°ã€‚'
- en: On line 9 we apply the ReLu activation function after the convolution layer.
  id: totrans-543
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬9è¡Œï¼Œæˆ‘ä»¬åœ¨å·ç§¯å±‚ä¹‹ååº”ç”¨ReLUæ¿€æ´»å‡½æ•°ã€‚
- en: '`import flaxfrom flax import linen as nnclass CNN(nn.Module):`'
  id: totrans-544
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import flaxfrom flax import linen as nnclass CNN(nn.Module):`'
- en: '`@nn.compact`'
  id: totrans-545
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`@nn.compact`'
- en: '`def __call__(self, x):`'
  id: totrans-546
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def __call__(self, x):`'
- en: '`x = nn.Conv(features=32, kernel_size=(3, 3))(x)`'
  id: totrans-547
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = nn.Conv(features=32, kernel_size=(3, 3))(x)`'
- en: '`x = nn.relu(x)`'
  id: totrans-548
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = nn.relu(x)`'
- en: '`x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))`'
  id: totrans-549
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))`'
- en: '`x = nn.Conv(features=64, kernel_size=(3, 3))(x)`'
  id: totrans-550
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = nn.Conv(features=64, kernel_size=(3, 3))(x)`'
- en: '`x = nn.relu(x)`'
  id: totrans-551
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = nn.relu(x)`'
- en: '`x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))`'
  id: totrans-552
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))`'
- en: '`x = x.reshape((x.shape[0], -1))`'
  id: totrans-553
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = x.reshape((x.shape[0], -1))`'
- en: '`x = nn.Dense(features=256)(x)`'
  id: totrans-554
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = nn.Dense(features=256)(x)`'
- en: '`x = nn.relu(x)`'
  id: totrans-555
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = nn.relu(x)`'
- en: '`x = nn.Dense(features=2)(x)`'
  id: totrans-556
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = nn.Dense(features=2)(x)`'
- en: '`x = nn.log_softmax(x)`'
  id: totrans-557
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = nn.log_softmax(x)`'
- en: return x
  id: totrans-558
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: return x
- en: '`PReLUâ€“ Parametric Rectified Linear Unit`'
  id: totrans-559
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '`PReLU â€“ å‚æ•°åŒ–ä¿®æ­£çº¿æ€§å•å…ƒ`'
- en: '`Parametric Rectified Linear Unit` Â is ReLU with extra parameters equal to
    the number of channels. It works by introducingÂ *aâ€“Â *a learnable parameter. PReLU
    allows for non-negative values.'
  id: totrans-560
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`å‚æ•°åŒ–ä¿®æ­£çº¿æ€§å•å…ƒ`æ˜¯å¸¦æœ‰é¢å¤–å‚æ•°çš„ReLUï¼Œå‚æ•°æ•°é‡ç­‰äºé€šé“æ•°ã€‚å®ƒé€šè¿‡å¼•å…¥ä¸€ä¸ªå¯å­¦ä¹ å‚æ•° *a* æ¥å·¥ä½œã€‚PReLUå…è®¸éè´Ÿå€¼ã€‚'
- en: '`![](../images/00016.gif)x = nn.PReLU(x)`'
  id: totrans-561
  prefs: []
  stylish: true
  type: TYPE_IMG
  zh: '`![](../images/00016.gif)x = nn.PReLU(x)`'
- en: '`Sigmoid`'
  id: totrans-562
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '`Sigmoid`'
- en: TheÂ  **sigmoid activation functionÂ **caps output to a number between 0 and 1
    and is mainly used for binary classification tasks. Sigmoid is used where the
    classes are non-exclusive. For example, an image can have a car, a building, a
    tree, etc. Just because there is a car in the image doesnâ€™t mean a tree canâ€™t
    be in the picture. Use the sigmoid function when there is more than one correct
    answer.
  id: totrans-563
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '**Sigmoidæ¿€æ´»å‡½æ•°**å°†è¾“å‡ºé™åˆ¶åœ¨0åˆ°1ä¹‹é—´ï¼Œä¸»è¦ç”¨äºäºŒå…ƒåˆ†ç±»ä»»åŠ¡ã€‚å½“ç±»åˆ«ä¸æ˜¯äº’æ–¥çš„æ—¶å€™ä½¿ç”¨sigmoidã€‚ä¾‹å¦‚ï¼Œä¸€å¼ å›¾ç‰‡å¯èƒ½åŒæ—¶æœ‰æ±½è½¦ã€å»ºç­‘å’Œæ ‘ç­‰å¤šç§ç‰©ä½“ã€‚ä½¿ç”¨sigmoidå‡½æ•°æ¥å¤„ç†è¿™ç§æƒ…å†µã€‚'
- en: x = `nn.sigmoid(x)`
  id: totrans-564
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.sigmoid(x)`
- en: '**Log sigmoid**'
  id: totrans-565
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**Log sigmoid**'
- en: '**Log sigmoid**Â computes the log of the sigmoid activation, and its output
    is within the range of âˆ’âˆ to 0.![](../images/00017.gif)x = `nn.log_sigmoid(x)`'
  id: totrans-566
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '**Log sigmoid**è®¡ç®—sigmoidæ¿€æ´»çš„å¯¹æ•°ï¼Œå…¶è¾“å‡ºåœ¨èŒƒå›´âˆ’âˆåˆ°0ä¹‹é—´ã€‚![](../images/00017.gif)x = `nn.log_sigmoid(x)`'
- en: '**Softmax**'
  id: totrans-567
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**Softmax**'
- en: TheÂ  **softmax activation function**Â is a variant of the sigmoid function used
    in multi-class problems where labels are mutually exclusive. For example, a picture
    is either grayscale or color. Use the softmax activation when there is only one
    correct answer.
  id: totrans-568
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '**Softmaxæ¿€æ´»å‡½æ•°**æ˜¯sigmoidå‡½æ•°çš„ä¸€ç§å˜ä½“ï¼Œç”¨äºå¤šç±»é—®é¢˜ï¼Œå…¶ä¸­æ ‡ç­¾æ˜¯ç›¸äº’æ’æ–¥çš„ã€‚ä¾‹å¦‚ï¼Œä¸€å¼ å›¾ç‰‡åªèƒ½æ˜¯ç°åº¦æˆ–è€…å½©è‰²ã€‚å½“åªæœ‰ä¸€ä¸ªæ­£ç¡®ç­”æ¡ˆæ—¶ä½¿ç”¨softmaxæ¿€æ´»ã€‚'
- en: x = `nn.softmax(x)`
  id: totrans-569
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.softmax(x)`
- en: '**Log softmax**'
  id: totrans-570
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**Log softmax**'
- en: '**Log softmaxÂ **computes the logarithm of the softmax function, which rescales
    elements to the range âˆ’âˆ to 0.![](../images/00018.gif)x = `nn.log_softmax(x)`'
  id: totrans-571
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '**Log softmax**è®¡ç®—softmaxå‡½æ•°çš„å¯¹æ•°ï¼Œå°†å…ƒç´ é‡æ–°ç¼©æ”¾åˆ°èŒƒå›´âˆ’âˆåˆ°0ä¹‹é—´ã€‚![](../images/00018.gif)x =
    `nn.log_softmax(x)`'
- en: '**ELU â€“ Exponential linear unit activation**'
  id: totrans-572
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**ELU â€“ æŒ‡æ•°çº¿æ€§å•å…ƒæ¿€æ´»**'
- en: '**ELU activation** Â function helps in solving the vanishing and exploding gradients
    problem. Unlike ReLu, ELU allows negative numbers pushing the mean unit activations
    closer to zero. ELUs may lead to faster training and better generalization in
    networks with more than five layers.'
  id: totrans-573
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '**ELUæ¿€æ´»å‡½æ•°**æœ‰åŠ©äºè§£å†³æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸é—®é¢˜ã€‚ä¸ReLUä¸åŒï¼ŒELUå…è®¸è´Ÿæ•°ï¼Œä»è€Œå°†å•ä½å‡å€¼æ¿€æ´»æ¨å‘é›¶é™„è¿‘ã€‚ELUå¯èƒ½å¯¼è‡´è®­ç»ƒé€Ÿåº¦æ›´å¿«ï¼Œä»¥åŠåœ¨å¤šå±‚ç½‘ç»œä¸­æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚'
- en: For values above zero the number is returned as is but for numbers below zeros
    they are a number that is less that but close to zero.![](../images/00019.gif)x
    = `nn.elu(x)`
  id: totrans-574
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å¯¹äºå¤§äºé›¶çš„å€¼ï¼Œè¿”å›åŸæ•°å€¼ï¼Œä½†å¯¹äºå°äºé›¶çš„æ•°å€¼ï¼Œå®ƒä»¬ä¼šå˜æˆæ¥è¿‘é›¶ä½†ç¨å°çš„æ•°ã€‚![](../images/00019.gif)x = `nn.elu(x)`
- en: '**CELU â€“ Continuouslydifferentiable exponential linear unit**'
  id: totrans-575
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**CELU â€“ è¿ç»­å¯å¾®çš„æŒ‡æ•°çº¿æ€§å•å…ƒ**'
- en: '`CELU`Â is ELU that is continuously differentiable.![](../images/00020.gif)x
    = `nn.celu(x)`'
  id: totrans-576
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`CELU`æ˜¯è¿ç»­å¯å¾®çš„ELUæ¿€æ´»å‡½æ•°çš„å˜ç§ã€‚![](../images/00020.gif)x = `nn.celu(x)`'
- en: '**GELUâ€“ Gaussian error linear unit activation**'
  id: totrans-577
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**GELU â€“ é«˜æ–¯è¯¯å·®çº¿æ€§å•å…ƒæ¿€æ´»**'
- en: '**GELU**Â non-linearity weights inputs by their value rather than gates inputs
    by their sign as in ReLUâ€“Â Source.![](../images/00021.gif)x = `nn.gelu(x)` ![](../images/00022.jpeg)'
  id: totrans-578
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '-   **GELU** éçº¿æ€§æŒ‰å…¶å€¼åŠ æƒè¾“å…¥ï¼Œè€Œä¸æ˜¯åƒ ReLU é‚£æ ·æŒ‰å…¶ç¬¦å·é—¨æ§è¾“å…¥â€“ æ¥æº.![](../images/00021.gif)x
    = `nn.gelu(x)` ![](../images/00022.jpeg)'
- en: '**GLU â€“ Gated linear unit activation**'
  id: totrans-579
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '-   **GLU â€“ é—¨æ§çº¿æ€§å•å…ƒæ¿€æ´»**'
- en: '**GLU** Â is computed asÂ [`GLU ( a , b )= a âŠ— Ïƒ ( b )`]. It has been applied
    inÂ Gated CNNsÂ for natural language processing. In the formula, the b gate controls
    what information is passed to the next layer. GLU helps tackle the vanishing gradient
    problem.'
  id: totrans-580
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '-   **GLU** æ˜¯è®¡ç®—ä¸º [`GLU ( a , b )= a âŠ— Ïƒ ( b )`]. å®ƒå·²è¢«åº”ç”¨äºç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†çš„é—¨æ§ CNNsã€‚åœ¨å…¬å¼ä¸­ï¼Œb
    é—¨æ§åˆ¶ç€ä¼ é€’åˆ°ä¸‹ä¸€å±‚çš„ä¿¡æ¯ã€‚GLU æœ‰åŠ©äºè§£å†³æ¶ˆå¤±æ¢¯åº¦é—®é¢˜ã€‚'
- en: x = `nn.glu(x)`
  id: totrans-581
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.glu(x)`
- en: '**Soft sign**'
  id: totrans-582
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '-   **Soft sign**'
- en: TheÂ  **Soft sign**Â activation function caps values between -1 and 1\. It is
    similar to the hyperbolic tangent activation functionâ€“ tanh. The difference is
    that tanh converges exponentially while Â Soft sign converges polynomially.
  id: totrans-583
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '**Soft sign** æ¿€æ´»å‡½æ•°å°†å€¼é™åˆ¶åœ¨ -1 åˆ° 1 ä¹‹é—´ã€‚å®ƒç±»ä¼¼äºåŒæ›²æ­£åˆ‡æ¿€æ´»å‡½æ•°â€“ tanhã€‚ä¸åŒä¹‹å¤„åœ¨äº tanh æŒ‡æ•°çº§æ”¶æ•›ï¼Œè€Œ Soft
    sign å¤šé¡¹å¼çº§æ”¶æ•›ã€‚'
- en: x = `nn.soft_sign(x)` ![](../images/00023.gif)
  id: totrans-584
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.soft_sign(x)` ![](../images/00023.gif)
- en: '**Softplus**'
  id: totrans-585
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '-   **Softplus**'
- en: TheÂ **Softplus activation**Â returns values as zero and above. It is a smooth
    version of the ReLu. x = `nn.soft_plus(x)` ![](../images/00024.gif)![](../images/00025.jpeg)**The
    Softplus activation ** **## Swishâ€“SigmoidÂ Linear Unit(Â SiLU)**
  id: totrans-586
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '-   **Softplus æ¿€æ´»** å°†è¿”å›å€¼ä¸ºé›¶åŠä»¥ä¸Šã€‚å®ƒæ˜¯ ReLu çš„å¹³æ»‘ç‰ˆæœ¬ã€‚x = `nn.soft_plus(x)` ![](../images/00024.gif)![](../images/00025.jpeg)**The
    Softplus activation ** **## Swishâ€“SigmoidÂ Linear Unit(Â SiLU)**'
- en: TheÂ SiLU activation functionÂ is computed asÂ [`x * sigmoid(beta * x)`]Â where
    beta is the hyperparameter for Swish activation function. SiLU, is, therefore,
    computed by multiplying the sigmoid function with its input.
  id: totrans-587
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: SiLU æ¿€æ´»å‡½æ•°è®¡ç®—ä¸º [`x * sigmoid(beta * x)`]ï¼Œå…¶ä¸­ beta æ˜¯ Swish æ¿€æ´»å‡½æ•°çš„è¶…å‚æ•°ã€‚å› æ­¤ï¼ŒSiLU æ˜¯é€šè¿‡å°†å…¶è¾“å…¥ä¸
    sigmoid å‡½æ•°ç›¸ä¹˜æ¥è®¡ç®—çš„ã€‚
- en: x = `nn.swish(x)`![](../images/00026.gif)
  id: totrans-588
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.swish(x)`![](../images/00026.gif)
- en: '**Custom activation functions in JAX and Flax**'
  id: totrans-589
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '-   **åœ¨ JAX å’Œ Flax ä¸­çš„è‡ªå®šä¹‰æ¿€æ´»å‡½æ•°**'
- en: 'You can also define custom activation functions in JAX. For example, here''s
    how you''d define the LeakyReLu activation function.from flax import linen as
    nnimport jax.numpy as jnpclass `LeakyReLU`(nn.Module):`alpha` : float = 0.1def
    `__call__`(self, `x`):return jnp.where(`x` > 0, `x`, `self.alpha` * `x`)'
  id: totrans-590
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'æ‚¨è¿˜å¯ä»¥åœ¨ JAX ä¸­å®šä¹‰è‡ªå®šä¹‰æ¿€æ´»å‡½æ•°ã€‚ä¾‹å¦‚ï¼Œä»¥ä¸‹æ˜¯å¦‚ä½•å®šä¹‰ LeakyReLU æ¿€æ´»å‡½æ•°ã€‚from flax import linen as nnimport
    jax.numpy as jnpclass `LeakyReLU`(nn.Module):`alpha` : float = 0.1def `__call__`(self,
    `x`):return jnp.where(`x` > 0, `x`, `self.alpha` * `x`)'
- en: '**Final thoughts**'
  id: totrans-591
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '-   **æœ€ç»ˆæ€è€ƒ**'
- en: You have learned about the various activation functions you can use in JAX and
    Flax. You have also seen that you can create new functions by creating a class
    that implements the`[__call__]`method.
  id: totrans-592
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ‚¨å·²ç»äº†è§£äº†å¯ä»¥åœ¨ JAX å’Œ Flax ä¸­ä½¿ç”¨çš„å„ç§æ¿€æ´»å‡½æ•°ã€‚æ‚¨è¿˜çœ‹åˆ°ï¼Œå¯ä»¥é€šè¿‡åˆ›å»ºä¸€ä¸ªå®ç°`[__call__]`æ–¹æ³•çš„ç±»æ¥åˆ›å»ºæ–°å‡½æ•°ã€‚
- en: How to load datasets in`JAX`with`TensorFlow`
  id: totrans-593
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '-   å¦‚ä½•åœ¨`JAX`ä¸­åŠ è½½æ•°æ®é›†ä¸`TensorFlow`'
- en: '`JAX`doesn''t ship with data loading utilities. This keeps`JAX`focused on providing
    a fast tool for building and training machine learning models. Loading data in`JAX`is
    done using'
  id: totrans-594
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`JAX` ä¸æä¾›æ•°æ®åŠ è½½å®ç”¨ç¨‹åºã€‚è¿™ä½¿`JAX`ä¸“æ³¨äºæä¾›ä¸€ä¸ªå¿«é€Ÿæ„å»ºå’Œè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹çš„å·¥å…·ã€‚åœ¨`JAX`ä¸­åŠ è½½æ•°æ®æ˜¯ä½¿ç”¨'
- en: either`TensorFlow`or`PyTorch`. This article will focus on how to load datasets
    in`JAX`using`TensorFlow`.
  id: totrans-595
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¦ä¹ˆ`TensorFlow`è¦ä¹ˆ`PyTorch`ã€‚æœ¬æ–‡å°†é‡ç‚¹ä»‹ç»å¦‚ä½•ä½¿ç”¨`TensorFlow`åœ¨`JAX`ä¸­åŠ è½½æ•°æ®é›†ã€‚
- en: Let's dive in!
  id: totrans-596
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '-   è®©æˆ‘ä»¬æ·±å…¥æ¢è®¨ï¼'
- en: How to load text data in`JAX`
  id: totrans-597
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: å¦‚ä½•åœ¨`JAX`ä¸­åŠ è½½æ–‡æœ¬æ•°æ®
- en: Let's use the`IMDB dataset from Kaggle`to illustrate how to load text datasets
    with`JAX`. We'll use the`Kaggle`Python`library to download the data. That requires
    your Kaggle username and key. Head over
  id: totrans-598
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä½¿ç”¨`Kaggle`çš„`IMDB æ•°æ®é›†`æ¥è¯´æ˜å¦‚ä½•åœ¨`JAX`ä¸­åŠ è½½æ–‡æœ¬æ•°æ®é›†ã€‚æˆ‘ä»¬å°†ä½¿ç”¨`Kaggle`Python`åº“ä¸‹è½½æ•°æ®ã€‚è¿™éœ€è¦æ‚¨çš„
    Kaggle ç”¨æˆ·åå’Œå¯†é’¥ã€‚å‰å¾€
- en: '`https://www.kaggle.com/your_username/ account to obtain the API`'
  id: totrans-599
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`https://www.kaggle.com/your_username/ account to obtain the API`'
- en: key.
  id: totrans-600
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å…³é”®ã€‚
- en: The library downloads the data as a zip file. We'll therefore extract it afterward.
  id: totrans-601
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¯¥åº“å°†æ•°æ®ä¸‹è½½ä¸º zip æ–‡ä»¶ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†åœ¨ä¹‹åæå–å®ƒã€‚
- en: import`os`
  id: totrans-602
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: import`os`
- en: Obtain from`https://www.kaggle.com/username/account os.environ["KAGGLE_USERNAME"]="YOUR_KAGGLE_USERNAME"
    os.environ["KAGGLE_KEY"]="YOUR_KAGGLE_KEY"`
  id: totrans-603
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: Obtain from`https://www.kaggle.com/username/account os.environ["KAGGLE_USERNAME"]="YOUR_KAGGLE_USERNAME"
    os.environ["KAGGLE_KEY"]="YOUR_KAGGLE_KEY"`
- en: import`kaggle`
  id: totrans-604
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: import`kaggle`
- en: '!`kaggle datasets download lakshmi25npathi/imdb-dataset-of-50k-m ovie-reviews`'
  id: totrans-605
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '!`kaggle datasets download lakshmi25npathi/imdb-dataset-of-50k-movie-reviews`'
- en: import`zipfile`
  id: totrans-606
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: import`zipfile`
- en: with zipfile.ZipFile('imdb-dataset-of-50k-movie-reviews.zip',
  id: totrans-607
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: with zipfile.ZipFile('imdb-dataset-of-50k-movie-reviews.zip',
- en: '''r'') as zip_ref:zip_ref.extractall(''imdb-dataset-of-50k-movie-reviews'')Next,
    import the standard data science packages and view a sample of the data.'
  id: totrans-608
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '''r'') as zip_ref:zip_ref.extractall(''imdb-dataset-of-50k-movie-reviews'')æ¥ä¸‹æ¥ï¼Œå¯¼å…¥æ ‡å‡†çš„æ•°æ®ç§‘å­¦åŒ…ï¼Œå¹¶æŸ¥çœ‹æ•°æ®çš„æ ·æœ¬ã€‚'
- en: import`numpy as np`
  id: totrans-609
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import`numpy as np`'
- en: import`pandas`as`pd`
  id: totrans-610
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import`pandas`as`pd`'
- en: from`numpy`import`array`
  id: totrans-611
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from`numpy`import`array`'
- en: import`tensorflow as tf`
  id: totrans-612
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import`tensorflow as tf`'
- en: from`sklearn.model_selection`import`train_test_split`
  id: totrans-613
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from`sklearn.model_selection`import`train_test_split`'
- en: from`sklearn.preprocessing`import`LabelEncoder`
  id: totrans-614
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from`sklearn.preprocessing`import`LabelEncoder`'
- en: import`matplotlib.pyplot`as`plt`
  id: totrans-615
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import`matplotlib.pyplot`as`plt`'
- en: '`df = pd.read_csv("imdb-dataset-of-50k-movie-reviews/IMDB Dataset.c df.head()`'
  id: totrans-616
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`df = pd.read_csv("imdb-dataset-of-50k-movie-reviews/IMDB Dataset.c df.head()`'
- en: '!`[](../images/00027.jpeg)`'
  id: totrans-617
  prefs: []
  stylish: true
  type: TYPE_IMG
  zh: '!`[](../images/00027.jpeg)`'
- en: Clean the text data
  id: totrans-618
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: æ¸…ç†æ–‡æœ¬æ•°æ®
- en: Let's do some processing of the data before we proceed to load it using`TensorFlow`.
    Standard processing in text problems is to remove stop words. Stop words are common
    words such as`a`,`the`that don't help the model in identifying the polarity of
    a
  id: totrans-619
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨ä½¿ç”¨`TensorFlow`åŠ è½½æ•°æ®ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å¯¹æ•°æ®è¿›è¡Œä¸€äº›å¤„ç†ã€‚æ–‡æœ¬é—®é¢˜çš„æ ‡å‡†å¤„ç†æ˜¯å»é™¤åœç”¨è¯ã€‚åœç”¨è¯æ˜¯å¦‚`a`ã€`the`ç­‰å¸¸è§è¯æ±‡ï¼Œå®ƒä»¬å¯¹æ¨¡å‹è¯†åˆ«æƒ…æ„Ÿææ€§æ²¡æœ‰å¸®åŠ©ã€‚
- en: sentence.`NLTK`provides the stops words. We can, therefore, write a function
    to remove them from the IMDB dataset.
  id: totrans-620
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å¥å­ã€‚`NLTK`æä¾›äº†åœç”¨è¯ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ç¼–å†™ä¸€ä¸ªå‡½æ•°ä» IMDB æ•°æ®é›†ä¸­å»é™¤å®ƒä»¬ã€‚
- en: import`nltk`
  id: totrans-621
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import`nltk`'
- en: from`nltk.corpus`import`stopwords`
  id: totrans-622
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from`nltk.corpus`import`stopwords`'
- en: '`nltk.download(''stopwords'')`'
  id: totrans-623
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`nltk.download(''stopwords'')`'
- en: '`def remove_stop_words(review):`'
  id: totrans-624
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def remove_stop_words(review):`'
- en: '`review_minus_sw = []`'
  id: totrans-625
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`review_minus_sw = []`'
- en: '`stop_words = stopwords.words(''english'')`'
  id: totrans-626
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`stop_words = stopwords.words(''english'')`'
- en: '`review = review.split()`'
  id: totrans-627
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`review = review.split()`'
- en: '`cleaned_review = [review_minus_sw.append(word) for word in`'
  id: totrans-628
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[cleaned_review = [review_minus_sw.append(word) for word in`'
- en: '`review if word not in stop_words]`'
  id: totrans-629
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`review if word not in stop_words]`'
- en: '`cleaned_review = '' ''.join(review_minus_sw)`'
  id: totrans-630
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`cleaned_review = '' ''.join(review_minus_sw)`'
- en: '`return cleaned_review`'
  id: totrans-631
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return cleaned_review`'
- en: '`df[''review''] = df[''review''].apply(remove_stop_words) view raw`'
  id: totrans-632
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`df[''review''] = df[''review''].apply(remove_stop_words) view raw`'
- en: Label encode the sentiment column
  id: totrans-633
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: å¯¹æƒ…æ„Ÿåˆ—è¿›è¡Œæ ‡ç­¾ç¼–ç 
- en: Convert the sentiment column to numerical representation using`Scikit-learn's
    label encoder. This is important because neural networks expect numerical data.
  id: totrans-634
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`Scikit-learn`çš„æ ‡ç­¾ç¼–ç å°†æƒ…æ„Ÿåˆ—è½¬æ¢ä¸ºæ•°å€¼è¡¨ç¤ºã€‚è¿™å¾ˆé‡è¦ï¼Œå› ä¸ºç¥ç»ç½‘ç»œæœŸæœ›æ•°å€¼æ•°æ®ã€‚
- en: '`labelencoder = LabelEncoder()`'
  id: totrans-635
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`labelencoder = LabelEncoder()`'
- en: '`df = df.assign(sentiment = labelencoder.fit_transform(df["senti ment"]))`'
  id: totrans-636
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`df = df.assign(sentiment = labelencoder.fit_transform(df["senti ment"]))`'
- en: '!`[](../images/00028.jpeg)`'
  id: totrans-637
  prefs: []
  stylish: true
  type: TYPE_IMG
  zh: '!`[](../images/00028.jpeg)`'
- en: Text preprocessing with`TensorFlow`
  id: totrans-638
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`TensorFlow`è¿›è¡Œæ–‡æœ¬é¢„å¤„ç†
- en: We have converted the sentiment column to a numerical
  id: totrans-639
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²å°†æƒ…æ„Ÿåˆ—è½¬æ¢ä¸ºæ•°å­—
- en: representation. However, the reviews are still in text form. We need to convert
    them to numbers as well.
  id: totrans-640
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¡¨ç¤ºã€‚ç„¶è€Œï¼Œè¯„è®ºä»ç„¶æ˜¯æ–‡æœ¬å½¢å¼ã€‚æˆ‘ä»¬ä¹Ÿéœ€è¦å°†å®ƒä»¬è½¬æ¢ä¸ºæ•°å­—ã€‚
- en: We start by splitting the dataset into a training and testing set.
  id: totrans-641
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆå°†æ•°æ®é›†åˆ†å‰²ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚
- en: from`sklearn.model_selection`import`train_test_split df = df.drop_duplicates()`
  id: totrans-642
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from`sklearn.model_selection`import`train_test_split df = df.drop_duplicates()`'
- en: '`docs = df[''review'']`'
  id: totrans-643
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`docs = df[''review'']`'
- en: '`labels = array(df[''sentiment''])`'
  id: totrans-644
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`labels = array(df[''sentiment''])`'
- en: '`X_train, X_test , y_train, y_test = train_test_split(docs, labe ls , test_size
    = 0.20, random_state=0)`'
  id: totrans-645
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`X_train, X_test , y_train, y_test = train_test_split(docs, labe ls , test_size
    = 0.20, random_state=0)`'
- en: 'Next, we use TensorFlow''s`TextVectorization`function to convert the text data
    to integer representations. The function expects:'
  id: totrans-646
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä½¿ç”¨ TensorFlow çš„`TextVectorization`å‡½æ•°å°†æ–‡æœ¬æ•°æ®è½¬æ¢ä¸ºæ•´æ•°è¡¨ç¤ºã€‚è¯¥å‡½æ•°æœŸæœ›ï¼š
- en: '`[standardize]`used to specify how the text data is processed. For example,
    the`[lower_and_strip_punctuation]`option will lowercase the data and remove punctuations.'
  id: totrans-647
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[standardize]`ç”¨äºæŒ‡å®šå¦‚ä½•å¤„ç†æ–‡æœ¬æ•°æ®ã€‚ä¾‹å¦‚ï¼Œé€‰é¡¹`[lower_and_strip_punctuation]`å°†æ–‡æœ¬è½¬æ¢ä¸ºå°å†™å¹¶åˆ é™¤æ ‡ç‚¹ç¬¦å·ã€‚'
- en: '`max_tokens`dictates the maximum size of the vocabulary. `[output_mode]`determines
    the output of the vectorization layer. Setting`[int]`outputs integers.'
  id: totrans-648
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`max_tokens`å†³å®šè¯æ±‡è¡¨çš„æœ€å¤§å¤§å°ã€‚`[output_mode]`ç¡®å®šå‘é‡åŒ–å±‚çš„è¾“å‡ºã€‚è®¾ç½®`[int]`å°†è¾“å‡ºæ•´æ•°ã€‚'
- en: '`[output_sequence_length]`indicates the maximum length of the output sequence.
    This ensures that all sequences have the same length.'
  id: totrans-649
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[output_sequence_length]`æŒ‡ç¤ºè¾“å‡ºåºåˆ—çš„æœ€å¤§é•¿åº¦ã€‚è¿™ç¡®ä¿æ‰€æœ‰åºåˆ—å…·æœ‰ç›¸åŒçš„é•¿åº¦ã€‚'
- en: import`tensorflow as tf`
  id: totrans-650
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import`tensorflow as tf`'
- en: '`max_features = 5000`# Maximum vocab size.'
  id: totrans-651
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`max_features = 5000`# æœ€å¤§è¯æ±‡é‡å¤§å°ã€‚'
- en: '`batch_size = 32`'
  id: totrans-652
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`batch_size = 32`'
- en: '`max_len = 512`# Sequence length to pad the outputs to. `vectorize_layer =
    tf.keras.layers.TextVectorization(standardize =''lower_and_strip_punctuation'',max_tokens=max_features,output_m
    ode=''int'',output_sequence_length=max_len)`'
  id: totrans-653
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`max_len = 512`# å°†è¾“å‡ºå¡«å……åˆ°çš„åºåˆ—é•¿åº¦ã€‚`vectorize_layer = tf.keras.layers.TextVectorization(standardize
    =''lower_and_strip_punctuation'',max_tokens=max_features,output_m ode=''int'',output_sequence_length=max_len)`'
- en: '`vectorize_layer.adapt(X_train, batch_size=None)`'
  id: totrans-654
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`vectorize_layer.adapt(X_train, batch_size=None)`'
- en: Next, apply this layer to the training and testing data.`X_train_padded = vectorize_layer(X_train)
    X_test_padded = vectorize_layer(X_test)![](../images/00029.jpeg)`
  id: totrans-655
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œå°†æ­¤å±‚åº”ç”¨äºè®­ç»ƒå’Œæµ‹è¯•æ•°æ®ã€‚`X_train_padded = vectorize_layer(X_train) X_test_padded =
    vectorize_layer(X_test)![](../images/00029.jpeg)`
- en: Convert the data to a`TensorFlow dataset and create a function to fetch the
    data in batches. We also convert the data to NumPy arrays because JAX expects
    NumPy or JAX arrays.
  id: totrans-656
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å°†æ•°æ®è½¬æ¢ä¸º`TensorFlowæ•°æ®é›†ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥æ‰¹é‡è·å–æ•°æ®ã€‚æˆ‘ä»¬è¿˜å°†æ•°æ®è½¬æ¢ä¸ºNumPyæ•°ç»„ï¼Œå› ä¸ºJAXæœŸæœ›ä½¿ç”¨NumPyæˆ–JAXæ•°ç»„ã€‚
- en: import`tensorflow_datasets`as`tfds`
  id: totrans-657
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: import`tensorflow_datasets`as`tfds`
- en: '`training_data = tf.data.Dataset.from_tensor_slices((X_train_padded, y_train))`'
  id: totrans-658
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`training_data = tf.data.Dataset.from_tensor_slices((X_train_padded, y_train))`'
- en: '`validation_data = tf.data.Dataset.from_tensor_slices((X_test_padded, y_test))`'
  id: totrans-659
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`validation_data = tf.data.Dataset.from_tensor_slices((X_test_padded, y_test))`'
- en: '`training_data = training_data.batch(batch_size)`'
  id: totrans-660
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`training_data = training_data.batch(batch_size)`'
- en: '`validation_data = validation_data.batch(batch_size)`'
  id: totrans-661
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`validation_data = validation_data.batch(batch_size)`'
- en: '`def get_train_batches():`'
  id: totrans-662
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def get_train_batches():`'
- en: '`ds = training_data.prefetch(1)`'
  id: totrans-663
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`ds = training_data.prefetch(1)`'
- en: '`tfds.dataset_as_numpy`converts the`tf.data.Dataset`into an'
  id: totrans-664
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`tfds.dataset_as_numpy`å°†`tf.data.Dataset`è½¬æ¢ä¸º'
- en: iterable of NumPy arrays`return tfds.as_numpy(ds)`The data is now in the right
    format and to be passed to a Flax network.
  id: totrans-665
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å¯è¿­ä»£çš„NumPyæ•°ç»„`return tfds.as_numpy(ds)`ç°åœ¨æ•°æ®æ ¼å¼æ­£ç¡®ï¼Œå¹¶å°†ä¼ é€’ç»™Flaxç½‘ç»œã€‚
- en: Let's quickly walk through the rest of the steps required to train neural networks
    in Flax using this data.
  id: totrans-666
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å¿«é€Ÿæµè§ˆä¸€ä¸‹ä½¿ç”¨è¿™äº›æ•°æ®åœ¨Flaxä¸­è®­ç»ƒç¥ç»ç½‘ç»œæ‰€éœ€çš„å…¶ä½™æ­¥éª¤ã€‚
- en: First, create a simple neural network in Flax.
  id: totrans-667
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œåœ¨Flaxä¸­åˆ›å»ºä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œã€‚
- en: pip install`flax`
  id: totrans-668
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: pip install`flax`
- en: import`flax`
  id: totrans-669
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: import`flax`
- en: from`flax`import`linen as nn`
  id: totrans-670
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: from`flax`import`linen as nn`
- en: '`class Model(nn.Module):`'
  id: totrans-671
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`class Model(nn.Module):`'
- en: '@nn.compact'
  id: totrans-672
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '@nn.compact'
- en: '`def __call__(self, x): x = nn.Dense(features=256)(x) x = nn.relu(x)`'
  id: totrans-673
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def __call__(self, x): x = nn.Dense(features=256)(x) x = nn.relu(x)`'
- en: '`x = nn.Dense(features=2)(x) x = nn.log_softmax(x)`'
  id: totrans-674
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = nn.Dense(features=2)(x) x = nn.log_softmax(x)`'
- en: return`x`
  id: totrans-675
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: return`x`
- en: Define a function to compute the loss.
  id: totrans-676
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å®šä¹‰ä¸€ä¸ªè®¡ç®—æŸå¤±çš„å‡½æ•°ã€‚
- en: import`optax`
  id: totrans-677
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: import`optax`
- en: import`jax.numpy`as`jnp`
  id: totrans-678
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: import`jax.numpy`as`jnp`
- en: '`def cross_entropy_loss(*, logits, labels):`'
  id: totrans-679
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def cross_entropy_loss(*, logits, labels):`'
- en: '`labels_onehot = jax.nn.one_hot(labels, num_classes=2) return optax.softmax_cross_entropy(logits=logits,
    labels=labe ls_onehot).mean()`Next, define the function to compute the network
    metrics.'
  id: totrans-680
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`labels_onehot = jax.nn.one_hot(labels, num_classes=2) return optax.softmax_cross_entropy(logits=logits,
    labels=labe ls_onehot).mean()`æ¥ä¸‹æ¥ï¼Œå®šä¹‰è®¡ç®—ç½‘ç»œæŒ‡æ ‡çš„å‡½æ•°ã€‚'
- en: '`def compute_metrics(*, logits, labels):`'
  id: totrans-681
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def compute_metrics(*, logits, labels):`'
- en: '`loss = cross_entropy_loss(logits=logits, labels=labels)` `accuracy = jnp.mean(jnp.argmax(logits,
    -1) == labels)` `metrics = {`'
  id: totrans-682
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`loss = cross_entropy_loss(logits=logits, labels=labels)` `accuracy = jnp.mean(jnp.argmax(logits,
    -1) == labels)` `metrics = {`'
- en: '`''loss'': loss,`'
  id: totrans-683
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`''loss'': loss,`'
- en: '`''accuracy'': accuracy`,'
  id: totrans-684
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`''accuracy'': accuracy`,'
- en: '}'
  id: totrans-685
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '}'
- en: return `metrics`
  id: totrans-686
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: return `metrics`
- en: The training state is used to track the network training. It tracks the optimizer
    and model parameters and can be modified to track other things such as dropout
    and batch normalization statistics.
  id: totrans-687
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è®­ç»ƒçŠ¶æ€ç”¨äºè·Ÿè¸ªç½‘ç»œè®­ç»ƒã€‚å®ƒè·Ÿè¸ªä¼˜åŒ–å™¨å’Œæ¨¡å‹å‚æ•°ï¼Œå¹¶å¯ä»¥ä¿®æ”¹ä»¥è·Ÿè¸ªå…¶ä»–å†…å®¹ï¼Œä¾‹å¦‚dropoutå’Œæ‰¹å½’ä¸€åŒ–ç»Ÿè®¡ä¿¡æ¯ã€‚
- en: from `flax.training import train_state` def `create_train_state(rng, learning_rate,
    momentum):` """Creates initial `TrainState`."""
  id: totrans-688
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: from `flax.training import train_state` def `create_train_state(rng, learning_rate,
    momentum):` """åˆ›å»ºåˆå§‹`TrainState`ã€‚"""
- en: '`model = Model()`'
  id: totrans-689
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`model = Model()`'
- en: '`params = model.init(rng, X_train_padded[0])[''params'']` `tx = optax.sgd(learning_rate,
    momentum)`'
  id: totrans-690
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`params = model.init(rng, X_train_padded[0])[''params'']` `tx = optax.sgd(learning_rate,
    momentum)`'
- en: return `train_state.TrainState.create(
  id: totrans-691
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: return `train_state.TrainState.create(
- en: '`apply_fn=model.apply, params=params, tx=tx)`In the training step, weÂ [Apply]Â the
    model to obtain the loss. This is then used to compute the gradients that update
    the model parameters.'
  id: totrans-692
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`apply_fn=model.apply, params=params, tx=tx)`åœ¨è®­ç»ƒæ­¥éª¤ä¸­ï¼Œæˆ‘ä»¬[åº”ç”¨]æ¨¡å‹ä»¥è·å–æŸå¤±ã€‚ç„¶åç”¨è¿™ä¸ªæŸå¤±æ¥è®¡ç®—æ›´æ–°æ¨¡å‹å‚æ•°çš„æ¢¯åº¦ã€‚'
- en: '`def compute_loss(params,text,labels):`'
  id: totrans-693
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def compute_loss(params,text,labels):`'
- en: '`logits = Model().apply({''params'': params}, text)` `loss = cross_entropy_loss(logits=logits,
    labels=labels)` return `loss, logits`'
  id: totrans-694
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`logits = Model().apply({''params'': params}, text)` `loss = cross_entropy_loss(logits=logits,
    labels=labels)` return `loss, logits`'
- en: '@jax.jit'
  id: totrans-695
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '@jax.jit'
- en: '`def train_step(state,text, labels):`'
  id: totrans-696
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def train_step(state,text, labels):`'
- en: '"""Train for a single step."""'
  id: totrans-697
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""å¯¹å•ä¸ªæ­¥éª¤è¿›è¡Œè®­ç»ƒã€‚"""'
- en: '`(_, logits), grads = jax.value_and_grad(compute_loss, has_aux =True)(state.params,text,labels)`'
  id: totrans-698
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`(_, logits), grads = jax.value_and_grad(compute_loss, has_aux =True)(state.params,text,labels)`'
- en: '`state = state.apply_gradients(grads=grads)`'
  id: totrans-699
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`state = state.apply_gradients(grads=grads)`'
- en: '`metrics = compute_metrics(logits=logits, labels=labels)`'
  id: totrans-700
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`metrics = compute_metrics(logits=logits, labels=labels)`'
- en: return `state, metrics`
  id: totrans-701
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: return `state, metrics`
- en: The evaluation step applies the model to the testing data to compute the test
    metrics.
  id: totrans-702
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¯„ä¼°æ­¥éª¤å°†æ¨¡å‹åº”ç”¨äºæµ‹è¯•æ•°æ®ä»¥è®¡ç®—æµ‹è¯•æŒ‡æ ‡ã€‚
- en: '@jax.jit'
  id: totrans-703
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '@jax.jit'
- en: '`def eval_step(state, text, labels):`'
  id: totrans-704
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def eval_step(state, text, labels):`'
- en: '`logits = Model().apply({''params'': state.params}, text)`'
  id: totrans-705
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`logits = Model().apply({''params'': state.params}, text)`'
- en: return `compute_metrics(logits=logits, labels=labels)`
  id: totrans-706
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: return `compute_metrics(logits=logits, labels=labels)`
- en: The evaluation function runs the above evaluation step to obtain the evaluation
    metrics.
  id: totrans-707
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¯„ä¼°å‡½æ•°è¿è¡Œä¸Šè¿°è¯„ä¼°æ­¥éª¤ä»¥è·å–è¯„ä¼°æŒ‡æ ‡ã€‚
- en: '`def evaluate_model(state, text, test_lbls):`'
  id: totrans-708
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def evaluate_model(state, text, test_lbls):`'
- en: '"""Evaluate on the validation set."""'
  id: totrans-709
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""åœ¨éªŒè¯é›†ä¸Šè¿›è¡Œè¯„ä¼°ã€‚"""'
- en: '`metrics = eval_step(state, text, test_lbls)` `metrics = jax.device_get(metrics)`'
  id: totrans-710
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`metrics = eval_step(state, text, test_lbls)` `metrics = jax.device_get(metrics)`'
- en: '`metrics = jax.tree_map(lambda x: x.item(), metrics)` return `metrics`'
  id: totrans-711
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`metrics = jax.tree_map(lambda x: x.item(), metrics)` return `metrics`'
- en: We use theÂ `get_train_batches` function in the`train_epoch`Â method. We loop
    through the batches as we apply theÂ `[train_step]` method. We obtain the train
    metrics and return them.
  id: totrans-712
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨`train_epoch`æ–¹æ³•ä¸­ä½¿ç”¨`get_train_batches`å‡½æ•°ã€‚æˆ‘ä»¬é€šè¿‡åº”ç”¨`[train_step]`æ–¹æ³•å¾ªç¯éå†æ‰¹æ¬¡ã€‚æˆ‘ä»¬è·å–è®­ç»ƒæŒ‡æ ‡å¹¶è¿”å›å®ƒä»¬ã€‚
- en: '`def train_one_epoch(state):`'
  id: totrans-713
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def train_one_epoch(state):`'
- en: '"""Train for 1 epoch on the training set.""" `batch_metrics = []`'
  id: totrans-714
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œ1è½®è®­ç»ƒã€‚""" `batch_metrics = []`'
- en: for `text, labels in get_train_batches():`
  id: totrans-715
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: for `text, labels in get_train_batches():`
- en: '`state, metrics = train_step(state, text, labels)` `batch_metrics.append(metrics)`batch_metrics_np
    = jax.device_get(batch_metrics) epoch_metrics_np = {k: np.mean([metrics[k] for
    metrics in batch_metrics_n p])for k in batch_metrics_np[0] }return `state, epoch_metrics_np`'
  id: totrans-716
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`state, metrics = train_step(state, text, labels)` `batch_metrics.append(metrics)`batch_metrics_np
    = jax.device_get(batch_metrics) epoch_metrics_np = {k: np.mean([metrics[k] for
    metrics in batch_metrics_n p])for k in batch_metrics_np[0] }return `state, epoch_metrics_np`'
- en: The final step is to train the network on the training set and evaluate it on
    the test set. A training state is required before training the model. This is
    becauseÂ JAX expects pure functions.
  id: totrans-717
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æœ€åä¸€æ­¥æ˜¯åœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒç½‘ç»œï¼Œå¹¶åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œè¯„ä¼°ã€‚åœ¨è®­ç»ƒæ¨¡å‹ä¹‹å‰éœ€è¦ä¸€ä¸ªè®­ç»ƒçŠ¶æ€ã€‚è¿™æ˜¯å› ä¸ºJAXæœŸæœ›çº¯å‡½æ•°ã€‚
- en: '`rng = jax.random.PRNGKey(0)`ï¼Œ`rng, init_rng = jax.random.split(rng)`'
  id: totrans-718
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`rng = jax.random.PRNGKey(0)`ï¼Œ`rng, init_rng = jax.random.split(rng)`'
- en: '`learning_rate = 0.1 momentum = 0.9`'
  id: totrans-719
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`learning_rate = 0.1 momentum = 0.9`'
- en: '`seed = 0`'
  id: totrans-720
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`seed = 0`'
- en: '`state = create_train_state(init_rng, learning_rate, momentum)` del `init_rng
    # Must not be used anymore.`'
  id: totrans-721
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`state = create_train_state(init_rng, learning_rate, momentum)` del `init_rng
    # ä¸å†ä½¿ç”¨ã€‚`'
- en: '`num_epochs = 30`'
  id: totrans-722
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`num_epochs = 30`'
- en: '`(text, test_labels) = next(iter(validation_data))` `text = jnp.array(text)`'
  id: totrans-723
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`(text, test_labels) = next(iter(validation_data))` `text = jnp.array(text)`'
- en: '`test_labels = jnp.array(test_labels)`'
  id: totrans-724
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`test_labels = jnp.array(test_labels)`'
- en: '`state` = `create_train_state(jax.random.PRNGKey(seed), learning_r ate, momentum)`'
  id: totrans-725
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`state` = `create_train_state(jax.random.PRNGKey(seed), learning_r ate, momentum)`'
- en: '`training_loss` = []'
  id: totrans-726
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`training_loss` = []'
- en: '`training_accuracy` = []'
  id: totrans-727
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`training_accuracy` = []'
- en: '`testing_loss` = []'
  id: totrans-728
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_loss` = []'
- en: '`testing_accuracy` = []'
  id: totrans-729
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_accuracy` = []'
- en: '`for epoch in range(1, num_epochs + 1):`'
  id: totrans-730
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'for epoch in range(1, num_epochs + 1):'
- en: '`train_state, train_metrics = train_one_epoch(state) training_loss.append(train_metrics[''loss''])`'
  id: totrans-731
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_state, train_metrics = train_one_epoch(state) training_loss.append(train_metrics[''loss''])`'
- en: '`training_accuracy.append(train_metrics[''accuracy'']) print(f"Train epoch:
    {epoch}, loss: {train_metrics[''los`'
  id: totrans-732
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`training_accuracy.append(train_metrics[''accuracy'']) print(f"Train epoch:
    {epoch}, loss: {train_metrics[''los`'
- en: 's'']}, accuracy: {train_metrics[''accuracy''] * 100}")`'
  id: totrans-733
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 's'']}, accuracy: {train_metrics[''accuracy''] * 100}")`'
- en: '`test_metrics` = `evaluate_model(train_state, text, test_label s)`'
  id: totrans-734
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`test_metrics` = `evaluate_model(train_state, text, test_label s)`'
- en: '`testing_loss.append(test_metrics[''loss''])`'
  id: totrans-735
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_loss.append(test_metrics[''loss''])`'
- en: '`testing_accuracy.append(test_metrics[''accuracy''])`'
  id: totrans-736
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_accuracy.append(test_metrics[''accuracy''])`'
- en: '`print(f"Test epoch: {epoch}, loss: {test_metrics[''loss'']}, accuracy: {test_metrics[''accuracy'']
    * 100}")`'
  id: totrans-737
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`print(f"Test epoch: {epoch}, loss: {test_metrics[''loss'']}, accuracy: {test_metrics[''accuracy'']
    * 100}")`'
- en: '`![](../images/00030.jpeg)`'
  id: totrans-738
  prefs: []
  stylish: true
  type: TYPE_IMG
  zh: '`![](../images/00030.jpeg)`'
- en: '**How to load image data in JAX**'
  id: totrans-739
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**å¦‚ä½•åœ¨JAXä¸­åŠ è½½å›¾åƒæ•°æ®**'
- en: Let's now see how we can load image data with TensorFlow. We'll use the popularÂ cats
    and dogs images from Kaggle. We start by downloading the data.
  id: totrans-740
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•ä½¿ç”¨TensorFlowåŠ è½½å›¾åƒæ•°æ®ã€‚æˆ‘ä»¬å°†ä½¿ç”¨æ¥è‡ªKaggleçš„æµè¡Œçš„çŒ«å’Œç‹—å›¾åƒã€‚æˆ‘ä»¬é¦–å…ˆä¸‹è½½æ•°æ®ã€‚
- en: '`import wget # pip install wgetimport zipfile`'
  id: totrans-741
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import wget # pip install wgetimport zipfile`'
- en: '`wget.download("https://ml.machinelearningnuggets.com/train.zi p")`'
  id: totrans-742
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`wget.download("https://ml.machinelearningnuggets.com/train.zi p")`'
- en: '`with zipfile.ZipFile(''train.zip'', ''r'') as zip_ref:`'
  id: totrans-743
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`with zipfile.ZipFile(''train.zip'', ''r'') as zip_ref:`'
- en: '`zip_ref.extractall(''.'')Next, create aÂ Pandas DataFrameÂ containing the labels
    and paths to the images.`'
  id: totrans-744
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`zip_ref.extractall(''.'')æ¥ä¸‹æ¥ï¼Œåˆ›å»ºä¸€ä¸ªåŒ…å«æ ‡ç­¾å’Œå›¾åƒè·¯å¾„çš„`Pandas DataFrame`ã€‚'
- en: '`import pandas as pd`'
  id: totrans-745
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import pandas as pd`'
- en: '`base_dir` = `''train''`'
  id: totrans-746
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`base_dir` = `''train''`'
- en: '`filenames = os.listdir(base_dir) categories = []`'
  id: totrans-747
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`filenames = os.listdir(base_dir) categories = []`'
- en: '`for filename in filenames:`'
  id: totrans-748
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`for filename in filenames:`'
- en: '`category = filename.split(''.'')[0] if category == ''dog'':categories.append("dog")`'
  id: totrans-749
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`category = filename.split(''.'')[0] if category == ''dog'':categories.append("dog")`'
- en: '`else:`'
  id: totrans-750
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`else:`'
- en: '`categories.append("cat") df = pd.DataFrame({''filename'': filenames,''category'':
    categorie s})`'
  id: totrans-751
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`categories.append("cat") df = pd.DataFrame({''filename'': filenames,''category'':
    categorie s})`'
- en: The next step is to define anÂ `ImageDataGenerator` for scaling the images and
    performing simple augmentation.
  id: totrans-752
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€æ­¥æ˜¯ä¸ºå›¾åƒç¼©æ”¾å’Œæ‰§è¡Œç®€å•æ•°æ®å¢å¼ºå®šä¹‰ä¸€ä¸ª`ImageDataGenerator`ã€‚
- en: '`from tensorflow.keras.preprocessing.image import ImageDataGener ator`'
  id: totrans-753
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from tensorflow.keras.preprocessing.image import ImageDataGener ator`'
- en: '`train_datagen` = `ImageDataGenerator(rescale=1./255,`'
  id: totrans-754
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_datagen` = `ImageDataGenerator(rescale=1./255,`'
- en: '`shear_range=0.2, zoom_range=0.2,`'
  id: totrans-755
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`shear_range=0.2, zoom_range=0.2,`'
- en: '`horizontal_flip=True, width_shift_range=0.1, height_shift_range=0.1, validation_split=0.2
    )`'
  id: totrans-756
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`horizontal_flip=True, width_shift_range=0.1, height_shift_range=0.1, validation_split=0.2
    )`'
- en: '`validation_gen` = `ImageDataGenerator(rescale=1./255,validation_s plit=0.2)`'
  id: totrans-757
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`validation_gen` = `ImageDataGenerator(rescale=1./255,validation_s plit=0.2)`'
- en: Load the images using theÂ `[flow_from_dataframe]of these generators. This will
    match the image paths in the DataFrame to the images we downloaded.`
  id: totrans-758
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¿™äº›ç”Ÿæˆå™¨çš„`[flow_from_dataframe]`åŠ è½½å›¾åƒã€‚è¿™å°†æŠŠ DataFrame ä¸­çš„å›¾åƒè·¯å¾„ä¸æˆ‘ä»¬ä¸‹è½½çš„å›¾åƒè¿›è¡ŒåŒ¹é…ã€‚
- en: '`image_size = (128, 128)`'
  id: totrans-759
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`image_size = (128, 128)`'
- en: '`batch_size = 128`'
  id: totrans-760
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`batch_size = 128`'
- en: '`training_set` = `train_datagen.flow_from_dataframe(df,base_dir,`'
  id: totrans-761
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`training_set` = `train_datagen.flow_from_dataframe(df,base_dir,`'
- en: '`seed=101, target_size=ima ge_size,`'
  id: totrans-762
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`seed=101, target_size=ima ge_size,`'
- en: '`batch_size=batc h_size,`'
  id: totrans-763
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`batch_size=batc h_size,`'
- en: '`x_col=''filenam e''`,'
  id: totrans-764
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x_col=''filenam e''`,'
- en: '`y_col=''categor y''`'
  id: totrans-765
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`y_col=''åˆ†ç±»''`'
- en: subset = `"train ing"`
  id: totrans-766
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: subset = `"è®­ç»ƒ"`
- en: '`class_mode=''bin ary'')`'
  id: totrans-767
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`class_mode=''bin ary'')`'
- en: '`validation_set` = `validation_gen.flow_from_dataframe(df,base_di r,`'
  id: totrans-768
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`validation_set` = `validation_gen.flow_from_dataframe(df,base_di r,`'
- en: '`target_size=image _size`,'
  id: totrans-769
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`target_size=image _size`,'
- en: '`batch_size=batch_b size,`'
  id: totrans-770
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`batch_size=batch_b size,`'
- en: '`x_col=''filename''`,'
  id: totrans-771
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x_col=''æ–‡ä»¶å''`,'
- en: '`y_col=''categor y''`,'
  id: totrans-772
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`y_col=''åˆ†ç±»''`,'
- en: subset = `"validat ion"`
  id: totrans-773
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: subset = `"éªŒè¯"`
- en: '`class_mode=''binar y'')`'
  id: totrans-774
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`class_mode=''binar y'')`'
- en: Loop through the training set to confirm that a batch of images are being generated.
  id: totrans-775
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: éå†è®­ç»ƒé›†ä»¥ç¡®è®¤æ˜¯å¦ç”Ÿæˆäº†ä¸€æ‰¹å›¾åƒã€‚
- en: '`for train_images, train_labels in training_set: print(''Train:'', train_images.shape,
    train_labels.shape) break`'
  id: totrans-776
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`for train_images, train_labels in training_set: print(''Train:'', train_images.shape,
    train_labels.shape) break`'
- en: 'Train: `(128, 128, 128, 3) (128,)`'
  id: totrans-777
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: è®­ç»ƒï¼š`(128, 128, 128, 3) (128,)`
- en: The next step is to define a network and pass the data. The steps are similar
    to what we did for the text data above
  id: totrans-778
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€æ­¥æ˜¯å®šä¹‰ä¸€ä¸ªç¥ç»ç½‘ç»œå¹¶ä¼ é€’æ•°æ®ã€‚æ­¥éª¤ä¸æˆ‘ä»¬ä¸Šé¢å¯¹æ–‡æœ¬æ•°æ®æ‰€åšçš„æ­¥éª¤ç±»ä¼¼
- en: '**How to load CSV data in JAX**'
  id: totrans-779
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**å¦‚ä½•åœ¨ JAX ä¸­åŠ è½½ CSV æ•°æ®**'
- en: You can useÂ `Pandas`Â to load CSV data as we did for the text data at the beginning
    of the article. Convert the data to NumPy orÂ `JAX arrays`Â once preprocessing is
    done. Passing Torch tensors or TensorFlow tensors toÂ `JAX`Â neural networks will
    result in an error.
  id: totrans-780
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥ä½¿ç”¨`Pandas`æ¥åŠ è½½ CSV æ•°æ®ï¼Œå°±åƒæˆ‘ä»¬åœ¨æ–‡ç« å¼€å¤´çš„æ–‡æœ¬æ•°æ®é‚£æ ·ã€‚åœ¨é¢„å¤„ç†å®Œæˆåï¼Œå°†æ•°æ®è½¬æ¢ä¸º NumPy æˆ–`JAX æ•°ç»„`ã€‚å°† Torch
    å¼ é‡æˆ– TensorFlow å¼ é‡ä¼ é€’ç»™`JAX`ç¥ç»ç½‘ç»œä¼šå¯¼è‡´é”™è¯¯ã€‚
- en: '`**Final thoughts**`'
  id: totrans-781
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '`**æœ€åçš„æ€è€ƒ**`'
- en: This article shows how you can use TensorFlow to load datasets in `JAX`Â and
    `Flax`Â applications. We have walked through an example of loading text data with
    TensorFlow. After that, we discussed loading image and CSV data in `JAX`.
  id: totrans-782
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æœ¬æ–‡å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ TensorFlow åœ¨`JAX`å’Œ`Flax`åº”ç”¨ç¨‹åºä¸­åŠ è½½æ•°æ®é›†ã€‚æˆ‘ä»¬å·²ç»ä»‹ç»äº†å¦‚ä½•ä½¿ç”¨ TensorFlow åŠ è½½æ–‡æœ¬æ•°æ®çš„ç¤ºä¾‹ã€‚ä¹‹åï¼Œæˆ‘ä»¬è®¨è®ºäº†åœ¨`JAX`ä¸­åŠ è½½å›¾åƒå’Œ
    CSV æ•°æ®ã€‚
- en: '`**Image classification with JAX & Flax**`'
  id: totrans-783
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '`**ä½¿ç”¨ JAX å’Œ Flax è¿›è¡Œå›¾åƒåˆ†ç±»**`'
- en: '`Flax`Â is a neural network library for `JAX`.Â `JAX`Â is aÂ `Python`Â library that
    provides high-performance computing in machine learning research. `JAX`Â provides
    an API similar toÂ `NumPy`Â making it easy to adopt.Â `JAX`Â also includes other functionalities
    for improving machine learning research. They include:'
  id: totrans-784
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Flax`æ˜¯ä¸€ä¸ªé¢å‘`JAX`çš„ç¥ç»ç½‘ç»œåº“ã€‚`JAX`æ˜¯ä¸€ä¸ªæä¾›é«˜æ€§èƒ½è®¡ç®—çš„`Python`åº“ï¼Œç”¨äºæœºå™¨å­¦ä¹ ç ”ç©¶ã€‚`JAX`æä¾›äº†ä¸`NumPy`ç±»ä¼¼çš„
    APIï¼Œä½¿å…¶æ˜“äºé‡‡ç”¨ã€‚`JAX`è¿˜åŒ…æ‹¬å…¶ä»–ç”¨äºæ”¹è¿›æœºå™¨å­¦ä¹ ç ”ç©¶çš„åŠŸèƒ½ã€‚å…¶ä¸­åŒ…æ‹¬ï¼š'
- en: '`**Automatic differentiation**` .Â `JAX`Â supports forward and reverse automatic
    differential of numerical functions with functions such asÂ `jacrev`,Â `grad`,Â `hessian`Â andÂ `jacfwd`.'
  id: totrans-785
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '**è‡ªåŠ¨å¾®åˆ†** .Â `JAX`Â æ”¯æŒä½¿ç”¨Â `jacrev`,Â `grad`,Â `hessian`Â å’ŒÂ `jacfwd`Â ç­‰å‡½æ•°è¿›è¡Œæ•°å€¼å‡½æ•°çš„å‰å‘å’Œåå‘è‡ªåŠ¨å¾®åˆ†ã€‚'
- en: '`**Vectorization**` .Â `JAX`Â supports automatic vectorization via theÂ `[vmap]`Â function.
    It also makes it easy to parallelize large-scale data processing via theÂ `[pmap]`Â function.'
  id: totrans-786
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '**å‘é‡åŒ–** .Â `JAX`Â é€šè¿‡Â `[vmap]`Â å‡½æ•°æ”¯æŒè‡ªåŠ¨å‘é‡åŒ–ã€‚å®ƒè¿˜é€šè¿‡Â `[pmap]`Â å‡½æ•°ä½¿å¾—å¤§è§„æ¨¡æ•°æ®å¤„ç†æ˜“äºå¹¶è¡ŒåŒ–ã€‚'
- en: '`**JIT compilation**` .Â `JAX`Â usesÂ `XLA`Â for Just In Time (JIT) compilation
    and execution of code on GPUs and TPUs. In this article, let''s look at how you
    can useÂ `JAX`Â and `Flax`Â to build a simple convolutional neural network.'
  id: totrans-787
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '**å³æ—¶ç¼–è¯‘** .Â `JAX`Â ä½¿ç”¨Â `XLA`Â æ¥è¿›è¡Œä»£ç çš„å³æ—¶ç¼–è¯‘å’Œåœ¨ GPU å’Œ TPU ä¸Šæ‰§è¡Œã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•ä½¿ç”¨Â `JAX`Â å’Œ
    `Flax`Â æ¥æ„å»ºä¸€ä¸ªç®€å•çš„å·ç§¯ç¥ç»ç½‘ç»œã€‚'
- en: '`**Loading the dataset**`'
  id: totrans-788
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**åŠ è½½æ•°æ®é›†**'
- en: 'We''ll use theÂ `cats and dogs dataset`Â from Kaggle. Let''s start by downloading
    and extracting it.`import wget # pip install wget import zipfile`'
  id: totrans-789
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'æˆ‘ä»¬å°†ä½¿ç”¨æ¥è‡ª Kaggle çš„`çŒ«ç‹—æ•°æ®é›†`ã€‚è®©æˆ‘ä»¬ä»ä¸‹è½½å’Œè§£å‹å¼€å§‹ã€‚`import wget # pip install wget import
    zipfile`'
- en: '`wget.download("https://ml.machinelearningnuggets.com/train.zi p")`'
  id: totrans-790
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`wget.download("https://ml.machinelearningnuggets.com/train.zi p")`'
- en: '`with zipfile.ZipFile(''train.zip'', ''r'') as zip_ref:`'
  id: totrans-791
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`with zipfile.ZipFile(''train.zip'', ''r'') as zip_ref:`'
- en: '`zip_ref.extractall(''.'')`'
  id: totrans-792
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`zip_ref.extractall(''.'')`'
- en: '`Flax doesn''t ship with any data loading tools. You can use the data loaders
    fromÂ `PyTorch`'
  id: totrans-793
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: Flax æ²¡æœ‰ä»»ä½•æ•°æ®åŠ è½½å·¥å…·ã€‚ä½ å¯ä»¥ä½¿ç”¨æ¥è‡ªÂ `PyTorch`Â çš„æ•°æ®åŠ è½½å™¨ã€‚
- en: '`orÂ `TensorFlow. In this case, let''s load the data using PyTorch. The first
    step is to define the dataset class.'
  id: totrans-794
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æˆ–è€…ä½¿ç”¨Â `TensorFlow`ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨ PyTorch åŠ è½½æ•°æ®ã€‚é¦–å…ˆè¦å®šä¹‰æ•°æ®é›†ç±»ã€‚
- en: '`from PIL import Image`'
  id: totrans-795
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from PIL import Image`'
- en: '`import pandas as pd`'
  id: totrans-796
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import pandas as pd`'
- en: '`from torch.utils.data import Dataset`'
  id: totrans-797
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from torch.utils.data import Dataset`'
- en: '`class CatsDogsDataset(Dataset):`'
  id: totrans-798
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`class CatsDogsDataset(Dataset):`'
- en: '`def __init__(self, root_dir, annotation_file, transform=Non`'
  id: totrans-799
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def __init__(self, root_dir, annotation_file, transform=Non`'
- en: '`e):`'
  id: totrans-800
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`e):`'
- en: '`self.root_dir = root_dir`'
  id: totrans-801
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.root_dir = root_dir`'
- en: '`self.annotations = pd.read_csv(annotation_file) self.transform = transform`'
  id: totrans-802
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.annotations = pd.read_csv(annotation_file) self.transform = transform`'
- en: '`def __len__(self):return len(self.annotations) def __getitem__(self, index):img_id
    = self.annotations.iloc[index, 0]img = Image.open(os.path.join(self.root_dir,
    img_id)).c onvert("RGB")y_label = torch.tensor(float(self.annotations.iloc[inde
    x, 1]))if self.transform is not None: img = self.transform(img)return (img, y_label)`Next,
    we create aÂ `Pandas DataFrame`Â that will contain the categories.`import osimport
    pandas as pd`'
  id: totrans-803
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def __len__(self):return len(self.annotations) def __getitem__(self, index):img_id
    = self.annotations.iloc[index, 0]img = Image.open(os.path.join(self.root_dir,
    img_id)).c onvert("RGB")y_label = torch.tensor(float(self.annotations.iloc[inde
    x, 1]))if self.transform is not None: img = self.transform(img)return (img, y_label)`æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå°†åŒ…å«ç±»åˆ«çš„Â `Pandas
    DataFrame`ã€‚`import osimport pandas as pd`'
- en: '`train_df = pd.DataFrame(columns=["img_path","label"]) train_df["img_path"]
    = os.listdir("train/") for idx, i in enumerate(os.listdir("train/")):`'
  id: totrans-804
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_df = pd.DataFrame(columns=["img_path","label"]) train_df["img_path"]
    = os.listdir("train/") for idx, i in enumerate(os.listdir("train/")):`'
- en: '`if "cat" in i:`'
  id: totrans-805
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`if "cat" in i:`'
- en: '`train_df["label"][idx] = 0`'
  id: totrans-806
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_df["label"][idx] = 0`'
- en: '`if "dog" in i:`'
  id: totrans-807
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`if "dog" in i:`'
- en: '`train_df["label"][idx] = 1`'
  id: totrans-808
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_df["label"][idx] = 1`'
- en: '`train_df.to_csv (r''train_csv.csv'', index = False, header=True)`Define a
    function that will stack the data and return it as NumPy arrays.'
  id: totrans-809
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_df.to_csv (r''train_csv.csv'', index = False, header=True)`å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œå°†å †å æ•°æ®å¹¶ä»¥
    NumPy æ•°ç»„å½¢å¼è¿”å›ã€‚'
- en: '`import numpy as np`'
  id: totrans-810
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import numpy as np`'
- en: def `custom_collate_fn(batch)`
  id: totrans-811
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: def `custom_collate_fn(batch)`
- en: transposed_data = `list(zip(*batch))`
  id: totrans-812
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: transposed_data = `list(zip(*batch))`
- en: labels = `np.array(transposed_data[1])`
  id: totrans-813
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: labels = `np.array(transposed_data[1])`
- en: imgs = `np.stack(transposed_data[0])`
  id: totrans-814
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: imgs = `np.stack(transposed_data[0])`
- en: return `imgs, labels`
  id: totrans-815
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: return `imgs, labels`
- en: We are now ready to define the training and test data and use that with the
    PyTorch DataLoader. We also define aÂ PyTorch transformation for resizing the images.
  id: totrans-816
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å‡†å¤‡å¥½å®šä¹‰è®­ç»ƒå’Œæµ‹è¯•æ•°æ®ï¼Œå¹¶ä½¿ç”¨ PyTorch çš„ DataLoaderã€‚æˆ‘ä»¬è¿˜å®šä¹‰äº†ä¸€ä¸ªç”¨äºè°ƒæ•´å›¾åƒå¤§å°çš„ PyTorch è½¬æ¢ã€‚
- en: import `torch`
  id: totrans-817
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: import `torch`
- en: from `torch.utils.data import DataLoader` from `torchvision import transforms`
    import `numpy as np`
  id: totrans-818
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: from `torch.utils.data import DataLoader` from `torchvision import transforms`
    import `numpy as np`
- en: size_image = 64 batch_size = 32
  id: totrans-819
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: size_image = 64 batch_size = 32
- en: transform = `transforms.Compose([`
  id: totrans-820
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: transform = `transforms.Compose([`
- en: transforms.Resize((size_image,size_image)),
  id: totrans-821
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: transforms.Resize((size_image,size_image)),
- en: '`np.array]`'
  id: totrans-822
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`np.array]`'
- en: dataset = `CatsDogsDataset("train","train_csv.csv",transform=tra nsform)`
  id: totrans-823
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: dataset = `CatsDogsDataset("train","train_csv.csv",transform=tra nsform)`
- en: train_set, validation_set = `torch.utils.data.random_split(datas et,[20000,5000])`
  id: totrans-824
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è®­ç»ƒé›†ã€éªŒè¯é›† = `torch.utils.data.random_split(datas et,[20000,5000])`
- en: train_loader = `DataLoader(dataset=train_set, collate_fn=custom_ collate_fn,shuffle=True,
    batch_size=batch_size)`
  id: totrans-825
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: train_loader = `DataLoader(dataset=train_set, collate_fn=custom_collate_fn,shuffle=True,
    batch_size=batch_size)`
- en: validation_loader = `DataLoader(dataset=validation_set,collate_f n=custom_collate_fn,
    shuffle=False, batch_size=batch_size)`
  id: totrans-826
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: validation_loader = `DataLoader(dataset=validation_set,collate_f n=custom_collate_fn,
    shuffle=False, batch_size=batch_size)`
- en: Define Convolution Neural Network with Flax
  id: totrans-827
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Flax å®šä¹‰å·ç§¯ç¥ç»ç½‘ç»œ
- en: Install FlaxÂ to create a simple neural network.`pip install flax`
  id: totrans-828
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å®‰è£… Flax æ¥åˆ›å»ºä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œã€‚`pip install flax`
- en: Networks are created in Flax using theÂ Linen API by
  id: totrans-829
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨ Flax ä¸­ä½¿ç”¨ Linen API åˆ›å»ºç½‘ç»œ
- en: subclassingÂ Module. Â All Flax modules are PythonÂ dataclasses. This means that
    they have Â [`__.init__`] by default. You should, therefore, overrideÂ [`setup()`]
    instead to initialize the network. However, you can use theÂ compact wrapper
  id: totrans-830
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å­ç±»åŒ– Moduleã€‚æ‰€æœ‰çš„ Flax æ¨¡å—éƒ½æ˜¯ Python çš„ dataclassesã€‚è¿™æ„å‘³ç€å®ƒä»¬é»˜è®¤å…·æœ‰ `__init__` æ–¹æ³•ã€‚å› æ­¤ï¼Œä½ åº”è¯¥è¦†ç›–
    `setup()` æ–¹æ³•æ¥åˆå§‹åŒ–ç½‘ç»œã€‚ä½†æ˜¯ï¼Œä½ å¯ä»¥ä½¿ç”¨ç´§å‡‘çš„åŒ…è£…å™¨
- en: to make the model definition more concise.
  id: totrans-831
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä½¿æ¨¡å‹å®šä¹‰æ›´åŠ ç®€æ´ã€‚
- en: import `flaxfrom flax import linen as nnclass CNN(nn.Module):`
  id: totrans-832
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å¯¼å…¥ `flaxfrom flax import linen as nnclass CNN(nn.Module):`
- en: '@`nn.compact`'
  id: totrans-833
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '@`nn.compact`'
- en: def `__call__(self, x):`
  id: totrans-834
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: def `__call__(self, x):`
- en: x = `nn.Conv(features=32, kernel_size=(3, 3))(x)`
  id: totrans-835
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.Conv(features=32, kernel_size=(3, 3))(x)`
- en: x = `nn.relu(x)`
  id: totrans-836
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.relu(x)`
- en: x = `nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))`
  id: totrans-837
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))`
- en: x = `nn.Conv(features=64, kernel_size=(3, 3))(x)`
  id: totrans-838
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.Conv(features=64, kernel_size=(3, 3))(x)`
- en: x = `nn.relu(x)`
  id: totrans-839
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.relu(x)`
- en: x = `nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))`
  id: totrans-840
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))`
- en: x = `x.reshape((x.shape[0], -1))`
  id: totrans-841
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `x.reshape((x.shape[0], -1))`
- en: x = `nn.Dense(features=256)(x)`
  id: totrans-842
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.Dense(features=256)(x)`
- en: x = `nn.relu(x)`
  id: totrans-843
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.relu(x)`
- en: x = `nn.Dense(features=2)(x)`
  id: totrans-844
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.Dense(features=2)(x)`
- en: x = `nn.log_softmax(x)`
  id: totrans-845
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.log_softmax(x)`
- en: return x
  id: totrans-846
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¿”å› x
- en: Define loss
  id: totrans-847
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: å®šä¹‰æŸå¤±
- en: The loss can be computed using the Optax package. We one-hot encode the integer
    labels before passing them to theÂ softmax crossentropyÂ function.Â num_classesÂ is
    2 because we are dealing with two classes.
  id: totrans-848
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨ Optax åŒ…è®¡ç®—æŸå¤±ã€‚æˆ‘ä»¬åœ¨ä¼ é€’ç»™ softmax äº¤å‰ç†µå‡½æ•°ä¹‹å‰å¯¹æ•´æ•°æ ‡ç­¾è¿›è¡Œäº† one-hot ç¼–ç ã€‚num_classes ä¸º 2ï¼Œå› ä¸ºæˆ‘ä»¬å¤„ç†çš„æ˜¯ä¸¤ç±»é—®é¢˜ã€‚
- en: import `optax`
  id: totrans-849
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å¯¼å…¥ `optax`
- en: def `cross_entropy_loss(*, logits, labels)`
  id: totrans-850
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: def `cross_entropy_loss(*, logits, labels)`
- en: labels_onehot = `jax.nn.one_hot(labels, num_classes=2)`
  id: totrans-851
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: labels_onehot = `jax.nn.one_hot(labels, num_classes=2)`
- en: return `optax.softmax_cross_entropy(logits=logits, labels=labe ls_onehot).mean()`
  id: totrans-852
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¿”å› `optax.softmax_cross_entropy(logits=logits, labels=labels_onehot).mean()`
- en: Compute metrics
  id: totrans-853
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: è®¡ç®—æŒ‡æ ‡
- en: Next, we define a function that will use the above loss function to compute
    and return the loss. We also compute the accuracy in the same function.
  id: totrans-854
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œä½¿ç”¨ä¸Šè¿°æŸå¤±å‡½æ•°è®¡ç®—å¹¶è¿”å›æŸå¤±ã€‚æˆ‘ä»¬è¿˜åœ¨åŒä¸€ä¸ªå‡½æ•°ä¸­è®¡ç®—å‡†ç¡®ç‡ã€‚
- en: def `compute_metrics(*, logits, labels):`
  id: totrans-855
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: def `compute_metrics(*, logits, labels):`
- en: loss = `cross_entropy_loss(logits=logits, labels=labels)` accuracy = `jnp.mean(jnp.argmax(logits,
    -1) == labels)` metrics = {
  id: totrans-856
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: loss = `cross_entropy_loss(logits=logits, labels=labels)` accuracy = `jnp.mean(jnp.argmax(logits,
    -1) == labels)` metrics = {
- en: '''loss'': loss,'
  id: totrans-857
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '''loss'': loss,'
- en: '`''accuracy'': accuracy,`'
  id: totrans-858
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`''accuracy'': accuracy,`'
- en: '}'
  id: totrans-859
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '}'
- en: return metrics
  id: totrans-860
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¿”å›æŒ‡æ ‡
- en: Create training state
  id: totrans-861
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: åˆ›å»ºè®­ç»ƒçŠ¶æ€
- en: A training state holds the model variables such as parameters and optimizer
    state. These variables are modified at each iteration using the optimizer. You
    can subclassÂ `[flax.training.train_state]`Â to track more data. You might want
    to do that for tracking the state of dropout and batch statistics if you include
    those layers in your model. For this simple model, the default class will suffice.
  id: totrans-862
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è®­ç»ƒçŠ¶æ€ä¿å­˜æ¨¡å‹å˜é‡ï¼Œå¦‚å‚æ•°å’Œä¼˜åŒ–å™¨çŠ¶æ€ã€‚è¿™äº›å˜é‡åœ¨æ¯æ¬¡è¿­ä»£ä¸­ä½¿ç”¨ä¼˜åŒ–å™¨è¿›è¡Œä¿®æ”¹ã€‚å¦‚æœåœ¨æ¨¡å‹ä¸­åŒ…å« dropout å’Œæ‰¹å¤„ç†ç»Ÿè®¡ä¿¡æ¯ï¼Œä½ å¯ä»¥å­ç±»åŒ– `flax.training.train_state`
    æ¥è·Ÿè¸ªæ›´å¤šæ•°æ®ã€‚å¯¹äºè¿™ä¸ªç®€å•çš„æ¨¡å‹ï¼Œé»˜è®¤çš„ç±»å°±è¶³å¤Ÿäº†ã€‚
- en: from `flax.training` import `train_state`
  id: totrans-863
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä» `flax.training` å¯¼å…¥ `train_state`
- en: 'def `create_train_state`(rng, learning_rate, momentum): """Creates initial
    `TrainState`."""'
  id: totrans-864
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def `create_train_state`(rng, learning_rate, momentum): """Creates initial
    `TrainState`."""'
- en: '`cnn = CNN()`'
  id: totrans-865
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`cnn = CNN()`'
- en: params = `cnn.init`(rng, jnp.ones([1, size_image, size_image,
  id: totrans-866
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: params = `cnn.init`(rng, jnp.ones([1, size_image, size_image,
- en: '`3`]))[''params'']'
  id: totrans-867
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`3`]))[''params'']'
- en: tx = `optax.sgd`(learning_rate, momentum)
  id: totrans-868
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: tx = `optax.sgd`(learning_rate, momentum)
- en: return `train_state.TrainState.create`(
  id: totrans-869
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¿”å› `train_state.TrainState.create`(
- en: apply_fn=`cnn.apply`, params=`params`, tx=`tx`)
  id: totrans-870
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: apply_fn=`cnn.apply`, params=`params`, tx=`tx`)
- en: '**Define training step**'
  id: totrans-871
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**å®šä¹‰è®­ç»ƒæ­¥éª¤**'
- en: In this function, we evaluate the model with a set of input images using theÂ `Apply`Â method.
    We use the obtained logits to compute the loss. We then use Â `value_and_grad`Â Â to
    evaluate the loss function and its gradient. The gradients are then used to update
    the model parameters. Finally, it uses theÂ `[compute_metrics]`Â function defined
    above to calculate the loss and accuracy.
  id: totrans-872
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªå‡½æ•°ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨Â `Apply`Â æ–¹æ³•å¯¹ä¸€ç»„è¾“å…¥å›¾åƒè¯„ä¼°æ¨¡å‹ã€‚æˆ‘ä»¬ä½¿ç”¨è·å–çš„logitsè®¡ç®—æŸå¤±ã€‚ç„¶åæˆ‘ä»¬ä½¿ç”¨Â `value_and_grad`Â æ¥è¯„ä¼°æŸå¤±å‡½æ•°åŠå…¶æ¢¯åº¦ã€‚ç„¶åä½¿ç”¨æ¢¯åº¦æ¥æ›´æ–°æ¨¡å‹å‚æ•°ã€‚æœ€åï¼Œå®ƒä½¿ç”¨ä¸Šé¢å®šä¹‰çš„Â `[compute_metrics]`Â å‡½æ•°æ¥è®¡ç®—æŸå¤±å’Œå‡†ç¡®ç‡ã€‚
- en: 'def `compute_loss`(params,images,labels):'
  id: totrans-873
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def `compute_loss`(params,images,labels):'
- en: 'logits = `CNN().apply`({''params'': params}, images) loss = `cross_entropy_loss`(logits=`logits`,
    labels=`labels`) return `loss`, `logits`'
  id: totrans-874
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'logits = `CNN().apply`({''params'': params}, images) loss = `cross_entropy_loss`(logits=`logits`,
    labels=`labels`) return `loss`, `logits`'
- en: '@`jax.jit`'
  id: totrans-875
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '@`jax.jit`'
- en: 'def `train_step`(state,images, labels):'
  id: totrans-876
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def `train_step`(state,images, labels):'
- en: '"""Train for a single step."""'
  id: totrans-877
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""å•æ­¥è®­ç»ƒã€‚"""'
- en: (_, logits), grads = jax.value_and_grad(`compute_loss`, has_aux =`True`)(state.params,images,labels)
  id: totrans-878
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: (_, logits), grads = jax.value_and_grad(`compute_loss`, has_aux =`True`)(state.params,images,labels)
- en: state = `state.apply_gradients`(grads=`grads`)
  id: totrans-879
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: state = `state.apply_gradients`(grads=`grads`)
- en: metrics = `compute_metrics`(logits=`logits`, labels=`labels`)
  id: totrans-880
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: metrics = `compute_metrics`(logits=`logits`, labels=`labels`)
- en: return `state`, `metrics`
  id: totrans-881
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: return `state`, `metrics`
- en: The function is decorated with theÂ @`Jit` decorator to trace the function and
    compile just-in-time for faster computation.
  id: totrans-882
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å‡½æ•°è¢«Â @`Jit`Â ä¿®é¥°ï¼Œä»¥è·Ÿè¸ªå‡½æ•°å¹¶å³æ—¶ç¼–è¯‘ï¼Œä»¥æé«˜è®¡ç®—é€Ÿåº¦ã€‚
- en: '**Define evaluation step**'
  id: totrans-883
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**å®šä¹‰è¯„ä¼°æ­¥éª¤**'
- en: The evaluation function will useÂ `[Apply]`Â Â to evaluate the model on the test
    data.
  id: totrans-884
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¯„ä¼°å‡½æ•°å°†ä½¿ç”¨Â `[Apply]`Â Â æ¥åœ¨æµ‹è¯•æ•°æ®ä¸Šè¯„ä¼°æ¨¡å‹ã€‚
- en: '@`jax.jit`'
  id: totrans-885
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '@`jax.jit`'
- en: 'def `eval_step`(state, images, labels):'
  id: totrans-886
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def `eval_step`(state, images, labels):'
- en: 'logits = `CNN().apply`({''params'': state.params}, images)'
  id: totrans-887
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'logits = `CNN().apply`({''params'': state.params}, images)'
- en: return `compute_metrics`(logits=`logits`, labels=`labels`)
  id: totrans-888
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: return `compute_metrics`(logits=`logits`, labels=`labels`)
- en: '**Training function**'
  id: totrans-889
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**è®­ç»ƒå‡½æ•°**'
- en: In this function, we apply the training step we defined above. Â We loop through
    each batch in the data loader and perform optimization for each batch. We use
    theÂ `[jax.device_get]`Â to get the metrics and compute the mean.
  id: totrans-890
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨æ­¤å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬åº”ç”¨äº†ä¸Šé¢å®šä¹‰çš„è®­ç»ƒæ­¥éª¤ã€‚æˆ‘ä»¬éå†æ•°æ®åŠ è½½å™¨ä¸­çš„æ¯ä¸ªæ‰¹æ¬¡å¹¶å¯¹æ¯ä¸ªæ‰¹æ¬¡è¿›è¡Œä¼˜åŒ–ã€‚æˆ‘ä»¬ä½¿ç”¨Â `[jax.device_get]`Â æ¥è·å–æŒ‡æ ‡å¹¶è®¡ç®—å‡å€¼ã€‚
- en: 'def `train_one_epoch`(state, dataloader):'
  id: totrans-891
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def `train_one_epoch`(state, dataloader):'
- en: '"""Train for 1 epoch on the training set.""" batch_metrics = []'
  id: totrans-892
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""åœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒ 1 ä¸ª epochã€‚""" batch_metrics = []'
- en: 'for cnt, (images, labels) in enumerate(dataloader):'
  id: totrans-893
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å¯¹äº cnt, (images, labels) åœ¨ dataloader ä¸­çš„æ¯ä¸€ä¸ªæšä¸¾ï¼š
- en: images = images / `255.0`
  id: totrans-894
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: images = images / `255.0`
- en: state, metrics = `train_step`(state, images, labels) batch_metrics.append(metrics)
  id: totrans-895
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: state, metrics = `train_step`(state, images, labels) batch_metrics.append(metrics)
- en: 'batch_metrics_np = jax.device_get(batch_metrics) epoch_metrics_np = {k: np.mean([metrics[k]
    for metrics in batch_metrics_n p])for k in batch_metrics_np[0] }return `state`,
    `epoch_metrics_np`'
  id: totrans-896
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'batch_metrics_np = jax.device_get(batch_metrics) epoch_metrics_np = {k: np.mean([metrics[k]
    for metrics in batch_metrics_n p])for k in batch_metrics_np[0] }return `state`,
    `epoch_metrics_np`'
- en: '**Evaluate the model**'
  id: totrans-897
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**è¯„ä¼°æ¨¡å‹**'
- en: The evaluation function runs the evaluation step and returns the test metrics.
  id: totrans-898
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¯„ä¼°å‡½æ•°è¿è¡Œè¯„ä¼°æ­¥éª¤å¹¶è¿”å›æµ‹è¯•æŒ‡æ ‡ã€‚
- en: 'def `evaluate_model`(state, test_imgs, test_lbls): """Evaluate on the validation
    set."""'
  id: totrans-899
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def `evaluate_model`(state, test_imgs, test_lbls): """åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°ã€‚"""'
- en: metrics = `eval_step`(state, test_imgs, test_lbls) metrics = jax.device_get(metrics)
  id: totrans-900
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: metrics = `eval_step`(state, test_imgs, test_lbls) metrics = jax.device_get(metrics)
- en: '`metrics = jax.tree_map(lambda x: x.item(), metrics) return metrics`'
  id: totrans-901
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`metrics = jax.tree_map(lambda x: x.item(), metrics) return metrics`'
- en: Train and evaluate the model
  id: totrans-902
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹
- en: We need to initialize the train state before training the model. The function
    to initialize the state requires a pseudo-random number (PRNG) key. Use theÂ [PRNGKey]Â function
    to obtain a key and split it to get another key that you'll use for parameter
    initialization. Follow this link to learn more aboutÂ JAXÂ PRNG Design.
  id: totrans-903
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒæ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦åˆå§‹åŒ–è®­ç»ƒçŠ¶æ€ã€‚åˆå§‹åŒ–çŠ¶æ€å‡½æ•°éœ€è¦ä¸€ä¸ªä¼ªéšæœºæ•°ï¼ˆPRNGï¼‰å¯†é’¥ã€‚ä½¿ç”¨Â [PRNGKey]Â å‡½æ•°è·å–å¯†é’¥å¹¶æ‹†åˆ†å®ƒä»¥è·å¾—å¦ä¸€ä¸ªç”¨äºå‚æ•°åˆå§‹åŒ–çš„å¯†é’¥ã€‚ç‚¹å‡»æ­¤é“¾æ¥äº†è§£æ›´å¤šå…³äº
    JAX PRNG è®¾è®¡çš„ä¿¡æ¯ã€‚
- en: Pass this key to theÂ [create_train_state] function together with the learning
    rate and momentum. You can now use theÂ [train_one_epoch] function to train the
    model and theÂ eval_modelfunction to evaluate the model.
  id: totrans-904
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å°†æ­¤å¯†é’¥ä¸å­¦ä¹ ç‡å’ŒåŠ¨é‡ä¸€èµ·ä¼ é€’ç»™Â [create_train_state]Â å‡½æ•°ã€‚ç°åœ¨å¯ä»¥ä½¿ç”¨Â [train_one_epoch]Â å‡½æ•°è®­ç»ƒæ¨¡å‹ä»¥åŠÂ eval_model
    å‡½æ•°è¯„ä¼°æ¨¡å‹ã€‚
- en: '`import jaxrng = jax.random.PRNGKey(0)rng, init_rng = jax.random.split(rng)`'
  id: totrans-905
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import jaxrng = jax.random.PRNGKey(0)rng, init_rng = jax.random.split(rng)`'
- en: '`learning_rate = 0.1 momentum = 0.9`'
  id: totrans-906
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`learning_rate = 0.1 momentum = 0.9`'
- en: '`seed = 0`'
  id: totrans-907
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`seed = 0`'
- en: '`state = create_train_state(init_rng, learning_rate, momentum) del init_rng
    # Must not be used anymore.`'
  id: totrans-908
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`state = create_train_state(init_rng, learning_rate, momentum) del init_rng
    # ä¸å†ä½¿ç”¨ã€‚`'
- en: '`num_epochs = 30`'
  id: totrans-909
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`num_epochs = 30`'
- en: '`(test_images, test_labels) = next(iter(validation_loader)) test_images = test_images
    / 255.0`'
  id: totrans-910
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`(test_images, test_labels) = next(iter(validation_loader)) test_images = test_images
    / 255.0`'
- en: '`state = create_train_state(jax.random.PRNGKey(seed), learning_r ate, momentum)`'
  id: totrans-911
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`state = create_train_state(jax.random.PRNGKey(seed), learning_r ate, momentum)`'
- en: '`training_loss = []`'
  id: totrans-912
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`training_loss = []`'
- en: '`training_accuracy = []`'
  id: totrans-913
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`training_accuracy = []`'
- en: '`testing_loss = []`'
  id: totrans-914
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_loss = []`'
- en: '`testing_accuracy = []`'
  id: totrans-915
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_accuracy = []`'
- en: '`for epoch in range(1, num_epochs + 1):`'
  id: totrans-916
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`for epoch in range(1, num_epochs + 1):`'
- en: '`train_state, train_metrics = train_one_epoch(state, train_l oader)`'
  id: totrans-917
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_state, train_metrics = train_one_epoch(state, train_l oader)`'
- en: '`training_loss.append(train_metrics[''loss''])`'
  id: totrans-918
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`training_loss.append(train_metrics[''loss''])`'
- en: '`training_accuracy.append(train_metrics[''accuracy''])`'
  id: totrans-919
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`training_accuracy.append(train_metrics[''accuracy''])`'
- en: '`print(f"Train epoch: {epoch}, loss: {train_metrics[''los s'']}, accuracy:
    {train_metrics[''accuracy''] * 100}")`'
  id: totrans-920
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`print(f"è®­ç»ƒå‘¨æœŸï¼š{epoch}ï¼ŒæŸå¤±ï¼š{train_metrics[''loss'']}ï¼Œå‡†ç¡®ç‡ï¼š{train_metrics[''accuracy'']
    * 100}")`'
- en: '`test_metrics = evaluate_model(train_state, test_images, tes t_labels)`'
  id: totrans-921
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`test_metrics = evaluate_model(train_state, test_images, tes t_labels)`'
- en: '`testing_loss.append(test_metrics[''loss''])`'
  id: totrans-922
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_loss.append(test_metrics[''loss''])`'
- en: '`testing_accuracy.append(test_metrics[''accuracy''])`'
  id: totrans-923
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_accuracy.append(test_metrics[''accuracy''])`'
- en: '`print(f"Test epoch: {epoch}, loss: {test_metrics[''loss'']}, accuracy: {test_metrics[''accuracy'']
    * 100}")`'
  id: totrans-924
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`print(f"æµ‹è¯•å‘¨æœŸï¼š{epoch}ï¼ŒæŸå¤±ï¼š{test_metrics[''loss'']}ï¼Œå‡†ç¡®ç‡ï¼š{test_metrics[''accuracy'']
    * 100}")`'
- en: Model performance
  id: totrans-925
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: æ¨¡å‹æ€§èƒ½
- en: While the training is happening, we print the training and validation metrics.
    You can also use the resulting metrics to plot training and validation charts.
  id: totrans-926
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¼šæ‰“å°è®­ç»ƒå’ŒéªŒè¯æŒ‡æ ‡ã€‚æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨è¿™äº›æŒ‡æ ‡ç»˜åˆ¶è®­ç»ƒå’ŒéªŒè¯å›¾è¡¨ã€‚
- en: '`import matplotlib.pyplot as plt`'
  id: totrans-927
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import matplotlib.pyplot as plt`'
- en: '`plt.plot(training_accuracy, label="Training") plt.plot(testing_accuracy, label="Test")
    plt.xlabel("Epoch")`'
  id: totrans-928
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.plot(training_accuracy, label="Training") plt.plot(testing_accuracy, label="Test")
    plt.xlabel("Epoch")`'
- en: '`plt.ylabel("Accuracy")`'
  id: totrans-929
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.ylabel("Accuracy")`'
- en: '`plt.legend()`'
  id: totrans-930
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.legend()`'
- en: '`plt.show()`'
  id: totrans-931
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.show()`'
- en: '`plt.plot(training_loss, label="Training") plt.plot(testing_loss, label="Test")
    plt.xlabel("Epoch")`'
  id: totrans-932
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.plot(training_loss, label="Training") plt.plot(testing_loss, label="Test")
    plt.xlabel("Epoch")`'
- en: '`plt.ylabel("Accuracy")`'
  id: totrans-933
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.ylabel("Accuracy")`'
- en: '`plt.legend()`'
  id: totrans-934
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.legend()`'
- en: '`plt.show()`'
  id: totrans-935
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.show()`'
- en: Final thoughts
  id: totrans-936
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: æ€»ç»“æ€è€ƒ
- en: In this article, we have seen how to set up a simple neural network with Flax
    and train it on the CPU.
  id: totrans-937
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†å¦‚ä½•åœ¨Flaxä¸Šè®¾ç½®ä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œå¹¶åœ¨CPUä¸Šè®­ç»ƒå®ƒã€‚
- en: Distributed training with JAX & Flax
  id: totrans-938
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: ä½¿ç”¨JAXå’ŒFlaxè¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ
- en: Training models on accelerators withÂ JAXÂ and Flax differs slightly from training
    with CPU. For instance, the data needs to be replicated in the different devices
    when using multiple accelerators. After that, we need to execute the training
    on multiple devices and aggregate the results. Flax supports TPU and GPU accelerators.
  id: totrans-939
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä½¿ç”¨JAXå’ŒFlaxåœ¨åŠ é€Ÿå™¨ä¸Šè®­ç»ƒæ¨¡å‹ä¸åœ¨CPUä¸Šè®­ç»ƒç•¥æœ‰ä¸åŒã€‚ä¾‹å¦‚ï¼Œåœ¨ä½¿ç”¨å¤šä¸ªåŠ é€Ÿå™¨æ—¶ï¼Œæ•°æ®éœ€è¦åœ¨ä¸åŒè®¾å¤‡ä¹‹é—´å¤åˆ¶ã€‚ä¹‹åï¼Œæˆ‘ä»¬éœ€è¦åœ¨å¤šä¸ªè®¾å¤‡ä¸Šæ‰§è¡Œè®­ç»ƒå¹¶èšåˆç»“æœã€‚Flaxæ”¯æŒTPUå’ŒGPUåŠ é€Ÿå™¨ã€‚
- en: In the last article, we saw how toÂ train models with the CPU. This article will
    focus on training models with Flax andÂ JAXÂ using GPUs and TPU.
  id: totrans-940
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†å¦‚ä½•ä½¿ç”¨CPUè®­ç»ƒæ¨¡å‹ã€‚æœ¬æ–‡å°†ä¸“æ³¨äºä½¿ç”¨Flaxå’ŒJAXåœ¨GPUå’ŒTPUä¸Šè®­ç»ƒæ¨¡å‹ã€‚
- en: Perform standard imports
  id: totrans-941
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: æ‰§è¡Œæ ‡å‡†å¯¼å…¥
- en: You'll need toÂ install FlaxÂ for this illustration.pip install flaxLet's import
    all the packages we'll use in this project.
  id: totrans-942
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¿›è¡Œæ¼”ç¤ºï¼Œæ‚¨éœ€è¦å®‰è£…Flaxã€‚å¯ä»¥é€šè¿‡pip install flaxè¿›è¡Œå®‰è£…ã€‚è®©æˆ‘ä»¬å¯¼å…¥è¿™ä¸ªé¡¹ç›®ä¸­å°†è¦ä½¿ç”¨çš„æ‰€æœ‰åŒ…ã€‚
- en: '`import wget`'
  id: totrans-943
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import wget`'
- en: '`import zipfile`'
  id: totrans-944
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import zipfile`'
- en: '`import torch`'
  id: totrans-945
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import torch`'
- en: '`from torch.utils.data import DataLoader import os`'
  id: totrans-946
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from torch.utils.data import DataLoader import os`'
- en: '`from PIL import Image`'
  id: totrans-947
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from PIL import Image`'
- en: '`from torchvision import transforms from torch.utils.data import Dataset import
    numpy as np`'
  id: totrans-948
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from torchvision import transforms from torch.utils.data import Dataset import
    numpy as np`'
- en: '`import pandas as pd`'
  id: totrans-949
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import pandas as pd`'
- en: '`import matplotlib.pyplot as plt`'
  id: totrans-950
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import matplotlib.pyplot as plt`'
- en: '`import functools`'
  id: totrans-951
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import functools`'
- en: '`import time`'
  id: totrans-952
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import time`'
- en: '`from tqdm.notebook import tqdm`'
  id: totrans-953
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from tqdm.notebook import tqdm`'
- en: ignore harmless warnings
  id: totrans-954
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: å¿½ç•¥æ— å®³çš„è­¦å‘Š
- en: '`import warnings`'
  id: totrans-955
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import warnings`'
- en: '`warnings.filterwarnings("ignore") warnings.simplefilter(''ignore'') import
    jax`'
  id: totrans-956
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`warnings.filterwarnings("ignore") warnings.simplefilter(''ignore'') import
    jax`'
- en: '`from jax import numpy as jnp`'
  id: totrans-957
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from jax import numpy as jnp`'
- en: '`from flax import linen as nn`'
  id: totrans-958
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from flax import linen as nn`'
- en: '`from flax.training import train_state import optax`'
  id: totrans-959
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from flax.training import train_state import optax`'
- en: '`import math`'
  id: totrans-960
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import math`'
- en: '`from flax import jax_utils`'
  id: totrans-961
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from flax import jax_utils`'
- en: '`import jax.tools.colab_tpu`'
  id: totrans-962
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import jax.tools.colab_tpu`'
- en: '**Setup TPUs on Colab**'
  id: totrans-963
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**åœ¨Colabä¸Šè®¾ç½®TPUs**'
- en: Change the runtime on Colab to TPUs. Next, run the code below to set upÂ `JAX`Â to
    use TPUs.`jax.tools.colab_tpu.setup_tpu() jax.devices()![](../images/00031.gif)`
  id: totrans-964
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨ Colab ä¸Šåˆ‡æ¢åˆ° TPUs è¿è¡Œæ—¶ã€‚æ¥ä¸‹æ¥ï¼Œè¿è¡Œä»¥ä¸‹ä»£ç è®¾ç½®`JAX`ä»¥ä½¿ç”¨ TPUsã€‚`jax.tools.colab_tpu.setup_tpu()
    jax.devices()![](../images/00031.gif)`
- en: '**Download the dataset**'
  id: totrans-965
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**ä¸‹è½½æ•°æ®é›†**'
- en: 'We''ll use theÂ `cats and dogs dataset`Â from Kaggle. Let''s start by downloading
    and extracting it.`import wget # pip install wget import zipfile`'
  id: totrans-966
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'æˆ‘ä»¬å°†ä½¿ç”¨æ¥è‡ª Kaggle çš„`çŒ«å’Œç‹—æ•°æ®é›†`ã€‚è®©æˆ‘ä»¬ä»ä¸‹è½½å’Œè§£å‹å¼€å§‹ã€‚`import wget # pip install wget import
    zipfile`'
- en: '`wget.download("https://ml.machinelearningnuggets.com/train.zi p")`'
  id: totrans-967
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`wget.download("https://ml.machinelearningnuggets.com/train.zi p")`'
- en: '`with zipfile.ZipFile(''train.zip'', ''r'') as zip_ref:`'
  id: totrans-968
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`with zipfile.ZipFile(''train.zip'', ''r'') as zip_ref:`'
- en: '`zip_ref.extractall(''.'')`'
  id: totrans-969
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`zip_ref.extractall(''.'')`'
- en: '**Load the dataset**'
  id: totrans-970
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**åŠ è½½æ•°æ®é›†**'
- en: We'll use existing data loaders to load the data sinceÂ `JAX`Â and Flax don't
    ship with any data loaders. In this case, let's useÂ `PyTorch`Â to load the dataset.
    The first step is to set up a dataset class.
  id: totrans-971
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ç”±äº`JAX`å’ŒFlaxæ²¡æœ‰ä»»ä½•æ•°æ®åŠ è½½å™¨ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ç°æœ‰çš„æ•°æ®åŠ è½½å™¨æ¥åŠ è½½æ•°æ®ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨`PyTorch`æ¥åŠ è½½æ•°æ®é›†ã€‚ç¬¬ä¸€æ­¥æ˜¯è®¾ç½®ä¸€ä¸ªæ•°æ®é›†ç±»ã€‚
- en: class `CatsDogsDataset(Dataset):`
  id: totrans-972
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`CatsDogsDataset`ç±»ï¼š'
- en: '`def __init__(self, root_dir, annotation_file, transform=Non`'
  id: totrans-973
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`__init__`æ–¹æ³•ï¼š'
- en: '`e):`'
  id: totrans-974
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`e):`'
- en: '`self.root_dir = root_dir`'
  id: totrans-975
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.root_dir = root_dir`'
- en: '`self.annotations = pd.read_csv(annotation_file) self.transform = transform`'
  id: totrans-976
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.annotations = pd.read_csv(annotation_file) self.transform = transform`'
- en: '`def __len__(self):return len(self.annotations)`'
  id: totrans-977
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`__len__`æ–¹æ³•ï¼š'
- en: '`def __getitem__(self, index):`'
  id: totrans-978
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`__getitem__`æ–¹æ³•ï¼š'
- en: '`img_id = self.annotations.iloc[index, 0]`'
  id: totrans-979
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`img_id = self.annotations.iloc[index, 0]`'
- en: '`img = Image.open(os.path.join(self.root_dir, img_id)).c`'
  id: totrans-980
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`img = Image.open(os.path.join(self.root_dir, img_id)).c`'
- en: '`onvert("RGB")`'
  id: totrans-981
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`onvert("RGB")`'
- en: '`y_label = torch.tensor(float(self.annotations.iloc[inde`'
  id: totrans-982
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`y_label = torch.tensor(float(self.annotations.iloc[inde`'
- en: '`x, 1]))`'
  id: totrans-983
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x, 1]))`'
- en: '`if self.transform is not None: img = self.transform(img)return (img, y_label)Next,
    we create a DataFrame containing the categories.`'
  id: totrans-984
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`if self.transform is not None: img = self.transform(img)return (img, y_label)æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªåŒ…å«ç±»åˆ«çš„
    DataFrameã€‚`'
- en: '`train_df = pd.DataFrame(columns=["img_path","label"]) train_df["img_path"]
    = os.listdir("train/") for idx, i in enumerate(os.listdir("train/")):`'
  id: totrans-985
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_df = pd.DataFrame(columns=["img_path","label"]) train_df["img_path"]
    = os.listdir("train/") for idx, i in enumerate(os.listdir("train/")):`'
- en: '`if "cat" in i:`'
  id: totrans-986
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`if "cat" in i:`'
- en: '`train_df["label"][idx] = 0`'
  id: totrans-987
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_df["label"][idx] = 0`'
- en: '`if "dog" in i:`'
  id: totrans-988
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`if "dog" in i:`'
- en: '`train_df["label"][idx] = 1`'
  id: totrans-989
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_df["label"][idx] = 1`'
- en: '`train_df.to_csv (r''train_csv.csv'', index = False, header=True)`'
  id: totrans-990
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_df.to_csv (r''train_csv.csv'', index = False, header=True)`'
- en: We then use the dataset class to create training and testing data. We also apply
    a custom function to return the data asÂ NumPy arrays. Later, we'll use thisÂ `train_loader`
  id: totrans-991
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨æ•°æ®é›†ç±»åˆ›å»ºè®­ç»ƒå’Œæµ‹è¯•æ•°æ®ã€‚æˆ‘ä»¬è¿˜åº”ç”¨ä¸€ä¸ªè‡ªå®šä¹‰å‡½æ•°å°†æ•°æ®è¿”å›ä¸º`NumPy`æ•°ç»„ã€‚ç¨åï¼Œæˆ‘ä»¬å°†ä½¿ç”¨è¿™ä¸ª`train_loader`
- en: when training the model. We'll then evaluate it on a batch of the test data.
  id: totrans-992
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒæ¨¡å‹æ—¶ã€‚ç„¶åæˆ‘ä»¬å°†åœ¨ä¸€æ‰¹æµ‹è¯•æ•°æ®ä¸Šè¯„ä¼°å®ƒã€‚
- en: '`def custom_collate_fn(batch):`'
  id: totrans-993
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`custom_collate_fn`å‡½æ•°ï¼š'
- en: '`transposed_data = list(zip(*batch)) labels = np.array(transposed_data[1])
    imgs = np.stack(transposed_data[0]) return imgs, labels`'
  id: totrans-994
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`transposed_data = list(zip(*batch)) labels = np.array(transposed_data[1])
    imgs = np.stack(transposed_data[0]) return imgs, labels`'
- en: '`size_image = 224 batch_size = 64`'
  id: totrans-995
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`size_image = 224 batch_size = 64`'
- en: '`transform = transforms.Compose([`'
  id: totrans-996
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`transform = transforms.Compose([`'
- en: '`transforms.Resize((size_image,size_image)),`'
  id: totrans-997
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`transforms.Resize((size_image,size_image)),`'
- en: '`np.array])`'
  id: totrans-998
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`np.array])`'
- en: '`dataset = CatsDogsDataset("train","train_csv.csv",transform=tra nsform)`'
  id: totrans-999
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`dataset = CatsDogsDataset("train","train_csv.csv",transform=tra nsform)`'
- en: '`train_set, validation_set = torch.utils.data.random_split(datas et,[20000,5000])`'
  id: totrans-1000
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_set, validation_set = torch.utils.data.random_split(datas et,[20000,5000])`'
- en: '`train_loader = DataLoader(dataset=train_set, collate_fn=custom_ collate_fn,shuffle=True,
    batch_size=batch_size)`'
  id: totrans-1001
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_loader = DataLoader(dataset=train_set, collate_fn=custom_ collate_fn,shuffle=True,
    batch_size=batch_size)`'
- en: '`validation_loader = DataLoader(dataset=validation_set,collate_f n=custom_collate_fn,
    shuffle=False, batch_size=batch_size)`'
  id: totrans-1002
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`validation_loader = DataLoader(dataset=validation_set,collate_f n=custom_collate_fn,
    shuffle=False, batch_size=batch_size)`'
- en: Define the model with Flax
  id: totrans-1003
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Flax å®šä¹‰æ¨¡å‹
- en: In Flax, models are defined using theÂ Linen API. It provides the building blocks
    for defining convolution layers, dropout, etc.
  id: totrans-1004
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨ Flax ä¸­ï¼Œæ¨¡å‹æ˜¯ä½¿ç”¨ Linen API å®šä¹‰çš„ã€‚å®ƒæä¾›äº†å®šä¹‰å·ç§¯å±‚ã€dropout ç­‰çš„åŸºæœ¬æ„ä»¶ã€‚
- en: Networks are created by subclassingÂ `[Module]`. Â  Flax allows you to define
    your networks usingÂ `[setup]` orÂ `[nn.compact]`. Both approaches behave the same
    way butÂ `[nn.compact]`
  id: totrans-1005
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ç½‘ç»œé€šè¿‡å­ç±»åŒ–`[Module]`åˆ›å»ºã€‚Flax å…è®¸æ‚¨ä½¿ç”¨`[setup]`æˆ–`[nn.compact]`å®šä¹‰ç½‘ç»œã€‚è¿™ä¸¤ç§æ–¹æ³•çš„è¡Œä¸ºç›¸åŒï¼Œä½†`[nn.compact]`
- en: is more concise.
  id: totrans-1006
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ›´ç®€æ´ã€‚
- en: Create training state
  id: totrans-1007
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: åˆ›å»ºè®­ç»ƒçŠ¶æ€
- en: We now need to create parallel versions of our functions. Parallelization inÂ JAXÂ is
    done using
  id: totrans-1008
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬éœ€è¦åˆ›å»ºå‡½æ•°çš„å¹¶è¡Œç‰ˆæœ¬ã€‚åœ¨Â JAXÂ ä¸­ï¼Œä½¿ç”¨å¹¶è¡ŒåŒ–
- en: theÂ `[pmap]`Â function.Â `[pmap]`Â compiles a function with XLA and executes it
    on multiple devices.
  id: totrans-1009
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[pmap]`Â å‡½æ•°ã€‚Â `[pmap]`Â ä¼šä½¿ç”¨ XLA ç¼–è¯‘å‡½æ•°å¹¶åœ¨å¤šä¸ªè®¾å¤‡ä¸Šæ‰§è¡Œã€‚'
- en: '```python'
  id: totrans-1010
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '```python'
- en: cnn = CNN()
  id: totrans-1011
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`cnn = CNN()`'
- en: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image, 3]))[''params'']`'
  id: totrans-1012
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`params = cnn.init(rng, jnp.ones([1, size_image, size_image, 3]))[''params'']`'
- en: '`tx = optax.sgd(learning_rate, momentum)`'
  id: totrans-1013
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`tx = optax.sgd(learning_rate, momentum)`'
- en: '`return train_state.TrainState.create(`'
  id: totrans-1014
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return train_state.TrainState.create(`'
- en: '`apply_fn=cnn.apply, params=params, tx=tx)`'
  id: totrans-1015
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`apply_fn=cnn.apply, params=params, tx=tx)`'
- en: Apply the model
  id: totrans-1016
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: åº”ç”¨æ¨¡å‹
- en: The next step is to define
  id: totrans-1017
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€æ­¥æ˜¯å®šä¹‰
- en: parallelÂ `apply_model`Â andÂ `update_model`functions.
  id: totrans-1018
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å¹¶è¡Œåº”ç”¨ `apply_model` å’Œ `update_model` å‡½æ•°ã€‚
- en: 'TheÂ `[apply_model]`Â function:'
  id: totrans-1019
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[apply_model]`Â å‡½æ•°ï¼š'
- en: Computes the loss.
  id: totrans-1020
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è®¡ç®—æŸå¤±ã€‚
- en: Computes predictions from all devices by calculating the average of the probabilities
    usingÂ `[jax.lax.pmean()]`.```python
  id: totrans-1021
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: é€šè¿‡è®¡ç®—ä½¿ç”¨ `[jax.lax.pmean()]` è®¡ç®—æ‰€æœ‰è®¾å¤‡çš„æ¦‚ç‡å¹³å‡å€¼æ¥ç”Ÿæˆé¢„æµ‹ã€‚```python
- en: 'logits = CNN().apply({''params'': params}, images) one_hot = jax.nn.one_hot(labels,
    2)'
  id: totrans-1022
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'logits = CNN().apply({''params'': params}, images) one_hot = jax.nn.one_hot(labels,
    2)'
- en: '`loss = optax.softmax_cross_entropy(logits=logits, labels=on`'
  id: totrans-1023
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`loss = optax.softmax_cross_entropy(logits=logits, labels=on`'
- en: e_hot).mean()return loss, logits
  id: totrans-1024
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: e_hot).mean()return loss, logits
- en: '`grad_fn = jax.value_and_grad(loss_fn, has_aux=True) (loss, logits), grads
    = grad_fn(state.params)`'
  id: totrans-1025
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`grad_fn = jax.value_and_grad(loss_fn, has_aux=True) (loss, logits), grads
    = grad_fn(state.params)`'
- en: '`probs = jax.lax.pmean(jax.nn.softmax(logits), axis_name=''ense`'
  id: totrans-1026
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`probs = jax.lax.pmean(jax.nn.softmax(logits), axis_name=''ense`'
- en: '```python'
  id: totrans-1027
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '```python'
- en: Notice the use of theÂ `[axis_name]`. You can give this any name. You'll need
    to specify that when computing the mean of the probabilities and accuracies.
  id: totrans-1028
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ³¨æ„ä½¿ç”¨äº† `[axis_name]`ã€‚ä½ å¯ä»¥éšæ„æŒ‡å®šåç§°ã€‚åœ¨è®¡ç®—æ¦‚ç‡å’Œå‡†ç¡®ç‡çš„å¹³å‡å€¼æ—¶ï¼Œéœ€è¦æŒ‡å®šè¿™ä¸ªåç§°ã€‚
- en: TheÂ `[update_model]`Â function updates the model parameters.
  id: totrans-1029
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[update_model]`Â å‡½æ•°æ›´æ–°æ¨¡å‹å‚æ•°ã€‚'
- en: Training function
  id: totrans-1030
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: è®­ç»ƒå‡½æ•°
- en: 'The next step is to define the model training function. In the function, we:'
  id: totrans-1031
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€æ­¥æ˜¯å®šä¹‰æ¨¡å‹è®­ç»ƒå‡½æ•°ã€‚åœ¨è¯¥å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬ï¼š
- en: Replicate the training data at batch level
  id: totrans-1032
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨æ‰¹çº§åˆ«ä¸Šå¤åˆ¶è®­ç»ƒæ•°æ®
- en: usingÂ `jax_utils.replicat e`.
  id: totrans-1033
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Â `jax_utils.replicate`ã€‚
- en: '`[apply_model]`Â to the replicated data.'
  id: totrans-1034
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[apply_model]`Â åº”ç”¨äºå¤åˆ¶çš„æ•°æ®ã€‚'
- en: Obtain the epoch loss and accuracy and unreplicate them
  id: totrans-1035
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è·å– epoch æŸå¤±å’Œå‡†ç¡®ç‡ï¼Œå¹¶å¯¹å…¶è¿›è¡Œè§£å¤åˆ¶
- en: usingÂ `jax_utils.unreplicate`.
  id: totrans-1036
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Â `jax_utils.unreplicate`ã€‚
- en: Compute the mean of the loss and accuracy.
  id: totrans-1037
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è®¡ç®—æŸå¤±å’Œå‡†ç¡®ç‡çš„å‡å€¼ã€‚
- en: '`[apply_model]`Â to the test data and obtain test metrics.'
  id: totrans-1038
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[apply_model]`Â åº”ç”¨äºæµ‹è¯•æ•°æ®å¹¶è·å–æµ‹è¯•æŒ‡æ ‡ã€‚'
- en: Print the training and evaluation metrics per epoch. Append the training and
    test metrics to lists for visualization later.
  id: totrans-1039
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ‰“å°æ¯ä¸ª epoch çš„è®­ç»ƒå’Œè¯„ä¼°æŒ‡æ ‡ã€‚å°†è®­ç»ƒå’Œæµ‹è¯•æŒ‡æ ‡é™„åŠ åˆ°åˆ—è¡¨ä¸­ï¼Œä»¥ä¾¿ç¨åå¯è§†åŒ–ã€‚
- en: '```python'
  id: totrans-1040
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '```python'
- en: '`epoch_loss = []`'
  id: totrans-1041
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`epoch_loss = []`'
- en: '`epoch_accuracy = []`'
  id: totrans-1042
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`epoch_accuracy = []`'
- en: '`testing_accuracy = []`'
  id: totrans-1043
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_accuracy = []`'
- en: '`testing_loss = []`'
  id: totrans-1044
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_loss = []`'
- en: '`for epoch in range(num_epochs):`'
  id: totrans-1045
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`for epoch in range(num_epochs):`'
- en: '`for cnt, (images, labels) in tqdm(enumerate(dataloade r), total=(math.ceil(len(train_set)/batch_size))):
    images = images / 255.0`'
  id: totrans-1046
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`for cnt, (images, labels) in tqdm(enumerate(dataloade r), total=(math.ceil(len(train_set)/batch_size))):
    images = images / 255.0`'
- en: '`images` = `jax_utils.replicate(images)`'
  id: totrans-1047
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`images` = `jax_utils.replicate(images)`'
- en: '`labels` = `jax_utils.replicate(labels)`'
  id: totrans-1048
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`labels` = `jax_utils.replicate(labels)`'
- en: '`grads`, `loss`, `accuracy` = `apply_model(state, images`,'
  id: totrans-1049
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`grads`, `loss`, `accuracy` = `apply_model(state, images`,'
- en: '`labels)`'
  id: totrans-1050
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`labels)`'
- en: '`state` = `update_model(state, grads)`'
  id: totrans-1051
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`state` = `update_model(state, grads)`'
- en: '`epoch_loss.append(jax_utils.unreplicate(loss)) epoch_accuracy.append(jax_utils.unreplicate(accuracy))
    train_loss = np.mean(epoch_loss)`'
  id: totrans-1052
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`epoch_loss.append(jax_utils.unreplicate(loss)) epoch_accuracy.append(jax_utils.unreplicate(accuracy))
    train_loss = np.mean(epoch_loss)`'
- en: '`train_accuracy = np.mean(epoch_accuracy)`'
  id: totrans-1053
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_accuracy = np.mean(epoch_accuracy)`'
- en: '`_, test_loss, test_accuracy` = `jax_utils.unreplicate(apply_model(state, test_images,
    test_labels))`'
  id: totrans-1054
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`_, test_loss, test_accuracy` = `jax_utils.unreplicate(apply_model(state, test_images,
    test_labels))`'
- en: '`testing_accuracy.append(test_accuracy)`'
  id: totrans-1055
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_accuracy.append(test_accuracy)`'
- en: '`testing_loss.append(test_loss)`'
  id: totrans-1056
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_loss.append(test_loss)`'
- en: '`print(f"Epoch: {epoch + 1}, train loss: {train_loss:.4 f}, train accuracy:
    {train_accuracy * 100:.4f}, test loss: {tes t_loss:.4f}, test accuracy: {test_accuracy*
    100:.4f}", flush=Tr ue)`'
  id: totrans-1057
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`print(f"Epoch: {epoch + 1}, train loss: {train_loss:.4 f}, train accuracy:
    {train_accuracy * 100:.4f}, test loss: {tes t_loss:.4f}, test accuracy: {test_accuracy*
    100:.4f}", flush=Tr ue)`'
- en: '`return state, epoch_loss, epoch_accuracy, testing_accuracy, testing_loss`'
  id: totrans-1058
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return state, epoch_loss, epoch_accuracy, testing_accuracy, testing_loss`'
- en: Train the model
  id: totrans-1059
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ¨¡å‹
- en: When creating the training state, we generate pseudo-random numbers equivalent
    to the number of devices. We also replicate a small batch of the test data for
    testing. The next step is to run the training function and unpack the training
    and test metrics.
  id: totrans-1060
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åˆ›å»ºè®­ç»ƒçŠ¶æ€æ—¶ï¼Œæˆ‘ä»¬ç”Ÿæˆä¸è®¾å¤‡æ•°é‡ç›¸å½“çš„ä¼ªéšæœºæ•°ã€‚æˆ‘ä»¬è¿˜ä¸ºæµ‹è¯•å¤åˆ¶äº†ä¸€å°æ‰¹æµ‹è¯•æ•°æ®ã€‚ä¸‹ä¸€æ­¥æ˜¯è¿è¡Œè®­ç»ƒå‡½æ•°å¹¶è§£å‹ç¼©è®­ç»ƒå’Œæµ‹è¯•æŒ‡æ ‡ã€‚
- en: '`learning_rate = 0.1 momentum = 0.9`'
  id: totrans-1061
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`learning_rate = 0.1 momentum = 0.9`'
- en: '`seed = 0`'
  id: totrans-1062
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`seed = 0`'
- en: '`num_epochs = 30`'
  id: totrans-1063
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`num_epochs = 30`'
- en: '`rng` = `jax.random.PRNGKey(0)`'
  id: totrans-1064
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`rng` = `jax.random.PRNGKey(0)`'
- en: '`rng`, `init_rng` = `jax.random.split(rng)`'
  id: totrans-1065
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`rng`, `init_rng` = `jax.random.split(rng)`'
- en: '`state` = `create_train_state(jax.random.split(init_rng, jax.device_count()),learning_rate,
    momentum)` `del init_rng` # Must not be used anymore.'
  id: totrans-1066
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`state` = `create_train_state(jax.random.split(init_rng, jax.device_count()),learning_rate,
    momentum)` `del init_rng` # ä¸å†ä½¿ç”¨ã€‚'
- en: '`(test_images, test_labels)` = `next(iter(validation_loader))` `test_images
    = test_images / 255.0`'
  id: totrans-1067
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`(test_images, test_labels)` = `next(iter(validation_loader))` `test_images
    = test_images / 255.0`'
- en: '`test_images` = `jax_utils.replicate(test_images)`'
  id: totrans-1068
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`test_images` = `jax_utils.replicate(test_images)`'
- en: '`test_labels` = `jax_utils.replicate(test_labels)`'
  id: totrans-1069
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`test_labels` = `jax_utils.replicate(test_labels)`'
- en: '`start = time.time()`'
  id: totrans-1070
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`start = time.time()`'
- en: '`state`, `epoch_loss`, `epoch_accuracy`, `testing_accuracy`, `testing_loss`
    = `train_one_epoch(state, train_loader,num_epochs)` `print("Total time: ", time.time()
    - start, "seconds")`'
  id: totrans-1071
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`state`, `epoch_loss`, `epoch_accuracy`, `testing_accuracy`, `testing_loss`
    = `train_one_epoch(state, train_loader,num_epochs)` `print("Total time: ", time.time()
    - start, "seconds")`'
- en: '![](../images/00032.jpeg)'
  id: totrans-1072
  prefs: []
  stylish: true
  type: TYPE_IMG
  zh: '![](../images/00032.jpeg)'
- en: Model evaluation
  id: totrans-1073
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: æ¨¡å‹è¯„ä¼°
- en: The metrics obtained above can be used to plot the metrics.
  id: totrans-1074
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä¸Šé¢è·å¾—çš„æŒ‡æ ‡å¯ç”¨äºç»˜åˆ¶æŒ‡æ ‡ã€‚
- en: '`plt.plot(epoch_accuracy, label="Training")`'
  id: totrans-1075
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.plot(epoch_accuracy, label="Training")`'
- en: '`plt.plot(testing_accuracy, label="Test")`'
  id: totrans-1076
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.plot(testing_accuracy, label="Test")`'
- en: '`plt.xlabel("Epoch")`'
  id: totrans-1077
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.xlabel("Epoch")`'
- en: '`plt.ylabel("Accuracy")`'
  id: totrans-1078
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.ylabel("Accuracy")`'
- en: '`plt.legend()`'
  id: totrans-1079
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.legend()`'
- en: '`plt.show()`'
  id: totrans-1080
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.show()`'
- en: '![](../images/00033.jpeg)'
  id: totrans-1081
  prefs: []
  stylish: true
  type: TYPE_IMG
  zh: '![](../images/00033.jpeg)'
- en: Final thoughts
  id: totrans-1082
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: æœ€ç»ˆæƒ³æ³•
- en: This article shows how you can use JAX and Flax to train machine learning models
    in parallel on multiple devices. You have seen that the process involves making
    a few functions parallel using JAX's pmap function. We have also covered how to
    replicate the training and test data on multiple devices.
  id: totrans-1083
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æœ¬æ–‡å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ JAX å’Œ Flax åœ¨å¤šè®¾å¤‡ä¸Šå¹¶è¡Œè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹ã€‚æ‚¨å·²ç»çœ‹åˆ°è¯¥è¿‡ç¨‹æ¶‰åŠä½¿ç”¨ JAX çš„ pmap å‡½æ•°å°†å‡ ä¸ªå‡½æ•°å¹¶è¡ŒåŒ–ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†å¦‚ä½•åœ¨å¤šä¸ªè®¾å¤‡ä¸Šå¤åˆ¶è®­ç»ƒå’Œæµ‹è¯•æ•°æ®ã€‚
- en: How to use TensorBoard in JAX & Flax
  id: totrans-1084
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: å¦‚ä½•åœ¨ JAX å’Œ Flax ä¸­ä½¿ç”¨ TensorBoard
- en: Tracking machine learning experiments makes understanding and visualizing the
    model's performance easy. It also makes it possible to spot any problems in the
    network. For example, you can quickly spot overfitting by looking at the training
    and validation charts. You can plot these charts using your favorite charts package,
    such as Matplotlib. However, you can also use more advanced tools such as TensorBoard.
  id: totrans-1085
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è·Ÿè¸ªæœºå™¨å­¦ä¹ å®éªŒä½¿ç†è§£å’Œå¯è§†åŒ–æ¨¡å‹æ€§èƒ½å˜å¾—ç®€å•ã€‚å®ƒè¿˜å¯ä»¥å¸®åŠ©æ‚¨å¿«é€Ÿå‘ç°ç½‘ç»œä¸­çš„ä»»ä½•é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œé€šè¿‡æŸ¥çœ‹è®­ç»ƒå’ŒéªŒè¯å›¾è¡¨ï¼Œæ‚¨å¯ä»¥å¿«é€Ÿå‘ç°è¿‡æ‹Ÿåˆé—®é¢˜ã€‚æ‚¨å¯ä»¥ä½¿ç”¨è‡ªå·±å–œæ¬¢çš„å›¾è¡¨åŒ…ï¼ˆå¦‚
    Matplotlibï¼‰ç»˜åˆ¶è¿™äº›å›¾è¡¨ã€‚ä½†æ˜¯ï¼Œæ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨æ›´å…ˆè¿›çš„å·¥å…·ï¼Œå¦‚ TensorBoardã€‚
- en: 'TensorBoard is an open-source library that provides tools for experiment tracking
    in machine learning. You can use TensorBoard for:'
  id: totrans-1086
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: TensorBoard æ˜¯ä¸€ä¸ªå¼€æºåº“ï¼Œæä¾›äº†æœºå™¨å­¦ä¹ å®éªŒè·Ÿè¸ªå·¥å…·ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ TensorBoard è¿›è¡Œï¼š
- en: Tracking and visualizing model evaluation metrics such as accuracy.
  id: totrans-1087
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è·Ÿè¸ªå’Œå¯è§†åŒ–æ¨¡å‹è¯„ä¼°æŒ‡æ ‡ï¼Œå¦‚å‡†ç¡®æ€§ã€‚
- en: Logging images.
  id: totrans-1088
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è®°å½•å›¾åƒã€‚
- en: Visualize hyper-parameter tuning.
  id: totrans-1089
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å¯è§†åŒ–è¶…å‚æ•°è°ƒæ•´ã€‚
- en: Project embeddings such as word embedding in natural language processing problems.
  id: totrans-1090
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: é¡¹ç›®åµŒå…¥ï¼Œä¾‹å¦‚è‡ªç„¶è¯­è¨€å¤„ç†é—®é¢˜ä¸­çš„è¯åµŒå…¥ã€‚
- en: Visualize histograms of the model's weights and biases.
  id: totrans-1091
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å¯è§†åŒ–æ¨¡å‹æƒé‡å’Œåå·®çš„ç›´æ–¹å›¾ã€‚
- en: Plot the architecture of the model.
  id: totrans-1092
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ç»˜åˆ¶æ¨¡å‹çš„æ¶æ„ã€‚
- en: Profile the performance of the network.
  id: totrans-1093
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¯„ä¼°ç½‘ç»œæ€§èƒ½ã€‚
- en: You can useÂ TensorBoardÂ with popular machine learning libraries such asÂ XGBoost,Â JAX,Â Flax,
    andÂ PyTorch.
  id: totrans-1094
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åœ¨æµè¡Œçš„æœºå™¨å­¦ä¹ åº“ï¼ˆå¦‚ XGBoostã€JAXã€Flax å’Œ PyTorchï¼‰ä¸­ä½¿ç”¨ TensorBoardã€‚
- en: This article will focus on how to use TensorBoard when building networks withÂ JAXÂ and
    Flax.
  id: totrans-1095
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æœ¬æ–‡å°†é‡ç‚¹ä»‹ç»åœ¨ä½¿ç”¨ JAX å’Œ Flax æ„å»ºç½‘ç»œæ—¶å¦‚ä½•ä½¿ç”¨ TensorBoardã€‚
- en: '**How to use TensorBoard**'
  id: totrans-1096
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**å¦‚ä½•ä½¿ç”¨ TensorBoard**'
- en: Let's start by exploring how to use TensorBoard.
  id: totrans-1097
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»æ¢ç´¢å¦‚ä½•ä½¿ç”¨ TensorBoard å¼€å§‹ã€‚
- en: '**How to install TensorBoard**'
  id: totrans-1098
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**å¦‚ä½•å®‰è£… TensorBoard**'
- en: The first step is to install TensorBoard from theÂ PythonÂ Index. `pip install
    tensorboard`
  id: totrans-1099
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€æ­¥æ˜¯ä» Python Index å®‰è£… TensorBoardã€‚`pip install tensorboard`
- en: '**Using TensorBoard with Jupyter notebooks and Google Colab**'
  id: totrans-1100
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**åœ¨ Jupyter ç¬”è®°æœ¬å’Œ Google Colab ä¸­ä½¿ç”¨ TensorBoard**'
- en: Once TensorBoard is installed, you need to load it in your environment, usually
    Google Colab or your local notebook.`%load_ext tensorboard`Next, inform TensorBoard
    which folder will contain the log information.`log_folder = "runs"`
  id: totrans-1101
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å®‰è£…äº† TensorBoard åï¼Œæ‚¨éœ€è¦åœ¨ç¯å¢ƒä¸­åŠ è½½å®ƒï¼Œé€šå¸¸æ˜¯åœ¨ Google Colab æˆ–æœ¬åœ°ç¬”è®°æœ¬ä¸­ã€‚ `%load_ext tensorboard`
    æ¥ä¸‹æ¥ï¼Œå‘Šè¯‰ TensorBoard å“ªä¸ªæ–‡ä»¶å¤¹å°†åŒ…å«æ—¥å¿—ä¿¡æ¯ã€‚ `log_folder = "runs"`
- en: '**How to launch TensorBoard**'
  id: totrans-1102
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**å¦‚ä½•å¯åŠ¨ TensorBoard**'
- en: Tensorboard is launched using theÂ `[tensorboard]`Â magic command in notebook
    environments while specifying theÂ `[logdir]`. `%tensorboard --logdir={log_folder}`
  id: totrans-1103
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: Tensorboard ä½¿ç”¨ `[tensorboard]` é­”æœ¯å‘½ä»¤åœ¨ç¬”è®°æœ¬ç¯å¢ƒä¸­å¯åŠ¨ï¼ŒåŒæ—¶æŒ‡å®š `[logdir]`ã€‚ `%tensorboard
    --logdir={log_folder}`
- en: 'You can also launch TensorBoard on the command line using a similar pattern.
    Apart from viewing the terminal on the notebook environment, you can also view
    it on the browser by visiting: http://localhost:6006.'
  id: totrans-1104
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ç±»ä¼¼çš„æ¨¡å¼åœ¨å‘½ä»¤è¡Œä¸Šå¯åŠ¨ TensorBoardã€‚é™¤äº†åœ¨ç¬”è®°æœ¬ç¯å¢ƒä¸­æŸ¥çœ‹ç»ˆç«¯å¤–ï¼Œæ‚¨è¿˜å¯ä»¥é€šè¿‡è®¿é—®ä»¥ä¸‹åœ°å€åœ¨æµè§ˆå™¨ä¸­æŸ¥çœ‹ï¼šhttp://localhost:6006ã€‚
- en: '**Tensorboard dashboards**'
  id: totrans-1105
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**Tensorboard ä»ªè¡¨æ¿**'
- en: TensorBoard has various dashboards for showing different types of information.
  id: totrans-1106
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: TensorBoard æ‹¥æœ‰å„ç§ä»ªè¡¨æ¿ï¼Œç”¨äºæ˜¾ç¤ºä¸åŒç±»å‹çš„ä¿¡æ¯ã€‚
- en: TheÂ  **Scalars**Â dashboard tracks numerical information such as training metrics
    per epoch. You can use it to track other scalar values such as model training
    speed and learning rate.
  id: totrans-1107
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '**Scalars** ä»ªè¡¨æ¿è·Ÿè¸ªæ•°å­—ä¿¡æ¯ï¼Œå¦‚æ¯ä¸ª epoch çš„è®­ç»ƒæŒ‡æ ‡ã€‚æ‚¨å¯ä»¥ä½¿ç”¨å®ƒæ¥è·Ÿè¸ªæ¨¡å‹è®­ç»ƒé€Ÿåº¦å’Œå­¦ä¹ ç‡ç­‰å…¶ä»–æ ‡é‡å€¼ã€‚'
- en: TheÂ **Graphs**Â dashboard is used for showing visualizations. For example, you
    can use it to check the architecture of the network.
  id: totrans-1108
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '**Graphs** ä»ªè¡¨æ¿ç”¨äºæ˜¾ç¤ºå¯è§†åŒ–ã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å®ƒæ¥æ£€æŸ¥ç½‘ç»œçš„æ¶æ„ã€‚'
- en: TheÂ  **Distributions and Histograms**Â dashboard show the distribution of tensors
    over time. Use it to check the weights and biases of the network.
  id: totrans-1109
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '**Distributions and Histograms** ä»ªè¡¨æ¿æ˜¾ç¤ºå¼ é‡éšæ—¶é—´çš„åˆ†å¸ƒã€‚ç”¨å®ƒæ¥æ£€æŸ¥ç½‘ç»œçš„æƒé‡å’Œåç½®ã€‚'
- en: TheÂ **Images**Â dashboard shows the images you have logged to TensorBoard.
  id: totrans-1110
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '**Images** ä»ªè¡¨æ¿æ˜¾ç¤ºæ‚¨å·²è®°å½•åˆ° TensorBoard çš„å›¾åƒã€‚'
- en: TheÂ **HParams**Â dashboard visualizes hyperparameter optimization. It helps identify
    the best parameters for the network.
  id: totrans-1111
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '**HParams** ä»ªè¡¨æ¿å¯è§†åŒ–è¶…å‚æ•°ä¼˜åŒ–ã€‚å®ƒå¸®åŠ©ç¡®å®šç½‘ç»œçš„æœ€ä½³å‚æ•°ã€‚'
- en: TheÂ  **Embedding Projector**Â is used to visualize low-level embeddings, for
    example, text embeddings.
  id: totrans-1112
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '**Embedding Projector** ç”¨äºå¯è§†åŒ–ä½çº§åµŒå…¥ï¼Œä¾‹å¦‚æ–‡æœ¬åµŒå…¥ã€‚'
- en: TheÂ **What-If Tool**Â dashboard helps in understanding the performance of a model.
    It also enables the measurement of a model's fairness on data subsets.
  id: totrans-1113
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '**What-If Tool** ä»ªè¡¨æ¿å¸®åŠ©ç†è§£æ¨¡å‹çš„æ€§èƒ½ã€‚å®ƒè¿˜èƒ½å¤Ÿåœ¨æ•°æ®å­é›†ä¸Šæµ‹é‡æ¨¡å‹çš„å…¬å¹³æ€§ã€‚'
- en: TheÂ  **TensorFlow Profiler**Â monitors the model training process. It also shows
    the events in the CPU and GPU during training. The TensorFlow profiler goes further
    to offer recommendations based on the data collected. You can also use it to debug
    performance issues in the input pipeline.
  id: totrans-1114
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '**TensorFlow Profiler** ç›‘æ§æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ã€‚å®ƒè¿˜æ˜¾ç¤ºäº†è®­ç»ƒæœŸé—´ CPU å’Œ GPU ä¸Šçš„äº‹ä»¶ã€‚TensorFlow åˆ†æå™¨è¿›ä¸€æ­¥æ ¹æ®æ”¶é›†çš„æ•°æ®æä¾›å»ºè®®ã€‚æ‚¨è¿˜å¯ä»¥ä½¿ç”¨å®ƒæ¥è°ƒè¯•è¾“å…¥ç®¡é“ä¸­çš„æ€§èƒ½é—®é¢˜ã€‚'
- en: '**How to use TensorBoard with Flax**'
  id: totrans-1115
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**å¦‚ä½•åœ¨ Flax ä¸­ä½¿ç”¨ TensorBoard**'
- en: With TensorBoard installed and some basics out of the way, let's look at how
    you can use it inÂ Flax. Let's use
  id: totrans-1116
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å®‰è£…äº† TensorBoard å¹¶å®Œæˆäº†ä¸€äº›åŸºç¡€è®¾ç½®åï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åœ¨ Flax ä¸­ä½¿ç”¨å®ƒã€‚ è®©æˆ‘ä»¬ä½¿ç”¨
- en: theÂ `[SummaryWriter]`Â fromÂ PyTorchÂ to write to the log folder.
  id: totrans-1117
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ PyTorch ä¸­çš„ `[SummaryWriter]` å‘æ—¥å¿—æ–‡ä»¶å¤¹å†™å…¥ã€‚
- en: '**How to log images with TensorBoard in Flax**'
  id: totrans-1118
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**å¦‚ä½•åœ¨ Flax ä¸­ä½¿ç”¨ TensorBoard è®°å½•å›¾åƒ**'
- en: You may want to log sample images when solving computer vision problems. You
    can also log predictions while training the model. For example, you can log prediction
    images containing bounding boxes for an object detection network.
  id: totrans-1119
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨è§£å†³è®¡ç®—æœºè§†è§‰é—®é¢˜æ—¶ï¼Œæ‚¨å¯èƒ½å¸Œæœ›è®°å½•æ ·æœ¬å›¾åƒã€‚æ‚¨è¿˜å¯ä»¥åœ¨è®­ç»ƒæ¨¡å‹æ—¶è®°å½•é¢„æµ‹ã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥è®°å½•åŒ…å«å¯¹è±¡æ£€æµ‹ç½‘ç»œè¾¹ç•Œæ¡†çš„é¢„æµ‹å›¾åƒã€‚
- en: 'Let''s look at how we can log an image to TensorBoard.from`torch.utils.tensorboard`import`SummaryWriter`import`torchvision.transforms.functional`as`Fwriter
    = SummaryWriter(logdir)`def show(imgs):if not isinstance(imgs, list):'
  id: totrans-1120
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•å°†å›¾åƒè®°å½•åˆ° TensorBoardã€‚ from `torch.utils.tensorboard` import `SummaryWriter`
    import `torchvision.transforms.functional` as `Fwriter = SummaryWriter(logdir)`
    def show(imgs): if not isinstance(imgs, list):'
- en: '`imgs = [imgs]`'
  id: totrans-1121
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`imgs = [imgs]`'
- en: '`fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)` for i, img in enumerate(imgs):'
  id: totrans-1122
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)` for i, img in enumerate(imgs):'
- en: '`img = img.detach()`'
  id: totrans-1123
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`img = img.detach()`'
- en: '`img = F.to_pil_image(img)`'
  id: totrans-1124
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`img = F.to_pil_image(img)`'
- en: '`axs[0, i].imshow(np.asarray(img))`'
  id: totrans-1125
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`axs[0, i].imshow(np.asarray(img))`'
- en: '`axs[0, i].set(xticklabels=[], yticklabels=[], xticks=`'
  id: totrans-1126
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`axs[0, i].set(xticklabels=[], yticklabels=[], xticks=`'
- en: '[], yticks=[])writer.flush() # Ensure that everything is written to diskNext,
    create a grid with the images that will be logged.'
  id: totrans-1127
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '[], yticks=[])writer.flush() # ç¡®ä¿æ‰€æœ‰å†…å®¹éƒ½å·²å†™å…¥ç£ç›˜æ¥ä¸‹æ¥ï¼Œåˆ›å»ºå°†è®°å½•çš„å›¾åƒçš„ç½‘æ ¼ã€‚'
- en: from`torchvision.utils`import`make_grid`from`torchvision.io`import`read_image`from`pathlib`import`Path`
  id: totrans-1128
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: from`torchvision.utils`import`make_grid`from`torchvision.io`import`read_image`from`pathlib`import`Path`
- en: '`cat = read_image(str(Path(''train'') / ''cat.1.jpg'')) grid = make_grid(cat)`'
  id: totrans-1129
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`cat = read_image(str(Path(''train'') / ''cat.1.jpg'')) grid = make_grid(cat)`'
- en: '`show(grid)`'
  id: totrans-1130
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`show(grid)`'
- en: TheÂ `[add_image]`Â function is used to write images to TensorBoard.`writer.add_image('sample_cat',
    grid)`Now, load the TensorBoard extension and point it to the logs folder.`%tensorboard
    --logdir={logdir}`The logged images will be visible on the Images dashboard. ![](../images/00034.gif)**TensorBoard
    Image dashboard ** **## How to log text with TensorBoard in Flax** Writing text
    to TensorBoard is done using theÂ `[add_text]`Â function.`writer.add_text('Text',
    'Write image to TensorBoard', 0)` The logged data is available on the Text dashboard.![](../images/00035.jpeg)
  id: totrans-1131
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`[add_image]`å‡½æ•°å°†å›¾åƒå†™å…¥TensorBoardã€‚`writer.add_image('sample_cat', grid)`ç°åœ¨ï¼ŒåŠ è½½TensorBoardæ‰©å±•å¹¶å°†å…¶æŒ‡å‘æ—¥å¿—æ–‡ä»¶å¤¹ã€‚`%tensorboard
    --logdir={logdir}`è®°å½•çš„å›¾åƒå°†åœ¨å›¾åƒä»ªè¡¨æ¿ä¸Šå¯è§ã€‚![](../images/00034.gif)**TensorBoard å›¾åƒä»ªè¡¨æ¿**
    **## å¦‚ä½•åœ¨Flaxä¸­ä½¿ç”¨TensorBoardè®°å½•æ–‡æœ¬** ä½¿ç”¨`[add_text]`å‡½æ•°å‘TensorBoardå†™å…¥æ–‡æœ¬ã€‚`writer.add_text('Text',
    'Write image to TensorBoard', 0)`è®°å½•çš„æ•°æ®åœ¨æ–‡æœ¬ä»ªè¡¨æ¿ä¸Šå¯ç”¨ã€‚![](../images/00035.jpeg)
- en: '**Track model training in JAX using TensorBoard**'
  id: totrans-1132
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**ä½¿ç”¨TensorBoardè·Ÿè¸ªJAXæ¨¡å‹è®­ç»ƒ**'
- en: You can log the evaluation metrics whenÂ training machine learning modelsÂ with
    JAX. They obtained at the training stage. At this point, you can log the metrics
    to TensorBoard. In the example below, we log the training and evaluation metrics.
  id: totrans-1133
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹æ—¶ï¼Œå¯ä»¥è®°å½•è¯„ä¼°æŒ‡æ ‡ã€‚å®ƒä»¬åœ¨è®­ç»ƒé˜¶æ®µè·å–ã€‚æ­¤æ—¶ï¼Œæ‚¨å¯ä»¥å°†æŒ‡æ ‡è®°å½•åˆ°TensorBoardã€‚åœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬è®°å½•è®­ç»ƒå’Œè¯„ä¼°æŒ‡æ ‡ã€‚
- en: '`for epoch in range(1, num_epochs + 1):`'
  id: totrans-1134
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`for epoch in range(1, num_epochs + 1):`'
- en: '`train_state, train_metrics = train_one_epoch(state, train_l`'
  id: totrans-1135
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_state, train_metrics = train_one_epoch(state, train_l`'
- en: '`oader)`'
  id: totrans-1136
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`oader)`'
- en: '`training_loss.append(train_metrics[''loss''])`'
  id: totrans-1137
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`training_loss.append(train_metrics[''loss''])`'
- en: '`training_accuracy.append(train_metrics[''accuracy'']) print(f"Train epoch:
    {epoch}, loss: {train_metrics[''los`'
  id: totrans-1138
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`training_accuracy.append(train_metrics[''accuracy'']) print(f"Train epoch:
    {epoch}, loss: {train_metrics[''los`'
- en: 's'']}, accuracy: {train_metrics[''accuracy''] * 100}")'
  id: totrans-1139
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 's'']}, å‡†ç¡®ç‡: {train_metrics[''accuracy''] * 100}")'
- en: '`test_metrics = evaluate_model(train_state, test_images, tes t_labels)`'
  id: totrans-1140
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`test_metrics = evaluate_model(train_state, test_images, tes t_labels)`'
- en: '`testing_loss.append(test_metrics[''loss''])`'
  id: totrans-1141
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_loss.append(test_metrics[''loss''])`'
- en: '`testing_accuracy.append(test_metrics[''accuracy''])`'
  id: totrans-1142
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_accuracy.append(test_metrics[''accuracy''])`'
- en: '`writer.add_scalar(''Loss/train'', train_metrics[''loss''], epoc h)`'
  id: totrans-1143
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`writer.add_scalar(''Loss/train'', train_metrics[''loss''], epoc h)`'
- en: '`writer.add_scalar(''Loss/test'', test_metrics[''loss''], epoch)`'
  id: totrans-1144
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`writer.add_scalar(''Loss/test'', test_metrics[''loss''], epoch)`'
- en: '`writer.add_scalar(''Accuracy/train'', train_metrics[''accuracy''], epoch)`writer.add_scalar(''Accuracy/test'',
    test_metrics[''accuracy''], epoch)print(f"Test epoch: {epoch}, loss: {test_metrics[''loss'']},
    accuracy: {test_metrics[''accuracy''] * 100}")These metrics will be available
    on theÂ **Scalars**Â dashboard of TensorBoard.![](../images/00036.gif)'
  id: totrans-1145
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`writer.add_scalar(''Accuracy/train'', train_metrics[''accuracy''], epoch)`writer.add_scalar(''Accuracy/test'',
    test_metrics[''accuracy''], epoch)print(f"Test epoch: {epoch}, loss: {test_metrics[''loss'']},
    accuracy: {test_metrics[''accuracy''] * 100}")è¿™äº›æŒ‡æ ‡å°†åœ¨TensorBoardçš„**Scalars**ä»ªè¡¨æ¿ä¸Šå¯ç”¨ã€‚![](../images/00036.gif)'
- en: '**How to profile JAX programs with TensorBoard**'
  id: totrans-1146
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**å¦‚ä½•ä½¿ç”¨TensorBoardåˆ†æJAXç¨‹åº**'
- en: To profileJAX programs, send data to the TensorBoard profiler. The first step
    is to install the profile plugin.`pip install -U tensorboard-plugin-profile`
  id: totrans-1147
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¦åˆ†æJAXç¨‹åºï¼Œè¯·å°†æ•°æ®å‘é€åˆ°TensorBoardåˆ†æå™¨ã€‚ç¬¬ä¸€æ­¥æ˜¯å®‰è£…åˆ†ææ’ä»¶ã€‚`pip install -U tensorboard-plugin-profile`
- en: '**Programmatic profiling**'
  id: totrans-1148
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**ç¨‹åºåŒ–åˆ†æ**'
- en: UseÂ `jax.profiler.start_trace()`Â to start a trace
  id: totrans-1149
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`jax.profiler.start_trace()`æ¥å¯åŠ¨è·Ÿè¸ª
- en: andÂ `jax.profiler.stop_trace()`Â to stop a trace.
  id: totrans-1150
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å’Œ`jax.profiler.stop_trace()`æ¥åœæ­¢è·Ÿè¸ªã€‚
- en: The `[start_trace()]` expects the path to the directory where the traces will
    be written.`import jax`jax.profiler.start_trace("runs")
  id: totrans-1151
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[start_trace()]`æœŸæœ›å°†è·Ÿè¸ªå†™å…¥çš„ç›®å½•è·¯å¾„ã€‚`import jax`jax.profiler.start_trace("runs")'
- en: Run the operations to be profiled `key = jax.random.PRNGKey(0)`
  id: totrans-1152
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: è¿è¡Œè¦åˆ†æçš„æ“ä½œ `key = jax.random.PRNGKey(0)`
- en: '`x = jax.random.normal(key, (5000, 5000)) y = x @ x`'
  id: totrans-1153
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = jax.random.normal(key, (5000, 5000)) y = x @ x`'
- en: '`y.block_until_ready()`'
  id: totrans-1154
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`y.block_until_ready()`'
- en: '`jax.profiler.stop_trace()`'
  id: totrans-1155
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jax.profiler.stop_trace()`'
- en: '**Manual profiling with `TensorBoard`**'
  id: totrans-1156
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**ä½¿ç”¨`TensorBoard`è¿›è¡Œæ‰‹åŠ¨åˆ†æ**'
- en: 'The second option is to profile theÂ `JAX`Â program manually. `![](../images/00037.jpeg)`This
    is done in the following steps:'
  id: totrans-1157
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ç¬¬äºŒä¸ªé€‰é¡¹æ˜¯æ‰‹åŠ¨åˆ†æ`JAX`ç¨‹åºã€‚ `![](../images/00037.jpeg)`ä»¥ä¸‹æ˜¯æ“ä½œæ­¥éª¤ï¼š
- en: Initialize `TensorBoard`Â `tensorboard --logdir /runs`Start a `JAX` profiler
    server at the beginning of the program and stop the server at the end of the program.
  id: totrans-1158
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨ç¨‹åºå¼€å§‹æ—¶åˆå§‹åŒ–`TensorBoard`ï¼Œä½¿ç”¨`tensorboard --logdir /runs`å¯åŠ¨ä¸€ä¸ª`JAX`æ€§èƒ½åˆ†ææœåŠ¡å™¨ï¼Œå¹¶åœ¨ç¨‹åºç»“æŸæ—¶åœæ­¢æœåŠ¡å™¨ã€‚
- en: import `jax.profiler`
  id: totrans-1159
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å¯¼å…¥`jax.profiler`ã€‚
- en: '`jax.profiler.start_server(9999)`'
  id: totrans-1160
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jax.profiler.start_server(9999)`ã€‚'
- en: '`train_one_epoch(state, train_loader,num_epochs)`'
  id: totrans-1161
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_one_epoch(state, train_loader,num_epochs)`ã€‚'
- en: '`jax.profiler.stop_server()`'
  id: totrans-1162
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`jax.profiler.stop_server()`ã€‚'
- en: Open the Profile dashboard ofÂ `TensorBoard`. ClickÂ  **CAPTURE PROFILE**Â and
    enter the URL of the server that you started above, in this case localhost:9999\.
    Click CAPTURE to start profiling.
  id: totrans-1163
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ‰“å¼€`TensorBoard`çš„Profileä»ªè¡¨æ¿ã€‚ç‚¹å‡» **CAPTURE PROFILE**ï¼Œè¾“å…¥ä¸Šè¿°å¯åŠ¨æœåŠ¡å™¨çš„ URLï¼Œä¾‹å¦‚ localhost:9999ã€‚ç‚¹å‡»
    CAPTURE å¼€å§‹æ€§èƒ½åˆ†æã€‚
- en: SelectÂ  **trace_viewer**Â underÂ **Tools**Â on the profile dashboard. Use the navigation
    tools here to click specific events to see more information about them.
  id: totrans-1164
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨Profileä»ªè¡¨æ¿ä¸Šçš„**å·¥å…·**ä¸‹é€‰æ‹© **trace_viewer**ã€‚ä½¿ç”¨å¯¼èˆªå·¥å…·ç‚¹å‡»ç‰¹å®šäº‹ä»¶ä»¥æŸ¥çœ‹æ›´å¤šä¿¡æ¯ã€‚
- en: '**How to profile `JAX` program on a remote machine**'
  id: totrans-1165
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**å¦‚ä½•åœ¨è¿œç¨‹æœºå™¨ä¸Šå¯¹`JAX`ç¨‹åºè¿›è¡Œæ€§èƒ½åˆ†æ**ã€‚'
- en: You can profile a `JAX` program on a remote server by executing the above instructions
    on the remote server. This involves starting a `TensorBoard` server on the remote
    machine, and port forwarding it to your local machine. You will then access `TensorBoard`
    locally via the web UI.
  id: totrans-1166
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥é€šè¿‡åœ¨è¿œç¨‹æœåŠ¡å™¨ä¸Šæ‰§è¡Œä¸Šè¿°æŒ‡ä»¤æ¥å¯¹ä¸€ä¸ª`JAX`ç¨‹åºè¿›è¡Œæ€§èƒ½åˆ†æã€‚è¿™æ¶‰åŠåœ¨è¿œç¨‹æœºå™¨ä¸Šå¯åŠ¨`TensorBoard`æœåŠ¡å™¨ï¼Œå¹¶å°†å…¶ç«¯å£è½¬å‘åˆ°æœ¬åœ°æœºå™¨ã€‚ç„¶åï¼Œä½ å¯ä»¥é€šè¿‡
    web UI åœ¨æœ¬åœ°è®¿é—®`TensorBoard`ã€‚
- en: '`ssh -L 6006:localhost:6006 <remote server address>`'
  id: totrans-1167
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`ssh -L 6006:localhost:6006 <remote server address>`ã€‚'
- en: '**Share `TensorBoard` dashboards**'
  id: totrans-1168
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**åˆ†äº«`TensorBoard`ä»ªè¡¨æ¿**ã€‚'
- en: '`TensorBoard.dev`Â is a hosted version of `TensorBoard` that makes it easy to
    share your experiments. Let''s upload the above `TensorBoard` to `TensorBoard.dev`.'
  id: totrans-1169
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`TensorBoard.dev` æ˜¯`TensorBoard`çš„æ‰˜ç®¡ç‰ˆæœ¬ï¼Œæ–¹ä¾¿åˆ†äº«ä½ çš„å®éªŒã€‚è®©æˆ‘ä»¬å°†ä¸Šè¿°`TensorBoard`ä¸Šä¼ åˆ°`TensorBoard.dev`ã€‚'
- en: On Colab or Jupyter nmotebook
  id: totrans-1170
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨Colabæˆ–Jupyterç¬”è®°æœ¬ä¸Š
- en: '`!tensorboard dev upload --logdir ./runs \`'
  id: totrans-1171
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`!tensorboard dev upload --logdir ./runs \`'
- en: '`--name "Flax experiments" \`'
  id: totrans-1172
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`--name "Flax experiments" \`'
- en: '`--description "Logging model metrics with JAX" \`'
  id: totrans-1173
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`--description "Logging model metrics with JAX" \`'
- en: '`--one_shot`'
  id: totrans-1174
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`--one_shot`ã€‚'
- en: When you run the above code, you will get a prompt to authorize the upload.
    You should be keen not to share sensitive data because `TensorBoard.dev` experiments
    are public.
  id: totrans-1175
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å½“ä½ è¿è¡Œä»¥ä¸Šä»£ç æ—¶ï¼Œä½ å°†æ”¶åˆ°ä¸€ä¸ªæˆæƒä¸Šä¼ çš„æç¤ºã€‚è¯·æ³¨æ„ä¸è¦åˆ†äº«æ•æ„Ÿæ•°æ®ï¼Œå› ä¸º`TensorBoard.dev`å®éªŒæ˜¯å…¬å¼€çš„ã€‚
- en: You can view the experiment onÂ `TensorBoard.dev`.
  id: totrans-1176
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åœ¨`TensorBoard.dev`ä¸ŠæŸ¥çœ‹å®éªŒã€‚
- en: '`![](../images/00038.jpeg)`'
  id: totrans-1177
  prefs: []
  stylish: true
  type: TYPE_IMG
  zh: '`![](../images/00038.jpeg)`ã€‚'
- en: '**Final thoughts**'
  id: totrans-1178
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**æœ€åçš„æ€è€ƒ**ã€‚'
- en: 'In this article, we have seen how you can use `TensorBoard` to log your experiments
    in `Flax`. More specifically, you have learned: What is `TensorBoard`?'
  id: totrans-1179
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å·²ç»çœ‹åˆ°å¦‚ä½•ä½¿ç”¨`TensorBoard`æ¥è®°å½•ä½ åœ¨`Flax`ä¸­çš„å®éªŒã€‚æ›´å…·ä½“åœ°è¯´ï¼Œä½ å­¦åˆ°äº†ï¼šä»€ä¹ˆæ˜¯`TensorBoard`ï¼Ÿ
- en: How to install and launch `TensorBoard`.
  id: totrans-1180
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å¦‚ä½•å®‰è£…å’Œå¯åŠ¨`TensorBoard`ã€‚
- en: How to log images and text to `TensorBoard`.
  id: totrans-1181
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å¦‚ä½•å°†å›¾åƒå’Œæ–‡æœ¬æ—¥å¿—è®°å½•åˆ°`TensorBoard`ã€‚
- en: How to log model metrics to `TensorBoard`.
  id: totrans-1182
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å¦‚ä½•å°†æ¨¡å‹æŒ‡æ ‡è®°å½•åˆ°`TensorBoard`ã€‚
- en: How to profile `JAX` and `Flax` programs using `TensorBoard` How to upload the
    log to `TensorBoard.dev`.
  id: totrans-1183
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å¦‚ä½•ä½¿ç”¨`TensorBoard`æ¥å¯¹`JAX`å’Œ`Flax`ç¨‹åºè¿›è¡Œæ€§èƒ½åˆ†æã€‚å¦‚ä½•å°†æ—¥å¿—ä¸Šä¼ åˆ°`TensorBoard.dev`ã€‚
- en: '**Handling state in `JAX` & `Flax` (BatchNorm and DropOut layers)**'
  id: totrans-1184
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**å¤„ç†`JAX`å’Œ`Flax`ä¸­çš„çŠ¶æ€ï¼ˆBatchNormå’ŒDropOutå±‚ï¼‰**ã€‚'
- en: Jitting functions in `Flax` makes them faster but requires that the functions
    have no side effects. The fact that jitted functions can't have side effects introduces
    a challenge when dealing with stateful items such as model parameters and stateful
    layers such as batch normalization layers.
  id: totrans-1185
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨`Flax`ä¸­å¯¹å‡½æ•°è¿›è¡Œ`JIT`ç¼–è¯‘å¯ä»¥ä½¿å…¶è¿è¡Œæ›´å¿«ï¼Œä½†è¦æ±‚å‡½æ•°æ²¡æœ‰å‰¯ä½œç”¨ã€‚`JIT`å‡½æ•°ä¸èƒ½æœ‰å‰¯ä½œç”¨çš„äº‹å®åœ¨å¤„ç†çŠ¶æ€é¡¹ï¼ˆå¦‚æ¨¡å‹å‚æ•°ï¼‰å’ŒçŠ¶æ€å±‚ï¼ˆå¦‚æ‰¹é‡å½’ä¸€åŒ–å±‚ï¼‰æ—¶å¸¦æ¥äº†æŒ‘æˆ˜ã€‚
- en: In this article, we'll create a network with the BatchNorm and DropOut layers.
    After that, we'll see how to deal with generating the random number for the DropOut
    layer and adding the batch statistics when training the network.
  id: totrans-1186
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªå¸¦æœ‰BatchNormå’ŒDropOutå±‚çš„ç½‘ç»œã€‚ç„¶åï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•å¤„ç†ç”ŸæˆDropOutå±‚çš„éšæœºæ•°ä»¥åŠåœ¨è®­ç»ƒç½‘ç»œæ—¶æ·»åŠ æ‰¹æ¬¡ç»Ÿè®¡ä¿¡æ¯ã€‚
- en: '**Perform standard imports**'
  id: totrans-1187
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**æ‰§è¡Œæ ‡å‡†å¯¼å…¥æ“ä½œ**ã€‚'
- en: We kick off by importing standard data science packages that we'll use in this
    article.
  id: totrans-1188
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»å¯¼å…¥æœ¬æ–‡ä¸­å°†è¦ä½¿ç”¨çš„æ ‡å‡†æ•°æ®ç§‘å­¦åŒ…å¼€å§‹ã€‚
- en: '`import torch`'
  id: totrans-1189
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import torch`ã€‚'
- en: '`from torch.utils.data import DataLoader import os`'
  id: totrans-1190
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from torch.utils.data import DataLoader import os`ã€‚'
- en: '`from PIL import Image`'
  id: totrans-1191
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from PIL import Image`ã€‚'
- en: '`from torchvision import transforms from torch.utils.data import Dataset import
    numpy as np`'
  id: totrans-1192
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from torchvision import transforms from torch.utils.data import Dataset import
    numpy as np`ã€‚'
- en: '`import pandas as pd`'
  id: totrans-1193
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import pandas as pd`ã€‚'
- en: '`from typing import Any`'
  id: totrans-1194
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from typing import Any`ã€‚'
- en: '`import matplotlib.pyplot as plt`'
  id: totrans-1195
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import matplotlib.pyplot as plt`'
- en: '`%matplotlib inline`'
  id: totrans-1196
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`%matplotlib inline`'
- en: ignore harmless warnings
  id: totrans-1197
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: å¿½ç•¥æ— å®³çš„è­¦å‘Š
- en: '`import warnings`'
  id: totrans-1198
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import warnings`'
- en: '`warnings.filterwarnings("ignore") import jax`'
  id: totrans-1199
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`warnings.filterwarnings("ignore") import jax`'
- en: '`from jax import numpy as jnp`'
  id: totrans-1200
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from jax import numpy as jnp`'
- en: '`import flax`'
  id: totrans-1201
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import flax`'
- en: '`from flax import linen as nn`'
  id: totrans-1202
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from flax import linen as nn`'
- en: '`from flax.training import train_state import optax`'
  id: totrans-1203
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from flax.training import train_state import optax`'
- en: '**Download the dataset**'
  id: totrans-1204
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**ä¸‹è½½æ•°æ®é›†**'
- en: Let's illustrate how to include BatchNorm and DropOut layers in a Flax network
    by designing a simpleÂ Convolutional Neural NetworkÂ using theÂ cat and dogs datasetÂ from
    Kaggle.
  id: totrans-1205
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é€šè¿‡è®¾è®¡ä¸€ä¸ªç®€å•çš„ä½¿ç”¨Kaggleçš„çŒ«å’Œç‹—æ•°æ®é›†çš„å·ç§¯ç¥ç»ç½‘ç»œæ¥è¯´æ˜å¦‚ä½•åœ¨Flaxç½‘ç»œä¸­åŒ…å«BatchNormå’ŒDropOutå±‚ã€‚
- en: Download and extract the dataset.
  id: totrans-1206
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä¸‹è½½å¹¶æå–æ•°æ®é›†ã€‚
- en: '`import wget`'
  id: totrans-1207
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import wget`'
- en: '`wget.download("https://ml.machinelearningnuggets.com/train.zi p")`'
  id: totrans-1208
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`wget.download("https://ml.machinelearningnuggets.com/train.zi p")`'
- en: '`import zipfilewith zipfile.ZipFile(''train.zip'', ''r'') as zip_ref: zip_ref.extractall(''.'')`'
  id: totrans-1209
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import zipfilewith zipfile.ZipFile(''train.zip'', ''r'') as zip_ref: zip_ref.extractall(''.'')`'
- en: '**Loading datasets in JAX**'
  id: totrans-1210
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**åœ¨JAXä¸­åŠ è½½æ•°æ®é›†**'
- en: Since JAX doesn't ship with data loading tools, load the dataset using PyTorch.
    We start by creating a PyTorch Dataset class.
  id: totrans-1211
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ç”±äºJAXä¸åŒ…å«æ•°æ®åŠ è½½å·¥å…·ï¼Œä½¿ç”¨PyTorchåŠ è½½æ•°æ®é›†ã€‚æˆ‘ä»¬é¦–å…ˆåˆ›å»ºä¸€ä¸ªPyTorchæ•°æ®é›†ç±»ã€‚
- en: '`class CatsDogsDataset(Dataset):`'
  id: totrans-1212
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`class CatsDogsDataset(Dataset):`'
- en: '`def __init__(self, root_dir, annotation_file, transform=Non`'
  id: totrans-1213
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def __init__(self, root_dir, annotation_file, transform=Non`'
- en: '`e):`'
  id: totrans-1214
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`e):`'
- en: '`self.root_dir = root_dir`'
  id: totrans-1215
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.root_dir = root_dir`'
- en: '`self.annotations = pd.read_csv(annotation_file) self.transform = transform`'
  id: totrans-1216
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.annotations = pd.read_csv(annotation_file) self.transform = transform`'
- en: '`def __len__(self):return len(self.annotations)`'
  id: totrans-1217
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def __len__(self):return len(self.annotations)`'
- en: '`def __getitem__(self, index):`'
  id: totrans-1218
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def __getitem__(self, index):`'
- en: '`img_id = self.annotations.iloc[index, 0]`'
  id: totrans-1219
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`img_id = self.annotations.iloc[index, 0]`'
- en: '`img = Image.open(os.path.join(self.root_dir, img_id)).c`'
  id: totrans-1220
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`img = Image.open(os.path.join(self.root_dir, img_id)).c`'
- en: '`onvert("RGB")`'
  id: totrans-1221
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`onvert("RGB")`'
- en: '`y_label = torch.tensor(float(self.annotations.iloc[inde`'
  id: totrans-1222
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`y_label = torch.tensor(float(self.annotations.iloc[inde`'
- en: '`x, 1]))`'
  id: totrans-1223
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x, 1]))`'
- en: 'if self.transform is not None: `img = self.transform(img)return (img, y_label)`Interested
    in learning more about loading datasets in JAX?ğŸ‘‰ Check ourÂ How to load datasets
    in JAX with TensorFlowÂ tutorial.'
  id: totrans-1224
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å¦‚æœ`self.transform`ä¸ä¸º`None`ï¼š`img = self.transform(img)return (img, y_label)`æœ‰å…´è¶£äº†è§£å¦‚ä½•åœ¨JAXä¸­åŠ è½½æ•°æ®é›†ï¼ŸğŸ‘‰æŸ¥çœ‹æˆ‘ä»¬çš„ã€Šå¦‚ä½•ä½¿ç”¨TensorFlowåœ¨JAXä¸­åŠ è½½æ•°æ®é›†ã€‹æ•™ç¨‹ã€‚
- en: Next, create aÂ `Pandas DataFrame`Â containing the image path and the labels.
  id: totrans-1225
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œåˆ›å»ºä¸€ä¸ªåŒ…å«å›¾åƒè·¯å¾„å’Œæ ‡ç­¾çš„`Pandas DataFrame`ã€‚
- en: '`train_df = pd.DataFrame(columns=["img_path","label"]) train_df["img_path"]
    = os.listdir("train/") for idx, i in enumerate(os.listdir("train/")):`'
  id: totrans-1226
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_df = pd.DataFrame(columns=["img_path","label"]) train_df["img_path"]
    = os.listdir("train/") for idx, i in enumerate(os.listdir("train/")):`'
- en: 'if `"cat"` in `i`:'
  id: totrans-1227
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å¦‚æœ`"cat"`åœ¨`i`ä¸­ï¼š
- en: '`train_df["label"][idx] = 0`'
  id: totrans-1228
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_df["label"][idx] = 0`'
- en: 'if `"dog"` in `i`:'
  id: totrans-1229
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å¦‚æœ`"dog"`åœ¨`i`ä¸­ï¼š
- en: '`train_df["label"][idx] = 1`'
  id: totrans-1230
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_df["label"][idx] = 1`'
- en: '`train_df.to_csv (r''train_csv.csv'', index = False, header=True)`'
  id: totrans-1231
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_df.to_csv (r''train_csv.csv'', index = False, header=True)`'
- en: '**Data processing with PyTorch**'
  id: totrans-1232
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**ä½¿ç”¨PyTorchè¿›è¡Œæ•°æ®å¤„ç†**'
- en: Next, create a function to stack the dataset and return it as aÂ `NumPy array`.
  id: totrans-1233
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œåˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥å †å æ•°æ®é›†å¹¶å°†å…¶ä½œä¸º`NumPy array`è¿”å›ã€‚
- en: '`def custom_collate_fn(batch):`'
  id: totrans-1234
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def custom_collate_fn(batch):`'
- en: '`transposed_data = list(zip(*batch)) labels = np.array(transposed_data[1])
    imgs = np.stack(transposed_data[0]) return imgs, labels`'
  id: totrans-1235
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`transposed_data = list(zip(*batch)) labels = np.array(transposed_data[1])
    imgs = np.stack(transposed_data[0]) return imgs, labels`'
- en: We then use PyTorch to create training and testing data loaders.`size_image
    = 224 batch_size = 64`
  id: totrans-1236
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨PyTorchåˆ›å»ºè®­ç»ƒå’Œæµ‹è¯•æ•°æ®åŠ è½½å™¨ã€‚`size_image = 224 batch_size = 64`
- en: '`transform = transforms.Compose([`'
  id: totrans-1237
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`transform = transforms.Compose([`'
- en: '`transforms.Resize((size_image,size_image)),`'
  id: totrans-1238
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`transforms.Resize((size_image,size_image)),`'
- en: '`np.array])`'
  id: totrans-1239
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`np.array])`'
- en: '`dataset = CatsDogsDataset("train","train_csv.csv",transform=tra nsform)`'
  id: totrans-1240
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`dataset = CatsDogsDataset("train","train_csv.csv",transform=tra nsform)`'
- en: '`train_set, validation_set = torch.utils.data.random_split(datas et,[20000,5000])`'
  id: totrans-1241
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_set, validation_set = torch.utils.data.random_split(datas et,[20000,5000])`'
- en: '`train_loader = DataLoader(dataset=train_set, collate_fn=custom_ collate_fn,shuffle=True,
    batch_size=batch_size)`'
  id: totrans-1242
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_loader = DataLoader(dataset=train_set, collate_fn=custom_collate_fn,shuffle=True,
    batch_size=batch_size)`'
- en: x = `nn.relu(x)`
  id: totrans-1243
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.relu(x)`
- en: variables = `model.init(key, jnp.ones([1, size_image, size_imag e, 3]), training=False)`
  id: totrans-1244
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: å˜é‡ = `model.init(key, jnp.ones([1, size_image, size_imag e, 3]), training=False)`
- en: Define the Flax network with the BatchNorm and DropOut layers. In the network,
    we introduce theÂ `[training]`Â variable to control when the batch stats should
    be updated. We ensure that they aren't updated during testing.
  id: totrans-1245
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨ç½‘ç»œä¸­ä½¿ç”¨BatchNormå’ŒDropOutå±‚å®šä¹‰Flaxç½‘ç»œã€‚åœ¨ç½‘ç»œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†`[training]`å˜é‡æ¥æ§åˆ¶ä½•æ—¶æ›´æ–°æ‰¹æ¬¡ç»Ÿè®¡ä¿¡æ¯ã€‚æˆ‘ä»¬ç¡®ä¿åœ¨æµ‹è¯•æœŸé—´ä¸æ›´æ–°å®ƒä»¬ã€‚
- en: '`model = CNN()`'
  id: totrans-1246
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`model = CNN()`'
- en: '**Define Flax model with BatchNorm and DropOut**'
  id: totrans-1247
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '**å®šä¹‰å…·æœ‰ BatchNorm å’Œ DropOut çš„ Flax æ¨¡å‹**'
- en: x = `nn.Dense(features=2)(x)`
  id: totrans-1248
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.Dense(features=2)(x)`
- en: The rate drop out probability.
  id: totrans-1249
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ¦‚ç‡æ”¾å¼ƒç‡ã€‚
- en: 'Whether it''sÂ deterministic. If deterministic inputs are scaled and masked.
    Otherwise, they are not masked and returned as they are.class `CNN(nn.Module)`:'
  id: totrans-1250
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'æ˜¯å¦æ˜¯ç¡®å®šæ€§çš„ã€‚å¦‚æœæ˜¯ç¡®å®šæ€§çš„è¾“å…¥åˆ™è¢«ç¼©æ”¾å’Œæ©ç ã€‚å¦åˆ™ï¼Œå®ƒä»¬ä¸ä¼šè¢«æ©ç å¹¶åŸæ ·è¿”å›ã€‚class `CNN(nn.Module)`:'
- en: '`@nn.compact`'
  id: totrans-1251
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`@nn.compact`'
- en: 'def `__call__(self, x, training)`:'
  id: totrans-1252
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def `__call__(self, x, training)`:'
- en: 'The next step is to create the loss function. When applying the model, we:'
  id: totrans-1253
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€æ­¥æ˜¯åˆ›å»ºæŸå¤±å‡½æ•°ã€‚åœ¨åº”ç”¨æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬ï¼š
- en: x = `nn.relu(x)`
  id: totrans-1254
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.relu(x)`
- en: x = `nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))`
  id: totrans-1255
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))`
- en: x = `nn.Conv(features=64, kernel_size=(3, 3))(x)`
  id: totrans-1256
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.Conv(features=64, kernel_size=(3, 3))(x)`
- en: x = `nn.relu(x)`
  id: totrans-1257
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.relu(x)`
- en: x = `nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))`
  id: totrans-1258
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))`
- en: x = `nn.Conv(features=32, kernel_size=(3, 3))(x)`
  id: totrans-1259
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.Conv(features=32, kernel_size=(3, 3))(x)`
- en: In theÂ `BatchNorm`Â layer we setÂ `use_running_average`Â toÂ `False`Â meaning
  id: totrans-1260
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨ `BatchNorm` å±‚ä¸­ï¼Œæˆ‘ä»¬å°† `use_running_average` è®¾ç½®ä¸º `False`ï¼Œæ„å‘³ç€
- en: x = `nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))`
  id: totrans-1261
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))`
- en: x = `x.reshape((x.shape[0], -1))`
  id: totrans-1262
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `x.reshape((x.shape[0], -1))`
- en: x = `nn.Dense(features=256)(x)`
  id: totrans-1263
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.Dense(features=256)(x)`
- en: x = `nn.Dense(features=128)(x)`
  id: totrans-1264
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.Dense(features=128)(x)`
- en: x = `nn.BatchNorm(use_running_average=not training)(x)`
  id: totrans-1265
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.BatchNorm(use_running_average=not training)(x)`
- en: '}'
  id: totrans-1266
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '}'
- en: '`validation_loader = DataLoader(dataset=validation_set,collate_f n=custom_collate_fn,
    shuffle=False, batch_size=batch_size)`'
  id: totrans-1267
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`validation_loader = DataLoader(dataset=validation_set,collate_f n=custom_collate_fn,
    shuffle=False, batch_size=batch_size)`'
- en: '**Compute metrics**'
  id: totrans-1268
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '**è®¡ç®—æŒ‡æ ‡**'
- en: x = `nn.log_softmax(x)` return `x`
  id: totrans-1269
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.log_softmax(x)` return `x`
- en: '**Create loss function**'
  id: totrans-1270
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**åˆ›å»ºæŸå¤±å‡½æ•°**'
- en: 'TheÂ `[DropOut]`Â layer takes the following rate:'
  id: totrans-1271
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[DropOut]` å±‚ä½¿ç”¨ä»¥ä¸‹æ¯”ä¾‹ï¼š'
- en: Pass the batch stats parameters.
  id: totrans-1272
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä¼ é€’æ‰¹é‡ç»Ÿè®¡å‚æ•°ã€‚
- en: '`[training]`Â as True. Set theÂ `[batch_stats]`Â as mutable.'
  id: totrans-1273
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[training]` ä¸º Trueã€‚å°† `[batch_stats]` è®¾ç½®ä¸º mutableã€‚'
- en: Set the random number for theÂ `DropOut`
  id: totrans-1274
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è®¾ç½®ç”¨äº `DropOut` çš„éšæœºæ•°
- en: 'def `cross_entropy_loss(*, logits, labels)`:'
  id: totrans-1275
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def `cross_entropy_loss(*, logits, labels)`:'
- en: '`labels_onehot = jax.nn.one_hot(labels, num_classes=2)` return `optax.softmax_cross_entropy(logits=logits,
    labels=labe`'
  id: totrans-1276
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`labels_onehot = jax.nn.one_hot(labels, num_classes=2)` return `optax.softmax_cross_entropy(logits=logits,
    labels=labe`'
- en: x = `nn.Dropout(0.2, deterministic=not training)(x)`
  id: totrans-1277
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.Dropout(0.2, deterministic=not training)(x)`
- en: 'def `compute_loss(params, batch_stats, images, labels)`:'
  id: totrans-1278
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def `compute_loss(params, batch_stats, images, labels)`:'
- en: '`logits,batch_stats = CNN().apply({''params'': params,''batch_s tats'': batch_stats},images,
    training=True,rngs={''dropout'': jax. random.PRNGKey(0)}, mutable=[''batch_stats''])`'
  id: totrans-1279
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`logits,batch_stats = CNN().apply({''params'': params,''batch_s tats'': batch_stats},images,
    training=True,rngs={''dropout'': jax.random.PRNGKey(0)}, mutable=[''batch_stats''])`'
- en: loss = `cross_entropy_loss(logits=logits, labels=labels)` return `loss, (logits,
    batch_stats)`
  id: totrans-1280
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: loss = `cross_entropy_loss(logits=logits, labels=labels)` return `loss, (logits,
    batch_stats)`
- en: The compute metrics function calculates the loss and accuracy and returns them.
  id: totrans-1281
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è®¡ç®—æŒ‡æ ‡å‡½æ•°è®¡ç®—æŸå¤±å’Œå‡†ç¡®æ€§å¹¶è¿”å›å®ƒä»¬ã€‚
- en: 'def `compute_metrics(*, logits, labels)`:'
  id: totrans-1282
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def `compute_metrics(*, logits, labels)`:'
- en: loss = `cross_entropy_loss(logits=logits, labels=labels)` accuracy = `jnp.mean(jnp.argmax(logits,
    -1) == labels)` metrics = {
  id: totrans-1283
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: loss = `cross_entropy_loss(logits=logits, labels=labels)` å‡†ç¡®ç‡ = `jnp.mean(jnp.argmax(logits,
    -1) == labels)` metrics = {
- en: '`''loss'': loss`,'
  id: totrans-1284
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`''loss'': loss`,'
- en: '`''accuracy'': accuracy`,'
  id: totrans-1285
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`''accuracy'': accuracy`,'
- en: x = `nn.relu(x)`
  id: totrans-1286
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.relu(x)`
- en: return `metrics`
  id: totrans-1287
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: return `metrics`
- en: '**Create custom Flax training state**'
  id: totrans-1288
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**åˆ›å»ºè‡ªå®šä¹‰çš„ Flax è®­ç»ƒçŠ¶æ€**'
- en: Let's create a custom Flax training state that will store the batch stats information.
    To do that, create a new training state class that subclasses Flax'sÂ `TrainState`.
  id: totrans-1289
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰çš„ Flax è®­ç»ƒçŠ¶æ€ï¼Œç”¨äºå­˜å‚¨æ‰¹é‡ç»Ÿè®¡ä¿¡æ¯ã€‚ä¸ºæ­¤ï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„è®­ç»ƒçŠ¶æ€ç±»ï¼Œå®ƒæ˜¯ Flax çš„ `TrainState` çš„å­ç±»ã€‚
- en: x = `nn.Conv(features=128, kernel_size=(3, 3))(x)`
  id: totrans-1290
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: x = `nn.Conv(features=128, kernel_size=(3, 3))(x)`
- en: that the stats stored inÂ `[batch_stats]`Â will not be used, but batch stats of
    the input will be computed.
  id: totrans-1291
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å­˜å‚¨åœ¨ `[batch_stats]` ä¸­çš„ç»Ÿè®¡ä¿¡æ¯ä¸ä¼šè¢«ä½¿ç”¨ï¼Œä½†ä¼šè®¡ç®—è¾“å…¥çš„æ‰¹æ¬¡ç»Ÿè®¡ã€‚
- en: '`key = jax.random.PRNGKey(0)`'
  id: totrans-1292
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`key = jax.random.PRNGKey(0)`'
- en: initialize weights
  id: totrans-1293
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åˆå§‹åŒ–æƒé‡
- en: '`class TrainState(train_state.TrainState): batch_stats: flax.core.FrozenDictTo
    define a Flax training state, use [TrainState.create] and pass the: Apply function.`'
  id: totrans-1294
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`class TrainState(train_state.TrainState): batch_stats: flax.core.FrozenDictè¦å®šä¹‰ä¸€ä¸ª
    Flax è®­ç»ƒçŠ¶æ€ï¼Œä½¿ç”¨ [TrainState.create] å¹¶ä¼ é€’ï¼šåº”ç”¨å‡½æ•°ã€‚`'
- en: Model parameters.
  id: totrans-1295
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ¨¡å‹å‚æ•°ã€‚
- en: The optimizer function. The batch stats.
  id: totrans-1296
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–å™¨å‡½æ•°ã€‚æ‰¹é‡ç»Ÿè®¡ã€‚
- en: '`state = TrainState.create(`'
  id: totrans-1297
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`state = TrainState.create(`'
- en: apply_fn = model.apply,
  id: totrans-1298
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: apply_fn = model.apply,
- en: '`params = variables[''params''],`'
  id: totrans-1299
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`params = variables[''params''],`'
- en: tx = optax.sgd(0.01),
  id: totrans-1300
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: tx = optax.sgd(0.01),
- en: batch_stats = variables['batch_stats'],
  id: totrans-1301
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: batch_stats = variables['batch_stats'],
- en: )
  id: totrans-1302
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: )
- en: '**Training step**'
  id: totrans-1303
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**è®­ç»ƒæ­¥éª¤**'
- en: In the training step, we compute the gradients with respect to the loss and
    model parametersâ€“ the **model parameters **and **batch statistics**. We use the
    gradients to update the model parameters and return the new state and model metrics.
    The function is decorated
  id: totrans-1304
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒæ­¥éª¤ä¸­ï¼Œæˆ‘ä»¬è®¡ç®—ç›¸å¯¹äºæŸå¤±å’Œæ¨¡å‹å‚æ•°çš„æ¢¯åº¦ - **æ¨¡å‹å‚æ•°**å’Œ**æ‰¹æ¬¡ç»Ÿè®¡**ã€‚æˆ‘ä»¬ä½¿ç”¨è¿™äº›æ¢¯åº¦æ¥æ›´æ–°æ¨¡å‹å‚æ•°å¹¶è¿”å›æ–°çš„çŠ¶æ€å’Œæ¨¡å‹æŒ‡æ ‡ã€‚è¿™ä¸ªå‡½æ•°è¢«è£…é¥°
- en: with @jax.jit to make the computation faster.
  id: totrans-1305
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä»¥ @jax.jit ä½¿è®¡ç®—æ›´å¿«ã€‚
- en: '@jax.jit'
  id: totrans-1306
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '@jax.jit'
- en: '`def train_step(state,images, labels):`'
  id: totrans-1307
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def train_step(state,images, labels):`'
- en: '"""Train for a single step."""'
  id: totrans-1308
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""è¿›è¡Œå•æ­¥è®­ç»ƒã€‚"""'
- en: '`(batch_loss, (logits, batch_stats)), grads= jax.value_and_gra d(compute_loss,
    has_aux=True)(state.params,state.batch_stats, i mages,labels)`'
  id: totrans-1309
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`(batch_loss, (logits, batch_stats)), grads= jax.value_and_gra d(compute_loss,
    has_aux=True)(state.params,state.batch_stats, i mages,labels)`'
- en: '`state = state.apply_gradients(grads=grads)`'
  id: totrans-1310
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`state = state.apply_gradients(grads=grads)`'
- en: '`metrics = compute_metrics(logits=logits, labels=labels) return state, metricsNext,
    define a function that applies the training step for one epoch. The functions:`'
  id: totrans-1311
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`metrics = compute_metrics(logits=logits, labels=labels) return state, metricsNext,
    define a function that applies the training step for one epoch. The functions:`'
- en: Loops through the training data.
  id: totrans-1312
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: é€šè¿‡è®­ç»ƒæ•°æ®å¾ªç¯ã€‚
- en: Passes each training batch the training step.
  id: totrans-1313
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å°†æ¯ä¸ªè®­ç»ƒæ‰¹æ¬¡ä¼ é€’ç»™è®­ç»ƒæ­¥éª¤ã€‚
- en: Obtains the batch metrics. Computes the mean to obtain the epoch metrics.
  id: totrans-1314
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è·å–æ‰¹æ¬¡æŒ‡æ ‡ã€‚ è®¡ç®—å‡å€¼ä»¥è·å¾— epoch æŒ‡æ ‡ã€‚
- en: Returns the new state and metrics.
  id: totrans-1315
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¿”å›æ–°çŠ¶æ€å’Œåº¦é‡ã€‚
- en: '`def train_one_epoch(state, dataloader):`'
  id: totrans-1316
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def train_one_epoch(state, dataloader):`'
- en: '"""Train for 1 epoch on the training set.""" batch_metrics = []'
  id: totrans-1317
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œ 1 ä¸ª epoch çš„è®­ç»ƒã€‚""" batch_metrics = []'
- en: 'for cnt, (images, labels) in enumerate(dataloader):'
  id: totrans-1318
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'for cnt, (images, labels) in enumerate(dataloader):'
- en: '`images = images / 255.0`'
  id: totrans-1319
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`images = images / 255.0`'
- en: '`state, metrics = train_step(state, images, labels) batch_metrics.append(metrics)`'
  id: totrans-1320
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`state, metrics = train_step(state, images, labels) batch_metrics.append(metrics)`'
- en: '`batch_metrics_np = jax.device_get(batch_metrics) epoch_metrics_np = {k: np.mean([metrics[k]
    for metrics in batch_metrics_n`'
  id: totrans-1321
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`batch_metrics_np = jax.device_get(batch_metrics) epoch_metrics_np = {k: np.mean([metrics[k]
    for metrics in batch_metrics_n`'
- en: p])
  id: totrans-1322
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: p])
- en: for k in batch_metrics_np[0] }
  id: totrans-1323
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å¯¹äº `batch_metrics_np[0]` ä¸­çš„`k` }
- en: '`return state, epoch_metrics_np`'
  id: totrans-1324
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return state, epoch_metrics_np`'
- en: '**Evaluation step**'
  id: totrans-1325
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**è¯„ä¼°æ­¥éª¤**'
- en: We pass the test images and labels to the model in the evaluation step and obtain
    the evaluation metrics. The function is also jitted to take advantage of JAX's
    fast computation. During the evaluation, set [training] to [False] so that the
    model parameters are not updated. In this step, we also pass the batch stats and
    the random number generator for the [DropOut] layer.
  id: totrans-1326
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨è¯„ä¼°æ­¥éª¤ä¸­å°†æµ‹è¯•å›¾åƒå’Œæ ‡ç­¾ä¼ é€’ç»™æ¨¡å‹å¹¶è·å–è¯„ä¼°æŒ‡æ ‡ã€‚è¯¥å‡½æ•°è¿˜é€šè¿‡JAXçš„å¿«é€Ÿè®¡ç®—åˆ©ç”¨JITç¼–è¯‘ã€‚åœ¨è¯„ä¼°ä¸­ï¼Œå°† [training] è®¾ç½®ä¸º [False]ï¼Œä»¥ä¾¿ä¸æ›´æ–°æ¨¡å‹å‚æ•°ã€‚åœ¨è¿™ä¸€æ­¥ä¸­ï¼Œæˆ‘ä»¬è¿˜ä¼ é€’æ‰¹æ¬¡ç»Ÿè®¡å’Œ
    [DropOut] å±‚çš„éšæœºæ•°ç”Ÿæˆå™¨ã€‚
- en: '@jax.jitdef eval_step(batch_stats, params, images, labels): logits = CNN().apply({''params'':
    params,''batch_stats'': batch _stats}, images, training=False,rngs={''dropout'':
    jax.random.PRN GKey(0)})return compute_metrics(logits=logits, labels=labels)The
    [evaluate_model] function applies the [eval_step] to the test data and returns
    the evaluation metrics.'
  id: totrans-1327
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '@jax.jitdef eval_step(batch_stats, params, images, labels): logits = CNN().apply({''params'':
    params,''batch_stats'': batch _stats}, images, training=False,rngs={''dropout'':
    jax.random.PRN GKey(0)})return compute_metrics(logits=logits, labels=labels)`[evaluate_model]`
    å‡½æ•°å°†`[eval_step]` åº”ç”¨äºæµ‹è¯•æ•°æ®ï¼Œå¹¶è¿”å›è¯„ä¼°æŒ‡æ ‡ã€‚'
- en: '`def evaluate_model(state, test_imgs, test_lbls):`'
  id: totrans-1328
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def evaluate_model(state, test_imgs, test_lbls):`'
- en: '"""Evaluate on the validation set."""'
  id: totrans-1329
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°ã€‚"""'
- en: '`metrics = eval_step(state.batch_stats,state.params, test_im`'
  id: totrans-1330
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`metrics = eval_step(state.batch_stats,state.params, test_im`'
- en: gs, test_lbls)
  id: totrans-1331
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: gs, test_lbls)
- en: '`metrics = jax.device_get(metrics)`'
  id: totrans-1332
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`metrics = jax.device_get(metrics)`'
- en: '`metrics = jax.tree_map(lambda x: x.item(), metrics) return metrics`'
  id: totrans-1333
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`metrics = jax.tree_map(lambda x: x.item(), metrics) return metrics`'
- en: '**Train Flax model**'
  id: totrans-1334
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**è®­ç»ƒ Flax æ¨¡å‹**'
- en: To train the model, we define another function that implements `[train_one_epoch]`.
    Let's start by defining the model evaluation data.(`test_images`, `test_labels`)
    = next(iter(validation_loader)) `test_images = test_images / 255.0`
  id: totrans-1335
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä¸ºäº†è®­ç»ƒæ¨¡å‹ï¼Œæˆ‘ä»¬å®šä¹‰å¦ä¸€ä¸ªå‡½æ•°æ¥å®æ–½`[train_one_epoch]`ã€‚é¦–å…ˆå®šä¹‰æ¨¡å‹è¯„ä¼°æ•°æ®ã€‚(`test_images`, `test_labels`)
    = next(iter(validation_loader)) `test_images = test_images / 255.0`
- en: '**Set up TensorBoard in Flax**'
  id: totrans-1336
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**åœ¨ Flax ä¸­è®¾ç½® TensorBoard**'
- en: You can log the model metrics to TensorBoard by writing the scalars to TensorBoard.
  id: totrans-1337
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥å°†æ¨¡å‹æŒ‡æ ‡è®°å½•åˆ°TensorBoardä¸­ï¼Œæ–¹æ³•æ˜¯å°†æ ‡é‡å†™å…¥TensorBoardã€‚
- en: '`from torch.utils.tensorboard import SummaryWriter import torchvision.transforms.functional
    as F logdir = "flax_logs"`'
  id: totrans-1338
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from torch.utils.tensorboard import SummaryWriter import torchvision.transforms.functional
    as F logdir = "flax_logs"`'
- en: '`writer = SummaryWriter(logdir)`'
  id: totrans-1339
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`writer = SummaryWriter(logdir)`'
- en: '**Train model**'
  id: totrans-1340
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**è®­ç»ƒæ¨¡å‹**'
- en: We can also append the metrics to a list and visualize them with Matplotlib.
  id: totrans-1341
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å¯ä»¥å°†æŒ‡æ ‡é™„åŠ åˆ°åˆ—è¡¨ä¸­ï¼Œå¹¶ä½¿ç”¨ Matplotlib è¿›è¡Œå¯è§†åŒ–ã€‚
- en: '`training_loss = [] training_accuracy = [] testing_loss = []`'
  id: totrans-1342
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`training_loss = [] training_accuracy = [] testing_loss = []`'
- en: '`testing_accuracy = []`'
  id: totrans-1343
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_accuracy = []`'
- en: 'Next, define the training function that will: Train the Flax model for the
    specified number of epochs.'
  id: totrans-1344
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œå®šä¹‰å°†è®­ç»ƒ Flax æ¨¡å‹æŒ‡å®šè½®æ•°çš„è®­ç»ƒå‡½æ•°ã€‚
- en: Evaluate the model on the test data.
  id: totrans-1345
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨æµ‹è¯•æ•°æ®ä¸Šè¯„ä¼°æ¨¡å‹ã€‚
- en: Append the metrics to a list. Write model metrics to TensorBoard.
  id: totrans-1346
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å°†æŒ‡æ ‡é™„åŠ åˆ°åˆ—è¡¨ä¸­ã€‚å°†æ¨¡å‹æŒ‡æ ‡å†™å…¥ TensorBoardã€‚
- en: Print the metrics on every epoch. Return the trained model state`def train_model(epochs):for
    epoch in range(1, epochs + 1):train_state, train_metrics = train_one_epoch(state,
    tra`
  id: totrans-1347
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨æ¯ä¸ª epoch æ‰“å°æŒ‡æ ‡ã€‚è¿”å›è®­ç»ƒåçš„æ¨¡å‹çŠ¶æ€`def train_model(epochs):for epoch in range(1, epochs
    + 1):train_state, train_metrics = train_one_epoch(state, tra`
- en: '`in_loader)`'
  id: totrans-1348
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`in_loader)`'
- en: '`training_loss.append(train_metrics[''loss''])` `training_accuracy.append(train_metrics[''accuracy''])`
    `test_metrics = evaluate_model(train_state, test_images,`'
  id: totrans-1349
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`training_loss.append(train_metrics[''loss''])` `training_accuracy.append(train_metrics[''accuracy''])`
    `test_metrics = evaluate_model(train_state, test_images,`'
- en: '`test_labels`'
  id: totrans-1350
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`test_labels`'
- en: '`testing_loss.append(test_metrics[''loss''])`'
  id: totrans-1351
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_loss.append(test_metrics[''loss''])`'
- en: '`testing_accuracy.append(test_metrics[''accuracy''])`'
  id: totrans-1352
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_accuracy.append(test_metrics[''accuracy''])`'
- en: '`writer.add_scalar(''Loss/train'', train_metrics[''loss''], epoch)`'
  id: totrans-1353
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`writer.add_scalar(''Loss/train'', train_metrics[''loss''], epoch)`'
- en: '`writer.add_scalar(''Loss/test'', test_metrics[''loss''], epoch)`'
  id: totrans-1354
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`writer.add_scalar(''Loss/test'', test_metrics[''loss''], epoch)`'
- en: '`writer.add_scalar(''Accuracy/train'', train_metrics[''accuracy''], epoch)`'
  id: totrans-1355
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`writer.add_scalar(''Accuracy/train'', train_metrics[''accuracy''], epoch)`'
- en: '`writer.add_scalar(''Accuracy/test'', test_metrics[''accuracy''], epoch)`'
  id: totrans-1356
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`writer.add_scalar(''Accuracy/test'', test_metrics[''accuracy''], epoch)`'
- en: '`print(f"Epoch: {epoch}, training loss: {train_metrics [''loss'']}, training
    accuracy: {train_metrics[''accuracy''] * 10 0}, validation loss: {test_metrics[''loss'']},
    validation accuracy: {test_metrics[''accuracy''] * 100}")`'
  id: totrans-1357
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`print(f"Epoch: {epoch}, training loss: {train_metrics [''loss'']}, training
    accuracy: {train_metrics[''accuracy''] * 10 0}, validation loss: {test_metrics[''loss'']},
    validation accuracy: {test_metrics[''accuracy''] * 100}")`'
- en: '`return train_stateRun the training function to train the model. trained_model_state
    = train_model(30)`'
  id: totrans-1358
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return train_stateRun the training function to train the model. trained_model_state
    = train_model(30)`'
- en: '**Save Flax model**'
  id: totrans-1359
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**ä¿å­˜ Flax æ¨¡å‹**'
- en: 'The `[save_checkpoint]` saves a Flax model. It expects:'
  id: totrans-1360
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[save_checkpoint]` ä¿å­˜ Flax æ¨¡å‹ã€‚å®ƒæœŸæœ›ï¼š'
- en: The directory to save the model checkpoint.
  id: totrans-1361
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹çš„ç›®å½•ã€‚
- en: The Flax trained model, in this case `[trained_model_state]`. The model's prefix.
  id: totrans-1362
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒFlax è®­ç»ƒçš„æ¨¡å‹æ˜¯ `[trained_model_state]`ã€‚è¯¥æ¨¡å‹çš„å‰ç¼€ã€‚
- en: Whether to overwrite existing models.
  id: totrans-1363
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ˜¯å¦è¦†ç›–ç°æœ‰æ¨¡å‹ã€‚
- en: '`from flax.training import checkpoints`'
  id: totrans-1364
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from flax.training import checkpoints`'
- en: ckpt_dir = 'model_checkpoint/'
  id: totrans-1365
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ckpt_dir = 'model_checkpoint/'
- en: '`checkpoints.save_checkpoint(ckpt_dir=ckpt_dir`'
  id: totrans-1366
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`checkpoints.save_checkpoint(ckpt_dir=ckpt_dir)`'
- en: '`target=trained_model_state`, `step=100,`'
  id: totrans-1367
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`target=trained_model_state`, `step=100,`'
- en: '`prefix=''flax_model'', overwrite=True`'
  id: totrans-1368
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`prefix=''flax_model'', overwrite=True`'
- en: )![](../images/00039.jpeg)
  id: totrans-1369
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: )![](../images/00039.jpeg)
- en: '**Load Flax model**'
  id: totrans-1370
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**åŠ è½½ Flax æ¨¡å‹**'
- en: The `[restore_checkpoint]` method loads a saved Flax model from the saved location.
  id: totrans-1371
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[restore_checkpoint]` æ–¹æ³•ä»ä¿å­˜çš„ä½ç½®åŠ è½½å·²ä¿å­˜çš„ Flax æ¨¡å‹ã€‚'
- en: '`loaded_model = checkpoints.restore_checkpoint(`'
  id: totrans-1372
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`loaded_model = checkpoints.restore_checkpoint(`'
- en: '`ckpt_dir=ckpt_dir`, `target=state`, `prefix=''flax_mode`'
  id: totrans-1373
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`ckpt_dir=ckpt_dir`, `target=state`, `prefix=''flax_mode`'
- en: '`l''`'
  id: totrans-1374
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`l''`'
- en: '**Evaluate Flax model**'
  id: totrans-1375
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**è¯„ä¼° Flax æ¨¡å‹**'
- en: Run the `[evalaute_model]` function to check the performance of the model on
    test data. `evaluate_model(trained_model_state,test_images, test_labels)`
  id: totrans-1376
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¿è¡Œ `[evalaute_model]` å‡½æ•°æ¥æ£€æŸ¥æ¨¡å‹åœ¨æµ‹è¯•æ•°æ®ä¸Šçš„æ€§èƒ½ã€‚`evaluate_model(trained_model_state,test_images,
    test_labels)`
- en: '**Visualize Flax model performance**'
  id: totrans-1377
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**å¯è§†åŒ– Flax æ¨¡å‹çš„æ€§èƒ½**'
- en: To visualize the performance of the Flax model, you can plot the metrics using
    Matplotlib or load TensorBoard and check the scalars tab.
  id: totrans-1378
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¦å¯è§†åŒ– Flax æ¨¡å‹çš„æ€§èƒ½ï¼Œå¯ä»¥ä½¿ç”¨ Matplotlib ç»˜åˆ¶æŒ‡æ ‡å›¾è¡¨æˆ–åŠ è½½ TensorBoard å¹¶æ£€æŸ¥æ ‡é‡é€‰é¡¹å¡ã€‚
- en: '`%load_ext tensorboard` `%tensorboard --logdir={logdir}`'
  id: totrans-1379
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`%load_ext tensorboard` `%tensorboard --logdir={logdir}`'
- en: '**Final thoughts**'
  id: totrans-1380
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**æœ€ç»ˆæ€è€ƒ**'
- en: '`In this article, you have seen how to build networks in Flax containing BatchNorm
    and DropOut layers. You have also seen how to adjust the training process to cater
    to these new layers. Specifically, you have learned:`'
  id: totrans-1381
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`åœ¨æœ¬æ–‡ä¸­ï¼Œæ‚¨å·²ç»çœ‹åˆ°å¦‚ä½•åœ¨ Flax ä¸­æ„å»ºåŒ…å« BatchNorm å’Œ DropOut å±‚çš„ç½‘ç»œã€‚æ‚¨è¿˜å­¦ä¹ äº†å¦‚ä½•è°ƒæ•´è®­ç»ƒè¿‡ç¨‹ä»¥é€‚åº”è¿™äº›æ–°å±‚ã€‚å…·ä½“æ¥è¯´ï¼Œæ‚¨å­¦åˆ°äº†ï¼š`'
- en: '`How to define Flax models with BatchNorm and DropOut layers.`'
  id: totrans-1382
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å¦‚ä½•å®šä¹‰åŒ…å« BatchNorm å’Œ DropOut å±‚çš„ Flax æ¨¡å‹ã€‚
- en: '`How to create a custom Flax training state.`'
  id: totrans-1383
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`å¦‚ä½•åˆ›å»ºè‡ªå®šä¹‰çš„ Flax è®­ç»ƒçŠ¶æ€ã€‚`'
- en: '`Training and evaluating a Flax model with BatchNorm and DropOut layers.`'
  id: totrans-1384
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ BatchNorm å’Œ DropOut å±‚è®­ç»ƒå’Œè¯„ä¼° Flax æ¨¡å‹ã€‚
- en: '`How to save and load a Flax model.`'
  id: totrans-1385
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`å¦‚ä½•ä¿å­˜å’ŒåŠ è½½ Flax æ¨¡å‹ã€‚`'
- en: '`How to evaluate the performance of a Flax model`'
  id: totrans-1386
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`å¦‚ä½•è¯„ä¼°Flaxæ¨¡å‹çš„æ€§èƒ½`'
- en: '`**LSTM in JAX & Flax**`'
  id: totrans-1387
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '`**JAXå’ŒFlaxä¸­çš„LSTM**`'
- en: '`LSTMs are a class of neural networks used to solve sequence problems such
    as time series and natural language processing. The LSTMs maintain some internal
    state that is useful in solving these problems. LSTMs apply for loops to iterate
    over each time step. We can use functions from JAX and Flax instead of writing
    these for loops from scratch. In this article, we will build a natural language
    processing model using LSTMs in Flax.`'
  id: totrans-1388
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`LSTMæ˜¯ä¸€ç±»ç”¨äºè§£å†³åºåˆ—é—®é¢˜ï¼ˆå¦‚æ—¶é—´åºåˆ—å’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼‰çš„ç¥ç»ç½‘ç»œã€‚LSTMä¿æŒä¸€äº›å†…éƒ¨çŠ¶æ€ï¼Œåœ¨è§£å†³è¿™äº›é—®é¢˜æ—¶éå¸¸æœ‰ç”¨ã€‚LSTMåº”ç”¨äºå¾ªç¯éå†æ¯ä¸ªæ—¶é—´æ­¥ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨JAXå’ŒFlaxä¸­çš„å‡½æ•°ï¼Œè€Œä¸æ˜¯ä»å¤´ç¼–å†™è¿™äº›å¾ªç¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨Flaxä¸­çš„LSTMæ„å»ºè‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹ã€‚`'
- en: '`Let''s get started.`'
  id: totrans-1389
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`è®©æˆ‘ä»¬å¼€å§‹å§ã€‚`'
- en: '`**Dataset download**`'
  id: totrans-1390
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '`**æ•°æ®é›†ä¸‹è½½**`'
- en: '`We''ll use theÂ movie review dataset from Kaggle. We download the dataset using
    Kaggle''sÂ PythonÂ package.`'
  id: totrans-1391
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`æˆ‘ä»¬å°†ä½¿ç”¨Kaggleæä¾›çš„ç”µå½±è¯„è®ºæ•°æ®é›†ã€‚æˆ‘ä»¬ä½¿ç”¨Kaggleçš„PythonåŒ…ä¸‹è½½æ•°æ®é›†ã€‚`'
- en: '`import os`'
  id: totrans-1392
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import os`'
- en: è¿›å…¥`#Obtain from https://www.kaggle.com/username/account` `os.environ["KAGGLE_USERNAME"]="KAGGLE_USERNAME"`
  id: totrans-1393
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¿›å…¥`#Obtain from https://www.kaggle.com/username/account` `os.environ["KAGGLE_USERNAME"]="KAGGLE_USERNAME"`
- en: '`os.environ["KAGGLE_KEY"]="KAGGLE_KEY"`'
  id: totrans-1394
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`os.environ["KAGGLE_KEY"]="KAGGLE_KEY"`'
- en: '`import kaggle`'
  id: totrans-1395
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import kaggle`'
- en: '`kaggle datasets download lakshmi25npathi/imdb-dataset-of-50k-mo vie-reviews`'
  id: totrans-1396
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`kaggle datasets download lakshmi25npathi/imdb-dataset-of-50k-mo vie-reviews`'
- en: '`Next, extract the dataset.import zipfilewith zipfile.ZipFile(''imdb-dataset-of-50k-movie-reviews.zip'',
    ''r'') as zip_ref:zip_ref.extractall(''imdb-dataset-of-50k-movie-reviews'')Load
    the dataset usingÂ PandasÂ and display a sample of the reviews.`'
  id: totrans-1397
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`æ¥ä¸‹æ¥ï¼Œæå–æ•°æ®é›†ã€‚ä½¿ç”¨zipfileè§£å‹ç¼©zipæ–‡ä»¶''imdb-dataset-of-50k-movie-reviews.zip''ï¼Œç„¶åä½¿ç”¨PandasåŠ è½½æ•°æ®å¹¶æ˜¾ç¤ºéƒ¨åˆ†è¯„è®ºã€‚`'
- en: '`df = pd.read_csv("imdb-dataset-of-50k-movie-reviews/IMDB Datase t.csv")`'
  id: totrans-1398
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`df = pd.read_csv("imdb-dataset-of-50k-movie-reviews/IMDB Datase t.csv")`'
- en: '`df.head()`'
  id: totrans-1399
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`df.head()`'
- en: '`![](../images/00040.gif)`'
  id: totrans-1400
  prefs: []
  stylish: true
  type: TYPE_IMG
  zh: '`![](../images/00040.gif)`'
- en: '`**Data processing with NLTK**`'
  id: totrans-1401
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '`**ä½¿ç”¨NLTKè¿›è¡Œæ•°æ®å¤„ç†**`'
- en: '`The dataset contains unnecessary characters for predicting whether a movie
    review is negative or positive. For instance, punctuation marks and special characters.
    We, therefore, remove these from the reviews. We also need to convert theÂ [sentiment]Â column
    into a numerical representation. This is achieved using Â [LabelEncoder]Â fromÂ Scikitlearn.
    Let''s import that together with other packages we''ll use throughout this article.`'
  id: totrans-1402
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`è¯¥æ•°æ®é›†åŒ…å«ä¸€äº›ä¸å¿…è¦çš„å­—ç¬¦ï¼Œç”¨äºé¢„æµ‹ç”µå½±è¯„è®ºæ˜¯è´Ÿé¢è¿˜æ˜¯æ­£é¢ã€‚ä¾‹å¦‚ï¼Œæ ‡ç‚¹ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä»è¯„è®ºä¸­å»é™¤è¿™äº›å­—ç¬¦ã€‚æˆ‘ä»¬è¿˜éœ€è¦å°†[sentiment]åˆ—è½¬æ¢ä¸ºæ•°å€¼è¡¨ç¤ºã€‚è¿™å¯ä»¥ä½¿ç”¨Scikitlearnä¸­çš„[LabelEncoder]å®Œæˆã€‚è®©æˆ‘ä»¬å¯¼å…¥è¿™äº›åŠå…¶ä»–æˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­å°†ä½¿ç”¨çš„åŒ…ã€‚`'
- en: '`import numpy as np`'
  id: totrans-1403
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import numpy as np`'
- en: '`import pandas as pd`'
  id: totrans-1404
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import pandas as pd`'
- en: '`from numpy import array`'
  id: totrans-1405
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from numpy import array`'
- en: '`import tensorflow as tf`'
  id: totrans-1406
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import tensorflow as tf`'
- en: '`from sklearn.model_selection import train_test_split from sklearn.preprocessing
    import LabelEncoder import matplotlib.pyplot as plt`'
  id: totrans-1407
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from sklearn.model_selection import train_test_split from sklearn.preprocessing
    import LabelEncoder import matplotlib.pyplot as plt`'
- en: '`The reviews also contain words that are not useful in the sentiment prediction.
    These are common words in English, such as the, at, and, etc. These words are
    known asÂ **stopwords**. We remove them with the help of theÂ nltkÂ library. Let''s
    start by defining a function to remove all the English stopwords.`'
  id: totrans-1408
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`è¯„è®ºä¸­è¿˜åŒ…å«å¯¹æƒ…æ„Ÿé¢„æµ‹æ— ç”¨çš„è¯è¯­ã€‚è¿™äº›æ˜¯è‹±è¯­ä¸­å¸¸è§çš„è¯ï¼Œå¦‚theï¼Œatï¼Œandç­‰ã€‚è¿™äº›è¯ç§°ä¸º**åœç”¨è¯**ã€‚æˆ‘ä»¬ä½¿ç”¨nltkåº“å¸®åŠ©å»é™¤å®ƒä»¬ã€‚è®©æˆ‘ä»¬å¼€å§‹å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œä»¥åˆ é™¤æ‰€æœ‰è‹±æ–‡åœç”¨è¯ã€‚`'
- en: '`pip install nltk`'
  id: totrans-1409
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`pip install nltk`'
- en: '`import nltk`'
  id: totrans-1410
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import nltk`'
- en: '`from nltk.corpus import stopwords`'
  id: totrans-1411
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from nltk.corpus import stopwords`'
- en: '`nltk.download(''stopwords'')`'
  id: totrans-1412
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`nltk.download(''stopwords'')`'
- en: '`def remove_stop_words(review):`'
  id: totrans-1413
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def remove_stop_words(review):`'
- en: '`review_minus_sw = []`'
  id: totrans-1414
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`review_minus_sw = []`'
- en: '`stop_words = stopwords.words(''english'')`'
  id: totrans-1415
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`stop_words = stopwords.words(''english'')`'
- en: '`review = review.split()`'
  id: totrans-1416
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`review = review.split()`'
- en: '`cleaned_review = [review_minus_sw.append(word) for word in`'
  id: totrans-1417
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[review_minus_sw.append(word) for word in cleaned_review]`'
- en: '`review if word not in stop_words]`'
  id: totrans-1418
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`review if word not in stop_words]`'
- en: '`cleaned_review = '' ''.join(review_minus_sw)`'
  id: totrans-1419
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`cleaned_review = '' ''.join(review_minus_sw)`'
- en: '`return cleaned_review`'
  id: totrans-1420
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return cleaned_review`'
- en: Apply the function to the sentiment column.`df['review'] = df['review'].apply(remove_stop_words)`Let's
    also convert the sentiment column to numerical representation.
  id: totrans-1421
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`å°†è¯¥å‡½æ•°åº”ç”¨äºæƒ…æ„Ÿåˆ—ã€‚`df[''review''] = df[''review''].apply(remove_stop_words)`è®©æˆ‘ä»¬è¿˜å°†æƒ…æ„Ÿåˆ—è½¬æ¢ä¸ºæ•°å€¼è¡¨ç¤ºã€‚`'
- en: '`labelencoder = LabelEncoder()`'
  id: totrans-1422
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`labelencoder = LabelEncoder()`'
- en: '`df = df.assign(sentiment = labelencoder.fit_transform(df["senti ment"]))`'
  id: totrans-1423
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`df = df.assign(sentiment = labelencoder.fit_transform(df["senti ment"]))`'
- en: Compare the reviews with the review with and without the stop words.![](../images/00041.jpeg)
  id: totrans-1424
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å°†å…·æœ‰å’Œä¸å…·æœ‰åœç”¨è¯çš„è¯„è®ºè¿›è¡Œæ¯”è¾ƒï¼[](../images/00041.jpeg)
- en: Looking at the third review, we notice that the
  id: totrans-1425
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹ç¬¬ä¸‰æ¡è¯„è®ºæ—¶ï¼Œæˆ‘ä»¬æ³¨æ„åˆ°
- en: '`wordsÂ [this],Â [was]Â andÂ [a]Â have been dropped from the sentence. However,
    we can still see some special characters, such asÂ [<br>]Â in the review. Let''s
    resolve that next.`'
  id: totrans-1426
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`wordsÂ [this],Â [was]Â andÂ [a]Â have been dropped from the sentence. However,
    we can still see some special characters, such asÂ [<br>]Â in the review. Let''s
    resolve that next.`'
- en: '**Text vectorization with Keras**'
  id: totrans-1427
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**ä½¿ç”¨ Keras è¿›è¡Œæ–‡æœ¬å‘é‡åŒ–**'
- en: The review data is still in text form. However, we need to convert it to a numeric
    representation like the sentiment column. Before we do that, let's split the dataset
    into aÂ training and testing set.
  id: totrans-1428
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¯„è®ºæ•°æ®ä»ç„¶æ˜¯æ–‡æœ¬å½¢å¼ã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬éœ€è¦å°†å…¶è½¬æ¢ä¸ºç±»ä¼¼æƒ…æ„Ÿåˆ—çš„æ•°å€¼è¡¨ç¤ºã€‚åœ¨è¿™ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å°†æ•°æ®é›†æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚
- en: '`from sklearn.model_selection import train_test_split df = df.drop_duplicates()`'
  id: totrans-1429
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from sklearn.model_selection import train_test_split df = df.drop_duplicates()`'
- en: '`docs = df[''review'']`'
  id: totrans-1430
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`docs = df[''review'']`'
- en: '`labels = array(df[''sentiment''])`'
  id: totrans-1431
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`labels = array(df[''sentiment''])`'
- en: '`X_train, X_test , y_train, y_test = train_test_split(docs, labe ls , test_size
    = 0.20, random_state=0)`'
  id: totrans-1432
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`X_train, X_test , y_train, y_test = train_test_split(docs, labe ls , test_size
    = 0.20, random_state=0)`'
- en: 'We use theÂ Keras text vectorization layerÂ to convert the reviews to integer
    form. This function lets us filter out all punctuation marks and convert the reviews
    to lowercase. We pass the following parameters:'
  id: totrans-1433
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨ Keras æ–‡æœ¬å‘é‡åŒ–å±‚å°†è¯„è®ºè½¬æ¢ä¸ºæ•´æ•°å½¢å¼ã€‚æ­¤å‡½æ•°å¯ä»¥è¿‡æ»¤æ‰æ‰€æœ‰æ ‡ç‚¹ç¬¦å·å¹¶å°†è¯„è®ºè½¬æ¢ä¸ºå°å†™ã€‚æˆ‘ä»¬ä¼ é€’ä»¥ä¸‹å‚æ•°ï¼š
- en: '`standardize`Â asÂ `lower_and_strip_punctuation`Â to convert to'
  id: totrans-1434
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`standardize`Â è®¾ç½®ä¸ºÂ `lower_and_strip_punctuation`Â ä»¥è½¬æ¢ä¸º'
- en: lowercase and remove punctuation marks.
  id: totrans-1435
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è½¬æ¢ä¸ºå°å†™å¹¶åˆ é™¤æ ‡ç‚¹ç¬¦å·ã€‚
- en: '`[output_mode]`Â toÂ `[int]`Â to get the result as integers.Â `[tf_idf]`Â would
    apply theÂ TF-IDF algorithm.'
  id: totrans-1436
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[output_mode]`Â è½¬æ¢ä¸ºÂ `[int]`Â ä»¥è·å¾—æ•´æ•°ç»“æœã€‚Â `[tf_idf]`Â å°†åº”ç”¨Â TF-IDF ç®—æ³•ã€‚'
- en: '`[output_sequence_length]`Â as 50 to get sentences of that length. Change this
    number to see how it affects the model''s performance. I found 50 to five some
    good results. Sentences longer than the specified length will be truncated, while
    shorter ones will be padded with zeros.'
  id: totrans-1437
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[output_sequence_length]`Â è®¾ç½®ä¸º 50ï¼Œä»¥è·å¾—è¯¥é•¿åº¦çš„å¥å­ã€‚æ›´æ”¹æ­¤æ•°å­—ä»¥æŸ¥çœ‹å®ƒå¦‚ä½•å½±å“æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘å‘ç° 50 ç»™å‡ºäº†ä¸€äº›ä¸é”™çš„ç»“æœã€‚è¶…è¿‡æŒ‡å®šé•¿åº¦çš„å¥å­å°†è¢«æˆªæ–­ï¼Œè€Œè¾ƒçŸ­çš„å¥å­å°†ç”¨é›¶å¡«å……ã€‚'
- en: '`[max_tokens]`Â as 10,000 to have a vocabulary size of that number. Tweak this
    number and check how the model''s performance changes.'
  id: totrans-1438
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[max_tokens]`Â è®¾ç½®ä¸º 10,000ï¼Œä»¥è·å¾—è¯¥æ•°é‡çš„è¯æ±‡é‡ã€‚è°ƒæ•´æ­¤æ•°å­—å¹¶æ£€æŸ¥æ¨¡å‹æ€§èƒ½çš„å˜åŒ–ã€‚'
- en: After defining the vectorization layer, we apply it to the training data. This
    is done by calling theÂ `adapt`Â function. The function computes the vocabulary
    from the provided dataset. The vocabulary will be truncated to `[max_tokens]`,
    if that is provided.
  id: totrans-1439
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å®šä¹‰å‘é‡åŒ–å±‚åï¼Œæˆ‘ä»¬å°†å…¶åº”ç”¨äºè®­ç»ƒæ•°æ®ã€‚é€šè¿‡è°ƒç”¨Â `adapt`Â å‡½æ•°æ¥å®ç°ã€‚è¯¥å‡½æ•°ä»æä¾›çš„æ•°æ®é›†ä¸­è®¡ç®—è¯æ±‡è¡¨ã€‚å¦‚æœæä¾›äº†Â `[max_tokens]`ï¼Œåˆ™è¯æ±‡è¡¨å°†è¢«æˆªæ–­ã€‚
- en: '`import tensorflow as tf`'
  id: totrans-1440
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import tensorflow as tf`'
- en: '`max_features = 10000` # Maximum vocab size.'
  id: totrans-1441
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`max_features = 10000` # æœ€å¤§è¯æ±‡é‡å¤§å°ã€‚'
- en: '`batch_size = 128`'
  id: totrans-1442
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`batch_size = 128`'
- en: '`max_len = 50` # Sequence length to pad the outputs to. `vectorize_layer =
    tf.keras.layers.TextVectorization(standardize =''lower_and_strip_punctuation'',max_tokens=max_features,output_m
    ode=''int'',output_sequence_length=max_len)`'
  id: totrans-1443
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`max_len = 50` # åºåˆ—é•¿åº¦ï¼Œç”¨äºå¡«å……è¾“å‡ºã€‚`vectorize_layer = tf.keras.layers.TextVectorization(standardize
    =''lower_and_strip_punctuation'',max_tokens=max_features,output_m ode=''int'',output_sequence_length=max_len)`'
- en: '`vectorize_layer.adapt(X_train)`'
  id: totrans-1444
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`vectorize_layer.adapt(X_train)`'
- en: To view the generated vocabulary, call theÂ `get_vocabulary`Â function.`vectorize_layer.get_vocabulary()`![](../images/00042.jpeg)Convert
    the training and test data to numerical form using the trained vectorization layer.`X_train_padded
    = vectorize_layer(X_train)` `X_test_padded = vectorize_layer(X_test)`![](../images/00043.gif)
  id: totrans-1445
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¦æŸ¥çœ‹ç”Ÿæˆçš„è¯æ±‡è¡¨ï¼Œè¯·è°ƒç”¨Â `get_vocabulary`Â å‡½æ•°ã€‚`vectorize_layer.get_vocabulary()`ï¼[](../images/00042.jpeg)ä½¿ç”¨è®­ç»ƒå¥½çš„å‘é‡åŒ–å±‚å°†è®­ç»ƒå’Œæµ‹è¯•æ•°æ®è½¬æ¢ä¸ºæ•°å€¼å½¢å¼ã€‚`X_train_padded
    = vectorize_layer(X_train)` `X_test_padded = vectorize_layer(X_test)`ï¼[](../images/00043.gif)
- en: '**Create tf.data dataset**'
  id: totrans-1446
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**åˆ›å»º tf.data æ•°æ®é›†**'
- en: Let's generate and prefetch batches from the training and test set to make loading
    them to the LSTM model more efficient. We start by creating aÂ `tf.data.Dataset`.
  id: totrans-1447
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­ç”Ÿæˆå’Œé¢„å–æ‰¹æ¬¡ï¼Œä»¥ä½¿åŠ è½½åˆ° LSTM æ¨¡å‹æ›´é«˜æ•ˆã€‚æˆ‘ä»¬é¦–å…ˆåˆ›å»ºä¸€ä¸ªÂ `tf.data.Dataset`ã€‚
- en: '`training_data = tf.data.Dataset.from_tensor_slices((X_train_padded, y_train))`'
  id: totrans-1448
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`training_data = tf.data.Dataset.from_tensor_slices((X_train_padded, y_train))`'
- en: '`validation_data = tf.data.Dataset.from_tensor_slices((X_test_padded, y_test))`'
  id: totrans-1449
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`validation_data = tf.data.Dataset.from_tensor_slices((X_test_padded, y_test))`'
- en: '`training_data = training_data.batch(batch_size)`'
  id: totrans-1450
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`training_data = training_data.batch(batch_size)`'
- en: '`validation_data = validation_data.batch(batch_size)`'
  id: totrans-1451
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`validation_data = validation_data.batch(batch_size)`'
- en: Next, we prefetch one batch, shuffle the data and return it as a `NumPy array.`
  id: totrans-1452
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬é¢„å–ä¸€ä¸ªæ‰¹æ¬¡ï¼Œæ´—ç‰Œæ•°æ®ï¼Œå¹¶å°†å…¶ä½œä¸º `NumPy array` è¿”å›ã€‚
- en: '`pip install tensorflow_datasets`'
  id: totrans-1453
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`pip install tensorflow_datasets`'
- en: '`import tensorflow_datasets as tfds`'
  id: totrans-1454
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import tensorflow_datasets as tfds`'
- en: '`def get_train_batches():`'
  id: totrans-1455
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def get_train_batches():`'
- en: '`ds = training_data.prefetch(1)`'
  id: totrans-1456
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`ds = training_data.prefetch(1)`'
- en: '`ds = ds.shuffle(3, reshuffle_each_iteration=True)` # `tfds.dataset_as_numpy`
    converts the `tf.data.Dataset` into an'
  id: totrans-1457
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`ds = ds.shuffle(3, reshuffle_each_iteration=True)` # `tfds.dataset_as_numpy`
    å°† `tf.data.Dataset` è½¬æ¢ä¸ºä¸€ä¸ª'
- en: iterable of NumPy arrays`return tfds.as_numpy(ds)`
  id: totrans-1458
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ç”± NumPy æ•°ç»„ç»„æˆçš„è¿­ä»£å™¨ `return tfds.as_numpy(ds)`
- en: '`Define LSTM model in Flax`'
  id: totrans-1459
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '`åœ¨ Flax ä¸­å®šä¹‰ LSTM æ¨¡å‹`'
- en: We are now ready to define the LSTM model in Flax. To design LSTMs in Flax,
    we use the `LSTMCell` or the `OptimizedLSTMCell`.
  id: totrans-1460
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å¯ä»¥åœ¨ Flax ä¸­å®šä¹‰ LSTM æ¨¡å‹äº†ã€‚è¦åœ¨ Flax ä¸­è®¾è®¡ LSTMï¼Œæˆ‘ä»¬ä½¿ç”¨ `LSTMCell` æˆ– `OptimizedLSTMCell`ã€‚
- en: The `OptimizedLSTMCell` is the efficient `LSTMCell`.
  id: totrans-1461
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`OptimizedLSTMCell` æ˜¯é«˜æ•ˆçš„ `LSTMCell`ã€‚'
- en: 'The `[LSTMCell.initialize_carry]` function is used to initialize the hidden
    state of the LSTM cell. It expects:'
  id: totrans-1462
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[LSTMCell.initialize_carry]` å‡½æ•°ç”¨äºåˆå§‹åŒ– LSTM å•å…ƒçš„éšè—çŠ¶æ€ã€‚å®ƒæœŸæœ›ï¼š'
- en: A random number.
  id: totrans-1463
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªéšæœºæ•°ã€‚
- en: The batch dimensions.
  id: totrans-1464
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ‰¹æ¬¡ç»´åº¦ã€‚
- en: The number of units.
  id: totrans-1465
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å•å…ƒçš„æ•°é‡ã€‚
- en: 'Let''s use the `setup method` to define the LSTM model. The LSTM contains the
    following layers:'
  id: totrans-1466
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä½¿ç”¨ `setup æ–¹æ³•` æ¥å®šä¹‰ LSTM æ¨¡å‹ã€‚LSTM åŒ…å«ä»¥ä¸‹å±‚ï¼š
- en: An `Embedding layer` with the same number of features and length as defined
    in the vectorization layer.
  id: totrans-1467
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªä¸å‘é‡åŒ–å±‚ä¸­å®šä¹‰çš„ç‰¹å¾æ•°å’Œé•¿åº¦ç›¸åŒçš„ `Embedding layer`ã€‚
- en: LSTM layers that pass data in one direction as specified by the `[reverse]`
    argument.
  id: totrans-1468
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: LSTM å±‚æ ¹æ® `[reverse]` å‚æ•°åœ¨ä¸€ä¸ªæ–¹å‘ä¸Šä¼ é€’æ•°æ®ã€‚
- en: A couple of `Dense layers`.
  id: totrans-1469
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä¸€å¯¹ `Dense layers`ã€‚
- en: Final dense output layer.from flax import linen as nn
  id: totrans-1470
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æœ€ç»ˆçš„å¯†é›†è¾“å‡ºå±‚ã€‚from flax import linen as nn
- en: '`class LSTMModel(nn.Module):`'
  id: totrans-1471
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`class LSTMModel(nn.Module):`'
- en: '`def setup(self):`'
  id: totrans-1472
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def setup(self):`'
- en: '`self.embedding = nn.Embed(max_features, max_len) lstm_layer = nn.scan(nn.OptimizedLSTMCell,`'
  id: totrans-1473
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.embedding = nn.Embed(max_features, max_len) lstm_layer = nn.scan(nn.OptimizedLSTMCell,`'
- en: '`variable_broadcast="params", split_rngs={"params": False}, in_axes=1,`'
  id: totrans-1474
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`variable_broadcast="params", split_rngs={"params": False}, in_axes=1,`'
- en: '`out_axes=1,`'
  id: totrans-1475
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`out_axes=1,`'
- en: '`length=max_len,`'
  id: totrans-1476
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`length=max_len,`'
- en: '`reverse=False)`'
  id: totrans-1477
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`reverse=False)`'
- en: '`self.lstm1 = lstm_layer()`'
  id: totrans-1478
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.lstm1 = lstm_layer()`'
- en: '`self.dense1 = nn.Dense(256)`'
  id: totrans-1479
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.dense1 = nn.Dense(256)`'
- en: '`self.lstm2 = lstm_layer()`'
  id: totrans-1480
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.lstm2 = lstm_layer()`'
- en: '`self.dense2 = nn.Dense(128)`'
  id: totrans-1481
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.dense2 = nn.Dense(128)`'
- en: '`self.lstm3 = lstm_layer()`'
  id: totrans-1482
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.lstm3 = lstm_layer()`'
- en: '`self.dense3 = nn.Dense(64)`'
  id: totrans-1483
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.dense3 = nn.Dense(64)`'
- en: '`self.dense4 = nn.Dense(2)`'
  id: totrans-1484
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.dense4 = nn.Dense(2)`'
- en: '`@nn.rematdef __call__(self, x_batch): x = self.embedding(x_batch)`'
  id: totrans-1485
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`@nn.rematdef __call__(self, x_batch): x = self.embedding(x_batch)`'
- en: '`carry, hidden = nn.OptimizedLSTMCell.initialize_carry(jax.random.PRNGKey(0),
    batch_dims=(len(x_batch),), size=128)`'
  id: totrans-1486
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`carry, hidden = nn.OptimizedLSTMCell.initialize_carry(jax.random.PRNGKey(0),
    batch_dims=(len(x_batch),), size=128)`'
- en: '`(carry, hidden), x = self.lstm1((carry, hidden), x)`'
  id: totrans-1487
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`(carry, hidden), x = self.lstm1((carry, hidden), x)`'
- en: '`x = self.dense1(x) x = nn.relu(x)`'
  id: totrans-1488
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = self.dense1(x) x = nn.relu(x)`'
- en: '`carry, hidden = nn.OptimizedLSTMCell.initialize_carry(jax.random.PRNGKey(0),
    batch_dims=(len(x_batch),), size=64)`'
  id: totrans-1489
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`carry, hidden = nn.OptimizedLSTMCell.initialize_carry(jax.random.PRNGKey(0),
    batch_dims=(len(x_batch),), size=64)`'
- en: '`(carry, hidden), x = self.lstm2((carry, hidden), x) x = self.dense2(x) x =
    nn.relu(x)`'
  id: totrans-1490
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`(carry, hidden), x = self.lstm2((carry, hidden), x) x = self.dense2(x) x =
    nn.relu(x)`'
- en: '`carry, hidden = nn.OptimizedLSTMCell.initialize_carry(jax.random.PRNGKey(0),
    batch_dims=(len(x_batch),), size=32)`'
  id: totrans-1491
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`carry, hidden = nn.OptimizedLSTMCell.initialize_carry(jax.random.PRNGKey(0),
    batch_dims=(len(x_batch),), size=32)`'
- en: '`(carry, hidden), x = self.lstm3((carry, hidden), x)`'
  id: totrans-1492
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`(carry, hidden), x = self.lstm3((carry, hidden), x)`'
- en: '`x = self.dense3(x)`'
  id: totrans-1493
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = self.dense3(x)`'
- en: '`x = nn.relu(x)`'
  id: totrans-1494
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = nn.relu(x)`'
- en: '`x = self.dense4(x[:, -1]) return nn.log_softmax(x)`'
  id: totrans-1495
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = self.dense4(x[:, -1]) return nn.log_softmax(x)`'
- en: We apply the `scan function` to iterate over the data. It expects:`scan` the
    items to be looped over. They must be the same size and will be stacked along
    the scan axis.`carry` a carried value that is updated at each iteration. The value
    must be the same shape and `[dtype]` throughout the iteration.
  id: totrans-1496
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°† `scan å‡½æ•°` åº”ç”¨äºæ•°æ®çš„è¿­ä»£ã€‚å®ƒæœŸæœ›ï¼š`scan` å¾…å¾ªç¯çš„é¡¹ç›®ã€‚å®ƒä»¬å¿…é¡»å…·æœ‰ç›¸åŒçš„å¤§å°ï¼Œå¹¶ä¸”å°†æ²¿ç€æ‰«æè½´å †å ã€‚`carry` ä¸€ä¸ªåœ¨æ¯æ¬¡è¿­ä»£ä¸­æ›´æ–°çš„ä¼ é€’å€¼ã€‚è¯¥å€¼åœ¨æ•´ä¸ªè¿­ä»£è¿‡ç¨‹ä¸­å¿…é¡»å…·æœ‰ç›¸åŒçš„å½¢çŠ¶å’Œ
    `[dtype]`ã€‚
- en: '`[broadcast]` a value that is closed over by the loop ` [<axis:int>]` axis
    along which to scan.'
  id: totrans-1497
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[å¹¿æ’­]` ä¸€ä¸ªåœ¨å¾ªç¯ä¸­å°é—­çš„å€¼ ` [<axis:int>]` æ‰«æçš„è½´ã€‚'
- en: '`[split_rngs]` to define if to split the random number generator at each step.'
  id: totrans-1498
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[split_rngs]` ç”¨äºå®šä¹‰æ˜¯å¦åœ¨æ¯ä¸€æ­¥åˆ†å‰²éšæœºæ•°ç”Ÿæˆå™¨ã€‚'
- en: TheÂ [nn.remat]Â call saves memory when using LSTMs to compute long sequences.
  id: totrans-1499
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨ä½¿ç”¨ LSTMs è®¡ç®—é•¿åºåˆ—æ—¶ï¼Œ`[nn.remat]` è°ƒç”¨èŠ‚çœå†…å­˜ã€‚
- en: Compute metrics in Flax
  id: totrans-1500
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨ Flax ä¸­è®¡ç®—æŒ‡æ ‡
- en: Next, we define a function to compute the loss and accuracy of the network.
  id: totrans-1501
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—ç½‘ç»œçš„æŸå¤±å’Œå‡†ç¡®ç‡ã€‚
- en: import optax
  id: totrans-1502
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: import optax
- en: import jax.numpy as jnp
  id: totrans-1503
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: import jax.numpy as jnp
- en: 'def compute_metrics(logits, labels):'
  id: totrans-1504
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def compute_metrics(logits, labels):'
- en: loss = jnp.mean(optax.softmax_cross_entropy(logits, jax.nn.on e_hot(labels,
    num_classes=2)))
  id: totrans-1505
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: loss = jnp.mean(optax.softmax_cross_entropy(logits, jax.nn.on e_hot(labels,
    num_classes=2)))
- en: accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)
  id: totrans-1506
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)
- en: metrics = {
  id: totrans-1507
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: metrics = {
- en: '''loss'': loss,'
  id: totrans-1508
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '''loss'': loss,'
- en: '''accuracy'': accuracy'
  id: totrans-1509
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '''accuracy'': accuracy'
- en: '}'
  id: totrans-1510
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '}'
- en: return metrics
  id: totrans-1511
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: return metrics
- en: Create training state
  id: totrans-1512
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: åˆ›å»ºè®­ç»ƒçŠ¶æ€
- en: 'The training state applies gradients and updates the parameters and optimizer
    state. Flax providesÂ [train_state]Â for this purpose. We define a function that:'
  id: totrans-1513
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è®­ç»ƒçŠ¶æ€åº”ç”¨æ¢¯åº¦å¹¶æ›´æ–°å‚æ•°å’Œä¼˜åŒ–å™¨çŠ¶æ€ã€‚Flaxæä¾›äº†[train_state]ç”¨äºæ­¤ç›®çš„ã€‚æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼š
- en: Creates an instance of theÂ [LSTMModel].
  id: totrans-1514
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åˆ›å»º[LSTMModel]çš„ä¸€ä¸ªå®ä¾‹ã€‚
- en: Initializes the model to obtain theÂ [`params`]Â by passing a sample of the training
    data.
  id: totrans-1515
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åˆå§‹åŒ–æ¨¡å‹ä»¥é€šè¿‡è®­ç»ƒæ•°æ®æ ·æœ¬è·å–[`params`]ã€‚
- en: Returns the created state after applying the Adam optimizer.from flax.training
    import train_state
  id: totrans-1516
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨åº”ç”¨Adamä¼˜åŒ–å™¨åè¿”å›åˆ›å»ºçš„çŠ¶æ€ã€‚from flax.training import train_state
- en: 'def create_train_state(rng):'
  id: totrans-1517
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def create_train_state(rng):'
- en: '"""Creates initial `TrainState`."""'
  id: totrans-1518
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""åˆ›å»ºåˆå§‹`TrainState`ã€‚"""'
- en: model = LSTMModel()
  id: totrans-1519
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: model = LSTMModel()
- en: params = model.init(rng, jnp.array(X_train_padded[0]))['param
  id: totrans-1520
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: params = model.init(rng, jnp.array(X_train_padded[0]))['param
- en: s']
  id: totrans-1521
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: s']
- en: tx = optax.adam(0.001,0.9,0.999,1e-07) return train_state.TrainState.create(
  id: totrans-1522
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: tx = optax.adam(0.001,0.9,0.999,1e-07) return train_state.TrainState.create(
- en: apply_fn=model.apply, params=params, tx=tx)
  id: totrans-1523
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: apply_fn=model.apply, params=params, tx=tx)
- en: Define training step
  id: totrans-1524
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: å®šä¹‰è®­ç»ƒæ­¥éª¤
- en: 'The training function does the following:'
  id: totrans-1525
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'è®­ç»ƒå‡½æ•°æ‰§è¡Œä»¥ä¸‹æ“ä½œ:'
- en: Compute the loss and logits from the model with theÂ `apply`Â method.
  id: totrans-1526
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`apply`æ–¹æ³•ä»æ¨¡å‹è®¡ç®—æŸå¤±å’Œlogitsã€‚
- en: Compute the gradients usingÂ [value_and_grad]. Use the gradients to update the
    model parameters.
  id: totrans-1527
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[value_and_grad]è®¡ç®—æ¢¯åº¦ã€‚ä½¿ç”¨æ¢¯åº¦æ›´æ–°æ¨¡å‹å‚æ•°ã€‚
- en: Compute the metrics using the function defined earlier. Returns theÂ stateÂ and
    metrics.
  id: totrans-1528
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å…ˆå‰å®šä¹‰çš„å‡½æ•°è®¡ç®—metricsã€‚è¿”å›çŠ¶æ€å’Œmetricsã€‚
- en: 'ApplyingÂ [@jax.jit]Â makes the function run faster.`@jax.jit`def train_step(state,
    text, labels):def loss_fn(params):'
  id: totrans-1529
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'åº”ç”¨[@jax.jit]ä½¿å‡½æ•°è¿è¡Œæ›´å¿«ã€‚`@jax.jit`def train_step(state, text, labels):def loss_fn(params):'
- en: 'logits = LSTMModel().apply({''params'': params}, text) loss = jnp.mean(optax.softmax_cross_entropy(
    logits=logits,'
  id: totrans-1530
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'logits = LSTMModel().apply({''params'': params}, text) loss = jnp.mean(optax.softmax_cross_entropy(
    logits=logits,'
- en: labels=jax.nn.one_hot(labels, num_classes=2))) return loss, logits
  id: totrans-1531
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: labels=jax.nn.one_hot(labels, num_classes=2))) return loss, logits
- en: grad_fn = jax.value_and_grad(loss_fn, has_aux=True) (_, logits), grads = grad_fn(state.params)
  id: totrans-1532
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: grad_fn = jax.value_and_grad(loss_fn, has_aux=True) (_, logits), grads = grad_fn(state.params)
- en: state = state.apply_gradients(grads=grads)
  id: totrans-1533
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: state = state.apply_gradients(grads=grads)
- en: metrics = compute_metrics(logits, labels)
  id: totrans-1534
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: metrics = compute_metrics(logits, labels)
- en: return state, metrics
  id: totrans-1535
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¿”å›çŠ¶æ€å’Œmetrics
- en: Evaluate the Flax model
  id: totrans-1536
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: è¯„ä¼°Flaxæ¨¡å‹
- en: TheÂ [`eval_step`]Â evaluates the model's performance on the test set usingÂ `Module.apply`.
    It returns the loss and accuracy on the testing set.
  id: totrans-1537
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '[`eval_step`]ä½¿ç”¨`Module.apply`è¯„ä¼°æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°ã€‚å®ƒè¿”å›æµ‹è¯•é›†ä¸Šçš„æŸå¤±å’Œå‡†ç¡®ç‡ã€‚'
- en: TheÂ [`evaluate_model`]Â function applies theÂ [`eval_step`]Â , obtains the metrics
    from the device and returns them as aÂ [`jax.tree_map`].
  id: totrans-1538
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '[`evaluate_model`]å‡½æ•°åº”ç”¨[`eval_step`]ï¼Œä»è®¾å¤‡è·å–metricsï¼Œå¹¶ä½œä¸º[`jax.tree_map`]è¿”å›å®ƒä»¬ã€‚'
- en: '@jax.jit'
  id: totrans-1539
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '@jax.jit'
- en: 'def eval_step(state, text, labels):'
  id: totrans-1540
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def eval_step(state, text, labels):'
- en: 'logits = LSTMModel().apply({''params'': state.params}, text)'
  id: totrans-1541
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'logits = LSTMModel().apply({''params'': state.params}, text)'
- en: return compute_metrics(logits=logits, labels=labels)
  id: totrans-1542
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¿”å›ä½¿ç”¨logitså’Œlabelsè®¡ç®—metricsçš„ç»“æœ
- en: 'def evaluate_model(state, text, test_lbls): """Evaluate on the validation set."""'
  id: totrans-1543
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def evaluate_model(state, text, test_lbls): """åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹ã€‚"""'
- en: metrics = eval_step(state, text, test_lbls) metrics = jax.device_get(metrics)
  id: totrans-1544
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: metrics = eval_step(state, text, test_lbls) metrics = jax.device_get(metrics)
- en: 'metrics = jax.tree_map(lambda x: x.item(), metrics) return metrics'
  id: totrans-1545
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'metrics = jax.tree_map(lambda x: x.item(), metrics) return metrics'
- en: Create training function
  id: totrans-1546
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: åˆ›å»ºè®­ç»ƒå‡½æ•°
- en: Next, define a function that trains the Flax LSTM model on one epoch. The function
    applies Â [train_step]Â to each batch in the training data. After each batch, it
    appends the metrics to a list.
  id: totrans-1547
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œå®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œåœ¨ä¸€ä¸ªepochä¸Šè®­ç»ƒFlax LSTMæ¨¡å‹ã€‚è¯¥å‡½æ•°å¯¹è®­ç»ƒæ•°æ®ä¸­çš„æ¯ä¸ªæ‰¹æ¬¡åº”ç”¨[train_step]ã€‚åœ¨æ¯ä¸ªæ‰¹æ¬¡ä¹‹åï¼Œå®ƒå°†metricsé™„åŠ åˆ°ä¸€ä¸ªåˆ—è¡¨ä¸­ã€‚
- en: 'def train_one_epoch(state):'
  id: totrans-1548
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def train_one_epoch(state):'
- en: Train for 1 epoch on the training set. batch_metrics = []
  id: totrans-1549
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒ1ä¸ªepochã€‚batch_metrics = []
- en: 'for text, labels in get_train_batches():'
  id: totrans-1550
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'å¯¹äºtext, labels in get_train_batches():'
- en: 'state, metrics = `train_step(state, text, labels)` batch_metrics.append(metrics)batch_metrics_np
    = jax.device_get(batch_metrics) epoch_metrics_np = {k: np.mean([metrics[k] for
    metrics in batch_metrics_np[0] ])for k in batch_metrics_np[0] }return state, epoch_metrics_np'
  id: totrans-1551
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'state, metrics = `train_step(state, text, labels)` batch_metrics.append(metrics)batch_metrics_np
    = jax.device_get(batch_metrics) epoch_metrics_np = {k: np.mean([metrics[k] for
    metrics in batch_metrics_np[0] ])for k in batch_metrics_np[0] }return state, epoch_metrics_np'
- en: The function obtains the metrics from the device and computes the mean from
    all the trained batches. This gives the loss and accuracy for one epoch.
  id: totrans-1552
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å‡½æ•°ä»è®¾å¤‡è·å–æŒ‡æ ‡å¹¶è®¡ç®—æ‰€æœ‰è®­ç»ƒæ‰¹æ¬¡çš„å¹³å‡å€¼ã€‚è¿™æä¾›äº†ä¸€ä¸ªæ—¶æœŸçš„æŸå¤±å’Œå‡†ç¡®æ€§ã€‚
- en: Train LSTM model in Flax
  id: totrans-1553
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨ Flax ä¸­è®­ç»ƒ LSTM æ¨¡å‹
- en: To train the LSTM model, we run theÂ [train_one_epoch]Â function for several iterations.
    Next, apply theÂ [evaluate_model]Â to obtain the test metrics for each epoch. Before
    training starts, we create
  id: totrans-1554
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¦è®­ç»ƒ LSTM æ¨¡å‹ï¼Œæˆ‘ä»¬è¿è¡ŒÂ [train_one_epoch]Â å‡½æ•°è¿›è¡Œå¤šæ¬¡è¿­ä»£ã€‚æ¥ä¸‹æ¥ï¼Œåº”ç”¨Â [evaluate_model]Â è·å–æ¯ä¸ªæ—¶æœŸçš„æµ‹è¯•æŒ‡æ ‡ã€‚åœ¨å¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬åˆ›å»º
- en: aÂ [create_train_state]Â to hold the training information. The function initializes
    the model parameters and the optimizer. This information is stored in the training
    stateÂ dataclass.
  id: totrans-1555
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Â [create_train_state]Â æ¥ä¿å­˜è®­ç»ƒä¿¡æ¯ã€‚è¯¥å‡½æ•°åˆå§‹åŒ–æ¨¡å‹å‚æ•°å’Œä¼˜åŒ–å™¨ã€‚æ­¤ä¿¡æ¯å­˜å‚¨åœ¨è®­ç»ƒçŠ¶æ€Â dataclassÂ ä¸­ã€‚
- en: 'rng = jax.random.PRNGKey(0)rng, input_rng, init_rng = jax.random.split(rng,num=3)seed
    = 0state = `create_train_state(init_rng)` del init_rng # Must not be used anymore.'
  id: totrans-1556
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'rng = jax.random.PRNGKey(0)rng, input_rng, init_rng = jax.random.split(rng,num=3)seed
    = 0state = `create_train_state(init_rng)` del init_rng # ä¸å†ä½¿ç”¨ã€‚'
- en: num_epochs = 30
  id: totrans-1557
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: num_epochs = 30
- en: (text, test_labels) = next(iter(validation_data)) text = jnp.array(text)
  id: totrans-1558
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: (text, test_labels) = next(iter(validation_data)) text = jnp.array(text)
- en: test_labels = jnp.array(test_labels) training_loss = []
  id: totrans-1559
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: test_labels = jnp.array(test_labels) training_loss = []
- en: training_accuracy = []
  id: totrans-1560
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: training_accuracy = []
- en: testing_loss = []
  id: totrans-1561
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: testing_loss = []
- en: testing_accuracy = []
  id: totrans-1562
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: testing_accuracy = []
- en: 'def `train_model()`:'
  id: totrans-1563
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def `train_model()`:'
- en: 'for epoch in range(1, num_epochs + 1):'
  id: totrans-1564
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'for epoch in range(1, num_epochs + 1):'
- en: train_state, train_metrics = `train_one_epoch(state)` training_loss.append(train_metrics['loss'])
    training_accuracy.append(train_metrics['accuracy']) test_metrics = `evaluate_model(train_state,
    text, test_l`
  id: totrans-1565
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: train_state, train_metrics = `train_one_epoch(state)` training_loss.append(train_metrics['loss'])
    training_accuracy.append(train_metrics['accuracy']) test_metrics = `evaluate_model(train_state,
    text, test_l`
- en: abels)
  id: totrans-1566
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: abels)
- en: testing_loss.append(test_metrics['loss'])
  id: totrans-1567
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: testing_loss.append(test_metrics['loss'])
- en: 'testing_accuracy.append(test_metrics[''accuracy'']) print(f"Epoch: {epoch},
    train loss: {train_metrics[''los'
  id: totrans-1568
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'testing_accuracy.append(test_metrics[''accuracy'']) print(f"Epoch: {epoch},
    train loss: {train_metrics[''los'
- en: 's'']}, train accuracy: {train_metrics[''accuracy''] * 100}, test l oss: {test_metrics[''loss'']},
    test accuracy: {test_metrics[''accu racy''] * 100}")'
  id: totrans-1569
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 's'']}, train accuracy: {train_metrics[''accuracy''] * 100}, test l oss: {test_metrics[''loss'']},
    test accuracy: {test_metrics[''accu racy''] * 100}")'
- en: return `train_statetrained_model_state = train_model()`After each epoch, we
    print the metrics and append them to a list.
  id: totrans-1570
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¿”å› `train_statetrained_model_state = train_model()`æ¯ä¸ªæ—¶æœŸç»“æŸåï¼Œæˆ‘ä»¬æ‰“å°æŒ‡æ ‡å¹¶å°†å…¶é™„åŠ åˆ°åˆ—è¡¨ä¸­ã€‚
- en: Visualize LSTM model performance in Flax
  id: totrans-1571
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨ Flax ä¸­å¯è§†åŒ– LSTM æ¨¡å‹æ€§èƒ½
- en: You can then useÂ `Matplotlib`Â to visualize the metrics appended to the list.
    The training is not quite smooth, but you can tweak the architecture of the network,
    the length of each review, and the vocabulary size to improve performance.
  id: totrans-1572
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨Â `Matplotlib`Â æ¥å¯è§†åŒ–é™„åŠ åˆ°åˆ—è¡¨ä¸­çš„æŒ‡æ ‡ã€‚è®­ç»ƒè¿‡ç¨‹ä¸æ˜¯å¾ˆå¹³ç¨³ï¼Œä½†æ‚¨å¯ä»¥è°ƒæ•´ç½‘ç»œçš„æ¶æ„ã€æ¯ä¸ªè¯„å®¡çš„é•¿åº¦å’Œè¯æ±‡é‡å¤§å°ä»¥æé«˜æ€§èƒ½ã€‚
- en: Save LSTM model
  id: totrans-1573
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: ä¿å­˜ LSTM æ¨¡å‹
- en: 'To save a Flax model checkpoint, use theÂ [save_checkpoint]Â method. It expects:'
  id: totrans-1574
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¦ä¿å­˜ Flax æ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œè¯·ä½¿ç”¨Â [save_checkpoint]Â æ–¹æ³•ã€‚å®ƒéœ€è¦ï¼š
- en: The directory to save the checkpoint files.
  id: totrans-1575
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä¿å­˜æ£€æŸ¥ç‚¹æ–‡ä»¶çš„ç›®å½•ã€‚
- en: The Flax object to be saved, that is,Â [target].
  id: totrans-1576
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¦ä¿å­˜çš„ Flax å¯¹è±¡ï¼Œå³Â [target]ã€‚
- en: The prefix of the checkpoint file name.
  id: totrans-1577
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ£€æŸ¥ç‚¹æ–‡ä»¶åçš„å‰ç¼€ã€‚
- en: Whether to overwrite previous checkpoints
  id: totrans-1578
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ˜¯å¦è¦†ç›–å…ˆå‰çš„æ£€æŸ¥ç‚¹
- en: from flax.training import `checkpoints`
  id: totrans-1579
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: from flax.training import `checkpoints`
- en: checkpoints.save_checkpoint(ckpt_dir='lstm_model_checkpoint/', target=trained_model_state,
  id: totrans-1580
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: checkpoints.save_checkpoint(ckpt_dir='lstm_model_checkpoint/', target=trained_model_state,
- en: step=100,
  id: totrans-1581
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: step=100,
- en: prefix='lstm_model',
  id: totrans-1582
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: prefix='lstm_model',
- en: overwrite=False
  id: totrans-1583
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: overwrite=False
- en: )
  id: totrans-1584
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: )
- en: To restore the saved model, useÂ [restore_checkpoint]Â method.
  id: totrans-1585
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¦æ¢å¤ä¿å­˜çš„æ¨¡å‹ï¼Œè¯·ä½¿ç”¨Â [restore_checkpoint]Â æ–¹æ³•ã€‚
- en: loaded_model = `checkpoints.restore_checkpoint(`
  id: totrans-1586
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: loaded_model = `checkpoints.restore_checkpoint(`
- en: ckpt_dir='lstm_mod
  id: totrans-1587
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ckpt_dir='lstm_mod
- en: el_checkpoint/',
  id: totrans-1588
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: el_checkpoint/',
- en: target=state, prefix='lstm_mode
  id: totrans-1589
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: target=state, prefix='lstm_mode
- en: l'
  id: totrans-1590
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: l'
- en: )
  id: totrans-1591
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: )
- en: loaded_model ![](../images/00044.gif)This model can be used to make predictions
    right away.![](../images/00045.gif)
  id: totrans-1592
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: loaded_model ![](../images/00044.gif)æ­¤æ¨¡å‹å¯ç«‹å³ç”¨äºè¿›è¡Œé¢„æµ‹ï¼[](../images/00045.gif)
- en: Final thoughts
  id: totrans-1593
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: æœ€ç»ˆæƒ³æ³•
- en: '`You have learned to solve natural language processing problems with JAX and
    Flax in this article. In particular, the nuggets you have covered include:`'
  id: totrans-1594
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`åœ¨æœ¬æ–‡ä¸­ï¼Œæ‚¨å·²ç»å­¦ä¼šäº†å¦‚ä½•åœ¨ JAX å’Œ Flax ä¸­è§£å†³è‡ªç„¶è¯­è¨€å¤„ç†é—®é¢˜ã€‚ç‰¹åˆ«æ˜¯æ‚¨æ¶µç›–çš„å…³é”®ç‚¹åŒ…æ‹¬ï¼š`'
- en: '`How to process text data with NLTK.`'
  id: totrans-1595
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`å¦‚ä½•ä½¿ç”¨ NLTK å¤„ç†æ–‡æœ¬æ•°æ®ã€‚`'
- en: '`Text vectorization with Keras.`'
  id: totrans-1596
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`ä½¿ç”¨ Keras è¿›è¡Œæ–‡æœ¬å‘é‡åŒ–ã€‚`'
- en: Creating batches of text data with Keras and TensorFlow.
  id: totrans-1597
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Keras å’Œ TensorFlow åˆ›å»ºæ–‡æœ¬æ•°æ®æ‰¹å¤„ç†ã€‚
- en: '`How to create LSTM models in JAX and Flax. How to train and evaluate the LSTM
    model in Flax. Saving and restoring Flax LSTM models.`'
  id: totrans-1598
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`å¦‚ä½•åœ¨ JAX å’Œ Flax ä¸­åˆ›å»º LSTM æ¨¡å‹ã€‚å¦‚ä½•åœ¨ Flax ä¸­è®­ç»ƒå’Œè¯„ä¼° LSTM æ¨¡å‹ã€‚ä¿å­˜å’Œæ¢å¤ Flax LSTM æ¨¡å‹ã€‚`'
- en: '**Flax vs. TensorFlow**'
  id: totrans-1599
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**Flax vs. TensorFlow**'
- en: '`Flax is the neural network library for JAX. TensorFlow is a deep learning
    library with a large ecosystem of tools and resources. Flax and TensorFlow are
    similar but different in some ways. For instance, both Flax and TensorFlow can
    run on XLA.`'
  id: totrans-1600
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Flax æ˜¯å»ºç«‹åœ¨ JAX ä¸Šçš„ç¥ç»ç½‘ç»œåº“ã€‚TensorFlow æ˜¯ä¸€ä¸ªæ‹¥æœ‰å¤§é‡å·¥å…·å’Œèµ„æºçš„æ·±åº¦å­¦ä¹ åº“ã€‚Flax å’Œ TensorFlow åœ¨æŸäº›æ–¹é¢ç›¸ä¼¼ä½†åˆä¸åŒã€‚ä¾‹å¦‚ï¼ŒFlax
    å’Œ TensorFlow éƒ½å¯ä»¥åœ¨ XLA ä¸Šè¿è¡Œã€‚`'
- en: '`Let''s look at the differences between Flax and TensorFlow from my perspective
    as a user of both libraries.`'
  id: totrans-1601
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»ä½¿ç”¨è¿™ä¸¤ä¸ªåº“çš„ç”¨æˆ·è§’åº¦æ¥çœ‹ä¸€ä¸‹ Flax å’Œ TensorFlow çš„åŒºåˆ«ã€‚
- en: '**Random number generation in TensorFlow and Flax**'
  id: totrans-1602
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**TensorFlow å’Œ Flax ä¸­çš„éšæœºæ•°ç”Ÿæˆ**'
- en: '`In TensorFlow, you can set global or function level seeds. Generating random
    numbers in TensorFlow is quite straightforward.`tf.random.set_seed(6853)`'
  id: totrans-1603
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`åœ¨ TensorFlow ä¸­ï¼Œæ‚¨å¯ä»¥è®¾ç½®å…¨å±€æˆ–å‡½æ•°çº§ç§å­ã€‚åœ¨ TensorFlow ä¸­ç”Ÿæˆéšæœºæ•°éå¸¸ç®€å•ã€‚`tf.random.set_seed(6853)`'
- en: However, this is not the case in Flax. Flax is built on top of JAX. JAX expects
    pure functions, meaning functions without any side effects. To achieve this JAX
    introduces stateless pseudo-random number generators (PRNGs). For example, calling
    the random number generator from NumPy will result in a different number every
    time.
  id: totrans-1604
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œåœ¨ Flax ä¸­å¹¶éå¦‚æ­¤ã€‚Flax æ˜¯å»ºç«‹åœ¨ JAX ä¹‹ä¸Šçš„ã€‚JAX æœŸæœ›çº¯å‡½æ•°ï¼Œå³æ²¡æœ‰ä»»ä½•å‰¯ä½œç”¨çš„å‡½æ•°ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼ŒJAX å¼•å…¥äº†æ— çŠ¶æ€çš„ä¼ªéšæœºæ•°ç”Ÿæˆå™¨ï¼ˆPRNGsï¼‰ã€‚ä¾‹å¦‚ï¼Œä»
    NumPy è°ƒç”¨éšæœºæ•°ç”Ÿæˆå™¨æ¯æ¬¡éƒ½ä¼šå¾—åˆ°ä¸åŒçš„æ•°å­—ã€‚
- en: '`import numpy as np`'
  id: totrans-1605
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import numpy as np`'
- en: '`print(np.random.random()) print(np.random.random()) print(np.random.random())`'
  id: totrans-1606
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`print(np.random.random()) print(np.random.random()) print(np.random.random())`'
- en: '`![](../images/00046.jpeg)`'
  id: totrans-1607
  prefs: []
  stylish: true
  type: TYPE_IMG
  zh: '`![](../images/00046.jpeg)`'
- en: '`In JAX and Flax, the result should be the same on every call. We, therefore,
    generate random numbers from a random state. The state should not be re-used.
    It can be split to obtain several pseudo-random numbers.`'
  id: totrans-1608
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`åœ¨ JAX å’Œ Flax ä¸­ï¼Œæ¯æ¬¡è°ƒç”¨æ—¶ç»“æœåº”è¯¥ç›¸åŒã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä»éšæœºçŠ¶æ€ç”Ÿæˆéšæœºæ•°ã€‚çŠ¶æ€ä¸åº”è¯¥è¢«é‡ç”¨ã€‚å¯ä»¥æ‹†åˆ†çŠ¶æ€ä»¥è·å–å¤šä¸ªä¼ªéšæœºæ•°ã€‚`'
- en: '`import jax`'
  id: totrans-1609
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import jax`'
- en: '`key = jax.random.PRNGKey(0)`'
  id: totrans-1610
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`key = jax.random.PRNGKey(0)`'
- en: '`key1, key2, key3 = jax.random.split(key, num=3)`'
  id: totrans-1611
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`key1, key2, key3 = jax.random.split(key, num=3)`'
- en: '`![](../images/00047.gif)`'
  id: totrans-1612
  prefs: []
  stylish: true
  type: TYPE_IMG
  zh: '`![](../images/00047.gif)`'
- en: '**Model definition in Flax and TensorFlow**'
  id: totrans-1613
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**Flax å’Œ TensorFlow çš„æ¨¡å‹å®šä¹‰**'
- en: '`Model definition in TensorFlow is made easy by the Keras API. You can use
    Keras to define Sequential or Functional networks. Keras has many layers for designing
    various types of networks, such as CNNs, and LSTMS.`'
  id: totrans-1614
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`åœ¨ TensorFlow ä¸­ï¼Œé€šè¿‡ Keras API å¯ä»¥è½»æ¾å®šä¹‰æ¨¡å‹ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ Keras å®šä¹‰é¡ºåºæˆ–åŠŸèƒ½å‹ç½‘ç»œã€‚Keras æä¾›äº†è®¸å¤šå±‚ç”¨äºè®¾è®¡å„ç§ç±»å‹çš„ç½‘ç»œï¼Œä¾‹å¦‚
    CNN å’Œ LSTMã€‚`'
- en: '`In Flax, networks are designed using the setup or compact way. The setup method
    is explicit, while the compact way is in-line. Setup is very similar to how networks
    are designed in PyTorch. For example, here is a network designed with the setup
    way.`'
  id: totrans-1615
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨ Flax ä¸­ï¼Œç½‘ç»œå¯ä»¥ä½¿ç”¨ setup æˆ–ç´§å‡‘æ–¹å¼è®¾è®¡ã€‚setup æ–¹æ³•æ˜¾å¼ï¼Œè€Œç´§å‡‘æ–¹å¼å†…è”ã€‚Setup éå¸¸ç±»ä¼¼äº PyTorch ä¸­è®¾è®¡ç½‘ç»œçš„æ–¹å¼ã€‚ä¾‹å¦‚ï¼Œè¿™é‡Œæ˜¯ä½¿ç”¨
    setup æ–¹å¼è®¾è®¡çš„ç½‘ç»œã€‚
- en: '`class MLP(nn.Module):def setup(self):`'
  id: totrans-1616
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`class MLP(nn.Module):def setup(self):`'
- en: '`Submodule names are derived by the attributes you assign to. In this`'
  id: totrans-1617
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`å­æ¨¡å—åç§°ç”±æ‚¨åˆ†é…çš„å±æ€§æ´¾ç”Ÿè€Œæ¥ã€‚åœ¨è¿™`'
- en: '`case`, `"dense1"` and `"dense2"`. This follows the logic in Py Torch.'
  id: totrans-1618
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`case`, `"dense1"` å’Œ `"dense2"`ã€‚è¿™éµå¾ªäº† PyTorch çš„é€»è¾‘ã€‚'
- en: '`self.dense1 = nn.Dense(32)`'
  id: totrans-1619
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.dense1 = nn.Dense(32)`'
- en: '`self.dense2 = nn.Dense(32)`'
  id: totrans-1620
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.dense2 = nn.Dense(32)`'
- en: '`def __call__(self, x): x = self.dense1(x) x = nn.relu(x)`'
  id: totrans-1621
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def __call__(self, x): x = self.dense1(x) x = nn.relu(x)`'
- en: '`x = self.dense2(x) return x`'
  id: totrans-1622
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = self.dense2(x) return x`'
- en: Here's the same network designed in a compact way. The compact way is more straightforward
    because there is less code duplicity.`class MLP(nn.Module):`
  id: totrans-1623
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯åŒä¸€ä¸ªç½‘ç»œçš„ç´§å‡‘è®¾è®¡æ–¹å¼ã€‚ç´§å‡‘çš„æ–¹å¼æ›´ä¸ºç›´æ¥ï¼Œå› ä¸ºä»£ç é‡å¤è¾ƒå°‘ã€‚`class MLP(nn.Module):`
- en: '`@nn.compact`'
  id: totrans-1624
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`@nn.compact`'
- en: '`def __call__(self, x):`'
  id: totrans-1625
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def __call__(self, x):`'
- en: '`x = nn.Dense(32, name="dense1")(x)`'
  id: totrans-1626
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = nn.Dense(32, name="dense1")(x)`'
- en: '`x = nn.relu(x)`'
  id: totrans-1627
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = nn.relu(x)`'
- en: '`x = nn.Dense(32, name="dense2")(x)`'
  id: totrans-1628
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = nn.Dense(32, name="dense2")(x)`'
- en: '`return x`'
  id: totrans-1629
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return x`'
- en: '**Activations in Flax and TensorFlow**'
  id: totrans-1630
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**Flax å’Œ TensorFlow ä¸­çš„æ¿€æ´»å‡½æ•°**'
- en: The `[tf.keras.activations]` module in TensorFlow provides most of the activations
    needed when designing networks. In Flax, activation functions are available via
    the linen module.
  id: totrans-1631
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[tf.keras.activations]`æ¨¡å—åœ¨TensorFlowä¸­æä¾›è®¾è®¡ç½‘ç»œæ—¶æ‰€éœ€çš„å¤§éƒ¨åˆ†æ¿€æ´»å‡½æ•°ã€‚åœ¨Flaxä¸­ï¼Œæ¿€æ´»å‡½æ•°é€šè¿‡linenæ¨¡å—æä¾›ã€‚'
- en: '**Optimizers in Flax and TensorFlow**'
  id: totrans-1632
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**åœ¨Flaxå’ŒTensorFlowä¸­çš„ä¼˜åŒ–å™¨**'
- en: The `[tf.keras.optimizers]` in TensorFlow has popular optimizer functions. However,
    Flax doesn't ship with any optimizer functions. Optimizers used in Flax are provided
    by another library known as Optax.
  id: totrans-1633
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[tf.keras.optimizers]`ä¸­çš„ä¼˜åŒ–å™¨åœ¨TensorFlowä¸­æœ‰æµè¡Œçš„ä¼˜åŒ–å™¨å‡½æ•°ã€‚ä½†æ˜¯ï¼ŒFlaxä¸æä¾›ä»»ä½•ä¼˜åŒ–å™¨å‡½æ•°ã€‚Flaxä¸­ä½¿ç”¨çš„ä¼˜åŒ–å™¨ç”±å¦ä¸€ä¸ªåä¸ºOptaxçš„åº“æä¾›ã€‚'
- en: '**Metrics in Flax and TensorFlow**'
  id: totrans-1634
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**åœ¨Flaxå’ŒTensorFlowä¸­çš„æŒ‡æ ‡**'
- en: In TensorFlow, metrics are available via
  id: totrans-1635
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨TensorFlowä¸­ï¼ŒæŒ‡æ ‡å¯ä»¥é€šè¿‡
- en: the `[tf.keras.metrics]` module. As of this writing, Flax has no metrics module.
    You'll need to define metric functions for your networks or use other third-party
    libraries.
  id: totrans-1636
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[tf.keras.metrics]`æ¨¡å—ã€‚æˆªè‡³ç›®å‰ï¼ŒFlaxæ²¡æœ‰æŒ‡æ ‡æ¨¡å—ã€‚æ‚¨éœ€è¦ä¸ºæ‚¨çš„ç½‘ç»œå®šä¹‰æŒ‡æ ‡å‡½æ•°æˆ–ä½¿ç”¨å…¶ä»–ç¬¬ä¸‰æ–¹åº“ã€‚'
- en: '`import optax`'
  id: totrans-1637
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import optax`'
- en: '`import jax.numpy as jnp`'
  id: totrans-1638
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import jax.numpy as jnp`'
- en: '`def compute_metrics(logits, labels):`'
  id: totrans-1639
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def compute_metrics(logits, labels):`'
- en: '`loss = jnp.mean(optax.softmax_cross_entropy(logits, jax.nn.on e_hot(labels,
    num_classes=2)))`'
  id: totrans-1640
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`loss = jnp.mean(optax.softmax_cross_entropy(logits, jax.nn.on e_hot(labels,
    num_classes=2)))`'
- en: '`accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)`'
  id: totrans-1641
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)`'
- en: '`metrics = {`'
  id: totrans-1642
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`metrics = {`'
- en: '`''loss'': loss,`'
  id: totrans-1643
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`''loss'': loss,`'
- en: '`''accuracy'': accuracy`'
  id: totrans-1644
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`''accuracy'': accuracy`'
- en: '`}`'
  id: totrans-1645
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`}`'
- en: '`return metrics`'
  id: totrans-1646
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return metrics`'
- en: '**Computing gradients in Flax and TensorFlow**'
  id: totrans-1647
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**åœ¨Flaxå’ŒTensorFlowä¸­è®¡ç®—æ¢¯åº¦**'
- en: The `[jax.grad]` function is used to compute gradients in Flax. It offers the
    ability to return auxillary data. For example, you can return loss and gradients
    at the same time.
  id: totrans-1648
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`[jax.grad]`å‡½æ•°ç”¨äºåœ¨Flaxä¸­è®¡ç®—æ¢¯åº¦ã€‚å®ƒæä¾›åŒæ—¶è¿”å›æŸå¤±å’Œæ¢¯åº¦çš„èƒ½åŠ›ï¼Œä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥åŒæ—¶è¿”å›æŸå¤±å’Œæ¢¯åº¦ã€‚'
- en: '`@jax.jitdef sum_logistic(x):return jnp.sum(1.0 / (1.0 + jnp.exp(-x))),(x +
    1)`'
  id: totrans-1649
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`@jax.jitdef sum_logistic(x):return jnp.sum(1.0 / (1.0 + jnp.exp(-x))),(x +
    1)`'
- en: '`x_small = jnp.arange(6.)`'
  id: totrans-1650
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x_small = jnp.arange(6.)`'
- en: '`derivative_fn = jax.grad(sum_logistic, has_aux=True) print(derivative_fn(x_small))`'
  id: totrans-1651
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`derivative_fn = jax.grad(sum_logistic, has_aux=True) print(derivative_fn(x_small))`'
- en: '`(DeviceArray([0.25 , 0.19661194, 0.10499357, 0.04517666, 0.01766271,`'
  id: totrans-1652
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`(DeviceArray([0.25 , 0.19661194, 0.10499357, 0.04517666, 0.01766271,`'
- en: '`0.00664806], dtype=float32), DeviceArray([1., 2.,`'
  id: totrans-1653
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`0.00664806], dtype=float32), DeviceArray([1., 2.,`'
- en: '`3., 4., 5., 6.], dtype=float32))Advanced automatic differentiation can also
    be done`'
  id: totrans-1654
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`3., 4., 5., 6.], dtype=float32))é«˜çº§è‡ªåŠ¨å¾®åˆ†ä¹Ÿå¯ä»¥å®Œæˆ`'
- en: using `jax.vjp()` and `jax.jvp()`.
  id: totrans-1655
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`jax.vjp()`å’Œ`jax.jvp()`ã€‚
- en: In TensorFlow, gradients are computed using `[tf.GradientTape].def grad(model,
    inputs, targets):with tf.GradientTape() as tape:loss_value = loss(model, inputs,
    targets, training=True) return loss_value, tape.gradient(loss_value, model.trainable_
    variables)`
  id: totrans-1656
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨TensorFlowä¸­ï¼Œä½¿ç”¨`[tf.GradientTape]`è®¡ç®—æ¢¯åº¦ã€‚def grad(model, inputs, targets):with
    tf.GradientTape() as tape:loss_value = loss(model, inputs, targets, training=True)
    return loss_value, tape.gradient(loss_value, model.trainable_ variables)`
- en: Unless you are creating custom training loops in TensorFlow, you will not define
    a gradient function. This is done automatically when you train the network.
  id: totrans-1657
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: é™¤éæ‚¨åœ¨TensorFlowä¸­åˆ›å»ºè‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ï¼Œå¦åˆ™ä¸ä¼šå®šä¹‰æ¢¯åº¦å‡½æ•°ã€‚å½“æ‚¨è®­ç»ƒç½‘ç»œæ—¶ï¼Œè¿™æ˜¯è‡ªåŠ¨å®Œæˆçš„ã€‚
- en: '**Loading datasets in Flax and TensorFlow**'
  id: totrans-1658
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**åœ¨Flaxå’ŒTensorFlowä¸­åŠ è½½æ•°æ®é›†**'
- en: TensorFlow provides utilities for loading data. Flax doesn't ship with any data
    loaders. You have to use the data loaders from other libraries such as TensorFlow.
    As long as the data is in JAX NumPy or regular arrays and has the proper shape,
    it can be passed to Flax networks.
  id: totrans-1659
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: TensorFlowæä¾›äº†åŠ è½½æ•°æ®çš„å®ç”¨ç¨‹åºã€‚Flaxä¸é™„å¸¦ä»»ä½•æ•°æ®åŠ è½½å™¨ã€‚æ‚¨å¿…é¡»ä½¿ç”¨æ¥è‡ªå…¶ä»–åº“ï¼ˆå¦‚TensorFlowï¼‰çš„æ•°æ®åŠ è½½å™¨ã€‚åªè¦æ•°æ®æ˜¯JAX
    NumPyæˆ–å¸¸è§„æ•°ç»„ï¼Œå¹¶ä¸”å…·æœ‰é€‚å½“çš„å½¢çŠ¶ï¼Œå°±å¯ä»¥ä¼ é€’ç»™Flaxç½‘ç»œã€‚
- en: '**Training model in Flax vs. TensorFlow**'
  id: totrans-1660
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**åœ¨Flaxä¸TensorFlowä¸­è®­ç»ƒæ¨¡å‹**'
- en: Training models in TensorFlow is done by compiling the network and calling the
    fit method. However, in Flax, we create a training state to hold the training
    information and then pass data to the network.
  id: totrans-1661
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨TensorFlowä¸­ï¼Œé€šè¿‡ç¼–è¯‘ç½‘ç»œå¹¶è°ƒç”¨fitæ–¹æ³•æ¥è®­ç»ƒæ¨¡å‹ã€‚ç„¶è€Œï¼Œåœ¨Flaxä¸­ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªè®­ç»ƒçŠ¶æ€æ¥ä¿å­˜è®­ç»ƒä¿¡æ¯ï¼Œç„¶åå°†æ•°æ®ä¼ é€’ç»™ç½‘ç»œã€‚
- en: '`from flax.training import train_state`'
  id: totrans-1662
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from flax.training import train_state`'
- en: '`def create_train_state(rng):`'
  id: totrans-1663
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def create_train_state(rng):`'
- en: '"""Creates initial `TrainState`."""'
  id: totrans-1664
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""åˆ›å»ºåˆå§‹`TrainState`ã€‚"""'
- en: '`model = LSTMModel()`'
  id: totrans-1665
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`model = LSTMModel()`'
- en: '`params = model.init(rng, jnp.array(X_train_padded[0]))[''param`'
  id: totrans-1666
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`params = model.init(rng, jnp.array(X_train_padded[0]))[''param`'
- en: '`s'']`'
  id: totrans-1667
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`s'']`'
- en: '`tx = optax.adam(0.001,0.9,0.999,1e-07)`'
  id: totrans-1668
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`tx = optax.adam(0.001,0.9,0.999,1e-07)`'
- en: '`return train_state.TrainState.create(`'
  id: totrans-1669
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return train_state.TrainState.create(`'
- en: '`apply_fn=model.apply, params=params, tx=tx)`'
  id: totrans-1670
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`apply_fn=model.apply, params=params, tx=tx)`'
- en: After that, we define a training step that will compute the loss and gradients.
    It then uses these gradients to update the model parameters and returns the model
    metrics and the new state.
  id: totrans-1671
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä¹‹åï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªè®­ç»ƒæ­¥éª¤ï¼Œè®¡ç®—æŸå¤±å’Œæ¢¯åº¦ã€‚ç„¶åä½¿ç”¨è¿™äº›æ¢¯åº¦æ¥æ›´æ–°æ¨¡å‹å‚æ•°å¹¶è¿”å›æ¨¡å‹æŒ‡æ ‡å’Œæ–°çŠ¶æ€ã€‚
- en: '`@jax.jitdef train_step(state, text, labels):def loss_fn(params):`'
  id: totrans-1672
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`@jax.jitdef train_step(state, text, labels):def loss_fn(params):`'
- en: '`logits = LSTMModel().apply({''params'': params}, text) loss = jnp.mean(optax.softmax_cross_entropy(
    logits=logits,`'
  id: totrans-1673
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`logits = LSTMModel().apply({''params'': params}, text) loss = jnp.mean(optax.softmax_cross_entropy(
    logits=logits,`'
- en: '`labels=jax.nn.one_hot(labels, num_classes=2))) return loss, logits`'
  id: totrans-1674
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`labels=jax.nn.one_hot(labels, num_classes=2))) return loss, logits`'
- en: '`grad_fn = jax.value_and_grad(loss_fn, has_aux=True) (_, logits), grads = grad_fn(state.params)`'
  id: totrans-1675
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`grad_fn = jax.value_and_grad(loss_fn, has_aux=True) (_, logits), grads = grad_fn(state.params)`'
- en: '`state = state.apply_gradients(grads=grads)`'
  id: totrans-1676
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`state = state.apply_gradients(grads=grads)`'
- en: '`metrics = compute_metrics(logits, labels)`'
  id: totrans-1677
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`metrics = compute_metrics(logits, labels)`'
- en: '`return state, metrics`'
  id: totrans-1678
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return state, metrics`'
- en: UseÂ `Elegy`Â to train networks like in Keras. Elegy is a high-level API for JAX
    neural network libraries.
  id: totrans-1679
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`Elegy`æ¥è®­ç»ƒç±»ä¼¼äºKerasçš„ç½‘ç»œã€‚Elegyæ˜¯ä¸€ä¸ªåŸºäºJAXç¥ç»ç½‘ç»œåº“çš„é«˜çº§APIã€‚
- en: '**Distributed training in Flax and TensorFlow**'
  id: totrans-1680
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**Flaxå’ŒTensorFlowä¸­çš„åˆ†å¸ƒå¼è®­ç»ƒ**'
- en: Training networks in TensorFlow in a distributed manner is done by creatingÂ `distributed
    strategy.mirrored_strategy = tf.distribute.MirroredStrategy()`
  id: totrans-1681
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨TensorFlowä¸­ä»¥åˆ†å¸ƒå¼æ–¹å¼è®­ç»ƒç½‘ç»œæ˜¯é€šè¿‡åˆ›å»º`distributed strategy.mirrored_strategy = tf.distribute.MirroredStrategy()`æ¥å®Œæˆçš„ã€‚
- en: '`with mirrored_strategy.scope():`'
  id: totrans-1682
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`with mirrored_strategy.scope():`'
- en: '`model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_s`'
  id: totrans-1683
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_s`'
- en: '`hape=(1,))])`'
  id: totrans-1684
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`hape=(1,))])`'
- en: '`model.compile(loss=''mse'', optimizer=''sgd'')`'
  id: totrans-1685
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`model.compile(loss=''mse'', optimizer=''sgd'')`'
- en: To train networks in a distributed way in Flax, we define distributed versions
    of our Flax functions. This is done using theÂ [`pmap`]Â function that executes
    a function on multiple devices. You'll then compute predictions from all devices
    and get the average
  id: totrans-1686
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¦åœ¨Flaxä¸­ä»¥åˆ†å¸ƒå¼æ–¹å¼è®­ç»ƒç½‘ç»œï¼Œæˆ‘ä»¬å®šä¹‰æˆ‘ä»¬çš„Flaxå‡½æ•°çš„åˆ†å¸ƒå¼ç‰ˆæœ¬ã€‚è¿™é€šè¿‡`pmap`å‡½æ•°å®Œæˆï¼Œè¯¥å‡½æ•°åœ¨å¤šä¸ªè®¾å¤‡ä¸Šæ‰§è¡Œå‡½æ•°ã€‚ç„¶åï¼Œæ‚¨å°†è®¡ç®—æ‰€æœ‰è®¾å¤‡ä¸Šçš„é¢„æµ‹å¹¶è·å¾—å¹³å‡å€¼ã€‚
- en: usingÂ [`jax.lax.pmean()`]. You also need to replicate the data on all the devices
    usingÂ [`jax_utils.replicate`]Â . To obtain metrics from the
  id: totrans-1687
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[`jax.lax.pmean()`]ã€‚ä½ è¿˜éœ€è¦ä½¿ç”¨[`jax_utils.replicate`]åœ¨æ‰€æœ‰è®¾å¤‡ä¸Šå¤åˆ¶æ•°æ®ï¼Œä»¥è·å–æŒ‡æ ‡ã€‚
- en: device useÂ `jax_utils.unreplicate`.
  id: totrans-1688
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è®¾å¤‡ä½¿ç”¨`jax_utils.unreplicate`ã€‚
- en: '**Working with TPU accelerators**'
  id: totrans-1689
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**ä½¿ç”¨TPUåŠ é€Ÿå™¨**'
- en: You can use Flax and TensorFlow with TPU and GPU accelerators. To use Flax with
    TPUs on Colab, you'll need to set it up:`jax.tools.colab_tpu.setup_tpu() jax.devices()![](../images/00048.gif)`For
    TensorFlow, set up theÂ `TPU distributed strategy.cluster_resolver = tf.distribute.cluster_resolver.TPUClusterRes
    olver(tpu=tpu_address)`
  id: totrans-1690
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ä½¿ç”¨Flaxå’ŒTensorFlowä¸TPUå’ŒGPUåŠ é€Ÿå™¨ã€‚è¦åœ¨Colabä¸Šä½¿ç”¨Flaxä¸TPUï¼Œæ‚¨éœ€è¦è®¾ç½®å®ƒï¼š`jax.tools.colab_tpu.setup_tpu()
    jax.devices()![](../images/00048.gif)`å¯¹äºTensorFlowï¼Œè®¾ç½®`TPU distributed strategy.cluster_resolver
    = tf.distribute.cluster_resolver.TPUClusterRes olver(tpu=tpu_address)`
- en: '`tf.config.experimental_connect_to_cluster(cluster_resolver) tf.tpu.experimental.initialize_tpu_system(cluster_resolver)
    tpu_strategy = tf.distribute.TPUStrategy(cluster_resolver)`'
  id: totrans-1691
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`tf.config.experimental_connect_to_cluster(cluster_resolver) tf.tpu.experimental.initialize_tpu_system(cluster_resolver)
    tpu_strategy = tf.distribute.TPUStrategy(cluster_resolver)`'
- en: '**Model evaluation**'
  id: totrans-1692
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**æ¨¡å‹è¯„ä¼°**'
- en: TensorFlow provides theÂ [`evaluate`]Â function for evaluating networks. Flax
    doesn't ship with such a function. You'll need to create a function that applies
    the model and returns the test metrics.Â `Elegy`Â provides Keras-like functions
    such as theÂ `evaluate`Â method.
  id: totrans-1693
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: TensorFlowæä¾›äº†`evaluate`å‡½æ•°ç”¨äºè¯„ä¼°ç½‘ç»œã€‚Flaxæ²¡æœ‰æä¾›è¿™æ ·çš„å‡½æ•°ã€‚æ‚¨éœ€è¦åˆ›å»ºä¸€ä¸ªåº”ç”¨æ¨¡å‹å¹¶è¿”å›æµ‹è¯•æŒ‡æ ‡çš„å‡½æ•°ã€‚`Elegy`æä¾›ç±»ä¼¼Kerasçš„åŠŸèƒ½ï¼Œä¾‹å¦‚`evaluate`æ–¹æ³•ã€‚
- en: '`@jax.jit`'
  id: totrans-1694
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`@jax.jit`'
- en: '`def eval_step(state, text, labels):`'
  id: totrans-1695
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def eval_step(state, text, labels):`'
- en: '`logits = LSTMModel().apply({''params'': state.params}, text)`'
  id: totrans-1696
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`logits = LSTMModel().apply({''params'': state.params}, text)`'
- en: '`return compute_metrics(logits=logits, labels=labels)`'
  id: totrans-1697
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return compute_metrics(logits=logits, labels=labels)`'
- en: '`def evaluate_model(state, text, test_lbls): """Evaluate on the validation
    set."""`'
  id: totrans-1698
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def evaluate_model(state, text, test_lbls): """Evaluate on the validation
    set."""`'
- en: '`metrics = eval_step(state, text, test_lbls) metrics = jax.device_get(metrics)`'
  id: totrans-1699
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`metrics = eval_step(state, text, test_lbls) metrics = jax.device_get(metrics)`'
- en: '`metrics = jax.tree_map(lambda x: x.item(), metrics) return metrics`'
  id: totrans-1700
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`metrics = jax.tree_map(lambda x: x.item(), metrics) return metrics`'
- en: '**Visualize model performance**'
  id: totrans-1701
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**å¯è§†åŒ–æ¨¡å‹æ€§èƒ½**'
- en: Model visualizing is similar in Flax and TensorFlow. Once you obtain the metrics,
    you can use a package such asÂ `Matplotlib`Â to visualize the model's performance.
    You can also useÂ `TensorBoard`Â in both Flax and TensorFlow.
  id: totrans-1702
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ¨¡å‹çš„å¯è§†åŒ–åœ¨ Flax å’Œ TensorFlow ä¸­æ˜¯ç±»ä¼¼çš„ã€‚ä¸€æ—¦è·å¾—æŒ‡æ ‡ï¼Œå¯ä»¥ä½¿ç”¨è¯¸å¦‚Â `Matplotlib`Â ä¹‹ç±»çš„è½¯ä»¶åŒ…æ¥å¯è§†åŒ–æ¨¡å‹çš„æ€§èƒ½ã€‚ä½ ä¹Ÿå¯ä»¥åœ¨
    Flax å’Œ TensorFlow ä¸­ä½¿ç”¨Â `TensorBoard`ã€‚
- en: '**Final thoughts**'
  id: totrans-1703
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**æœ€åçš„æ€è€ƒ**'
- en: You have seen the differences between the Flax and TensorFlow libraries. In
    particular, have seen the difference in model definition and training.
  id: totrans-1704
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä½ å·²ç»çœ‹åˆ°äº† Flax å’Œ TensorFlow åº“ä¹‹é—´çš„å·®å¼‚ã€‚ç‰¹åˆ«æ˜¯ï¼Œåœ¨æ¨¡å‹å®šä¹‰å’Œè®­ç»ƒæ–¹é¢æœ‰æ‰€ä¸åŒã€‚
- en: Train ResNet in Flax from scratch(Distributed ResNet training)
  id: totrans-1705
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨ Flax ä¸­ä»å¤´å¼€å§‹è®­ç»ƒ ResNetï¼ˆåˆ†å¸ƒå¼ ResNet è®­ç»ƒï¼‰
- en: Apart from designing custom CNN architectures, you can use architectures that
    have already been built. ResNet is one such popular architecture. In most cases,
    you'll achieve better performance by using such architectures. In this article,
    you will learn how to perform distributed training of a ResNet model in Flax.
  id: totrans-1706
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: é™¤äº†è®¾è®¡å®šåˆ¶çš„ CNN æ¶æ„å¤–ï¼Œæ‚¨è¿˜å¯ä»¥ä½¿ç”¨å·²ç»æ„å»ºå¥½çš„æ¶æ„ã€‚ResNet å°±æ˜¯è¿™æ ·ä¸€ä¸ªæµè¡Œçš„æ¶æ„ä¹‹ä¸€ã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œä½¿ç”¨è¿™æ ·çš„æ¶æ„ä¼šè·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæ‚¨å°†å­¦ä¹ å¦‚ä½•åœ¨
    Flax ä¸­è¿›è¡Œ ResNet æ¨¡å‹çš„åˆ†å¸ƒå¼è®­ç»ƒã€‚
- en: Install Flax models
  id: totrans-1707
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: å®‰è£… Flax æ¨¡å‹
- en: 'TheÂ [flaxmodels]Â package provides pre-trained models for Jax and Flax, including:'
  id: totrans-1708
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '[flaxmodels]Â åŒ…ä¸º Jax å’Œ Flax æä¾›äº†é¢„è®­ç»ƒæ¨¡å‹ï¼ŒåŒ…æ‹¬ï¼š'
- en: StyleGAN2
  id: totrans-1709
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: StyleGAN2
- en: '`GPT2`'
  id: totrans-1710
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`GPT2`'
- en: '`VGG`'
  id: totrans-1711
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`VGG`'
- en: '`ResNet`'
  id: totrans-1712
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`ResNet`'
- en: '`git clone https://github.com/matthias-wright/flaxmodels.git pip install -r
    flaxmodels/training/resnet/requirements.txt`'
  id: totrans-1713
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`git clone https://github.com/matthias-wright/flaxmodels.git pip install -r
    flaxmodels/training/resnet/requirements.txt`'
- en: In this project, we will train the model from scratchâ€“ meaning that we will
    not use the pre-trained weights. In a separate article, we have covered how to
    perform transfer learning with ResNet.
  id: totrans-1714
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªé¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬å°†ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹â€”â€”ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬ä¸ä¼šä½¿ç”¨é¢„è®­ç»ƒçš„æƒé‡ã€‚åœ¨å¦ä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å·²ç»è®¨è®ºäº†å¦‚ä½•ä½¿ç”¨ ResNet è¿›è¡Œè¿ç§»å­¦ä¹ ã€‚
- en: Perform standard imports
  id: totrans-1715
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: æ‰§è¡Œæ ‡å‡†å¯¼å…¥
- en: WithÂ [flaxmodels]Â installed, let's import the standard libraries used in this
    article.
  id: totrans-1716
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å®‰è£…äº†Â [flaxmodels]Â åï¼Œè®©æˆ‘ä»¬å¯¼å…¥æœ¬æ–‡ä¸­ä½¿ç”¨çš„æ ‡å‡†åº“ã€‚
- en: import`wget`# pip install wget
  id: totrans-1717
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: import`wget`# pip install wget
- en: import`zipfile`
  id: totrans-1718
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: import`zipfile`
- en: '`import torch`'
  id: totrans-1719
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import torch`'
- en: from`torch.utils.data`import`DataLoader import os`
  id: totrans-1720
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: from`torch.utils.data`import`DataLoader import os`
- en: from`PIL`import`Image`
  id: totrans-1721
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: from`PIL`import`Image`
- en: from`torchvision`import`transforms from torch.utils.data import Dataset import
    numpy as np`
  id: totrans-1722
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: from`torchvision`import`transforms from torch.utils.data import Dataset import
    numpy as np`
- en: '`import pandas as pd`'
  id: totrans-1723
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import pandas as pd`'
- en: '`import matplotlib.pyplot as plt`'
  id: totrans-1724
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import matplotlib.pyplot as plt`'
- en: '%matplotlib inline'
  id: totrans-1725
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '%matplotlib inline'
- en: ignore harmless warnings
  id: totrans-1726
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: å¿½ç•¥æ— å®³çš„è­¦å‘Š
- en: '`import warnings`'
  id: totrans-1727
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import warnings`'
- en: '`warnings.filterwarnings("ignore") import jax`'
  id: totrans-1728
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`warnings.filterwarnings("ignore") import jax`'
- en: from`jax`import`numpy as jnp`
  id: totrans-1729
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: from`jax`import`numpy as jnp`
- en: '`import flax`'
  id: totrans-1730
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import flax`'
- en: from`flax`import`linen as nn`
  id: totrans-1731
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: from`flax`import`linen as nn`
- en: from`flax.training`import`train_state import optax`
  id: totrans-1732
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: from`flax.training`import`train_state import optax`
- en: import`time`
  id: totrans-1733
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: import`time`
- en: '`from tqdm.notebook import tqdm`'
  id: totrans-1734
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from tqdm.notebook import tqdm`'
- en: '`import math`'
  id: totrans-1735
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import math`'
- en: from`flax`import`jax_utils`
  id: totrans-1736
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: from`flax`import`jax_utils`
- en: Download dataset
  id: totrans-1737
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: ä¸‹è½½æ•°æ®é›†
- en: We will train the ResNet model to predict two classes from theÂ cats and dogsÂ dataset.
    Download and extract the cat and dog images.
  id: totrans-1738
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†è®­ç»ƒ ResNet æ¨¡å‹æ¥é¢„æµ‹æ¥è‡ªçŒ«å’Œç‹—æ•°æ®é›†çš„ä¸¤ç±»ã€‚ä¸‹è½½å¹¶æå–çŒ«å’Œç‹—çš„å›¾åƒã€‚
- en: '`wget.download("https://ml.machinelearningnuggets.com/train.zi p")`'
  id: totrans-1739
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`wget.download("https://ml.machinelearningnuggets.com/train.zi p")`'
- en: with`zipfile.ZipFile('train.zip', 'r') as zip_ref:`
  id: totrans-1740
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: with`zipfile.ZipFile('train.zip', 'r') as zip_ref:`
- en: '`zip_ref.extractall(''.'')`'
  id: totrans-1741
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`zip_ref.extractall(''.'')`'
- en: Loading dataset in Flax
  id: totrans-1742
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨ Flax ä¸­åŠ è½½æ•°æ®é›†
- en: Since JAX and Flax don't ship with any data loaders, we use data loading utilities
    from PyTorch orÂ TensorFlow. When using PyTorch, we start by creating a dataset
    class.
  id: totrans-1743
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ç”±äº JAX å’Œ Flax ä¸åŒ…å«ä»»ä½•æ•°æ®åŠ è½½å™¨ï¼Œæˆ‘ä»¬ä½¿ç”¨ PyTorch æˆ– TensorFlow ä¸­çš„æ•°æ®åŠ è½½å·¥å…·ã€‚å½“ä½¿ç”¨ PyTorch æ—¶ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ›å»ºä¸€ä¸ªæ•°æ®é›†ç±»ã€‚
- en: '`class CatsDogsDataset(Dataset):`'
  id: totrans-1744
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`class CatsDogsDataset(Dataset):`'
- en: '`def __init__(self, root_dir, annotation_file, transform=Non`'
  id: totrans-1745
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def __init__(self, root_dir, annotation_file, transform=Non`'
- en: 'e):'
  id: totrans-1746
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'e):'
- en: '`self.root_dir = root_dir`'
  id: totrans-1747
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.root_dir = root_dir`'
- en: '`self.annotations = pd.read_csv(annotation_file) self.transform = transform`'
  id: totrans-1748
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.annotations = pd.read_csv(annotation_file) self.transform = transform`'
- en: '`def __len__(self):return len(self.annotations)`'
  id: totrans-1749
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def __len__(self):return len(self.annotations)`'
- en: '`def __getitem__(self, index):`'
  id: totrans-1750
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def __getitem__(self, index):`'
- en: '`img_id = self.annotations.iloc[index, 0]`'
  id: totrans-1751
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`img_id = self.annotations.iloc[index, 0]`'
- en: '`img = Image.open(os.path.join(self.root_dir, img_id)).c`'
  id: totrans-1752
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`img = Image.open(os.path.join(self.root_dir, img_id)).c`'
- en: '`convert("RGB")`'
  id: totrans-1753
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`convert("RGB")`'
- en: '`y_label = torch.tensor(float(self.annotations.iloc[inde`'
  id: totrans-1754
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`y_label = torch.tensor(float(self.annotations.iloc[inde`'
- en: x, 1]))
  id: totrans-1755
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x, 1]))
- en: 'if self.transform is not None: img = self.transform(img)return (img, y_label)Next,
    create aÂ Pandas DataFrameÂ containing the image paths and labels.'
  id: totrans-1756
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å¦‚æœå­˜åœ¨è½¬æ¢ï¼Œåˆ™è¿›è¡Œè½¬æ¢ï¼šimg = self.transform(img)return (img, y_label)æ¥ä¸‹æ¥ï¼Œåˆ›å»ºåŒ…å«å›¾åƒè·¯å¾„å’Œæ ‡ç­¾çš„Pandas
    DataFrameã€‚
- en: '`train_df = pd.DataFrame(columns=["img_path","label"]) train_df["img_path"]
    = os.listdir("train/") for idx, i in enumerate(os.listdir("train/")):`'
  id: totrans-1757
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_df = pd.DataFrame(columns=["img_path","label"]) train_df["img_path"]
    = os.listdir("train/") for idx, i in enumerate(os.listdir("train/")):`'
- en: '`if "cat" in i:`'
  id: totrans-1758
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`if "cat" in i:`'
- en: '`train_df["label"][idx] = 0`'
  id: totrans-1759
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_df["label"][idx] = 0`'
- en: '`if "dog" in i:`'
  id: totrans-1760
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`if "dog" in i:`'
- en: '`train_df["label"][idx] = 1`'
  id: totrans-1761
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_df["label"][idx] = 1`'
- en: '`train_df.to_csv (r''train_csv.csv'', index = False, header=True)`'
  id: totrans-1762
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_df.to_csv (r''train_csv.csv'', index = False, header=True)`'
- en: Data transformation in Flax
  id: totrans-1763
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: Flaxä¸­çš„æ•°æ®è½¬æ¢
- en: Define a function that will stack the data and return it as aÂ NumPy array.
  id: totrans-1764
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å®šä¹‰ä¸€ä¸ªå°†æ•°æ®å †å å¹¶ä½œä¸ºNumPyæ•°ç»„è¿”å›çš„å‡½æ•°ã€‚
- en: '`def custom_collate_fn(batch):`'
  id: totrans-1765
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def custom_collate_fn(batch):`'
- en: '`transposed_data = list(zip(*batch)) labels = np.array(transposed_data[1])
    imgs = np.stack(transposed_data[0]) return imgs, labels`'
  id: totrans-1766
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`transposed_data = list(zip(*batch)) labels = np.array(transposed_data[1])
    imgs = np.stack(transposed_data[0]) return imgs, labels`'
- en: Create a transformation for resizing the images. Next, apply the transformation
    to the dataset created earlier.`size_image = 224`
  id: totrans-1767
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åˆ›å»ºç”¨äºè°ƒæ•´å›¾åƒå¤§å°çš„è½¬æ¢ã€‚æ¥ä¸‹æ¥ï¼Œå°†è¯¥è½¬æ¢åº”ç”¨äºæ—©æœŸåˆ›å»ºçš„æ•°æ®é›†ã€‚`size_image = 224`
- en: '`transform = transforms.Compose([`'
  id: totrans-1768
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`transform = transforms.Compose([`'
- en: '`transforms.Resize((size_image,size_image)), np.array])`'
  id: totrans-1769
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`transforms.Resize((size_image,size_image)), np.array])`'
- en: '`dataset = CatsDogsDataset("train","train_csv.csv",transform=transform)`'
  id: totrans-1770
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`dataset = CatsDogsDataset("train","train_csv.csv",transform=transform)`'
- en: Split this dataset into a training and testing set and create data loaders for
    each set.`batch_size = 32`
  id: totrans-1771
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å°†æ­¤æ•°æ®é›†æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œå¹¶ä¸ºæ¯ä¸ªé›†åˆ›å»ºæ•°æ®åŠ è½½å™¨ã€‚`batch_size = 32`
- en: '`train_set, validation_set = torch.utils.data.random_split(dataset,[20000,5000])`'
  id: totrans-1772
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_set, validation_set = torch.utils.data.random_split(dataset,[20000,5000])`'
- en: '`train_loader = DataLoader(dataset=train_set, collate_fn=custom_collate_fn,shuffle=True,
    batch_size=batch_size)`'
  id: totrans-1773
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_loader = DataLoader(dataset=train_set, collate_fn=custom_collate_fn,shuffle=True,
    batch_size=batch_size)`'
- en: '`validation_loader = DataLoader(dataset=validation_set,collate_fn=custom_collate_fn,
    shuffle=False, batch_size=batch_size)`'
  id: totrans-1774
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`validation_loader = DataLoader(dataset=validation_set,collate_fn=custom_collate_fn,
    shuffle=False, batch_size=batch_size)`'
- en: Instantiate Flax ResNet model
  id: totrans-1775
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: å®ä¾‹åŒ–Flax ResNetæ¨¡å‹
- en: 'With the data in place, instantiate the Flax ResNet model using theÂ `[flaxmodels]`Â package.
    The instantiation requires:'
  id: totrans-1776
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æœ‰äº†æ•°æ®å‡†å¤‡å°±ç»ªï¼Œä½¿ç”¨`[flaxmodels]`åŒ…å®ä¾‹åŒ–Flax ResNetæ¨¡å‹ã€‚å®ä¾‹åŒ–éœ€è¦ï¼š
- en: The desired number of classes.
  id: totrans-1777
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ‰€éœ€çš„ç±»åˆ«æ•°ã€‚
- en: The type of output.
  id: totrans-1778
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¾“å‡ºç±»å‹ã€‚
- en: The data type.
  id: totrans-1779
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ•°æ®ç±»å‹ã€‚
- en: Whether the model is pre-trainedâ€“ in this caseÂ `[False]`.import jax.numpy as
    jnp import flaxmodels as fm
  id: totrans-1780
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ˜¯å¦é¢„è®­ç»ƒæ¨¡å‹ - åœ¨è¿™ç§æƒ…å†µä¸‹`[False]`ã€‚import jax.numpy as jnp import flaxmodels as fm
- en: '`num_classes = 2`'
  id: totrans-1781
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`num_classes = 2`'
- en: '`dtype = jnp.float32 model = fm.ResNet50(output=''log_softmax'', pretrained=None,
    num_classes=num_classes, dtype=dtype)`'
  id: totrans-1782
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`dtype = jnp.float32 model = fm.ResNet50(output=''log_softmax'', pretrained=None,
    num_classes=num_classes, dtype=dtype)`'
- en: Compute metrics
  id: totrans-1783
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: è®¡ç®—æŒ‡æ ‡
- en: Define the metrics for evaluating the model during training. Let's start by
    creating the loss function.
  id: totrans-1784
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å®šä¹‰è¯„ä¼°æ¨¡å‹è®­ç»ƒæœŸé—´çš„æŒ‡æ ‡ã€‚è®©æˆ‘ä»¬é¦–å…ˆåˆ›å»ºæŸå¤±å‡½æ•°ã€‚
- en: '`def cross_entropy_loss(*, logits, labels):`'
  id: totrans-1785
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def cross_entropy_loss(*, logits, labels):`'
- en: '`labels_onehot = jax.nn.one_hot(labels, num_classes=num_classes)`'
  id: totrans-1786
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`labels_onehot = jax.nn.one_hot(labels, num_classes=num_classes)`'
- en: '`s)`'
  id: totrans-1787
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`s)`'
- en: '`return optax.softmax_cross_entropy(logits=logits, labels=labels)`'
  id: totrans-1788
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return optax.softmax_cross_entropy(logits=logits, labels=labels)`'
- en: '`ls_onehot).mean()`'
  id: totrans-1789
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`ls_onehot).mean()`'
- en: Next, define a function that computes and returns the loss and accuracy.
  id: totrans-1790
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œå®šä¹‰ä¸€ä¸ªè®¡ç®—å¹¶è¿”å›æŸå¤±å’Œå‡†ç¡®ç‡çš„å‡½æ•°ã€‚
- en: '`def compute_metrics(*, logits, labels):`'
  id: totrans-1791
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def compute_metrics(*, logits, labels):`'
- en: '`loss = cross_entropy_loss(logits=logits, labels=labels) accuracy = jnp.mean(jnp.argmax(logits,
    -1) == labels) metrics = {`'
  id: totrans-1792
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`loss = cross_entropy_loss(logits=logits, labels=labels) accuracy = jnp.mean(jnp.argmax(logits,
    -1) == labels) metrics = {`'
- en: '`''loss'': loss,`'
  id: totrans-1793
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`''loss'': loss'',`'
- en: '`''accuracy'': accuracy`,'
  id: totrans-1794
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`''accuracy'': accuracy`,'
- en: '`}`'
  id: totrans-1795
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`}`'
- en: '`return metrics`'
  id: totrans-1796
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return metrics`'
- en: Create Flax model training state
  id: totrans-1797
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: åˆ›å»ºFlaxæ¨¡å‹è®­ç»ƒçŠ¶æ€
- en: Flax provides a training state for storing training information. The training
    state can be modified to add new information. In this case, we need to alter the
    training state to add the batch statistics since the ResNet model computesÂ `[batch_stats]`.
  id: totrans-1798
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: Flaxæä¾›äº†ç”¨äºå­˜å‚¨è®­ç»ƒä¿¡æ¯çš„è®­ç»ƒçŠ¶æ€ã€‚å¯ä»¥ä¿®æ”¹è®­ç»ƒçŠ¶æ€ä»¥æ·»åŠ æ–°ä¿¡æ¯ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éœ€è¦ä¿®æ”¹è®­ç»ƒçŠ¶æ€ä»¥æ·»åŠ æ‰¹æ¬¡ç»Ÿè®¡ä¿¡æ¯ï¼Œå› ä¸ºResNetæ¨¡å‹è®¡ç®—`[batch_stats]`ã€‚
- en: '`class TrainState(train_state.TrainState): batch_stats: flax.core.FrozenDict`'
  id: totrans-1799
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`class TrainState(train_state.TrainState): batch_stats: flax.core.FrozenDict`'
- en: We need the model parameters and batch statistics to create the training state
    function. We can access these by initializing the model with theÂ `[train]`Â asÂ `[False]`.
  id: totrans-1800
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬éœ€è¦æ¨¡å‹å‚æ•°å’Œæ‰¹æ¬¡ç»Ÿè®¡æ¥åˆ›å»ºè®­ç»ƒçŠ¶æ€å‡½æ•°ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡å°†æ¨¡å‹åˆå§‹åŒ–ä¸º`[train]`ä¸º`[False]`æ¥è®¿é—®è¿™äº›å†…å®¹ã€‚
- en: '`key = jax.random.PRNGKey(0)`'
  id: totrans-1801
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`key = jax.random.PRNGKey(0)`'
- en: '`variables = model.init(key, jnp.ones([1, size_image, size_image, 3]), train=False)`'
  id: totrans-1802
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`variables = model.init(key, jnp.ones([1, size_image, size_image, 3]), train=False)`'
- en: 'TheÂ `create`Â method ofÂ TrainStateÂ expects the following parameters: TheÂ `[apply_fn]`Â â€“
    model apply function. The model parametersâ€“Â `[variables[''params'']]`.'
  id: totrans-1803
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`TrainState`çš„`create`æ–¹æ³•éœ€è¦ä»¥ä¸‹å‚æ•°ï¼š`[apply_fn]`â€“ æ¨¡å‹åº”ç”¨å‡½æ•°ã€‚æ¨¡å‹å‚æ•°â€“ `variables[''params'']`ã€‚'
- en: TheÂ optimizer, usually defined usingÂ Optax.
  id: totrans-1804
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–å™¨é€šå¸¸ä½¿ç”¨Optaxå®šä¹‰ã€‚
- en: The batch statisticsâ€“Â `variables['batch_stats']`.
  id: totrans-1805
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ‰¹æ¬¡ç»Ÿè®¡â€“ `variables['batch_stats']`ã€‚
- en: We apply `[pmap]` to this function to create a distributed version of the training
    state.  `[pmap]` compiles the function for execution on multiple devices such
    as multiple GPUs and TPUs.
  id: totrans-1806
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯¹è¿™ä¸ªå‡½æ•°åº”ç”¨`[pmap]`æ¥åˆ›å»ºä¸€ä¸ªåˆ†å¸ƒå¼ç‰ˆæœ¬çš„è®­ç»ƒçŠ¶æ€ã€‚ `[pmap]` ç¼–è¯‘è¯¥å‡½æ•°ä»¥åœ¨å¤šä¸ªè®¾å¤‡ä¸Šæ‰§è¡Œï¼Œä¾‹å¦‚å¤šä¸ªGPUå’ŒTPUã€‚
- en: '`import functools`'
  id: totrans-1807
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import functools`'
- en: '`@functools.partial(jax.pmap)`'
  id: totrans-1808
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`@functools.partial(jax.pmap)`'
- en: '`def create_train_state(rng):`'
  id: totrans-1809
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def create_train_state(rng):`'
- en: '"""Creates initial `TrainState`."""'
  id: totrans-1810
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""åˆ›å»ºåˆå§‹çš„`TrainState`ã€‚"""'
- en: '`return TrainState.create(apply_fn = model.apply,params = variables[''params''],tx
    = optax.adam(0.01,0.9),batch_stats = variables[''batch_stats''])`'
  id: totrans-1811
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return TrainState.create(apply_fn = model.apply,params = variables[''params''],tx
    = optax.adam(0.01,0.9),batch_stats = variables[''batch_stats''])`'
- en: '**Apply model function**'
  id: totrans-1812
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**åº”ç”¨æ¨¡å‹å‡½æ•°**'
- en: 'Next, define a parallel model training function. Pass an `[axis_name]` so you
    can use that to aggregate the metrics from all the devices. The function:'
  id: totrans-1813
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œå®šä¹‰ä¸€ä¸ªå¹¶è¡Œæ¨¡å‹è®­ç»ƒå‡½æ•°ã€‚ä¼ é€’ä¸€ä¸ª`[axis_name]`ä»¥ä¾¿æ‚¨å¯ä»¥ä½¿ç”¨å®ƒæ¥èšåˆæ¥è‡ªæ‰€æœ‰è®¾å¤‡çš„æŒ‡æ ‡ã€‚è¯¥å‡½æ•°ï¼š
- en: Computes the loss.
  id: totrans-1814
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è®¡ç®—æŸå¤±ã€‚
- en: Computes predictions from all devices by calculating the average of the probabilities
    using `[jax.lax.pmean()]` .
  id: totrans-1815
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: é€šè¿‡è®¡ç®—ä½¿ç”¨`[jax.lax.pmean()]` çš„æ¦‚ç‡çš„å¹³å‡å€¼æ¥ä»æ‰€æœ‰è®¾å¤‡ä¸Šè®¡ç®—é¢„æµ‹ã€‚
- en: When applying the model, we also include the batch statistics and the random
    number for  `[DropOut]`. Since this is the training function, the `[train]` parameter
    is `[True]`. The `[batch_stats]` are also included when computing the gradients.
    The `[update_model]` function applies the computed gradientsâ€“ updates the model
    parameters.
  id: totrans-1816
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨åº”ç”¨æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬è¿˜åŒ…æ‹¬äº†æ‰¹æ¬¡ç»Ÿè®¡å’Œ`[DropOut]`çš„éšæœºæ•°ã€‚ç”±äºè¿™æ˜¯è®­ç»ƒå‡½æ•°ï¼Œ`[train]`å‚æ•°æ˜¯`[True]`ã€‚åœ¨è®¡ç®—æ¢¯åº¦æ—¶ï¼Œä¹ŸåŒ…æ‹¬äº†`[batch_stats]`ã€‚`[update_model]`å‡½æ•°åº”ç”¨è®¡ç®—å¾—åˆ°çš„æ¢¯åº¦â€“
    æ›´æ–°æ¨¡å‹å‚æ•°ã€‚
- en: '`@functools.partial(jax.pmap, axis_name=''ensemble'')` `def apply_model(state,
    images, labels):` `def loss_fn(params,batch_stats):`'
  id: totrans-1817
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`@functools.partial(jax.pmap, axis_name=''ensemble'')` `def apply_model(state,
    images, labels):` `def loss_fn(params,batch_stats):`'
- en: '`logits, batch_stats = model.apply({''params'': params, ''batch_stats'': batch_stats},
    images, train=True, rngs={''dropout'': jax.random.PRNGKey(0)}, mutable=[''batch_stats''])'
  id: totrans-1818
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`logits, batch_stats = model.apply({''params'': params, ''batch_stats'': batch_stats},
    images, train=True, rngs={''dropout'': jax.random.PRNGKey(0)}, mutable=[''batch_stats''])`'
- en: '`one_hot = jax.nn.one_hot(labels, num_classes)` `loss = optax.softmax_cross_entropy(logits=logits,
    labels=one_hot).mean()` return loss, `(logits, batch_stats)` `(loss, (logits,
    batch_stats)), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params,state.batch_stats)`
    `probs = jax.lax.pmean(jax.nn.softmax(logits), axis_name=''ensemble'')` `accuracy
    = jnp.mean(jnp.argmax(probs, -1) == labels)` return grads, `loss, accuracy@jax.pmap`
    `def update_model(state, grads):` return state.apply_gradients(grads=grads)'
  id: totrans-1819
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`one_hot = jax.nn.one_hot(labels, num_classes)` `loss = optax.softmax_cross_entropy(logits=logits,
    labels=one_hot).mean()` è¿”å›æŸå¤±ï¼Œ`(logits, batch_stats)` `(loss, (logits, batch_stats)),
    grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params,state.batch_stats)`
    `probs = jax.lax.pmean(jax.nn.softmax(logits), axis_name=''ensemble'')` `accuracy
    = jnp.mean(jnp.argmax(probs, -1) == labels)` è¿”å›æ¢¯åº¦ï¼Œ`æŸå¤±ï¼Œå‡†ç¡®ç‡@jax.pmap` `def update_model(state,
    grads):` åº”ç”¨æ¢¯åº¦ `return state.apply_gradients(grads=grads)`'
- en: '**TensorBoard in Flax**'
  id: totrans-1820
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**åœ¨Flaxä¸­ä½¿ç”¨TensorBoard**'
- en: The next step is to train the ResNet model. However, you might be interested
    in tracking the training using `[TensorBoard]`. In that case, you have to configure
    TensorBoard. You can write the metrics to TensorBoard using the PyTorch `SummaryWriter`.
  id: totrans-1821
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€æ­¥æ˜¯è®­ç»ƒResNetæ¨¡å‹ã€‚ä½†æ˜¯ï¼Œæ‚¨å¯èƒ½æœ‰å…´è¶£ä½¿ç”¨`[TensorBoard]`è·Ÿè¸ªè®­ç»ƒã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨éœ€è¦é…ç½®TensorBoardã€‚æ‚¨å¯ä»¥ä½¿ç”¨PyTorchçš„`SummaryWriter`å°†æŒ‡æ ‡å†™å…¥TensorBoardã€‚
- en: '`rm -rf ./flax_logs/`'
  id: totrans-1822
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`rm -rf ./flax_logs/`'
- en: '`from torch.utils.tensorboard import SummaryWriter` `import torchvision.transforms.functional
    as F` `logdir = "flax_logs"`'
  id: totrans-1823
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from torch.utils.tensorboard import SummaryWriter` `import torchvision.transforms.functional
    as F` `logdir = "flax_logs"`'
- en: '`writer = SummaryWriter(logdir)`'
  id: totrans-1824
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`writer = SummaryWriter(logdir)`'
- en: '**Train Flax ResNet model**'
  id: totrans-1825
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**è®­ç»ƒFlax ResNetæ¨¡å‹**'
- en: Let's train the ResNet model on the entire training set and evaluate it on a
    subset of the test set. You can also evaluate it on the whole test set. Replicate
    the test set to the available devices.
  id: totrans-1826
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åœ¨æ•´ä¸ªè®­ç»ƒé›†ä¸Šè®­ç»ƒ ResNet æ¨¡å‹ï¼Œå¹¶åœ¨æµ‹è¯•é›†çš„å­é›†ä¸Šè¿›è¡Œè¯„ä¼°ã€‚æ‚¨ä¹Ÿå¯ä»¥åœ¨æ•´ä¸ªæµ‹è¯•é›†ä¸Šè¿›è¡Œè¯„ä¼°ã€‚å°†æµ‹è¯•é›†å¤åˆ¶åˆ°å¯ç”¨è®¾å¤‡ä¸Šã€‚
- en: '`(test_images, test_labels) = next(iter(validation_loader))` `test_images =
    test_images / 255.0`'
  id: totrans-1827
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`(test_images, test_labels) = next(iter(validation_loader))` `test_images =
    test_images / 255.0`'
- en: '`test_images = np.array(jax_utils.replicate(test_images))` `test_labels = np.array(jax_utils.replicate(test_labels))`'
  id: totrans-1828
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`test_images = np.array(jax_utils.replicate(test_images))` `test_labels = np.array(jax_utils.replicate(test_labels))`'
- en: Create some lists to hold the training and evaluation metrics.
  id: totrans-1829
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸€äº›åˆ—è¡¨ä»¥ä¿å­˜è®­ç»ƒå’Œè¯„ä¼°æŒ‡æ ‡ã€‚
- en: '`epoch_loss = []`'
  id: totrans-1830
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`epoch_loss = []`'
- en: '`epoch_accuracy = []` `testing_accuracy = []` `testing_loss = []`'
  id: totrans-1831
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`epoch_accuracy = []` `testing_accuracy = []` `testing_loss = []`'
- en: 'Next, define the ResNet model training function. The function does the following:'
  id: totrans-1832
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œå®šä¹‰ ResNet æ¨¡å‹è®­ç»ƒå‡½æ•°ã€‚è¯¥å‡½æ•°æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
- en: Loops through the training dataset and scales it.
  id: totrans-1833
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å¾ªç¯éå†è®­ç»ƒæ•°æ®é›†å¹¶å¯¹å…¶è¿›è¡Œç¼©æ”¾ã€‚
- en: Replicates the data on the available devices.
  id: totrans-1834
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨å¯ç”¨è®¾å¤‡ä¸Šå¤åˆ¶æ•°æ®ã€‚
- en: Applies the model on the dataset and computes the metrics. Obtains the metrics
    from the devices
  id: totrans-1835
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨æ•°æ®é›†ä¸Šåº”ç”¨æ¨¡å‹å¹¶è®¡ç®—æŒ‡æ ‡ã€‚ä»è®¾å¤‡è·å–æŒ‡æ ‡
- en: '`usingÂ jax_utils.unreplicate.`'
  id: totrans-1836
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`using jax_utils.unreplicate.`'
- en: Appends the metrics to a list.
  id: totrans-1837
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å°†æŒ‡æ ‡é™„åŠ åˆ°åˆ—è¡¨ã€‚
- en: Computes the mean of the loss and accuracy to obtain the metrics for each epoch.
  id: totrans-1838
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è®¡ç®—æŸå¤±å’Œå‡†ç¡®åº¦çš„å‡å€¼ä»¥è·å–æ¯ä¸ª epoch çš„æŒ‡æ ‡ã€‚
- en: Applies the model to the test set and obtains the metrics.
  id: totrans-1839
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å°†æ¨¡å‹åº”ç”¨äºæµ‹è¯•é›†å¹¶è·å–æŒ‡æ ‡ã€‚
- en: Append the test metrics to a list.
  id: totrans-1840
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å°†æµ‹è¯•æŒ‡æ ‡é™„åŠ åˆ°åˆ—è¡¨ã€‚
- en: Writes the training and evaluation metrics to TensorBaord.
  id: totrans-1841
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å°†è®­ç»ƒå’Œè¯„ä¼°æŒ‡æ ‡å†™å…¥ TensorBaordã€‚
- en: 'Prints the training and evaluation metrics.def `train_one_epoch(state, dataloader,num_epochs):
    """Train for 1 epoch on the training set."""for epoch in range(num_epochs):for
    cnt, (images, labels) in tqdm(enumerate(dataloade r), total=(math.ceil(len(train_set)/batch_size))):`'
  id: totrans-1842
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ‰“å°è®­ç»ƒå’Œè¯„ä¼°æŒ‡æ ‡ã€‚
- en: '`images = images / 255.0`'
  id: totrans-1843
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`images = images / 255.0`'
- en: '`images = jax_utils.replicate(images)`'
  id: totrans-1844
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`images = jax_utils.replicate(images)`'
- en: '`labels = jax_utils.replicate(labels)`'
  id: totrans-1845
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`labels = jax_utils.replicate(labels)`'
- en: '`grads, loss, accuracy = apply_model(state, images,`'
  id: totrans-1846
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`grads, loss, accuracy = apply_model(state, images,`'
- en: '`labels)state = update_model(state, grads)`'
  id: totrans-1847
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`labels)state = update_model(state, grads)`'
- en: '`epoch_loss.append(jax_utils.unreplicate(loss)) epoch_accuracy.append(jax_utils.unreplicate(accuracy))
    train_loss = np.mean(epoch_loss)`'
  id: totrans-1848
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`epoch_loss.append(jax_utils.unreplicate(loss)) epoch_accuracy.append(jax_utils.unreplicate(accuracy))
    train_loss = np.mean(epoch_loss)`'
- en: '`train_accuracy = np.mean(epoch_accuracy)`'
  id: totrans-1849
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_accuracy = np.mean(epoch_accuracy)`'
- en: '`_`, `test_loss`, `test_accuracy` = `jax_utils.unreplicate(app ly_model(state,
    test_images, test_labels))`'
  id: totrans-1850
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`_`, `test_loss`, `test_accuracy` = `jax_utils.unreplicate(app ly_model(state,
    test_images, test_labels))`'
- en: '`testing_accuracy.append(test_accuracy)`'
  id: totrans-1851
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_accuracy.append(test_accuracy)`'
- en: '`testing_loss.append(test_loss)`'
  id: totrans-1852
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_loss.append(test_loss)`'
- en: '`writer.add_scalar(''Loss/train'', np.array(train_loss), e poch)`'
  id: totrans-1853
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`writer.add_scalar(''Loss/train'', np.array(train_loss), e poch)`'
- en: '`writer.add_scalar(''Loss/test'', np.array(test_loss), epo ch)`'
  id: totrans-1854
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`writer.add_scalar(''Loss/test'', np.array(test_loss), epo ch)`'
- en: '`writer.add_scalar(''Accuracy/train'', np.array(train_accu racy), epoch)`'
  id: totrans-1855
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`writer.add_scalar(''Accuracy/train'', np.array(train_accu racy), epoch)`'
- en: '`writer.add_scalar(''Accuracy/test'', np.array(test_accura cy), epoch)`'
  id: totrans-1856
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`writer.add_scalar(''Accuracy/test'', np.array(test_accura cy), epoch)`'
- en: '`print(f"Epoch: {epoch + 1}, train loss: {train_loss:.4 f}, train accuracy:
    {train_accuracy * 100:.4f}, test loss: {tes t_loss:.4f}, test accuracy: {test_accuracy*
    100:.4f}", flush=Tr ue)`'
  id: totrans-1857
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`print(f"Epoch: {epoch + 1}, train loss: {train_loss:.4 f}, train accuracy:
    {train_accuracy * 100:.4f}, test loss: {tes t_loss:.4f}, test accuracy: {test_accuracy*
    100:.4f}", flush=Tr ue)`'
- en: return `state, epoch_loss, epoch_accuracy, testing_accuracy, testing_lossCreate
    a training state by generating random numbers equivalent to the number of devices.`
  id: totrans-1858
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¿”å› `state, epoch_loss, epoch_accuracy, testing_accuracy, testing_loss` é€šè¿‡ç”Ÿæˆä¸è®¾å¤‡æ•°é‡ç›¸åŒçš„éšæœºæ•°æ¥åˆ›å»ºè®­ç»ƒçŠ¶æ€ã€‚
- en: '`seed = 0`'
  id: totrans-1859
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`seed = 0`'
- en: '`rng = jax.random.PRNGKey(seed)`'
  id: totrans-1860
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`rng = jax.random.PRNGKey(seed)`'
- en: '`rng, init_rng = jax.random.split(rng)`'
  id: totrans-1861
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`rng, init_rng = jax.random.split(rng)`'
- en: '`state = create_train_state(jax.random.split(init_rng, jax.devic e_count()))`'
  id: totrans-1862
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`state = create_train_state(jax.random.split(init_rng, jax.devic e_count()))`'
- en: '`del init_rng # Must not be used anymore.`'
  id: totrans-1863
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`del init_rng # Must not be used anymore.`'
- en: Train the ResNet model by passing the training data and the number of epochs.
  id: totrans-1864
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: é€šè¿‡ä¼ é€’è®­ç»ƒæ•°æ®å’Œ epochs æ•°é‡æ¥è®­ç»ƒ ResNet æ¨¡å‹ã€‚
- en: '`start = time.time()`'
  id: totrans-1865
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`start = time.time()`'
- en: '`num_epochs = 30`'
  id: totrans-1866
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`num_epochs = 30`'
- en: '`state, epoch_loss, epoch_accuracy, testing_accuracy, testing_lo ss = train_one_epoch(state,
    train_loader,num_epochs) print("Total time: ", time.time() - start, "seconds")`'
  id: totrans-1867
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`state, epoch_loss, epoch_accuracy, testing_accuracy, testing_lo ss = train_one_epoch(state,
    train_loader,num_epochs) print("Total time: ", time.time() - start, "seconds")`'
- en: '`![](../images/00049.jpeg)`'
  id: totrans-1868
  prefs: []
  stylish: true
  type: TYPE_IMG
  zh: '`![](../images/00049.jpeg)`'
- en: '**Evaluate model with TensorBoard**'
  id: totrans-1869
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**ä½¿ç”¨ TensorBoard è¯„ä¼°æ¨¡å‹**'
- en: Run TensorBoard to see the logged scalars on TensorBoard.%load_ext tensorboard%tensorboard
    --logdir={logdir}![](../images/00050.jpeg)![](../images/00051.jpeg)
  id: totrans-1870
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¿è¡Œ TensorBoard æŸ¥çœ‹ TensorBoard ä¸Šè®°å½•çš„æ ‡é‡ã€‚%load_ext tensorboard%tensorboard --logdir={logdir}![](../images/00050.jpeg)![](../images/00051.jpeg)
- en: '**Visualize Flax model performance**'
  id: totrans-1871
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**å¯è§†åŒ– Flax æ¨¡å‹æ€§èƒ½**'
- en: The metrics that were stored in a list can be plotted usingÂ Matplotlib.
  id: totrans-1872
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å­˜å‚¨åœ¨åˆ—è¡¨ä¸­çš„æŒ‡æ ‡å¯ä»¥ä½¿ç”¨ Matplotlib ç»˜åˆ¶ã€‚
- en: '`plt.plot(epoch_accuracy, label="Training")`'
  id: totrans-1873
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.plot(epoch_accuracy, label="Training")`'
- en: '`plt.plot(testing_accuracy, label="Test")`'
  id: totrans-1874
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.plot(testing_accuracy, label="Test")`'
- en: '`plt.xlabel("Epoch")`'
  id: totrans-1875
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.xlabel("Epoch")`'
- en: '`plt.ylabel("Accuracy")`'
  id: totrans-1876
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.ylabel("Accuracy")`'
- en: '`plt.legend()`'
  id: totrans-1877
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.legend()`'
- en: '`plt.show()`'
  id: totrans-1878
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.show()`'
- en: '`plt.plot(epoch_loss, label="Training")`'
  id: totrans-1879
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.plot(epoch_loss, label="Training")`'
- en: '`plt.plot(testing_loss, label="Test")`'
  id: totrans-1880
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.plot(testing_loss, label="Test")`'
- en: '`plt.xlabel("Epoch")`'
  id: totrans-1881
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.xlabel("Epoch")`'
- en: '`plt.ylabel("Accuracy")`'
  id: totrans-1882
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.ylabel("Accuracy")`'
- en: '`plt.legend()`'
  id: totrans-1883
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.legend()`'
- en: '`plt.show()`'
  id: totrans-1884
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`plt.show()`'
- en: '`![](../images/00052.jpeg)![](../images/00053.jpeg)`'
  id: totrans-1885
  prefs: []
  stylish: true
  type: TYPE_IMG
  zh: '`![](../images/00052.jpeg)![](../images/00053.jpeg)`'
- en: Save Flax ResNet model
  id: totrans-1886
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: ä¿å­˜ Flax ResNet æ¨¡å‹
- en: '`To save the trained Flax ResNet model use`'
  id: totrans-1887
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`ä¿å­˜è®­ç»ƒå¥½çš„ Flax ResNet æ¨¡å‹ä½¿ç”¨`'
- en: 'theÂ save_checkpointÂ function. The function expects:'
  id: totrans-1888
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: save_checkpointÂ å‡½æ•°ã€‚è¯¥å‡½æ•°æœŸæœ›ï¼š
- en: The folder where the ResNet model will be saved.
  id: totrans-1889
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å°†ä¿å­˜ ResNet æ¨¡å‹çš„æ–‡ä»¶å¤¹ã€‚
- en: The model to be savedâ€“Â [target]. The step â€“ training step number.
  id: totrans-1890
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¦ä¿å­˜çš„æ¨¡å‹â€“ [ç›®æ ‡]ã€‚è®­ç»ƒæ­¥éª¤â€“ è®­ç»ƒæ­¥éª¤ç¼–å·ã€‚
- en: The model prefix. Whether to overwrite existing models.
  id: totrans-1891
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ¨¡å‹å‰ç¼€ã€‚æ˜¯å¦è¦†ç›–ç°æœ‰æ¨¡å‹ã€‚
- en: '`!pip install tensorstore`'
  id: totrans-1892
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`!pip install tensorstore`'
- en: '`from flax.training import checkpoints`'
  id: totrans-1893
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from flax.training import checkpoints`'
- en: '`ckpt_dir = ''model_checkpoint/''`'
  id: totrans-1894
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`ckpt_dir = ''model_checkpoint/''`'
- en: '`checkpoints.save_checkpoint(ckpt_dir=ckpt_dir,`'
  id: totrans-1895
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`checkpoints.save_checkpoint(ckpt_dir=ckpt_dir,`'
- en: '`target=state, step=100,`'
  id: totrans-1896
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`target=state, step=100,`'
- en: '`prefix=''flax_model'', overwrite=True`'
  id: totrans-1897
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`prefix=''flax_model'', overwrite=True`'
- en: '`)![](../images/00054.jpeg)`'
  id: totrans-1898
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`)![](../images/00054.jpeg)`'
- en: Load Flax RestNet model
  id: totrans-1899
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: åŠ è½½ Flax RestNet æ¨¡å‹
- en: 'The saved ResNet Flax model can also be loaded to make predictions. Flax models
    are loaded using theÂ [restore_checkpoint]Â function. The function expects:'
  id: totrans-1900
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä¿å­˜çš„ ResNet Flax æ¨¡å‹ä¹Ÿå¯ä»¥åŠ è½½ä»¥è¿›è¡Œé¢„æµ‹ã€‚Flax æ¨¡å‹ä½¿ç”¨Â [restore_checkpoint]Â å‡½æ•°åŠ è½½ã€‚è¯¥å‡½æ•°æœŸæœ›ï¼š
- en: The target state.
  id: totrans-1901
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ç›®æ ‡çŠ¶æ€ã€‚
- en: The folder containing the saved model.
  id: totrans-1902
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åŒ…å«ä¿å­˜æ¨¡å‹çš„æ–‡ä»¶å¤¹ã€‚
- en: The model's prefix.
  id: totrans-1903
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ¨¡å‹çš„å‰ç¼€ã€‚
- en: '`loaded_model = checkpoints.restore_checkpoint(`'
  id: totrans-1904
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`loaded_model = checkpoints.restore_checkpoint(`'
- en: '`ckpt_dir=ckpt_dir, target=state, prefix=''flax_mode`'
  id: totrans-1905
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`ckpt_dir=ckpt_dir, target=state, prefix=''flax_mode`'
- en: '`l'') ![](../images/00055.gif)`'
  id: totrans-1906
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`l'') ![](../images/00055.gif)`'
- en: Final thoughts
  id: totrans-1907
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: æœ€åçš„æƒ³æ³•
- en: 'In this article, you have learned how to train a ResNet model from scratch
    in Flax. In particular, you have covered:'
  id: totrans-1908
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæ‚¨å·²ç»å­¦ä¹ äº†å¦‚ä½•åœ¨ Flax ä¸­ä»å¤´å¼€å§‹è®­ç»ƒ ResNet æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæ‚¨å·²ç»æ¶µç›–äº†ï¼š
- en: Creating a ResNet model in Flax.
  id: totrans-1909
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨ Flax ä¸­åˆ›å»º ResNet æ¨¡å‹ã€‚
- en: Defining the training state for the ResNet Flax model.
  id: totrans-1910
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä¸º ResNet Flax æ¨¡å‹å®šä¹‰è®­ç»ƒçŠ¶æ€ã€‚
- en: Training the Flax ResNet model in a distributed manner. Track the performance
    of the Flax ResNet model with TensorBoard.
  id: totrans-1911
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨åˆ†å¸ƒå¼æ–¹å¼ä¸‹è®­ç»ƒ Flax ResNet æ¨¡å‹ã€‚ä½¿ç”¨ TensorBoard è·Ÿè¸ª Flax ResNet æ¨¡å‹çš„æ€§èƒ½ã€‚
- en: Saving and loading the Flax ResNet model.
  id: totrans-1912
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä¿å­˜å’ŒåŠ è½½ Flax ResNet æ¨¡å‹ã€‚
- en: Transfer learning with JAX & Flax
  id: totrans-1913
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ JAX å’Œ Flax è¿›è¡Œè¿ç§»å­¦ä¹ 
- en: Training large neural networks can take days or weeks. Once these networks are
    trained, you can take advantage of their weights and apply them to new tasksâ€“Â transfer
    learning. As a result, youÂ finetuneÂ a new network and get good results in a short
    period. Let's look at how you can fine-tune a pre-trained ResNet network in JAX
    and Flax.
  id: totrans-1914
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è®­ç»ƒå¤§å‹ç¥ç»ç½‘ç»œå¯èƒ½éœ€è¦å‡ å¤©æˆ–å‡ å‘¨ã€‚ä¸€æ—¦è¿™äº›ç½‘ç»œè¢«è®­ç»ƒå¥½ï¼Œæ‚¨å¯ä»¥åˆ©ç”¨å®ƒä»¬çš„æƒé‡å¹¶å°†å®ƒä»¬åº”ç”¨äºæ–°ä»»åŠ¡â€“ è¿ç§»å­¦ä¹ ã€‚å› æ­¤ï¼Œæ‚¨å¯ä»¥å¾®è°ƒä¸€ä¸ªé¢„è®­ç»ƒçš„ ResNet
    ç½‘ç»œï¼Œå¹¶åœ¨çŸ­æ—¶é—´å†…è·å¾—è‰¯å¥½çš„ç»“æœã€‚è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åœ¨ JAX å’Œ Flax ä¸­å¯¹é¢„è®­ç»ƒçš„ ResNet ç½‘ç»œè¿›è¡Œå¾®è°ƒã€‚
- en: Install JAX ResNet
  id: totrans-1915
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: å®‰è£… JAX ResNet
- en: '`We''ll use ResNet checkpoints provided by theÂ jax-resnetÂ library.pip install
    jax-resnetLet''s import it together with other packages used in this article.`'
  id: totrans-1916
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`æˆ‘ä»¬å°†ä½¿ç”¨Â jax-resnetÂ åº“æä¾›çš„ ResNet æ£€æŸ¥ç‚¹ã€‚pip install jax-resnetè®©æˆ‘ä»¬ä¸€èµ·å¯¼å…¥å®ƒå’Œæœ¬æ–‡ä¸­ä½¿ç”¨çš„å…¶ä»–è½¯ä»¶åŒ…ã€‚`'
- en: '`pip install flax`'
  id: totrans-1917
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`pip install flax`'
- en: '`import numpy as np`'
  id: totrans-1918
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import numpy as np`'
- en: '`import pandas as pd`'
  id: totrans-1919
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import pandas as pd`'
- en: '`from PIL import Image`'
  id: totrans-1920
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from PIL import Image`'
- en: '`import jax`'
  id: totrans-1921
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import jax`'
- en: '`import optax`'
  id: totrans-1922
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import optax`'
- en: '`import flax`'
  id: totrans-1923
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import flax`'
- en: '`import jax.numpy as jnp`'
  id: totrans-1924
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import jax.numpy as jnp`'
- en: '`from jax_resnet import pretrained_resnet, slice_variables, Sequ ential`'
  id: totrans-1925
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from jax_resnet import pretrained_resnet, slice_variables, Sequ ential`'
- en: '`from flax.training import train_state from flax import linen as nn`'
  id: totrans-1926
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from flax.training import train_state from flax import linen as nn`'
- en: '`from flax.core import FrozenDict,frozen_dict from functools import partial`'
  id: totrans-1927
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from flax.core import FrozenDict,frozen_dict from functools import partial`'
- en: '`import os`'
  id: totrans-1928
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import os`'
- en: '`import torch`'
  id: totrans-1929
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import torch`'
- en: '`from torch.utils.data import DataLoader from torchvision import transforms`'
  id: totrans-1930
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from torch.utils.data import DataLoader from torchvision import transforms`'
- en: '`from torch.utils.data import Dataset import matplotlib.pyplot as plt`'
  id: totrans-1931
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from torch.utils.data import Dataset import matplotlib.pyplot as plt`'
- en: '`%matplotlib inline`'
  id: totrans-1932
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`%matplotlib inline`'
- en: ignore harmless warnings
  id: totrans-1933
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: å¿½ç•¥æ— å®³çš„è­¦å‘Š
- en: '`import warnings`'
  id: totrans-1934
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import warnings`'
- en: '`warnings.filterwarnings("ignore")`'
  id: totrans-1935
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`warnings.filterwarnings("ignore")`'
- en: Download dataset
  id: totrans-1936
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: ä¸‹è½½æ•°æ®é›†
- en: We will fine-tune the ResNet model to predict two classes from theÂ cats and
    dogsÂ dataset. Download and extract the cat and dog images.
  id: totrans-1937
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å¾®è°ƒ ResNet æ¨¡å‹ä»¥é¢„æµ‹æ¥è‡ªçŒ«ç‹—æ•°æ®é›†çš„ä¸¤ä¸ªç±»ã€‚ä¸‹è½½å¹¶æå–çŒ«å’Œç‹—çš„å›¾åƒã€‚
- en: '`pip install wget`'
  id: totrans-1938
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`pip install wget`'
- en: '`import wget`'
  id: totrans-1939
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import wget`'
- en: '`wget.download("https://ml.machinelearningnuggets.com/train.zi p")`'
  id: totrans-1940
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`wget.download("https://ml.machinelearningnuggets.com/train.zi p")`'
- en: '`import zipfile`'
  id: totrans-1941
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import zipfile`'
- en: '`with zipfile.ZipFile(''train.zip'', ''r'') as zip_ref:`'
  id: totrans-1942
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`with zipfile.ZipFile(''train.zip'', ''r'') as zip_ref:`'
- en: '`zip_ref.extractall(''.'')`'
  id: totrans-1943
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`zip_ref.extractall(''.'')`'
- en: Data loading in JAX
  id: totrans-1944
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: åœ¨ JAX ä¸­åŠ è½½æ•°æ®
- en: JAX doesn't ship with data loading utilities. We use existing data loaders inÂ TensorFlowÂ and
    PyTorch to load the data. Let's use PyTorch to load the image data.
  id: totrans-1945
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAX ä¸åŒ…å«æ•°æ®åŠ è½½å·¥å…·ã€‚æˆ‘ä»¬ä½¿ç”¨ç°æœ‰çš„ TensorFlow å’Œ PyTorch æ•°æ®åŠ è½½å™¨åŠ è½½æ•°æ®ã€‚è®©æˆ‘ä»¬ä½¿ç”¨ PyTorch åŠ è½½å›¾åƒæ•°æ®ã€‚
- en: The first step is to create a PyTorchÂ [Dataset]Â class.
  id: totrans-1946
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€æ­¥æ˜¯åˆ›å»ºä¸€ä¸ª PyTorch [æ•°æ®é›†] ç±»ã€‚
- en: class CatsDogsDataset(Dataset):`
  id: totrans-1947
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: class CatsDogsDataset(Dataset):`
- en: def __init__(self, root_dir, annotation_file, transform=Non
  id: totrans-1948
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: def __init__(self, root_dir, annotation_file, transform=Non
- en: 'e):'
  id: totrans-1949
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'e):'
- en: self.root_dir = root_dir
  id: totrans-1950
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: self.root_dir = root_dir
- en: self.annotations = pd.read_csv(annotation_file) self.transform = transform
  id: totrans-1951
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: self.annotations = pd.read_csv(annotation_file) self.transform = transform
- en: '`def __len__(self):return len(self.annotations)`'
  id: totrans-1952
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def __len__(self):return len(self.annotations)`'
- en: '`def __getitem__(self, index):`'
  id: totrans-1953
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def __getitem__(self, index):`'
- en: img_id = self.annotations.iloc[index, 0]
  id: totrans-1954
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: img_id = self.annotations.iloc[index, 0]
- en: img = Image.open(os.path.join(self.root_dir, img_id)).c
  id: totrans-1955
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: img = Image.open(os.path.join(self.root_dir, img_id)).c
- en: '`onvert("RGB")`'
  id: totrans-1956
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`onvert("RGB")`'
- en: '`y_label = torch.tensor(float(self.annotations.iloc[inde`'
  id: totrans-1957
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`y_label = torch.tensor(float(self.annotations.iloc[inde`'
- en: '`x, 1])`'
  id: totrans-1958
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x, 1])`'
- en: 'if self.transform is not None: img = self.transform(img) return (img, y_label)'
  id: totrans-1959
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å¦‚æœ self.transform ä¸ä¸ºç©ºï¼šimg = self.transform(img) return (img, y_label)
- en: '**Data processing**'
  id: totrans-1960
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**æ•°æ®å¤„ç†**'
- en: Next, create aÂ Pandas DataFrameÂ with the image paths and labels.
  id: totrans-1961
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œä½¿ç”¨å›¾åƒè·¯å¾„å’Œæ ‡ç­¾åˆ›å»ºä¸€ä¸ª Pandas DataFrameã€‚
- en: '`train_df = pd.DataFrame(columns=["img_path","label"]) train_df["img_path"]
    = os.listdir("train/") for idx, i in enumerate(os.listdir("train/")):`'
  id: totrans-1962
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_df = pd.DataFrame(columns=["img_path","label"]) train_df["img_path"]
    = os.listdir("train/") for idx, i in enumerate(os.listdir("train/")):`'
- en: if "cat" in i:`train_df["label"][idx] = 0if "dog" in i:train_df["label"][idx]
    = 1train_df.to_csv (r'train_csv.csv', index = False, header=True)![](../images/00056.jpeg)Define
    a function to stack the data and return the images and labels as aÂ NumPy array.
  id: totrans-1963
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å¦‚æœ "cat" åœ¨ i ä¸­ï¼š`train_df["label"][idx] = 0if "dog" åœ¨ i ä¸­ï¼štrain_df["label"][idx]
    = 1train_df.to_csv (r'train_csv.csv', index = False, header=True)![](../images/00056.jpeg)å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥å †å æ•°æ®å¹¶å°†å›¾åƒå’Œæ ‡ç­¾ä½œä¸º
    NumPy æ•°ç»„è¿”å›ã€‚
- en: '`def custom_collate_fn(batch):`'
  id: totrans-1964
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def custom_collate_fn(batch):`'
- en: '`transposed_data = list(zip(*batch))`'
  id: totrans-1965
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`transposed_data = list(zip(*batch))`'
- en: '`labels = np.array(transposed_data[1])`'
  id: totrans-1966
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`labels = np.array(transposed_data[1])`'
- en: imgs = np.stack(transposed_data[0])
  id: totrans-1967
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: imgs = np.stack(transposed_data[0])
- en: return imgs, labels
  id: totrans-1968
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¿”å›å›¾åƒå’Œæ ‡ç­¾ä½œä¸º NumPy æ•°ç»„
- en: Let's also resize the images to ensure they are the same size. Define the size
    in a configuration dictionary. We'll use the other config variables later.
  id: totrans-1969
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜éœ€è¦è°ƒæ•´å›¾åƒå¤§å°ä»¥ç¡®ä¿å®ƒä»¬çš„å°ºå¯¸ä¸€è‡´ã€‚åœ¨é…ç½®å­—å…¸ä¸­å®šä¹‰å°ºå¯¸ã€‚ç¨åæˆ‘ä»¬å°†ä½¿ç”¨å…¶ä»–é…ç½®å˜é‡ã€‚
- en: config = {
  id: totrans-1970
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: config = {
- en: '''NUM_LABELS'': 2,'
  id: totrans-1971
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '''æ ‡ç­¾æ•°é‡'': 2,'
- en: '''BATCH_SIZE'': 32,'
  id: totrans-1972
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '''æ‰¹å¤„ç†å¤§å°'': 32,'
- en: '''N_EPOCHS'': 5,'
  id: totrans-1973
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '''è®­ç»ƒå‘¨æœŸ'': 5,'
- en: '`''LR'': 0.001,`'
  id: totrans-1974
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`''å­¦ä¹ é€Ÿç‡'': 0.001,`'
- en: '`''IMAGE_SIZE'': 224,`'
  id: totrans-1975
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`''å›¾åƒå°ºå¯¸'': 224,`'
- en: '`''WEIGHT_DECAY'': 1e-5, ''FREEZE_BACKBONE'': True,`'
  id: totrans-1976
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`''æƒé‡è¡°å‡'': 1e-5, ''å†»ç»“éª¨å¹²ç½‘ç»œ'': True,`'
- en: '}'
  id: totrans-1977
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '}'
- en: Resize the images using PyTorch transforms. Next, use theÂ CatsDogsDatasetÂ class
    to define the training and testing data loaders.
  id: totrans-1978
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ PyTorch è½¬æ¢è°ƒæ•´å›¾åƒå¤§å°ã€‚æ¥ä¸‹æ¥ï¼Œä½¿ç”¨ CatsDogsDataset ç±»å®šä¹‰è®­ç»ƒå’Œæµ‹è¯•æ•°æ®åŠ è½½å™¨ã€‚
- en: '`transform = transforms.Compose([`'
  id: totrans-1979
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`transform = transforms.Compose([`'
- en: transforms.Resize((config["IMAGE_SIZE"],config["IMAGE_SIZ
  id: totrans-1980
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: transforms.Resize((config["IMAGE_SIZE"],config["IMAGE_SIZ
- en: '`E"]))`,'
  id: totrans-1981
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`E"]))`,'
- en: np.array])
  id: totrans-1982
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`np.array`'
- en: dataset = CatsDogsDataset("train","train_csv.csv",transform=tra
  id: totrans-1983
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: dataset = CatsDogsDataset("train","train_csv.csv",transform=tra
- en: nsform)
  id: totrans-1984
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: nsform)
- en: '`train_set`, `validation_set = torch.utils.data.random_split(datas`'
  id: totrans-1985
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_set`, `validation_set = torch.utils.data.random_split(datas`'
- en: '`et,[20000,5000])`'
  id: totrans-1986
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`et,[20000,5000])`'
- en: '`train_loader = DataLoader(dataset=train_set, collate_fn=custom_`'
  id: totrans-1987
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`train_loader = DataLoader(dataset=train_set, collate_fn=custom_`'
- en: collate_fn,shuffle=True, batch_size=config["BATCH_SIZE"])
  id: totrans-1988
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: collate_fn,shuffle=True, batch_size=config["BATCH_SIZE"])
- en: '`validation_loader = DataLoader(dataset=validation_set,collate_f`'
  id: totrans-1989
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`validation_loader = DataLoader(dataset=validation_set,collate_f`'
- en: n=custom_collate_fn, shuffle=False, batch_size=config["BATCH_SI
  id: totrans-1990
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: n=custom_collate_fn, shuffle=False, batch_size=config["BATCH_SI
- en: '`ZE"])`'
  id: totrans-1991
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`ZE"])`'
- en: '**ResNet model definition**'
  id: totrans-1992
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**ResNetæ¨¡å‹å®šä¹‰**'
- en: Pre-trained ResNet models are trained on many classes. However, the dataset
    we have has two classes. We, therefore, use the ResNet as the backbone and define
    a custom classification layer.
  id: totrans-1993
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: é¢„è®­ç»ƒçš„ResNetæ¨¡å‹åœ¨è®¸å¤šç±»ä¸Šè¿›è¡Œäº†è®­ç»ƒã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬çš„æ•°æ®é›†åªæœ‰ä¸¤ç±»ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨ResNetä½œä¸ºä¸»å¹²ï¼Œå¹¶å®šä¹‰ä¸€ä¸ªè‡ªå®šä¹‰åˆ†ç±»å±‚ã€‚
- en: '**Create head network**'
  id: totrans-1994
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**åˆ›å»ºå¤´éƒ¨ç½‘ç»œ**'
- en: Create a head network with output as per the problem, in this case, a binary
    image classification.
  id: totrans-1995
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸€ä¸ªå¤´éƒ¨ç½‘ç»œï¼Œè¾“å‡ºä¸é—®é¢˜ç›¸ç¬¦ï¼Œæœ¬ä¾‹ä¸­ä¸ºäºŒè¿›åˆ¶å›¾åƒåˆ†ç±»ã€‚
- en: '"""'
  id: totrans-1996
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""'
- en: reference - https://www.kaggle.com/code/alexlwh/happywhale-flax
  id: totrans-1997
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å‚è€ƒ - https://www.kaggle.com/code/alexlwh/happywhale-flax
- en: '`-jax-tpu-gpu-resnet-baseline`'
  id: totrans-1998
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`-jax-tpu-gpu-resnet-baseline`'
- en: '"""'
  id: totrans-1999
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""'
- en: '`class Head(nn.Module):`'
  id: totrans-2000
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`class Head(nn.Module):`'
- en: '''''''head model''''''`batch_norm_cls: partial = partial(nn.BatchNorm, momentum=0.`'
  id: totrans-2001
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '''''''head model''''''`batch_norm_cls: partial = partial(nn.BatchNorm, momentum=0.`'
- en: 9)
  id: totrans-2002
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 9)
- en: '@nn.compact'
  id: totrans-2003
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '@nn.compact'
- en: '`def __call__(self, inputs, train: bool):`'
  id: totrans-2004
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def __call__(self, inputs, train: bool):`'
- en: output_n = inputs.shape[-1]x = self.batch_norm_cls(use_running_average=not train)
  id: totrans-2005
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: output_n = inputs.shape[-1]x = self.batch_norm_cls(use_running_average=not train)
- en: '`(inputs)'
  id: totrans-2006
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`(inputs)'
- en: '`x = nn.Dropout(rate=0.25)(x, deterministic=not train) x = nn.Dense(features=output_n)(x)'
  id: totrans-2007
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = nn.Dropout(rate=0.25)(x, deterministic=not train) x = nn.Dense(features=output_n)(x)'
- en: '`x = nn.relu(x)'
  id: totrans-2008
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = nn.relu(x)'
- en: '`x = self.batch_norm_cls(use_running_average=not train)'
  id: totrans-2009
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = self.batch_norm_cls(use_running_average=not train)'
- en: '`(x)'
  id: totrans-2010
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`(x)'
- en: '`x = nn.Dropout(rate=0.5)(x, deterministic=not train) x = nn.Dense(features=config["NUM_LABELS"])(x)
    return x'
  id: totrans-2011
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = nn.Dropout(rate=0.5)(x, deterministic=not train) x = nn.Dense(features=config["NUM_LABELS"])(x)
    return x'
- en: '**Combine ResNet backbone with head**'
  id: totrans-2012
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**å°†ResNetä¸»å¹²ä¸å¤´éƒ¨æ¨¡å‹ç»“åˆ**'
- en: Combine the pre-trained ResNet backbone with the custom head you created above.
  id: totrans-2013
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å°†é¢„è®­ç»ƒçš„ResNetä¸»å¹²ä¸æ‚¨ä¸Šé¢åˆ›å»ºçš„è‡ªå®šä¹‰å¤´éƒ¨ç»“åˆã€‚
- en: '`class Model(nn.Module):'
  id: totrans-2014
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`class Model(nn.Module):'
- en: '```Combines backbone and head model``` backbone: Sequential'
  id: totrans-2015
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '```ç»“åˆä¸»å¹²å’Œå¤´éƒ¨æ¨¡å‹``` backbone: Sequential'
- en: 'head: Head'
  id: totrans-2016
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'head: Head'
- en: 'def __call__(self, inputs, train: bool): x = self.backbone(inputs)'
  id: totrans-2017
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def __call__(self, inputs, train: bool): x = self.backbone(inputs)'
- en: average pool layer
  id: totrans-2018
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: å¹³å‡æ± åŒ–å±‚
- en: x = jnp.mean(x, axis=(1, 2)) x = self.head(x, train)
  id: totrans-2019
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: x = jnp.mean(x, axis=(1, 2)) x = self.head(x, train)
- en: return x
  id: totrans-2020
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¿”å› x
- en: '**Load pre-trained ResNet 50**'
  id: totrans-2021
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**åŠ è½½é¢„è®­ç»ƒçš„ResNet 50**'
- en: Next, create a function that loads the pre-trained ResNet model. Omit the last
    two layers of the network because we have defined a custom head. The function
    returns the ResNet model and its parameters. The model parameters are obtained
    using theÂ [slice_variables]Â function.
  id: totrans-2022
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œåˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼ŒåŠ è½½é¢„è®­ç»ƒçš„ResNetæ¨¡å‹ã€‚çœç•¥ç½‘ç»œçš„æœ€åä¸¤å±‚ï¼Œå› ä¸ºæˆ‘ä»¬å·²å®šä¹‰äº†ä¸€ä¸ªè‡ªå®šä¹‰å¤´éƒ¨ã€‚è¯¥å‡½æ•°è¿”å›ResNetæ¨¡å‹åŠå…¶å‚æ•°ã€‚ä½¿ç”¨Â [slice_variables]Â å‡½æ•°è·å–æ¨¡å‹å‚æ•°ã€‚
- en: '```def get_backbone_and_params(model_arch: str):```'
  id: totrans-2023
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '```def get_backbone_and_params(model_arch: str):```'
- en: '```'
  id: totrans-2024
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '```'
- en: Get backbone and params
  id: totrans-2025
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è·å–ä¸»å¹²å’Œå‚æ•°
- en: 1\. Loads pretrained model (resnet50)
  id: totrans-2026
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 1\. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹(resnet50)
- en: '2\. Get model and param structure except last 2 layers 3\. Extract the corresponding
    subset of the variables dict INPUT : model_arch'
  id: totrans-2027
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 2\. è·å–æ¨¡å‹å’Œå‚æ•°ç»“æ„ï¼Œé™¤äº†æœ€å2å±‚ 3\. æå–å˜é‡å­—å…¸çš„ç›¸åº”å­é›† è¾“å…¥ï¼šmodel_arch
- en: RETURNS backbone , backbone_params
  id: totrans-2028
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: RETURNS backbone , backbone_params
- en: '```'
  id: totrans-2029
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '```'
- en: '`if model_arch == ''resnet50'':'
  id: totrans-2030
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`if model_arch == ''resnet50'':'
- en: resnet_tmpl, params = pretrained_resnet(50) model = resnet_tmpl()else:raise
    NotImplementedError
  id: totrans-2031
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: resnet_tmpl, params = pretrained_resnet(50) model = resnet_tmpl()else:raise
    NotImplementedError
- en: get model & param structure for backbone start, end = 0, len(model.layers) -
    2 backbone = Sequential(model.layers[start:end]) backbone_params = slice_variables(params,
    start, end) return backbone, backbone_params
  id: totrans-2032
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: è·å–ä¸»å¹²æ¨¡å‹åŠå…¶å‚æ•°ç»“æ„çš„èµ·å§‹ç‚¹å’Œç»“æŸç‚¹ = 0, len(model.layers) - 2 backbone = Sequential(model.layers[start:end])
    backbone_params = slice_variables(params, start, end) return backbone, backbone_params
- en: Get model and variables
  id: totrans-2033
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: è·å–æ¨¡å‹å’Œå˜é‡
- en: 'Use the above function to create the final model. Define a function that:'
  id: totrans-2034
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä¸Šè¿°å‡½æ•°åˆ›å»ºæœ€ç»ˆæ¨¡å‹ã€‚å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼š
- en: Initializes the network's input.
  id: totrans-2035
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åˆå§‹åŒ–ç½‘ç»œçš„è¾“å…¥ã€‚
- en: Obtains the ResNet backbone and its parameters. Passes the input to the backbone
    and gets the output.
  id: totrans-2036
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è·å–ResNetä¸»å¹²åŠå…¶å‚æ•°ã€‚å°†è¾“å…¥ä¼ é€’ç»™ä¸»å¹²å¹¶è·å–è¾“å‡ºã€‚
- en: Initializes the network's head. Creates the final model using backbone and head.
  id: totrans-2037
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åˆå§‹åŒ–ç½‘ç»œçš„å¤´éƒ¨ã€‚ä½¿ç”¨ä¸»å¹²å’Œå¤´éƒ¨åˆ›å»ºæœ€ç»ˆæ¨¡å‹ã€‚
- en: Combines the parameters from backbone and head.
  id: totrans-2038
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ç»“åˆæ¥è‡ªä¸»å¹²å’Œå¤´éƒ¨çš„å‚æ•°ã€‚
- en: '```def get_model_and_variables(model_arch: str, head_init_key: in t):```'
  id: totrans-2039
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '```def get_model_and_variables(model_arch: str, head_init_key: in t):```'
- en: '```'
  id: totrans-2040
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '```'
- en: Get model and variables
  id: totrans-2041
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: Get model and variables
- en: 1\. Initialise inputs(shape=(1,image_size,image_size,3))
  id: totrans-2042
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 1\. åˆå§‹åŒ–è¾“å…¥(shape=(1,image_size,image_size,3))
- en: 2\. Get backbone and params
  id: totrans-2043
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 2\. Get backbone and params
- en: 3\. Apply backbone model and get outputs
  id: totrans-2044
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 3\. Apply backbone model and get outputs
- en: 4\. Initialise head
  id: totrans-2045
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 4\. Initialise head
- en: 5\. Create final model using backbone and head
  id: totrans-2046
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 5\. Create final model using backbone and head
- en: 6\. Combine params from backbone and head
  id: totrans-2047
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 6\. Combine params from backbone and head
- en: INPUT model_arch, head_init_key RETURNS model, variables '''
  id: totrans-2048
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: INPUT model_arch, head_init_key RETURNS model, variables '''
- en: '#backboneinputs = jnp.ones((1, config[''IMAGE_SIZE''],config[''IMAGE_SI ZE''],
    3), jnp.float32)backbone, backbone_params = get_backbone_and_params(model_a rch)
    key = jax.random.PRNGKey(head_init_key)backbone_output = backbone.apply(backbone_params,
    inputs, m utable=False)#headhead_inputs = jnp.ones((1, backbone_output.shape[-1]),
    jnp. float32)head = Head()head_params = head.init(key, head_inputs, train=False)'
  id: totrans-2049
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '#backboneinputs = jnp.ones((1, config[''IMAGE_SIZE''],config[''IMAGE_SI ZE''],
    3), jnp.float32)backbone, backbone_params = get_backbone_and_params(model_a rch)
    key = jax.random.PRNGKey(head_init_key)backbone_output = backbone.apply(backbone_params,
    inputs, m utable=False)#headhead_inputs = jnp.ones((1, backbone_output.shape[-1]),
    jnp. float32)head = Head()head_params = head.init(key, head_inputs, train=False)'
- en: '#final model'
  id: totrans-2050
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '#final model'
- en: model = Model(backbone, head)
  id: totrans-2051
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: model = Model(backbone, head)
- en: variables = FrozenDict({
  id: totrans-2052
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: variables = FrozenDict({
- en: '`''params'': {'
  id: totrans-2053
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`''params'': {'
- en: '`''backbone'': backbone_params[''params''], ''head'': head_params[''params'']'
  id: totrans-2054
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`''backbone'': backbone_params[''params''], ''head'': head_params[''params'']'
- en: '},'
  id: totrans-2055
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '},'
- en: '`''batch_stats'': {'
  id: totrans-2056
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`''batch_stats'': {'
- en: '''backbone'': backbone_params[''batch_stats''], ''head'': head_params[''batch_stats'']'
  id: totrans-2057
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`''backbone'': backbone_params[''batch_stats''], ''head'': head_params[''batch_stats'']'
- en: '}'
  id: totrans-2058
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '}'
- en: '})'
  id: totrans-2059
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '})'
- en: return model, variables
  id: totrans-2060
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: return model, variables
- en: All names relating to the backbone network are prefixed with the nameÂ [backbone].
    You can use any name, but all backbone variable names should be the same. This
    is important when freezing layers, as we'll see later.
  id: totrans-2061
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ‰€æœ‰ä¸éª¨å¹²ç½‘ç»œç›¸å…³çš„åç§°éƒ½ä»¥â€œbackboneâ€ä½œä¸ºå‰ç¼€ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ä»»ä½•åç§°ï¼Œä½†æ‰€æœ‰éª¨å¹²å˜é‡åç§°åº”è¯¥ç›¸åŒã€‚åœ¨å†»ç»“å±‚æ—¶ï¼Œè¿™ä¸€ç‚¹éå¸¸é‡è¦ï¼Œæ­£å¦‚æˆ‘ä»¬å°†åœ¨åé¢çœ‹åˆ°çš„é‚£æ ·ã€‚
- en: Next, use the function defined above to create the model.model, variables =
    get_model_and_variables('resnet50', 0) ![](../images/00057.jpeg)
  id: totrans-2062
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œä½¿ç”¨ä¸Šè¿°å®šä¹‰çš„å‡½æ•°åˆ›å»ºæ¨¡å‹ã€‚model, variables = get_model_and_variables('resnet50', 0)
    ![](../images/00057.jpeg)
- en: Zero gradients
  id: totrans-2063
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: Zero gradients
- en: Since we are applyingÂ transfer learning, we need to ensure that the backbone
    is not updated. Otherwise, we'll be training the network from scratch. We want
    to take advantage of the pre-trained weights and use them as aÂ feature extractorÂ for
    the network. To achieve this, we freeze the parameters of all layers whose name
    starts
  id: totrans-2064
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ç”±äºæˆ‘ä»¬æ­£åœ¨åº”ç”¨è¿ç§»å­¦ä¹ ï¼Œéœ€è¦ç¡®ä¿ä¸æ›´æ–°éª¨å¹²ç½‘ç»œã€‚å¦åˆ™ï¼Œæˆ‘ä»¬å°†ä»å¤´å¼€å§‹è®­ç»ƒç½‘ç»œã€‚æˆ‘ä»¬å¸Œæœ›åˆ©ç”¨é¢„è®­ç»ƒçš„æƒé‡ï¼Œå¹¶å°†å®ƒä»¬ç”¨ä½œç½‘ç»œçš„ç‰¹å¾æå–å™¨ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å†»ç»“æ‰€æœ‰ä»¥â€œbackboneâ€å’Œâ€œheadâ€å¼€å¤´çš„å±‚çš„å‚æ•°ã€‚
- en: withÂ [backbone]. As a result, these parameters will not be updated during training.
  id: totrans-2065
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: withÂ [backbone]. As a result, these parameters will not be updated during training.
- en: '"""'
  id: totrans-2066
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""'
- en: reference - https://github.com/deepmind/optax/issues/159#issuec omment-896459491
  id: totrans-2067
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å‚è€ƒ - https://github.com/deepmind/optax/issues/159#issuec omment-896459491
- en: '"""'
  id: totrans-2068
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""'
- en: 'def zero_grads():'
  id: totrans-2069
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def zero_grads():'
- en: ''''''''
  id: totrans-2070
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ''''''''
- en: Zero out the previous gradient computation
  id: totrans-2071
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: Zero out the previous gradient computation
- en: '```'
  id: totrans-2072
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '```'
- en: 'def init_fn(_):'
  id: totrans-2073
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def init_fn(_):'
- en: return ()
  id: totrans-2074
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¿”å›()
- en: 'def update_fn(updates, state, params=None):'
  id: totrans-2075
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def update_fn(updates, state, params=None):'
- en: return jax.tree_map(jnp.zeros_like, updates), () return optax.GradientTransformation(init_fn,
    update_fn)
  id: totrans-2076
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: return jax.tree_map(jnp.zeros_like, updates), () return optax.GradientTransformation(init_fn,
    update_fn)
- en: '"""'
  id: totrans-2077
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""'
- en: reference - https://colab.research.google.com/drive/1g_pt2Rc3bv 6H6qchvGHD-BpgF-Pt4vrC#scrollTo=TqDvTL_tIQCH&line=2&uniqifier=1
    """
  id: totrans-2078
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å‚è€ƒ - https://colab.research.google.com/drive/1g_pt2Rc3bv 6H6qchvGHD-BpgF-Pt4vrC#scrollTo=TqDvTL_tIQCH&line=2&uniqifier=1
    """
- en: 'def create_mask(params, label_fn):'
  id: totrans-2079
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def create_mask(params, label_fn):'
- en: 'def _map(params, mask, label_fn):for k in params:if label_fn(k):'
  id: totrans-2080
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def _map(params, mask, label_fn):for k in params:if label_fn(k):'
- en: mask[k] = 'zero'
  id: totrans-2081
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: mask[k] = 'zero'
- en: 'else:'
  id: totrans-2082
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'else:'
- en: 'if isinstance(params[k], FrozenDict): mask[k] = {}'
  id: totrans-2083
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'if isinstance(params[k], FrozenDict): mask[k] = {}'
- en: _map(params[k], mask[k], label_fn)
  id: totrans-2084
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: _map(params[k], mask[k], label_fn)
- en: 'else:'
  id: totrans-2085
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'else:'
- en: mask[k] = 'adam'
  id: totrans-2086
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: mask[k] = 'adam'
- en: mask = {}
  id: totrans-2087
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: mask = {}
- en: _map(params, mask, label_fn)
  id: totrans-2088
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: _map(params, mask, label_fn)
- en: return frozen_dict.freeze(mask)
  id: totrans-2089
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: return frozen_dict.freeze(mask)
- en: Define Flax optimizer
  id: totrans-2090
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: å®šä¹‰ Flax ä¼˜åŒ–å™¨
- en: Create an optimizer that will only be applied to the head and not backbone layers.
    This is done using theÂ optax.multi_transformÂ while passing the desired transformations.
  id: totrans-2091
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸€ä¸ªä»…åº”ç”¨äºå¤´éƒ¨è€Œä¸æ˜¯éª¨å¹²å±‚çš„ä¼˜åŒ–å™¨ã€‚è¿™æ˜¯é€šè¿‡ optax.multi_transform å®ç°çš„ï¼ŒåŒæ—¶ä¼ é€’æ‰€éœ€çš„å˜æ¢ã€‚
- en: adamw = optax.adamw(
  id: totrans-2092
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: adamw = optax.adamw(
- en: learning_rate=config['LR'],
  id: totrans-2093
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: learning_rate=config['LR'],
- en: b1=0.9, b2=0.999,
  id: totrans-2094
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: b1=0.9, b2=0.999,
- en: eps=1e-6, weight_decay=1e-2
  id: totrans-2095
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: eps=1e-6, weight_decay=1e-2
- en: )
  id: totrans-2096
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: )
- en: optimizer = optax.multi_transform(
  id: totrans-2097
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: optimizer = optax.multi_transform(
- en: '{''adam'': adamw, ''zero'': zero_grads()},'
  id: totrans-2098
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '{''adam'': adamw, ''zero'': zero_grads()},'
- en: 'create_mask(variables[''params''], lambda s: s.startswith(''ba ckbone''))'
  id: totrans-2099
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'create_mask(variables[''params''], lambda s: s.startswith(''ba ckbone''))'
- en: )
  id: totrans-2100
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: )
- en: Define Flax loss function
  id: totrans-2101
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: å®šä¹‰ Flax æŸå¤±å‡½æ•°
- en: Next, define the function to compute the loss function.
  id: totrans-2102
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€æ­¥ï¼Œå®šä¹‰è®¡ç®—æŸå¤±å‡½æ•°çš„å‡½æ•°ã€‚
- en: 'def cross_entropy_loss(*, logits, labels):'
  id: totrans-2103
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def cross_entropy_loss(*, logits, labels):'
- en: labels_onehot = jax.nn.one_hot(labels, num_classes=config["NU
  id: totrans-2104
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: labels_onehot = jax.nn.one_hot(labels, num_classes=config["NU
- en: M_LABELS"])
  id: totrans-2105
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: M_LABELS"])
- en: return optax.softmax_cross_entropy(logits=logits, labels=labe
  id: totrans-2106
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: return optax.softmax_cross_entropy(logits=logits, labels=labe
- en: ls_onehot).mean()
  id: totrans-2107
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ls_onehot).mean()
- en: 'When computing the loss during training, setÂ [train]Â toÂ [True]. You also have
    to:'
  id: totrans-2108
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è®¡ç®—æŸå¤±æ—¶ï¼Œå°†[train]è®¾ç½®ä¸º[True]ã€‚æ‚¨è¿˜éœ€è¦ï¼š`'
- en: Set theÂ batch_statsDefine the random number for theÂ [dropout]Â layers. Set theÂ [batch_stats]Â as
    mutable.
  id: totrans-2109
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è®¾ç½®[batch_stats]å®šä¹‰[dropout]å±‚çš„éšæœºæ•°ã€‚å°†[batch_stats]è®¾ç½®ä¸ºå¯å˜ã€‚
- en: 'def compute_loss(params, batch_stats, images, labels): logits,batch_stats =
    model.apply({''params'': params,''batch_s'
  id: totrans-2110
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def compute_loss(params, batch_stats, images, labels): logits,batch_stats =
    model.apply({''params'': params,''batch_s'
- en: 'tats'': batch_stats},images, train=True,rngs={''dropout'': jax.ran'
  id: totrans-2111
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'tats'': batch_stats},images, train=True,rngs={''dropout'': jax.ran'
- en: dom.PRNGKey(0)}, mutable=['batch_stats'])
  id: totrans-2112
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: dom.PRNGKey(0)}, mutable=['batch_stats'])
- en: loss = cross_entropy_loss(logits=logits, labels=labels) return loss, (logits,
    batch_stats)
  id: totrans-2113
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: loss = cross_entropy_loss(logits=logits, labels=labels) return loss, (logits,
    batch_stats)
- en: Define Flax metrics
  id: totrans-2114
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: å®šä¹‰FlaxæŒ‡æ ‡
- en: '`Using the loss function, define a function that will return the loss and accuracy
    during training.`'
  id: totrans-2115
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`ä½¿ç”¨æŸå¤±å‡½æ•°ï¼Œå®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œåœ¨è®­ç»ƒæœŸé—´è¿”å›æŸå¤±å’Œå‡†ç¡®åº¦ã€‚`'
- en: '`def compute_metrics(*, logits, labels):`'
  id: totrans-2116
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def compute_metrics(*, logits, labels):`'
- en: '`loss = cross_entropy_loss(logits=logits, labels=labels) accuracy = jnp.mean(jnp.argmax(logits,
    -1) == labels) metrics = {`'
  id: totrans-2117
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`loss = cross_entropy_loss(logits=logits, labels=labels) accuracy = jnp.mean(jnp.argmax(logits,
    -1) == labels) metrics = {`'
- en: '`''loss'': loss,`'
  id: totrans-2118
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`''loss'': loss,`'
- en: '`''accuracy'': accuracy,`'
  id: totrans-2119
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`''accuracy'': accuracy,`'
- en: '`}`'
  id: totrans-2120
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`}`'
- en: '`return metrics`'
  id: totrans-2121
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return metrics`'
- en: '**Create Flax training state**'
  id: totrans-2122
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**åˆ›å»ºFlaxè®­ç»ƒçŠ¶æ€**'
- en: '`Flax provides a training state for storing training information. In this case,
    we add theÂ [batch_stats]Â information.class TrainState(train_state.TrainState):
    batch_stats: FrozenDict`'
  id: totrans-2123
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Flaxæä¾›ä¸€ä¸ªè®­ç»ƒçŠ¶æ€æ¥å­˜å‚¨è®­ç»ƒä¿¡æ¯ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æ·»åŠ [batch_stats]ä¿¡æ¯ã€‚`'
- en: '`state = TrainState.create(`'
  id: totrans-2124
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`state = TrainState.create(`'
- en: '`apply_fn = model.apply,`'
  id: totrans-2125
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`apply_fn = model.apply,`'
- en: '`params = variables[''params''],`'
  id: totrans-2126
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`params = variables[''params''],`'
- en: '`tx = optimizer,`'
  id: totrans-2127
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`tx = optimizer,`'
- en: '`batch_stats = variables[''batch_stats''],`'
  id: totrans-2128
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`batch_stats = variables[''batch_stats''],`'
- en: '`)`'
  id: totrans-2129
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`)`'
- en: '**Training step**'
  id: totrans-2130
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**è®­ç»ƒæ­¥éª¤**'
- en: '`The training step receives the images and labels and computes the gradient
    with respect to the model parameters. It then returns the new state and the model
    metrics.`'
  id: totrans-2131
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`è®­ç»ƒæ­¥éª¤æ¥æ”¶å›¾åƒå’Œæ ‡ç­¾ï¼Œå¹¶è®¡ç®—ç›¸å¯¹äºæ¨¡å‹å‚æ•°çš„æ¢¯åº¦ã€‚ç„¶åè¿”å›æ–°çš„çŠ¶æ€å’Œæ¨¡å‹æŒ‡æ ‡ã€‚`'
- en: '`@jax.jit`'
  id: totrans-2132
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`@jax.jit`'
- en: '`def train_step(state: TrainState,images, labels):`'
  id: totrans-2133
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def train_step(state: TrainState,images, labels):`'
- en: '"""Train for a single step."""'
  id: totrans-2134
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""è®­ç»ƒå•æ­¥ã€‚"""'
- en: '`(batch_loss, (logits, batch_stats)), grads= jax.value_and_gra d(compute_loss,
    has_aux=True)(state.params,state.batch_stats, i mages,labels)`'
  id: totrans-2135
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`(batch_loss, (logits, batch_stats)), grads= jax.value_and_gra d(compute_loss,
    has_aux=True)(state.params,state.batch_stats, i mages,labels)`'
- en: '`state = state.apply_gradients(grads=grads)`'
  id: totrans-2136
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`state = state.apply_gradients(grads=grads)`'
- en: '`metrics = compute_metrics(logits=logits, labels=labels)`'
  id: totrans-2137
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`metrics = compute_metrics(logits=logits, labels=labels)`'
- en: '`return state, metrics`'
  id: totrans-2138
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return state, metrics`'
- en: '`To train the network for one epoch, loop through the training data while applying
    the training step.`'
  id: totrans-2139
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`è¦è®­ç»ƒç½‘ç»œä¸€ä¸ªepochï¼Œå¾ªç¯éå†è®­ç»ƒæ•°æ®å¹¶åº”ç”¨è®­ç»ƒæ­¥éª¤ã€‚`'
- en: '`def train_one_epoch(state, dataloader):`'
  id: totrans-2140
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def train_one_epoch(state, dataloader):`'
- en: '"""Train for 1 epoch on the training set.""" batch_metrics = []'
  id: totrans-2141
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""åœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒ1ä¸ªepochã€‚""" batch_metrics = []'
- en: '`for cnt, (images, labels) in enumerate(dataloader):`'
  id: totrans-2142
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`for cnt, (images, labels) in enumerate(dataloader):`'
- en: '`images = images / 255.0`'
  id: totrans-2143
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`images = images / 255.0`'
- en: '`state, metrics = train_step(state, images, labels) batch_metrics.append(metrics)`'
  id: totrans-2144
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`state, metrics = train_step(state, images, labels) batch_metrics.append(metrics)`'
- en: '`batch_metrics_np = jax.device_get(batch_metrics) epoch_metrics_np = {k: np.mean([metrics[k]
    for metrics in batch_metrics_n`'
  id: totrans-2145
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`batch_metrics_np = jax.device_get(batch_metrics) epoch_metrics_np = {k: np.mean([metrics[k]
    for metrics in batch_metrics_n`'
- en: '`p])`'
  id: totrans-2146
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`p])`'
- en: '`for k in batch_metrics_np[0] }`'
  id: totrans-2147
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`for k in batch_metrics_np[0] }`'
- en: '`return state, epoch_metrics_np`'
  id: totrans-2148
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return state, epoch_metrics_np`'
- en: '**Evaluation step**'
  id: totrans-2149
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**è¯„ä¼°æ­¥éª¤**'
- en: '`The model evaluation steps accept the test labels and images and applies them
    to the network. It then returns the model evaluation metrics. During evaluation,
    set theÂ [train]Â parameter toÂ [False]. You''ll also define theÂ [batch_stats]Â and
    the random number for theÂ [dropout]Â layer.`'
  id: totrans-2150
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`æ¨¡å‹è¯„ä¼°æ­¥éª¤æ¥å—æµ‹è¯•æ ‡ç­¾å’Œå›¾åƒï¼Œå¹¶å°†å…¶åº”ç”¨äºç½‘ç»œã€‚ç„¶åè¿”å›æ¨¡å‹è¯„ä¼°æŒ‡æ ‡ã€‚åœ¨è¯„ä¼°è¿‡ç¨‹ä¸­ï¼Œå°†[train]å‚æ•°è®¾ä¸º[False]ã€‚è¿˜éœ€å®šä¹‰[batch_stats]å’Œ[dropout]å±‚çš„éšæœºæ•°ã€‚`'
- en: '`@jax.jitdef eval_step(batch_stats, params, images, labels): logits = model.apply({''params'':
    params,''batch_stats'': batch _stats}, images, train=False,rngs={''dropout'':
    jax.random.PRNGKe y(0)})return compute_metrics(logits=logits, labels=labels)`'
  id: totrans-2151
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`@jax.jitdef eval_step(batch_stats, params, images, labels): logits = model.apply({''params'':
    params,''batch_stats'': batch _stats}, images, train=False,rngs={''dropout'':
    jax.random.PRNGKe y(0)})return compute_metrics(logits=logits, labels=labels)`'
- en: '`def evaluate_model(state, test_imgs, test_lbls):`'
  id: totrans-2152
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def evaluate_model(state, test_imgs, test_lbls):`'
- en: '"""Evaluate on the validation set."""'
  id: totrans-2153
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '"""åœ¨éªŒè¯é›†ä¸Šè¿›è¡Œè¯„ä¼°ã€‚"""'
- en: '`metrics = eval_step(state.batch_stats,state.params, test_im`'
  id: totrans-2154
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`metrics = eval_step(state.batch_stats,state.params, test_im`'
- en: '`gs, test_lbls)`'
  id: totrans-2155
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`gs, test_lbls)`'
- en: '`metrics = jax.device_get(metrics)`'
  id: totrans-2156
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`metrics = jax.device_get(metrics)`'
- en: '`metrics = jax.tree_map(lambda x: x.item(), metrics) return metrics`'
  id: totrans-2157
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`metrics = jax.tree_map(lambda x: x.item(), metrics) return metrics`'
- en: '**Train ResNet modelÂ in Flax**'
  id: totrans-2158
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**åœ¨Flaxä¸­è®­ç»ƒResNetæ¨¡å‹**'
- en: '`Train the ResNet model by applying theÂ [train_one_epoch]Â function for the
    desired number of epochs. This is a few epochs since we are finetuning the network.`'
  id: totrans-2159
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: é€šè¿‡åº”ç”¨`[train_one_epoch]`å‡½æ•°æ¥è®­ç»ƒResNetæ¨¡å‹ã€‚ç”±äºæˆ‘ä»¬åœ¨å¾®è°ƒç½‘ç»œï¼Œæ‰€ä»¥åªéœ€è¦å‡ ä¸ªepochsã€‚
- en: '**Set up TensorBoard in Flax**'
  id: totrans-2160
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**åœ¨Flaxä¸­è®¾ç½®TensorBoard**'
- en: '`To monitor model training via TensorBoard, you can write the training and
    validation metrics to TensorBoard.`'
  id: totrans-2161
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`è¦é€šè¿‡TensorBoardç›‘æ§æ¨¡å‹è®­ç»ƒï¼Œå¯ä»¥å°†è®­ç»ƒå’ŒéªŒè¯æŒ‡æ ‡å†™å…¥TensorBoardã€‚`'
- en: '`from torch.utils.tensorboard import SummaryWriter import torchvision.transforms.functional
    as F logdir = "flax_logs"`'
  id: totrans-2162
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from torch.utils.tensorboard import SummaryWriter import torchvision.transforms.functional
    as F logdir = "flax_logs"`'
- en: '`writer = SummaryWriter(logdir)`'
  id: totrans-2163
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`writer = SummaryWriter(logdir)`'
- en: Train model
  id: totrans-2164
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ¨¡å‹
- en: '`Define a function to train and evaluate the model while writing the metrics
    to TensorBoard.(test_images, test_labels) = next(iter(validation_loader)) test_images
    = test_images / 255.0`'
  id: totrans-2165
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹ï¼Œå¹¶å°†æŒ‡æ ‡å†™å…¥TensorBoardã€‚(test_images, test_labels) = next(iter(validation_loader))
    test_images = test_images / 255.0`'
- en: '`training_loss = [] training_accuracy = [] testing_loss = []`'
  id: totrans-2166
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`training_loss = [] training_accuracy = [] testing_loss = []`'
- en: '`testing_accuracy = []`'
  id: totrans-2167
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_accuracy = []`'
- en: '`def train_model(epochs):for epoch in range(1, epochs + 1):train_state, train_metrics
    = train_one_epoch(state, tra`'
  id: totrans-2168
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def train_model(epochs):for epoch in range(1, epochs + 1):train_state, train_metrics
    = train_one_epoch(state, tra`'
- en: '`in_loader)`'
  id: totrans-2169
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`in_loader)`'
- en: '`training_loss.append(train_metrics[''loss'']) training_accuracy.append(train_metrics[''accuracy''])`'
  id: totrans-2170
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`training_loss.append(train_metrics[''loss'']) training_accuracy.append(train_metrics[''accuracy''])`'
- en: '`test_metrics = evaluate_model(train_state, test_images, test_labels)`'
  id: totrans-2171
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`test_metrics = evaluate_model(train_state, test_images, test_labels)`'
- en: '`testing_loss.append(test_metrics[''loss''])`'
  id: totrans-2172
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_loss.append(test_metrics[''loss''])`'
- en: '`testing_accuracy.append(test_metrics[''accuracy''])`'
  id: totrans-2173
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`testing_accuracy.append(test_metrics[''accuracy''])`'
- en: '`writer.add_scalar(''Loss/train'', train_metrics[''loss''], epoch)`'
  id: totrans-2174
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`writer.add_scalar(''Loss/train'', train_metrics[''loss''], epoch)`'
- en: '`writer.add_scalar(''Loss/test'', test_metrics[''loss''], epoch)`'
  id: totrans-2175
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`writer.add_scalar(''Loss/test'', test_metrics[''loss''], epoch)`'
- en: '`writer.add_scalar(''Accuracy/train'', train_metrics[''accu racy''], epoch)`'
  id: totrans-2176
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`writer.add_scalar(''Accuracy/train'', train_metrics[''accu racy''], epoch)`'
- en: '`writer.add_scalar(''Accuracy/test'', test_metrics[''accura cy''], epoch)`'
  id: totrans-2177
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`writer.add_scalar(''Accuracy/test'', test_metrics[''accura cy''], epoch)`'
- en: '`print(f"Epoch: {epoch}, training loss: {train_metrics [''loss'']}, training
    accuracy: {train_metrics[''accuracy''] * 10 0}, validation loss: {test_metrics[''loss'']},
    validation accurac y: {test_metrics[''accuracy''] * 100}")`'
  id: totrans-2178
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`print(f"Epoch: {epoch}, training loss: {train_metrics [''loss'']}, training
    accuracy: {train_metrics[''accuracy''] * 10 0}, validation loss: {test_metrics[''loss'']},
    validation accurac y: {test_metrics[''accuracy''] * 100}")`'
- en: '`return train_stateRun the training function. trained_model_state = train_model(config["N_EPOCHS"])`'
  id: totrans-2179
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return train_stateRun the training function. trained_model_state = train_model(config["N_EPOCHS"])`'
- en: Save Flax model
  id: totrans-2180
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: ä¿å­˜Flaxæ¨¡å‹
- en: Use the `[save_checkpoint]` to save a trained Flax model.
  id: totrans-2181
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`[save_checkpoint]`ä¿å­˜è®­ç»ƒåçš„Flaxæ¨¡å‹ã€‚
- en: '`from flax.training import checkpoints`'
  id: totrans-2182
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from flax.training import checkpoints`'
- en: '`ckpt_dir = ''model_checkpoint/''`'
  id: totrans-2183
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`ckpt_dir = ''model_checkpoint/''`'
- en: '`checkpoints.save_checkpoint(ckpt_dir=ckpt_dir,`'
  id: totrans-2184
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`checkpoints.save_checkpoint(ckpt_dir=ckpt_dir,`'
- en: '`target=trained_model_state, step=100,`'
  id: totrans-2185
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`target=trained_model_state, step=100,`'
- en: '`prefix=''resnet_model'', overwrite=True`'
  id: totrans-2186
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`prefix=''resnet_model'', overwrite=True`'
- en: '`)`'
  id: totrans-2187
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`)`'
- en: Load saved Flax model
  id: totrans-2188
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: åŠ è½½ä¿å­˜çš„Flaxæ¨¡å‹
- en: A saved Flax model is loaded using the `[restore_checkpoint]` method.
  id: totrans-2189
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`[restore_checkpoint]`æ–¹æ³•åŠ è½½ä¿å­˜çš„Flaxæ¨¡å‹ã€‚
- en: '`loaded_model = checkpoints.restore_checkpoint('
  id: totrans-2190
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`loaded_model = checkpoints.restore_checkpoint('
- en: '`ckpt_dir=ckpt_dir, target=state, prefix=''resnet_mod`'
  id: totrans-2191
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`ckpt_dir=ckpt_dir, target=state, prefix=''resnet_mod`'
- en: '`el'') loaded_model`'
  id: totrans-2192
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`el'') loaded_model`'
- en: Evaluate Flax ResNet model
  id: totrans-2193
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: è¯„ä¼°Flax ResNetæ¨¡å‹
- en: '`To evaluate a Flax model, pass the test and training data to`'
  id: totrans-2194
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`è¦è¯„ä¼°Flaxæ¨¡å‹ï¼Œè¯·å°†æµ‹è¯•å’Œè®­ç»ƒæ•°æ®ä¼ é€’ç»™`'
- en: '`theÂ evalaute_modelÂ function.evaluate_model(loaded_model,test_images, test_labels)
    ![](../images/00058.gif)`'
  id: totrans-2195
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`theÂ evalaute_modelÂ function.evaluate_model(loaded_model,test_images, test_labels)
    ![](../images/00058.gif)`'
- en: Visualize model performance
  id: totrans-2196
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: å¯è§†åŒ–æ¨¡å‹æ€§èƒ½
- en: You can check the network's performance via TensorBoard or plot the metrics
    using Matplotlib.
  id: totrans-2197
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥é€šè¿‡TensorBoardæ£€æŸ¥ç½‘ç»œçš„æ€§èƒ½ï¼Œæˆ–è€…ä½¿ç”¨Matplotlibç»˜åˆ¶æŒ‡æ ‡ã€‚
- en: Final thoughts
  id: totrans-2198
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: Final thoughts
- en: 'You can apply transfer learning to take advantage of pre-trained models and
    get results with minimal effort. You have learned how to train a ResNet model
    in Flax. Specially, you have covered:'
  id: totrans-2199
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å¯ä»¥åº”ç”¨è¿ç§»å­¦ä¹ æ¥åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶ä¸”åªéœ€ä»˜å‡ºæœ€å°çš„åŠªåŠ›å°±èƒ½è·å¾—ç»“æœã€‚ä½ å·²ç»å­¦ä¼šäº†å¦‚ä½•åœ¨Flaxä¸­è®­ç»ƒResNetæ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œä½ å·²ç»è¦†ç›–äº†ï¼š
- en: '`How to define the ResNet model in Flax.`'
  id: totrans-2200
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`How to define the ResNet model in Flax.`'
- en: How to freeze the layers of the ResNet network.
  id: totrans-2201
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å¦‚ä½•å†»ç»“ResNetç½‘ç»œçš„å±‚ã€‚
- en: '`Training a ResNet model on custom data in Flax.`'
  id: totrans-2202
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Training a ResNet model on custom data in Flax.`'
- en: '`Saving and loading a ResNet model in Flax.`'
  id: totrans-2203
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Saving and loading a ResNet model in Flax.`'
- en: Elegy(High-level API for deep learning in JAX & Flax)
  id: totrans-2204
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: Elegy(High-level API for deep learning in JAX & Flax)
- en: '`Training deep learning networks in Flax is done in a couple of steps. It involves
    creating the following functions:`'
  id: totrans-2205
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Training deep learning networks in Flax is done in a couple of steps. It involves
    creating the following functions:`'
- en: Model definition.
  id: totrans-2206
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: Model definition.
- en: '`Compute metrics.`'
  id: totrans-2207
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Compute metrics.`'
- en: '`Training state.`'
  id: totrans-2208
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Training state.`'
- en: Training step.
  id: totrans-2209
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: Training step.
- en: Training and evaluation function.
  id: totrans-2210
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: Training and evaluation function.
- en: '`Flax and JAX give more control in defining and training deep learning networks.
    However, this comes with more verbosity. EnterÂ Elegy. Elegy is a high-level API
    for creating deep learning networks in JAX. Elegy''s API is like the one in Keras.`'
  id: totrans-2211
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Flax and JAX give more control in defining and training deep learning networks.
    However, this comes with more verbosity. EnterÂ Elegy. Elegy is a high-level API
    for creating deep learning networks in JAX. Elegy''s API is like the one in Keras.`'
- en: '`Let''s look at how to use Elegy to define and train deep learning networks
    in Flax.`'
  id: totrans-2212
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Let''s look at how to use Elegy to define and train deep learning networks
    in Flax.`'
- en: '`Data pre-processing`'
  id: totrans-2213
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '`Data pre-processing`'
- en: '`To make this illustration concrete, we''ll use theÂ movie review data from
    KaggleÂ to create an LSTM network in Flax.`'
  id: totrans-2214
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`To make this illustration concrete, we''ll use theÂ movie review data from
    KaggleÂ to create an LSTM network in Flax.`'
- en: '`The first step is to download and extract the data.`'
  id: totrans-2215
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`The first step is to download and extract the data.`'
- en: '`import os`'
  id: totrans-2216
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import os`'
- en: '`import kaggle`'
  id: totrans-2217
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import kaggle`'
- en: '`Obtain from https://www.kaggle.com/username/account os.environ["KAGGLE_USERNAME"]="KAGGLE_USERNAME"`'
  id: totrans-2218
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: '`Obtain from https://www.kaggle.com/username/account os.environ["KAGGLE_USERNAME"]="KAGGLE_USERNAME"`'
- en: '`os.environ["KAGGLE_KEY"]="KAGGLE_KEY"`'
  id: totrans-2219
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`os.environ["KAGGLE_KEY"]="KAGGLE_KEY"`'
- en: '`!kaggle datasets download lakshmi25npathi/imdb-dataset-of-50k-m ovie-reviews`'
  id: totrans-2220
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`!kaggle datasets download lakshmi25npathi/imdb-dataset-of-50k-m ovie-reviews`'
- en: '`import zipfile`'
  id: totrans-2221
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import zipfile`'
- en: '`with zipfile.ZipFile(''imdb-dataset-of-50k-movie-reviews.zip'',`'
  id: totrans-2222
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`with zipfile.ZipFile(''imdb-dataset-of-50k-movie-reviews.zip'',`'
- en: '`''r'') as zip_ref:zip_ref.extractall(''imdb-dataset-of-50k-movie-reviews'')Next,
    we define the following processing steps:`'
  id: totrans-2223
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`''r'') as zip_ref:zip_ref.extractall(''imdb-dataset-of-50k-movie-reviews'')Next,
    we define the following processing steps:`'
- en: '`Split the data into a training and testing set. Remove stopwords from the
    data.`'
  id: totrans-2224
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Split the data into a training and testing set. Remove stopwords from the
    data.`'
- en: '`Clean the data by removing punctuations and other special characters.`'
  id: totrans-2225
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ¸…ç†æ•°æ®ï¼Œå»é™¤æ ‡ç‚¹å’Œå…¶ä»–ç‰¹æ®Šå­—ç¬¦ã€‚
- en: '`Convert the data to a TensorFlow dataset.`'
  id: totrans-2226
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Convert the data to a TensorFlow dataset.`'
- en: '`Conver the data to numerical representation using the Keras vectorization
    layer.`'
  id: totrans-2227
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Conver the data to numerical representation using the Keras vectorization
    layer.`'
- en: '`import numpy as np`'
  id: totrans-2228
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import numpy as np`'
- en: '`import pandas as pd`'
  id: totrans-2229
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import pandas as pd`'
- en: '`from numpy import array`'
  id: totrans-2230
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from numpy import array`'
- en: '`import tensorflow_datasets as tfds`'
  id: totrans-2231
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import tensorflow_datasets as tfds`'
- en: '`import tensorflow as tf`'
  id: totrans-2232
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import tensorflow as tf`'
- en: '`from sklearn.model_selection import train_test_split from sklearn.preprocessing
    import LabelEncoder import matplotlib.pyplot as plt`'
  id: totrans-2233
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from sklearn.model_selection import train_test_split from sklearn.preprocessing
    import LabelEncoder import matplotlib.pyplot as plt`'
- en: '`from sklearn.model_selection import train_test_split import tensorflow as
    tf`'
  id: totrans-2234
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from sklearn.model_selection import train_test_split import tensorflow as
    tf`'
- en: '`df = pd.read_csv("imdb-dataset-of-50k-movie-reviews/IMDB Datase t.csv")`'
  id: totrans-2235
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`df = pd.read_csv("imdb-dataset-of-50k-movie-reviews/IMDB Datase t.csv")`'
- en: '`import nltk`'
  id: totrans-2236
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import nltk`'
- en: '`from nltk.corpus import stopwords`'
  id: totrans-2237
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from nltk.corpus import stopwords`'
- en: '`nltk.download(''stopwords'')`'
  id: totrans-2238
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`nltk.download(''stopwords'')`'
- en: '`def remove_stop_words(review):`'
  id: totrans-2239
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def remove_stop_words(review):`'
- en: '`review_minus_sw = []`'
  id: totrans-2240
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`review_minus_sw = []`'
- en: '`stop_words = stopwords.words(''english'')`'
  id: totrans-2241
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`stop_words = stopwords.words(''english'')`'
- en: '`review = review.split()`'
  id: totrans-2242
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`review = review.split()`'
- en: '`cleaned_review = [review_minus_sw.append(word) for word in`'
  id: totrans-2243
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`cleaned_review = [review_minus_sw.append(word) for word in`'
- en: '`review if word not in stop_words]`'
  id: totrans-2244
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`review if word not in stop_words]`'
- en: '`cleaned_review = '' ''.join(review_minus_sw)`'
  id: totrans-2245
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`cleaned_review = '' ''.join(review_minus_sw)`'
- en: '`return cleaned_review`'
  id: totrans-2246
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`return cleaned_review`'
- en: '`df[''review''] = df[''review''].apply(remove_stop_words) labelencoder = LabelEncoder()`'
  id: totrans-2247
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`df[''review''] = df[''review''].apply(remove_stop_words) labelencoder = LabelEncoder()`'
- en: '`df = df.assign(sentiment = labelencoder.fit_transform(df["senti ment"]))`'
  id: totrans-2248
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`df = df.assign(sentiment = labelencoder.fit_transform(df["senti ment"]))`'
- en: '`df = df.drop_duplicates()`'
  id: totrans-2249
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`df = df.drop_duplicates()`'
- en: '`docs = df[''review'']`'
  id: totrans-2250
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`docs = df[''review'']`'
- en: '`labels = array(df[''sentiment''])`'
  id: totrans-2251
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`labels = array(df[''sentiment''])`'
- en: '`X_train, X_test , y_train, y_test = train_test_split(docs, labe ls , test_size
    = 0.20, random_state=0)`'
  id: totrans-2252
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`X_train, X_test , y_train, y_test = train_test_split(docs, labe ls , test_size
    = 0.20, random_state=0)`'
- en: '`max_features = 10000 # Maximum vocab size.`'
  id: totrans-2253
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`max_features = 10000 # æœ€å¤§è¯æ±‡é‡å¤§å°ã€‚`'
- en: '`batch_size = 128`'
  id: totrans-2254
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`batch_size = 128`'
- en: '`max_len = 50 # Sequence length to pad the outputs to. vectorize_layer = tf.keras.layers.TextVectorization(standardize
    =''lower_and_strip_punctuation'',max_tokens=max_features,output_m ode=''int'',output_sequence_length=max_len)`'
  id: totrans-2255
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`max_len = 50 # åºåˆ—é•¿åº¦ï¼Œç”¨äºå¡«å……è¾“å‡ºã€‚vectorize_layer = tf.keras.layers.TextVectorization(standardize
    =''lower_and_strip_punctuation'',max_tokens=max_features,output_m ode=''int'',output_sequence_length=max_len)`'
- en: '`vectorize_layer.adapt(X_train)`'
  id: totrans-2256
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`vectorize_layer.adapt(X_train)`'
- en: '`X_train_padded = vectorize_layer(X_train)`'
  id: totrans-2257
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`X_train_padded = vectorize_layer(X_train)`'
- en: '`X_test_padded = vectorize_layer(X_test)`'
  id: totrans-2258
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`X_test_padded = vectorize_layer(X_test)`'
- en: '`training_data = tf.data.Dataset.from_tensor_slices((X_train_pad ded, y_train))`'
  id: totrans-2259
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`training_data = tf.data.Dataset.from_tensor_slices((X_train_pad ded, y_train))`'
- en: '`validation_data = tf.data.Dataset.from_tensor_slices((X_test_pa dded, y_test))`'
  id: totrans-2260
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`validation_data = tf.data.Dataset.from_tensor_slices((X_test_pa dded, y_test))`'
- en: '`training_data = training_data.batch(batch_size)`'
  id: totrans-2261
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`training_data = training_data.batch(batch_size)`'
- en: '`validation_data = validation_data.batch(batch_size) def get_train_batches():`'
  id: totrans-2262
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`validation_data = validation_data.batch(batch_size) def get_train_batches():`'
- en: '`ds = training_data.prefetch(1)`'
  id: totrans-2263
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`ds = training_data.prefetch(1)`'
- en: '`ds = ds.repeat(3)`'
  id: totrans-2264
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`ds = ds.repeat(3)`'
- en: '`ds = ds.shuffle(3, reshuffle_each_iteration=True)` # `tfds.dataset_as_numpy`
    converts the `tf.data.Dataset` into an'
  id: totrans-2265
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`ds = ds.shuffle(3, reshuffle_each_iteration=True)` # `tfds.dataset_as_numpy`
    converts the `tf.data.Dataset` into an'
- en: '`iterable of NumPy arraysreturn tfds.as_numpy(ds)`'
  id: totrans-2266
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`iterable of NumPy arraysreturn tfds.as_numpy(ds)`'
- en: '**Model definition in Elegy**'
  id: totrans-2267
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**Elegyæ¨¡å‹å®šä¹‰**'
- en: Start by installing `Elegy`, `Flax`, and `JAX`.`pip install -U elegy flax jax
    jaxlib`Next, define the LSTM model.
  id: totrans-2268
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: é¦–å…ˆå®‰è£…`Elegy`ï¼Œ`Flax`å’Œ`JAX`ã€‚`pip install -U elegy flax jax jaxlib`æ¥ä¸‹æ¥ï¼Œå®šä¹‰LSTMæ¨¡å‹ã€‚
- en: '`import jax`'
  id: totrans-2269
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import jax`'
- en: '`import jax.numpy as jnp import elegy as eg`'
  id: totrans-2270
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import jax.numpy as jnp import elegy as eg`'
- en: '`from flax import linen as nn`'
  id: totrans-2271
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`from flax import linen as nn`'
- en: '`class LSTMModel(nn.Module):`'
  id: totrans-2272
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`class LSTMModel(nn.Module):`'
- en: '`def setup(self):`'
  id: totrans-2273
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`def setup(self):`'
- en: '`self.embedding = nn.Embed(max_features, max_len)` `lstm_layer = nn.scan(nn.OptimizedLSTMCell,
    variable_broadcast="params", split_rngs={"params": False}, in_axes=1,`'
  id: totrans-2274
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.embedding = nn.Embed(max_features, max_len)` `lstm_layer = nn.scan(nn.OptimizedLSTMCell,
    variable_broadcast="params", split_rngs={"params": False}, in_axes=1,`'
- en: '`out_axes=1,`'
  id: totrans-2275
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`out_axes=1,`'
- en: '`length=max_len,`'
  id: totrans-2276
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`length=max_len,`'
- en: '`reverse=False)`'
  id: totrans-2277
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`reverse=False)`'
- en: '`self.lstm1 = lstm_layer()`'
  id: totrans-2278
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.lstm1 = lstm_layer()`'
- en: '`self.dense1 = nn.Dense(256)`'
  id: totrans-2279
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.dense1 = nn.Dense(256)`'
- en: '`self.lstm2 = lstm_layer()`'
  id: totrans-2280
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.lstm2 = lstm_layer()`'
- en: '`self.dense2 = nn.Dense(128)`'
  id: totrans-2281
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.dense2 = nn.Dense(128)`'
- en: '`self.lstm3 = lstm_layer()`'
  id: totrans-2282
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.lstm3 = lstm_layer()`'
- en: '`self.dense3 = nn.Dense(64)`'
  id: totrans-2283
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.dense3 = nn.Dense(64)`'
- en: '`self.dense4 = nn.Dense(2)`'
  id: totrans-2284
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`self.dense4 = nn.Dense(2)`'
- en: '`@nn.rematdef __call__(self, x_batch): x = self.embedding(x_batch)`'
  id: totrans-2285
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`@nn.rematdef __call__(self, x_batch): x = self.embedding(x_batch)`'
- en: '`carry, hidden = nn.OptimizedLSTMCell.initialize_carry(j ax.random.PRNGKey(0),
    batch_dims=(len(x_batch),), size=128)`'
  id: totrans-2286
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`carry, hidden = nn.OptimizedLSTMCell.initialize_carry(j ax.random.PRNGKey(0),
    batch_dims=(len(x_batch),), size=128)`'
- en: '`(carry, hidden)`, `x = self.lstm1((carry, hidden), x)`'
  id: totrans-2287
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`(carry, hidden)`, `x = self.lstm1((carry, hidden), x)`'
- en: '`x = self.dense1(x) x = nn.relu(x)`'
  id: totrans-2288
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = self.dense1(x) x = nn.relu(x)`'
- en: '`carry, hidden = nn.OptimizedLSTMCell.initialize_carry(j ax.random.PRNGKey(0),
    batch_dims=(len(x_batch),), size=64)`'
  id: totrans-2289
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`carry, hidden = nn.OptimizedLSTMCell.initialize_carry(j ax.random.PRNGKey(0),
    batch_dims=(len(x_batch),), size=64)`'
- en: '`(carry, hidden), x = self.lstm2((carry, hidden), x)`'
  id: totrans-2290
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`(carry, hidden), x = self.lstm2((carry, hidden), x)`'
- en: '`x = self.dense2(x) x = nn.relu(x)`'
  id: totrans-2291
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = self.dense2(x) x = nn.relu(x)`'
- en: '`carry, hidden = nn.OptimizedLSTMCell.initialize_carry(j ax.random.PRNGKey(0),
    batch_dims=(len(x_batch),), size=32)`'
  id: totrans-2292
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`carry, hidden = nn.OptimizedLSTMCell.initialize_carry(j ax.random.PRNGKey(0),
    batch_dims=(len(x_batch),), size=32)`'
- en: '`(carry, hidden), x = self.lstm3((carry, hidden), x)`'
  id: totrans-2293
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`(carry, hidden), x = self.lstm3((carry, hidden), x)`'
- en: '`x = self.dense3(x)`'
  id: totrans-2294
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = self.dense3(x)`'
- en: '`x = nn.relu(x)`'
  id: totrans-2295
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = nn.relu(x)`'
- en: '`x = self.dense4(x[:, -1]) return nn.log_softmax(x)`'
  id: totrans-2296
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`x = self.dense4(x[:, -1]) return nn.log_softmax(x)`'
- en: Let's now create an Elegy model using the above network. As you can see, the
    loss and metrics are defined like in `Keras`. The model compilation is done in
    the constructor, so you don't have to do this manually.
  id: totrans-2297
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨ä¸Šè¿°ç½‘ç»œåˆ›å»ºä¸€ä¸ªElegyæ¨¡å‹ã€‚æ­£å¦‚ä½ æ‰€è§ï¼ŒæŸå¤±å’ŒæŒ‡æ ‡çš„å®šä¹‰ç±»ä¼¼äº`Keras`ã€‚æ¨¡å‹çš„ç¼–è¯‘åœ¨æ„é€ å‡½æ•°ä¸­å®Œæˆï¼Œå› æ­¤æ‚¨æ— éœ€æ‰‹åŠ¨æ‰§è¡Œæ­¤æ“ä½œã€‚
- en: '`import optax`'
  id: totrans-2298
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`import optax`'
- en: '`model = eg.Model(`'
  id: totrans-2299
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`model = eg.Model(`'
- en: '`module=LSTMModel(),`'
  id: totrans-2300
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`module=LSTMModel(),`'
- en: '`loss=[`'
  id: totrans-2301
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`loss=[`'
- en: '`eg.losses.Crossentropy()`, `eg.regularizers.L2(l=1e-4)`, ],'
  id: totrans-2302
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`eg.losses.Crossentropy()`, `eg.regularizers.L2(l=1e-4)`, ],'
- en: '`metrics=eg.metrics.Accuracy(),`'
  id: totrans-2303
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`metrics=eg.metrics.Accuracy(),`'
- en: '`optimizer=optax.adam(1e-3), )`'
  id: totrans-2304
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`optimizer=optax.adam(1e-3), )`'
- en: '**Elegy model summary**'
  id: totrans-2305
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**Elegyæ¨¡å‹æ‘˜è¦**'
- en: Like in Keras, we can print the model's summary. `model.summary(jnp.array(X_train_padded[:64]))`
    ![](../images/00059.jpeg)
  id: totrans-2306
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åƒåœ¨ Keras ä¸­ä¸€æ ·ï¼Œæˆ‘ä»¬å¯ä»¥æ‰“å°æ¨¡å‹çš„æ‘˜è¦ã€‚`model.summary(jnp.array(X_train_padded[:64]))` ![](../images/00059.jpeg)
- en: '**Distributed training in Elegy**'
  id: totrans-2307
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**Elegy ä¸­çš„åˆ†å¸ƒå¼è®­ç»ƒ**'
- en: To train models in a distributed manner in Flax, we define parallel versions
    of our model training functions.
  id: totrans-2308
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¦åœ¨ Flax ä¸­ä»¥åˆ†å¸ƒå¼æ–¹å¼è®­ç»ƒæ¨¡å‹ï¼Œæˆ‘ä»¬å®šä¹‰æ¨¡å‹è®­ç»ƒå‡½æ•°çš„å¹¶è¡Œç‰ˆæœ¬ã€‚
- en: However, with Elegy, we call the `[distributed]` method.`model = model.distributed()`
  id: totrans-2309
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ï¼Œåœ¨ Elegy ä¸­ï¼Œæˆ‘ä»¬è°ƒç”¨ `[distributed]` æ–¹æ³•æ¥è¿›è¡Œåˆ†å¸ƒå¼æ–¹æ³•ã€‚`model = model.distributed()`
- en: '**Keras-like callbacks in Flax**'
  id: totrans-2310
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**Flax ä¸­ç±»ä¼¼ Keras çš„å›è°ƒ**'
- en: 'Elegy supports callbacks similar to Keras callbacks. In this case, we train
    the model with the following callbacks:'
  id: totrans-2311
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: Elegy æ”¯æŒç±»ä¼¼ Keras å›è°ƒçš„å›è°ƒå‡½æ•°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹å›è°ƒå‡½æ•°è®­ç»ƒæ¨¡å‹ï¼š
- en: '`TensorBoard.`'
  id: totrans-2312
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`TensorBoard.`'
- en: '`Model checkpoint.`'
  id: totrans-2313
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`æ¨¡å‹æ£€æŸ¥ç‚¹.`'
- en: '`Early stopping.callbacks = [ eg.callbacks.TensorBoard("summaries"), eg.callbacks.ModelCheckpoint("models/high-level",
    save_best_only=True),eg.callbacks.EarlyStopping(monitor = ''val_loss'',pa tience=10)]`'
  id: totrans-2314
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Early stopping.callbacks = [ eg.callbacks.TensorBoard("summaries"), eg.callbacks.ModelCheckpoint("models/high-level",
    save_best_only=True),eg.callbacks.EarlyStopping(monitor = ''val_loss'',pa tience=10)]`'
- en: '**Train Elegy models**'
  id: totrans-2315
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**è®­ç»ƒ Elegy æ¨¡å‹**'
- en: 'Elegy provides the `[fit]` method for training models. The method supports
    the following data sources:'
  id: totrans-2316
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: Elegy æä¾›äº† `[fit]` æ–¹æ³•æ¥è®­ç»ƒæ¨¡å‹ã€‚è¯¥æ–¹æ³•æ”¯æŒä»¥ä¸‹æ•°æ®æºï¼š
- en: '`Tensorflow Dataset.`'
  id: totrans-2317
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Tensorflow æ•°æ®é›†.`'
- en: '`Pytorch DataLoader`'
  id: totrans-2318
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Pytorch DataLoader`'
- en: '`Elegy DataLoader, and`'
  id: totrans-2319
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Elegy DataLoaderï¼Œå¹¶ä¸”`'
- en: '`Python Generators.`'
  id: totrans-2320
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`Python ç”Ÿæˆå™¨.`'
- en: '`history = model.fit(`'
  id: totrans-2321
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`history = model.fit(`'
- en: '`training_data,`'
  id: totrans-2322
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`training_data,`'
- en: '`epochs=100,`'
  id: totrans-2323
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: '`epochs=100,`'
- en: validation_data=(validation_data), callbacks=callbacks,
  id: totrans-2324
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: validation_data=(validation_data), callbacks=callbacks,
- en: )![](../images/00060.gif)
  id: totrans-2325
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: )![](../images/00060.gif)
- en: '**Evaluate Elegy models**'
  id: totrans-2326
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**è¯„ä¼° Elegy æ¨¡å‹**'
- en: To evaluate Elegy models, use theÂ [evaluate]Â function.model.evaluate(validation_data)![](../images/00061.jpeg)
  id: totrans-2327
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¦è¯„ä¼° Elegy æ¨¡å‹ï¼Œè¯·ä½¿ç”¨Â [evaluate]Â å‡½æ•°ã€‚model.evaluate(validation_data)![](../images/00061.jpeg)
- en: '**Visualize Elegy model with TensorBoard**'
  id: totrans-2328
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**ä½¿ç”¨ TensorBoard å¯è§†åŒ– Elegy æ¨¡å‹**'
- en: Since we applied the TensorBoard callback, we can view the performance of the
    model in TensorBoard.%load_ext tensorboard%tensorboard --logdir summaries ![](../images/00062.jpeg)
  id: totrans-2329
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ç”±äºæˆ‘ä»¬åº”ç”¨äº† TensorBoard å›è°ƒï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ TensorBoard ä¸­æŸ¥çœ‹æ¨¡å‹çš„æ€§èƒ½ã€‚%load_ext tensorboard%tensorboard
    --logdir summaries ![](../images/00062.jpeg)
- en: '**Plot model performance with Matplotlib**'
  id: totrans-2330
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**ä½¿ç”¨ Matplotlib ç»˜åˆ¶æ¨¡å‹æ€§èƒ½**'
- en: We can also plot the performance of the model usingÂ Matplotlib.import matplotlib.pyplot
    as plt
  id: totrans-2331
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨ Matplotlib ç»˜åˆ¶æ¨¡å‹çš„æ€§èƒ½ã€‚import matplotlib.pyplot as plt
- en: 'def plot_history(history):'
  id: totrans-2332
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'def plot_history(history):'
- en: n_plots = len(history.history.keys()) // 2
  id: totrans-2333
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: n_plots = len(history.history.keys()) // 2
- en: plt.figure(figsize=(14, 24))
  id: totrans-2334
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: plt.figure(figsize=(14, 24))
- en: for i, key in enumerate(list(history.history.keys())[:n_plo
  id: totrans-2335
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: for i, key in enumerate(list(history.history.keys())[:n_plo
- en: 'ts]):'
  id: totrans-2336
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'ts]):'
- en: metric = history.history[`key`]
  id: totrans-2337
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: metric = history.history[`key`]
- en: val_metric = history.history[f`val_{key}`]
  id: totrans-2338
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: val_metric = history.history[f`val_{key}`]
- en: plt.subplot(n_plots, 1, i + 1)
  id: totrans-2339
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: plt.subplot(n_plots, 1, i + 1)
- en: plt.plot(metric, label=f`Training {key}`) plt.plot(val_metric, label=f`Validation
    {key}`) plt.legend(loc=`lower right`)
  id: totrans-2340
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: plt.plot(metric, label=f`Training {key}`) plt.plot(val_metric, label=f`Validation
    {key}`) plt.legend(loc=`lower right`)
- en: plt.ylabel(`key`)
  id: totrans-2341
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: plt.ylabel(`key`)
- en: plt.title(`Training and Validation {key}`)
  id: totrans-2342
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: plt.title(`Training and Validation {key}`)
- en: plt.show()plot_history(history) ![](../images/00063.jpeg)
  id: totrans-2343
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: plt.show()plot_history(history) ![](../images/00063.jpeg)
- en: '**Making predictions with Elegy models**'
  id: totrans-2344
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**ä½¿ç”¨ Elegy æ¨¡å‹è¿›è¡Œé¢„æµ‹**'
- en: Like Keras, Elegy provides theÂ [predict]Â method for making predictions.(text,
    test_labels) = next(iter(validation_data)) y_pred = model.predict(jnp.array(text))
    ![](../images/00064.jpeg)
  id: totrans-2345
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åƒ Keras ä¸€æ ·ï¼ŒElegy æä¾›äº†è¿›è¡Œé¢„æµ‹çš„Â [predict]Â æ–¹æ³•ã€‚(text, test_labels) = next(iter(validation_data))
    y_pred = model.predict(jnp.array(text)) ![](../images/00064.jpeg)
- en: '**Saving and loading Elegy models**'
  id: totrans-2346
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**ä¿å­˜å’ŒåŠ è½½ Elegy æ¨¡å‹**'
- en: Elegy models can also be saved like Keras models and used to make predictions
    immediately.
  id: totrans-2347
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: Elegy æ¨¡å‹ä¹Ÿå¯ä»¥åƒ Keras æ¨¡å‹ä¸€æ ·ä¿å­˜ï¼Œå¹¶ç«‹å³ç”¨äºé¢„æµ‹ã€‚
- en: You can use can use `save` but `ModelCheckpoint already seria lized the model
  id: totrans-2348
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ä½¿ç”¨ `save` ä½†æ˜¯ `ModelCheckpoint` å·²ç»åºåˆ—åŒ–äº†æ¨¡å‹
- en: model.save(`model`)
  id: totrans-2349
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: model.save(`model`)
- en: current model referenceprint(`current model id:`, id(model))
  id: totrans-2350
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: å½“å‰æ¨¡å‹å‚è€ƒprint(`current model id:`, id(model))
- en: load model from disk
  id: totrans-2351
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: ä»ç£ç›˜åŠ è½½æ¨¡å‹
- en: model = eg.load(`models/high-level`)
  id: totrans-2352
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: model = eg.load(`models/high-level`)
- en: new model reference
  id: totrans-2353
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: æ–°æ¨¡å‹å‚è€ƒ
- en: 'print(`new model id: `, id(model))'
  id: totrans-2354
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: 'print(`new model id: `, id(model))'
- en: check that it works!model.evaluate(validation_data) ![](../images/00065.jpeg)
  id: totrans-2355
  prefs:
  - PREF_H1
  stylish: true
  type: TYPE_NORMAL
  zh: æ£€æŸ¥å…¶æ˜¯å¦æ­£å¸¸å·¥ä½œï¼model.evaluate(validation_data) ![](../images/00065.jpeg)
- en: '**Final thoughts**'
  id: totrans-2356
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**æœ€ç»ˆæƒ³æ³•**'
- en: 'This article has been a quick dive into Elegyâ€“ a JAX high-level API that you
    can use to build and train Flax networks. You have seen that Elegy is very similar
    to Keras and has a simple API for Flax. It also contains similar functions to
    Keras, like:'
  id: totrans-2357
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¿™ç¯‡æ–‡ç« ç®€è¦ä»‹ç»äº† Elegy â€”â€” ä¸€ç§æ‚¨å¯ä»¥ç”¨æ¥æ„å»ºå’Œè®­ç»ƒ Flax ç½‘ç»œçš„ JAX é«˜çº§ APIã€‚æ‚¨å·²ç»çœ‹åˆ° Elegy éå¸¸ç±»ä¼¼äº Kerasï¼Œå¹¶ä¸”å…·æœ‰ç”¨äº
    Flax çš„ç®€å• APIã€‚å®ƒè¿˜åŒ…å«ç±»ä¼¼äº Keras çš„åŠŸèƒ½ï¼Œå¦‚ï¼š
- en: Model training.
  id: totrans-2358
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ¨¡å‹è®­ç»ƒã€‚
- en: Making predictions.
  id: totrans-2359
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: è¿›è¡Œé¢„æµ‹ã€‚
- en: Creating callbacks.
  id: totrans-2360
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: åˆ›å»ºå›è°ƒã€‚
- en: Defining model loss and metrics.
  id: totrans-2361
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å®šä¹‰æ¨¡å‹æŸå¤±å’ŒæŒ‡æ ‡ã€‚
- en: '**Appendix**'
  id: totrans-2362
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**é™„å½•**'
- en: This book is provided in line with our terms and privacy policy.
  id: totrans-2363
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æœ¬ä¹¦çš„æä¾›ç¬¦åˆæˆ‘ä»¬çš„æ¡æ¬¾å’Œéšç§æ”¿ç­–ã€‚
- en: '**Disclaimer**'
  id: totrans-2364
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**å…è´£å£°æ˜**'
- en: The information in this eBook is not meant to be applied as is in a production
  id: totrans-2365
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æœ¬ç”µå­ä¹¦ä¸­çš„ä¿¡æ¯ä¸é€‚ç”¨äºç›´æ¥åœ¨ç”Ÿäº§ä¸­åº”ç”¨
- en: environment. By applying it to a production environment, you take full responsibility
  id: totrans-2366
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ç¯å¢ƒã€‚å°†å…¶åº”ç”¨äºç”Ÿäº§ç¯å¢ƒæ—¶ï¼Œæ‚¨éœ€æ‰¿æ‹…å…¨éƒ¨è´£ä»»
- en: for your actions.
  id: totrans-2367
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ç”±äºæ‚¨çš„è¡Œä¸ºã€‚
- en: The author has made every effort to ensure the accuracy of the information within
  id: totrans-2368
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ä½œè€…å·²å°½ä¸€åˆ‡åŠªåŠ›ç¡®ä¿ä¿¡æ¯çš„å‡†ç¡®æ€§
- en: this book was correct at the time of publication. The author does not assume
    and
  id: totrans-2369
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æœ¬ä¹¦å‡ºç‰ˆæ—¶çš„ä¿¡æ¯æ˜¯æ­£ç¡®çš„ã€‚ä½œè€…ä¸æ‰¿æ‹…ä»»ä½•è´£ä»»ï¼Œå¹¶ä¸”
- en: hereby disclaims any liability to any party for any loss, damage, or disruption
    caused
  id: totrans-2370
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ç‰¹æ­¤å£°æ˜å¯¹ä»»ä½•å› æŸå¤±ã€æŸåæˆ–é€ æˆä¸­æ–­è€Œå¯¼è‡´çš„è´£ä»»ä¸æ‰¿æ‹…ã€‚
- en: by errors or omissions, whether such errors or omissions result from accident,
  id: totrans-2371
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ç”±äºæ„å¤–ã€
- en: negligence, or any other cause.
  id: totrans-2372
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: ç–å¿½æˆ–å…¶ä»–åŸå› ã€‚
- en: No part of this eBook may be reproduced or transmitted in any form or by any
  id: totrans-2373
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æœªç»ä»»ä½•å½¢å¼æˆ–ä»»ä½•
- en: means, electronic or mechanical, recording or by any information storage and
  id: totrans-2374
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ‰‹æ®µï¼Œç”µå­æˆ–æœºæ¢°ï¼Œå½•åˆ¶æˆ–é€šè¿‡ä»»ä½•ä¿¡æ¯å­˜å‚¨å’Œ
- en: retrieval system, without written permission from the author.
  id: totrans-2375
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: æ£€ç´¢ç³»ç»Ÿï¼Œæœªç»ä½œè€…ä¹¦é¢è®¸å¯ã€‚
- en: '**Copyright**'
  id: totrans-2376
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**ç‰ˆæƒ**'
- en: JAX and Flax bookâ€” Deep learning with Flax and JAX Â© Copyright Derrick Mwiti.
    All Rights Reserved.
  id: totrans-2377
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: JAX å’Œ Flax ä¹¦ç±â€”â€” ä½¿ç”¨ Flax å’Œ JAX è¿›è¡Œæ·±åº¦å­¦ä¹  Â© ç‰ˆæƒ Derrick Mwitiã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚
- en: '**Other things to learn**'
  id: totrans-2378
  prefs:
  - PREF_H2
  stylish: true
  type: TYPE_NORMAL
  zh: '**å…¶ä»–å­¦ä¹ å†…å®¹**'
- en: Learn Python
  id: totrans-2379
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å­¦ä¹  Python
- en: Learn data science
  id: totrans-2380
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å­¦ä¹ æ•°æ®ç§‘å­¦
- en: Learn Streamlit
  id: totrans-2381
  prefs: []
  stylish: true
  type: TYPE_NORMAL
  zh: å­¦ä¹  Streamlit
