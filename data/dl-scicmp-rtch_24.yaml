- en: 19  Image segmentation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 19 图像分割
- en: 原文：[https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/image_segmentation.html](https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/image_segmentation.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/image_segmentation.html](https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/image_segmentation.html)'
- en: 19.1 Segmentation vs. classification
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 19.1 分割与分类的比较
- en: 'Both classification and segmentation are about labeling – but both don’t label
    the same thing. In classification, it’s complete images that are categorized;
    in segmentation, it’s individual pixels. For each and every pixel, we ask: Which
    object, or what kind of object, is this pixel part of? No longer are we just interested
    in saying “this is a cat”; this time, we need to know where exactly that cat is.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 分类和分割都是关于标记——但两者并不标记相同的东西。在分类中，是完整的图像被分类；在分割中，是单个像素。对于每个像素，我们问：这个像素是哪个对象，或是什么类型的对象的一部分？我们不再只是对说“这是一只猫”感兴趣；这次，我们需要知道那只猫的确切位置。
- en: 'Note the qualifier: exactly. That’s what constitutes the difference to *object
    detection*, where the class instances found are surrounded by a so-called bounding
    box. The type of localization hint provided by such boxes (rectangles, basically)
    is not sufficient for many tasks in, say, health care, biology, or the earth sciences.
    For example, in segmenting cell tissue, we need to see the actual boundaries between
    clusters of cell types, not their straightened versions.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 注意限定词：正好。这就是它与*目标检测*的区别所在，在目标检测中，找到的类实例被所谓的边界框所包围。这种由这些框（基本上是矩形）提供的定位提示类型对于许多任务来说是不够的，比如在医疗保健、生物学或地球科学中。例如，在分割细胞组织时，我们需要看到细胞类型簇的实际边界，而不是它们的直线版本。
- en: Object detection is not something we discuss in this book; but we’ve built up
    quite some experience with classification. Compared to classification, then, what
    changes?
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本书不讨论目标检测；但我们已经积累了相当多的分类经验。那么，与分类相比，有什么变化？
- en: Remember how, in the initial chapter on image classification, we talked about
    translation invariance and translation equivariance. If an operator is translation-invariant,
    it will return the same measurement when applied to location \(x\) and to \(x=x+n\).
    If, on the other hand, it is translation-equivariant, it will return a measurement
    adapted to the new location. In classification, the difference did not really
    matter. Now though, in segmentation, we want to avoid transformations that are
    not shift-equivariant. We can’t afford to lose location-related information anymore.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 记得在图像分类的初始章节中，我们讨论了平移不变性和平移等变性。如果一个算子是平移不变的，那么当它应用于位置 \(x\) 和 \(x=x+n\) 时，将返回相同的测量值。相反，如果它是平移等变的，它将返回适应新位置的测量值。在分类中，这种差异并不真正重要。但现在，在分割中，我们希望避免非平移等变的变换。我们不能再失去与位置相关的信息了。
- en: Technically, this means that no layer type should be used that abstracts over
    location. However, we just saw how successful the typical convolutional architecture
    is at learning about images. We certainly want to to re-use as much of that architecture
    as we can.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 技术上，这意味着不应使用任何抽象位置层的层类型。然而，我们刚刚看到典型的卷积架构在关于图像的学习方面是多么成功。我们当然希望尽可能多地重用该架构。
- en: 'We will avoid pooling layers, since those destroy information about location.
    But how, then, are we going to build up a hierarchy of features? For that hierarchy
    to emerge, it would seem like *some form* of spatial downsizing *has to* occur
    – no matter how we make it happen. And for sure, that representational hierarchy
    is something we can’t sacrifice: Whatever the downstream task, the features are
    required for the model to develop an “understanding” of sorts about what is displayed
    in the image. But: If we need to label every single pixel, the network must output
    an image exactly equal, in resolution, to the input! These are conflicting goals
    – can they be combined?'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将避免使用池化层，因为那些会破坏位置信息。那么，我们如何构建特征层次结构呢？为了出现这种层次结构，似乎必须发生某种形式的**空间下采样**——无论我们如何实现它。而且，确实，这种表示层次结构是我们不能牺牲的：无论下游任务是什么，特征都是模型发展某种“理解”所必需的。但是：如果我们需要标记每个像素，网络必须输出与输入分辨率完全相同的图像！这些是相互冲突的目标——它们能结合在一起吗？
- en: 19.2 U-Net, a “classic” in image segmentation
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 19.2 U-Net，图像分割中的“经典”
- en: 'The general U-Net architecture, first described in Ronneberger, Fischer, and
    Brox ([2015](references.html#ref-RonnebergerFB15)), has been used for countless
    tasks involving image segmentation, as a sub-module of numerous composite models
    as well as in various standalone forms. To explain the name “U-Net”, there is
    no better way than to reproduce a figure from the paper ([fig. 19.1](#fig-segmentation-unet)):'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 最初由Ronneberger、Fischer和Brox在[2015](references.html#ref-RonnebergerFB15)年描述的通用U-Net架构，已被用于无数涉及图像分割的任务，作为众多复合模型的子模块，以及各种独立形式。要解释“U-Net”这个名字，没有比重现论文中的图（[图19.1](#fig-segmentation-unet)）更好的方法了：
- en: '![A neural network whose layers are arranged in blocks that form a U-shape.
    Arrows indicate how processing first descends the left leg of the U, then ascends
    via the right leg. In addition, at all levels, there are arrows pointing from
    the left to the right leg.](../Images/865a29cec927ac0faf8a48f17df622e3.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![一个其层以形成U形块的神经网络。箭头指示处理首先下降U的左侧腿，然后通过右侧腿上升。此外，在所有级别，都有从左侧指向右侧腿的箭头。](../Images/865a29cec927ac0faf8a48f17df622e3.png)'
- en: 'Figure 19.1: U-Net architecture from Ronneberger, Fischer, and Brox ([2015](references.html#ref-RonnebergerFB15)),
    reproduced with the principal author’s permission.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.1：Ronneberger、Fischer和Brox在[2015](references.html#ref-RonnebergerFB15)年描述的U-Net架构，经主要作者许可重现。
- en: 'The left “leg” of the U shows a sequence of steps implementing a successive
    decrease in spatial resolution, accompanied by an increase in number of filters.
    The right leg illustrates the opposite mechanism: While number of filters goes
    down, spatial size increases right until we reach the original resolution of the
    input. (This is achieved via *upsampling*, a technique we’ll talk about below.)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: U的左侧“腿”显示了实现空间分辨率逐步降低的一系列步骤，同时滤波器数量增加。右侧的腿说明了相反的机制：当滤波器数量减少时，空间大小增加，直到我们达到输入的原始分辨率。（这是通过*上采样*实现的，我们将在下面讨论这一技术。）
- en: In consequence, we do in fact build up a feature hierarchy, and at the same
    time, we are able to classify individual pixels. But the upsampling process should
    result in significant loss of spatial information – are we really to expect sensible
    results?
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们确实建立了一个特征层次结构，同时我们也能够对单个像素进行分类。但是，上采样过程应该导致空间信息的显著损失——我们真的期望得到合理的结果吗？
- en: 'Well, probably not, were it not for the mechanism, also displayed in the above
    schematic, of channeling lower-level feature maps through the system. This is
    the constitutive U-Net idea: In the “down” sequence, when creating higher-level
    feature maps, we don’t throw away the lower-level ones; instead we keep them,
    to be eventually fed back into the “up” sequence. In the “up” sequence, once some
    small-resolution input has been upsampled, the matching-in-size features from
    the “down” process are appended. This means that each “up” step works on an ensemble
    of feature maps: ones kept while downsizing, *and* ones incorporating higher-level
    information.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，如果不是因为上述示意图中也显示的机制，即通过系统引导较低级别的特征图，可能不是这样。这是构成U-Net理念的关键：在“下”序列中，当创建高级特征图时，我们不会丢弃较低级别的特征图；相反，我们保留它们，以便最终反馈到“上”序列。在“上”序列中，一旦对小的分辨率输入进行了上采样，就会附加来自“下”过程的匹配尺寸的特征。这意味着每个“上”步骤都在处理一组特征图：在缩小尺寸时保留的，*以及*包含高级信息的那些。
- en: The fact that U-Net-based architectures are so pervasively used speaks to the
    power of this idea.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 基于U-Net的架构被广泛使用的事实，说明了这一想法的力量。
- en: 19.3 U-Net – a `torch` implementation
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 19.3 U-Net – `torch`实现
- en: Our `torch` implementation follows the general U-Net idea. As always in this
    book, kernel sizes, number of filters, as well as hyper-parameters should be seen
    as subject to experimentation.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`torch`实现遵循了U-Net的一般理念。正如本书中一贯的做法，核大小、滤波器数量以及超参数都应该被视为实验的对象。
- en: 'The implementation is modular, emphasizing the fact that we can distinguish
    two phases, an encoding and a decoding phase. Unlike in many other encoder-decoder
    architectures, these are coupled: Since the decoder needs to incorporate “messages”
    from the encoder, namely, the conserved feature maps, it will have to know about
    their sizes.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 实现是模块化的，强调我们可以区分两个阶段，一个编码阶段和一个解码阶段。与许多其他编码器-解码器架构不同，这些阶段是耦合的：由于解码器需要包含来自编码器的“消息”，即保留的特征图，它将需要了解它们的大小。
- en: 19.3.1 Encoder
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 19.3.1 编码器
- en: In the last chapter, we’ve seen how using a pre-trained feature extractor speeds
    up training. Now that we need to keep feature maps on our way “down”, can we still
    apply this technique? We can, and we’ll see how to do it shortly. First, let’s
    talk about the pre-trained model we’ll be using.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一章中，我们看到了如何使用预训练的特征提取器来加速训练。现在我们需要在“向下”的过程中保持特征图，我们是否还可以应用这项技术？我们可以，我们很快就会看到如何做到这一点。首先，让我们谈谈我们将使用的预训练模型。
- en: 'MobileNet v2 (Sandler et al. ([2018](references.html#ref-abs-1801-04381)))
    features a convolutional architecture optimized for mobile use. We won’t go into
    details here, but there is one thing we’d like to check: Does it lose translation
    equivariance; for example, by using local pooling? Let’s poke around a bit to
    find out.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: MobileNet v2（Sandler 等人，[2018](references.html#ref-abs-1801-04381)）具有针对移动使用优化的卷积架构。我们这里不会深入细节，但有一件事我们想确认：它是否失去了平移等变性；例如，通过使用局部池化？让我们探索一番，看看结果如何。
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE1]'
- en: 'Immediately, we see that `model_mobilenet_v2()` is a wrapping a sequence of
    two modules: a container called `features` (the feature detector, evidently),
    and another called `classifier` (again, with a telling name). It’s the former
    we’re interested in.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们立即看到`model_mobilenet_v2()`是一个包含两个模块的序列：一个名为`features`的容器（显然是特征检测器），另一个名为`classifier`（再次，有一个说明性的名字）。我们感兴趣的是前者。
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '*[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE3]'
- en: 'Thus MobileNet is mostly made up of a bunch of “inverted residual” blocks.
    What do these consist of? Some further poking around tells us:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，MobileNet主要由许多“残差反转”块组成。这些由什么组成？进一步的探索告诉我们：
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '*[PRE5]'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE5]'
- en: 'If we want to be paranoid, we still need to check the first of these modules:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想保持警惕，我们仍然需要检查这些模块中的第一个：
- en: '[PRE6]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '*[PRE7]'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE7]'
- en: 'It seems like there really is no pooling applied. The question then is, how
    would one obtain – and keep around – feature maps from different stages? Here
    is how the encoder does it:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来真的没有应用池化。那么问题来了，如何获得并保留来自不同阶段的特征图？以下是编码器是如何做到的：
- en: '[PRE8]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '*The encoder splits up MobileNet v2’s feature extraction blocks into several
    stages, and applies one stage after the other. Respective results are saved in
    a list.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*编码器将 MobileNet v2 的特征提取块分成几个阶段，并依次应用一个阶段。相应的结果保存在一个列表中。'
- en: 'We can construct an example, and inspect the sizes of the feature maps obtained.
    For three-channel input of resolution 224 x 224 pixels, we see:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以构建一个示例，并检查获得的特征图的大小。对于分辨率为224 x 224像素的三通道输入，我们看到：
- en: '[PRE9]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '*[PRE10]'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE10]'
- en: Next, we look at the decoder, which is a bit more complex.******  ***### 19.3.2
    Decoder
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们看看解码器，它稍微复杂一些。******  ***### 19.3.2 解码器
- en: 'The decoder is made up of configurable blocks. A block receives two input tensors:
    one that is the result of applying the previous decoder block, and one that holds
    the feature map produced in the matching encoder stage. In the forward pass, first
    the former is upsampled, and passed through a nonlinearity. The intermediate result
    is then prepended to the second argument, the channeled-through feature map. On
    the resultant tensor, a convolution is applied, followed by another nonlinearity.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器由可配置的块组成。一个块接收两个输入张量：一个是应用前一个解码器块的结果，另一个是在匹配的编码阶段产生的特征图。在正向传播中，首先将前者上采样，并通过非线性函数。然后将中间结果添加到第二个参数，即通道化的特征图。在结果张量上应用卷积，然后应用另一个非线性函数。
- en: '[PRE11]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '*We now look closer at how upsampling is achieved. Technically, what is applied
    is a so-called transposed convolution – hence the name of the layer, `nn_conv_transpose2d()`.(If
    you’re wondering about the `transpose`: Quite literally, the kernel *is* the transpose
    of a corresponding one that performs downsampling.) However, it’s more intuitive
    to picture the operation like this: First zeroes are inserted between individual
    tensor values, and then, a convolution with `strides` greater than `1` is applied.
    See [fig. 19.2](#fig-segmentation-conv-arithmetic-transposed) for a visualization.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们现在更详细地看看如何实现上采样。技术上，应用的是所谓的转置卷积——因此层的名字叫`nn_conv_transpose2d()`。（如果你对`transpose`感到好奇：字面上，核确实是执行下采样的相应核的转置。）然而，更直观的方式是这样的：首先在单个张量值之间插入零，然后应用步长大于`1`的卷积。参见[图19.2](#fig-segmentation-conv-arithmetic-transposed)以获取可视化。'
- en: '![Transposed convolution. A 5 x 5 image is obtained from a 3 x 3 one by inserting
    zeros. This image is then padded by one pixel at all edges. Then, a 3 x 3 filter
    slides over the image, resulting in a 5 x 5 output.](../Images/fca9b9d363fb1cc5b7f381b5ff2520ac.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![转置卷积。通过插入零从3 x 3的图像中获得一个5 x 5的图像。然后，在该图像的所有边缘添加一个像素。接着，一个3 x 3的滤波器在图像上滑动，得到一个5
    x 5的输出。](../Images/fca9b9d363fb1cc5b7f381b5ff2520ac.png)'
- en: 'Figure 19.2: Transposed convolution. Copyright Dumoulin and Visin ([2016](references.html#ref-2016arXiv160307285D)),
    reproduced under [MIT license](https://github.com/vdumoulin/conv_arithmetic/blob/master/LICENSE).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.2：转置卷积。版权所有杜莫林和维辛 ([2016](references.html#ref-2016arXiv160307285D))，经[MIT许可证](https://github.com/vdumoulin/conv_arithmetic/blob/master/LICENSE)重新发布。
- en: Even though we won’t go into technical details, we can do a quick check that
    really, convolution and transposed convolution affect resolution in opposite ways.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们不会深入技术细节，但我们可以快速检查，实际上，卷积和转置卷积以相反的方式影响分辨率。
- en: We start from a 1 x 1 “image”, and apply a 3 x 3 filter with a `stride` of 2\.
    Together with padding, this results in an output tensor of size 3 x 3.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个1 x 1的“图像”开始，并应用一个`stride`为2的3 x 3滤波器。连同填充，这导致一个3 x 3大小的输出张量。
- en: '[PRE12]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '*[PRE13]'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE13]'
- en: 'If we take that output, and now apply a transposed convolution, with the same
    kernel size, strides, and padding as the above convolution, we are back to the
    original resolution of 5 x 5:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用那个输出，并现在应用一个与上述卷积具有相同内核大小、步长和填充的转置卷积，我们将回到原始的5 x 5分辨率：
- en: '[PRE14]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '*[PRE15]'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE15]'
- en: After that quick check, back to the decoder block. What is the outcome of its
    very first application?
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在快速检查之后，回到解码器块。它的第一次应用会有什么结果？
- en: Above, we saw that at the “bottom of the U”, we will have a tensor of size 7
    x 7, with 320 channels. This tensor will be upsampled, and concatenated with feature
    maps from the previous “down” stage. At that stage, there had been 96 channels.
    That makes two thirds of the information needed to instantiate a decoder block
    (`in_channels` and `skip_channels`). The missing third, `out_channels`, really
    is up to us. Here we choose 256.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面，我们看到在“U”的底部，我们将有一个7 x 7大小的张量，具有320个通道。这个张量将被上采样，并与来自先前“下”阶段的特征图连接。在那个阶段，有96个通道。这构成了实例化解码器块所需的信息的三分之二（`in_channels`和`skip_channels`）。缺失的三分之一，`out_channels`，实际上取决于我们。这里我们选择256。
- en: 'We can thus instantiate a decoder block:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以实例化一个解码器块：
- en: '[PRE16]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '*To do a forward pass, the block needs to be passed two tensors: the maximally-processed
    features, and their immediate precursors. Let’s check that our understanding is
    correct:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*为了进行前向传递，该块需要传递两个张量：最大处理的特征及其直接的前驱。让我们检查我们的理解是否正确：'
- en: '[PRE17]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '*[PRE18]'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE18]'
- en: Let me remark in passing that the purpose of exercises like this is not just
    to explain some concrete architecture. They’re also supposed to illustrate how
    with `torch`, instead of having to rely on some overall idea of what a piece of
    code will probably do, you can usually find a way to *know*.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我顺便提一下，这种练习的目的不仅仅是解释某种具体的架构。它们还旨在说明如何使用`torch`，而不是依赖于对某段代码可能做什么的整体想法，你通常可以找到一种方法来*知道*。
- en: Now that we’ve talked at length about the decoder blocks, we can quickly characterize
    their “manager” of sorts, the decoder module. It “merely” instantiates and runs
    through the blocks.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经详细讨论了解码器块，我们可以快速描述它们的“管理者”类型的解码器模块。它“仅仅”实例化和运行这些块。
- en: '[PRE19]*****  ***### 19.3.3 The “U”'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE19]*****  ***### 19.3.3 “U”'
- en: 'Before we take the last step and look at the top-level module, let’s see how
    the U-shape comes about in our case. Here ([tbl. 19.1](#tbl-segmentation-u)) is
    a table, displaying “image” sizes at every step of the “down” and “up” passes,
    as well as the actors responsible for shape manipulations:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们采取最后一步并查看顶层模块之前，让我们看看在我们的情况下U形是如何产生的。这里有一个表格，显示了“下”和“上”遍历的每个步骤中的“图像”大小，以及负责形状变换的参与者：
- en: 'Table 19.1: Tensor sizes at various stages of the encoding and decoding chains.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 表19.1：编码和解码链中各个阶段的张量大小。
- en: '| **Encoder steps** |  | **Decoder steps** |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| **编码步骤** |  | **解码步骤** |'
- en: '| --- | --- | --- |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| ***Input***: `224 x 224` (channels: 3) |  | ***Output***: `224 x 224` (channels:
    16) |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| **输入**：`224 x 224` (通道：3) |  | **输出**：`224 x 224` (通道：16) |'
- en: '|  |  |  |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |'
- en: '| \(\Downarrow\) *convolve (MobileNet)* |  | *upsample* \(\Uparrow\) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| \(\Downarrow\) *卷积（MobileNet）* |  | *上采样* \(\Uparrow\) |'
- en: '|  |  |  |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |'
- en: '| `112 x 112` (channels: 16) | *append* \(\Rightarrow\) | `112 x 112` (channels:
    32) |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| `112 x 112` (通道：16) | *附加* \(\Rightarrow\) | `112 x 112` (通道：32) |'
- en: '|  |  |  |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |'
- en: '| \(\Downarrow\) *convolve (MobileNet)* |  | *upsample* \(\Uparrow\) |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| \(\Downarrow\) *卷积(MobileNet)* |  | *上采样* \(\Uparrow\) |'
- en: '|  |  |  |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |'
- en: '| `56 x 56` (channels: 24) | *append* \(\Rightarrow\) | `56 x 56` (channels:
    64) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| `56 x 56` (通道：24) | *附加* \(\Rightarrow\) | `56 x 56` (通道：64) |'
- en: '|  |  |  |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |'
- en: '| \(\Downarrow\) *convolve (MobileNet)* |  | *upsample* \(\Uparrow\) |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| \(\Downarrow\) *卷积(MobileNet)* |  | *上采样* \(\Uparrow\) |'
- en: '|  |  |  |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |'
- en: '| `28 x 28` (channels: 32) | *append* \(\Rightarrow\) | `28 x 28` (channels:
    128) |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| `28 x 28` (通道：32) | *附加* \(\Rightarrow\) | `28 x 28` (通道：128) |'
- en: '|  |  |  |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |'
- en: '| \(\Downarrow\) *convolve (MobileNet)* |  | *upsample* \(\Uparrow\) |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| \(\Downarrow\) *卷积(MobileNet)* |  | *上采样* \(\Uparrow\) |'
- en: '|  |  |  |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |'
- en: '| `14 x 14` (channels: 96) | *append* \(\Rightarrow\) | `14 x 14` (channels:
    256) |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| `14 x 14` (通道：96) | *附加* \(\Rightarrow\) | `14 x 14` (通道：256) |'
- en: '|  |  |  |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |'
- en: '| \(\Downarrow\) *convolve (MobileNet)* |  | *upsample* \(\Uparrow\) |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| \(\Downarrow\) *卷积(MobileNet)* |  | *上采样* \(\Uparrow\) |'
- en: '|  |  |  |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |'
- en: '| `7 x 7` (channels: 320) | *use as input* \(\Rightarrow\) | `7 x 7` (channels:
    320) |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| `7 x 7` (通道：320) | *用作输入* \(\Rightarrow\) | `7 x 7` (通道：320) |'
- en: Did you notice that the final output has sixteen channels? In the end, we want
    to use the *channels* dimension for class scores; so really, we’ll need to have
    as many channels as there are different “pixel classes”. This, of course, is task-dependent,
    so it makes sense to have a dedicated module take care of it. The top-level module
    will then be a composition of the “U” part and a final, score-generating layer.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 你注意到最终输出有十六个通道吗？最后，我们希望使用*通道*维度来表示类别得分；因此，我们实际上需要与不同“像素类别”数量一样多的通道。这当然取决于任务，因此有一个专门的模块来处理它是有意义的。顶层模块将是“U”部分和最终得分生成层的组合。
- en: 19.3.4 Top-level module
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 19.3.4 顶层模块
- en: 'In our task, there will be three pixel classes. The score-producing submodule
    can then just be a final convolution, producing three channels:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的任务中，将有三个像素类别。得分生成子模块可以只是一个最终的卷积，生成三个通道：
- en: '[PRE20]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '*Now that I’ve already mentioned that there’ll be three pixel classes to tell
    apart, it’s time for full disclosure: What, then, is the task we’ll use this model
    on?*******  ***## 19.4 Dogs and cats'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*既然我已经提到将有三种像素类别需要区分，现在是时候完全公开了：那么，我们将使用这个模型来完成什么任务？*******  ***## 19.4 狗和猫'
- en: 'This time, we need an image dataset that has each individual pixel tagged.
    One of those, the [Oxford Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/),
    is an ensemble of cats and dogs. As provided by `torchdatasets`, it comes with
    three types of target data to choose from: the overall class (cat or dog), the
    individual breed (there are thirty-seven of them), and a pixel-level segmentation
    with three categories: foreground, boundary, and background. The default is exactly
    the type of target we need: the segmentation map.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们需要一个每个像素都有标签的图像数据集。其中之一，[牛津宠物数据集](https://www.robots.ox.ac.uk/~vgg/data/pets/)，是猫和狗的集合。由`torchdatasets`提供，它包含三种类型的目标数据可供选择：整体类别（猫或狗）、个体品种（共有三十七种），以及具有三个类别的像素级分割：前景、边界和背景。默认情况下，这正是我们所需要的类型：分割图。
- en: '[PRE21]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '*Images come in different sizes. As in the previous chapter, we want all of
    them to share the same resolution; one that also fulfills the requirements of
    MobileNet v2\. The masks will need to be resized in the same way. So far, then,
    this looks like a case for the `transform =` and `target_transform =` arguments
    we encountered in the last chapter. But if we also want to apply data augmentation,
    things get more complex.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*图片大小各不相同。正如前一章所述，我们希望所有图片都具有相同的分辨率；这个分辨率还要满足MobileNet v2的要求。掩码也需要以同样的方式进行缩放。因此，这似乎是我们在上章中遇到的`transform
    =`和`target_transform =`参数的用例。但如果我们也想应用数据增强，事情就会变得更加复杂。'
- en: Imagine we make use of random flipping. An input image will be flipped – or
    not – according to some probability. But if the image is flipped, the mask better
    had be, as well! Input and target transformations are not independent, in this
    case.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，我们使用随机翻转。输入图像将根据某种概率翻转或不翻转。但如果图像翻转了，掩码也最好一起翻转！在这种情况下，输入和目标变换不是独立的。
- en: 'A solution is to create a wrapper around `oxford_pet_dataset()` that lets us
    “hook into” the `.getitem()` method, like so:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一个解决方案是创建一个围绕`oxford_pet_dataset()`的包装器，使我们能够“钩入”`.getitem()`方法，如下所示：
- en: '[PRE22]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '*With this wrapper, all we have to do is create a custom function that decides
    what augmentation to apply once per input-target *pair*, and manually calls the
    respective transformation functions.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '*有了这个包装器，我们只需要创建一个自定义函数，该函数决定每次输入-目标*对*应用哪种增强，并手动调用相应的变换函数。'
- en: Here, we flip every second image (on average). And if we do, we flip the mask
    as well.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们每隔一张图像翻转一次（平均来说）。如果我们这样做，我们也会翻转掩码。
- en: '[PRE23]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '*Since we’re at it, let me mention types of augmentation that should be helpful
    with slightly different formulations of the task. Why don’t we try (small-ish)
    random rotations, or translations, or both?'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*既然我们提到了这一点，让我提一下应该有助于任务不同形式化的增强类型。我们为什么不尝试（小到中等）的随机旋转，或者平移，或者两者都尝试？'
- en: 'Like this:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 就像这样：
- en: '[PRE24]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '*As-is, that piece of code does not work. This is because a rotation will introduce
    black pixels, or – technically speaking – zeroes in the tensor. Before, possible
    target classes went from `1` to `3`. Now there’s an additional class, `0`. As
    a consequence, loss computation will expect the model output to have four, not
    three, slots in the second dimension – and fail.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*当前的代码片段不起作用。这是因为旋转会引入黑色像素，或者从技术上讲，在张量中引入零。之前，可能的目标类别从`1`增加到`3`。现在又增加了一个类别，`0`。因此，损失计算将期望模型输出在第二维有四个槽位，而不是三个，从而导致失败。'
- en: There are several possible workarounds. First, we could take the risk and assume
    that in nearly all cases, this will affect only background pixels. Under that
    assumption, we can just set all values that have turned to `0` to `2`, the background
    class.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种可能的解决方案。首先，我们可以冒险假设在几乎所有情况下，这只会影响背景像素。在这个假设下，我们可以将所有变为`0`的值设置为`2`，即背景类别。
- en: 'Another thing we can do is upsize the image, trigger the rotation, and downsize
    again:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以做的一件事是放大图像，触发旋转，然后再缩小：
- en: '[PRE25]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '*In this specific case, there still is a problem, though. In my experiments,
    training performance got a lot worse. This could be because we have “boundary”
    class. The whole *râison d’être* of a boundary is to be sharp; the downside of
    sharp boundaries is their not dealing well with resizings.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*然而，在这个特定情况下，仍然存在问题。在我的实验中，训练性能大幅下降。这可能是因为我们有一个“边界”类别。整个*râison d’être*的边界是为了保持锐利；锐利的边界的缺点是它们在调整大小时处理得不好。'
- en: However, in the real world, segmentation requirements will vary widely. Maybe
    you have just two classes, foreground and background. Maybe there are many. Experimenting
    with rotations (and translations, that also will introduce black pixels) cannot
    hurt.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在现实世界中，分割要求可能会有很大差异。也许你只有两个类别，前景和背景。也许有很多。尝试旋转（以及也会引入黑色像素的平移）不会有害。
- en: 'Back on the main track, we can now make use of the dataset wrapper, `pet_dataset()`,
    to instantiate the training and validation sets:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 回到主要轨道上，我们现在可以利用数据集包装器`pet_dataset()`来实例化训练集和验证集：
- en: '[PRE26]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '*We create the data loaders, and run the learning rate finder ([fig. 19.3](#fig-segmentation-lr-finder)):'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们创建了数据加载器，并运行了学习率查找器（[图19.3](#fig-segmentation-lr-finder)）：'
- en: '[PRE27]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '*![A curve that, from left to right, first stays flat (until after x=0.01),
    then begins to rise very sharply.](../Images/d623c008fec2f6bad5dfbbb9676f7b6f.png)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '*![一条曲线，从左到右，首先保持平坦（直到x=0.01之后），然后开始非常急剧地上升。](../Images/d623c008fec2f6bad5dfbbb9676f7b6f.png)'
- en: 'Figure 19.3: Learning rate finder, run on the Oxford Pet Dataset.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.3：在牛津宠物数据集上运行的学习率查找器。
- en: From this plot, we conclude that a maximum learning rate of 0.01, at least when
    run with a one-cycle strategy, should work fine.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个图中，我们得出结论，最大学习率为0.01，至少当使用一周期策略运行时应该可以正常工作。
- en: '[PRE28]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '*[PRE29]'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE29]'
- en: This looks like decent improvement; but – as always – we want to see what the
    model actually says. To that end, we generate segmentation masks for the first
    eight observations in the validation set, and plot them overlayed on the images.
    (We probably don’t need to also display the ground truth, humans being rather
    familiar with cats and dogs.)
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来像是一个不错的改进；但——就像往常一样——我们想看看模型实际上说了什么。为此，我们为验证集的前八个观测值生成分割掩码，并将它们叠加在图像上。（我们可能不需要也显示真实值，因为人类对猫和狗相当熟悉。）
- en: A convenient way to plot an image and superimpose a mask is provided by the
    `raster` package.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`raster`包提供了一个方便的方法来绘制图像并叠加掩码。'
- en: '[PRE30]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '*Pixel intensities have to be between zero and one, which is why in the dataset
    wrapper, we have made it so normalization can be switched off. To plot the actual
    images, we just instantiate a clone of `valid_ds` that leaves the pixel values
    unchanged. (The predictions, on the other hand, will still have to be obtained
    from the original validation set.)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '*像素强度必须在零到一之间，这就是为什么在数据集包装器中，我们使其能够关闭归一化。为了绘制实际图像，我们只需实例化一个`valid_ds`的克隆，保持像素值不变。（另一方面，预测仍然需要从原始验证集中获得。）'
- en: '[PRE31]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '*Finally, the predictions are generated, and overlaid over the images one-by-one
    ([fig. 19.4](#fig-segmentation-segmentation)):'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '*最后，预测被生成，并逐个叠加到图像上（[图 19.4](#fig-segmentation-segmentation)）：'
- en: '[PRE32]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '*![Eight images of cats and dogs, with green boundaries indicating which pixels
    the network thinks belong to the animal in question. With minor exceptions, the
    boundaries are correct.](../Images/c1db721cb18a82b735f92d84dbf8868e.png)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '*![八张猫和狗的图像，绿色边界表示网络认为属于所讨论动物的像素。除少数例外，边界是正确的。](../Images/c1db721cb18a82b735f92d84dbf8868e.png)'
- en: 'Figure 19.4: Cats and dogs: Sample images and predicted segmentation masks.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19.4：猫和狗：样本图像和预测的分割掩码。
- en: This looks pretty reasonable!
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来相当合理！
- en: Now, it’s time we let images be, and look at different application domains.
    In the next chapter, we explore deep learning on tabular data.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候让图像自由，看看不同的应用领域了。在下一章，我们将探讨表格数据的深度学习。
- en: 'Dumoulin, Vincent, and Francesco Visin. 2016\. “A guide to convolution arithmetic
    for deep learning.” *arXiv e-Prints*, March, arXiv:1603.07285\. [https://arxiv.org/abs/1603.07285](https://arxiv.org/abs/1603.07285).Ronneberger,
    Olaf, Philipp Fischer, and Thomas Brox. 2015\. “U-Net: Convolutional Networks
    for Biomedical Image Segmentation.” *CoRR* abs/1505.04597\. [http://arxiv.org/abs/1505.04597](http://arxiv.org/abs/1505.04597).Sandler,
    Mark, Andrew G. Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.
    2018\. “Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification,
    Detection and Segmentation.” *CoRR* abs/1801.04381\. [http://arxiv.org/abs/1801.04381](http://arxiv.org/abs/1801.04381).**************'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Dumoulin, Vincent, 和 Francesco Visin. 2016. “深度学习卷积算术指南。” *arXiv e-Prints*，三月，arXiv:1603.07285。[https://arxiv.org/abs/1603.07285](https://arxiv.org/abs/1603.07285)。Ronneberger,
    Olaf, Philipp Fischer, 和 Thomas Brox. 2015. “U-Net：用于生物医学图像分割的卷积网络。” *CoRR* abs/1505.04597。[http://arxiv.org/abs/1505.04597](http://arxiv.org/abs/1505.04597)。Sandler,
    Mark, Andrew G. Howard, Menglong Zhu, Andrey Zhmoginov, 和 Liang-Chieh Chen. 2018.
    “倒置残差和线性瓶颈：用于分类、检测和分割的移动网络。” *CoRR* abs/1801.04381。[http://arxiv.org/abs/1801.04381](http://arxiv.org/abs/1801.04381)。
