- en: 21  Time series
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 21 时间序列
- en: 原文：[https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/time_series.html](https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/time_series.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/time_series.html](https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/time_series.html)
- en: 'In this chapter, we’ll again look at a new type of data: time series. Previously,
    moving on from images to tabular data, we found a substantial difference in that
    image data are homogeneous, while tabular data aren’t. With images, individual
    values correspond to pixels, or positions on a grid. With tabular data, values
    can – in principle – be “anything”; but most often, we deal with a mix of categorical,
    ordinal, and numerical data. However, both types of application have one thing
    in common: All values relate to the same point in time.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将再次探讨一种新的数据类型：时间序列。之前，从图像到表格数据的转变，我们发现两者之间有实质性的差异，即图像数据是同质的，而表格数据不是。在图像中，单个值对应于像素，或网格上的位置。在表格数据中，值在原则上可以是“任何东西”；但大多数情况下，我们处理的是类别、顺序和数值数据的混合。然而，这两种类型的应用有一个共同点：所有值都关联到相同的时间点。
- en: With time series, we’re facing a new situation. Assume the series is one-dimensional,
    that is, it has a single feature. Thus, the data type is homogeneous. But now,
    the input is a sequence. What follows?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间序列中，我们面临一个新的情况。假设序列是一维的，也就是说，它只有一个特征。因此，数据类型是同质的。但现在，输入是一个序列。接下来会发生什么？
- en: Before, when a mix of data types were present, we found we had to do some pre-processing
    up-front. We also saw that, by adding in a new type of module - the embedding
    module – we could refine and enhance the overall (linear) model. Now though, a
    bigger change is needed. We again will have to do some pre-processing; but this
    time, we’ll also need a different type of top-level *model*, a type as different
    from the standard feed-forward architecture as is the convolutional one we already
    studied.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前，当存在多种数据类型混合时，我们发现我们不得不进行一些前置处理。我们也看到，通过添加一种新的模块——嵌入模块，我们可以精炼和增强整体（线性）模型。但现在，我们需要更大的改变。我们又将不得不进行一些前置处理；但这次，我们还需要一种不同类型的顶级
    *模型*，这种模型与标准的前馈架构的不同程度，就像我们已研究过的卷积架构一样。
- en: '21.1 Deep learning for sequences: the idea'
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 21.1 深度学习序列：理念
- en: 'Say we have a sequence of daily average temperatures, measured in degrees Celsius:
    `-1.1, 2.0, -0.2, -0.9, 4.5, -3.6, -9.1`. Clearly, these values are not independent;
    we’d hardly guess that the very next measurement would result in in, say, `21`.
    In fact, if these seven averages were all you’re given, your best guess for the
    next day would probably just be `-9.1`. But when people say “time series”, they
    have longer sequences in mind. With longer sequences, you can try to detect patterns,
    such as trends or periodicities. And that’s what the established techniques in
    time series analysis do.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个由每天的平均气温组成的序列，以摄氏度为单位：`-1.1, 2.0, -0.2, -0.9, 4.5, -3.6, -9.1`。显然，这些值并不是独立的；我们几乎不可能猜测下一个测量值会是`21`。实际上，如果这七个平均值是你所拥有的全部，你对于下一天的最好猜测可能只是`-9.1`。但当我们说“时间序列”时，我们指的是更长的序列。随着序列的变长，你可以尝试检测模式，比如趋势或周期性。这正是时间序列分析中已建立的技术的目的。
- en: 'For a deep learning model to do the same, it first of all has to “perceive”
    individual values as sequential. We make that happen by increasing tensor dimensionality
    by one, and using the additional dimension for sequential ordering. Now, the model
    has to do something useful with that. Think back of what it is a linear model
    is doing: It takes input \(\mathbf{X}\), multiplies by its weight matrix \(\mathbf{W}\),
    and adds bias vector \(\mathbf{b}\):'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让深度学习模型做到这一点，首先它必须“感知”单个值作为序列。我们通过增加张量维度性一个维度，并使用额外的维度进行序列排序来实现这一点。现在，模型必须对它做一些有用的事情。回想一下线性模型在做什么：它接受输入
    \(\mathbf{X}\)，乘以其权重矩阵 \(\mathbf{W}\)，并加上偏置向量 \(\mathbf{b}\)：
- en: \[ f(\mathbf{X}) = \mathbf{X}\mathbf{W} + \mathbf{b} \]
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\mathbf{X}) = \mathbf{X}\mathbf{W} + \mathbf{b} \]
- en: In sequence models, this type of operation is still present; it’s just that
    now, it is executed for every time step – i.e., every position in the sequence
    – in isolation. But this means that now, the relationship *between time steps*
    has to be taken care of. To that end, the module takes what was obtained at the
    previous time step, applies a different weight matrix, and adds a different bias
    vector. This, in itself, is again an affine transformation, – just not of the
    input, but of what is called the previous *state*. The outputs from both affine
    computations are added, and the result then serves as prior state to the computation
    due at the next time step.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在序列模型中，这种操作仍然存在；只是现在，它对每个时间步——即序列中的每个位置——独立执行。但这意味着现在，必须处理时间步之间的**关系**。为此，模块使用前一时间步获得的结果，应用不同的权重矩阵，并添加不同的偏置向量。这本身又是一个仿射变换——只是不是对输入，而是对所谓的先前**状态**。将两个仿射计算的结果相加，然后作为下一个时间步计算的前置状态。
- en: 'In other words, *at each time step*, two types of information are combined:
    the (weight-transformed) input for the current time step, and the (weight-transformed)
    state that resulted from processing the previous one. In math:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，*在每个时间步*，结合两种类型的信息：当前时间步的（加权变换）输入，以及处理前一时间步产生的（加权变换）状态。用数学表示：
- en: \[ state_{(t)} = f(\mathbf{W_{input}}\mathbf{X_{(t)}} + \mathbf{b_{input}} +
    \mathbf{W_{state}}\mathbf{X_{(t-1)}} + \mathbf{b_{state}}) \]
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: \[ state_{(t)} = f(\mathbf{W_{input}}\mathbf{X_{(t)}} + \mathbf{b_{input}} +
    \mathbf{W_{state}}\mathbf{X_{(t-1)}} + \mathbf{b_{state}}) \]
- en: This logic specifies a *recurrence relation*, and modules implementing it are
    called *recurrent neural networks* (RNNs). In the next section, we’ll implement
    such a module ourselves; you’ll see how, in code, that recurrence maps to straightforward
    iteration. Before, two remarks.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这种逻辑指定了一个**递归关系**，实现这种关系的模块被称为**循环神经网络**（RNNs）。在下一节中，我们将自己实现这样一个模块；你将看到在代码中，递归是如何映射到简单的迭代的。在此之前，有两点需要注意。
- en: First, in the above formula, the function applied to the sum of the two transformations
    represents an activation; typically for RNNs, the default is the hyperbolic tangent,
    `torch_tanh()`.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在上面的公式中，应用于两个变换之和的函数代表一个激活函数；对于循环神经网络来说，通常默认的是双曲正切函数，`torch_tanh()`。
- en: 'Second, in the official `torch` documentation, you’ll see the formula written
    this way:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，在官方的`torch`文档中，你会看到公式这样写：
- en: \[ h_{(t)} = f(\mathbf{W_{i\_h}\mathbf{X_{(t)}} }+ \mathbf{b_{i\_h}} + \mathbf{W_{h\_h}\mathbf{X}_{(t-1)}}
    + \mathbf{b}_{h\_h}) \]
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: \[ h_{(t)} = f(\mathbf{W_{i\_h}\mathbf{X_{(t)}} }+ \mathbf{b_{i\_h}} + \mathbf{W_{h\_h}\mathbf{X}_{(t-1)}}
    + \mathbf{b}_{h\_h}) \]
- en: Here \(h\) stands for “hidden”, as in “hidden state”, and subscripts \(i\_h\)
    and \(h\_h\) stand for “input-to-hidden” and “hidden-to-hidden”, respectively.
    The reason I’d like to de-emphasize the “hidden” in “hidden state” is because
    in the `torch` implementation, the state is not necessarily hidden from the user.
    You’ll see what I mean below. (Pretty soon, I’ll give up resistance against the
    term, though, since it is ubiquitous in descriptive prose as well as code (for
    example, as regards variable naming). But I wanted to state this clearly at least
    once, so you won’t be confused when mapping your mental model of the algorithm
    to the behavior of `torch` RNNs.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 \(h\) 代表“隐藏”，正如“隐藏状态”中的“隐藏”，下标 \(i\_h\) 和 \(h\_h\) 分别代表“输入到隐藏”和“隐藏到隐藏”。我想淡化“隐藏状态”中的“隐藏”一词的原因是，在`torch`实现中，状态并不一定是隐藏给用户的。你将在下面看到我的意思。（很快，我将对这个术语放弃抵抗，因为它在描述性文字和代码（例如，关于变量命名）中无处不在。但我至少想明确地说明这一点，这样你就不至于在将你的算法心理模型映射到`torch`
    RNNs的行为时感到困惑。）
- en: 21.2 A basic recurrent neural network
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 21.2 基本循环神经网络
- en: 'In the above discussion, we identified two basic things a recurrent neural
    network has to do: (1) iterate over the input sequence; and (2) execute the “business
    logic” of a sequence, that is, combine information from the previous as well as
    the current time step.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述讨论中，我们确定了循环神经网络必须完成的两个基本任务：(1) 遍历输入序列；以及(2) 执行序列的“业务逻辑”，即结合前一时间步和当前时间步的信息。
- en: Commonly, these duties are divided between two different objects. One, referred
    to as the “cell”, implements the logic. The other takes care of the iteration.
    The reason for this modularization is that both “inner” and “outer” logic should
    be modifiable independently. For example, you might want to keep the way you iterate
    over time steps, but modify what happens at each point in the iteration. This
    will get more concrete later, when we talk about the most-used sub-types of RNNs.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这些职责由两个不同的对象分担。一个被称为“细胞”，实现了逻辑。另一个负责迭代。这种模块化的原因在于“内部”和“外部”逻辑都应该可以独立修改。例如，您可能想保持遍历时间步的方式，但修改迭代中的每个点发生的事情。当我们谈到最常用的
    RNN 子类型时，这会变得更加具体。
- en: In our basic implementation of a basic RNN, both cell and iteration handler
    are `nn_module()`s. First, we have the *cell*.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们基本实现的简单 RNN 中，细胞和迭代处理程序都是 `nn_module()`。首先，我们有 *cell*。
- en: 21.2.1 Basic `rnn_cell()`
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 21.2.1 基本的 `rnn_cell()`
- en: The logic combines two affine operations; and affine operations are just what
    linear modules are for. We therefore just have our cell delegate to two linear
    modules, append the respective outputs, and apply a *tanh* activation.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑结合了两个仿射操作；仿射操作正是线性模块的作用。因此，我们的细胞只是将任务委托给两个线性模块，将相应的输出附加，并应用一个 *tanh* 激活。
- en: As already alluded to above, in naming the modules and parameters, I’m following
    the `torch` conventions, so things will sound familiar when we move on to actual
    `torch` modules. Most notably, this includes referring to the state as “hidden
    state”, and thus, to its dimensionality as `hidden_size`, even though state is
    hidden from the user only under certain circumstances (which we’ll come to).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 正如上面已经提到的，在命名模块和参数时，我遵循了 `torch` 的约定，因此当我们继续到实际的 `torch` 模块时，这些名称会听起来很熟悉。最值得注意的是，这包括将状态称为“隐藏状态”，因此其维度为
    `hidden_size`，尽管状态在特定情况下才对用户隐藏（我们将在后面讨论）。
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*From the way the cell has been defined, we see that to instantiate it, we
    need to pass `hidden_size` and `input_size` – the latter referring to the number
    of features in the dataset. Let’s make those 3 and 1, respectively:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*从细胞定义的方式来看，我们可以看到要实例化它，我们需要传递 `hidden_size` 和 `input_size` – 后者指的是数据集中的特征数量。让我们将它们分别设置为
    3 和 1：'
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*As a quick test, we call the module on a (tiny) batch of data, passing in
    the previous (or: initial) state. As in the actual `torch` implementation, the
    state is initialized to an all-zeros tensor:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*作为一个快速测试，我们在一个（微小的）数据批次上调用该模块，传递先前的（或：初始）状态。与实际的 `torch` 实现一样，状态被初始化为一个全零张量：'
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '*[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE3]'
- en: Note the dimensionality of the output. For each batch item, we get the new state,
    of size `hidden_size`.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 注意输出的维度。对于每个批次项，我们得到一个新状态，大小为 `hidden_size`。
- en: Now, a cell is not normally supposed to be called by the user; instead, we should
    call the to-be-defined `rnn_module()`. That module will take care of the iteration,
    delegating to an instance of `rnn_cell()` at each step. Let’s implement this one
    next.***  ***### 21.2.2 Basic `rnn_module()`
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在通常不应该由用户调用细胞；相反，我们应该调用待定义的 `rnn_module()`。该模块将负责迭代，并在每一步将任务委托给 `rnn_cell()`
    的一个实例。让我们接下来实现这个模块。***  ***### 21.2.2 基本的 `rnn_module()`
- en: Conceptually, this module is easily characterized – it iterates over points
    in time. But there are a few things to note in the implementation that follows.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，这个模块很容易描述——它遍历时间点。但在接下来的实现中，还有一些需要注意的事项。
- en: First, note that it expects a single argument to `forward()`, not two – there
    is no need to pass in an initial state. (In actual `torch` implementations, the
    user can pass an initial state to start from, if they want. But if they don’t,
    the state will start out as all zeros, just like in this prototype.)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，请注意，它期望 `forward()` 接收单个参数，而不是两个——不需要传递初始状态。（在实际的 `torch` 实现中，如果用户想要从某个状态开始，他们可以传递一个初始状态。但如果他们不这样做，状态将从头开始为全零，就像在这个原型中一样。）
- en: Second, and most importantly, let’s talk about the dimensionality of `x`, the
    single input argument. Where the cell operates on tensors of size `batch_size`
    times `num_features`, the iteration module expects its input to have an additional
    dimension, inserted at position two – right “in the middle”. You can see this
    in the line
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，也是最重要的一点，让我们谈谈 `x` 的维度，它是单个输入参数。细胞在 `batch_size` 乘以 `num_features` 大小的张量上操作，迭代模块期望其输入有一个额外的维度，插入在第二个位置——正好“在中间”。您可以在以下行中看到这一点：
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '*This additional dimension is used to capture evolution over time. `rnn_module()`
    will iterate over its values, call `rnn_cell()` for each step in the sequence,
    and keep track of the outputs:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*这个额外的维度用于捕捉时间的演变。`rnn_module()`将遍历其值，对序列中的每个步骤调用`rnn_cell()`，并跟踪输出：'
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '*As you see, in every call to `self$cell`, the previous state is passed as
    well, fulfilling the contract on `rnn_cell$forward()`.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*正如你所见，在每次调用`self$cell`时，都会传递前一个状态，履行`rnn_cell$forward()`的合约。'
- en: 'The complete code for `rnn_module()` is just slightly longer than that for
    the cell:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`rnn_module()`的完整代码比cell的代码稍微长一点：'
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '*Note how dimension two, the one that, in the input, held the time dimension,
    now is used to pack the states obtained for each time step. I’ll say more about
    that in a second, but first, let’s test that module. I’ll stay with a state size
    (`hidden_size`) of three, and make our sample input have four consecutive measurements:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意维度二，在输入中持有时间维度的那个维度，现在用于打包每个时间步长获得的状态。我会在下一部分更多地谈论这一点，但首先，让我们测试一下这个模块。我将保持状态大小（`hidden_size`）为三，并使我们的样本输入有四个连续的测量值：'
- en: '[PRE7]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '*[PRE8]'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE8]'
- en: 'So, `rnn_module()` returns a list of length two. First in the list is a tensor
    containing the states at all time steps – for each batch item, and for each unit
    in the state. At the risk of being redundant, here are its dimensions:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，`rnn_module()`返回一个长度为两个的列表。列表中的第一个是一个tensor，包含所有时间步长的状态——对于每个批次的项，以及每个状态中的单元。冒着重复的风险，这里是其维度：
- en: '[PRE9]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '*[PRE10]'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE10]'
- en: The reason I’m stressing this is that you’ll see the same convention reappear
    in the actual `torch` implementation, and the conventions associated with processing
    time series data can take some time to get accustomed to.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我之所以强调这一点，是因为你将在实际的`torch`实现中看到相同的惯例再次出现，与处理时间序列数据相关的惯例可能需要一些时间来习惯。
- en: 'Now, what about the second tensor? It really is a slice of the first – one
    reflecting the final state only. Correspondingly, its number of dimensions is
    reduced by one:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，关于第二个tensor呢？它确实是第一个的切片——只反映最终状态。相应地，其维度数减少了一个：
- en: '[PRE11]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '*[PRE12]'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE12]'
- en: Now, as users, why would we need that second tensor?
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，作为用户，我们为什么需要第二个tensor呢？
- en: 'We don’t. We can just do the slicing ourselves. Remember how, above, I first
    tried to avoid the term “hidden states”, and said I’d rather just talk about “states”
    instead? This is why: Whether the states *really* are hidden is up to implementation,
    that is, *developer choice*. A framework could decide to return the very last
    state only, unless the caller explicitly asks for the preceding ones. In that
    case, it would make sense to talk about “the” output on the one hand, and the
    sequence of “hidden states”, on the other. We *could* have coded our sample implementation
    like that. Instead, we were following `torch`’s `nn_rnn()`, which you’ll encounter
    in a second.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要。我们可以自己进行切片。记得上面我首先试图避免使用“隐藏状态”这个术语，而是说我更愿意只谈论“状态”吗？这就是原因：状态*是否*真正是隐藏的取决于实现，即*开发者选择*。一个框架可以决定只返回最后一个状态，除非调用者明确要求前一个状态。在这种情况下，谈论“输出”的一方面和“隐藏状态”的序列的另一方面是有意义的。我们*可以*像那样编写我们的示例实现。相反，我们遵循了`torch`的`nn_rnn()`，你将在下一部分遇到。
- en: 'Thus, what I’m saying is: It all is a matter of conventions. But this doesn’t
    explain *why* the `torch` developers chose to return an additional, sliced-to-the-last-time-step,
    tensor: This clearly seems redundant. Is it?'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我想说的是：这一切都是惯例问题。但这并不能解释为什么`torch`开发者选择返回一个额外的、切片到最后一步的tensor：这显然看起来是多余的。是吗？
- en: Well, it often is. *Whether* it is or not depends, for one, on the type of RNN.
    If you use `torch`’s `nn_rnn()` (a “simple” RNN implementation not much employed
    in practice) or `nn_gru()` – creating a default *Gated Recurrent Network*, one
    of two “classical”, tried-and-true architectures – it will be. If, on the other
    hand, you ask `torch` for a setup where a single RNN module really is a composite
    of layers, and/or you use an *LSTM* (*Long Short-Term Memory* Network, the second
    “classic”), then one tensor will not be a subset of the other.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，通常是这样的。*是否*是这样取决于，一方面，RNN的类型。如果你使用`torch`的`nn_rnn()`（一个“简单”的RNN实现，在实践中的应用不多）或`nn_gru()`——创建一个默认的*门控循环神经网络*，两种“经典”、经过验证的架构之一——它将是。另一方面，如果你要求`torch`提供一个设置，其中单个RNN模块实际上是由层组成的复合体，并且/或者你使用*LSTM*（*长短期记忆*网络，第二种“经典”），那么一个tensor将不会是另一个的子集。
- en: At this point, it definitely is time to look at those `torch` modules.*********  ***##
    21.3 Recurrent neural networks in `torch`
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，确实是时候看看那些`torch`模块了。*********  ***## 21.3 `torch`中的循环神经网络
- en: Here, first, is `nn_rnn()`, a more feature-rich, but similar-in-spirit to our
    prototype recurrent module. In practice, you’ll basically always use either `nn_gru()`
    or `nn_lstm()`, which is why we won’t spend much time on it. Especially, we don’t
    talk about optional arguments (yet), with two exceptions.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这里首先介绍的是 `nn_rnn()`，这是一个功能更丰富，但与我们的原型循环模块精神相似的模块。在实践中，你基本上总是使用 `nn_gru()` 或
    `nn_lstm()`，这就是为什么我们不会在这方面花费太多时间。特别是，我们还没有讨论可选参数（尚未），但有两个例外。
- en: '[PRE13]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '*Both `batch_first` and `num_layers` arguments are optional. The latter, `num_layers`,
    allows for creating a stack of RNN modules instead of a single one; this is convenient
    because the user does not have to worry about how to correctly wire them together.
    The default, though, is `1`: exactly what we’re passing in, above. The reason
    I’m specifying it explicitly is just so you know it exists, and aren’t confused
    by the module’s output. Namely, you’ll see that the second in the list of tensors
    returned by `rnn$forward()` has an additional dimension that indicates the layer.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*`batch_first` 和 `num_layers` 参数都是可选的。后者 `num_layers` 允许创建一个 RNN 模块的堆叠，而不是单个模块；这很方便，因为用户不必担心如何正确地将它们连接起来。默认值是
    `1`：正好是我们上面传递的值。我明确指定它的原因只是让你知道它的存在，并且不会因为模块的输出而感到困惑。具体来说，你会发现 `rnn$forward()`
    返回的张量列表中的第二个张量有一个额外的维度，表示层。*'
- en: In contrast, `batch_first` is not set to its default; and it’s essential to
    be aware of this. By default, the convention for RNNs differs from those for other
    modules; if we didn’t pass the argument, `torch` would expect the first dimension
    to be representing time steps, not batch items. In this book, we’ll always pass
    `batch_first = TRUE`.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，`batch_first` 没有设置为默认值；这一点非常重要。默认情况下，RNN 的约定与其他模块不同；如果我们没有传递参数，`torch`
    会期望第一个维度代表时间步，而不是批量项。在这本书中，我们总是传递 `batch_first = TRUE`。
- en: 'Now, calling that RNN on the same test tensor we used in our manual implementation
    above, and checking the dimensions of the output, we see:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，调用我们在上面的手动实现中使用的相同测试张量，并检查输出维度，我们看到：
- en: '[PRE14]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '*[PRE15]'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE15]'
- en: The first tensor in the list has a shape that exactly matches what would be
    returned by our manual implementation.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 列表中的第一个张量的形状与我们手动实现返回的完全匹配。
- en: 'Semantically, the respective second tensors in the output lists match up as
    well, in that both of them zoom in on the final state. But `torch`, allowing for
    the chaining of several RNNs in a single module, returns the final state *per
    layer*. In the `torch` implementation, is that second tensor redundant? It is,
    in our example. But were we to create a multi-layer RNN, it would give us information
    not contained in the first tensor: namely, the last hidden state for each non-final
    layer.**  **## 21.4 RNNs in practice: GRU and LSTM'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 从语义上讲，输出列表中的相应第二个张量也匹配得很好，因为它们都聚焦于最终状态。但是，`torch` 允许在一个模块中链式调用多个 RNN，它返回的是每个层的最终状态。在
    `torch` 的实现中，第二个张量是多余的。在我们的例子中确实是。但是，如果我们创建一个多层 RNN，它将提供第一个张量中不包含的信息：即每个非最终层的最后一个隐藏状态。**  **
- en: Basic recurrent networks, as created by `nn_rnn()`, are nice for explanatory
    purposes, but hardly ever used in practice. The reason is that when you back-propagate
    through a long recurrence structure, gradients are likely to either “die” or get
    out of bounds. These are the so-called “vanishing gradient” and “exploding gradient”
    problems, respectively.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 基本循环神经网络，如 `nn_rnn()` 创建的，对于解释目的来说很好，但在实际应用中几乎从未使用过。原因是当你通过一个长的循环结构进行反向传播时，梯度很可能会“消失”或超出范围。这些被称为“梯度消失”和“梯度爆炸”问题。
- en: Already three decades before the compute- and big-data accelerated “era of deep
    learning”, an algorithmic solution had been found. *Long Short-Term Memory Networks*,
    described in Hochreiter and Schmidhuber ([1997](references.html#ref-hochreiter1997long)),
    enabled training on reasonably long sequences by introducing so-called *gates*
    that act as filters in various places of the state-threading calculation. *Gated
    Recurrent Units*, put forward (much more recently) in Cho et al. ([2014](references.html#ref-ChoMGBSB14)),
    are similar in spirit, but a bit simpler. Together, both architectures dominate
    the space.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算和大数据加速的“深度学习时代”三十年前，就已经找到了一个算法解决方案。Hochreiter 和 Schmidhuber 在 [1997](references.html#ref-hochreiter1997long)
    中描述的 *长短期记忆网络*，通过引入所谓的 *门* 作为状态线程计算中各个位置的过滤器，使得在合理长的序列上进行训练成为可能。Cho 等人（[2014](references.html#ref-ChoMGBSB14)）提出的
    *门控循环单元*，在精神上相似，但稍微简单一些。这两个架构共同占据了主导地位。
- en: 'With these models introducing additional logic, we see how the division-of-labor
    strategy introduced above is useful: Iteration and state threading are taken care
    of by different modules. This means that, in principle, we could design our own
    LSTM or GRU *cell*, and then, iterate over it in the same fashion as before. Of
    course, there is no need to re-implement existing functionality. But following
    the same modularization approach, we can nicely experiment with variations to
    the processing logic if we want.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型引入了额外的逻辑，我们看到了上面提到的分工策略是如何有用的：迭代和状态线程由不同的模块处理。这意味着，原则上，我们可以设计自己的 LSTM 或
    GRU *单元*，然后以同样的方式迭代它。当然，没有必要重新实现现有功能。但遵循相同的模块化方法，如果我们想的话，可以很好地对处理逻辑进行变体实验。
- en: Now, let’s see what is returned by `nn_gru()` and `nn_lstm()`, the constructors
    corresponding to the aforementioned architectures. At this point, I should quickly
    comment on optional arguments I haven’t mentioned before.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看 `nn_gru()` 和 `nn_lstm()` 返回的内容，这两个构造函数对应于上述架构。在这个时候，我应该快速评论一下之前没有提到的可选参数。
- en: In general, the argument list is the same for `nn_rnn()`, `nn_gru()`, and `nn_lstm()`.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，`nn_rnn()`, `nn_gru()` 和 `nn_lstm()` 的参数列表是相同的。
- en: We have to indicate the number of features (`input_size`) and the size of the
    state (`hidden_size`); we deliberately pass in `batch_first = TRUE;` we can have
    `torch` chain several RNNs together using `num_layers`. In case we *do* want to
    stack layers, we can drop out a fraction of interconnections using, well, `dropout`.
    Finally, there is `bidirectional`. By default, this argument is set to `FALSE`,
    meaning we’re passing through the sequence chronologically. With `bidirectional
    = TRUE`, there is an additional pass in reverse order, and weights from both passes
    are combined. Essentially, what we’re doing is predicting present from past as
    well as past from present. This may sound like “cheating”, but really, is not;
    it’s just making optimal use of dependencies in past data.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须指明特征的数量（`input_size`）和状态的大小（`hidden_size`）；我们故意传递 `batch_first = TRUE;`
    我们可以使用 `num_layers` 将多个 RNN 连接起来。如果我们确实想堆叠层，我们可以使用 `dropout` 来丢弃一部分互连。最后，有 `bidirectional`。默认情况下，此参数设置为
    `FALSE`，这意味着我们按时间顺序传递序列。如果 `bidirectional = TRUE`，则还有一个反向顺序的额外传递，并且将两个传递的权重组合起来。本质上，我们正在从过去预测现在，以及从现在预测过去。这听起来可能像是“作弊”，但实际上并不是；这只是充分利用了过去数据中的依赖关系。
- en: To keep our examples in sync, I’ll now instantiate the GRU and LSTM modules
    in the same way as `nn_rnn()` above, making use of single layer and a single direction.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持我们的示例同步，我现在将以与上面 `nn_rnn()` 相同的方式实例化 GRU 和 LSTM 模块，使用单层和单方向。
- en: 'First, a GRU:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，一个 GRU：
- en: '[PRE16]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '*[PRE17]'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE17]'
- en: As you see, dimension-wise, the output returned from a GRU is analogous to that
    of a simple RNN.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，GRU 返回的输出在维度上与简单 RNN 的输出类似。
- en: 'With LSTM, however, we see a difference:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于 LSTM，我们看到了一些不同：
- en: '[PRE18]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '*[PRE19]'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE19]'
- en: Instead of two, we now have three tensors. The first and second are no different
    from what we’ve seen so far; meaning, the second is as redundant as for a GRU
    or a simple RNN. (At least when there’s just a single layer). What about the third?
    Shape-wise, it looks like the second, the one we know returns the “hidden state”.
    In fact, it reflects an *additional* state, one not present in a GRU. And this
    one – often called *cell* state – *really* is available to the user only for the
    last time step, even for single-layer LSTMs.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有三个张量，而不是两个。第一个和第二个与我们之前看到的不同；也就是说，第二个与 GRU 或简单 RNN 一样冗余。（至少当只有单层时）。第三个呢？从形状上看，它看起来像第二个，我们已知它返回“隐藏状态”。实际上，它反映了一个*额外*的状态，一个在
    GRU 中不存在的状态。而且这个状态——通常被称为*单元*状态——对于用户来说，仅在最后一个时间步才真正可用，即使是单层 LSTM。
- en: You could say that with LSTMs, some hidden states are more hidden than others.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以说，在 LSTM 中，一些隐藏状态比其他状态更隐蔽。
- en: Now that we’ve gained some familiarity with `torch`’s RNN-related conventions,
    we look at an actual time series application.**  **## 21.5 Forecasting electricity
    demand
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对 `torch` 的 RNN 相关约定有了更多的了解，我们来看一个实际的时间序列应用。**  **## 21.5 预测电力需求
- en: Our example time series, called `vic_elec`, is available from package `tsibbledata`.
    It reflects aggregated electricity demand for Victoria, Australia, measured in
    half-hour intervals. Additional features (which we won’t use here) include temperature
    and a holiday indicator. The dataset spans three years, ranging from January,
    2012 to December, 2014.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的示例时间序列，称为`vic_elec`，可以从`tsibbledata`包中获取。它反映了澳大利亚维多利亚州半小时间隔的电力需求总和。还包括一些额外的特征（我们在这里不会使用），如温度和假日指示器。数据集覆盖了三年，从2012年1月到2014年12月。
- en: Before we start, we need to define our task. In fact, there’ll be two of them.
    In both, we’ll attempt to predict future temperature based on past measurements.
    First, we’ll see how to predict the very next measurement; in terms of measurement
    intervals, that’s a single time step ahead. Then, we’ll modify the code to allow
    for forecasting several time steps in advance.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，我们需要定义我们的任务。实际上，会有两个任务。在这两个任务中，我们将尝试根据过去的测量值预测未来的温度。首先，我们将看到如何预测下一个测量值；从测量间隔的角度来看，这是向前一步。然后，我们将修改代码以允许预测几个时间步长。
- en: 21.5.1 Data inspection
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 21.5.1 数据检查
- en: The dataset being part of an ecosystem of packages dedicated to time series
    analysis, there is not much to be done in terms of pre-processing. However, as
    always or even more so, it is worth our while to take time for data exploration.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据集是时间序列分析包生态系统的一部分，因此在预处理方面没有太多要做。然而，正如往常一样，甚至更加如此，花时间进行数据探索是值得的。
- en: '[PRE20]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '*[PRE21]'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE21]'
- en: 'Concretely, we’ll want to know what kinds of periodicities there are in the
    data. Conveniently, we can obtain a decomposition into trend, various seasonal
    components, and a remainder using `feasts::STL()`. Here’s what we see for a single
    year ([fig. 21.1](#fig-timeseries-vic-elec-stl)):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们想知道数据中存在哪些类型的周期性。方便的是，我们可以使用`feasts::STL()`将数据分解为趋势、各种季节性成分和剩余部分。以下是针对一年数据的情况（[图21.1](#fig-timeseries-vic-elec-stl)）：
- en: '[PRE22]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '*![Five rows, displaying different components of a time series. Row 1: Original
    data. Row 2: Trend; irregular. Row 3: Weekly pattern, strongly periodic. Row 4:
    Variation over the day; strongly periodic. Row 5: Half-hourly variation; irrelevant.
    Row 5: Reminder; irregular.](../Images/4d70b91f2c834958a5c53c6907595138.png)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*![五行，显示时间序列的不同组成部分。第1行：原始数据。第2行：趋势；不规则。第3行：周模式，周期性强。第4行：日变化；周期性强。第5行：半小时变化；无关紧要。第5行：提醒；不规则。](../Images/4d70b91f2c834958a5c53c6907595138.png)'
- en: 'Figure 21.1: One year of electricity demand, decomposed into trend, seasonal
    components, and remainder.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图21.1：一年电需求的分解，包括趋势、季节性成分和剩余部分。
- en: 'In this plot, the scale bar on the left immediately signals a component’s importance:
    the smaller the bar, the more dominant the effect. Not surprisingly, day of week
    matters; so does time of day.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图表中，左侧的刻度条立即表明了一个成分的重要性：刻度条越小，影响越显著。不出所料，星期几和时间对需求有影响。
- en: For greater granularity, we zoom in on a single month ([fig. 21.2](#fig-timeseries-vic-elec-stl-month)).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得更高的粒度，我们聚焦于单一月份（[图21.2](#fig-timeseries-vic-elec-stl-month)）。
- en: '[PRE23]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '*![Five rows, displaying different components of a time series. Row 1: Original
    data. Row 2: Trend; shows decline over the first quarter of the month, then rises
    and stays on a high level, then starts falling again. Row 3: Weekly pattern, strongly
    periodic. Row 4: Variation over the day; strongly periodic. Row 5: Half-hourly
    variation; irrelevant. Row 5: Reminder; irregular.](../Images/5b10c00e64e580bcad315685b0ea95ac.png)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*![五行，显示时间序列的不同组成部分。第1行：原始数据。第2行：趋势；显示该月的前三个月呈下降趋势，然后上升并保持在较高水平，然后又开始下降。第3行：周模式，周期性强。第4行：日变化；周期性强。第5行：半小时变化；无关紧要。第5行：提醒；不规则。](../Images/5b10c00e64e580bcad315685b0ea95ac.png)'
- en: 'Figure 21.2: A single month of electricity demand, decomposed into trend, seasonal
    components, and remainder.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图21.2：单一月份电需求的分解，包括趋势、季节性成分和剩余部分。
- en: Here, I’ve picked January, right in the hot Australian summer. Demand for electricity,
    then, arises as a need for cooling, not heating. We clearly see how it’s highest
    around noon, and lowest during the night. Every week, it peaks on Mondays and
    Tuesdays, declines on Wednesdays, and is more or less stable in the second half
    of the week.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我选择了1月，正值炎热的澳大利亚夏季。因此，对电力的需求是由于冷却而不是加热。我们清楚地看到需求在中午最高，在夜间最低。每周，需求在周一和周二达到峰值，在周三下降，在周的后半段基本稳定。
- en: Now, while those two periodicities are important, the half-hourly rhythm clearly
    isn’t. For training the network, I’ll thus aggregate pairs of adjacent values,
    reducing the number of measurements to a half. Since now, changes between consecutive
    values are bigger, this also makes the task harder.***  ***### 21.5.2 Forecasting
    the very next value
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，虽然这两个周期很重要，但半小时的节奏显然不是。因此，为了训练网络，我将相邻值对进行聚合，将测量值数量减半。由于现在连续值之间的变化更大，这也使得任务更难。***  ***###
    21.5.2 预测下一个值
- en: 'We start with the more modest goal: predicting the very next data point. First
    of all, we need a custom `torch::dataset()`.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从更 modest 的目标开始：预测下一个数据点。首先，我们需要一个定制的 `torch::dataset()`。
- en: 21.5.2.1 A `dataset()` for single-step prediction
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 21.5.2.1 用于单步预测的 `dataset()`
- en: 'In our demonstration of how `torch` RNNs work, we made use of a toy data tensor
    that looked like this: `torch_randn(2, 4, 1)`.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们展示 `torch` RNNs 如何工作的演示中，我们使用了看起来像这样的玩具数据张量：`torch_randn(2, 4, 1)`。
- en: Here, the first dimension corresponded to batch items; the second, to time steps;
    and the third, to features. The batch dimension will be taken care of by the `dataloader()`.
    Thus, for the `dataset()`, the task to return a single predictor reduces to picking
    just as many consecutive measurements as desired, and stacking them in the first
    dimension. The corresponding target, in this first setup, should be the single
    measurement right after the last step in the predictor sequence.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，第一个维度对应于批处理项目；第二个，对应于时间步；第三个，对应于特征。批处理维度将由 `dataloader()` 处理。因此，对于 `dataset()`，返回单个预测器的任务简化为只选择所需数量的连续测量值，并在第一个维度中堆叠它们。在这个第一个设置中，相应的目标应该是预测器序列中最后一步之后的单个测量值。
- en: 'So in principle, `.getitem()`, when asked for the item at position `i`, should
    return the following:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在原则上，`.getitem()` 当被要求返回位置 `i` 的项目时，应该返回以下内容：
- en: '[PRE24]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '*The code you’ll find below is a variation on this theme. Depending on the
    time series in question, stacking consecutive measurements like this may result
    in individual batches looking hardly any different from each other. To avoid redundancy,
    we could do the following: Instead of iterating over the series in order, we take
    samples. At construction time, the `torch::dataset()` is told what fraction of
    the data we’d like to be served in each epoch, and prepares a set of indices.
    Then at runtime, it iterates over those indices. In the present case, sampling
    is not really needed, since anyway we’ve decided on hourly aggregation, and the
    time series itself isn’t excessively long to start with. The reason I’m showing
    this technique is that you might find it useful in other applications.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*下面找到的代码是这一主题的变体。根据所涉及的时间序列，连续堆叠测量值可能会使得各个批次看起来几乎没有什么区别。为了避免冗余，我们可以这样做：不是按顺序遍历序列，而是取样本。在构建时，`torch::dataset()`
    被告知我们希望在每次epoch中提供的数据比例，并准备一组索引。然后在运行时，它遍历这些索引。在当前情况下，采样实际上并不需要，因为我们已经决定按小时聚合，而且时间序列本身一开始就不算特别长。我展示这个技术的原因是，你可能在其他应用中找到它有用。'
- en: Now, take a look at how we initialize the `dataset()`. If it is supposed to
    stack consecutive measurements, it has to be told the desired number of time steps.
    For easy experimentation, we make that parameter (called `n_timesteps`) a constructor
    argument.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，看看我们是如何初始化 `dataset()` 的。如果它应该堆叠连续的测量值，它必须被告知所需的时间步数。为了便于实验，我们将该参数（称为 `n_timesteps`）作为构造函数的参数。
- en: 'Here is the complete `dataset()` code:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是完整的 `dataset()` 代码：
- en: '[PRE25]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '*Given that `vic_elec` holds three years of data, a by-year split into training,
    validation, and test sets seems to suggest itself. Here, we perform the split,
    right after aggregating by full hour:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*鉴于 `vic_elec` 包含三年的数据，按年将数据分为训练集、验证集和测试集似乎是一个合理的选择。在这里，我们在按完整小时聚合后立即进行分割：'
- en: '[PRE26]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '*In the `dataset()` definition, you may have noticed the use of `train_mean`
    and `train_sd` to standardize the data. Time to compute them:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '*在 `dataset()` 定义中，你可能已经注意到了使用 `train_mean` 和 `train_sd` 来标准化数据。现在是计算它们的时候了：'
- en: '[PRE27]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '*Now, we’re all set to construct the `dataset()` objects – but for one decision:
    What is a good value for `n_timesteps`? As with every hyper-parameter, in the
    end, nothing can beat experimentation. But based on data exploration, we can establish
    a lower bound: We definitely want to capture variation over day *and* week. With
    the hourly-aggregated data, this gives us a minimum length of 168:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*现在，我们已经准备好构建`dataset()`对象——但有一个决定：`n_timesteps`的值是多少才算合适？与每个超参数一样，最终没有什么能胜过实验。但基于数据探索，我们可以确定一个下限：我们肯定想要捕捉到日和周的变化。对于按小时汇总的数据，这给我们一个最小长度为168：'
- en: '[PRE28]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '*Now, we instantiate the `dataset()` objects, and immediately check tensor
    shapes:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '*现在，我们实例化`dataset()`对象，并立即检查张量形状：'
- en: '[PRE29]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '*[PRE30]'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE30]'
- en: 'Next, the respective `dataloader()`s:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，相应的`dataloader()`：
- en: '[PRE31]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '*[PRE32]'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE32]'
- en: Once we’re dealing with batches we *do* see the required three-dimensional structure
    of the input tensor.*******  ***#### 21.5.2.2 Model
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们处理批量数据，我们确实会看到输入张量所需的三维结构。*******  ***#### 21.5.2.2 模型
- en: 'In the model, the main workhorse is the LSTM. The number of LSTM-internal layers
    is configurable, the default being set to one. Its final output (or: “hidden state”;
    more on that below) is passed on to a linear module that outputs a single prediction.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型中，主要的工作马是LSTM。LSTM内部层的数量是可配置的，默认设置为1。它的最终输出（或：“隐藏状态”；下面会详细说明）被传递到线性模块，该模块输出一个单一预测。
- en: '[PRE33]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '*Note two things about the top-level module’s definition. First, its constructor
    accepts two dropout-related arguments. The first, `dropout`, specifies the fraction
    of elements to be zeroed out on the path between LSTM and linear module; the second,
    `rec_dropout`, gets passed on to the LSTM, to be made use of between individual
    layers. (If there is just a single layer, this parameter has to equal zero, its
    default.)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意顶层模块定义的两个要点。首先，它的构造函数接受两个与dropout相关的参数。第一个参数`dropout`指定了在LSTM和线性模块之间路径上要置零的元素比例；第二个参数`rec_dropout`被传递给LSTM，用于在各个层之间使用。（如果只有一个层，则此参数必须等于零，这是默认值。）'
- en: Secondly, in `forward()`, I’m showing two equivalent ways to pass output from
    the LSTM to the linear module. We can either ask `torch` for “the output”, and
    then, take the subset of values corresponding to the last time step only; alternatively,
    we can extract the “hidden state” tensor and zoom in on the final layer. Of course,
    since whatever choice we make is fine, there is no need to complicate the decision;
    I merely wanted to take the occasion to, one final time, illustrate how to “talk
    RNN” with `torch`.*  *#### 21.5.2.3 Training
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，在`forward()`中，我展示了两种将LSTM输出传递到线性模块的等效方式。我们可以要求`torch`提供“输出”，然后只取对应于最后一个时间步的值子集；或者，我们可以提取“隐藏状态”张量并聚焦于最后一层。当然，由于我们做出的任何选择都是可行的，因此没有必要使决策复杂化；我仅仅想借此机会再次说明如何使用`torch`“谈论RNN”。*  *####
    21.5.2.3 训练
- en: Like so often, we start the training process by running the learning rate finder.
    The model we’re training is modest in size (12,928 parameters overall!), but powerful
    – not in the least due to the stacking of LSTM layers.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如同往常，我们通过运行学习率查找器来开始训练过程。我们正在训练的模型在大小上相当适中（总共有12,928个参数！），但功能强大——这不仅仅是因为LSTM层的堆叠。
- en: '[PRE34]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '*![A curve that, from left to right, first slowly declines (until about x=0.3),
    then begins to rise sharply, exhibiting high variability.](../Images/108e37627efc127ea471238f628b6710.png)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '*![一条曲线，从左到右，首先缓慢下降（直到大约x=0.3），然后开始急剧上升，表现出高度可变性。](../Images/108e37627efc127ea471238f628b6710.png)'
- en: 'Figure 21.3: Learning rate finder output for the one-step-ahead-forecast model.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图21.3：一步预测模型的学习率查找器输出。
- en: 'From the plot ([fig. 21.3](#fig-timeseries-vic-elec-lr-finder)), a maximal
    learning rate of 0.1 seems like a sensible choice, at least when combined with
    the one-cycle learning rate scheduler. Indeed, we see fast progress ([fig. 21.4](#fig-timeseries-vic-elec-fit)):'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 从图([图21.3](#fig-timeseries-vic-elec-lr-finder))中可以看出，最大学习率0.1似乎是一个明智的选择，至少当与单周期学习率调度器结合使用时。确实，我们看到快速进展([图21.4](#fig-timeseries-vic-elec-fit))：
- en: '[PRE35]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '*[PRE36]'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE36]'
- en: '![Curves representing training and validation loss. The training curve falls
    sharply, in just one epoch. Thereafter, it keeps descending, but slowly. The validation
    curve looks similar, apart from not showing the initial decline.](../Images/5178ebebc355417d8f1162bcdb1ee625.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![表示训练和验证损失的曲线。训练曲线在仅一个epoch内急剧下降。之后，它继续缓慢下降。验证曲线看起来相似，除了没有显示初始下降。](../Images/5178ebebc355417d8f1162bcdb1ee625.png)'
- en: 'Figure 21.4: Fitting a one-step forecast model on `vic_elec`.**  **#### 21.5.2.4
    Inspecting predictions'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图21.4：在`vic_elec`上拟合一步预测模型。**  **#### 21.5.2.4 检查预测
- en: 'Using `luz::evaluate()`, we can check whether performance on the test set approximately
    matches that on the validation set. Here is mean squared error, as obtained from
    test-set predictions:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`luz::evaluate()`，我们可以检查测试集上的性能是否大致与验证集上的性能相匹配。以下是来自测试集预测的平均平方误差：
- en: '[PRE37]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '*[PRE38]'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE38]'
- en: But really, what does this value tell us? We need to actually *look* at forecasts
    to decide whether predictions are any good.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 但实际上，这个值告诉我们什么？我们需要真正地*查看*预测来决定预测是否有效。
- en: 'For visualization, let’s choose a subset of the test data – the final month,
    say. We obtain predictions, and display them together with the actual measurements
    ([fig. 21.5](#fig-timeseries-vic-elec-preds)):'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化，让我们选择测试数据的一个子集——比如说最后一个月。我们获得预测，并将它们与实际测量值一起显示([图21.5](#fig-timeseries-vic-elec-preds))：
- en: '[PRE39]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '*![Hourly temperatures during the test period, plus forecasts obtained for
    weeks two to four. Forecasts hardly deviate from actual measurements.](../Images/b5d9eddfb04ea0a5d4ca6f55d79e50e5.png)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '*![测试期间每小时温度，以及第二周到第四周获得的预测。预测几乎不偏离实际测量值。](../Images/b5d9eddfb04ea0a5d4ca6f55d79e50e5.png)'
- en: 'Figure 21.5: One-step ahead forecast on the last month of test set.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图21.5：测试集最后一个月的一步预测。
- en: Predictions look good! However, we know this was the simpler task of the two.********  ***###
    21.5.3 Forecasting multiple time steps ahead
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 预测看起来不错！然而，我们知道这只是一个更简单的任务。********  ***### 21.5.3 预测多个时间步
- en: In reality, being able to predict a single measurement may not necessarily be
    of great use. Fortunately, not much adaptation is required to obtain multiple
    predictions from this type of model. In principle, we just have to modify the
    output size of the final linear layer. For performance reasons, we’ll go a bit
    further, but it’ll still be a moderate change.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实中，能够预测单个测量值可能不一定有很大用处。幸运的是，从这种类型的模型中获得多个预测不需要太多调整。原则上，我们只需要修改最终线性层的输出大小。出于性能考虑，我们将进行一些调整，但这仍然是一个适度的变化。
- en: Since the target now is a sequence, we also have to adapt the `dataset()`.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 由于目标是序列，我们也必须调整`dataset()`。
- en: 21.5.3.1 `Dataset()` adaptation
  id: totrans-146
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 21.5.3.1 `Dataset()`适应
- en: 'Just like the number of consecutive measurements in an input tensor, that of
    the target tensors should be configurable. We therefore introduce a new parameter,
    `n_forecast`, that indicates the desired length:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 就像输入张量中连续测量的数量一样，目标张量的数量也应该可配置。因此，我们引入了一个新的参数，`n_forecast`，它表示期望的长度：
- en: '[PRE40]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '*We’ll attempt to forecast demand for the subsequent seven days. Thus, `n_timesteps`
    and `n_forecast` are equal; but that’s a coincidence, not a requirement.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们将尝试预测未来七天的需求。因此，`n_timesteps`和`n_forecast`相等；但这只是一个巧合，并不是必需的。'
- en: '[PRE41]*  **#### 21.5.3.2 Model adaptation'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE41]*  **#### 21.5.3.2 模型适应'
- en: In the model, we replace the final linear layer by a tiny multi-layer perceptron.
    It will return `n_forecast` predictions for each time step.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型中，我们将最后的线性层替换为一个微小的多层感知器。它将为每个时间步返回`n_forecast`个预测。
- en: '[PRE42]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '*#### 21.5.3.3 Training'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '*#### 21.5.3.3 训练'
- en: 'Not surprisingly, the training process is unaltered. It again starts with running
    the learning rate finder ([fig. 21.6](#fig-vic-elec-lr-finder-mlp)):'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 毫不奇怪，训练过程没有改变。它再次从运行学习率查找器([图21.6](#fig-vic-elec-lr-finder-mlp))开始：
- en: '[PRE43]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '*![A highly fluctuating curve that, from left to right, first stays constant,
    then declines (until about x=0.05), then begins to rise sharply, exhibiting high
    variability.](../Images/c8812f8a583a6eb4bc2be80c6e6cddee.png)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '*![一条高度波动的曲线，从左到右，首先保持不变，然后下降（直到大约 x=0.05），然后开始急剧上升，表现出高度的可变性。](../Images/c8812f8a583a6eb4bc2be80c6e6cddee.png)'
- en: 'Figure 21.6: Learning rate finder output for multiple-step prediction.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图21.6：多步预测的学习率查找器输出。
- en: Based on that result, I’ll go with a lower maximal rate this time.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这个结果，这次我将选择一个较低的最大速率。
- en: '[PRE44]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '*[PRE45]'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE45]'
- en: '![Curves representing training and validation loss. The training curve falls
    sharply, in very few epochs. Thereafter, it keeps descending, but less fast, until
    it slowly flattens. The validation curve looks similar.](../Images/537430eafe36d2077c98fb8d3fea4208.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![表示训练和验证损失的曲线。训练曲线急剧下降，在非常少的时期内。之后，它继续下降，但速度较慢，直到它逐渐变平。验证曲线看起来类似。](../Images/537430eafe36d2077c98fb8d3fea4208.png)'
- en: 'Figure 21.7: Fitting a multiple-step forecast model on vic_elec.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图21.7：在`vic_elec`上拟合多步预测模型。
- en: The loss curves ([fig. 21.7](#fig-timeseries-vic-elec-fit-mlp)) look decent;
    how about actual forecasts?**  **#### 21.5.3.4 Inspecting predictions
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 损失曲线([图21.7](#fig-timeseries-vic-elec-fit-mlp))看起来不错；那么实际的预测呢？**  **#### 21.5.3.4
    检查预测
- en: As before, let’s first formally check that performance on the test set does
    not differ too much from that on the validation set.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们首先正式检查测试集上的性能是否与验证集上的性能差异不大。
- en: '[PRE46]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '*[PRE47]'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE47]'
- en: For visualization, we again look at the very last month only.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化，我们再次只查看最后一个月。
- en: '[PRE48]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '*With each forecast now covering a week of measurements, we can’t display them
    all in the same plot. What we *can* do is pick a few sample indices, nicely spread
    out over the month, and plot those ([fig. 21.8](#fig-timeseries-vic-elec-preds-mlp)):'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '*现在每个预报覆盖一周的测量数据，我们无法在同一个图表中显示所有数据。我们可以做的是选择几个样本索引，这些索引在一个月内分布得很好，并绘制这些数据([图21.8](#fig-timeseries-vic-elec-preds-mlp))：'
- en: '[PRE49]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '*![Hourly temperatures during the test period, plus forecasts obtained for
    weeks two to four. Forecasts are of varying quality.](../Images/2d34646d10aeca02625f3530497c129e.png)'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '*![测试期间每小时温度，以及第二周到第四周的预报。预报质量各异。](../Images/2d34646d10aeca02625f3530497c129e.png)'
- en: 'Figure 21.8: A sample of week-long forecasts on the last month of test set.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图21.8：测试集最后一个月一周长预报的样本。
- en: 'It is instructive to take a closer look. The daily and weekly rhythms are present;
    very much so in fact. What’s harder to devine, from the model’s point of view,
    are the mini-trends we see developing over the month. Without any external “cues”,
    how should it know that next week, demand for electricity will raise over (or
    fall below) the current level? To significantly improve predictions, we would
    have to incorporate additional data – for example, temperature forecasts. Since
    no book can cover everything, we won’t pursue that topic here; instead, we’ll
    move on to our last deep learning application: classifying speech utterances.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细观察是有益的。每日和每周的节奏都存在；实际上非常明显。但从模型的角度来看，更难以推断的是我们在一个月内看到的微型趋势。没有任何外部“线索”，它如何知道下周，电力需求将上升（或下降）到当前水平？为了显著提高预测准确性，我们必须纳入额外的数据——例如，温度预报。由于没有一本书能涵盖所有内容，我们在这里不会深入探讨这个话题；相反，我们将继续探讨我们的最后一个深度学习应用：语音话语的分类。
- en: 'Cho, Kyunghyun, Bart van Merrienboer, Çaglar Gülçehre, Fethi Bougares, Holger
    Schwenk, and Yoshua Bengio. 2014\. “Learning Phrase Representations Using RNN
    Encoder-Decoder for Statistical Machine Translation.” *CoRR* abs/1406.1078\. [http://arxiv.org/abs/1406.1078](http://arxiv.org/abs/1406.1078).Hochreiter,
    Sepp, and Jürgen Schmidhuber. 1997\. “Long Short-Term Memory.” *Neural Computation*
    9 (8): 1735–80.*********************'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 'Cho, Kyunghyun, Bart van Merrienboer, Çaglar Gülçehre, Fethi Bougares, Holger
    Schwenk, 和 Yoshua Bengio. 2014\. “使用RNN编码器-解码器学习短语表示用于统计机器翻译。” *CoRR* abs/1406.1078\.
    [http://arxiv.org/abs/1406.1078](http://arxiv.org/abs/1406.1078).Hochreiter, Sepp,
    和 Jürgen Schmidhuber. 1997\. “长短期记忆。” *Neural Computation* 9 (8): 1735–80.*********************'
