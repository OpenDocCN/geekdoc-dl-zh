- en: How to Scale Your Model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何扩展您的模型
- en: 原文：[https://jax-ml.github.io/scaling-book/index](https://jax-ml.github.io/scaling-book/index)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://jax-ml.github.io/scaling-book/index](https://jax-ml.github.io/scaling-book/index)
- en: '<d-title>A Systems View of LLMs on TPUs (Part 0: Intro | [Part 1: Rooflines](roofline))'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: <d-title>TPU上LLMs的系统视角（第0部分：简介 | [第1部分：屋顶线](roofline))
- en: 'Training LLMs often feels like alchemy, but understanding and optimizing the
    performance of your models doesn''t have to. This book aims to demystify the science
    of scaling language models: how TPUs (and GPUs) work and how they communicate
    with each other, how LLMs run on real hardware, and how to parallelize your models
    during training and inference so they run efficiently at massive scale. If you''ve
    ever wondered “how expensive should this LLM be to train” or “how much memory
    do I need to serve this model myself” or “what''s an AllGather”, we hope this
    will be useful to you.</d-title>  <d-byline><d-article><d-contents>### Contents'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 训练LLMs常常感觉像炼金术，但理解和优化模型性能不必如此。本书旨在揭示扩展语言模型科学的神秘面纱：TPUs（以及GPU）是如何工作的，它们是如何相互通信的，LLMs如何在真实硬件上运行，以及如何在训练和推理期间并行化您的模型，以便在大规模上高效运行。如果您曾想过“这个LLM的训练成本应该是多少”或“我需要多少内存来自己服务这个模型”或“什么是AllGather”，我们希望这将对您有所帮助。</d-title>  <d-byline><d-article><d-contents>###
    内容
- en: '[High-Level Outline](#high-level-outline)[Links to Sections](#links-to-sections)</d-contents>
    <picture>![](../Images/a1e35cb5a526166ac5d2582b612b3375.png)</picture>'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[高级概要](#high-level-outline)[章节链接](#links-to-sections)</d-contents> <picture>![](../Images/a1e35cb5a526166ac5d2582b612b3375.png)</picture>'
- en: 'Much of deep learning still boils down to a kind of black magic, but optimizing
    the performance of your models doesn’t have to — even at huge scale! Relatively
    simple principles apply everywhere — from dealing with a single accelerator to
    tens of thousands — and understanding them lets you do many useful things:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的大部分内容仍然归结为一种黑魔法，但优化模型性能不必如此——即使是在巨大规模上！相对简单的原则适用于所有地方——从处理单个加速器到数万个——理解它们让您可以做许多有用的事情：
- en: Ballpark how close parts of your model are to their theoretical optimum.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大致估计模型的部分与理论最优值之间的接近程度。
- en: Make informed choices about different parallelism schemes at different scales
    (how you split the computation across multiple devices).
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在不同规模上做出关于不同并行方案的有信息的选择（如何在多个设备之间分配计算）。
- en: Estimate the cost and time required to train and run large Transformer models.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 估算训练和运行大型Transformer模型所需的时间和成本。
- en: Design algorithms that take advantage of [specific](https://arxiv.org/abs/2205.14135)
    [hardware](https://arxiv.org/abs/1911.02150) [affordances](https://arxiv.org/abs/2007.00072).
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计利用[特定](https://arxiv.org/abs/2205.14135) [硬件](https://arxiv.org/abs/1911.02150)
    [特性](https://arxiv.org/abs/2007.00072)的算法。
- en: Design hardware driven by an explicit understanding of what limits current algorithm
    performance.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计由对当前算法性能限制有明确理解的硬件驱动。
- en: '**Expected background:** We’re going to assume you have a basic understanding
    of LLMs and the Transformer architecture but not necessarily how they operate
    at scale. You should know the basics of LLM training and ideally have some basic
    familiarity with JAX. Some useful background reading might include [this blog
    post](https://jalammar.github.io/illustrated-transformer/) on the Transformer
    architecture and [the original Transformer paper](https://arxiv.org/abs/1706.03762).
    Also check [this list](conclusion#further-reading) out for more useful concurrent
    and future reading.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**预期背景：** 我们假设您对LLMs和Transformer架构有基本的了解，但并不一定了解它们如何在大规模上运行。您应该了解LLMs训练的基本知识，并且最好对JAX有一些基本的熟悉度。一些有用的背景阅读可能包括关于Transformer架构的[这篇博客文章](https://jalammar.github.io/illustrated-transformer/)和[原始的Transformer论文](https://arxiv.org/abs/1706.03762)。还可以查看[这个列表](conclusion#further-reading)以获取更多有用的相关和未来阅读材料。'
- en: '**Goals & Feedback:** By the end, you should feel comfortable estimating the
    best parallelism scheme for a Transformer model on a given hardware platform,
    and roughly how long training and inference should take. If you don’t, email us
    or leave a comment! We’d love to know how we could make this clearer.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**目标与反馈：** 到最后，您应该能够舒适地估计给定硬件平台上Transformer模型的最佳并行方案，以及大致的训练和推理所需时间。如果您觉得还不够，请给我们发邮件或留下评论！我们很乐意知道我们如何使这更清晰。'
- en: You might also enjoy reading the new [Section 12](gpus) on NVIDIA GPUs!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能还会喜欢阅读关于NVIDIA GPU的新[第12节](gpus)！
- en: Why should you care?
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么你应该关心？
- en: Three or four years ago, I don’t think most ML researchers would have needed
    to understand any of the content in this book. But today even “small” models run
    so close to hardware limits that doing novel research requires you to think about
    efficiency at scale.<d-footnote>Historically, ML research has followed something
    of a tick-tock cycle between systems innovations and software improvements. Alex
    Krizhevsky had to write unholy CUDA code to make CNNs fast but within a couple
    years, libraries like Theano and TensorFlow meant you didn't have to. Maybe that
    will happen here too and everything in this book will be abstracted away in a
    few years. But scaling laws have pushed our models perpetually to the very frontier
    of our hardware, and it seems likely that, for the foreseeable future, doing cutting
    edge research will be inextricably tied to an understanding of how to efficiently
    scale models to large hardware topologies.</d-footnote> **A 20% win on benchmarks
    is irrelevant if it comes at a 20% cost to roofline efficiency.** Promising model
    architectures routinely fail either because they *can’t* run efficiently at scale
    or because no one puts in the work to make them do so.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 三四年前，我认为大多数机器学习研究人员不需要理解这本书中的任何内容。但如今，即使是“小型”模型也运行得如此接近硬件极限，进行新颖的研究需要你思考大规模的效率问题。<d-footnote>历史上，机器学习研究在系统创新和软件改进之间遵循某种“周期性”的节奏。亚历克斯·克里泽夫斯基不得不编写不洁的CUDA代码来使卷积神经网络（CNN）加速，但几年后，像Theano和TensorFlow这样的库意味着你不必这样做。也许这里也会发生同样的事情，这本书中的所有内容在几年内都将被抽象化。但是，扩展定律不断推动我们的模型达到硬件的前沿，因此，在可预见的未来，进行前沿研究似乎不可避免地与如何高效地将模型扩展到大型硬件拓扑结构的知识紧密相关。</d-footnote>**如果为了获得基准测试的20%的改进而牺牲了20%的屋顶线效率，那么这种改进是无关紧要的。**有前途的模型架构通常会失败，要么是因为它们*无法*在大规模上高效运行，要么是因为没有人投入精力使它们这样做。
- en: '**The goal of “model scaling” is to be able to increase the number of chips
    used for training or inference while achieving a proportional, linear increase
    in throughput.** This is known as “*strong scaling*”. Although adding additional
    chips (“parallelism”) usually decreases the computation time, it also comes at
    the cost of added communication between chips. When communication takes longer
    than computation we become “communication bound” and cannot scale strongly.<d-footnote>As
    your computation time decreases, you also typically face bottlenecks at the level
    of a single chip. Your shiny new TPU or GPU may be rated to perform 500 trillion
    operations-per-second, but if you aren''t careful it can just as easily do a tenth
    of that if it''s bogged down moving parameters around in memory. The interplay
    of per-chip computation, memory bandwidth, and total memory is critical to the
    scaling story.</d-footnote> If we understand our hardware well enough to anticipate
    where these bottlenecks will arise, we can design or reconfigure our models to
    avoid them.<d-footnote>Hardware designers face the inverse problem: building hardware
    that provides just enough compute, bandwidth, and memory for our algorithms while
    minimizing cost. You can imagine how stressful this "co-design" problem is: you
    have to bet on what algorithms will look like when the first chips actually become
    available, often 2 to 3 years down the road. The story of the TPU is a resounding
    success in this game. Matrix multiplication is a unique algorithm in the sense
    that it uses far more FLOPs per byte of memory than almost any other (N FLOPs
    per byte), and early TPUs and their systolic array architecture achieved far better
    perf / $ than GPUs did at the time they were built. TPUs were designed for ML
    workloads, and GPUs with their TensorCores are rapidly changing to fill this niche
    as well. But you can imagine how costly it would have been if neural networks
    had not taken off, or had changed in some fundamental way that TPUs (which are
    inherently less flexible than GPUs) could not handle.</d-footnote>'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: “**模型扩展**”的目标是在增加用于训练或推理的芯片数量的同时，实现吞吐量的成比例、线性增长。这被称为“**强扩展**”。虽然增加额外的芯片（“并行性”）通常会减少计算时间，但它也带来了芯片间通信的增加成本。当通信时间超过计算时间时，我们就会变成“通信限制”状态，无法进行强扩展。<d-footnote>随着计算时间的减少，你通常也会在单个芯片的层面上遇到瓶颈。你那光鲜亮丽的TPU或GPU可能被评定为每秒执行500万亿次操作，但如果你不小心，它也可能只完成十分之一，如果它在内存中移动参数时遇到瓶颈。单个芯片的计算、内存带宽和总内存之间的相互作用对于扩展故事至关重要。</d-footnote>如果我们足够了解我们的硬件，能够预测这些瓶颈将在哪里出现，我们就可以设计或重新配置我们的模型来避免它们。<d-footnote>硬件设计师面临的是相反的问题：构建提供足够的计算、带宽和内存以适应我们的算法，同时最小化成本的硬件。你可以想象这个“协同设计”问题有多么紧张：你必须对首批芯片实际可用时的算法进行赌注，这通常需要2到3年的时间。TPU的故事在这个游戏中是一个响亮的成功。矩阵乘法是一个独特的算法，因为它使用的每字节内存FLOPs比几乎所有其他算法都要多（每字节N
    FLOPs），而早期的TPU及其阵列架构在构建时比当时的GPU实现了更好的性能/成本比。TPU是为ML工作负载设计的，而具有TensorCores的GPU也在迅速改变以填补这一空白。但你可以想象，如果神经网络没有起飞，或者以TPU（本质上不如GPU灵活）无法处理的方式发生了某些根本性的变化，那将花费多少成本。</d-footnote>
- en: '*Our goal in this book is to explain how TPU (and GPU) hardware works and how
    the Transformer architecture has evolved to perform well on current hardware.
    We hope this will be useful both for researchers designing new architectures and
    for engineers working to make the current generation of LLMs run fast.*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*本书的目标是解释TPU（和GPU）硬件的工作原理以及Transformer架构是如何演变以在当前硬件上表现良好的。我们希望这对设计新架构的研究人员以及努力使当前一代LLM运行得更快的技术人员都有用。*'
- en: High-Level Outline
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高级大纲
- en: 'The overall structure of this book is as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书的整体结构如下：
- en: '[Section 1](roofline) explains roofline analysis and what factors can limit
    our ability to scale (communication, computation, and memory). [Section 2](tpus)
    and [Section 3](sharding) talk in detail about how TPUs work, both as individual
    chips and — of critical importance — as an interconnected system with inter-chip
    links of limited bandwidth and latency. We’ll answer questions like:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[第1节](roofline)解释了屋顶线分析以及哪些因素可以限制我们的扩展能力（通信、计算和内存）。[第2节](tpus)和[第3节](sharding)详细介绍了TPU的工作原理，包括作为单个芯片以及——至关重要的——作为一个具有有限带宽和延迟的芯片间链路的互联系统。我们将回答以下问题：'
- en: How long should a matrix multiply of a certain size take? At what point is it
    bound by compute or by memory or communication bandwidth?
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个特定大小的矩阵乘法应该需要多长时间？它在计算、内存或通信带宽的哪个点上受到限制？
- en: How are TPUs wired together to form training clusters? How much bandwidth does
    each part of the system have?
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何将TPU连接起来形成训练集群？系统的每一部分有多少带宽？
- en: How long does it take to gather, scatter, or re-distribute arrays across multiple
    TPUs?
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集、分散或重新分配数组到多个TPU需要多长时间？
- en: How do we efficiently multiply matrices that are distributed differently across
    devices?
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何高效地乘以分布在设备上不同位置的矩阵？
- en: <picture>![](../Images/56da1b6e2650dfa1d2b7b7a30c72ad49.png)</picture>
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: <picture>![](../Images/56da1b6e2650dfa1d2b7b7a30c72ad49.png)</picture>
- en: '**Figure:** a diagram from [Section 2](tpus) showing how a TPU performs an
    elementwise product. Depending on the size of our arrays and the bandwidth of
    various links, we can find ourselves compute-bound (using the full hardware compute
    capacity) or comms-bound (bottlenecked by memory loading).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**图示：**来自[第2节](tpus)的图表，展示了TPU如何执行逐元素乘法。根据我们数组的尺寸和各个链路的带宽，我们可能会发现自己处于计算受限（使用全部硬件计算能力）或通信受限（由内存加载瓶颈）的状态。'
- en: 'Five years ago ML had a colorful landscape of architectures — ConvNets, LSTMs,
    MLPs, Transformers — but now we mostly just have the Transformer<d-cite key="transformers">.
    We strongly believe it’s worth understanding every piece of the Transformer architecture:
    the exact sizes of every matrix, where normalization occurs, how many parameters
    and FLOPs<d-footnote>FLoating point OPs, basically the total number of adds and
    multiplies required. While many sources take FLOPs to mean "operations per second",
    we use FLOPs/s to indicate that explicitly.</d-footnote> are in each part. [Section
    4](transformers) goes through this “Transformer math” carefully, showing how to
    count the parameters and FLOPs for both training and inference. This tells us
    how much memory our model will use, how much time we’ll spend on compute or comms,
    and when attention will become important relative to the feed-forward blocks.</d-cite>'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 五年前，机器学习领域有着丰富多彩的架构——卷积神经网络（ConvNets）、长短期记忆网络（LSTMs）、多层感知器（MLPs）、Transformer——但现在我们主要只有Transformer[d-cite
    key="transformers"]。我们坚信理解Transformer架构的每一部分都是值得的：每个矩阵的确切尺寸、归一化发生的位置、每个部分有多少参数和FLOPs[d-footnote]（基本是所需的总加法和乘法次数）。虽然许多资料将FLOPs理解为“每秒操作数”，我们使用FLOPs/s来明确表示。</d-footnote>。[第4节](transformers)仔细地解释了这种“Transformer数学”，展示了如何计算训练和推理时的参数和FLOPs。这告诉我们模型将使用多少内存，我们将在计算或通信上花费多少时间，以及何时注意力相对于前馈块变得重要。</d-cite>
- en: <picture>![](../Images/2a272a1a4f8f6ddbe5aaa45fcb38ed57.png)</picture>
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: <picture>![](../Images/2a272a1a4f8f6ddbe5aaa45fcb38ed57.png)</picture>
- en: '**Figure:** a standard Transformer layer with each matrix multiplication (matmul)
    shown as a dot inside a circle. All parameters (excluding norms) are shown in
    purple. [Section 4](transformers) walks through this diagram in more detail.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**图示：**一个标准的Transformer层，每个矩阵乘法（matmul）都显示为一个圆圈内的点。所有参数（不包括范数）都显示为紫色。[第4节](transformers)更详细地解释了这个图表。'
- en: '[Section 5: Training](training) and [Section 7: Inference](inference) are the
    core of this essay, where we discuss the fundamental question: given a model of
    some size and some number of chips, how do I parallelize my model to stay in the
    “strong scaling” regime? This is a simple question with a surprisingly complicated
    answer. At a high level, there are 4 primary parallelism techniques used to split
    models over multiple chips (**data**, **tensor**, **pipeline** and **expert**),
    and a number of other techniques to reduce the memory requirements (**rematerialisation**,
    **optimizer/model sharding (aka ZeRO)**, **host offload**, **gradient accumulation**).
    We discuss many of these here.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[第5节：训练](training)和[第7节：推理](inference)是本文的核心，我们讨论了基本问题：给定一个大小和芯片数量的模型，我如何并行化我的模型以保持在“强扩展”区域？这是一个简单的问题，但答案却出人意料地复杂。从高层次来看，有4种主要的并行技术用于在多个芯片上分割模型（**数据**、**张量**、**流水线**和**专家**），以及一些其他技术来减少内存需求（**重材料化**、**优化器/模型分片（也称为ZeRO**）、**主机卸载**、**梯度累积**）。我们在这里讨论了许多这些技术。'
- en: We hope by the end of these sections you should be able to choose among them
    yourself for new architectures or settings. [Section 6](applied-training) and
    [Section 8](applied-inference) are practical tutorials that apply these concepts
    to LLaMA-3, a popular open-source model.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望到这些章节的结尾，你能够自己选择它们用于新的架构或设置。[第6节](applied-training)和[第8节](applied-inference)是实际教程，将这些概念应用于流行的开源模型LLaMA-3。
- en: Finally, [Section 9](profiling) and [Section 10](jax-stuff) look at how to implement
    some of these ideas in JAX and how to profile and debug your code when things
    go wrong. [Section 12](gpus) is a new section that dives into GPUs as well.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，[第9节](profiling)和[第10节](jax-stuff)探讨了如何在JAX中实现这些想法，以及当事情出错时如何进行代码的剖析和调试。[第12节](gpus)是一个新的章节，深入探讨了GPU。
- en: Throughout we try to give you problems to work for yourself. Please feel no
    pressure to read all the sections or read them in order. And please leave feedback.
    For the time being, this is a draft and will continue to be revised. Thank you!
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个过程中，我们尽量给你提供一些问题让你自己动手解决。请不要感到有压力去阅读所有章节，或者按照顺序阅读。请留下反馈。目前，这是一个草案，并将继续修订。谢谢！
- en: '*We’d like to acknowledge James Bradbury and Blake Hechtman who derived many
    of the ideas in this doc.*'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们想感谢James Bradbury和Blake Hechtman，他们推导出本文档中的许多想法。*'
- en: Without further ado, [here is Section 1](roofline) about TPU rooflines.
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 不再拖延，[这里是第1节](roofline)关于TPU屋顶线的内容。
- en: Links to Sections
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部分链接
- en: '*This series is probably longer than it needs to be, but we hope that won’t
    deter you. The first three chapters are preliminaries and can be skipped if familiar,
    although they introduce notation used later. The final three parts might be the
    most practically useful, since they explain how to work with real models.*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*这个系列可能比需要的更长，但我们希望这不会让你感到沮丧。前三章是预备知识，如果熟悉可以跳过，尽管它们介绍了后面会用到的符号。最后三章可能是最有实用价值的，因为它们解释了如何与真实模型一起工作。*'
- en: '**Part 1: Preliminaries**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**第一部分：预备知识**'
- en: '[**Chapter 1: A Brief Intro to Roofline Analysis**](roofline). Algorithms are
    bounded by three things: compute, communication, and memory. We can use these
    to approximate how fast our algorithms will run.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**第一章：屋顶线分析简介**](roofline)。算法受限于三个方面：计算、通信和内存。我们可以利用这些来估算我们的算法将运行得多快。'
- en: '[**Chapter 2: How to Think About TPUs**](tpus). How do TPUs work? How does
    that affect what models we can train and serve?'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**第二章：如何思考TPU**](tpus)。TPU是如何工作的？这对我们可以训练和提供哪些模型有何影响？'
- en: '[**Chapter 3: Sharded Matrices and How to Multiply Them**](sharding). Here
    we explain model sharding and multi-TPU parallelism by way of our favorite operation:
    (sharded) matrix multiplications.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**第三章：分片矩阵及其乘法**](sharding)。在这里，我们通过我们最喜欢的操作：（分片）矩阵乘法，解释了模型分片和多TPU并行。'
- en: '**Part 2: Transformers**'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**第二部分：Transformer**'
- en: '[**Chapter 4: All the Transformer Math You Need to Know**](transformers). How
    many FLOPs does a Transformer use in its forward and backwards pass? Can you calculate
    the number of parameters? The size of its KV caches? We work through this math
    here.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**第四章：你需要了解的所有Transformer数学**](transformers)。Transformer在正向和反向传递过程中使用了多少FLOPs？你能计算参数数量吗？其KV缓存的尺寸？我们在这里通过数学推导来解决这个问题。'
- en: '[**Chapter 5: How to Parallelize a Transformer for Training**](training). FSDP.
    Megatron sharding. Pipeline parallelism. Given some number of chips, how do I
    train a model of a given size with a given batch size as efficiently as possible?'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**第五章：如何并行化Transformer进行训练**](training)。FSDP。Megatron分片。管道并行。给定一定数量的芯片，我如何以最高效的方式训练一个给定大小、给定批次的模型？'
- en: '[**Chapter 6: Training LLaMA 3 on TPUs**](applied-training). How would we train
    LLaMA 3 on TPUs? How long would it take? How much would it cost?'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**第六章：在TPU上训练LLaMA 3**](applied-training)。我们如何在TPU上训练LLaMA 3？需要多长时间？需要多少成本？'
- en: '[**Chapter 7: All About Transformer Inference**](inference). Once we’ve trained
    a model, we have to serve it. Inference adds a new consideration — latency — and
    changes up the memory landscape. We’ll talk about how disaggregated serving works
    and how to think about KV caches.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**第七章：关于Transformer推理的所有内容**](inference)。一旦我们训练了一个模型，我们就必须提供服务。推理增加了一个新的考虑因素——延迟，并改变了内存的格局。我们将讨论如何进行解耦服务以及如何考虑KV缓存。'
- en: '[**Chapter 8: Serving LLaMA 3 on TPUs**](applied-inference). How much would
    it cost to serve LLaMA 3 on TPU v5e? What are the latency/throughput tradeoffs?'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**第八章：在TPU v5e上提供服务LLaMA 3**](applied-inference)。在TPU v5e上提供服务LLaMA 3需要多少成本？延迟/吞吐量之间的权衡是什么？'
- en: '**Part 3: Practical Tutorials**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**第三部分：实用教程**'
- en: '[**Chapter 9: How to Profile TPU Code**](profiling). Real LLMs are never as
    simple as the theory above. Here we explain the JAX + XLA stack and how to use
    the JAX/TensorBoard profiler to debug and fix real issues.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**第九章：如何剖析TPU代码**](profiling)。真实的LLM永远不像上面的理论那样简单。在这里，我们解释了JAX + XLA堆栈以及如何使用JAX/TensorBoard剖析器来调试和修复真实问题。'
- en: '[**Chapter 10: Programming TPUs in JAX**](jax-stuff). JAX provides a bunch
    of magical APIs for parallelizing computation, but you need to know how to use
    them. Fun examples and worked problems.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**第10章：在JAX中编程TPU**](jax-stuff)。JAX提供了一系列用于并行计算的神奇API，但你需要知道如何使用它们。有趣的示例和已解决的问题。'
- en: '**Part 4: Conclusions and Bonus Content**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**第四部分：结论与额外内容**'
- en: '[**Chapter 11: Conclusions and Further Reading**](conclusion). Closing thoughts
    and further reading on TPUs and LLMs.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**第11章：结论与进一步阅读**](conclusion)。关于TPUs和LLMs的总结性思考及进一步阅读内容。'
- en: '[**Chapter 12: How to Think About GPUs**](gpus). A bonus section about GPUs,
    how they work, how they’re networked, and how their rooflines differ from TPUs.</d-article>  <d-appendix><d-footnote-list><d-citation-list>###
    Miscellaneous'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**第12章：如何思考GPU**](gpus)。关于GPU的额外章节，包括它们的工作原理、网络方式以及它们的屋顶线与TPUs的不同之处。</d-article>  <d-appendix><d-footnote-list><d-citation-list>###
    杂项'
- en: ^*Work done at Google DeepMind, now at MatX.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ^*在Google DeepMind完成的工作，现在在MatX。
- en: Citation
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 引用
- en: 'For attribution in academic contexts, please cite this work as:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在学术环境中进行归属引用时，请将此作品引用如下：
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'or as a BibTeX entry:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 或者作为一个BibTeX条目：
- en: '[PRE1]</d-citation-list></d-footnote-list></d-appendix> <d-bibliography src="/scaling-book/assets/bibliography/main.bib"></d-bibliography></d-byline>'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE1]</d-citation-list></d-footnote-list></d-appendix> <d-bibliography src="/scaling-book/assets/bibliography/main.bib"></d-bibliography></d-byline>'
