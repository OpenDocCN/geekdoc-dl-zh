- en: Evaluation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估
- en: Evaluation is an ever evolving approach. The key to understanding language model
    evaluation, particularly with post-training, is that the current popular evaluation
    regimes represent a reflection of the popular training best practices and goals.
    While challenging evaluations drive progress in language models to new areas,
    the majority of evaluation is designed around building useful signals for new
    models.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 评估是一个不断发展的方法。理解语言模型评估的关键，尤其是后训练评估，在于当前流行的评估制度反映了流行的训练最佳实践和目标。虽然具有挑战性的评估推动了语言模型向新领域的发展，但大多数评估都是为了构建对新模型有用的信号。
- en: In many ways, this chapter is designed to present vignettes of popular evaluation
    regimes throughout the early history of RLHF, so readers can understand the common
    themes, details, and failure modes.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多方面，本章旨在展示RLHF早期历史中流行的评估制度的片段，以便读者可以了解常见的主题、细节和失败模式。
- en: 'Evaluation for RLHF and post-training has gone a few distinct phases in its
    early history:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF和后训练评估在其早期历史中经历了几个不同的阶段：
- en: '**Early chat-phase**: Early models trained with RLHF or preference tuning targeted
    evaluations focused on capturing the chat performance of a model, especially relative
    to known strong models such as GPT-4\. Early examples include MT-Bench [[131]](ch021.xhtml#ref-zheng2023judging),
    AlpacaEval [[132]](ch021.xhtml#ref-dubois2024length), and Arena-Hard [[133]](ch021.xhtml#ref-li2024crowdsourced).
    Models were evaluated narrowly and these are now considered as “chat” or “instruction
    following” domains.'
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**早期聊天阶段**：早期使用RLHF或偏好调整训练的模型，其评估重点在于捕捉模型的聊天性能，特别是相对于GPT-4等已知强大模型。早期例子包括MT-Bench
    [[131]](ch021.xhtml#ref-zheng2023judging)，AlpacaEval [[132]](ch021.xhtml#ref-dubois2024length)，和Arena-Hard
    [[133]](ch021.xhtml#ref-li2024crowdsourced)。模型评估范围较窄，这些现在被认为是“聊天”或“指令遵循”领域。'
- en: '**Multi-skill era**: Over time, common practice established that RLHF can be
    used to improve more skills than just chat. For example, the Tülu evaluation suite
    included tasks on knowledge (MMLU [[310]](ch021.xhtml#ref-hendrycks2020measuring),
    PopQA [[311]](ch021.xhtml#ref-mallen2023llm_memorization), TruthfulQA [[312]](ch021.xhtml#ref-lin2021truthfulqa)),
    Reasoning (BigBenchHard [[313]](ch021.xhtml#ref-suzgun2022challenging), DROP [[314]](ch021.xhtml#ref-dua2019drop)),
    Math (MATH [[315]](ch021.xhtml#ref-hendrycksmath2021), GSM8K [[129]](ch021.xhtml#ref-cobbe2021gsm8k)),
    Coding (HumanEval [[316]](ch021.xhtml#ref-chen2021codex), HumanEval+ [[317]](ch021.xhtml#ref-evalplus)),
    Instruction Following [[115]](ch021.xhtml#ref-zhou2023instructionfollowingevaluationlargelanguage),
    and Safety (a composite of many evaluations). This reflects the domain where post-training
    is embraced as a multi-faceted solution beyond safety and chat.'
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**多技能时代**：随着时间的推移，常见的做法是RLHF可以用来提高比聊天更多的技能。例如，Tülu评估套件包括知识（MMLU [[310]](ch021.xhtml#ref-hendrycks2020measuring)，PopQA
    [[311]](ch021.xhtml#ref-mallen2023llm_memorization)，TruthfulQA [[312]](ch021.xhtml#ref-lin2021truthfulqa))，推理（BigBenchHard
    [[313]](ch021.xhtml#ref-suzgun2022challenging)，DROP [[314]](ch021.xhtml#ref-dua2019drop))，数学（MATH
    [[315]](ch021.xhtml#ref-hendrycksmath2021)，GSM8K [[129]](ch021.xhtml#ref-cobbe2021gsm8k))，编码（HumanEval
    [[316]](ch021.xhtml#ref-chen2021codex)，HumanEval+ [[317]](ch021.xhtml#ref-evalplus))，指令遵循
    [[115]](ch021.xhtml#ref-zhou2023instructionfollowingevaluationlargelanguage)和安全（许多评估的组合）。这反映了后训练被接受为一个多方面的解决方案，超越了安全和聊天领域。'
- en: '**Reasoning & tools**: The current era for post-training is defined by a focus
    on challenging reasoning and tool use problems. These include much harder knowledge-intensive
    tasks such as GPQA Diamond [[318]](ch021.xhtml#ref-rein2023gpqa) and Humanity’s
    Last Exam [[319]](ch021.xhtml#ref-phan2025hle), intricate software engineering
    tasks such as SWE-Bench+ [[320]](ch021.xhtml#ref-aleithan2024swebenchplus) and
    LiveCodeBench [[321]](ch021.xhtml#ref-jain2024livecodebench), or challenging math
    problems exemplified by recent AIME contests.'
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**推理与工具**：后训练的当前时代以关注具有挑战性的推理和工具使用问题为特征。这些问题包括许多更难的知识密集型任务，如GPQA Diamond [[318]](ch021.xhtml#ref-rein2023gpqa)和人类最后的考试
    [[319]](ch021.xhtml#ref-phan2025hle)，复杂的软件工程任务，如SWE-Bench+ [[320]](ch021.xhtml#ref-aleithan2024swebenchplus)和LiveCodeBench
    [[321]](ch021.xhtml#ref-jain2024livecodebench)，或由最近的AIME竞赛展示的具有挑战性的数学问题。'
- en: Beyond this, new domains will evolve. As AI becomes more of an industrialized
    field, the incentives of evaluation are shifting and becoming multi-stakeholder.
    Since the release of ChatGPT, private evaluations such as the Scale Leaderboard
    [[322]](ch021.xhtml#ref-scale2024seal), community-driven evaluations such as ChatBotArena
    [[113]](ch021.xhtml#ref-chiang2024chatbot), and third-party evaluation companies
    such as ArtificialAnalysis and Epoch AI have proliferated. Throughout this chapter
    we will include details that map to how these evaluations were implemented and
    understood.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，新的领域将会发展。随着人工智能越来越成为一个工业化的领域，评估的激励正在转变并成为多利益相关者。自从ChatGPT发布以来，私人评估如Scale
    Leaderboard [[322]](ch021.xhtml#ref-scale2024seal)，社区驱动的评估如ChatBotArena [[113]](ch021.xhtml#ref-chiang2024chatbot)，以及第三方评估公司如ArtificialAnalysis和Epoch
    AI已经大量涌现。在本章中，我们将包括如何实施和理解这些评估的详细信息。
- en: 'Prompting Formatting: From Few-shot to Zero-shot to CoT'
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示格式化：从少样本到零样本再到CoT
- en: '**Prompting** language models is primarily a verb, but it is also considered
    a craft or art that one can practice and/or train in general [[323]](ch021.xhtml#ref-schulhoff2024prompt).
    A prompt is the way of structuring information and context for a language model.
    For common interactions, the prompt is relatively basic. For advanced scenarios,
    a well crafted prompt will mean success or failure on a specific one-off use-case.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示**语言模型主要是一个动词，但它也被认为是一种可以练习和/或训练的技艺或艺术 [[323]](ch021.xhtml#ref-schulhoff2024prompt)。提示是结构化信息并为语言模型提供上下文的方式。对于常见的交互，提示相对基础。对于高级场景，一个精心制作的提示将意味着在特定的一次性用例中的成功或失败。'
- en: When it comes to evaluation, prompting techniques can have a substantial impact
    on the performance of the model. Some prompting techniques – e.g. formatting discussed
    below – can make a model’s performance drop from 60% to near 0\. Similarly, a
    change of prompt can help models learn better during training. Colloquially, prompting
    a model well can give the subjective experience of using future models, unlocking
    performance outside of normal use.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当谈到评估时，提示技术可以对模型的性能产生重大影响。一些提示技术——例如下面讨论的格式化——可以使模型的表现从60%下降到几乎0%。同样，改变提示可以帮助模型在训练期间更好地学习。通俗地说，良好地提示模型可以提供使用未来模型的主观体验，解锁正常使用之外的性能。
- en: Prompting well with modern language models can involve preparing an entire report
    for the model to respond to (often with 1000s of tokens of generated text). This
    behavior is downstream of many changes in how language model performance has been
    measured and understood.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 使用现代语言模型进行良好提示可能包括为模型准备一份完整的报告以供其回应（通常包含成千上万的生成文本标记）。这种行为是语言模型性能测量和理解方面许多变化的下游。
- en: 'Early language models were only used as intelligent autocomplete. In order
    to use these models in a more open ended way, multiple examples were shown to
    the model and then a prompt that is an incomplete phrase. This was called few-shot
    or in-context learning [[167]](ch021.xhtml#ref-brown2020language), and at the
    time instruction tuning or RLHF was not involved. In the case of popular evaluations,
    this would look like:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的语言模型仅被用作智能自动完成。为了更开放地使用这些模型，向模型展示了多个示例，然后是一个不完整的短语提示。这被称为少样本或上下文学习 [[167]](ch021.xhtml#ref-brown2020language)，当时指令调整或RLHF并未涉及。在流行评估的情况下，这看起来可能如下：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here, there are multiple ways to evaluate an answer. If we consider a question
    in the style of MMLU, where the model has to choose between multiple answers:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，评估答案有多种方式。如果我们考虑MMLU风格的提问，其中模型需要在多个答案之间进行选择：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: To extract an answer here one could either generate a token based on some sampling
    parameters and see if the answer is correct, A,B,C, or D (formatting above like
    this proposed in [[324]](ch021.xhtml#ref-robinson2023leveraging)), or one could
    look at the probabilities of each token and mark the task as correct if the correct
    answer is more likely. This second method has two potential implementations –
    first, one could look at the probability of the letter (A) or the answer “The
    Mean Value Theorem.” Both of these are permissible metrics, but answer prediction
    is more common among probability base metrics.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 要在这里提取答案，可以基于某些采样参数生成一个标记，并查看答案是否正确，A、B、C或D（如上所述的格式化，如[[324]](ch021.xhtml#ref-robinson2023leveraging)中提出的那样），或者可以查看每个标记的概率，如果正确答案更有可能，则标记任务为正确。第二种方法有两种潜在的实施方式——首先，可以查看字母（A）或答案“平均值定理”的概率。这两个都是允许的指标，但基于概率的指标中，答案预测更为常见。
- en: A common challenge with few-shot prompting is that models will not follow the
    format, which is counted as an incorrect answer. When designing an evaluation
    domain, the number of examples used in-context is often considered a design parameter
    and ranges from 3 to 8 or more.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 少量提示的一个常见挑战是模型不会遵循格式，这被视为错误答案。在设计评估领域时，上下文中使用的示例数量通常被视为设计参数，范围从3到8个或更多。
- en: 'Within the evolution of few-shot prompting came the idea of including chain-of-thought
    examples for the model to follow. This comes in the form of examples where the
    in-context examples have written out reasoning, such as below (which later was
    superseded by explicit prompting to generate reasoning steps) [[54]](ch021.xhtml#ref-wei2022chain):'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '在少量提示的演变过程中，出现了为模型提供思维链示例的想法。这些示例包括在上下文中写出的推理，如下所示（后来被生成推理步骤的明确提示所取代） [[54]](ch021.xhtml#ref-wei2022chain):'
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Over time, as language models became stronger, they evolved to zero-shot evaluation,
    a.k.a. “zero-shot learners” [[325]](ch021.xhtml#ref-wei2022finetuned). The Finetuned
    Language Net (FLAN) showed that language models finetuned in specific tasks, as
    a precursor to modern instruction tuning, could generalize to zero-shot questions
    they were not trained on [[325]](ch021.xhtml#ref-wei2022finetuned) (similar results
    are also found in T0 [[326]](ch021.xhtml#ref-sanh2022multitask)). This is the
    emergence of instruction finetuning (IFT), an important precursor to RLHF and
    post-training. A zero shot question would look like:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间推移，随着语言模型变得更强大，它们演变为零样本评估，也称为“零样本学习器” [[325]](ch021.xhtml#ref-wei2022finetuned)。微调语言网络（FLAN）表明，在特定任务中微调的语言模型，作为现代指令微调的先导，可以推广到它们未训练过的零样本问题
    [[325]](ch021.xhtml#ref-wei2022finetuned)（在T0 [[326]](ch021.xhtml#ref-sanh2022multitask)中也发现了类似的结果）。这是指令微调（IFT）的出现，它是RLHF和训练后的重要先导。一个零样本问题可能看起来像：
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: From here in 2022, the timeline begins to include key early RLHF works, such
    as InstructGPT. The core capability and use-case shift that accompanied these
    models is even more open-ended usage. With more open-ended usage, generative evaluation
    became increasingly popular as it mirrors actual usage. In this period through
    recent years after ChatGPT, some multiple-choice evaluations were still used in
    RLHF research as a holdback to common practice.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 从2022年开始，时间线开始包括关键的早期RLHF工作，例如InstructGPT。伴随这些模型的核心能力和用例转变是更加开放的使用方式。随着更加开放的使用，生成式评估因其与实际使用相似而越来越受欢迎。在这个时期，从ChatGPT出现后的近几年，一些多项选择题评估仍然在RLHF研究中作为对常见实践的制约因素被使用。
- en: With the rise of reasoning models at the end of 2024 and the beginning of 2025,
    a major change in model behavior was the addition of a long Chain-of-Thought (CoT)
    reasoning process before every answer. These models no longer needed to be prompted
    with the canonical modification of “think step by step,” as proposed in [[327]](ch021.xhtml#ref-kojima2022large).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 2024年底和2025年初，随着推理模型的出现，模型行为发生了重大变化，即在每次回答之前添加了长思维链（CoT）推理过程。这些模型不再需要使用“逐步思考”的规范修改来提示，正如[[327]](ch021.xhtml#ref-kojima2022large)中提出的。
- en: 'For example, for every question or category there are specially designed prompts
    to help extract behavior from the model. Tülu 3 details some prompts used for
    CoT answering on multiple choice questions [[6]](ch021.xhtml#ref-lambert2024t):'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '例如，对于每个问题或类别，都有专门设计的提示来帮助从模型中提取行为。Tülu 3详细说明了用于多项选择题CoT回答的一些提示 [[6]](ch021.xhtml#ref-lambert2024t):'
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This, especially when the models use special formatting to separate thinking
    tokens from answer tokens, necessitated the most recent major update to evaluation
    regimes. Evaluation is moving to where the models are tested to respond in a generative
    manner with a chain of thought prompting.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其当模型使用特殊格式来区分思维标记和答案标记时，这促使最近的重大评估制度更新。评估正在转向测试模型以生成式方式响应思维链提示。
- en: Using Evaluations vs. Observing Evaluations
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用评估与观察评估
- en: '![Report from Epoch AI showing how major AI evaluations are rapidly saturated
    over time. License CC-BY.](../media/file19.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![Epoch AI的报告显示主要AI评估如何随着时间的推移迅速饱和。许可CC-BY。](../media/file19.jpg)'
- en: Report from Epoch AI showing how major AI evaluations are rapidly saturated
    over time. License CC-BY.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Epoch AI的报告显示主要AI评估如何随着时间的推移迅速饱和。许可CC-BY。
- en: Language model evaluations done within companies can only be compared to their
    peers with large error bars because the process that they use for evaluations
    internally is not matched with external evaluations. Internal evaluations are
    made to hillclimb on for training, as would be called a “training set” in traditional
    machine learning. The public evaluations that the community uses to compare leading
    models cannot be known if they were within said training set or as unseen “test
    sets” or “validation sets.”
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 公司内部进行的语言模型评估只能与具有大误差栏的同行进行比较，因为它们用于内部评估的过程与外部评估不匹配。内部评估是为了在训练中爬山，就像在传统机器学习中被称为“训练集”的那样。社区使用的公共评估，用于比较领先模型，如果它们在所谓的训练集中，或者作为未见的“测试集”或“验证集”，是无法知道的。
- en: As evaluation scores have become central components of corporate marketing schemes,
    their implementations within companies have drifted. There are rumors of major
    AI labs using “custom prompts” for important evaluations like GSM8k or MATH. These
    practices evolve rapidly.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 由于评估分数已成为企业营销方案的核心组成部分，公司内部的实施已经发生了变化。有传言称，一些主要的AI实验室使用“定制提示”来进行像GSM8k或MATH这样重要评估。这些做法发展迅速。
- en: Language model evaluation stacks are perceived as marketing because the evaluations
    have no hard source of truth. What is happening inside frontier labs is that evaluation
    suites are being tuned to suit their internal needs. When results are shared,
    we get output in the form of the numbers a lab got for their models, but not all
    the inputs to that function. The inputs are very sensitive configurations, and
    they’re different at all of OpenAI, Meta, Anthropic, and Google. Even fully open
    evaluation standards are hard to guarantee reproducibility on. Focusing efforts
    on your own models is the only way to get close to repeatable evaluation techniques.
    There are good intentions underpinning the marketing, starting with the technical
    teams.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型评估栈被视为营销，因为评估没有硬的真实来源。前沿实验室内部发生的事情是，评估套件正在调整以适应其内部需求。当结果被分享时，我们得到的是实验室为其模型获得的数字输出，但不是该函数的所有输入。输入是非常敏感的配置，而且在OpenAI、Meta、Anthropic和Google都不同。即使是完全开放的评估标准也难以保证可重复性。专注于自己的模型是接近可重复评估技术的唯一方法。营销背后有良好的意图，首先是技术团队。
- en: Evaluation of frontier language models is every bit as much an art today as
    it is a science.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 前沿语言模型的评估在今天既是一门艺术，也是一门科学。
- en: Different groups choose different evaluations to maintain independence on, i.e. making
    them a true test set, but no one discloses which ones they choose. For example,
    popular reasoning evaluations MATH and GSM8k both have training sets with prompts
    that can easily be used to improve performance. Improving performance with the
    prompts from the same distribution is very different than generalizing to these
    tasks by training on general math data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的群体选择不同的评估方法以保持独立性，即使它们成为真正的测试集，但没有人公开他们选择了哪些。例如，流行的推理评估MATH和GSM8k都拥有带有提示的训练集，这些提示可以轻易地用来提高性能。使用同一分布的提示来提高性能与通过一般数学数据训练来泛化这些任务非常不同。
- en: In fact, these *training sets* are very high quality data so models would benefit
    from training on them. If these companies are *not* using the corresponding evaluation
    as a core metric to track, training on the evaluation set could be a practical
    decision as high-quality data is a major limiting factor of model development.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，这些**训练集**是高质量的数据，因此模型从训练它们中受益。如果这些公司**没有**将相应的评估作为核心指标来跟踪，那么在评估集上训练可能是一个实际的决定，因为高质量数据是模型发展的主要限制因素。
- en: Leading AI laboratories hillclimb by focusing on a few key evaluations and report
    scores on the core public set at the end. The key point is that some of their
    evaluations for tracking progress, such as the datasets for cross-entropy loss
    predictions in scaling from the GPT-4 report [[328]](ch021.xhtml#ref-achiam2023gpt),
    are often not public.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 领先的AI实验室通过关注几个关键评估来进行爬山，并在最后报告核心公共集的分数。关键点是，他们用于跟踪进度的某些评估，例如从GPT-4报告中缩放时的交叉熵损失预测数据集
    [[328]](ch021.xhtml#ref-achiam2023gpt)，通常是不公开的。
- en: The post-training evaluations are heavily co-dependent on human evaluation.
    Human evaluation for generative language models yields Elo rankings (popular in
    early Anthropic papers, such as Constitutional AI), and human evaluation for reward
    models shows agreement. These can also be obtained by serving two different models
    to users with an A/B testing window (as discussed in the chapter on Preference
    Data).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 后期训练的评估在很大程度上依赖于人工评估。对于生成语言模型的人工评估产生Elo排名（在早期Anthropic论文中很受欢迎，如宪法AI），而对于奖励模型的人工评估则显示一致性。这些也可以通过向用户服务两个不同的模型并使用A/B测试窗口（如本章所述的偏好数据章节）来获得。
- en: The limited set of evaluations they choose to focus on forms a close link between
    evaluation and training. At one point one evaluation of focus was MMLU. GPQA was
    one of choice during reasoning models’ emergence. Labs will change the evaluations
    to make them better suited to their needs, such as OpenAI releasing SWE-Bench-Verified
    [[329]](ch021.xhtml#ref-openai2024swebench). There are many more internally the
    public does not have access to.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 他们选择关注的一组有限的评估形成了一个紧密的联系，即评估与训练之间的联系。在某个时刻，一个关注的评估是MMLU。GPQA是推理模型出现时的一个选择。实验室会改变评估，以使它们更适合他们的需求，例如OpenAI发布了SWE-Bench-Verified
    [[329]](ch021.xhtml#ref-openai2024swebench)。还有许多其他公众无法访问的内部评估。
- en: The key “capability” that improving evaluations internally has on downstream
    training is **improving the statistical power when comparing training runs**.
    By changing evaluations, these labs reduce the noise on their prioritized signals
    in order to make more informed training decisions.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 内部改进评估对下游训练的关键“能力”在于**提高比较训练运行时的统计功效**。通过改变评估，这些实验室减少了他们优先考虑的信号上的噪声，以便做出更明智的训练决策。
- en: This is compounded by the sophistication of post-training in the modern language
    model training stacks. Evaluating language models today involves a moderate amount
    of generating tokens (rather than just looking at log probabilities of answers).
    It is accepted that small tricks are used by frontier labs to boost performance
    on many tasks — the most common explanation is one-off prompts for certain evaluations.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这种复杂性在现代语言模型训练堆栈的后期训练的复杂性上进一步加剧。今天评估语言模型涉及生成一定数量的标记（而不仅仅是查看答案的日志概率）。人们普遍认为，前沿实验室为了提高许多任务的表现，会使用一些小技巧——最常见的解释是为某些评估提供一次性的提示。
- en: Another example of confusion when comparing evaluations from multiple laboratories
    is the addition of inference-time scaling to evaluation comparisons. Inference-time
    scaling shows that models can improve in performance by using more tokens at inference.
    Thus, controlling evaluation scores by the total number of tokens for inference
    is important, but not yet common practice.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个在比较多个实验室的评估时产生混淆的例子是将推理时间缩放添加到评估比较中。推理时间缩放表明，模型可以通过在推理时使用更多标记来提高性能。因此，通过推理的总标记数来控制评估分数是很重要的，但还不是常见的做法。
- en: 'Depending on how your data is formatted in post-training, models will have
    substantial differences across evaluation formats. For example, two popular, open
    math datasets NuminaMath [[330]](ch021.xhtml#ref-li2024numinamath) and MetaMath
    [[331]](ch021.xhtml#ref-yu2023metamath) conflict with each other in training due
    to small differences in how the answers are formatted – Numina puts the answer
    in `\boxed{XYZ}` and MetaMath puts the answer after `The answer is: XYZ` —- training
    on both can make performance worse than with just one. Strong models are trained
    to be able to function with multiple formats, but they generally have a strongest
    format.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '根据你的数据在后期训练中的格式，模型在评估格式上会有很大的差异。例如，两个流行的、公开的数学数据集NuminaMath [[330]](ch021.xhtml#ref-li2024numinamath)
    和 MetaMath [[331]](ch021.xhtml#ref-yu2023metamath) 由于答案格式上的微小差异而在训练中相互冲突——Numina将答案放在`\boxed{XYZ}`中，而MetaMath将答案放在`The
    answer is: XYZ`之后——在两者上训练可能会使性能不如仅在一个上训练。强大的模型被训练成能够使用多种格式运行，但它们通常有一个最强的格式。'
- en: 'In the end we are left with a few key points on the state of evaluating closed
    models:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们留下了关于评估封闭模型状态的几个关键点：
- en: We do not know or necessarily have the key test sets that labs are climbing
    on, so some evaluations are proxies.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不知道或不一定拥有实验室正在攀登的关键测试集，因此一些评估只是代理。
- en: Inference of frontier models is becoming more complicated with special system
    prompts, special tokens, etc., and we don’t know how it impacts evaluations, and
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前沿模型的推理变得越来越复杂，有特殊的系统提示、特殊的标记等，而我们不知道它如何影响评估，
- en: We do not know all the formats and details used to numerically report the closed
    evaluations.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不知道用于数值报告封闭评估的所有格式和细节。
- en: Contamination
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 污染
- en: A major issue with current language model practices (i.e. not restricted to
    RLHF and post-training) is intentional or unintentional use of data from evaluation
    datasets in training. This is called *dataset contamination* and respectively
    the practices to avoid it are *decontamination*. In order to decontaminate a dataset,
    one performs searches over the training and test datasets, looking for matches
    in n-grams (characters) or tokens [[332]](ch021.xhtml#ref-singh2024evaluation).
    There are many ways that data can become contaminated, but the most common is
    from scraping of training data for multiple stages from the web. Benchmarks are
    often listed on public web domains that are crawled, or users pass questions into
    models which can then end up in candidate training data for future models.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当前语言模型实践（即不仅限于RLHF和训练后）的一个主要问题是故意或无意中使用评估数据集的数据进行训练。这被称为*数据集污染*，相应地，避免这种做法的实践是*去污*。为了去污一个数据集，一个人会在训练和测试数据集中进行搜索，寻找n-gram（字符）或标记的匹配
    [[332]](ch021.xhtml#ref-singh2024evaluation)。数据可能以多种方式被污染，但最常见的是从网络上抓取多个阶段的训练数据。基准通常列在公共网络域名上，这些域名会被爬取，或者用户将问题输入到模型中，这可能导致未来模型的候选训练数据。
- en: 'For example, during the decontamination of the evaluation suite for Tülu 3,
    the authors found that popular open datasets were contaminated with popular evaluations
    for RLHF [[6]](ch021.xhtml#ref-lambert2024t). These overlaps include: UltraFeedback’s
    contamination with TruthfulQA, Evol-CodeAlpaca’s contamination with HumanEval,
    NuminaMath’s contamination with MATH, and WildChat’s contamination with safety
    evaluations. These were found via 8-gram overlap from the training prompt to the
    exact prompts in the evaluation set.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在Tülu 3评估套件的去污过程中，作者发现流行的开源数据集被流行的RLHF评估污染 [[6]](ch021.xhtml#ref-lambert2024t)。这些重叠包括：UltraFeedback与TruthfulQA的污染，Evol-CodeAlpaca与HumanEval的污染，NuminaMath与MATH的污染，以及WildChat与安全评估的污染。这些是通过从训练提示到评估集中确切提示的8-gram重叠发现的。
- en: In order to understand contamination of models that do not disclose or release
    the training data, new versions of benchmarks are created with slightly perturbed
    questions from the original, e.g. for MATH [[333]](ch021.xhtml#ref-huang2025math),
    in order to see which models were trained to match the original format or questions.
    High variance on these perturbation benchmarks is not confirmation of contamination,
    which is difficult to prove, but could indicate models that were trained with
    a specific format in mind that may not translate to real world performance.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解不披露或发布训练数据的模型污染，创建了带有略微扰动的原始问题的基准新版本，例如，对于MATH [[333]](ch021.xhtml#ref-huang2025math)，以查看哪些模型被训练来匹配原始格式或问题。这些扰动基准上的高方差并不是污染的确认，污染难以证明，但可能表明那些以特定格式进行训练的模型，这些格式可能不会转化为现实世界的性能。
- en: Tooling
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工具
- en: There are many open-sourced evaluation tools for people to choose from. There’s
    Inspect AI from the UK Safety Institute [[334]](ch021.xhtml#ref-inspectAI2024),
    HuggingFace’s LightEval [[335]](ch021.xhtml#ref-fourrier2023lighteval) that powered
    the Open LLM Leaderboard [[336]](ch021.xhtml#ref-open-llm-leaderboard-v2), Eleuther
    AI’s evaluation harness [[337]](ch021.xhtml#ref-gao2023evalharness) built on top
    of the infrastructure from their GPT-Neo-X model (around GPT-3 evaluation config)
    [[338]](ch021.xhtml#ref-gpt-neox-20b), AI2’s library based on OLMES [[339]](ch021.xhtml#ref-gu2024olmes),
    Stanford’s Center for Research on Foundation Model’s HELM [[340]](ch021.xhtml#ref-liang2023helm),
    Mosaic’s (now Databricks’) Eval Gauntlet [[341]](ch021.xhtml#ref-mosaicml2024gauntlet),
    and more.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多开源评估工具供人们选择。例如，来自英国安全研究所的Inspect AI [[334]](ch021.xhtml#ref-inspectAI2024)，由HuggingFace提供的LightEval
    [[335]](ch021.xhtml#ref-fourrier2023lighteval)，它为Open LLM排行榜 [[336]](ch021.xhtml#ref-open-llm-leaderboard-v2)
    提供动力，Eleuther AI基于其GPT-Neo-X模型（大约是GPT-3评估配置）构建的评估工具 [[337]](ch021.xhtml#ref-gao2023evalharness)，AI2基于OLMES的库
    [[339]](ch021.xhtml#ref-gu2024olmes)，斯坦福大学基础模型研究中心的HELM [[340]](ch021.xhtml#ref-liang2023helm)，Mosaic（现在是Databricks）的Eval
    Gauntlet [[341]](ch021.xhtml#ref-mosaicml2024gauntlet)，以及其他更多。
