- en: Training Overview
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ¦‚è¿°
- en: In this chapter we provide a cursory overview of RLHF training, before getting
    into the specifics later in the book. RLHF, while optimizing a simple loss function,
    involves training multiple, different AI models in sequence and then linking them
    together in a complex, online optimization.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†RLHFè®­ç»ƒçš„ç®€è¦æ¦‚è¿°ï¼Œä¹‹ååœ¨æœ¬ä¹¦çš„åç»­éƒ¨åˆ†å°†è¯¦ç»†ä»‹ç»ã€‚RLHFï¼Œè™½ç„¶ä¼˜åŒ–ä¸€ä¸ªç®€å•çš„æŸå¤±å‡½æ•°ï¼Œä½†æ¶‰åŠæŒ‰é¡ºåºè®­ç»ƒå¤šä¸ªä¸åŒçš„AIæ¨¡å‹ï¼Œç„¶ååœ¨å¤æ‚çš„åœ¨çº¿ä¼˜åŒ–ä¸­å°†å®ƒä»¬é“¾æ¥åœ¨ä¸€èµ·ã€‚
- en: Here, we introduce the core objective of RLHF, which is optimizing a proxy of
    reward of human preferences with a distance-based regularizer (along with showing
    how it relates to classical RL problems). Then we showcase canonical recipes which
    use RLHF to create leading models to show how RLHF fits in with the rest of post-training
    methods. These example recipes will serve as references for later in the book,
    where we describe different optimization choices you have when doing RLHF, and
    we will point back to how different key models used different steps in training.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»‹ç»äº†RLHFçš„æ ¸å¿ƒç›®æ ‡ï¼Œå³é€šè¿‡åŸºäºè·ç¦»çš„æ­£åˆ™åŒ–å™¨ï¼ˆä»¥åŠå±•ç¤ºå…¶å¦‚ä½•ä¸ç»å…¸RLé—®é¢˜ç›¸å…³è”ï¼‰æ¥ä¼˜åŒ–äººç±»åå¥½çš„å¥–åŠ±ä»£ç†ã€‚ç„¶åï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä½¿ç”¨RLHFåˆ›å»ºé¢†å…ˆæ¨¡å‹çš„å…¸å‹é…æ–¹ï¼Œä»¥å±•ç¤ºRLHFå¦‚ä½•ä¸å…¶ä»–åè®­ç»ƒæ–¹æ³•ç›¸ç»“åˆã€‚è¿™äº›ç¤ºä¾‹é…æ–¹å°†åœ¨æœ¬ä¹¦çš„åç»­éƒ¨åˆ†ä½œä¸ºå‚è€ƒï¼Œå…¶ä¸­æˆ‘ä»¬å°†æè¿°åœ¨è¿›è¡ŒRLHFæ—¶æ‚¨æ‰€æ‹¥æœ‰çš„ä¸åŒä¼˜åŒ–é€‰æ‹©ï¼Œå¹¶ä¸”æˆ‘ä»¬å°†å›é¡¾ä¸åŒå…³é”®æ¨¡å‹åœ¨è®­ç»ƒä¸­ä½¿ç”¨äº†ä¸åŒçš„æ­¥éª¤ã€‚
- en: Problem Formulation
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é—®é¢˜è¡¨è¿°
- en: 'The optimization of reinforcement learning from human feedback (RLHF) builds
    on top of the standard RL setup. In RL, an agent takes actions <semantics><msub><mi>a</mi><mi>t</mi></msub><annotation
    encoding="application/x-tex">a_t</annotation></semantics> sampled from a policy
    <semantics><mrow><mi>Ï€</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>âˆ£</mo><msub><mi>s</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\pi(a_t\mid
    s_t)</annotation></semantics> given the state of the environment <semantics><msub><mi>s</mi><mi>t</mi></msub><annotation
    encoding="application/x-tex">s_t</annotation></semantics> to maximize reward <semantics><mrow><mi>r</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">r(s_t,a_t)</annotation></semantics>
    [[55]](ch021.xhtml#ref-sutton2018reinforcement). Traditionally, the environment
    evolves according to transition (dynamics) <semantics><mrow><mi>p</mi><mo stretchy="false"
    form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>âˆ£</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">p(s_{t+1}\mid
    s_t, a_t)</annotation></semantics> with an initial state distribution <semantics><mrow><msub><mi>Ï</mi><mn>0</mn></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\rho_0(s_0)</annotation></semantics>.
    Together, the policy and dynamics induce a trajectory distribution:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ä»äººç±»åé¦ˆä¸­å¼ºåŒ–å­¦ä¹ çš„ä¼˜åŒ–å»ºç«‹åœ¨æ ‡å‡†RLè®¾ç½®ä¹‹ä¸Šã€‚åœ¨RLä¸­ï¼Œæ™ºèƒ½ä½“ä»ç­–ç•¥<semantics><mrow><mi>Ï€</mi><mo stretchy="false"
    form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>âˆ£</mo><msub><mi>s</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\pi(a_t\mid
    s_t)</annotation></semantics>ä¸­é‡‡æ ·åŠ¨ä½œ<semantics><msub><mi>a</mi><mi>t</mi></msub><annotation
    encoding="application/x-tex">a_t</annotation></semantics>ï¼Œç»™å®šç¯å¢ƒçš„çŠ¶æ€<semantics><msub><mi>s</mi><mi>t</mi></msub><annotation
    encoding="application/x-tex">s_t</annotation></semantics>ä»¥æœ€å¤§åŒ–å¥–åŠ±<semantics><mrow><mi>r</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">r(s_t,a_t)</annotation></semantics>
    [[55]](ch021.xhtml#ref-sutton2018reinforcement)ã€‚ä¼ ç»Ÿä¸Šï¼Œç¯å¢ƒæ ¹æ®è½¬æ¢ï¼ˆåŠ¨åŠ›å­¦ï¼‰<semantics><mrow><mi>p</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>âˆ£</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">p(s_{t+1}\mid
    s_t, a_t)</annotation></semantics>ä»¥åŠåˆå§‹çŠ¶æ€åˆ†å¸ƒ<semantics><mrow><msub><mi>Ï</mi><mn>0</mn></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\rho_0(s_0)</annotation></semantics>æ¼”å˜ã€‚å…±åŒåœ°ï¼Œç­–ç•¥å’ŒåŠ¨åŠ›å­¦è¯±å¯¼å‡ºä¸€ä¸ªè½¨è¿¹åˆ†å¸ƒï¼š
- en: <semantics><mrow><msub><mi>p</mi><mi>Ï€</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>Ï</mi><mn>0</mn></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo stretchy="false"
    form="postfix">)</mo><munderover><mo>âˆ</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>T</mi><mo>âˆ’</mo><mn>1</mn></mrow></munderover><mi>Ï€</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>âˆ£</mo><msub><mi>s</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mi>p</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>âˆ£</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mi>.</mi><mrow><mo stretchy="false" form="prefix">(</mo><mn>7</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p_{\pi}(\tau)=\rho_0(s_0)\prod_{t=0}^{T-1}\pi(a_t\mid
    s_t)\,p(s_{t+1}\mid s_t,a_t).\qquad{(7)}</annotation></semantics>
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><msub><mi>p</mi><mi>Ï€</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>Ï</mi><mn>0</mn></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo stretchy="false"
    form="postfix">)</mo><munderover><mo>âˆ</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>T</mi><mo>âˆ’</mo><mn>1</mn></mrow></munderover><mi>Ï€</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>âˆ£</mo><msub><mi>s</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mi>p</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>âˆ£</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mi>.</mi><mrow><mo stretchy="false" form="prefix">(</mo><mn>7</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p_{\pi}(\tau)=\rho_0(s_0)\prod_{t=0}^{T-1}\pi(a_t\mid
    s_t)\,p(s_{t+1}\mid s_t,a_t).\qquad{(7)}</annotation></semantics>
- en: 'Across a finite episode with horizon <semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics>,
    the goal of an RL agent is to solve the following optimization:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸€ä¸ªå…·æœ‰æœ‰é™èŒƒå›´ <semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics>
    çš„å›åˆä¸­ï¼Œå¼ºåŒ–å­¦ä¹ ä»£ç†çš„ç›®æ ‡æ˜¯è§£å†³ä»¥ä¸‹ä¼˜åŒ–é—®é¢˜ï¼š
- en: <semantics><mrow><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>Ï€</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ğ”¼</mi><mrow><mi>Ï„</mi><mo>âˆ¼</mo><msub><mi>p</mi><mi>Ï€</mi></msub></mrow></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><munderover><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>T</mi><mo>âˆ’</mo><mn>1</mn></mrow></munderover><msup><mi>Î³</mi><mi>t</mi></msup><mi>r</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow><mo>,</mo><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>8</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">J(\pi) = \mathbb{E}_{\tau \sim p_{\pi}} \left[ \sum_{t=0}^{T-1}
    \gamma^t r(s_t, a_t) \right],\qquad{(8)}</annotation></semantics>
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>Ï€</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ğ”¼</mi><mrow><mi>Ï„</mi><mo>âˆ¼</mo><msub><mi>p</mi><mi>Ï€</mi></msub></mrow></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><munderover><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>T</mi><mo>âˆ’</mo><mn>1</mn></mrow></munderover><msup><mi>Î³</mi><mi>t</mi></msup><mi>r</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow><mo>,</mo><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>8</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">J(\pi) = \mathbb{E}_{\tau \sim p_{\pi}} \left[ \sum_{t=0}^{T-1}
    \gamma^t r(s_t, a_t) \right],\qquad{(8)}</annotation></semantics>
- en: For continuing tasks, one often takes <semantics><mrow><mi>T</mi><mo>â†’</mo><mi>âˆ</mi></mrow><annotation
    encoding="application/x-tex">T\to\infty</annotation></semantics> and relies on
    discounting (<semantics><mrow><mi>Î³</mi><mo><</mo><mn>1</mn></mrow><annotation
    encoding="application/x-tex">\gamma<1</annotation></semantics>) to keep the objective
    well-defined. <semantics><mi>Î³</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics>
    is a discount factor from 0 to 1 that balances the desirability of near- versus
    future-rewards. Multiple methods for optimizing this expression are discussed
    in Chapter 11.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæŒç»­ä»»åŠ¡ï¼Œé€šå¸¸å– <semantics><mrow><mi>T</mi><mo>â†’</mo><mi>âˆ</mi></mrow><annotation
    encoding="application/x-tex">T\to\infty</annotation></semantics> å¹¶ä¾èµ–äºæŠ˜ç° (<semantics><mrow><mi>Î³</mi><mo><</mo><mn>1</mn></mrow><annotation
    encoding="application/x-tex">\gamma<1</annotation></semantics>) ä»¥ä¿æŒç›®æ ‡å®šä¹‰è‰¯å¥½ã€‚ <semantics><mi>Î³</mi><annotation
    encoding="application/x-tex">\gamma</annotation></semantics> æ˜¯ä¸€ä¸ªä»‹äº0åˆ°1ä¹‹é—´çš„æŠ˜æ‰£å› å­ï¼Œç”¨äºå¹³è¡¡è¿‘æœŸä¸æœªæ¥å¥–åŠ±çš„å¸å¼•åŠ›ã€‚ç¬¬11ç« è®¨è®ºäº†ä¼˜åŒ–æ­¤è¡¨è¾¾å¼çš„å¤šç§æ–¹æ³•ã€‚
- en: '![Figure 2: Standard RL loop](../media/file1.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾2ï¼šæ ‡å‡†å¼ºåŒ–å­¦ä¹ å¾ªç¯](../media/file1.png)'
- en: 'Figure 2: Standard RL loop'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2ï¼šæ ‡å‡†å¼ºåŒ–å­¦ä¹ å¾ªç¯
- en: A standard illustration of the RL loop is shown in fig.Â [2](#fig:rl) and (compare
    this to the RLHF loop in fig.Â [3](#fig:rlhf)).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ å¾ªç¯çš„æ ‡å‡†ç¤ºæ„å›¾å¦‚å›¾[2](#fig:rl)æ‰€ç¤ºï¼Œå¹¶ä¸å›¾[3](#fig:rlhf)ä¸­çš„RLHFå¾ªç¯è¿›è¡Œæ¯”è¾ƒã€‚
- en: 'Example RL Task: CartPole'
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ï¼šCartPole
- en: To make the transition function concrete, consider the classic *CartPole* (inverted
    pendulum) control task.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä½¿è½¬æ¢å‡½æ•°å…·ä½“åŒ–ï¼Œè€ƒè™‘ç»å…¸çš„*CartPole*ï¼ˆå€’ç«‹æ‘†ï¼‰æ§åˆ¶ä»»åŠ¡ã€‚
- en: '**State (<semantics><msub><mi>s</mi><mi>t</mi></msub><annotation encoding="application/x-tex">s_t</annotation></semantics>)**:
    the cart position/velocity and pole angle/angular velocity,'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**çŠ¶æ€ (<semantics><msub><mi>s</mi><mi>t</mi></msub><annotation encoding="application/x-tex">s_t</annotation></semantics>)**:
    å°è½¦çš„ä½ç½®/é€Ÿåº¦å’Œæ†çš„è§’åº¦/è§’é€Ÿåº¦ã€‚'
- en: <semantics><mrow><msub><mi>s</mi><mi>t</mi></msub><mo>=</mo><mo stretchy="false"
    form="prefix">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>,</mo><msub><mover><mi>x</mi><mo
    accent="true">Ì‡</mo></mover><mi>t</mi></msub><mo>,</mo><msub><mi>Î¸</mi><mi>t</mi></msub><mo>,</mo><msub><mover><mi>Î¸</mi><mo
    accent="true">Ì‡</mo></mover><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mi>.</mi></mrow><annotation
    encoding="application/x-tex">s_t = (x_t,\,\dot{x}_t,\,\theta_t,\,\dot{\theta}_t).</annotation></semantics>
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <semantics><mrow><msub><mi>s</mi><mi>t</mi></msub><mo>=</mo><mo stretchy="false"
    form="prefix">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>,</mo><msub><mover><mi>x</mi><mo
    accent="true">Ì‡</mo></mover><mi>t</mi></msub><mo>,</mo><msub><mi>Î¸</mi><mi>t</mi></msub><mo>,</mo><msub><mover><mi>Î¸</mi><mo
    accent="true">Ì‡</mo></mover><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mi>.</mi></mrow><annotation
    encoding="application/x-tex">s_t = (x_t,\,\dot{x}_t,\,\theta_t,\,\dot{\theta}_t).</annotation></semantics>
- en: '**Action (<semantics><msub><mi>a</mi><mi>t</mi></msub><annotation encoding="application/x-tex">a_t</annotation></semantics>)**:
    apply a left/right horizontal force to the cart, e.g.Â <semantics><mrow><msub><mi>a</mi><mi>t</mi></msub><mo>âˆˆ</mo><mo
    stretchy="false" form="prefix">{</mo><mi>âˆ’</mi><mi>F</mi><mo>,</mo><mi>+</mi><mi>F</mi><mo
    stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">a_t
    \in \{-F, +F\}</annotation></semantics>.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**åŠ¨ä½œ (<semantics><msub><mi>a</mi><mi>t</mi></msub><annotation encoding="application/x-tex">a_t</annotation></semantics>)**:
    å¯¹å°è½¦æ–½åŠ å·¦å³æ°´å¹³åŠ›ï¼Œä¾‹å¦‚ <semantics><mrow><msub><mi>a</mi><mi>t</mi></msub><mo>âˆˆ</mo><mo
    stretchy="false" form="prefix">{</mo><mi>âˆ’</mi><mi>F</mi><mo>,</mo><mi>+</mi><mi>F</mi><mo
    stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">a_t
    \in \{-F, +F\}</annotation></semantics>.'
- en: '**Reward (<semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics>)**:
    a simple reward is <semantics><mrow><msub><mi>r</mi><mi>t</mi></msub><mo>=</mo><mn>1</mn></mrow><annotation
    encoding="application/x-tex">r_t = 1</annotation></semantics> each step the pole
    remains balanced and the cart stays on the track (e.g.Â <semantics><mrow><mo stretchy="false"
    form="prefix">|</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy="false" form="prefix">|</mo><mo>â‰¤</mo><mn>2.4</mn></mrow><annotation
    encoding="application/x-tex">|x_t| \le 2.4</annotation></semantics> and <semantics><mrow><mo
    stretchy="false" form="prefix">|</mo><msub><mi>Î¸</mi><mi>t</mi></msub><mo stretchy="false"
    form="prefix">|</mo><mo>â‰¤</mo><msup><mn>12</mn><mo>âˆ˜</mo></msup></mrow><annotation
    encoding="application/x-tex">|\theta_t| \le 12^\circ</annotation></semantics>),
    and the episode terminates when either bound is violated.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¥–åŠ± (<semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics>)**:
    ç®€å•çš„å¥–åŠ±æ˜¯ <semantics><mrow><msub><mi>r</mi><mi>t</mi></msub><mo>=</mo><mn>1</mn></mrow><annotation
    encoding="application/x-tex">r_t = 1</annotation></semantics>ï¼Œæ¯æ¬¡æ†ä¿æŒå¹³è¡¡ä¸”å°è½¦ä¿æŒåœ¨è½¨é“ä¸Šï¼ˆä¾‹å¦‚ï¼Œ<semantics><mrow><mo
    stretchy="false" form="prefix">|</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy="false"
    form="prefix">|</mo><mo>â‰¤</mo><mn>2.4</mn></mrow><annotation encoding="application/x-tex">|x_t|
    \le 2.4</annotation></semantics> å’Œ <semantics><mrow><mo stretchy="false" form="prefix">|</mo><msub><mi>Î¸</mi><mi>t</mi></msub><mo
    stretchy="false" form="prefix">|</mo><mo>â‰¤</mo><msup><mn>12</mn><mo>âˆ˜</mo></msup></mrow><annotation
    encoding="application/x-tex">|\theta_t| \le 12^\circ</annotation></semantics>ï¼‰ï¼Œå½“ä»»ä¸€è¾¹ç•Œè¢«è¿åæ—¶ï¼Œå›åˆç»“æŸã€‚'
- en: '**Dynamics / transition (<semantics><mrow><mi>p</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>âˆ£</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">p(s_{t+1}\mid
    s_t,a_t)</annotation></semantics>)**: in many environments the dynamics are deterministic
    (so <semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics>
    is a point mass) and can be written as <semantics><mrow><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>f</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">s_{t+1}
    = f(s_t,a_t)</annotation></semantics> via Euler integration with step size <semantics><mrow><mi
    mathvariant="normal">Î”</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">\Delta
    t</annotation></semantics>. A standard simplified CartPole update uses constants
    cart mass <semantics><msub><mi>m</mi><mi>c</mi></msub><annotation encoding="application/x-tex">m_c</annotation></semantics>,
    pole mass <semantics><msub><mi>m</mi><mi>p</mi></msub><annotation encoding="application/x-tex">m_p</annotation></semantics>,
    pole half-length <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics>,
    and gravity <semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics>:'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**åŠ¨åŠ›å­¦/è½¬æ¢ (<semantics><mrow><mi>p</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>âˆ£</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">p(s_{t+1}\mid
    s_t,a_t)</annotation></semantics>)**: åœ¨è®¸å¤šç¯å¢ƒä¸­ï¼ŒåŠ¨åŠ›å­¦æ˜¯ç¡®å®šæ€§çš„ï¼ˆå› æ­¤ <semantics><mi>p</mi><annotation
    encoding="application/x-tex">p</annotation></semantics> æ˜¯ä¸€ä¸ªç‚¹è´¨é‡ï¼‰å¹¶ä¸”å¯ä»¥å†™æˆ <semantics><mrow><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>f</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">s_{t+1}
    = f(s_t,a_t)</annotation></semantics> é€šè¿‡æ¬§æ‹‰ç§¯åˆ†ï¼Œæ­¥é•¿ä¸º <semantics><mrow><mi mathvariant="normal">Î”</mi><mi>t</mi></mrow><annotation
    encoding="application/x-tex">\Delta t</annotation></semantics>. ä¸€ä¸ªæ ‡å‡†çš„ç®€åŒ–CartPoleæ›´æ–°ä½¿ç”¨å¸¸æ•°çš„è½¦ä½“è´¨é‡
    <semantics><msub><mi>m</mi><mi>c</mi></msub><annotation encoding="application/x-tex">m_c</annotation></semantics>ï¼Œæ†è´¨é‡
    <semantics><msub><mi>m</mi><mi>p</mi></msub><annotation encoding="application/x-tex">m_p</annotation></semantics>ï¼Œæ†åŠé•¿åº¦
    <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics>ï¼Œå’Œé‡åŠ›
    <semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics>ï¼š'
- en: <semantics><mrow><mtext mathvariant="normal">temp</mtext><mo>=</mo><mfrac><mrow><msub><mi>a</mi><mi>t</mi></msub><mo>+</mo><msub><mi>m</mi><mi>p</mi></msub><mi>l</mi><msubsup><mover><mi>Î¸</mi><mo
    accent="true">Ì‡</mo></mover><mi>t</mi><mn>2</mn></msubsup><mrow><mi mathvariant="normal">sin</mi><mo>â¡</mo></mrow><msub><mi>Î¸</mi><mi>t</mi></msub></mrow><mrow><msub><mi>m</mi><mi>c</mi></msub><mo>+</mo><msub><mi>m</mi><mi>p</mi></msub></mrow></mfrac></mrow><annotation
    encoding="application/x-tex">\text{temp} = \frac{a_t + m_p l\,\dot{\theta}_t^2\sin\theta_t}{m_c
    + m_p}</annotation></semantics>
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <semantics><mrow><mtext mathvariant="normal">temp</mtext><mo>=</mo><mfrac><mrow><msub><mi>a</mi><mi>t</mi></msub><mo>+</mo><msub><mi>m</mi><mi>p</mi></msub><mi>l</mi><msubsup><mover><mi>Î¸</mi><mo
    accent="true">Ì‡</mo></mover><mi>t</mi><mn>2</mn></msubsup><mrow><mi mathvariant="normal">sin</mi><mo>â¡</mo></mrow><msub><mi>Î¸</mi><mi>t</mi></msub></mrow><mrow><msub><mi>m</mi><mi>c</mi></msub><mo>+</mo><msub><mi>m</mi><mi>p</mi></msub></mrow></mfrac></mrow><annotation
    encoding="application/x-tex">\text{temp} = \frac{a_t + m_p l\,\dot{\theta}_t^2\sin\theta_t}{m_c
    + m_p}</annotation></semantics>
- en: <semantics><mrow><msub><mover><mi>Î¸</mi><mo accent="true">Ìˆ</mo></mover><mi>t</mi></msub><mo>=</mo><mfrac><mrow><mi>g</mi><mrow><mi
    mathvariant="normal">sin</mi><mo>â¡</mo></mrow><msub><mi>Î¸</mi><mi>t</mi></msub><mo>âˆ’</mo><mrow><mi
    mathvariant="normal">cos</mi><mo>â¡</mo></mrow><msub><mi>Î¸</mi><mi>t</mi></msub><mtext
    mathvariant="normal">temp</mtext></mrow><mrow><mi>l</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mstyle displaystyle="false"><mfrac><mn>4</mn><mn>3</mn></mfrac></mstyle><mo>âˆ’</mo><mfrac><mrow><msub><mi>m</mi><mi>p</mi></msub><msup><mrow><mi
    mathvariant="normal">cos</mi><mo>â¡</mo></mrow><mn>2</mn></msup><msub><mi>Î¸</mi><mi>t</mi></msub></mrow><mrow><msub><mi>m</mi><mi>c</mi></msub><mo>+</mo><msub><mi>m</mi><mi>p</mi></msub></mrow></mfrac><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac></mrow><annotation
    encoding="application/x-tex">\ddot{\theta}_t = \frac{g\sin\theta_t - \cos\theta_t\,\text{temp}}{l\left(\tfrac{4}{3}
    - \frac{m_p\cos^2\theta_t}{m_c + m_p}\right)}</annotation></semantics>
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <semantics><mrow><msub><mover><mi>x</mi><mo accent="true">Ìˆ</mo></mover><mi>t</mi></msub><mo>=</mo><mtext
    mathvariant="normal">temp</mtext><mo>âˆ’</mo><mfrac><mrow><msub><mi>m</mi><mi>p</mi></msub><mi>l</mi><msub><mover><mi>Î¸</mi><mo
    accent="true">Ìˆ</mo></mover><mi>t</mi></msub><mrow><mi mathvariant="normal">cos</mi><mo>â¡</mo></mrow><msub><mi>Î¸</mi><mi>t</mi></msub></mrow><mrow><msub><mi>m</mi><mi>c</mi></msub><mo>+</mo><msub><mi>m</mi><mi>p</mi></msub></mrow></mfrac></mrow><annotation
    encoding="application/x-tex">\ddot{x}_t = \text{temp} - \frac{m_p l\,\ddot{\theta}_t\cos\theta_t}{m_c
    + m_p}</annotation></semantics>
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <semantics><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>x</mi><mi>t</mi></msub><mo>+</mo><mi
    mathvariant="normal">Î”</mi><mi>t</mi><msub><mover><mi>x</mi><mo accent="true">Ì‡</mo></mover><mi>t</mi></msub><mo>,</mo><msub><mover><mi>x</mi><mo
    accent="true">Ì‡</mo></mover><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mover><mi>x</mi><mo
    accent="true">Ì‡</mo></mover><mi>t</mi></msub><mo>+</mo><mi mathvariant="normal">Î”</mi><mi>t</mi><msub><mover><mi>x</mi><mo
    accent="true">Ìˆ</mo></mover><mi>t</mi></msub><mo>,</mo></mrow><annotation encoding="application/x-tex">x_{t+1}=x_t+\Delta
    t\,\dot{x}_t,\quad \dot{x}_{t+1}=\dot{x}_t+\Delta t\,\ddot{x}_t,</annotation></semantics>
    <semantics><mrow><msub><mi>Î¸</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>Î¸</mi><mi>t</mi></msub><mo>+</mo><mi
    mathvariant="normal">Î”</mi><mi>t</mi><msub><mover><mi>Î¸</mi><mo accent="true">Ì‡</mo></mover><mi>t</mi></msub><mo>,</mo><msub><mover><mi>Î¸</mi><mo
    accent="true">Ì‡</mo></mover><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mover><mi>Î¸</mi><mo
    accent="true">Ì‡</mo></mover><mi>t</mi></msub><mo>+</mo><mi mathvariant="normal">Î”</mi><mi>t</mi><msub><mover><mi>Î¸</mi><mo
    accent="true">Ìˆ</mo></mover><mi>t</mi></msub><mi>.</mi></mrow><annotation encoding="application/x-tex">\theta_{t+1}=\theta_t+\Delta
    t\,\dot{\theta}_t,\quad \dot{\theta}_{t+1}=\dot{\theta}_t+\Delta t\,\ddot{\theta}_t.</annotation></semantics>
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <semantics><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>x</mi><mi>t</mi></msub><mo>+</mo><mi
    mathvariant="normal">Î”</mi><mi>t</mi><msub><mover><mi>x</mi><mo accent="true">Ì‡</mo></mover><mi>t</mi></msub><mo>,</mo><msub><mover><mi>x</mi><mo
    accent="true">Ì‡</mo></mover><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mover><mi>x</mi><mo
    accent="true">Ì‡</mo></mover><mi>t</mi></msub><mo>+</mo><mi mathvariant="normal">Î”</mi><mi>t</mi><msub><mover><mi>x</mi><mo
    accent="true">Ìˆ</mo></mover><mi>t</mi></msub><mo>,</mo></mrow><annotation encoding="application/x-tex">x_{t+1}=x_t+\Delta
    t\,\dot{x}_t,\quad \dot{x}_{t+1}=\dot{x}_t+\Delta t\,\ddot{x}_t,</annotation></semantics>
    <semantics><mrow><msub><mi>Î¸</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>Î¸</mi><mi>t</mi></msub><mo>+</mo><mi
    mathvariant="normal">Î”</mi><mi>t</mi><msub><mover><mi>Î¸</mi><mo accent="true">Ì‡</mo></mover><mi>t</mi></msub><mo>,</mo><msub><mover><mi>Î¸</mi><mo
    accent="true">Ì‡</mo></mover><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mover><mi>Î¸</mi><mo
    accent="true">Ì‡</mo></mover><mi>t</mi></msub><mo>+</mo><mi mathvariant="normal">Î”</mi><mi>t</mi><msub><mover><mi>Î¸</mi><mo
    accent="true">Ìˆ</mo></mover><mi>t</mi></msub><mi>.</mi></mrow><annotation encoding="application/x-tex">\theta_{t+1}=\theta_t+\Delta
    t\,\dot{\theta}_t,\quad \dot{\theta}_{t+1}=\dot{\theta}_t+\Delta t\,\ddot{\theta}_t.</annotation></semantics>
- en: 'This is a concrete instance of the general setup above: the policy chooses
    <semantics><msub><mi>a</mi><mi>t</mi></msub><annotation encoding="application/x-tex">a_t</annotation></semantics>,
    the transition function advances the state, and the reward is accumulated over
    the episode.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€èˆ¬è®¾ç½®çš„ä¸€ä¸ªå…·ä½“å®ä¾‹ï¼šç­–ç•¥é€‰æ‹©<semantics><msub><mi>a</mi><mi>t</mi></msub><annotation encoding="application/x-tex">a_t</annotation></semantics>ï¼ŒçŠ¶æ€è½¬ç§»å‡½æ•°æ¨è¿›çŠ¶æ€ï¼Œå¥–åŠ±åœ¨å›åˆä¸­ç´¯ç§¯ã€‚
- en: Manipulating the Standard RL Setup
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ“ä½œæ ‡å‡†å¼ºåŒ–å­¦ä¹ è®¾ç½®
- en: 'The RL formulation for RLHF is seen as a less open-ended problem, where a few
    key pieces of RL are set to specific definitions in order to accommodate language
    models. There are multiple core changes from the standard RL setup to that of
    RLHF: Table tbl.Â [1](ch004.xhtml#tbl:rl-vs-rlhf) summarizes these differences
    between standard RL and the RLHF setup used for language models.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºRLHFçš„å¼ºåŒ–å­¦ä¹ å…¬å¼è¢«è§†ä¸ºä¸€ä¸ªæ›´ä¸å¼€æ”¾çš„é—®é¢˜ï¼Œå…¶ä¸­ä¸€äº›å…³é”®çš„å¼ºåŒ–å­¦ä¹ å…ƒç´ è¢«è®¾å®šä¸ºç‰¹å®šçš„å®šä¹‰ï¼Œä»¥ä¾¿é€‚åº”è¯­è¨€æ¨¡å‹ã€‚ä»æ ‡å‡†å¼ºåŒ–å­¦ä¹ è®¾ç½®åˆ°RLHFè®¾ç½®æœ‰å¤šä¸ªæ ¸å¿ƒå˜åŒ–ï¼šè¡¨tbl.Â [1](ch004.xhtml#tbl:rl-vs-rlhf)æ€»ç»“äº†æ ‡å‡†å¼ºåŒ–å­¦ä¹ å’Œç”¨äºè¯­è¨€æ¨¡å‹çš„RLHFè®¾ç½®ä¹‹é—´çš„è¿™äº›å·®å¼‚ã€‚
- en: '**Switching from a reward function to a reward model.** In RLHF, a learned
    model of human preferences, <semantics><mrow><msub><mi>r</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">r_\theta(s_t,
    a_t)</annotation></semantics> (or any other classification model) is used instead
    of an environmental reward function. This gives the designer a substantial increase
    in the flexibility of the approach and control over the final results, but at
    the cost of implementation complexity. In standard RL, the reward is seen as a
    static piece of the environment that cannot be changed or manipulated by the person
    designing the learning agent.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ä»å¥–åŠ±å‡½æ•°åˆ‡æ¢åˆ°å¥–åŠ±æ¨¡å‹ã€‚** åœ¨RLHFä¸­ï¼Œä¸€ä¸ªå­¦ä¹ åˆ°çš„äººç±»åå¥½çš„æ¨¡å‹ï¼Œ<semantics><mrow><msub><mi>r</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">r_\theta(s_t,
    a_t)</annotation></semantics>ï¼ˆæˆ–ä»»ä½•å…¶ä»–åˆ†ç±»æ¨¡å‹ï¼‰è¢«ç”¨æ¥ä»£æ›¿ç¯å¢ƒå¥–åŠ±å‡½æ•°ã€‚è¿™ä¸ºè®¾è®¡è€…æä¾›äº†æ–¹æ³•çµæ´»æ€§å’Œå¯¹æœ€ç»ˆç»“æœçš„æ§åˆ¶çš„æ˜¾è‘—æå‡ï¼Œä½†ä»£ä»·æ˜¯å®æ–½å¤æ‚æ€§çš„å¢åŠ ã€‚åœ¨æ ‡å‡†RLä¸­ï¼Œå¥–åŠ±è¢«è§†ä¸ºç¯å¢ƒçš„ä¸€ä¸ªé™æ€éƒ¨åˆ†ï¼Œè®¾è®¡å­¦ä¹ ä»£ç†çš„äººæ— æ³•æ”¹å˜æˆ–æ“çºµå®ƒã€‚'
- en: '**No state transitions exist.** In RLHF, the initial states for the domain
    are prompts sampled from a training dataset and the â€œactionâ€ is the completion
    to said prompt. During standard practices, this action does not impact the next
    state and is only scored by the reward model.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ²¡æœ‰çŠ¶æ€è½¬æ¢ã€‚** åœ¨RLHFä¸­ï¼Œé¢†åŸŸçš„åˆå§‹çŠ¶æ€æ˜¯ä»è®­ç»ƒæ•°æ®é›†ä¸­é‡‡æ ·çš„æç¤ºï¼Œè€Œâ€œåŠ¨ä½œâ€æ˜¯å¯¹è¯¥æç¤ºçš„å®Œæˆã€‚åœ¨æ ‡å‡†å®è·µä¸­ï¼Œè¿™ä¸ªåŠ¨ä½œä¸ä¼šå½±å“ä¸‹ä¸€ä¸ªçŠ¶æ€ï¼Œå¹¶ä¸”åªç”±å¥–åŠ±æ¨¡å‹è¯„åˆ†ã€‚'
- en: '**Response level rewards.** Often referred to as a bandit problem, RLHF attribution
    of reward is done for an entire sequence of actions, composed of multiple generated
    tokens, rather than in a fine-grained manner.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å“åº”çº§åˆ«çš„å¥–åŠ±ã€‚** é€šå¸¸è¢«ç§°ä¸ºæŠ•å¸æœºé—®é¢˜ï¼ŒRLHFçš„å¥–åŠ±åˆ†é…æ˜¯å¯¹æ•´ä¸ªåŠ¨ä½œåºåˆ—è¿›è¡Œçš„ï¼Œç”±å¤šä¸ªç”Ÿæˆçš„æ ‡è®°ç»„æˆï¼Œè€Œä¸æ˜¯ä»¥ç»†ç²’åº¦çš„æ–¹å¼è¿›è¡Œã€‚'
- en: 'Table 1: Key differences between standard RL and RLHF for language models.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨1ï¼šæ ‡å‡†RLå’ŒRLHFï¼ˆè¯­è¨€æ¨¡å‹ï¼‰ä¹‹é—´çš„å…³é”®å·®å¼‚ã€‚
- en: '| Aspect | Standard RL | RLHF (language models) |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| æ–¹é¢ | æ ‡å‡†RL | RLHFï¼ˆè¯­è¨€æ¨¡å‹ï¼‰ |'
- en: '| --- | --- | --- |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Reward signal | Environment reward function <semantics><mrow><mi>r</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">r(s_t,a_t)</annotation></semantics>
    | Learned reward / preference model <semantics><mrow><msub><mi>r</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">r_\theta(x,y)</annotation></semantics>
    (prompt <semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics>,
    completion <semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics>)
    |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| å¥–åŠ±ä¿¡å· | ç¯å¢ƒå¥–åŠ±å‡½æ•° <semantics><mrow><mi>r</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">r(s_t,a_t)</annotation></semantics>
    | å­¦ä¹ åˆ°çš„å¥–åŠ±/åå¥½æ¨¡å‹ <semantics><mrow><msub><mi>r</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">r_\theta(x,y)</annotation></semantics>ï¼ˆæç¤º <semantics><mi>x</mi><annotation
    encoding="application/x-tex">x</annotation></semantics>ï¼Œå®Œæˆ <semantics><mi>y</mi><annotation
    encoding="application/x-tex">y</annotation></semantics>ï¼‰ |'
- en: '| State transition | Yes: dynamics <semantics><mrow><mi>p</mi><mo stretchy="false"
    form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>âˆ£</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">p(s_{t+1}\mid
    s_t,a_t)</annotation></semantics> | Typically no: prompts <semantics><mi>x</mi><annotation
    encoding="application/x-tex">x</annotation></semantics> sampled from a dataset;
    the completion does not define the next prompt |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| çŠ¶æ€è½¬æ¢ | æ˜¯ï¼šåŠ¨æ€ <semantics><mrow><mi>p</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>âˆ£</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">p(s_{t+1}\mid
    s_t,a_t)</annotation></semantics> | é€šå¸¸æ²¡æœ‰ï¼šä»æ•°æ®é›†ä¸­é‡‡æ ·çš„æç¤º <semantics><mi>x</mi><annotation
    encoding="application/x-tex">x</annotation></semantics>ï¼›å®Œæˆä¸å®šä¹‰ä¸‹ä¸€ä¸ªæç¤º |'
- en: '| Action | Single environment action <semantics><msub><mi>a</mi><mi>t</mi></msub><annotation
    encoding="application/x-tex">a_t</annotation></semantics> | A completion <semantics><mi>y</mi><annotation
    encoding="application/x-tex">y</annotation></semantics> (a sequence of tokens)
    sampled from <semantics><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><mi>â‹…</mi><mo>âˆ£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">\pi_\theta(\cdot\mid x)</annotation></semantics>
    |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| åŠ¨ä½œ | å•ä¸ªç¯å¢ƒåŠ¨ä½œ <semantics><msub><mi>a</mi><mi>t</mi></msub><annotation encoding="application/x-tex">a_t</annotation></semantics>
    | ä¸€ä¸ªå®Œæˆé¡¹ <semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics>ï¼ˆä¸€ä¸ªæ ‡è®°åºåˆ—ï¼‰ï¼Œä»
    <semantics><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>â‹…</mi><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\pi_\theta(\cdot\mid
    x)</annotation></semantics> |'
- en: '| Reward granularity | Often per-step / fine-grained | Usually response-level
    (bandit-style) over the full completion |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| å¥–åŠ±ç²’åº¦ | é€šå¸¸æ¯æ­¥/ç»†ç²’åº¦ | é€šå¸¸åœ¨æ•´ä¸ªå®Œæˆé¡¹ä¸Šï¼ˆç±»ä¼¼æŠ•å¸æœºé£æ ¼ï¼‰ |'
- en: '| Horizon | Multi-step episode (<semantics><mrow><mi>T</mi><mo>></mo><mn>1</mn></mrow><annotation
    encoding="application/x-tex">T>1</annotation></semantics>) | Often single-step
    (<semantics><mrow><mi>T</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">T=1</annotation></semantics>),
    though multi-turn can be modeled as longer-horizon |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| è§†é‡ | å¤šæ­¥å‰§é›†ï¼ˆ<semantics><mrow><mi>T</mi><mo>></mo><mn>1</mn></mrow><annotation
    encoding="application/x-tex">T>1</annotation></semantics>ï¼‰ | é€šå¸¸å•æ­¥ï¼ˆ<semantics><mrow><mi>T</mi><mo>=</mo><mn>1</mn></mrow><annotation
    encoding="application/x-tex">T=1</annotation></semantics>ï¼‰ï¼Œå°½ç®¡å¤šè½®å¯ä»¥å»ºæ¨¡ä¸ºæ›´é•¿çš„è§†é‡ |'
- en: 'Given the single-turn nature of the problem, the optimization can be re-written
    without the time horizon and discount factor (and the reward models): <semantics><mrow><mi>J</mi><mo
    stretchy="false" form="prefix">(</mo><mi>Ï€</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ğ”¼</mi><mrow><mi>Ï„</mi><mo>âˆ¼</mo><mi>Ï€</mi></mrow></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow><mi>.</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>9</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">J(\pi) = \mathbb{E}_{\tau \sim \pi} \left[r_\theta(s_t,
    a_t) \right].\qquad{(9)}</annotation></semantics>'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºé—®é¢˜çš„å•è½®æ€§è´¨ï¼Œä¼˜åŒ–å¯ä»¥é‡å†™ï¼Œæ— éœ€æ—¶é—´èŒƒå›´å’ŒæŠ˜æ‰£å› å­ï¼ˆä»¥åŠå¥–åŠ±æ¨¡å‹ï¼‰ï¼š<semantics><mrow><mi>J</mi><mo stretchy="false"
    form="prefix">(</mo><mi>Ï€</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ğ”¼</mi><mrow><mi>Ï„</mi><mo>âˆ¼</mo><mi>Ï€</mi></mrow></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow><mi>.</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>9</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">J(\pi) = \mathbb{E}_{\tau \sim \pi} \left[r_\theta(s_t,
    a_t) \right].\qquad{(9)}</annotation></semantics>
- en: In many ways, the result is that while RLHF is heavily inspired by RL optimizers
    and problem formulations, the actual implementation is very distinct from traditional
    RL.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®¸å¤šæ–¹é¢ï¼Œç»“æœæ˜¯ï¼Œè™½ç„¶RLHFåœ¨RLä¼˜åŒ–å™¨å’Œé—®é¢˜è¡¨è¿°æ–¹é¢å—åˆ°å¾ˆå¤§å¯å‘ï¼Œä½†å®é™…å®ç°ä¸ä¼ ç»ŸRLéå¸¸ä¸åŒã€‚
- en: '![Figure 3: Standard RLHF loop](../media/file2.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾3ï¼šæ ‡å‡†RLHFå¾ªç¯](../media/file2.png)'
- en: 'Figure 3: Standard RLHF loop'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾3ï¼šæ ‡å‡†RLHFå¾ªç¯
- en: Finetuning and Regularization
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¾®è°ƒå’Œæ­£åˆ™åŒ–
- en: 'In traditional RL problems, the agent must learn from a randomly initialized
    policy, but with RLHF, we start from a strong pretrained base model with many
    initial capabilities. This strong prior for RLHF induces a need to control the
    optimization from drifting too far from the initial policy. In order to succeed
    in a finetuning regime, RLHF techniques employ multiple types of regularization
    to control the optimization. The goal is to allow the reward maximization to still
    occur without the model succumbing to over-optimization, as discussed in Chapter
    18\. The most common change to the optimization function is to add a distance
    penalty on the difference between the current RLHF policy and the starting point
    of the optimization:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¼ ç»Ÿçš„RLé—®é¢˜ä¸­ï¼Œæ™ºèƒ½ä½“å¿…é¡»ä»éšæœºåˆå§‹åŒ–çš„ç­–ç•¥ä¸­å­¦ä¹ ï¼Œä½†ä½¿ç”¨RLHFï¼Œæˆ‘ä»¬ä»å…·æœ‰è®¸å¤šåˆå§‹èƒ½åŠ›çš„å¼ºå¤§é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹å¼€å§‹ã€‚è¿™ç§RLHFçš„å¼ºå¤§å…ˆéªŒæ€§å¯¼è‡´éœ€è¦æ§åˆ¶ä¼˜åŒ–ï¼Œä»¥é¿å…ä»åˆå§‹ç­–ç•¥åç¦»å¤ªè¿œã€‚ä¸ºäº†åœ¨å¾®è°ƒæ¨¡å¼ä¸‹å–å¾—æˆåŠŸï¼ŒRLHFæŠ€æœ¯é‡‡ç”¨å¤šç§ç±»å‹çš„æ­£åˆ™åŒ–æ¥æ§åˆ¶ä¼˜åŒ–ã€‚ç›®æ ‡æ˜¯å…è®¸å¥–åŠ±æœ€å¤§åŒ–ä»ç„¶å‘ç”Ÿï¼ŒåŒæ—¶æ¨¡å‹ä¸ä¼šå› è¿‡åº¦ä¼˜åŒ–è€Œå´©æºƒï¼Œæ­£å¦‚ç¬¬18ç« æ‰€è®¨è®ºçš„ã€‚å¯¹ä¼˜åŒ–å‡½æ•°æœ€å¸¸è§çš„å˜åŒ–æ˜¯åœ¨å½“å‰RLHFç­–ç•¥ä¸ä¼˜åŒ–èµ·ç‚¹ä¹‹é—´çš„å·®å¼‚ä¸Šæ·»åŠ è·ç¦»æƒ©ç½šï¼š
- en: <semantics><mrow><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>Ï€</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ğ”¼</mi><mrow><mi>Ï„</mi><mo>âˆ¼</mo><mi>Ï€</mi></mrow></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow><mo>âˆ’</mo><mi>Î²</mi><msub><mi>ğ’Ÿ</mi><mtext
    mathvariant="normal">KL</mtext></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>Ï€</mi><mtext
    mathvariant="normal">RL</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>â‹…</mi><mo
    stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false"
    form="postfix">)</mo><mo stretchy="false" form="postfix">âˆ¥</mo><msub><mi>Ï€</mi><mtext
    mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>â‹…</mi><mo
    stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false"
    form="postfix">)</mo><mo stretchy="false" form="postfix">)</mo><mi>.</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>10</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">J(\pi) = \mathbb{E}_{\tau \sim \pi} \left[r_\theta(s_t,
    a_t)\right] - \beta \mathcal{D}_{\text{KL}}(\pi_{\text{RL}}(\cdot|s_t) \| \pi_{\text{ref}}(\cdot|s_t)).\qquad{(10)}</annotation></semantics>
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>Ï€</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ğ”¼</mi><mrow><mi>Ï„</mi><mo>âˆ¼</mo><mi>Ï€</mi></mrow></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow><mo>âˆ’</mo><mi>Î²</mi><msub><mi>ğ’Ÿ</mi><mtext
    mathvariant="normal">KL</mtext></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>Ï€</mi><mtext
    mathvariant="normal">RL</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>â‹…</mi><mo
    stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false"
    form="postfix">)</mo><mo stretchy="false" form="postfix">âˆ¥</mo><msub><mi>Ï€</mi><mtext
    mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>â‹…</mi><mo
    stretchy="false" form="prefix">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false"
    form="postfix">)</mo><mo stretchy="false" form="postfix">)</mo><mi>.</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>10</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">J(\pi) = \mathbb{E}_{\tau \sim \pi} \left[r_\theta(s_t,
    a_t)\right] - \beta \mathcal{D}_{\text{KL}}(\pi_{\text{RL}}(\cdot|s_t) \| \pi_{\text{ref}}(\cdot|s_t)).\qquad{(10)}</annotation></semantics>
- en: Within this formulation, a lot of study into RLHF training goes into understanding
    how to spend a certain â€œKL budgetâ€ as measured by a distance from the initial
    model. For more details, see Chapter 8 on Regularization.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªå…¬å¼ä¸­ï¼Œå¯¹RLHFè®­ç»ƒçš„å¤§é‡ç ”ç©¶éƒ½é›†ä¸­åœ¨ç†è§£å¦‚ä½•èŠ±è´¹ä¸€å®šçš„â€œKLé¢„ç®—â€ï¼Œè¿™æ˜¯é€šè¿‡åˆå§‹æ¨¡å‹è·ç¦»æ¥è¡¡é‡çš„ã€‚æ›´å¤šç»†èŠ‚ï¼Œè¯·å‚é˜…ç¬¬8ç« å…³äºæ­£åˆ™åŒ–çš„å†…å®¹ã€‚
- en: Optimization Tools
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–å·¥å…·
- en: 'In this book, we detail many popular techniques for solving this optimization
    problem. The popular tools of post-training include:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ä¹¦ä¸­ï¼Œæˆ‘ä»¬è¯¦ç»†ä»‹ç»äº†è®¸å¤šè§£å†³æ­¤ä¼˜åŒ–é—®é¢˜çš„æµè¡ŒæŠ€æœ¯ã€‚åŒ…æ‹¬ä»¥ä¸‹åè®­ç»ƒå·¥å…·ï¼š
- en: '**Reward modeling** (Chapter 7): Where a model is trained to capture the signal
    from collected preference data and can then output a scalar reward indicating
    the quality of future text.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¥–åŠ±å»ºæ¨¡**ï¼ˆç¬¬7ç« ï¼‰ï¼šåœ¨æ­¤ï¼Œæ¨¡å‹è¢«è®­ç»ƒä»¥æ•æ‰æ”¶é›†åˆ°çš„åå¥½æ•°æ®ä¸­çš„ä¿¡å·ï¼Œç„¶åå¯ä»¥è¾“å‡ºä¸€ä¸ªè¡¨ç¤ºæœªæ¥æ–‡æœ¬è´¨é‡çš„æ ‡é‡å¥–åŠ±ã€‚'
- en: '**Instruction finetuning** (Chapter 9): A prerequisite to RLHF where models
    are taught the question-answer format used in the majority of language modeling
    interactions today by imitating preselected examples.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æŒ‡ä»¤å¾®è°ƒ**ï¼ˆç¬¬9ç« ï¼‰ï¼šRLHFçš„å…ˆå†³æ¡ä»¶ï¼Œé€šè¿‡æ¨¡ä»¿é¢„é€‰ç¤ºä¾‹ï¼Œæ•™ä¼šæ¨¡å‹ä»Šå¤©å¤§å¤šæ•°è¯­è¨€å»ºæ¨¡äº¤äº’ä¸­ä½¿ç”¨çš„é—®ç­”æ ¼å¼ã€‚'
- en: '**Rejection sampling** (Chapter 10): The most basic RLHF technique where candidate
    completions for instruction finetuning are filtered by a reward model imitating
    human preferences.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ‹’ç»é‡‡æ ·**ï¼ˆç¬¬10ç« ï¼‰ï¼šæœ€åŸºç¡€çš„RLHFæŠ€æœ¯ï¼Œé€šè¿‡æ¨¡ä»¿äººç±»åå¥½çš„å¥–åŠ±æ¨¡å‹æ¥è¿‡æ»¤æŒ‡ä»¤å¾®è°ƒçš„å€™é€‰å®Œæˆã€‚'
- en: '**Policy gradients** (Chapter 11): The reinforcement learning algorithms used
    in the seminal examples of RLHF to update parameters of a language model with
    respect to the signal from a reward model.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç­–ç•¥æ¢¯åº¦**ï¼ˆç¬¬11ç« ï¼‰ï¼šåœ¨RLHFçš„åŸå§‹ç¤ºä¾‹ä¸­ä½¿ç”¨çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œç”¨äºæ ¹æ®å¥–åŠ±æ¨¡å‹çš„ä¿¡å·æ›´æ–°è¯­è¨€æ¨¡å‹çš„å‚æ•°ã€‚'
- en: '**Direct alignment algorithms** (Chapter 12): Algorithms that directly optimize
    a policy from pairwise preference data, rather than learning an intermediate reward
    model to then optimize later.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç›´æ¥å¯¹é½ç®—æ³•**ï¼ˆç¬¬12ç« ï¼‰ï¼šç›´æ¥ä»æˆå¯¹åå¥½æ•°æ®ä¸­ä¼˜åŒ–ç­–ç•¥çš„ç®—æ³•ï¼Œè€Œä¸æ˜¯å­¦ä¹ ä¸€ä¸ªä¸­é—´å¥–åŠ±æ¨¡å‹ç„¶åå†è¿›è¡Œä¼˜åŒ–ã€‚'
- en: Modern RLHF-trained models always utilize instruction finetuning followed by
    a mixture of the other optimization options.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ç°ä»£RLHFè®­ç»ƒçš„æ¨¡å‹æ€»æ˜¯åˆ©ç”¨æŒ‡ä»¤å¾®è°ƒï¼Œç„¶åæ˜¯å…¶ä»–ä¼˜åŒ–é€‰é¡¹çš„æ··åˆã€‚
- en: Canonical Training Recipes
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**æ ‡å‡†è®­ç»ƒæ–¹æ¡ˆ**'
- en: Over time various models have been identified as canonical recipes for RLHF
    specifically or post-training generally. These recipes reflect data practices
    and model abilities at the time. As the recipes age, training models with the
    same characteristics becomes easier and takes fewer data. There is a general trend
    of post-training involving more optimization steps with more training algorithms
    across more diverse training datasets and evaluations.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€æ—¶é—´çš„æ¨ç§»ï¼Œå„ç§æ¨¡å‹è¢«ç¡®å®šä¸ºRLHFç‰¹å®šæˆ–è®­ç»ƒåä¸€èˆ¬çš„æ ‡å‡†é…æ–¹ã€‚è¿™äº›é…æ–¹åæ˜ äº†å½“æ—¶çš„æ•°æ®å®è·µå’Œæ¨¡å‹èƒ½åŠ›ã€‚éšç€é…æ–¹çš„è€åŒ–ï¼Œå…·æœ‰ç›¸åŒç‰¹å¾çš„æ¨¡å‹è®­ç»ƒå˜å¾—æ›´åŠ å®¹æ˜“ï¼Œæ‰€éœ€æ•°æ®æ›´å°‘ã€‚è®­ç»ƒåçš„ä¸€èˆ¬è¶‹åŠ¿æ˜¯æ¶‰åŠæ›´å¤šçš„ä¼˜åŒ–æ­¥éª¤ï¼Œä½¿ç”¨æ›´å¤šçš„è®­ç»ƒç®—æ³•ï¼Œåœ¨æ›´å¤šæ ·åŒ–çš„è®­ç»ƒæ•°æ®é›†å’Œè¯„ä¼°ä¸­è¿›è¡Œã€‚
- en: InstructGPT
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: InstructGPT
- en: 'Around the time ChatGPT first came out, the widely accepted (â€œcanonicalâ€) method
    for post-training an LM had three major steps, with RLHF being the central piece
    [[56]](ch021.xhtml#ref-lambert2022illustrating) [[3]](ch021.xhtml#ref-ouyang2022training)
    [[5]](ch021.xhtml#ref-bai2022training). The three steps taken on top of a â€œbaseâ€
    language model (the next-token prediction model trained on large-scale web text)
    are summarized below in fig.Â [4](#fig:rlhf-basic-repeat):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§çº¦åœ¨ChatGPTé¦–æ¬¡æ¨å‡ºæ—¶ï¼Œå¹¿æ³›æ¥å—çš„ï¼ˆæ ‡å‡†ï¼‰è®­ç»ƒLMçš„æ–¹æ³•æœ‰ä¸‰ä¸ªä¸»è¦æ­¥éª¤ï¼ŒRLHFæ˜¯æ ¸å¿ƒ [[56]](ch021.xhtml#ref-lambert2022illustrating)
    [[3]](ch021.xhtml#ref-ouyang2022training) [[5]](ch021.xhtml#ref-bai2022training)ã€‚åœ¨â€œåŸºç¡€â€è¯­è¨€æ¨¡å‹ï¼ˆåœ¨å¤§è§„æ¨¡ç½‘ç»œæ–‡æœ¬ä¸Šè®­ç»ƒçš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹æ¨¡å‹ï¼‰ä¹‹ä¸Šçš„ä¸‰ä¸ªæ­¥éª¤æ€»ç»“å¦‚ä¸‹å›¾[4](#fig:rlhf-basic-repeat)ã€‚
- en: '**Instruction tuning on ~10K examples**: This teaches the model to follow the
    question-answer format and teaches some basic skills from primarily human-written
    data.'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**åœ¨çº¦10Kä¸ªç¤ºä¾‹ä¸Šè¿›è¡ŒæŒ‡ä»¤è°ƒæ•´**ï¼šè¿™ä½¿æ¨¡å‹å­¦ä¼šéµå¾ªé—®ç­”æ ¼å¼ï¼Œå¹¶ä»ä¸»è¦ç”±äººç±»ç¼–å†™çš„æ•°æ®ä¸­å­¦ä¹ ä¸€äº›åŸºæœ¬æŠ€èƒ½ã€‚'
- en: '**Training a reward model on ~100K pairwise prompts**: This model is trained
    from the instruction-tuned checkpoint and captures the diverse values one wishes
    to model in their final training. The reward model is the optimization target
    for RLHF.'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**åœ¨çº¦100Kä¸ªæˆå¯¹æç¤ºä¸Šè¿›è¡Œå¥–åŠ±æ¨¡å‹è®­ç»ƒ**ï¼šæ­¤æ¨¡å‹ä»æŒ‡ä»¤è°ƒæ•´æ£€æŸ¥ç‚¹è®­ç»ƒè€Œæ¥ï¼Œæ•æ‰äº†åœ¨æœ€ç»ˆè®­ç»ƒä¸­å¸Œæœ›æ¨¡å‹ä½“ç°çš„å¤šæ ·åŒ–ä»·å€¼ã€‚å¥–åŠ±æ¨¡å‹æ˜¯RLHFçš„ä¼˜åŒ–ç›®æ ‡ã€‚'
- en: '**Training the instruction-tuned model with RLHF on another ~100K prompts**:
    The model is optimized against the reward model with a set of prompts that the
    model generates over before receiving ratings.'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ä½¿ç”¨RLHFåœ¨å¦ä¸€ä¸ªçº¦100Kä¸ªæç¤ºä¸Šè®­ç»ƒæŒ‡ä»¤è°ƒæ•´æ¨¡å‹**ï¼šæ¨¡å‹åœ¨æ¥æ”¶è¯„åˆ†ä¹‹å‰ç”Ÿæˆä¸€ç³»åˆ—æç¤ºï¼Œå¹¶é’ˆå¯¹å¥–åŠ±æ¨¡å‹è¿›è¡Œä¼˜åŒ–ã€‚'
- en: Once RLHF was done, the model was ready to be deployed to users. This recipe
    is the foundation of modern RLHF, but recipes have evolved substantially to include
    more stages and more data.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦å®ŒæˆRLHFï¼Œæ¨¡å‹å°±å‡†å¤‡å¥½éƒ¨ç½²ç»™ç”¨æˆ·ã€‚è¿™ä¸ªé…æ–¹æ˜¯ç°ä»£RLHFçš„åŸºç¡€ï¼Œä½†é…æ–¹å·²ç»å¤§å¹…æ¼”å˜ï¼ŒåŒ…æ‹¬æ›´å¤šé˜¶æ®µå’Œæ›´å¤šæ•°æ®ã€‚
- en: '![Figure 4: A rendition of the early, three stage RLHF process with SFT, a
    reward model, and then optimization.](../media/file0.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾4ï¼šæ—©æœŸä¸‰ä¸ªé˜¶æ®µRLHFè¿‡ç¨‹çš„å†ç°ï¼ŒåŒ…æ‹¬SFTã€å¥–åŠ±æ¨¡å‹ä»¥åŠéšåä¼˜åŒ–ã€‚](../media/file0.png)'
- en: 'Figure 4: A rendition of the early, three stage RLHF process with SFT, a reward
    model, and then optimization.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4ï¼šæ—©æœŸä¸‰ä¸ªé˜¶æ®µRLHFè¿‡ç¨‹çš„å†ç°ï¼ŒåŒ…æ‹¬SFTã€å¥–åŠ±æ¨¡å‹ä»¥åŠéšåä¼˜åŒ–ã€‚
- en: TÃ¼lu 3
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TÃ¼lu 3
- en: Modern versions of post-training involve many, many more model versions and
    training stages (i.e.Â well more than the 5 RLHF steps documented for Llama 2 [[44]](ch021.xhtml#ref-touvron2023llama)).
    An example is shown below in fig.Â [5](#fig:rlhf-complex) where the model undergoes
    numerous training iterations before convergence.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ç°ä»£ç‰ˆæœ¬çš„è®­ç»ƒåæ¶‰åŠè®¸å¤šè®¸å¤šæ›´å¤šçš„æ¨¡å‹ç‰ˆæœ¬å’Œè®­ç»ƒé˜¶æ®µï¼ˆå³è¿œè¶…è¿‡Llama 2æ–‡æ¡£ä¸­è®°å½•çš„5ä¸ªRLHFæ­¥éª¤ [[44]](ch021.xhtml#ref-touvron2023llama))ã€‚ä»¥ä¸‹å›¾[5](#fig:rlhf-complex)ä¸­å±•ç¤ºäº†æ¨¡å‹åœ¨æ”¶æ•›å‰è¿›è¡Œå¤šæ¬¡è®­ç»ƒè¿­ä»£çš„ä¸€ä¸ªç¤ºä¾‹ã€‚
- en: '![Figure 5: A rendition of modern post-training with many rounds.](../media/file3.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾5ï¼šå¤šè½®ç°ä»£è®­ç»ƒåçš„å†ç°ã€‚](../media/file3.png)'
- en: 'Figure 5: A rendition of modern post-training with many rounds.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾5ï¼šå¤šè½®ç°ä»£è®­ç»ƒåçš„å†ç°ã€‚
- en: The most complex models trained in this era and onwards have not released full
    details of their training process. Leading models such as ChatGPT or Claude circa
    2025 involve many, iterative rounds of training. This can even include techniques
    that train specialized models and then merge the weights together to get a final
    model capable on many subtasks [[57]](ch021.xhtml#ref-li2022branch) (e.g.Â Cohereâ€™s
    Command A [[58]](ch021.xhtml#ref-cohere2025command)).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªæ—¶ä»£åŠä»¥åè®­ç»ƒçš„æœ€å¤æ‚æ¨¡å‹æ²¡æœ‰å‘å¸ƒå…¶è®­ç»ƒè¿‡ç¨‹çš„å…¨éƒ¨ç»†èŠ‚ã€‚åˆ°2025å¹´å·¦å³ï¼Œé¢†å…ˆçš„æ¨¡å‹å¦‚ChatGPTæˆ–Claudeæ¶‰åŠè®¸å¤šè¿­ä»£è®­ç»ƒè½®æ¬¡ã€‚è¿™ç”šè‡³åŒ…æ‹¬è®­ç»ƒä¸“é—¨æ¨¡å‹çš„æŠ€æœ¯ï¼Œç„¶åå°†æƒé‡åˆå¹¶ä»¥è·å¾—ä¸€ä¸ªåœ¨è®¸å¤šå­ä»»åŠ¡ä¸Šéƒ½æœ‰èƒ½åŠ›çš„æœ€ç»ˆæ¨¡å‹
    [[57]](ch021.xhtml#ref-li2022branch)ï¼ˆä¾‹å¦‚ï¼ŒCohereçš„Command A [[58]](ch021.xhtml#ref-cohere2025command))ã€‚
- en: '![Figure 6: A summary of the TÃ¼lu 3 recipe with target skills and multi-step
    training recipe. Lambert et al.Â 2024, License CC-BY.](../media/file4.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾6ï¼šTÃ¼lu 3é…æ–¹æ€»ç»“ï¼ŒåŒ…æ‹¬ç›®æ ‡æŠ€èƒ½å’Œå¤šæ­¥éª¤è®­ç»ƒé…æ–¹ã€‚Lambertç­‰äººï¼Œ2024å¹´ï¼Œè®¸å¯CC-BYã€‚](../media/file4.png)'
- en: 'Figure 6: A summary of the TÃ¼lu 3 recipe with target skills and multi-step
    training recipe. Lambert et al.Â 2024, License CC-BY.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾6ï¼šTÃ¼lu 3é…æ–¹æ€»ç»“ï¼ŒåŒ…æ‹¬ç›®æ ‡æŠ€èƒ½å’Œå¤šæ­¥éª¤è®­ç»ƒé…æ–¹ã€‚Lambertç­‰äººï¼Œ2024å¹´ï¼Œè®¸å¯CC-BYã€‚
- en: 'A fully open example version of this multi-stage version of post-training where
    RLHF plays a major role is TÃ¼lu 3\. The TÃ¼lu 3 recipe consists of three stages:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªå¤šé˜¶æ®µåè®­ç»ƒçš„å®Œå…¨å¼€æ”¾ç¤ºä¾‹ç‰ˆæœ¬ï¼Œå…¶ä¸­RLHFæ‰®æ¼”ç€ä¸»è¦è§’è‰²ï¼Œå³TÃ¼lu 3ã€‚TÃ¼lu 3é…æ–¹åŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼š
- en: '**Instruction tuning on ~1M examples**: This primarily synthetic data from
    a mix of frontier models such as GPT-4o and Llama 3.1 405B teaches the model general
    instruction following and serves as the foundation of a variety of capabilities
    such as mathematics or coding.'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**åœ¨çº¦1Mä¸ªç¤ºä¾‹ä¸Šè¿›è¡ŒæŒ‡ä»¤è°ƒæ•´**ï¼šè¿™ä¸»è¦æ˜¯æ¥è‡ªGPT-4oå’ŒLlama 3.1 405Bç­‰å‰æ²¿æ¨¡å‹çš„åˆæˆæ•°æ®ï¼Œå®ƒæ•™ä¼šæ¨¡å‹éµå¾ªä¸€èˆ¬æŒ‡ä»¤ï¼Œå¹¶æˆä¸ºæ•°å­¦æˆ–ç¼–ç ç­‰å„ç§èƒ½åŠ›çš„åŸºç¡€ã€‚'
- en: '**On-policy preference data on ~1M preference pairs**: This stage substantially
    boosts the chattiness (e.g.Â ChatBotArena or AlpacaEval 2) of the model while also
    improving skills mentioned above in the instruction tuning stage.'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**çº¦1Mä¸ªåå¥½å¯¹ä¸Šçš„æŒ‰ç­–ç•¥åå¥½æ•°æ®**ï¼šè¿™ä¸€é˜¶æ®µæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å¥è°ˆæ€§ï¼ˆä¾‹å¦‚ChatBotArenaæˆ–AlpacaEval 2ï¼‰ï¼ŒåŒæ—¶ä¹Ÿæ”¹è¿›äº†æŒ‡ä»¤è°ƒæ•´é˜¶æ®µæåˆ°çš„ä¸Šè¿°æŠ€èƒ½ã€‚'
- en: '**Reinforcement Learning with Verifiable Rewards on ~10K prompts**: This stage
    is a small scale reinforcement learning run to boost core skills such as mathematics
    while maintaining overall performance (and is now seen as a precursor to modern
    reasoning models such as DeepSeek R1).'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**åœ¨çº¦10Kä¸ªæç¤ºä¸Šçš„å¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ **ï¼šè¿™ä¸€é˜¶æ®µæ˜¯ä¸€ä¸ªå°è§„æ¨¡çš„å¼ºåŒ–å­¦ä¹ è¿è¡Œï¼Œæ—¨åœ¨æé«˜æ ¸å¿ƒæŠ€èƒ½å¦‚æ•°å­¦ï¼ŒåŒæ—¶ä¿æŒæ•´ä½“æ€§èƒ½ï¼ˆç°åœ¨è¢«è§†ä¸ºç°ä»£æ¨ç†æ¨¡å‹å¦‚DeepSeek
    R1çš„å‰é©±ï¼‰ã€‚'
- en: The recipe has been successfully applied to Llama 3.1 [[6]](ch021.xhtml#ref-lambert2024t),
    OLMo 2 [[59]](ch021.xhtml#ref-olmo20242), and SmolLM models [[60]](ch021.xhtml#ref-alrashed2024smoltulu).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥é…æ–¹å·²æˆåŠŸåº”ç”¨äºLlama 3.1 [[6]](ch021.xhtml#ref-lambert2024t)ã€OLMo 2 [[59]](ch021.xhtml#ref-olmo20242)å’ŒSmolLMæ¨¡å‹[[60]](ch021.xhtml#ref-alrashed2024smoltulu)ã€‚
- en: DeepSeek R1
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DeepSeek R1
- en: 'With the rise of reasoning language models, such as OpenAIâ€™s o1, the best practices
    in post-training evolved again to re-order and redistribute compute across training
    stages. The clearest documentation of a reasoning model post-training recipe is
    DeepSeek R1 [[61]](ch021.xhtml#ref-guo2025deepseek), which has been mirrored by
    Alibabaâ€™s larger Qwen 3 models (i.e.Â only the 32B and 225B MoE models) [[62]](ch021.xhtml#ref-yang2025qwen3)
    or Xiaomiâ€™s MiMo 7B [[63]](ch021.xhtml#ref-xia2025mimo). The DeepSeek recipe follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€æ¨ç†è¯­è¨€æ¨¡å‹ï¼Œå¦‚OpenAIçš„o1çš„å…´èµ·ï¼Œåè®­ç»ƒçš„æœ€ä½³å®è·µå†æ¬¡æ¼”å˜ï¼Œä»¥é‡æ–°æ’åºå’Œé‡æ–°åˆ†é…è®­ç»ƒé˜¶æ®µçš„è®¡ç®—ã€‚æ¨ç†æ¨¡å‹åè®­ç»ƒé…æ–¹æœ€æ¸…æ™°çš„æ–‡æ¡£æ˜¯DeepSeek
    R1 [[61]](ch021.xhtml#ref-guo2025deepseek)ï¼Œå®ƒå·²è¢«é˜¿é‡Œå·´å·´çš„æ›´å¤§å‹çš„Qwen 3æ¨¡å‹ï¼ˆå³ä»…32Bå’Œ225B MoEæ¨¡å‹ï¼‰[[62]](ch021.xhtml#ref-yang2025qwen3)æˆ–å°ç±³çš„MiMo
    7B [[63]](ch021.xhtml#ref-xia2025mimo)æ‰€é•œåƒã€‚DeepSeeké…æ–¹å¦‚ä¸‹ï¼š
- en: '**â€œCold-startâ€ of 100K+ on-policy reasoning samples**: This data is sampled
    from an earlier RL checkpoint, R1-Zero, and heavily filtered to instill a specific
    reasoning process on the model. DeepSeek uses the term cold-start to describe
    how RL is learned from little supervised data.'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**â€œå†·å¯åŠ¨â€100K+çš„æŒ‰ç­–ç•¥æ¨ç†æ ·æœ¬**ï¼šè¿™äº›æ•°æ®æ˜¯ä»æ—©æœŸçš„RLæ£€æŸ¥ç‚¹R1-Zeroä¸­æŠ½å–çš„ï¼Œå¹¶ç»è¿‡å¤§é‡è¿‡æ»¤ï¼Œä»¥åœ¨æ¨¡å‹ä¸ŠçŒè¾“ç‰¹å®šçš„æ¨ç†è¿‡ç¨‹ã€‚DeepSeekä½¿ç”¨å†·å¯åŠ¨è¿™ä¸ªæœ¯è¯­æ¥æè¿°å¦‚ä½•ä»å°‘é‡ç›‘ç£æ•°æ®ä¸­å­¦ä¹ RLã€‚'
- en: '**Large-scale reinforcement learning training**: This stage repeatedly covers
    reasoning problems with the model, running RLVR â€œuntil convergenceâ€ on a variety
    of benchmarks.'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ è®­ç»ƒ**ï¼šè¿™ä¸€é˜¶æ®µåå¤è¦†ç›–æ¨ç†é—®é¢˜ï¼Œåœ¨å¤šä¸ªåŸºå‡†ä¸Šè¿è¡ŒRLVRâ€œç›´åˆ°æ”¶æ•›â€ã€‚'
- en: '**Rejection sampling** on 3/4 reasoning problems and 1/4 general queries to
    start the transition to a general-purpose model.'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ‹’ç»é‡‡æ ·**åœ¨3/4çš„æ¨ç†é—®é¢˜å’Œ1/4çš„ä¸€èˆ¬æŸ¥è¯¢ä¸Šå¼€å§‹å‘é€šç”¨æ¨¡å‹è¿‡æ¸¡ã€‚'
- en: '**Mixed reinforcement learning training** on reasoning problems (verifiable
    rewards) with general preference tuning reward models to polish the model.'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ··åˆå¼ºåŒ–å­¦ä¹ è®­ç»ƒ**é’ˆå¯¹æ¨ç†é—®é¢˜ï¼ˆå¯éªŒè¯å¥–åŠ±ï¼‰ä½¿ç”¨é€šç”¨åå¥½è°ƒæ•´å¥–åŠ±æ¨¡å‹æ¥å®Œå–„æ¨¡å‹ã€‚'
- en: As above, there are evolutions of the recipe, particularly with steps 3 and
    4 to finalize the model before exposing it to users. Many models start with tailored
    instruction datasets with Chain of Thought sequences that are heavily filtered
    and polished from existing models, providing a fast step to strong behaviors with
    SFT alone before moving onto RL [[64]](ch021.xhtml#ref-seed2025seed).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä¸Šæ‰€è¿°ï¼Œé£Ÿè°±æœ‰æ¼”å˜ï¼Œå°¤å…¶æ˜¯åœ¨æ­¥éª¤3å’Œ4ä¸­ï¼Œåœ¨å°†æ¨¡å‹å±•ç¤ºç»™ç”¨æˆ·ä¹‹å‰è¿›è¡Œæœ€ç»ˆåŒ–ã€‚è®¸å¤šæ¨¡å‹ä»å®šåˆ¶çš„æŒ‡ä»¤æ•°æ®é›†å¼€å§‹ï¼Œè¿™äº›æ•°æ®é›†åŒ…å«ç»è¿‡å¤§é‡è¿‡æ»¤å’Œæ‰“ç£¨çš„Chain
    of Thoughtåºåˆ—ï¼Œæ¥è‡ªç°æœ‰æ¨¡å‹ï¼Œä»è€Œé€šè¿‡SFTå•ç‹¬æä¾›å¿«é€Ÿæ­¥éª¤ä»¥è·å¾—å¼ºå¤§çš„è¡Œä¸ºï¼Œç„¶åå†è½¬å‘RL [[64]](ch021.xhtml#ref-seed2025seed)ã€‚
