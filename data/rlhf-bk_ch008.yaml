- en: Regularization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ­£åˆ™åŒ–
- en: Throughout the RLHF optimization, many regularization steps are used to prevent
    over-optimization of the reward model. Over-optimization in these contexts looks
    like models that output nonsensical text. Some examples of optimization â€œoff the
    railsâ€ are that models can output followable math reasoning with extremely incorrect
    answers, repeated text, switching languages, or excessive special characters.
    This chapter covers the different methods thatâ€™re used to control the optimization
    of models.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ•´ä¸ªRLHFä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨äº†è®¸å¤šæ­£åˆ™åŒ–æ­¥éª¤æ¥é˜²æ­¢å¥–åŠ±æ¨¡å‹çš„è¿‡åº¦ä¼˜åŒ–ã€‚åœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œè¿‡åº¦ä¼˜åŒ–çœ‹èµ·æ¥åƒæ˜¯è¾“å‡ºæ— æ„ä¹‰æ–‡æœ¬çš„æ¨¡å‹ã€‚ä¸€äº›ä¼˜åŒ–â€œè„±è½¨â€çš„ä¾‹å­åŒ…æ‹¬æ¨¡å‹å¯ä»¥è¾“å‡ºå¯éµå¾ªçš„æ•°å­¦æ¨ç†ï¼Œä½†ç­”æ¡ˆæç«¯é”™è¯¯ï¼Œé‡å¤æ–‡æœ¬ï¼Œåˆ‡æ¢è¯­è¨€æˆ–ä½¿ç”¨è¿‡å¤šçš„ç‰¹æ®Šå­—ç¬¦ã€‚æœ¬ç« æ¶µç›–äº†ç”¨äºæ§åˆ¶æ¨¡å‹ä¼˜åŒ–çš„ä¸åŒæ–¹æ³•ã€‚
- en: The most popular variant, used in most RLHF implementations at the time of writing,
    is a KL distance from the current policy to a reference policy across generated
    samples. â€œKL distanceâ€ is a colloquial term for expressing the *optimization distance*
    within the training process, even though KL divergenceâ€”the underlying mathematical
    method for measuring the separation of two probability distributionsâ€”does not
    satisfy the formal properties required to be a true distance metric (it is simply
    easier to call the number a distance than a numeric measure of distributional
    difference). Many other regularization techniques have emerged in the literature
    to then disappear in the next model iteration in that line of research. That is
    to say that regularization outside the core KL distance from generations is often
    used to stabilize experimental setups that can then be simplified in the next
    generation. Still, it is important to understand tools to constrain optimization
    in RLHF.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ’°å†™æœ¬æ–‡æ—¶ï¼Œæœ€å—æ¬¢è¿çš„å˜ä½“æ˜¯ä½¿ç”¨KLè·ç¦»æ¥è¡¡é‡å½“å‰ç­–ç•¥ä¸å‚è€ƒç­–ç•¥åœ¨ç”Ÿæˆæ ·æœ¬ä¹‹é—´çš„è·ç¦»ã€‚â€œKLè·ç¦»â€æ˜¯è®­ç»ƒè¿‡ç¨‹ä¸­è¡¨ç¤º*ä¼˜åŒ–è·ç¦»*çš„å£è¯­åŒ–æœ¯è¯­ï¼Œå°½ç®¡KLæ•£åº¦â€”â€”è¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒåˆ†ç¦»çš„æ•°å­¦æ–¹æ³•â€”â€”å¹¶ä¸æ»¡è¶³æˆä¸ºçœŸæ­£è·ç¦»åº¦é‡çš„å½¢å¼å±æ€§ï¼ˆç®€å•åœ°ç§°è¿™ä¸ªæ•°å­—ä¸ºè·ç¦»æ¯”ç§°å…¶ä¸ºåˆ†å¸ƒå·®å¼‚çš„æ•°å€¼åº¦é‡è¦å®¹æ˜“ï¼‰ã€‚æ–‡çŒ®ä¸­å·²ç»å‡ºç°äº†è®¸å¤šå…¶ä»–æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œç„¶ååœ¨ä¸‹ä¸€è½®æ¨¡å‹è¿­ä»£ä¸­æ¶ˆå¤±ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œåœ¨ç”Ÿæˆæ ¸å¿ƒKLè·ç¦»ä¹‹å¤–çš„æ­£åˆ™åŒ–é€šå¸¸ç”¨äºç¨³å®šå®éªŒè®¾ç½®ï¼Œç„¶ååœ¨ä¸‹ä¸€ä»£ä¸­ç®€åŒ–ã€‚å°½ç®¡å¦‚æ­¤ï¼Œäº†è§£çº¦æŸRLHFä¸­ä¼˜åŒ–çš„å·¥å…·ä»ç„¶å¾ˆé‡è¦ã€‚
- en: '*Throughout this chapter, we use <semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics>
    to denote prompts and <semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics>
    to denote completions. This notation is common in the language model literature,
    where methods operate on full prompt-completion pairs rather than individual tokens.*'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨<semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics>æ¥è¡¨ç¤ºæç¤ºï¼Œä½¿ç”¨<semantics><mi>y</mi><annotation
    encoding="application/x-tex">y</annotation></semantics>æ¥è¡¨ç¤ºå®Œæˆã€‚è¿™ç§ç¬¦å·åœ¨è¯­è¨€æ¨¡å‹æ–‡çŒ®ä¸­å¾ˆå¸¸è§ï¼Œå…¶ä¸­æ–¹æ³•ä½œç”¨äºå®Œæ•´çš„æç¤º-å®Œæˆå¯¹ï¼Œè€Œä¸æ˜¯å•ä¸ªæ ‡è®°ã€‚*'
- en: 'The general formulation, when used in an RLHF framework with a reward model,
    <semantics><msub><mi>r</mi><mi>Î¸</mi></msub><annotation encoding="application/x-tex">r_\theta</annotation></semantics>
    is as follows:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: å½“åœ¨å…·æœ‰å¥–åŠ±æ¨¡å‹çš„RLHFæ¡†æ¶ä¸­ä½¿ç”¨æ—¶ï¼Œä¸€èˆ¬å…¬å¼å¦‚ä¸‹ï¼š
- en: <semantics><mrow><mi>r</mi><mo>=</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo>âˆ’</mo><mi>Î»</mi><msub><mi>r</mi><mtext
    mathvariant="normal">reg.</mtext></msub><mrow><mo stretchy="false" form="prefix">(</mo><mn>22</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">r
    = r_\theta - \lambda r_{\text{reg.}} \qquad{(22)}</annotation></semantics>
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><mi>r</mi><mo>=</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo>âˆ’</mo><mi>Î»</mi><msub><mi>r</mi><mtext
    mathvariant="normal">reg.</mtext></msub><mrow><mo stretchy="false" form="prefix">(</mo><mn>22</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">r
    = r_\theta - \lambda r_{\text{reg.}} \qquad{(22)}</annotation></semantics>
- en: 'With the reference implementation being:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: å‚è€ƒå®ç°å¦‚ä¸‹ï¼š
- en: <semantics><mrow><mi>r</mi><mo>=</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo>âˆ’</mo><msub><mi>Î»</mi><mtext
    mathvariant="normal">KL</mtext></msub><msub><mi>ğ’Ÿ</mi><mtext mathvariant="normal">KL</mtext></msub><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>Ï€</mi><mtext mathvariant="normal">RL</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>y</mi><mo>âˆ£</mo><mi>x</mi><mo stretchy="false"
    form="postfix">)</mo><mo stretchy="false" form="postfix">âˆ¥</mo><msub><mi>Ï€</mi><mtext
    mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>23</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">r = r_\theta - \lambda_{\text{KL}} \mathcal{D}_{\text{KL}}
    \left( \pi_{\text{RL}}(y \mid x) \, \| \, \pi_{\text{ref}}(y \mid x) \right) \qquad{(23)}</annotation></semantics>
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><mi>r</mi><mo>=</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo>âˆ’</mo><msub><mi>Î»</mi><mtext
    mathvariant="normal">KL</mtext></msub><msub><mi>ğ’Ÿ</mi><mtext mathvariant="normal">KL</mtext></msub><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>Ï€</mi><mtext mathvariant="normal">RL</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>y</mi><mo>âˆ£</mo><mi>x</mi><mo stretchy="false"
    form="postfix">)</mo><mo stretchy="false" form="postfix">âˆ¥</mo><msub><mi>Ï€</mi><mtext
    mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>23</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">r = r_\theta - \lambda_{\text{KL}} \mathcal{D}_{\text{KL}}
    \left( \pi_{\text{RL}}(y \mid x) \, \| \, \pi_{\text{ref}}(y \mid x) \right) \qquad{(23)}</annotation></semantics>
- en: KL Divergences in RL Optimization
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ä¸­çš„KLæ•£åº¦
- en: 'For mathematical definitions, see Chapter 3 on Problem Setup. Recall that a
    KL divergence measure of probability difference is defined as follows:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ•°å­¦å®šä¹‰ï¼Œè¯·å‚é˜…ç¬¬3ç« å…³äºé—®é¢˜è®¾ç½®çš„å†…å®¹ã€‚å›å¿†ä¸€ä¸‹ï¼ŒKLæ•£åº¦ä½œä¸ºæ¦‚ç‡å·®å¼‚çš„åº¦é‡å®šä¹‰ä¸ºä»¥ä¸‹ï¼š
- en: <semantics><mrow><msub><mi>ğ’Ÿ</mi><mtext mathvariant="normal">KL</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>P</mi><mo stretchy="false" form="prefix">|</mo><mo
    stretchy="false" form="prefix">|</mo><mi>Q</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><munder><mo>âˆ‘</mo><mrow><mi>x</mi><mo>âˆˆ</mo><mi>ğ’³</mi></mrow></munder><mi>P</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mrow><mi
    mathvariant="normal">log</mi><mo>â¡</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>P</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><mi>Q</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo
    stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="false" form="prefix">(</mo><mn>24</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\mathcal{D}_{\text{KL}}(P
    || Q) = \sum_{x \in \mathcal{X}} P(x) \log \left(\frac{P(x)}{Q(x)}\right) \qquad{(24)}</annotation></semantics>
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><msub><mi>ğ’Ÿ</mi><mtext mathvariant="normal">KL</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>P</mi><mo stretchy="false" form="prefix">|</mo><mo
    stretchy="false" form="prefix">|</mo><mi>Q</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><munder><mo>âˆ‘</mo><mrow><mi>x</mi><mo>âˆˆ</mo><mi>ğ’³</mi></mrow></munder><mi>P</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mrow><mi
    mathvariant="normal">log</mi><mo>â¡</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>P</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><mi>Q</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo
    stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="false" form="prefix">(</mo><mn>24</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\mathcal{D}_{\text{KL}}(P
    || Q) = \sum_{x \in \mathcal{X}} P(x) \log \left(\frac{P(x)}{Q(x)}\right) \qquad{(24)}</annotation></semantics>
- en: In RLHF, the two distributions of interest are often the distribution of the
    new model version, say <semantics><mrow><mi>P</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">P(x)</annotation></semantics>,
    and a distribution of the reference policy, say <semantics><mrow><mi>Q</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">Q(x)</annotation></semantics>. Different optimizers
    use different KL directions. Throughout this book, the most common â€œKL Penaltyâ€
    that is used is called the reverse KL to the reference policy. In practice, this
    reduces to a Monte Carlo estimate that samples tokens from the RL model and computes
    probabilities from the reference model. Intuitively, this reverse KL has a numerical
    property that applies a large penalty when the new model, <semantics><mi>P</mi><annotation
    encoding="application/x-tex">P</annotation></semantics> or <semantics><msub><mi>Ï€</mi><mtext
    mathvariant="normal">RL</mtext></msub><annotation encoding="application/x-tex">\pi_{\text{RL}}</annotation></semantics>,
    puts substantial probability mass where the original reference model assigns low
    probability.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨RLHFä¸­ï¼Œæˆ‘ä»¬é€šå¸¸å…³æ³¨çš„ä¸¤ä¸ªåˆ†å¸ƒæ˜¯æ–°æ¨¡å‹ç‰ˆæœ¬çš„åˆ†å¸ƒï¼Œä¾‹å¦‚ <semantics><mrow><mi>P</mi><mo stretchy="false"
    form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">P(x)</annotation></semantics>ï¼Œä»¥åŠå‚è€ƒç­–ç•¥çš„åˆ†å¸ƒï¼Œä¾‹å¦‚ <semantics><mrow><mi>Q</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">Q(x)</annotation></semantics>ã€‚ä¸åŒçš„ä¼˜åŒ–å™¨ä½¿ç”¨ä¸åŒçš„KLæ–¹å‘ã€‚åœ¨è¿™æœ¬ä¹¦ä¸­ï¼Œæœ€å¸¸ç”¨çš„â€œKLæƒ©ç½šâ€æ˜¯ç›¸å¯¹äºå‚è€ƒç­–ç•¥çš„åå‘KLã€‚åœ¨å®è·µä¸­ï¼Œè¿™ç®€åŒ–ä¸ºä»RLæ¨¡å‹ä¸­é‡‡æ ·æ ‡è®°å¹¶ä»å‚è€ƒæ¨¡å‹ä¸­è®¡ç®—æ¦‚ç‡çš„è’™ç‰¹å¡æ´›ä¼°è®¡ã€‚ç›´è§‚ä¸Šï¼Œè¿™ç§åå‘KLå…·æœ‰æ•°å€¼å±æ€§ï¼Œå½“æ–°æ¨¡å‹ï¼Œ<semantics><mi>P</mi><annotation
    encoding="application/x-tex">P</annotation></semantics> æˆ– <semantics><msub><mi>Ï€</mi><mtext
    mathvariant="normal">RL</mtext></msub><annotation encoding="application/x-tex">\pi_{\text{RL}}</annotation></semantics>ï¼Œåœ¨åŸå§‹å‚è€ƒæ¨¡å‹åˆ†é…ä½æ¦‚ç‡çš„åŒºåŸŸèµ‹äºˆå¤§é‡æ¦‚ç‡æ—¶ï¼Œä¼šæ–½åŠ è¾ƒå¤§çš„æƒ©ç½šã€‚
- en: The other KL direction is still often used in ML, e.g.Â in the internal trust
    region calculation of some RL algorithms. This penalty intuitively penalizes the
    new model when its update does *not* apply probability to a high-likelihood region
    in <semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics>
    or <semantics><msub><mi>Ï€</mi><mtext mathvariant="normal">ref</mtext></msub><annotation
    encoding="application/x-tex">\pi_{\text{ref}}</annotation></semantics>. This is
    closer to an objective used for distillation or behavioral cloning.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªKLæ–¹å‘åœ¨MLä¸­ä»ç„¶ç»å¸¸è¢«ä½¿ç”¨ï¼Œä¾‹å¦‚åœ¨ä¸€äº›RLç®—æ³•çš„å†…éƒ¨ä¿¡ä»»åŒºåŸŸè®¡ç®—ä¸­ã€‚è¿™ç§æƒ©ç½šç›´è§‚ä¸Šæƒ©ç½šæ–°æ¨¡å‹ï¼Œå½“å…¶æ›´æ–°åœ¨<semantics><mi>Q</mi><annotation
    encoding="application/x-tex">Q</annotation></semantics> æˆ– <semantics><msub><mi>Ï€</mi><mtext
    mathvariant="normal">ref</mtext></msub><annotation encoding="application/x-tex">\pi_{\text{ref}}</annotation></semantics>ä¸­çš„é«˜æ¦‚ç‡åŒºåŸŸä¸åº”ç”¨æ¦‚ç‡æ—¶ã€‚è¿™æ›´æ¥è¿‘äºç”¨äºè’¸é¦æˆ–è¡Œä¸ºå…‹éš†çš„ç›®æ ‡ã€‚
- en: Reference Model to Generations
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é€‚ç”¨äºå„ä»£ä»£çš„å‚è€ƒæ¨¡å‹
- en: KL penalties are most commonly implemented by comparing the distance between
    the generated tokens during training to a static reference model. The intuition
    is that the model youâ€™re training from has a style that you would like to stay
    close to. This reference model is most often the instruction tuned model, but
    can also be a previous RL checkpoint. With simple substitution, the model we are
    sampling from becomes <semantics><mrow><msub><mi>Ï€</mi><mtext mathvariant="normal">RL</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">\pi_{\text{RL}}(x)</annotation></semantics> and <semantics><mrow><msub><mi>Ï€</mi><mtext
    mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\pi_{\text{ref}}(x)</annotation></semantics>,
    shown above in eq.Â [23](ch008.xhtml#eq:kl_standard) (often <semantics><mi>P</mi><annotation
    encoding="application/x-tex">P</annotation></semantics>, and <semantics><mi>Q</mi><annotation
    encoding="application/x-tex">Q</annotation></semantics>, in standard definitions,
    when applied for RL KL penalties). Such a KL divergence penalty was first applied
    to dialogue agents well before the popularity of large language models [[162]](ch021.xhtml#ref-jaques2017sequence),
    yet KL control was quickly established as a core technique for fine-tuning pretrained
    models [[163]](ch021.xhtml#ref-jaques2020human).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: KLæƒ©ç½šé€šå¸¸é€šè¿‡æ¯”è¾ƒè®­ç»ƒè¿‡ç¨‹ä¸­ç”Ÿæˆçš„æ ‡è®°ä¸é™æ€å‚è€ƒæ¨¡å‹ä¹‹é—´çš„è·ç¦»æ¥å®ç°ã€‚å…¶ç›´è§‰æ˜¯ï¼Œä½ æ­£åœ¨è®­ç»ƒçš„æ¨¡å‹å…·æœ‰ä½ å¸Œæœ›ä¿æŒæ¥è¿‘çš„é£æ ¼ã€‚è¿™ä¸ªå‚è€ƒæ¨¡å‹é€šå¸¸æ˜¯è°ƒæ•´è¿‡çš„æŒ‡ä»¤æ¨¡å‹ï¼Œä½†ä¹Ÿå¯ä»¥æ˜¯ä¹‹å‰çš„RLæ£€æŸ¥ç‚¹ã€‚é€šè¿‡ç®€å•çš„æ›¿æ¢ï¼Œæˆ‘ä»¬ä»æ¨¡å‹ä¸­é‡‡æ ·çš„æ¦‚ç‡åˆ†å¸ƒå˜ä¸º
    <semantics><mrow><msub><mi>Ï€</mi><mtext mathvariant="normal">RL</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">\pi_{\text{RL}}(x)</annotation></semantics> å’Œ <semantics><mrow><msub><mi>Ï€</mi><mtext
    mathvariant="normal">ref</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\pi_{\text{ref}}(x)</annotation></semantics>ï¼Œå¦‚ä¸Šå¼[23](ch008.xhtml#eq:kl_standard)æ‰€ç¤ºï¼ˆé€šå¸¸åœ¨æ ‡å‡†å®šä¹‰ä¸­ï¼Œå½“åº”ç”¨äºRL
    KLæƒ©ç½šæ—¶ï¼Œè¡¨ç¤ºä¸º <semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics>
    å’Œ <semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics>ï¼‰ã€‚è¿™ç§KLæ•£åº¦æƒ©ç½šæœ€æ—©åº”ç”¨äºå¯¹è¯ä»£ç†ï¼Œåœ¨å¤§è¯­è¨€æ¨¡å‹æµè¡Œä¹‹å‰ï¼Œä½†KLæ§åˆ¶å¾ˆå¿«å°±è¢«ç¡®ç«‹ä¸ºå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹çš„æ ¸å¿ƒæŠ€æœ¯[[162]](ch021.xhtml#ref-jaques2017sequence)ï¼Œ[[163]](ch021.xhtml#ref-jaques2020human)ã€‚
- en: Implementation Example
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å®ç°ç¤ºä¾‹
- en: 'In practice, the implementation of KL divergence is often approximated [[164]](ch021.xhtml#ref-schulman2016klapprox),
    making the implementation far simpler. With the above definition, the summation
    of KL can be converted to an expectation when sampling directly from the distribution
    <semantics><mrow><mi>P</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">P(x)</annotation></semantics>.
    In this case, the distribution <semantics><mrow><mi>P</mi><mo stretchy="false"
    form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">P(x)</annotation></semantics> is the generative distribution
    of the model currently being trained (i.e.Â not the reference model). Then, the
    computation for KL divergence changes to the following:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®è·µä¸­ï¼ŒKLæ•£åº¦çš„å®ç°é€šå¸¸è¢«è¿‘ä¼¼[[164]](ch021.xhtml#ref-schulman2016klapprox)ï¼Œè¿™ä½¿å¾—å®ç°å˜å¾—è¿œä¸ºç®€å•ã€‚æ ¹æ®ä¸Šè¿°å®šä¹‰ï¼Œå½“ç›´æ¥ä»åˆ†å¸ƒ
    <semantics><mrow><mi>P</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">P(x)</annotation></semantics>
    ä¸­é‡‡æ ·æ—¶ï¼ŒKLçš„æ€»å’Œå¯ä»¥è½¬æ¢ä¸ºæœŸæœ›ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œåˆ†å¸ƒ <semantics><mrow><mi>P</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">P(x)</annotation></semantics>
    æ˜¯å½“å‰æ­£åœ¨è®­ç»ƒçš„æ¨¡å‹çš„ç”Ÿæˆåˆ†å¸ƒï¼ˆå³ä¸æ˜¯å‚è€ƒæ¨¡å‹ï¼‰ã€‚ç„¶åï¼ŒKLæ•£åº¦çš„è®¡ç®—å˜ä¸ºä»¥ä¸‹ï¼š
- en: <semantics><mrow><msub><mi>ğ’Ÿ</mi><mtext mathvariant="normal">KL</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>P</mi><mo stretchy="false" form="prefix">|</mo><mo
    stretchy="false" form="prefix">|</mo><mi>Q</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ğ”¼</mi><mrow><mi>x</mi><mo>âˆ¼</mo><mi>P</mi></mrow></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><mi mathvariant="normal">log</mi><mi>P</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><mi
    mathvariant="normal">log</mi><mi>Q</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow><mi>.</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>25</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">\mathcal{D}_{\text{KL}}(P \,||\, Q) =
    \mathbb{E}_{x \sim P} \left[ \log P(x) - \log Q(x) \right]. \qquad{(25)}</annotation></semantics>
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><msub><mi>ğ’Ÿ</mi><mtext mathvariant="normal">KL</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>P</mi><mo stretchy="false" form="prefix">|</mo><mo
    stretchy="false" form="prefix">|</mo><mi>Q</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ğ”¼</mi><mrow><mi>x</mi><mo>âˆ¼</mo><mi>P</mi></mrow></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><mi mathvariant="normal">log</mi><mi>P</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><mi
    mathvariant="normal">log</mi><mi>Q</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow><mi>.</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>25</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">\mathcal{D}_{\text{KL}}(P \,||\, Q) =
    \mathbb{E}_{x \sim P} \left[ \log P(x) - \log Q(x) \right]. \qquad{(25)}</annotation></semantics>
- en: This mode is far simpler to implement, particularly when dealing directly with
    log probabilities used frequently in language model training.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ¨¡å¼å®ç°èµ·æ¥è¦ç®€å•å¾—å¤šï¼Œå°¤å…¶æ˜¯åœ¨ç›´æ¥å¤„ç†è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­ç»å¸¸ä½¿ç”¨çš„å¯¹æ•°æ¦‚ç‡æ—¶ã€‚
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Some example implementations include [TRL](https://github.com/huggingface/trl/blob/5c21de30ae210e4251ead85517ba8dfe3f210e81/trl/trainer/ppo_trainer.py#L1150)
    and [Hamish Ivisonâ€™s Jax Code](https://github.com/hamishivi/EasyLM/blob/main/EasyLM/models/llama/llama_train_ppo.py#L278).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€äº›ç¤ºä¾‹å®ç°åŒ…æ‹¬ [TRL](https://github.com/huggingface/trl/blob/5c21de30ae210e4251ead85517ba8dfe3f210e81/trl/trainer/ppo_trainer.py#L1150)
    å’Œ [Hamish Ivison çš„ Jax ä»£ç ](https://github.com/hamishivi/EasyLM/blob/main/EasyLM/models/llama/llama_train_ppo.py#L278)ã€‚
- en: Pretraining Gradients
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é¢„è®­ç»ƒæ¢¯åº¦
- en: 'Another way of viewing regularization is that you may have a *dataset* that
    you want the model to remain close to, as done in InstructGPT [[3]](ch021.xhtml#ref-ouyang2022training)
    â€œin order to fix the performance regressions on public NLP datasetsâ€. To implement
    this, they modify the training objective for RLHF. Taking eq.Â [22](ch008.xhtml#eq:rl_start),
    we can transform this into an objective function to optimize by sampling from
    the RL policy model, completions <semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics>
    from prompts <semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics>
    in the RL dataset used for RLHF, which yields: <semantics><mrow><mi>J</mi><mo
    stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ğ”¼</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false"
    form="postfix">)</mo><mo>âˆ¼</mo><msub><mi>ğ’Ÿ</mi><msub><mi>Ï€</mi><mrow><mtext mathvariant="normal">RL</mtext><mo>,</mo><mi>Î¸</mi></mrow></msub></msub></mrow></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><mi>y</mi><mo>âˆ£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><mi>Î»</mi><msub><mi>r</mi><mtext
    mathvariant="normal">reg.</mtext></msub><mo stretchy="true" form="postfix">]</mo></mrow><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>26</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">J(\theta) = \mathbb{E}_{(x,y) \sim \mathcal{D}_{\pi_{\text{RL},\theta}}}
    \left[ r_{\theta}(y \mid x) - \lambda r_{\text{reg.}} \right] \qquad{(26)}</annotation></semantics>'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è§‚å¯Ÿæ­£åˆ™åŒ–çš„å¦ä¸€ç§æ–¹å¼æ˜¯ï¼Œä½ å¯èƒ½æœ‰ä¸€ä¸ªå¸Œæœ›æ¨¡å‹ä¿æŒæ¥è¿‘çš„*æ•°æ®é›†*ï¼Œå°±åƒåœ¨InstructGPT [[3]](ch021.xhtml#ref-ouyang2022training)ä¸­æ‰€åšçš„é‚£æ ·ï¼Œâ€œä¸ºäº†ä¿®å¤å…¬å…±NLPæ•°æ®é›†ä¸Šçš„æ€§èƒ½é€€åŒ–â€ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œä»–ä»¬ä¿®æ”¹äº†RLHFçš„è®­ç»ƒç›®æ ‡ã€‚ä»¥ç­‰å¼Â [22](ch008.xhtml#eq:rl_start)
    ä¸ºä¾‹ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä»RLç­–ç•¥æ¨¡å‹ä¸­é‡‡æ ·ï¼Œä»ç”¨äºRLHFçš„RLæ•°æ®é›†ä¸­çš„æç¤º <semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics>
    ä¸­è·å– <semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics>ï¼Œå°†å…¶è½¬åŒ–ä¸ºä¸€ä¸ªä¼˜åŒ–ç›®æ ‡å‡½æ•°ï¼Œå¾—åˆ°ï¼š
    <semantics><mrow><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ğ”¼</mi><mrow><mo stretchy="false"
    form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false" form="postfix">)</mo><mo>âˆ¼</mo><msub><mi>ğ’Ÿ</mi><msub><mi>Ï€</mi><mrow><mtext
    mathvariant="normal">RL</mtext><mo>,</mo><mi>Î¸</mi></mrow></msub></msub></mrow></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><mi>y</mi><mo>âˆ£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><mi>Î»</mi><msub><mi>r</mi><mtext
    mathvariant="normal">reg.</mtext></msub><mo stretchy="true" form="postfix">]</mo></mrow><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>26</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">J(\theta) = \mathbb{E}_{(x,y) \sim \mathcal{D}_{\pi_{\text{RL},\theta}}}
    \left[ r_{\theta}(y \mid x) - \lambda r_{\text{reg.}} \right] \qquad{(26)}</annotation></semantics>
- en: 'Then, we can add an additional reward for higher probabilities on the standard
    autoregressive next-token prediction loss used at pretraining, over a set of documents
    sampled from the pretraining corpus (or another dataset) to maintain textual coherence:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥åœ¨é¢„è®­ç»ƒæœŸé—´ä½¿ç”¨çš„æ ‡å‡†è‡ªå›å½’ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹æŸå¤±ä¸Šæ·»åŠ é¢å¤–çš„å¥–åŠ±ï¼Œè¿™ä¸ªæŸå¤±æ˜¯åœ¨ä»é¢„è®­ç»ƒè¯­æ–™åº“ï¼ˆæˆ–å¦ä¸€ä¸ªæ•°æ®é›†ï¼‰ä¸­æŠ½å–çš„ä¸€ç»„æ–‡æ¡£ä¸Šè®¡ç®—çš„ï¼Œä»¥ä¿æŒæ–‡æœ¬ä¸€è‡´æ€§ï¼š
- en: <semantics><mrow><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ğ”¼</mi><mrow><mo stretchy="false"
    form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false" form="postfix">)</mo><mo>âˆ¼</mo><msub><mi>ğ’Ÿ</mi><msub><mi>Ï€</mi><mrow><mtext
    mathvariant="normal">RL</mtext><mo>,</mo><mi>Î¸</mi></mrow></msub></msub></mrow></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="prefix">(</mo><mi>y</mi><mo>âˆ£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><mi>Î»</mi><msub><mi>r</mi><mtext
    mathvariant="normal">reg.</mtext></msub><mo stretchy="true" form="postfix">]</mo></mrow><mo>+</mo><mi>Î³</mi><msub><mi>ğ”¼</mi><mrow><mi>x</mi><mo>âˆ¼</mo><msub><mi>ğ’Ÿ</mi><mtext
    mathvariant="normal">pretrain</mtext></msub></mrow></msub><mrow><mo stretchy="true"
    form="prefix">[</mo><mi mathvariant="normal">log</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>Ï€</mi><mrow><mtext
    mathvariant="normal">RL</mtext><mo>,</mo><mi>Î¸</mi></mrow></msub><mo stretchy="false"
    form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false"
    form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>27</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">J(\theta) = \mathbb{E}_{(x,y) \sim \mathcal{D}_{\pi_{\text{RL},\theta}}}
    \left[ r_{\theta}(y \mid x) - \lambda r_{\text{reg.}} \right] + \gamma \mathbb{E}_{x
    \sim \mathcal{D}_{\text{pretrain}}} \left[ \log(\pi_{\text{RL},\theta}(x)) \right]
    \qquad{(27)}</annotation></semantics>
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><mi>J</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ğ”¼</mi><mrow><mo stretchy="false"
    form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false" form="postfix">)</mo><mo>âˆ¼</mo><msub><mi>ğ’Ÿ</mi><msub><mi>Ï€</mi><mrow><mtext
    mathvariant="normal">RL</mtext><mo>,</mo><mi>Î¸</mi></mrow></msub></msub></mrow></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo stretchy="false"
    form="postfix">(</mo><mi>y</mi><mo>âˆ£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><mi>Î»</mi><msub><mi>r</mi><mtext
    mathvariant="normal">reg.</mtext></msub><mo stretchy="true" form="postfix">]</mo></mrow><mo>+</mo><mi>Î³</mi><msub><mi>ğ”¼</mi><mrow><mi>x</mi><mo>âˆ¼</mo><msub><mi>ğ’Ÿ</mi><mtext
    mathvariant="normal">pretrain</mtext></msub></mrow></msub><mrow><mo stretchy="true"
    form="prefix">[</mo><mi mathvariant="normal">log</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>Ï€</mi><mrow><mtext
    mathvariant="normal">RL</mtext><mo>,</mo><mi>Î¸</mi></mrow></msub><mo stretchy="false"
    form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false"
    form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow><mrow><mo
    stretchy="false" form="prefix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">J(\theta)
    = \mathbb{E}_{(x,y) \sim \mathcal{D}_{\pi_{\text{RL},\theta}}} \left[ r_{\theta}(y
    \mid x) - \lambda r_{\text{reg.}} \right] + \gamma \mathbb{E}_{x \sim \mathcal{D}_{\text{pretrain}}}
    \left[ \log(\pi_{\text{RL},\theta}(x)) \right] \qquad{(27)}</annotation></semantics>
- en: Recent work proposed using a negative log-likelihood term to balance the optimization
    of Direct Preference Optimization (DPO) [[165]](ch021.xhtml#ref-pang2024iterative).
    Given the pairwise nature of the DPO loss, the same loss modification can be made
    to reward model training, constraining the model to predict accurate text (rumors
    from laboratories that did not publish the work).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘çš„ç ”ç©¶æå‡ºäº†ä½¿ç”¨è´Ÿå¯¹æ•°ä¼¼ç„¶é¡¹æ¥å¹³è¡¡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰[[165]](ch021.xhtml#ref-pang2024iterative)çš„ä¼˜åŒ–ã€‚é‰´äºDPOæŸå¤±çš„æˆå¯¹æ€§è´¨ï¼Œå¯ä»¥å¯¹ç›¸åŒçš„æŸå¤±ä¿®æ”¹è¿›è¡Œå¥–åŠ±æ¨¡å‹è®­ç»ƒï¼Œçº¦æŸæ¨¡å‹é¢„æµ‹å‡†ç¡®æ–‡æœ¬ï¼ˆæœªå‘è¡¨å·¥ä½œçš„å®éªŒå®¤çš„è°£è¨€ï¼‰ã€‚
- en: The optimization follows as a modification to DPO. <semantics><mrow><msub><mi>â„’</mi><mtext
    mathvariant="normal">DPO+NLL</mtext></msub><mo>=</mo><msub><mi>â„’</mi><mtext mathvariant="normal">DPO</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><msubsup><mi>c</mi><mi>i</mi><mi>w</mi></msubsup><mo>,</mo><msubsup><mi>y</mi><mi>i</mi><mi>w</mi></msubsup><mo>,</mo><msubsup><mi>c</mi><mi>i</mi><mi>l</mi></msubsup><mo>,</mo><msubsup><mi>y</mi><mi>i</mi><mi>l</mi></msubsup><mo>âˆ£</mo><msub><mi>x</mi><mi>i</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo>+</mo><mi>Î±</mi><msub><mi>â„’</mi><mtext
    mathvariant="normal">NLL</mtext></msub><mo stretchy="false" form="prefix">(</mo><msubsup><mi>c</mi><mi>i</mi><mi>w</mi></msubsup><mo>,</mo><msubsup><mi>y</mi><mi>i</mi><mi>w</mi></msubsup><mo>âˆ£</mo><msub><mi>x</mi><mi>i</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mrow><mo stretchy="false" form="prefix">(</mo><mn>28</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}_{\text{DPO+NLL}}
    = \mathcal{L}_{\text{DPO}}(c_i^w, y_i^w, c_i^l, y_i^l \mid x_i) + \alpha \mathcal{L}_{\text{NLL}}(c_i^w,
    y_i^w \mid x_i) \qquad{(28)}</annotation></semantics>
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><mo>=</mo><mi>âˆ’</mi><mrow><mi mathvariant="normal">log</mi><mo>â¡</mo></mrow><mi>Ïƒ</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>Î²</mi><mi mathvariant="normal">log</mi><mfrac><mrow><msub><mi>P</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msubsup><mi>c</mi><mi>i</mi><mi>w</mi></msubsup><mo>,</mo><msubsup><mi>y</mi><mi>i</mi><mi>w</mi></msubsup><mo>âˆ£</mo><msub><mi>x</mi><mi>i</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>P</mi><mtext mathvariant="normal">ref.</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><msubsup><mi>c</mi><mi>i</mi><mi>w</mi></msubsup><mo>,</mo><msubsup><mi>y</mi><mi>i</mi><mi>w</mi></msubsup><mo>âˆ£</mo><msub><mi>x</mi><mi>i</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow></mfrac><mo>âˆ’</mo><mi>Î²</mi><mi mathvariant="normal">log</mi><mfrac><mrow><msub><mi>P</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msubsup><mi>c</mi><mi>i</mi><mi>l</mi></msubsup><mo>,</mo><msubsup><mi>y</mi><mi>i</mi><mi>l</mi></msubsup><mo>âˆ£</mo><msub><mi>x</mi><mi>i</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><msub><mi>P</mi><mtext mathvariant="normal">ref.</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><msubsup><mi>c</mi><mi>i</mi><mi>l</mi></msubsup><mo>,</mo><msubsup><mi>y</mi><mi>i</mi><mi>l</mi></msubsup><mo>âˆ£</mo><msub><mi>x</mi><mi>i</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mo>âˆ’</mo><mi>Î±</mi><mfrac><mrow><mrow><mi
    mathvariant="normal">log</mi><mo>â¡</mo></mrow><msub><mi>P</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msubsup><mi>c</mi><mi>i</mi><mi>w</mi></msubsup><mo>,</mo><msubsup><mi>y</mi><mi>i</mi><mi>w</mi></msubsup><mo>âˆ£</mo><msub><mi>x</mi><mi>i</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><mrow><mo stretchy="false" form="prefix">|</mo><msubsup><mi>c</mi><mi>i</mi><mi>w</mi></msubsup><mo
    stretchy="false" form="prefix">|</mo><mi>+</mi><mo stretchy="false" form="prefix">|</mo><msubsup><mi>y</mi><mi>i</mi><mi>w</mi></msubsup><mo
    stretchy="false" form="prefix">|</mo></mrow></mfrac><mo>,</mo><mrow><mo stretchy="false"
    form="prefix">(</mo><mn>29</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">= -\log \sigma \left( \beta \log \frac{P_\theta(c_i^w,
    y_i^w \mid x_i)}{P_{\text{ref.}}(c_i^w, y_i^w \mid x_i)} - \beta \log \frac{P_\theta(c_i^l,
    y_i^l \mid x_i)}{P_{\text{ref.}}(c_i^l, y_i^l \mid x_i)} \right) - \alpha \frac{\log
    P_\theta(c_i^w, y_i^w \mid x_i)}{|c_i^w| + |y_i^w|}, \qquad{(29)}</annotation></semantics>
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'where <semantics><msub><mi>P</mi><mi>Î¸</mi></msub><annotation encoding="application/x-tex">P_{\theta}</annotation></semantics>
    is the trainable policy model, <semantics><msub><mi>P</mi><mtext mathvariant="normal">ref.</mtext></msub><annotation
    encoding="application/x-tex">P_{\text{ref.}}</annotation></semantics> is a fixed
    reference model (often the SFT checkpoint), and <semantics><mrow><mo stretchy="false"
    form="prefix">(</mo><msubsup><mi>c</mi><mi>i</mi><mi>w</mi></msubsup><mo>,</mo><msubsup><mi>y</mi><mi>i</mi><mi>w</mi></msubsup><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(c_i^w,
    y_i^w)</annotation></semantics> and <semantics><mrow><mo stretchy="false" form="prefix">(</mo><msubsup><mi>c</mi><mi>i</mi><mi>l</mi></msubsup><mo>,</mo><msubsup><mi>y</mi><mi>i</mi><mi>l</mi></msubsup><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(c_i^l,
    y_i^l)</annotation></semantics> denote the winning and losing completions for
    prompt <semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding="application/x-tex">x_i</annotation></semantics>.
    The first term is the standard DPO logistic loss: it increases the margin between
    the win and loss using the difference of log-likelihood ratios, <semantics><mrow><mrow><mi
    mathvariant="normal">log</mi><mo>â¡</mo></mrow><mstyle displaystyle="false"><mfrac><msub><mi>P</mi><mi>Î¸</mi></msub><msub><mi>P</mi><mtext
    mathvariant="normal">ref.</mtext></msub></mfrac></mstyle></mrow><annotation encoding="application/x-tex">\log
    \tfrac{P_{\theta}}{P_{\text{ref.}}}</annotation></semantics>, and <semantics><mi>Î²</mi><annotation
    encoding="application/x-tex">\beta</annotation></semantics> controls how strongly
    this preference signal pulls away from the reference. The second term is a length-normalized
    negative log-likelihood penalty on the winning completion, weighted by <semantics><mi>Î±</mi><annotation
    encoding="application/x-tex">\alpha</annotation></semantics>, which helps keep
    the preferred text high-likelihood in an absolute language modeling sense rather
    than only relatively better than the rejected sample.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­<semantics><msub><mi>P</mi><mi>Î¸</mi></msub><annotation encoding="application/x-tex">P_{\theta}</annotation></semantics>æ˜¯å¯è®­ç»ƒçš„ç­–ç•¥æ¨¡å‹ï¼Œ<semantics><msub><mi>P</mi><mtext
    mathvariant="normal">ref.</mtext></msub><annotation encoding="application/x-tex">P_{\text{ref.}}</annotation></semantics>æ˜¯ä¸€ä¸ªå›ºå®šçš„å‚è€ƒæ¨¡å‹ï¼ˆé€šå¸¸æ˜¯SFTæ£€æŸ¥ç‚¹ï¼‰ï¼Œè€Œ<semantics><mrow><mo
    stretchy="false" form="prefix">(</mo><msubsup><mi>c</mi><mi>i</mi><mi>w</mi></msubsup><mo>,</mo><msubsup><mi>y</mi><mi>i</mi><mi>w</mi></msubsup><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(c_i^w,
    y_i^w)</annotation></semantics>å’Œ<semantics><mrow><mo stretchy="false" form="prefix">(</mo><msubsup><mi>c</mi><mi>i</mi><mi>l</mi></msubsup><mo>,</mo><msubsup><mi>y</mi><mi>i</mi><mi>l</mi></msubsup><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(c_i^l,
    y_i^l)</annotation></semantics>è¡¨ç¤ºå¯¹äºæç¤º<semantics><msub><mi>x</mi><mi>i</mi></msub><annotation
    encoding="application/x-tex">x_i</annotation></semantics>çš„è·èƒœå’Œå¤±è´¥å®Œæˆã€‚ç¬¬ä¸€ä¸ªé¡¹æ˜¯æ ‡å‡†çš„DPOé€»è¾‘æŸå¤±ï¼šå®ƒé€šè¿‡ä½¿ç”¨ä¼¼ç„¶æ¯”å·®å¼‚æ¥å¢åŠ è·èƒœå’Œå¤±è´¥ä¹‹é—´çš„è¾¹ç•Œï¼Œ<semantics><mrow><mrow><mi
    mathvariant="normal">log</mi><mo>â¡</mo></mrow><mstyle displaystyle="false"><mfrac><msub><mi>P</mi><mi>Î¸</mi></msub><msub><mi>P</mi><mtext
    mathvariant="normal">ref.</mtext></msub></mfrac></mstyle></mrow><annotation encoding="application/x-tex">\log
    \tfrac{P_{\theta}}{P_{\text{ref.}}}</annotation></semantics>ï¼Œè€Œ<semantics><mi>Î²</mi><annotation
    encoding="application/x-tex">\beta</annotation></semantics>æ§åˆ¶è¿™ç§åå¥½ä¿¡å·ä»å‚è€ƒä¸­æ‹‰å¼€çš„å¼ºåº¦ã€‚ç¬¬äºŒä¸ªé¡¹æ˜¯å¯¹è·èƒœå®Œæˆçš„é•¿åº¦å½’ä¸€åŒ–è´Ÿå¯¹æ•°ä¼¼ç„¶æƒ©ç½šï¼Œç”±<semantics><mi>Î±</mi><annotation
    encoding="application/x-tex">\alpha</annotation></semantics>åŠ æƒï¼Œè¿™æœ‰åŠ©äºåœ¨ç»å¯¹è¯­è¨€å»ºæ¨¡æ„ä¹‰ä¸Šä¿æŒé¦–é€‰æ–‡æœ¬çš„é«˜å¯èƒ½æ€§ï¼Œè€Œä¸ä»…ä»…æ˜¯ç›¸å¯¹äºè¢«æ‹’ç»æ ·æœ¬çš„ç›¸å¯¹æ›´å¥½ã€‚
- en: Other Regularization
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…¶ä»–æ­£åˆ™åŒ–
- en: Controlling the optimization is less well defined in other parts of the RLHF
    stack. Most reward models have no regularization beyond the standard contrastive
    loss function. Direct Alignment Algorithms handle regularization to KL divergences
    differently, through the <semantics><mi>Î²</mi><annotation encoding="application/x-tex">\beta</annotation></semantics>
    parameter (see the chapter on Direct Alignment).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨RLHFå †æ ˆçš„å…¶ä»–éƒ¨åˆ†ä¸­ï¼Œæ§åˆ¶ä¼˜åŒ–å®šä¹‰å¾—ä¸å¤Ÿæ˜ç¡®ã€‚å¤§å¤šæ•°å¥–åŠ±æ¨¡å‹é™¤äº†æ ‡å‡†çš„å¯¹æ¯”æŸå¤±å‡½æ•°ä¹‹å¤–æ²¡æœ‰å…¶ä»–æ­£åˆ™åŒ–ã€‚ç›´æ¥å¯¹é½ç®—æ³•é€šè¿‡<semantics><mi>Î²</mi><annotation
    encoding="application/x-tex">\beta</annotation></semantics>å‚æ•°ï¼ˆå‚è§ç›´æ¥å¯¹é½ç« èŠ‚ï¼‰ä»¥ä¸åŒçš„æ–¹å¼å¤„ç†KLå‘æ•£çš„æ­£åˆ™åŒ–ã€‚
- en: 'Llama 2 proposed a margin loss for reward model training [[44]](ch021.xhtml#ref-touvron2023llama):'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 'Llama 2æå‡ºäº†å¥–åŠ±æ¨¡å‹è®­ç»ƒçš„è¾¹ç•ŒæŸå¤± [[44]](ch021.xhtml#ref-touvron2023llama):'
- en: <semantics><mrow><mi>â„’</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mi>âˆ’</mi><mrow><mi mathvariant="normal">log</mi><mo>â¡</mo></mrow><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>Ïƒ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>r</mi></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><mi>m</mi><mo stretchy="false"
    form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>,</mo><msub><mi>y</mi><mi>r</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow><mo
    stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="false" form="prefix">(</mo><mn>30</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\mathcal{L}(\theta)
    = - \log \left( \sigma \left( r_{\theta}(y_c \mid x) - r_{\theta}(y_r \mid x)
    - m(y_c, y_r) \right) \right) \qquad{(30)}</annotation></semantics>
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><mi>â„’</mi><mo stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mi>âˆ’</mi><mrow><mi mathvariant="normal">log</mi><mo>â¡</mo></mrow><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>Ïƒ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><msub><mi>r</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>r</mi></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><mi>m</mi><mo stretchy="false"
    form="postfix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>,</mo><msub><mi>y</mi><mi>r</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow><mo
    stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="false" form="prefix">(</mo><mn>30</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\mathcal{L}(\theta)
    = - \log \left( \sigma \left( r_{\theta}(y_c \mid x) - r_{\theta}(y_r \mid x)
    - m(y_c, y_r) \right) \right) \qquad{(30)}</annotation></semantics>
- en: where <semantics><mrow><mi>m</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>,</mo><msub><mi>y</mi><mi>r</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">m(y_c,
    y_r)</annotation></semantics> is the margin between two datapoints <semantics><msub><mi>y</mi><mi>c</mi></msub><annotation
    encoding="application/x-tex">y_c</annotation></semantics> and <semantics><msub><mi>y</mi><mi>r</mi></msub><annotation
    encoding="application/x-tex">y_r</annotation></semantics> representing numerical
    difference in delta between the ratings of two annotators. This is either achieved
    by having annotators rate the outputs on a numerical scale or by using a quantified
    ranking method, such as [Likert scales](https://en.wikipedia.org/wiki/Likert_scale).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ <semantics><mrow><mi>m</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>,</mo><msub><mi>y</mi><mi>r</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">m(y_c,
    y_r)</annotation></semantics> æ˜¯ä¸¤ä¸ªæ•°æ®ç‚¹ä¹‹é—´çš„è¾¹ç¼˜ï¼Œè¿™ä¸¤ä¸ªæ•°æ®ç‚¹ <semantics><msub><mi>y</mi><mi>c</mi></msub><annotation
    encoding="application/x-tex">y_c</annotation></semantics> å’Œ <semantics><msub><mi>y</mi><mi>r</mi></msub><annotation
    encoding="application/x-tex">y_r</annotation></semantics> ä»£è¡¨äº†ä¸¤ä¸ªæ³¨é‡Šè€…ä¹‹é—´è¯„åˆ†çš„deltaæ•°å€¼å·®å¼‚ã€‚è¿™å¯ä»¥é€šè¿‡è®©æ³¨é‡Šè€…åœ¨æ•°å€¼å°ºåº¦ä¸Šå¯¹è¾“å‡ºè¿›è¡Œè¯„åˆ†æ¥å®ç°ï¼Œæˆ–è€…ä½¿ç”¨é‡åŒ–çš„æ’åæ–¹æ³•ï¼Œä¾‹å¦‚
    [æå…‹ç‰¹é‡è¡¨](https://en.wikipedia.org/wiki/Likert_scale)ã€‚
- en: Reward margins have been used heavily in the direct alignment literature, such
    as Reward weighted DPO, â€˜â€™Reward-aware Preference Optimizationâ€™â€™ (RPO), which
    integrates reward model scores into the update rule following a DPO loss [[25]](ch021.xhtml#ref-adler2024nemotron),
    or REBEL [[166]](ch021.xhtml#ref-gao2024rebel) that has a reward delta weighting
    in a regression-loss formulation.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å¥–åŠ±è¾¹ç¼˜åœ¨ç›´æ¥å¯¹é½æ–‡çŒ®ä¸­å¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨ï¼Œä¾‹å¦‚å¥–åŠ±åŠ æƒDPOï¼ˆReward weighted DPOï¼‰ï¼Œâ€œå¥–åŠ±æ„ŸçŸ¥åå¥½ä¼˜åŒ–â€ï¼ˆâ€œReward-aware
    Preference Optimizationâ€ï¼ŒRPOï¼‰ï¼Œå®ƒå°†å¥–åŠ±æ¨¡å‹å¾—åˆ†æ•´åˆåˆ°DPOæŸå¤±çš„æ›´æ–°è§„åˆ™ä¸­ [[25]](ch021.xhtml#ref-adler2024nemotron)ï¼Œæˆ–è€…å…·æœ‰å›å½’æŸå¤±å…¬å¼ä¸­å¥–åŠ±deltaåŠ æƒçš„REBEL
    [[166]](ch021.xhtml#ref-gao2024rebel)ã€‚
