- en: Constitutional AI & AI Feedback
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®ªç« AIä¸AIåé¦ˆ
- en: RL from AI Feedback (RLAIF) is a larger set of techniques for using AI to augment
    or generate feedback data, including pairwise preferences [[227]](ch021.xhtml#ref-lee2023rlaif)
    [[228]](ch021.xhtml#ref-sharma2024critical) [[229]](ch021.xhtml#ref-castricato2024suppressing).
    There are many motivations to using RLAIF to either entirely replace human feedback
    or augment it. AI models are far cheaper than humans, with a single piece of human
    preference data costing on the order of $1 or higher (or even above $10 per prompt),
    AI feedback with a frontier AI model, such as GPT-4o costs less than $0.01\. This
    cost difference opens the market of experimentation with RLHF methods to an entire
    population of people previously priced out. Other than price, AI feedback introduces
    different *tradeoffs* on performance than human feedback, which are still being
    investigated. The peak performance for AI feedback is at least in the same ballpark
    of human data on skill-based evaluations, but it is not studied if human data
    allows finer control of the models in real-world product settings or for newer
    training methods such as character training.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥è‡ªAIåé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLAIFï¼‰æ˜¯ä¸€å¥—æ›´å¹¿æ³›çš„æŠ€æœ¯ï¼Œç”¨äºä½¿ç”¨AIæ¥å¢å¼ºæˆ–ç”Ÿæˆåé¦ˆæ•°æ®ï¼ŒåŒ…æ‹¬æˆå¯¹åå¥½ [[227]](ch021.xhtml#ref-lee2023rlaif)
    [[228]](ch021.xhtml#ref-sharma2024critical) [[229]](ch021.xhtml#ref-castricato2024suppressing)ã€‚ä½¿ç”¨RLAIFçš„åŠ¨æœºæœ‰å¾ˆå¤šï¼Œæ—¢å¯ä»¥å®Œå…¨å–ä»£äººç±»åé¦ˆï¼Œä¹Ÿå¯ä»¥å¢å¼ºå®ƒã€‚AIæ¨¡å‹æ¯”äººç±»ä¾¿å®œå¾—å¤šï¼Œå•æ¡äººç±»åå¥½æ•°æ®æˆæœ¬çº¦ä¸º1ç¾å…ƒæˆ–æ›´é«˜ï¼ˆç”šè‡³æ¯ä¸ªæç¤ºè¯é«˜è¾¾10ç¾å…ƒä»¥ä¸Šï¼‰ï¼Œè€Œä½¿ç”¨å‰æ²¿AIæ¨¡å‹ï¼ˆå¦‚GPT-4oï¼‰çš„AIåé¦ˆæˆæœ¬ä¸åˆ°0.01ç¾å…ƒã€‚è¿™ç§æˆæœ¬å·®å¼‚ä½¿å¾—RLHFæ–¹æ³•çš„å®éªŒå¸‚åœºå¯¹ä¹‹å‰å› ä»·æ ¼è€Œè¢«æ’é™¤åœ¨å¤–çš„äººç¾¤å¼€æ”¾ã€‚é™¤äº†ä»·æ ¼ä¹‹å¤–ï¼ŒAIåé¦ˆåœ¨æ€§èƒ½ä¸Šå¼•å…¥äº†ä¸äººç±»åé¦ˆä¸åŒçš„*æƒè¡¡*ï¼Œè¿™äº›æƒè¡¡ä»åœ¨ç ”ç©¶ä¸­ã€‚AIåé¦ˆçš„å³°å€¼æ€§èƒ½è‡³å°‘ä¸åŸºäºæŠ€èƒ½çš„è¯„ä»·ä¸­çš„äººç±»æ•°æ®åœ¨åŒä¸€æ°´å¹³ï¼Œä½†å°šæœªç ”ç©¶äººç±»æ•°æ®æ˜¯å¦å…è®¸åœ¨ç°å®ä¸–ç•Œçš„äº§å“è®¾ç½®æˆ–æ–°çš„è®­ç»ƒæ–¹æ³•ï¼ˆå¦‚è§’è‰²è®­ç»ƒï¼‰ä¸­å¯¹æ¨¡å‹è¿›è¡Œæ›´ç²¾ç»†çš„æ§åˆ¶ã€‚
- en: 'The term RLAIF was introduced in Anthropicâ€™s work *Constitutional AI: Harmlessness
    from AI Feedback* [[19]](ch021.xhtml#ref-bai2022constitutional), which resulted
    in initial confusion in the AI community over the relationship between the methods.
    Since the release of the Constitutional AI (CAI) paper and the formalization of
    RLAIF, RLAIF has become a default method within the post-training and RLHF literatures
    â€“ there are far more examples than one can easily enumerate. The relationship
    should be understood as CAI was the example that kickstarted the broader field
    of RLAIF.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¯è¯­RLAIFæ˜¯åœ¨Anthropicçš„å·¥ä½œ*å®ªç« AIï¼šä»AIåé¦ˆä¸­è·å¾—æ— å®³æ€§* [[19]](ch021.xhtml#ref-bai2022constitutional)ä¸­å¼•å…¥çš„ï¼Œè¿™å¯¼è‡´AIç¤¾åŒºå¯¹æ–¹æ³•ä¹‹é—´å…³ç³»äº§ç”Ÿåˆæ­¥çš„å›°æƒ‘ã€‚è‡ªä»å®ªç« AIï¼ˆCAIï¼‰è®ºæ–‡å‘å¸ƒå’ŒRLAIFçš„æ­£å¼åŒ–ä»¥æ¥ï¼ŒRLAIFå·²æˆä¸ºåè®­ç»ƒå’ŒRLHFæ–‡çŒ®ä¸­çš„é»˜è®¤æ–¹æ³•â€”â€”ä¾‹å­å¤šå¾—æ— æ³•è½»æ˜“åˆ—ä¸¾ã€‚åº”å°†è¿™ç§å…³ç³»ç†è§£ä¸ºCAIæ˜¯å¯åŠ¨æ›´å¹¿æ³›RLAIFé¢†åŸŸçš„ä¾‹å­ã€‚
- en: 'A rule of thumb for the difference between human data and AI feedback data
    is as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: äººç±»æ•°æ®ä¸AIåé¦ˆæ•°æ®ä¹‹é—´å·®å¼‚çš„ä¸€ä¸ªç»éªŒæ³•åˆ™æ˜¯ï¼š
- en: Human data is high-noise and low-bias,
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: äººç±»æ•°æ®æ˜¯é«˜å™ªå£°å’Œä½åå·®çš„ï¼Œ
- en: Synthetic preference data is low-noise and high-bias,
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åˆæˆåå¥½æ•°æ®æ˜¯ä½å™ªå£°å’Œé«˜åå·®çš„ï¼Œ
- en: Results in many academic results showing how one can substitute AI preference
    data in RLHF workflows and achieve strong evaluation scores [[230]](ch021.xhtml#ref-miranda2024hybrid),
    but shows how the literature of RLHF is separated from industrial best practices.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: è®¸å¤šå­¦æœ¯æˆæœæ˜¾ç¤ºï¼Œå¦‚ä½•åœ¨RLHFå·¥ä½œæµç¨‹ä¸­ç”¨AIåå¥½æ•°æ®æ›¿ä»£ï¼Œå¹¶å®ç°å¼ºå¤§çš„è¯„ä¼°åˆ†æ•° [[230]](ch021.xhtml#ref-miranda2024hybrid)ï¼Œä½†æ˜¾ç¤ºäº†RLHFæ–‡çŒ®ä¸å·¥ä¸šæœ€ä½³å®è·µä¹‹é—´çš„åˆ†ç¦»ã€‚
- en: Constitutional AI
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®ªç« AI
- en: 'The method of Constitutional AI (CAI), which Anthropic uses extensively in
    their Claude models, is the earliest, large-scale use of synthetic data for RLHF
    training. Constitutional AI has two uses of synthetic data:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å®ªç« AIï¼ˆCAIï¼‰çš„æ–¹æ³•ï¼ŒAnthropicåœ¨ä»–ä»¬çš„Claudeæ¨¡å‹ä¸­å¹¿æ³›ä½¿ç”¨ï¼Œæ˜¯åˆæˆæ•°æ®ç”¨äºRLHFè®­ç»ƒçš„æœ€æ—©ã€æœ€å¤§è§„æ¨¡çš„åº”ç”¨ã€‚å®ªç« AIæœ‰ä¸¤ä¸ªåˆæˆæ•°æ®çš„ä½¿ç”¨ï¼š
- en: Critiques of instruction-tuned data to follow a set of principles like â€œIs the
    answer encouraging violenceâ€ or â€œIs the answer truthful.â€ When the model generates
    answers to questions, it checks the answer against the list of principles in the
    constitution, refining the answer over time. Then, they fine-tune the model on
    this resulting dataset.
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹æŒ‡ä»¤è°ƒæ•´æ•°æ®çš„æ‰¹è¯„ï¼Œä»¥éµå¾ªä¸€ç³»åˆ—åŸåˆ™ï¼Œå¦‚â€œç­”æ¡ˆæ˜¯å¦é¼“åŠ±æš´åŠ›â€æˆ–â€œç­”æ¡ˆæ˜¯å¦çœŸå®â€ã€‚å½“æ¨¡å‹ç”Ÿæˆé—®é¢˜çš„ç­”æ¡ˆæ—¶ï¼Œå®ƒä¼šå°†ç­”æ¡ˆä¸å®ªç« ä¸­çš„åŸåˆ™åˆ—è¡¨è¿›è¡Œæ ¸å¯¹ï¼Œéšç€æ—¶é—´çš„æ¨ç§»æ”¹è¿›ç­”æ¡ˆã€‚ç„¶åï¼Œä»–ä»¬åœ¨ç”Ÿæˆçš„æ•°æ®é›†ä¸Šå¾®è°ƒæ¨¡å‹ã€‚
- en: Generates pairwise preference data by using a language model to answer which
    completion was better, given the context of a random principle from the constitution
    (similar to this paper for principle-guided reward models). Then, RLHF proceeds
    as normal with synthetic data, hence the RLAIF name.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€šè¿‡ä½¿ç”¨è¯­è¨€æ¨¡å‹æ¥å›ç­”ä»å®ªæ³•ä¸­éšæœºæŠ½å–çš„åŸåˆ™ï¼ˆç±»ä¼¼äºè¿™ç¯‡å…³äºåŸåˆ™å¼•å¯¼å¥–åŠ±æ¨¡å‹çš„è®ºæ–‡ï¼‰çš„ä¸Šä¸‹æ–‡ä¸­å“ªä¸ªè¡¥å…¨æ›´å¥½ï¼Œç”Ÿæˆæˆå¯¹åå¥½æ•°æ®ã€‚ç„¶åï¼ŒRLHFä½¿ç”¨åˆæˆæ•°æ®æŒ‰æ­£å¸¸æµç¨‹è¿›è¡Œï¼Œå› æ­¤å¾—åRLAIFã€‚
- en: Largely, CAI is known for the second half above, the preference data, but the
    methods introduced for instruction data are used in general data filtering and
    synthetic data generation methods across post-training.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§éƒ¨åˆ†æƒ…å†µä¸‹ï¼ŒCAIå› ä¸Šè¿°ç¬¬äºŒéƒ¨åˆ†ï¼Œå³åå¥½æ•°æ®è€Œé—»åï¼Œä½†ç”¨äºæŒ‡ä»¤æ•°æ®çš„æ–¹æ³•åœ¨è®­ç»ƒåçš„ä¸€èˆ¬æ•°æ®è¿‡æ»¤å’Œåˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•ä¸­å¾—åˆ°åº”ç”¨ã€‚
- en: CAI can be formalized as follows.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: CAIå¯ä»¥å½¢å¼åŒ–ä¸ºä»¥ä¸‹å†…å®¹ã€‚
- en: By employing a human-written set of principles, which they term a *constitution*,
    Bai et al.Â 2022 use a separate LLM to generate artificial preference and instruction
    data used for fine-tuning [[19]](ch021.xhtml#ref-bai2022constitutional). A constitution
    <semantics><mi>ğ’</mi><annotation encoding="application/x-tex">\mathcal{C}</annotation></semantics>
    is a set of written principles indicating specific aspects to focus on during
    a critique phase. The instruction data is curated by repeatedly sampling a principle
    <semantics><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>âˆˆ</mo><mi>ğ’</mi></mrow><annotation
    encoding="application/x-tex">c_i \in \mathcal{C}</annotation></semantics> and
    asking the model to revise its latest output <semantics><msup><mi>y</mi><mi>i</mi></msup><annotation
    encoding="application/x-tex">y^i</annotation></semantics> to the prompt <semantics><mi>x</mi><annotation
    encoding="application/x-tex">x</annotation></semantics> to align with <semantics><msub><mi>c</mi><mi>i</mi></msub><annotation
    encoding="application/x-tex">c_i</annotation></semantics>. This yields a series
    of instruction variants <semantics><mrow><mo stretchy="false" form="prefix">{</mo><msup><mi>y</mi><mn>0</mn></msup><mo>,</mo><msup><mi>y</mi><mn>1</mn></msup><mo>,</mo><mi>â‹¯</mi><mo>,</mo><msup><mi>y</mi><mi>n</mi></msup><mo
    stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\{y^0,
    y^1, \cdots, y^n\}</annotation></semantics> from the principles <semantics><mrow><mo
    stretchy="false" form="prefix">{</mo><msub><mi>c</mi><mn>0</mn></msub><mo>,</mo><msub><mi>c</mi><mn>1</mn></msub><mo>,</mo><mi>â‹¯</mi><mo>,</mo><msub><mi>c</mi><mrow><mi>n</mi><mo>âˆ’</mo><mn>1</mn></mrow></msub><mo
    stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\{c_{0},
    c_{1}, \cdots, c_{n-1}\}</annotation></semantics> used for critique. The final
    data point is the prompt <semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics>
    together with the final completion <semantics><msup><mi>y</mi><mi>n</mi></msup><annotation
    encoding="application/x-tex">y^n</annotation></semantics>, for some <semantics><mi>n</mi><annotation
    encoding="application/x-tex">n</annotation></semantics>.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ä½¿ç”¨ä¸€ç»„ç”±äººç±»ç¼–å†™çš„åŸåˆ™ï¼Œä»–ä»¬ç§°ä¹‹ä¸ºâ€œå®ªæ³•â€ï¼ŒBaiç­‰äººï¼ˆ2022å¹´ï¼‰ä½¿ç”¨ä¸€ä¸ªå•ç‹¬çš„LLMç”Ÿæˆç”¨äºå¾®è°ƒçš„äººå·¥åå¥½å’ŒæŒ‡ä»¤æ•°æ®[[19]](ch021.xhtml#ref-bai2022constitutional)ã€‚å®ªæ³•<semantics><mi>ğ’</mi><annotation
    encoding="application/x-tex">\mathcal{C}</annotation></semantics>æ˜¯ä¸€ç»„ä¹¦é¢åŸåˆ™ï¼ŒæŒ‡ç¤ºåœ¨æ‰¹åˆ¤é˜¶æ®µéœ€è¦å…³æ³¨çš„ç‰¹å®šæ–¹é¢ã€‚æŒ‡ä»¤æ•°æ®æ˜¯é€šè¿‡åå¤é‡‡æ ·ä¸€ä¸ªåŸåˆ™<semantics><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>âˆˆ</mo><mi>ğ’</mi></mrow><annotation
    encoding="application/x-tex">c_i \in \mathcal{C}</annotation></semantics>å¹¶è¦æ±‚æ¨¡å‹ä¿®æ”¹å…¶æœ€æ–°çš„è¾“å‡º<semantics><msup><mi>y</mi><mi>i</mi></msup><annotation
    encoding="application/x-tex">y^i</annotation></semantics>ä»¥ä¸æç¤º<semantics><mi>x</mi><annotation
    encoding="application/x-tex">x</annotation></semantics>å¯¹é½<semantics><msub><mi>c</mi><mi>i</mi></msub><annotation
    encoding="application/x-tex">c_i</annotation></semantics>æ¥ç²¾å¿ƒåˆ¶ä½œçš„ã€‚è¿™äº§ç”Ÿäº†ä¸€ç³»åˆ—æŒ‡ä»¤å˜ä½“<semantics><mrow><mo
    stretchy="false" form="prefix">{</mo><msup><mi>y</mi><mn>0</mn></msup><mo>,</mo><msup><mi>y</mi><mn>1</mn></msup><mo>,</mo><mi>â‹¯</mi><mo>,</mo><msup><mi>y</mi><mi>n</mi></msup><mo
    stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\{y^0,
    y^1, \cdots, y^n\}</annotation></semantics>ï¼Œè¿™äº›å˜ä½“ç”¨äºæ‰¹åˆ¤ã€‚æœ€ç»ˆæ•°æ®ç‚¹æ˜¯æç¤º<semantics><mi>x</mi><annotation
    encoding="application/x-tex">x</annotation></semantics>ä¸æœ€ç»ˆçš„è¡¥å…¨<semantics><msup><mi>y</mi><mi>n</mi></msup><annotation
    encoding="application/x-tex">y^n</annotation></semantics>ï¼Œå¯¹äºæŸä¸ª<semantics><mi>n</mi><annotation
    encoding="application/x-tex">n</annotation></semantics>ã€‚
- en: The preference data is constructed in a similar, yet simpler way by using a
    subset of principles from <semantics><mi>ğ’</mi><annotation encoding="application/x-tex">\mathcal{C}</annotation></semantics>
    as context for a feedback model. The feedback model is presented with a prompt
    <semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics>,
    a set of principles <semantics><mrow><mo stretchy="false" form="prefix">{</mo><msub><mi>c</mi><mn>0</mn></msub><mo>,</mo><mi>â‹¯</mi><mo>,</mo><msub><mi>c</mi><mi>n</mi></msub><mo
    stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\{c_0,
    \cdots, c_n\}</annotation></semantics>, and two completions <semantics><msub><mi>y</mi><mn>0</mn></msub><annotation
    encoding="application/x-tex">y_0</annotation></semantics> and <semantics><msub><mi>y</mi><mn>1</mn></msub><annotation
    encoding="application/x-tex">y_1</annotation></semantics> labeled as answers (A)
    and (B) from a previous RLHF dataset. The feedback modelsâ€™ probability of outputting
    either (A) or (B) is recorded as a training sample for the reward model
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ä½¿ç”¨æ¥è‡ª <semantics><mi>ğ’</mi><annotation encoding="application/x-tex">\mathcal{C}</annotation></semantics>
    çš„åŸåˆ™å­é›†ä½œä¸ºåé¦ˆæ¨¡å‹çš„ä¸Šä¸‹æ–‡ï¼Œä»¥ç±»ä¼¼ä½†æ›´ç®€å•çš„æ–¹å¼æ„å»ºåå¥½æ•°æ®ã€‚åé¦ˆæ¨¡å‹è¢«å‘ˆç°ä¸€ä¸ªæç¤º <semantics><mi>x</mi><annotation
    encoding="application/x-tex">x</annotation></semantics>ï¼Œä¸€ç»„åŸåˆ™ <semantics><mrow><mo
    stretchy="false" form="prefix">{</mo><msub><mi>c</mi><mn>0</mn></msub><mo>,</mo><mi>â‹¯</mi><mo>,</mo><msub><mi>c</mi><mi>n</mi></msub><mo
    stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\{c_0,
    \cdots, c_n\}</annotation></semantics>ï¼Œä»¥åŠä¸¤ä¸ªæ ‡è®°ä¸ºç­”æ¡ˆï¼ˆAï¼‰å’Œï¼ˆBï¼‰çš„å®Œæˆ <semantics><msub><mi>y</mi><mn>0</mn></msub><annotation
    encoding="application/x-tex">y_0</annotation></semantics> å’Œ <semantics><msub><mi>y</mi><mn>1</mn></msub><annotation
    encoding="application/x-tex">y_1</annotation></semantics>ï¼Œè¿™äº›ç­”æ¡ˆæ¥è‡ªä¹‹å‰çš„RLHFæ•°æ®é›†ã€‚è®°å½•åé¦ˆæ¨¡å‹è¾“å‡ºï¼ˆAï¼‰æˆ–ï¼ˆBï¼‰çš„æ¦‚ç‡ä½œä¸ºå¥–åŠ±æ¨¡å‹çš„è®­ç»ƒæ ·æœ¬
- en: Specific LLMs for Judgement
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç”¨äºåˆ¤æ–­çš„ç‰¹å®šLLM
- en: As RLAIF methods have become more prevalent, many have wondered if we should
    be using the same models for generating responses as those for generating critiques
    or ratings. Specifically, the calibration of the LLM-as-a-judge used has come
    into question. Several works have shown that LLMs are inconsistent evaluators
    [[231]](ch021.xhtml#ref-wang2023large) and prefer their own responses over responses
    from other models (coined self-preference bias) [[232]](ch021.xhtml#ref-panickssery2024llm).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€RLAIFæ–¹æ³•è¶Šæ¥è¶Šæ™®éï¼Œè®¸å¤šäººéƒ½åœ¨æ€è€ƒæ˜¯å¦åº”è¯¥ä½¿ç”¨ä¸ç”Ÿæˆè¯„è®ºæˆ–è¯„åˆ†ç›¸åŒçš„æ¨¡å‹æ¥ç”Ÿæˆå“åº”ã€‚ç‰¹åˆ«æ˜¯ï¼Œæ‰€ä½¿ç”¨çš„LLMä½œä¸ºè£åˆ¤çš„æ ¡å‡†é—®é¢˜å¼•èµ·äº†è´¨ç–‘ã€‚ä¸€äº›ç ”ç©¶è¡¨æ˜ï¼ŒLLMæ˜¯ä¸ä¸€è‡´çš„è¯„ä¼°è€…
    [[231]](ch021.xhtml#ref-wang2023large) å¹¶ä¸”æ›´å€¾å‘äºè‡ªå·±çš„å“åº”è€Œä¸æ˜¯å…¶ä»–æ¨¡å‹çš„å“åº”ï¼ˆç§°ä¸ºè‡ªæˆ‘åå¥½åå·®ï¼‰ [[232]](ch021.xhtml#ref-panickssery2024llm)ã€‚
- en: As a result, many have wondered if we should be using the same models for generating
    responses as those for generating critiques or ratings. Would a solution be to
    train a separate model just for this? Multiple models have been released with
    the goal of substituting for frontier models as a data labeling tool, such as
    critic models Shepherd [[233]](ch021.xhtml#ref-wang2023shepherd) and CriticLLM
    [[234]](ch021.xhtml#ref-ke2023critiquellm) or models for evaluating response performance
    akin to Auto-J [[235]](ch021.xhtml#ref-li2023generative), Prometheus [[138]](ch021.xhtml#ref-kim2023prometheus),
    Prometheus 2 [[236]](ch021.xhtml#ref-kim2024prometheus), or Prometheus-Vision
    [[237]](ch021.xhtml#ref-lee2024prometheus) but they are not widely adopted in
    documented training recipes. Some find scaling inference via repeated sampling
    [[238]](ch021.xhtml#ref-brown2024large) [[239]](ch021.xhtml#ref-zhao2025sample)
    [[240]](ch021.xhtml#ref-kalra2025verdict), self-refinement [[241]](ch021.xhtml#ref-madaan2023self),
    or tournament ranking [[242]](ch021.xhtml#ref-pace2024west) provides a better
    estimate of the true judgement or higher-quality preference pairs. Other calibration
    techniques co-evolve the generation and judgement capabilities of the model [[243]](ch021.xhtml#ref-wu2024meta).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œè®¸å¤šäººéƒ½åœ¨æ€è€ƒæ˜¯å¦åº”è¯¥ä½¿ç”¨ä¸ç”Ÿæˆè¯„è®ºæˆ–è¯„åˆ†ç›¸åŒçš„æ¨¡å‹æ¥ç”Ÿæˆå“åº”ã€‚æ˜¯å¦åº”è¯¥è®­ç»ƒä¸€ä¸ªä¸“é—¨ç”¨äºæ­¤ç›®çš„çš„ç‹¬ç«‹æ¨¡å‹ï¼Ÿå·²ç»å‘å¸ƒäº†å¤šä¸ªæ¨¡å‹ï¼Œæ—¨åœ¨ä½œä¸ºæ•°æ®æ ‡æ³¨å·¥å…·æ›¿ä»£å‰æ²¿æ¨¡å‹ï¼Œä¾‹å¦‚è¯„è®ºæ¨¡å‹Shepherd
    [[233]](ch021.xhtml#ref-wang2023shepherd) å’Œ CriticLLM [[234]](ch021.xhtml#ref-ke2023critiquellm)
    æˆ–ç±»ä¼¼äºAuto-J [[235]](ch021.xhtml#ref-li2023generative)ã€Prometheus [[138]](ch021.xhtml#ref-kim2023prometheus)ã€Prometheus
    2 [[236]](ch021.xhtml#ref-kim2024prometheus) æˆ– Prometheus-Vision [[237]](ch021.xhtml#ref-lee2024prometheus)
    çš„è¯„ä¼°å“åº”æ€§èƒ½çš„æ¨¡å‹ï¼Œä½†å®ƒä»¬åœ¨æ–‡æ¡£åŒ–çš„è®­ç»ƒæ–¹æ¡ˆä¸­å¹¶æœªå¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚æœ‰äº›äººå‘ç°é€šè¿‡é‡å¤é‡‡æ · [[238]](ch021.xhtml#ref-brown2024large)
    [[239]](ch021.xhtml#ref-zhao2025sample) [[240]](ch021.xhtml#ref-kalra2025verdict)ã€è‡ªæˆ‘å®Œå–„
    [[241]](ch021.xhtml#ref-madaan2023self) æˆ–é”¦æ ‡èµ›æ’å [[242]](ch021.xhtml#ref-pace2024west)
    å¯ä»¥æä¾›å¯¹çœŸå®åˆ¤æ–­æˆ–æ›´é«˜è´¨é‡çš„åå¥½å¯¹çš„æ›´å¥½ä¼°è®¡ã€‚å…¶ä»–æ ¡å‡†æŠ€æœ¯å…±åŒè¿›åŒ–äº†æ¨¡å‹çš„ç”Ÿæˆå’Œåˆ¤æ–­èƒ½åŠ› [[243]](ch021.xhtml#ref-wu2024meta)ã€‚
- en: Further Reading
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¿›ä¸€æ­¥é˜…è¯»
- en: There are many related research directions and extensions of Constitutional
    AI, but few of them have been documented as clear improvements in RLHF and post-training
    recipes. For now, they are included as further reading.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å®ªæ³•AIç›¸å…³çš„ç ”ç©¶æ–¹å‘å’Œæ‰©å±•æœ‰å¾ˆå¤šï¼Œä½†å…¶ä¸­å¾ˆå°‘è¢«è®°å½•ä¸ºRLHFå’Œåè®­ç»ƒé£Ÿè°±ä¸­çš„æ˜ç¡®æ”¹è¿›ã€‚ç›®å‰ï¼Œå®ƒä»¬è¢«åŒ…æ‹¬ä¸ºè¿›ä¸€æ­¥é˜…è¯»çš„å†…å®¹ã€‚
- en: OpenAI has released a Model Spec [[124]](ch021.xhtml#ref-openai2024modelspec),
    which is a document stating the intended behavior for their models, and stated
    that they are exploring methods for alignment where the model references the document
    directly (which could be seen as a close peer to CAI). OpenAI has continued and
    trained their reasoning models such as o1 with a method called Deliberative Alignment
    [[244]](ch021.xhtml#ref-guan2024deliberative) to align the model while referencing
    these safety or behavior policies.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAIå‘å¸ƒäº†æ¨¡å‹è§„èŒƒ [[124]](ch021.xhtml#ref-openai2024modelspec)ï¼Œè¿™æ˜¯ä¸€ä»½è¯´æ˜å…¶æ¨¡å‹é¢„æœŸè¡Œä¸ºçš„æ–‡æ¡£ï¼Œå¹¶å£°æ˜ä»–ä»¬æ­£åœ¨æ¢ç´¢ç›´æ¥å¼•ç”¨æ–‡æ¡£çš„æ–¹æ³•æ¥å®ç°æ¨¡å‹çš„å¯¹é½ï¼ˆè¿™å¯ä»¥è¢«è§†ä¸ºä¸CAIçš„ç´§å¯†ä¼™ä¼´ï¼‰ã€‚OpenAIç»§ç»­ä½¿ç”¨åä¸ºDeliberative
    Alignment [[244]](ch021.xhtml#ref-guan2024deliberative) çš„æ–¹æ³•æ¥è®­ç»ƒä»–ä»¬çš„æ¨ç†æ¨¡å‹ï¼Œå¦‚o1ï¼Œä»¥åœ¨å¼•ç”¨è¿™äº›å®‰å…¨æˆ–è¡Œä¸ºç­–ç•¥çš„åŒæ—¶å¯¹é½æ¨¡å‹ã€‚
- en: Anthropic has continued to use CAI in their model training, updating the constitution
    Claude uses [[245]](ch021.xhtml#ref-Anthropic2023ClaudesConstitution) and experimenting
    with how population collectives converge on principles for models and how that
    changes model behavior [[246]](ch021.xhtml#ref-ganguli2023).
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthropicç»§ç»­åœ¨æ¨¡å‹è®­ç»ƒä¸­ä½¿ç”¨CAIï¼Œæ›´æ–°äº†Claudeä½¿ç”¨çš„å®ªæ³• [[245]](ch021.xhtml#ref-Anthropic2023ClaudesConstitution)
    å¹¶å®éªŒäº†ç§ç¾¤é›†ä½“å¦‚ä½•å°±æ¨¡å‹åŸåˆ™è¾¾æˆå…±è¯†ä»¥åŠè¿™å¦‚ä½•æ”¹å˜æ¨¡å‹è¡Œä¸º [[246]](ch021.xhtml#ref-ganguli2023)ã€‚
- en: The open-source community has explored replications of CAI applied to open datasets
    [[247]](ch021.xhtml#ref-Huang2024cai) and for explorations into creating dialogue
    data between LMs [[248]](ch021.xhtml#ref-lambert2024self).
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¼€æºç¤¾åŒºå·²ç»æ¢ç´¢äº†å°†CAIåº”ç”¨äºå¼€æ”¾æ•°æ®é›†çš„å¤åˆ¶ [[247]](ch021.xhtml#ref-Huang2024cai) ä»¥åŠæ¢ç´¢åœ¨LMä¹‹é—´åˆ›å»ºå¯¹è¯æ•°æ®çš„æ–¹æ³•
    [[248]](ch021.xhtml#ref-lambert2024self)ã€‚
- en: Other work has used principle-driven preferences or feedback with different
    optimization methods. [[249]](ch021.xhtml#ref-sun2023principledriven) uses principles
    as context for the reward models, which was used to train the Dromedary models
    [[250]](ch021.xhtml#ref-sun2024salmon). [[37]](ch021.xhtml#ref-glaese2022improving)
    uses principles to improve the accuracy of human judgments in the RLHF process.
    [[251]](ch021.xhtml#ref-liu2025inference) train a reward model to generate its
    own principles at inference time, and use these to deliver a final score. [[252]](ch021.xhtml#ref-franken2024self)
    formulate principle-following as a mutual information maximization problem that
    the pretrained model can learn with no labels.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å…¶ä»–å·¥ä½œä½¿ç”¨äº†åŸºäºåŸåˆ™çš„åå¥½æˆ–åé¦ˆä»¥åŠä¸åŒçš„ä¼˜åŒ–æ–¹æ³•ã€‚[[249]](ch021.xhtml#ref-sun2023principledriven) ä½¿ç”¨åŸåˆ™ä½œä¸ºå¥–åŠ±æ¨¡å‹çš„ä¸Šä¸‹æ–‡ï¼Œè¯¥æ¨¡å‹ç”¨äºè®­ç»ƒDromedaryæ¨¡å‹
    [[250]](ch021.xhtml#ref-sun2024salmon)ã€‚[[37]](ch021.xhtml#ref-glaese2022improving)
    ä½¿ç”¨åŸåˆ™æ¥æé«˜RLHFè¿‡ç¨‹ä¸­äººç±»åˆ¤æ–­çš„å‡†ç¡®æ€§ã€‚[[251]](ch021.xhtml#ref-liu2025inference) è®­ç»ƒä¸€ä¸ªå¥–åŠ±æ¨¡å‹åœ¨æ¨ç†æ—¶ç”Ÿæˆè‡ªå·±çš„åŸåˆ™ï¼Œå¹¶ä½¿ç”¨è¿™äº›åŸåˆ™æ¥æä¾›æœ€ç»ˆè¯„åˆ†ã€‚[[252]](ch021.xhtml#ref-franken2024self)
    å°†éµå¾ªåŸåˆ™è¡¨è¿°ä¸ºä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹å¯ä»¥æ— æ ‡ç­¾å­¦ä¹ çš„äº’ä¿¡æ¯æœ€å¤§åŒ–é—®é¢˜ã€‚
