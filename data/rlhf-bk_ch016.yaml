- en: Synthetic Data & Distillation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 合成数据与蒸馏
- en: Reinforcement learning from *human feedback* is deeply rooted in the idea of
    keeping a human influence on the models we are building. When the first models
    were trained successfully with RLHF, human data was *the only* viable way to improve
    the models in this way.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 从人类反馈中进行强化学习深深植根于保持人类对我们所构建的模型的影响这一理念中。当第一个模型成功使用RLHF进行训练时，人类数据是改善模型这种方式的唯一可行途径。
- en: Humans were the only way to create high enough quality responses to questions
    to train on them. Humans were the only way to collect reliable and specific feedback
    data to train reward models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 人类是唯一能够创建足够高质量响应以进行训练的方式。人类是唯一能够收集可靠和具体反馈数据以训练奖励模型的方式。
- en: As AI models got better, this assumption rapidly broke down. The possibility
    of synthetic data, which is far cheaper and easier to iterate on, enabled the
    proliferation from RLHF being the center of attention to the idea of a broader
    “post-training” shaping the models. This chapter provides a cursory overview of
    how and why synthetic data is replacing or expanding many pieces of the RLHF pipeline.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 随着人工智能模型变得越来越好，这个假设迅速瓦解了。合成数据（成本远低且易于迭代）的可能性使得从强化学习与人类反馈（RLHF）成为关注的中心，到更广泛的“后训练”塑造模型的想法得以扩散。本章简要概述了合成数据是如何以及为什么取代或扩展RLHF流程中的许多部分。
- en: We begin by starting with a common criticism of synthetic data, in order to
    highlight what the data is actually extremely capable at. Many reports have been
    made on how synthetic data causes “model collapse” or other issues in models [[302]](ch021.xhtml#ref-shumailov2024ai),
    but this has been emphatically rebuked in leading language models [[303]](ch021.xhtml#ref-gerstgrasser2024model)
    [[304]](ch021.xhtml#ref-feng2024beyond). Synthetic data *can* cause models to
    have performance issues, but this is caused by using repetitive data or solely
    data outputted by the model being trained (narrowing its potential distribution)
    rather than well-rounded data sources.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先从对合成数据的常见批评开始，以突出数据实际上具有极高的能力。许多报告都提到了合成数据如何导致“模型崩溃”或其他模型问题 [[302]](ch021.xhtml#ref-shumailov2024ai)，但在领先的语模型中这一点被明确反驳
    [[303]](ch021.xhtml#ref-gerstgrasser2024model) [[304]](ch021.xhtml#ref-feng2024beyond)。合成数据
    *确实* 可能会导致模型出现性能问题，但这通常是由于使用了重复数据或仅由训练模型输出的数据（限制了其潜在分布）而不是全面的数据来源。
- en: The leading models **need synthetic data** to reach the best performance. Synthetic
    data in modern post-training encompasses many pieces of training – language models
    are used to generate new training prompts from seed examples [[305]](ch021.xhtml#ref-wang2022self),
    modify existing prompts, generate completions to prompts [[306]](ch021.xhtml#ref-numina_math_7b),
    provide AI feedback to create preference data [[23]](ch021.xhtml#ref-cui2023ultrafeedback),
    filter completions [[307]](ch021.xhtml#ref-li2024superfiltering), and much more.
    Synthetic data is key to post-training.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 领先的模型 **需要合成数据** 来达到最佳性能。现代后训练中的合成数据包括许多训练环节——语言模型被用来从种子示例生成新的训练提示 [[305]](ch021.xhtml#ref-wang2022self)，修改现有提示，生成对提示的完成
    [[306]](ch021.xhtml#ref-numina_math_7b)，提供AI反馈以创建偏好数据 [[23]](ch021.xhtml#ref-cui2023ultrafeedback)，过滤完成
    [[307]](ch021.xhtml#ref-li2024superfiltering)，等等。合成数据是后训练的关键。
- en: The ability for synthetic data to be impactful to this extent emerged with GPT-4
    class models. With early language models, such as Llama 2 and GPT-3.5-Turbo, the
    models were not reliable enough in generating or supervising data pipelines. Within
    1-2 years, language models were far superior to humans for generating answers.
    In the transition from GPT-3.5 to GPT-4 class models, the ability for models to
    perform LLM-as-a-judge tasks also emerged. GPT-4 or better models are far more
    robust and consistent in generating feedback or scores with respect to a piece
    of content.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 合成数据能够产生如此大的影响，这一能力随着GPT-4级模型的出现而出现。在早期的语言模型，如Llama 2和GPT-3.5-Turbo中，模型在生成或监督数据管道方面不够可靠。在1-2年内，语言模型在生成答案方面远胜于人类。在GPT-3.5到GPT-4级模型过渡期间，模型执行LLM作为裁判任务的能力也出现了。GPT-4或更好的模型在生成反馈或评分方面对内容的稳健性和一致性远超以往。
- en: Since this transition, the role of synthetic data has only grown in language
    model training. Otherwise, there are two clear areas where human data continues
    to be important.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 自从这次转变以来，合成数据在语言模型训练中的作用仅是增长。否则，还有两个明显的领域，人类数据仍然非常重要。
- en: The role of human data continues to be at the fringe of capabilities in models
    – humans must generate data where AI’s do not yet have any ability. Once the first
    strong model exists, synthetic data proliferates.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 人类数据在模型中的角色继续处于能力边缘——人类必须在AI尚未具备任何能力的地方生成数据。一旦第一个强大模型存在，合成数据就会迅速增多。
- en: Human preference data is still used in the leading models, even though academic
    work shows synthetic versions to perform just as well. The role of human preferences
    is still being established in the literature.
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尽管学术研究表明合成版本的表现同样出色，但人类偏好数据仍然被用于领先模型中。人类偏好在文献中的作用仍在建立中。
- en: The term distillation has been the most powerful form of discussion around the
    role of synthetic data in language models. Distillation as a term comes from a
    technical definition of teacher-student knowledge distillation from the deep learning
    literature [[51]](ch021.xhtml#ref-hinton2015distilling).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: “蒸馏”这个术语已经成为围绕合成数据在语言模型中作用的讨论中最有力的形式。这个术语来源于深度学习文献中对教师-学生知识蒸馏的技术定义 [[51]](ch021.xhtml#ref-hinton2015distilling)。
- en: 'Distillation colloquially refers to using the outputs from a stronger model
    to train a smaller model. In post-training, this general notion of distillation
    takes two common forms:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: “蒸馏”通俗地说，是指使用更强模型的输出训练一个较小的模型。在后训练中，这种蒸馏的一般概念有两种常见形式：
- en: 'As a data engine to use across wide swaths of the post-training process: Completions
    for instructions, preference data (or Constitutional AI), or verification for
    RL.'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为在广泛的后训练过程中使用的数据引擎：用于指令的补全、偏好数据（或宪法AI），或用于强化学习（RL）的验证。
- en: To transfer specific skills from a stronger model to a weaker model, which is
    often done for specific skill such as mathematic reasoning or coding.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将特定技能从更强的模型转移到较弱的模型，这通常用于特定的技能，如数学推理或编码。
- en: The first strategy has grown in popularity as language models evolved to be
    more reliable than humans at writing answers to a variety of tasks. GPT-4 class
    models expanded the scope of this to use distillation of stronger models for complex
    tasks such as math and code (as mentioned above). Here, distillation motivates
    having a model suite where often a laboratory will train a large internal model,
    such as Claude Opus or Gemini Ultra, which is not released publicly and just used
    internally to make stronger models. With open models, common practice is to distill
    training data from closed API models into smaller, openly available weights [[21]](ch021.xhtml#ref-tunstall2023zephyr).
    Within this, curating high-quality prompts and filtering responses from the teacher
    model is crucial to maximize performance.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 随着语言模型在回答各种任务时比人类更可靠，第一种策略越来越受欢迎。GPT-4类模型将这一范围扩展到使用更强模型的蒸馏来完成复杂任务，如数学和代码（如上所述）。在这里，蒸馏促使拥有一个模型套件，通常实验室会训练一个大型内部模型，如Claude
    Opus或Gemini Ultra，这些模型不会公开发布，仅用于内部使用以制作更强的模型。对于开源模型，常见的做法是将封闭API模型的训练数据蒸馏到更小、公开可用的权重
    [[21]](ch021.xhtml#ref-tunstall2023zephyr)。在此过程中，精心制作高质量的提示和过滤来自教师模型的响应对于最大化性能至关重要。
- en: Transferring specific skills into smaller language models uses the same principles
    of distillation – get the best data possible for training. Here, many papers have
    studied using limited datasets from stronger models to improve alignment [[13]](ch021.xhtml#ref-zhou2023lima),
    mathematic reasoning [[308]](ch021.xhtml#ref-shridhar2023distilling) [[309]](ch021.xhtml#ref-hsieh2023distilling),
    and test-time scaling [[258]](ch021.xhtml#ref-muennighoff2025s1).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 将特定技能转移到较小的语言模型时，使用与蒸馏相同的原理——获取尽可能好的训练数据。在这里，许多论文研究了使用来自更强模型的有限数据集来提高一致性 [[13]](ch021.xhtml#ref-zhou2023lima)，数学推理
    [[308]](ch021.xhtml#ref-shridhar2023distilling) [[309]](ch021.xhtml#ref-hsieh2023distilling)，以及测试时缩放
    [[258]](ch021.xhtml#ref-muennighoff2025s1)。
