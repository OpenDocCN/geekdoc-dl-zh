- en: 3.1 Introduction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3.1 简介
- en: 原文：[https://boramorka.github.io/LLM-Book/en/CHAPTER-3/3.1%20Introduction/](https://boramorka.github.io/LLM-Book/en/CHAPTER-3/3.1%20Introduction/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[https://boramorka.github.io/LLM-Book/en/CHAPTER-3/3.1%20Introduction/](https://boramorka.github.io/LLM-Book/en/CHAPTER-3/3.1%20Introduction/)'
- en: 'Bringing large language models into the software development process is the
    next turn in the evolution of AI products. This section is a practical introduction
    to LLMOps, covering the full lifecycle of LLM‑based applications: from model selection
    and fine‑tuning to production deployment, monitoring, and ongoing operations.
    LLMs understand and generate human‑like text, so they are used for summarization,
    classification, content generation, and many other tasks. Their strengths are
    broad knowledge from training on large corpora, adaptability to a wide range of
    scenarios without heavy task‑specific training, and the ability to work with context
    and capture nuance. Building on this, LLMOps acts as the LLM‑focused layer of
    MLOps: model selection and domain preparation, thoughtful deployment to meet SLAs,
    continuous monitoring with metrics and alerts, plus security and privacy with
    ethical principles and data protection.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 将大型语言模型引入软件开发流程是 AI 产品演化的下一个转折点。本节是 LLMOps 的实用介绍，涵盖了基于 LLM 的应用程序的完整生命周期：从模型选择和微调到生产部署、监控和持续运营。LLM
    能够理解和生成类似人类的文本，因此它们被用于摘要、分类、内容生成和其他许多任务。它们的优点是来自大型语料库训练的广泛知识、适应广泛场景的能力，无需进行繁重的特定任务训练，以及能够处理上下文并捕捉细微差别的能力。在此基础上，LLMOps
    作为 MLOps 的 LLM 专用层：模型选择和领域准备、深思熟虑的部署以满足服务等级协议、使用指标和警报的持续监控，以及遵循道德原则和数据保护的安全和隐私。
- en: 'An LLMOps roadmap typically includes several steps. First, choose a model by
    size, training data, and benchmarks: match metrics to your task and prepare a
    fine‑tuning dataset that faithfully reflects the domain and goals. Next, design
    deployment architecture and infrastructure: plan for scale with headroom for peaks,
    minimize latency via caching and shorter execution paths, and account for integrations.
    In production, rely on continuous monitoring to catch degradation and data drift;
    define KPI/SLI up front, and bake in regular updates and regression tests. Throughout,
    protect privacy and security: anonymize sensitive fields, control access to models,
    prevent abuse, and formalize a responsible‑AI policy.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: LLMOps 路线图通常包括几个步骤。首先，根据模型大小、训练数据和基准选择一个模型：将指标与你的任务匹配，并准备一个忠实反映领域和目标的微调数据集。接下来，设计部署架构和基础设施：规划可扩展性，留有高峰期的余量，通过缓存和更短的执行路径来最小化延迟，并考虑集成。在生产中，依靠持续监控来捕捉退化和数据漂移；提前定义
    KPI/SLI，并内置定期更新和回归测试。在整个过程中，保护隐私和安全：匿名化敏感字段，控制对模型的访问，防止滥用，并正式制定负责任的 AI 政策。
- en: 'An LLM app’s structure typically involves selection and fine‑tuning: evaluate
    available options and their fit to your requirements, then adapt the model to
    your domain using prompt engineering, PEFT/LoRA, and other methods — paying attention
    to infrastructure compatibility and to the cost/efficiency balance of tuning techniques.
    Deployment is often a REST API around the model or an orchestrator; observability
    and real‑time metric tracking are critical to understand model health and react
    quickly to incidents. Automate anything repetitive: prompt management with versioning
    and A/B tests, automated tests and CI/CD, orchestration of multi‑step LLM chains
    and their dependencies. Data preparation underpins effective tuning: use SQL/ETL
    and open tooling to build clean data marts; orchestrate complex workflows to meet
    SLAs, with retries and idempotency as first‑class properties.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 LLM 应用程序的结构通常涉及选择和微调：评估可用选项及其与你的要求的匹配度，然后使用提示工程、PEFT/LoRA 和其他方法将模型适应你的领域——注意基础设施兼容性和微调技术的成本/效率平衡。部署通常是围绕模型的
    REST API 或一个编排器；可观测性和实时指标跟踪对于理解模型健康状况和快速响应事件至关重要。自动化任何重复性任务：带有版本控制和 A/B 测试的提示管理、自动测试和
    CI/CD、多步骤 LLM 链及其依赖关系的编排。数据准备是有效微调的基础：使用 SQL/ETL 和开源工具构建干净的数据集市；编排复杂的流程以满足服务等级协议，将重试和幂等性作为一等属性。
- en: 'Best practices rest on three pillars: automation (tests and CI/CD speed up
    reliable releases), prompt management (context‑aware dynamics and steady A/B testing
    improve quality), and case‑by‑case scaling (a modular architecture that adds new
    scenarios without breaking existing ones, and capacity planning for load). Given
    how fast LLMs and MLOps change, build in flexibility: follow trends, engage with
    the community, and regularly take courses and workshops.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳实践建立在三个支柱之上：自动化（测试和CI/CD加快了可靠的发布）、提示管理（上下文感知的动态和稳定的A/B测试提高了质量），以及逐案扩展（模块化架构在不破坏现有功能的情况下添加新场景，以及负载容量规划）。鉴于LLM和MLOps变化之快，请内置灵活性：跟随趋势，与社区互动，并定期参加课程和研讨会。
- en: 'From practice: automating support with an LLM chatbot plus dynamic prompt management
    reduces response time and improves service quality; in publishing, a summarization‑and‑editing
    pipeline together with prompt management radically speeds article production.
    Overall, a structured approach to LLMOps — with automation, solid prompt management,
    thoughtful scalability, and a culture of continuous learning — is key to building
    and operating successful LLM applications. For deeper study, keep these at hand:
    WhyLabs’ “A Guide to LLMOps” with material on prompts, evaluation, testing, and
    scaling; Weights & Biases “Understanding LLMOps” — a review of open and proprietary
    LLMs with monitoring practices; and the DataRobot AI Wiki, which positions LLMOps
    as a subset of MLOps and covers adjacent topics.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 从实践中来：使用大型语言模型聊天机器人和动态提示管理来自动化支持，可以减少响应时间并提高服务质量；在出版领域，总结和编辑流程加上提示管理可以极大地加快文章的生产速度。总的来说，采用结构化的LLMOps方法——包括自动化、坚实的提示管理、深思熟虑的可扩展性和持续学习的文化——是构建和运营成功的LLM应用的关键。为了进行更深入的研究，请保留以下资料：WhyLabs的《LLMOps指南》中包含有关提示、评估、测试和扩展的材料；《Weights
    & Biases “Understanding LLMOps”》——对开源和专有LLM的回顾，包括监控实践；以及DataRobot AI Wiki，它将LLMOps定位为MLOps的一个子集，并涵盖了相关主题。
