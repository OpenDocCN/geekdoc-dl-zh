- en: Definitions & Background
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®šä¹‰ä¸èƒŒæ™¯
- en: This chapter includes all the definitions, symbols, and operations frequently
    used in the RLHF process and with a quick overview of language models, which is
    the guiding application of this book.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« åŒ…æ‹¬åœ¨RLHFè¿‡ç¨‹ä¸­ç»å¸¸ä½¿ç”¨çš„æ‰€æœ‰å®šä¹‰ã€ç¬¦å·å’Œæ“ä½œï¼Œä»¥åŠå¯¹è¯­è¨€æ¨¡å‹çš„å¿«é€Ÿæ¦‚è¿°ï¼Œè¿™æ˜¯æœ¬ä¹¦çš„æŒ‡å¯¼åº”ç”¨ã€‚
- en: Language Modeling Overview
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯­è¨€æ¨¡å‹æ¦‚è¿°
- en: 'The majority of modern language models are trained to learn the joint probability
    distribution of sequences of tokens (words, subwords, or characters) in an autoregressive
    manner. Autoregression simply means that each next prediction depends on the previous
    entities in the sequence. Given a sequence of tokens <semantics><mrow><mi>x</mi><mo>=</mo><mo
    stretchy="false" form="prefix">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi>â€¦</mi><mo>,</mo><msub><mi>x</mi><mi>T</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">x
    = (x_1, x_2, \ldots, x_T)</annotation></semantics>, the model factorizes the probability
    of the entire sequence into a product of conditional distributions:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å¤šæ•°ç°ä»£è¯­è¨€æ¨¡å‹éƒ½æ˜¯è®­ç»ƒä»¥è‡ªå›å½’æ–¹å¼å­¦ä¹ æ ‡è®°åºåˆ—ï¼ˆå•è¯ã€å­è¯æˆ–å­—ç¬¦ï¼‰çš„è”åˆæ¦‚ç‡åˆ†å¸ƒã€‚è‡ªå›å½’ç®€å•æ¥è¯´å°±æ˜¯æ¯ä¸ªä¸‹ä¸€ä¸ªé¢„æµ‹éƒ½ä¾èµ–äºåºåˆ—ä¸­çš„å…ˆå‰å®ä½“ã€‚ç»™å®šä¸€ä¸ªæ ‡è®°åºåˆ—
    <semantics><mrow><mi>x</mi><mo>=</mo><mo stretchy="false" form="prefix">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi>â€¦</mi><mo>,</mo><msub><mi>x</mi><mi>T</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">x
    = (x_1, x_2, \ldots, x_T)</annotation></semantics>ï¼Œæ¨¡å‹å°†æ•´ä¸ªåºåˆ—çš„æ¦‚ç‡åˆ†è§£ä¸ºæ¡ä»¶åˆ†å¸ƒçš„ä¹˜ç§¯ï¼š
- en: <semantics><mrow><msub><mi>P</mi><mi>Î¸</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><munderover><mo>âˆ</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><msub><mi>P</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>âˆ£</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mi>â€¦</mi><mo>,</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>âˆ’</mo><mn>1</mn></mrow></msub><mo
    stretchy="false" form="postfix">)</mo><mi>.</mi><mrow><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P_{\theta}(x)
    = \prod_{t=1}^{T} P_{\theta}(x_{t} \mid x_{1}, \ldots, x_{t-1}).\qquad{(1)}</annotation></semantics>
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><msub><mi>P</mi><mi>Î¸</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><munderover><mo>âˆ</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><msub><mi>P</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>âˆ£</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mi>â€¦</mi><mo>,</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>âˆ’</mo><mn>1</mn></mrow></msub><mo
    stretchy="false" form="postfix">)</mo><mi>.</mi><mrow><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P_{\theta}(x)
    = \prod_{t=1}^{T} P_{\theta}(x_{t} \mid x_{1}, \ldots, x_{t-1}).\qquad{(1)}</annotation></semantics>
- en: 'In order to fit a model that accurately predicts this, the goal is often to
    maximize the likelihood of the training data as predicted by the current model.
    To do so, we can minimize a negative log-likelihood (NLL) loss:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ‹Ÿåˆä¸€ä¸ªèƒ½å¤Ÿå‡†ç¡®é¢„æµ‹æ­¤æƒ…å†µçš„æ¨¡å‹ï¼Œç›®æ ‡é€šå¸¸æ˜¯æœ€å¤§åŒ–å½“å‰æ¨¡å‹é¢„æµ‹çš„è®­ç»ƒæ•°æ®çš„ä¼¼ç„¶åº¦ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥æœ€å°åŒ–ä¸€ä¸ªè´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆNLLï¼‰æŸå¤±ï¼š
- en: <semantics><mrow><msub><mi>â„’</mi><mtext mathvariant="normal">LM</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>âˆ’</mi><msub><mi>ğ”¼</mi><mrow><mi>x</mi><mo>âˆ¼</mo><mi>ğ’Ÿ</mi></mrow></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><munderover><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mi
    mathvariant="normal">log</mi><msub><mi>P</mi><mi>Î¸</mi></msub><mrow><mo stretchy="true"
    form="prefix">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>âˆ£</mo><msub><mi>x</mi><mrow><mo><</mo><mi>t</mi></mrow></msub><mo
    stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mi>.</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>2</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">\mathcal{L}_{\text{LM}}(\theta)=-\,\mathbb{E}_{x
    \sim \mathcal{D}}\left[\sum_{t=1}^{T}\log P_{\theta}\left(x_t \mid x_{<t}\right)\right].
    \qquad{(2)}</annotation></semantics>
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><msub><mi>â„’</mi><mtext mathvariant="normal">LM</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>âˆ’</mi><msub><mi>ğ”¼</mi><mrow><mi>x</mi><mo>âˆ¼</mo><mi>ğ’Ÿ</mi></mrow></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><munderover><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mi
    mathvariant="normal">log</mi><msub><mi>P</mi><mi>Î¸</mi></msub><mrow><mo stretchy="true"
    form="prefix">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>âˆ£</mo><msub><mi>x</mi><mrow><mo><</mo><mi>t</mi></mrow></msub><mo
    stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mi>.</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>2</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">\mathcal{L}_{\text{LM}}(\theta)=-\,\mathbb{E}_{x
    \sim \mathcal{D}}\left[\sum_{t=1}^{T}\log P_{\theta}\left(x_t \mid x_{<t}\right)\right].
    \qquad{(2)}</annotation></semantics>
- en: In practice, one uses a cross-entropy loss with respect to each next-token prediction,
    computed by comparing the true token in a sequence to what was predicted by the
    model.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®è·µä¸­ï¼Œäººä»¬ä½¿ç”¨é’ˆå¯¹æ¯ä¸ªä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹çš„äº¤å‰ç†µæŸå¤±ï¼Œé€šè¿‡æ¯”è¾ƒåºåˆ—ä¸­çš„çœŸå®æ ‡è®°ä¸æ¨¡å‹é¢„æµ‹çš„å†…å®¹æ¥è®¡ç®—ã€‚
- en: Language models come in many architectures with different trade-offs in terms
    of knowledge, speed, and other performance characteristics. Modern LMs, including
    ChatGPT, Claude, Gemini, etc., most often use **decoder-only Transformers** [[49]](ch021.xhtml#ref-Vaswani2017AttentionIA).
    The core innovation of the Transformer was heavily utilizing the **self-attention**
    [[50]](ch021.xhtml#ref-Bahdanau2014NeuralMT) mechanism to allow the model to directly
    attend to concepts in context and learn complex mappings. Throughout this book,
    particularly when covering reward models in Chapter 7, we will discuss adding
    new heads or modifying a language modeling (LM) head of the transformer. The LM
    head is a final linear projection layer that maps from the modelâ€™s internal embedding
    space to the tokenizer space (a.k.a. vocabulary). Weâ€™ll see in this book that
    different â€œheadsâ€ of a language model can be applied to finetune the model to
    different purposes â€“ in RLHF this is most often done when training a reward model,
    which is highlighted in Chapter 7.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è¯­è¨€æ¨¡å‹æœ‰å¤šç§æ¶æ„ï¼Œåœ¨çŸ¥è¯†ã€é€Ÿåº¦å’Œå…¶ä»–æ€§èƒ½ç‰¹å¾æ–¹é¢æœ‰ä¸åŒçš„æƒè¡¡ã€‚ç°ä»£è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬ChatGPTã€Claudeã€Geminiç­‰ï¼Œé€šå¸¸ä½¿ç”¨**ä»…è§£ç å™¨Transformer**
    [[49]](ch021.xhtml#ref-Vaswani2017AttentionIA)ã€‚Transformerçš„æ ¸å¿ƒåˆ›æ–°æ˜¯å¤§é‡åˆ©ç”¨**è‡ªæ³¨æ„åŠ›** [[50]](ch021.xhtml#ref-Bahdanau2014NeuralMT)æœºåˆ¶ï¼Œå…è®¸æ¨¡å‹ç›´æ¥å…³æ³¨ä¸Šä¸‹æ–‡ä¸­çš„æ¦‚å¿µå¹¶å­¦ä¹ å¤æ‚çš„æ˜ å°„ã€‚åœ¨æœ¬ä¹¦ä¸­ï¼Œå°¤å…¶æ˜¯åœ¨ç¬¬7ç« ä»‹ç»å¥–åŠ±æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬å°†è®¨è®ºæ·»åŠ æ–°çš„å¤´æˆ–ä¿®æ”¹Transformerçš„è¯­è¨€æ¨¡å‹å¤´ã€‚è¯­è¨€æ¨¡å‹å¤´æ˜¯ä¸€ä¸ªæœ€ç»ˆçš„çº¿æ€§æŠ•å½±å±‚ï¼Œå®ƒå°†æ¨¡å‹å†…éƒ¨åµŒå…¥ç©ºé—´æ˜ å°„åˆ°åˆ†è¯å™¨ç©ºé—´ï¼ˆå³è¯æ±‡è¡¨ï¼‰ã€‚åœ¨æœ¬ä¹¦ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°è¯­è¨€æ¨¡å‹çš„ä¸åŒâ€œå¤´â€å¯ä»¥åº”ç”¨äºå¾®è°ƒæ¨¡å‹ä»¥é€‚åº”ä¸åŒçš„ç›®çš„â€”â€”åœ¨RLHFä¸­ï¼Œè¿™é€šå¸¸åœ¨è®­ç»ƒå¥–åŠ±æ¨¡å‹æ—¶è¿›è¡Œï¼Œè¿™åœ¨ç¬¬7ç« ä¸­å¾—åˆ°äº†å¼ºè°ƒã€‚
- en: ML Definitions
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æœºå™¨å­¦ä¹ å®šä¹‰
- en: '**Kullback-Leibler (KL) divergence (<semantics><mrow><msub><mi>ğ’Ÿ</mi><mtext
    mathvariant="normal">KL</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>P</mi><mo
    stretchy="false" form="prefix">|</mo><mo stretchy="false" form="prefix">|</mo><mi>Q</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{D}_{\text{KL}}(P
    || Q)</annotation></semantics>)**, also known as KL divergence, is a measure of
    the difference between two probability distributions. For discrete probability
    distributions <semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics>
    and <semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics>
    defined on the same probability space <semantics><mi>ğ’³</mi><annotation encoding="application/x-tex">\mathcal{X}</annotation></semantics>,
    the KL distance from <semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics>
    to <semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics>
    is defined as:'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kullback-Leibler (KL) æ•£åº¦ (KL æ•£åº¦ (<semantics><mrow><msub><mi>ğ’Ÿ</mi><mtext
    mathvariant="normal">KL</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>P</mi><mo
    stretchy="false" form="prefix">|</mo><mo stretchy="false" form="prefix">|</mo><mi>Q</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{D}_{\text{KL}}(P
    || Q)</annotation></semantics>)**)ï¼Œä¹Ÿç§°ä¸º KL æ•£åº¦ï¼Œæ˜¯è¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´å·®å¼‚çš„åº¦é‡ã€‚å¯¹äºåœ¨ç›¸åŒæ¦‚ç‡ç©ºé—´ <semantics><mi>ğ’³</mi><annotation
    encoding="application/x-tex">\mathcal{X}</annotation></semantics> ä¸Šå®šä¹‰çš„ç¦»æ•£æ¦‚ç‡åˆ†å¸ƒ <semantics><mi>P</mi><annotation
    encoding="application/x-tex">P</annotation></semantics> å’Œ <semantics><mi>Q</mi><annotation
    encoding="application/x-tex">Q</annotation></semantics>ï¼Œä» <semantics><mi>Q</mi><annotation
    encoding="application/x-tex">Q</annotation></semantics> åˆ° <semantics><mi>P</mi><annotation
    encoding="application/x-tex">P</annotation></semantics> çš„ KL è·ç¦»å®šä¹‰ä¸ºï¼š'
- en: <semantics><mrow><msub><mi>ğ’Ÿ</mi><mtext mathvariant="normal">KL</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>P</mi><mo stretchy="false" form="prefix">|</mo><mo
    stretchy="false" form="prefix">|</mo><mi>Q</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><munder><mo>âˆ‘</mo><mrow><mi>x</mi><mo>âˆˆ</mo><mi>ğ’³</mi></mrow></munder><mi>P</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mrow><mi
    mathvariant="normal">log</mi><mo>â¡</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>P</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><mi>Q</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo
    stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="false" form="prefix">(</mo><mn>3</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\mathcal{D}_{\text{KL}}(P
    || Q) = \sum_{x \in \mathcal{X}} P(x) \log \left(\frac{P(x)}{Q(x)}\right) \qquad{(3)}</annotation></semantics>
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><msub><mi>ğ’Ÿ</mi><mtext mathvariant="normal">KL</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>P</mi><mo stretchy="false" form="prefix">|</mo><mo
    stretchy="false" form="prefix">|</mo><mi>Q</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><munder><mo>âˆ‘</mo><mrow><mi>x</mi><mo>âˆˆ</mo><mi>ğ’³</mi></mrow></munder><mi>P</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mrow><mi
    mathvariant="normal">log</mi><mo>â¡</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>P</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><mi>Q</mi><mo
    stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo
    stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="false" form="prefix">(</mo><mn>3</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\mathcal{D}_{\text{KL}}(P
    || Q) = \sum_{x \in \mathcal{X}} P(x) \log \left(\frac{P(x)}{Q(x)}\right) \qquad{(3)}</annotation></semantics>
- en: NLP Definitions
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NLP å®šä¹‰
- en: '**Prompt (<semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics>)**:
    The input text given to a language model to generate a response or completion.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æç¤º (<semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics>)**ï¼šç»™è¯­è¨€æ¨¡å‹è¾“å…¥çš„æ–‡æœ¬ï¼Œç”¨äºç”Ÿæˆå“åº”æˆ–è¡¥å…¨ã€‚'
- en: '**Completion (<semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics>)**:
    The output text generated by a language model in response to a prompt. Often the
    completion is denoted as <semantics><mrow><mi>y</mi><mo>âˆ£</mo><mi>x</mi></mrow><annotation
    encoding="application/x-tex">y\mid x</annotation></semantics>. Rewards and other
    values are often computed as <semantics><mrow><mi>r</mi><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">r(y\mid
    x)</annotation></semantics> or <semantics><mrow><mi>P</mi><mo stretchy="false"
    form="prefix">(</mo><mi>y</mi><mo>âˆ£</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">P(y\mid x)</annotation></semantics>.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å®Œæˆæƒ…å†µï¼ˆ<semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics>ï¼‰**:
    è¯­è¨€æ¨¡å‹é’ˆå¯¹æç¤ºç”Ÿæˆçš„è¾“å‡ºæ–‡æœ¬ã€‚é€šå¸¸å®Œæˆæƒ…å†µè¡¨ç¤ºä¸º <semantics><mrow><mi>y</mi><mo>âˆ£</mo><mi>x</mi></mrow><annotation
    encoding="application/x-tex">y\mid x</annotation></semantics>ã€‚å¥–åŠ±å’Œå…¶ä»–å€¼é€šå¸¸è®¡ç®—ä¸º <semantics><mrow><mi>r</mi><mo
    stretchy="false" form="prefix">(</mo><mi>y</mi><mo>âˆ£</mo><mi>x</mi><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">r(y\mid x)</annotation></semantics>
    æˆ– <semantics><mrow><mi>P</mi><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">P(y\mid
    x)</annotation></semantics>.'
- en: '**Chosen Completion (<semantics><msub><mi>y</mi><mi>c</mi></msub><annotation
    encoding="application/x-tex">y_c</annotation></semantics>)**: The completion that
    is selected or preferred over other alternatives, often denoted as <semantics><msub><mi>y</mi><mrow><mi>c</mi><mi>h</mi><mi>o</mi><mi>s</mi><mi>e</mi><mi>n</mi></mrow></msub><annotation
    encoding="application/x-tex">y_{chosen}</annotation></semantics>.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é€‰æ‹©çš„å®Œæˆæƒ…å†µï¼ˆ<semantics><msub><mi>y</mi><mi>c</mi></msub><annotation encoding="application/x-tex">y_c</annotation></semantics>ï¼‰**:
    è¢«é€‰ä¸­æˆ–ä¼˜å…ˆäºå…¶ä»–æ›¿ä»£æ–¹æ¡ˆçš„å®Œæˆæƒ…å†µï¼Œé€šå¸¸è¡¨ç¤ºä¸º <semantics><msub><mi>y</mi><mrow><mi>c</mi><mi>h</mi><mi>o</mi><mi}s</mi><mi>e</mi><mi>n</mi></mrow></msub><annotation
    encoding="application/x-tex">y_{chosen}</annotation></semantics>.'
- en: '**Rejected Completion (<semantics><msub><mi>y</mi><mi>r</mi></msub><annotation
    encoding="application/x-tex">y_r</annotation></semantics>)**: The disfavored completion
    in a pairwise setting.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ‹’ç»çš„å®Œæˆæƒ…å†µï¼ˆ<semantics><msub><mi>y</mi><mi>r</mi></msub><annotation encoding="application/x-tex">y_r</annotation></semantics>ï¼‰**:
    åœ¨æˆå¯¹è®¾ç½®ä¸­ä¸å—æ¬¢è¿çš„å®Œæˆæƒ…å†µã€‚'
- en: '**Preference Relation (<semantics><mo>â‰»</mo><annotation encoding="application/x-tex">\succ</annotation></semantics>)**:
    A symbol indicating that one completion is preferred over another, e.g., <semantics><mrow><msub><mi>y</mi><mrow><mi>c</mi><mi>h</mi><mi>o</mi><mi>s</mi><mi>e</mi><mi>n</mi></mrow></msub><mo>â‰»</mo><msub><mi>y</mi><mrow><mi>r</mi><mi>e</mi><mi>j</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>e</mi><mi>d</mi></mrow></msub></mrow><annotation
    encoding="application/x-tex">y_{chosen} \succ y_{rejected}</annotation></semantics>.
    E.g. a reward model predicts the probability of a preference relation, <semantics><mrow><mi>P</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>â‰»</mo><msub><mi>y</mi><mi>r</mi></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">P(y_c
    \succ y_r \mid x)</annotation></semantics>.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**åå¥½å…³ç³»ï¼ˆ<semantics><mo>â‰»</mo><annotation encoding="application/x-tex">\succ</annotation></semantics>ï¼‰**:
    è¡¨ç¤ºä¸€ä¸ªå®Œæˆæƒ…å†µæ¯”å¦ä¸€ä¸ªæ›´å—æ¬¢è¿çš„ç¬¦å·ï¼Œä¾‹å¦‚ï¼Œ<semantics><mrow><msub><mi>y</mi><mrow><mi>c</mi><mi>h</mi><mi>o</mi><mi}s</mi><mi>e</mi><mi>n</mi></mrow></msub><mo>â‰»</mo><msub><mi>y</mi><mrow><mi>r</mi><mi>e</mi><mi>j</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>e</mi><mi>d</mi></mrow></msub></mrow><annotation
    encoding="application/x-tex">y_{chosen} \succ y_{rejected}</annotation></semantics>ã€‚ä¾‹å¦‚ï¼Œå¥–åŠ±æ¨¡å‹é¢„æµ‹åå¥½å…³ç³»çš„æ¦‚ç‡ï¼Œ<semantics><mrow><mi>P</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>â‰»</mo><msub><mi>y</mi><mi>r</mi></msub><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">P(y_c
    \succ y_r \mid x)</annotation></semantics>.'
- en: '**Policy (<semantics><mi>Ï€</mi><annotation encoding="application/x-tex">\pi</annotation></semantics>)**:
    A probability distribution over possible completions, parameterized by <semantics><mi>Î¸</mi><annotation
    encoding="application/x-tex">\theta</annotation></semantics>: <semantics><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><mi>y</mi><mo>âˆ£</mo><mi>x</mi><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\pi_\theta(y\mid
    x)</annotation></semantics>.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç­–ç•¥ï¼ˆ<semantics><mi>Ï€</mi><annotation encoding="application/x-tex">\pi</annotation></semantics>ï¼‰**:
    åœ¨å¯èƒ½çš„å®Œæˆæƒ…å†µä¸Šçš„æ¦‚ç‡åˆ†å¸ƒï¼Œç”± <semantics><mi>Î¸</mi><annotation encoding="application/x-tex">\theta</annotation></semantics>
    å‚æ•°åŒ–ï¼š<semantics><mrow><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>y</mi><mo>âˆ£</mo><mi>x</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\pi_\theta(y\mid
    x)</annotation></semantics>.'
- en: RL Definitions
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RL å®šä¹‰
- en: '**Reward (<semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics>)**:
    A scalar value indicating the desirability of an action or state, typically denoted
    as <semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics>.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¥–åŠ± (<semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics>)**:
    è¡¨ç¤ºåŠ¨ä½œæˆ–çŠ¶æ€çš„å¯å–æ€§çš„æ ‡é‡å€¼ï¼Œé€šå¸¸è¡¨ç¤ºä¸º <semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics>ã€‚'
- en: '**Action (<semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics>)**:
    A decision or move made by an agent in an environment, often represented as <semantics><mrow><mi>a</mi><mo>âˆˆ</mo><mi>A</mi></mrow><annotation
    encoding="application/x-tex">a \in A</annotation></semantics>, where <semantics><mi>A</mi><annotation
    encoding="application/x-tex">A</annotation></semantics> is the set of possible
    actions.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**åŠ¨ä½œ (<semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics>)**:
    æ™ºèƒ½ä½“åœ¨ç¯å¢ƒä¸­åšå‡ºçš„å†³ç­–æˆ–ç§»åŠ¨ï¼Œé€šå¸¸è¡¨ç¤ºä¸º <semantics><mrow><mi>a</mi><mo>âˆˆ</mo><mi>A</mi></mrow><annotation
    encoding="application/x-tex">a \in A</annotation></semantics>ï¼Œå…¶ä¸­ <semantics><mi>A</mi><annotation
    encoding="application/x-tex">A</annotation></semantics> æ˜¯å¯èƒ½åŠ¨ä½œçš„é›†åˆã€‚'
- en: '**State (<semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics>)**:
    The current configuration or situation of the environment, usually denoted as
    <semantics><mrow><mi>s</mi><mo>âˆˆ</mo><mi>S</mi></mrow><annotation encoding="application/x-tex">s
    \in S</annotation></semantics>, where <semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics>
    is the state space.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**çŠ¶æ€ (<semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics>)**:
    ç¯å¢ƒçš„å½“å‰é…ç½®æˆ–æƒ…å†µï¼Œé€šå¸¸è¡¨ç¤ºä¸º <semantics><mrow><mi>s</mi><mo>âˆˆ</mo><mi>S</mi></mrow><annotation
    encoding="application/x-tex">s \in S</annotation></semantics>ï¼Œå…¶ä¸­ <semantics><mi>S</mi><annotation
    encoding="application/x-tex">S</annotation></semantics> æ˜¯çŠ¶æ€ç©ºé—´ã€‚'
- en: '**Trajectory (<semantics><mi>Ï„</mi><annotation encoding="application/x-tex">\tau</annotation></semantics>)**:
    A trajectory <semantics><mi>Ï„</mi><annotation encoding="application/x-tex">\tau</annotation></semantics>
    is a sequence of states, actions, and rewards experienced by an agent: <semantics><mrow><mi>Ï„</mi><mo>=</mo><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo>,</mo><msub><mi>a</mi><mn>0</mn></msub><mo>,</mo><msub><mi>r</mi><mn>0</mn></msub><mo>,</mo><msub><mi>s</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>r</mi><mn>1</mn></msub><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi><mo>,</mo><msub><mi>s</mi><mi>T</mi></msub><mo>,</mo><msub><mi>a</mi><mi>T</mi></msub><mo>,</mo><msub><mi>r</mi><mi>T</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\tau
    = (s_0, a_0, r_0, s_1, a_1, r_1, ..., s_T, a_T, r_T)</annotation></semantics>.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è½¨è¿¹ (<semantics><mi>Ï„</mi><annotation encoding="application/x-tex">\tau</annotation></semantics>)**:
    ä¸€ä¸ªè½¨è¿¹ <semantics><mi>Ï„</mi><annotation encoding="application/x-tex">\tau</annotation></semantics>
    æ˜¯ä¸€ä¸ªç”±æ™ºèƒ½ä½“ç»å†çš„çŠ¶æ€ã€åŠ¨ä½œå’Œå¥–åŠ±çš„åºåˆ—ï¼š<semantics><mrow><mi>Ï„</mi><mo>=</mo><mo stretchy="false"
    form="prefix">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo>,</mo><msub><mi>a</mi><mn>0</mn></msub><mo>,</mo><msub><mi>r</mi><mn>0</mn></msub><mo>,</mo><msub><mi>s</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>r</mi><mn>1</mn></msub><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi><mo>,</mo><msub><mi>s</mi><mi>T</mi></msub><mo>,</mo><msub><mi>a</mi><mi>T</mi></msub><mo>,</mo><msub><mi>r</mi><mi>T</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\tau
    = (s_0, a_0, r_0, s_1, a_1, r_1, ..., s_T, a_T, r_T)</annotation></semantics>.'
- en: '**Trajectory Distribution (<semantics><mrow><mo stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo>âˆ£</mo><mi>Ï€</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\tau\mid\pi)</annotation></semantics>)**:
    The probability of a trajectory under policy <semantics><mi>Ï€</mi><annotation
    encoding="application/x-tex">\pi</annotation></semantics> is <semantics><mrow><mi>P</mi><mo
    stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo>âˆ£</mo><mi>Ï€</mi><mo stretchy="false"
    form="postfix">)</mo><mo>=</mo><mi>p</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo
    stretchy="false" form="postfix">)</mo><msubsup><mo>âˆ</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>T</mi></msubsup><mi>Ï€</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>âˆ£</mo><msub><mi>s</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mi>p</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>âˆ£</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">P(\tau\mid\pi)
    = p(s_0)\prod_{t=0}^T \pi(a_t\mid s_t)p(s_{t+1}\mid s_t,a_t)</annotation></semantics>,
    where <semantics><mrow><mi>p</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">p(s_0)</annotation></semantics>
    is the prior state distribution and <semantics><mrow><mi>p</mi><mo stretchy="false"
    form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>âˆ£</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">p(s_{t+1}\mid
    s_t,a_t)</annotation></semantics> is the transition probability.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è½¨è¿¹åˆ†å¸ƒï¼ˆ<semantics><mrow><mo stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo>âˆ£</mo><mi>Ï€</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\tau\mid\pi)</annotation></semantics>**)ï¼šåœ¨ç­–ç•¥<semantics><mi>Ï€</mi><annotation
    encoding="application/x-tex">\pi</annotation></semantics>ä¸‹ï¼Œè½¨è¿¹çš„æ¦‚ç‡æ˜¯<semantics><mrow><mi>P</mi><mo
    stretchy="false" form="prefix">(</mo><mi>Ï„</mi><mo>âˆ£</mo><mi>Ï€</mi><mo stretchy="false"
    form="postfix">)</mo><mo>=</mo><mi>p</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo
    stretchy="false" form="postfix">)</mo><msubsup><mo>âˆ</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>T</mi></msubsup><mi>Ï€</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>âˆ£</mo><msub><mi>s</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mi>p</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>âˆ£</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">P(\tau\mid\pi)
    = p(s_0)\prod_{t=0}^T \pi(a_t\mid s_t)p(s_{t+1}\mid s_t,a_t)</annotation></semantics>ï¼Œå…¶ä¸­<semantics><mrow><mi>p</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">p(s_0)</annotation></semantics>æ˜¯å…ˆéªŒçŠ¶æ€åˆ†å¸ƒï¼Œè€Œ<semantics><mrow><mi>p</mi><mo
    stretchy="false" form="prefix">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>âˆ£</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">p(s_{t+1}\mid
    s_t,a_t)</annotation></semantics>æ˜¯è½¬ç§»æ¦‚ç‡ã€‚'
- en: '**Policy (<semantics><mi>Ï€</mi><annotation encoding="application/x-tex">\pi</annotation></semantics>)**,
    also called the **policy model** in RLHF: In RL, a policy is a strategy or rule
    that the agent follows to decide which action to take in a given state: <semantics><mrow><mi>Ï€</mi><mo
    stretchy="false" form="prefix">(</mo><mi>a</mi><mo>âˆ£</mo><mi>s</mi><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\pi(a\mid
    s)</annotation></semantics>.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç­–ç•¥ï¼ˆ<semantics><mi>Ï€</mi><annotation encoding="application/x-tex">\pi</annotation></semantics>**)ï¼Œä¹Ÿç§°ä¸ºRLHFä¸­çš„**ç­–ç•¥æ¨¡å‹**ï¼šåœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œç­–ç•¥æ˜¯æ™ºèƒ½ä½“åœ¨ç»™å®šçŠ¶æ€ä¸‹å†³å®šé‡‡å–å“ªä¸ªåŠ¨ä½œçš„ç­–ç•¥æˆ–è§„åˆ™ï¼š<semantics><mrow><mi>Ï€</mi><mo
    stretchy="false" form="prefix">(</mo><mi>a</mi><mo>âˆ£</mo><mi>s</mi><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\pi(a\mid
    s)</annotation></semantics>.'
- en: '**Discount Factor (<semantics><mi>Î³</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics>)**:
    A scalar <semantics><mrow><mn>0</mn><mo>â‰¤</mo><mi>Î³</mi><mo><</mo><mn>1</mn></mrow><annotation
    encoding="application/x-tex">0 \le \gamma < 1</annotation></semantics> that exponentially
    down-weights future rewards in the return, trading off immediacy versus long-term
    gain and guaranteeing convergence for infinite-horizon sums. Sometimes discounting
    is not used, which is equivalent to <semantics><mrow><mi>Î³</mi><mo>=</mo><mn>1</mn></mrow><annotation
    encoding="application/x-tex">\gamma=1</annotation></semantics>.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æŠ˜ç°å› å­ (<semantics><mi>Î³</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics>)**:
    ä¸€ä¸ªæ ‡é‡ <semantics><mrow><mn>0</mn><mo>â‰¤</mo><mi>Î³</mi><mo><</mo><mn>1</mn></mrow><annotation
    encoding="application/x-tex">0 \le \gamma < 1</annotation></semantics>ï¼Œå®ƒåœ¨å›æŠ¥ä¸­å¯¹æœªæ¥å¥–åŠ±è¿›è¡ŒæŒ‡æ•°è¡°å‡ï¼Œæƒè¡¡å³æ—¶ä¸é•¿æœŸæ”¶ç›Šï¼Œå¹¶ä¿è¯æ— é™æ—¶åŸŸæ±‚å’Œçš„æ”¶æ•›æ€§ã€‚æœ‰æ—¶ä¸ä½¿ç”¨æŠ˜ç°ï¼Œè¿™ç›¸å½“äº
    <semantics><mrow><mi>Î³</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\gamma=1</annotation></semantics>.'
- en: '**Value Function (<semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation></semantics>)**:
    A function that estimates the expected cumulative reward from a given state: <semantics><mrow><mi>V</mi><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>ğ”¼</mi><mo
    stretchy="false" form="prefix">[</mo><msubsup><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>âˆ</mi></msubsup><msup><mi>Î³</mi><mi>t</mi></msup><msub><mi>r</mi><mi>t</mi></msub><mo>âˆ£</mo><msub><mi>s</mi><mn>0</mn></msub><mo>=</mo><mi>s</mi><mo
    stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">V(s)
    = \mathbb{E}[\sum_{t=0}^{\infty} \gamma^t r_t \mid s_0 = s]</annotation></semantics>.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å€¼å‡½æ•° (<semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation></semantics>)**:
    ä¸€ä¸ªä¼°è®¡ä»ç»™å®šçŠ¶æ€æœŸæœ›ç´¯ç§¯å¥–åŠ±çš„å‡½æ•°ï¼š<semantics><mrow><mi>V</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mi>ğ”¼</mi><mo stretchy="false"
    form="prefix">[</mo><msubsup><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>âˆ</mi></msubsup><msup><mi>Î³</mi><mi>t</mi></msup><msub><mi>r</mi><mi>t</mi></msub><mo>âˆ£</mo><msub><mi>s</mi><mn>0</mn></msub><mo>=</mo><mi>s</mi><mo
    stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">V(s)
    = \mathbb{E}[\sum_{t=0}^{\infty} \gamma^t r_t \mid s_0 = s]</annotation></semantics>.'
- en: '**Q-Function (<semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics>)**:
    A function that estimates the expected cumulative reward from taking a specific
    action in a given state: <semantics><mrow><mi>Q</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mi>ğ”¼</mi><mo stretchy="false"
    form="prefix">[</mo><msubsup><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>âˆ</mi></msubsup><msup><mi>Î³</mi><mi>t</mi></msup><msub><mi>r</mi><mi>t</mi></msub><mo>âˆ£</mo><msub><mi>s</mi><mn>0</mn></msub><mo>=</mo><mi>s</mi><mo>,</mo><msub><mi>a</mi><mn>0</mn></msub><mo>=</mo><mi>a</mi><mo
    stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">Q(s,a)
    = \mathbb{E}[\sum_{t=0}^{\infty} \gamma^t r_t \mid s_0 = s, a_0 = a]</annotation></semantics>.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Q-å‡½æ•° (<semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics>)**:
    ä¸€ä¸ªä¼°è®¡åœ¨ç»™å®šçŠ¶æ€ä¸‹é‡‡å–ç‰¹å®šåŠ¨ä½œçš„æœŸæœ›ç´¯ç§¯å¥–åŠ±çš„å‡½æ•°ï¼š<semantics><mrow><mi>Q</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mi>ğ”¼</mi><mo stretchy="false"
    form="prefix">[</mo><msubsup><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>âˆ</mi></msubsup><msup><mi>Î³</mi><mi>t</mi></msup><msub><mi>r</mi><mi>t</mi></msub><mo>âˆ£</mo><msub><mi>s</mi><mn>0</mn></msub><mo>=</mo><mi>s</mi><mo>,</mo><msub><mi>a</mi><mn>0</mn></msub><mo>=</mo><mi>a</mi><mo
    stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">Q(s,a)
    = \mathbb{E}[\sum_{t=0}^{\infty} \gamma^t r_t \mid s_0 = s, a_0 = a]</annotation></semantics>.'
- en: '**Advantage Function (<semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics>)**:
    The advantage function <semantics><mrow><mi>A</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">A(s,a)</annotation></semantics>
    quantifies the relative benefit of taking action <semantics><mi>a</mi><annotation
    encoding="application/x-tex">a</annotation></semantics> in state <semantics><mi>s</mi><annotation
    encoding="application/x-tex">s</annotation></semantics> compared to the average
    action. Itâ€™s defined as <semantics><mrow><mi>A</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mi>Q</mi><mo stretchy="false"
    form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><mi>V</mi><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">A(s,a) = Q(s,a) - V(s)</annotation></semantics>.
    Advantage functions (and value functions) can depend on a specific policy, <semantics><mrow><msup><mi>A</mi><mi>Ï€</mi></msup><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">A^\pi(s,a)</annotation></semantics>.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä¼˜åŠ¿å‡½æ•° (<semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics>)**:
    ä¼˜åŠ¿å‡½æ•° <semantics><mrow><mi>A</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">A(s,a)</annotation></semantics>
    é‡åŒ–äº†åœ¨çŠ¶æ€ <semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics>
    ä¸‹é‡‡å–è¡ŒåŠ¨ <semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics>
    ç›¸å¯¹äºå¹³å‡è¡ŒåŠ¨çš„ç›¸å¯¹æ”¶ç›Šã€‚å®ƒè¢«å®šä¹‰ä¸º <semantics><mrow><mi>A</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><mi>Q</mi><mo stretchy="false"
    form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><mi>V</mi><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">A(s,a) = Q(s,a) - V(s)</annotation></semantics>.
    ä¼˜åŠ¿å‡½æ•°ï¼ˆä»¥åŠä»·å€¼å‡½æ•°ï¼‰å¯èƒ½ä¾èµ–äºç‰¹å®šçš„ç­–ç•¥ï¼Œ<semantics><mrow><msup><mi>A</mi><mi>Ï€</mi></msup><mo stretchy="false"
    form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">A^\pi(s,a)</annotation></semantics>.'
- en: '**Policy-conditioned Values (<semantics><mrow><mo stretchy="false" form="prefix">[</mo><msup><mo
    stretchy="false" form="postfix">]</mo><mrow><mi>Ï€</mi><mo stretchy="false" form="prefix">(</mo><mi>â‹…</mi><mo
    stretchy="false" form="postfix">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">[]^{\pi(\cdot)}</annotation></semantics>)**:
    Across RL derivations and implementations, a crucial component of the theory and
    practice is collecting data or values conditioned on a specific policy. Throughout
    this book we will switch between the simpler notation of value functions (<semantics><mrow><mi>V</mi><mo>,</mo><mi>A</mi><mo>,</mo><mi>Q</mi><mo>,</mo><mi>G</mi></mrow><annotation
    encoding="application/x-tex">V,A,Q,G</annotation></semantics>) and their specific
    policy-conditioned values (<semantics><mrow><msup><mi>V</mi><mi>Ï€</mi></msup><mo>,</mo><msup><mi>A</mi><mi>Ï€</mi></msup><mo>,</mo><msup><mi>Q</mi><mi>Ï€</mi></msup></mrow><annotation
    encoding="application/x-tex">V^\pi,A^\pi,Q^\pi</annotation></semantics>). Also
    crucial in the expected value computation is sampling from data <semantics><mi>d</mi><annotation
    encoding="application/x-tex">d</annotation></semantics>, that is conditioned on
    a specific policy, <semantics><msub><mi>d</mi><mi>Ï€</mi></msub><annotation encoding="application/x-tex">d_\pi</annotation></semantics>
    (e.g., <semantics><mrow><mi>s</mi><mo>âˆ¼</mo><msub><mi>d</mi><mi>Ï€</mi></msub></mrow><annotation
    encoding="application/x-tex">s \sim d_\pi</annotation></semantics> and <semantics><mrow><mi>a</mi><mo>âˆ¼</mo><mi>Ï€</mi><mo
    stretchy="false" form="prefix">(</mo><mi>â‹…</mi><mo>âˆ£</mo><mi>s</mi><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">a \sim \pi(\cdot\mid
    s)</annotation></semantics> when estimating <semantics><mrow><msub><mi>ğ”¼</mi><mrow><mi>s</mi><mo>âˆ¼</mo><msub><mi>d</mi><mi>Ï€</mi></msub><mo>,</mo><mi>a</mi><mo>âˆ¼</mo><mi>Ï€</mi><mo
    stretchy="false" form="prefix">(</mo><mi>â‹…</mi><mo>âˆ£</mo><mi>s</mi><mo stretchy="false"
    form="postfix">)</mo></mrow></msub><mo stretchy="false" form="prefix">[</mo><msup><mi>A</mi><mi>Ï€</mi></msup><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false"
    form="postfix">)</mo><mo stretchy="false" form="postfix">]</mo></mrow><annotation
    encoding="application/x-tex">\mathbb{E}_{s\sim d_\pi,\,a\sim\pi(\cdot\mid s)}[A^\pi(s,a)]</annotation></semantics>).'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç­–ç•¥æ¡ä»¶å€¼ï¼ˆ<semantics><mrow><mo stretchy="false" form="prefix">[</mo><msup><mo
    stretchy="false" form="postfix">]</mo><mrow><mi>Ï€</mi><mo stretchy="false" form="prefix">(</mo><mi>â‹…</mi><mo
    stretchy="false" form="postfix">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">[]^{\pi(\cdot)}</annotation></semantics>ï¼‰**ï¼šåœ¨å¼ºåŒ–å­¦ä¹ çš„æ¨å¯¼å’Œå®ç°ä¸­ï¼Œç†è®ºå®è·µä¸­çš„ä¸€ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†æ˜¯æ”¶é›†åŸºäºç‰¹å®šç­–ç•¥çš„æ•°æ®æˆ–å€¼ã€‚åœ¨è¿™æœ¬ä¹¦ä¸­ï¼Œæˆ‘ä»¬å°†äº¤æ›¿ä½¿ç”¨å€¼å‡½æ•°çš„ç®€å•ç¬¦å·ï¼ˆ<semantics><mrow><mi>V</mi><mo>,</mo><mi>A</mi><mo>,</mo><mi>Q</mi><mo>,</mo><mi>G</mi></mrow><annotation
    encoding="application/x-tex">V,A,Q,G</annotation></semantics>ï¼‰åŠå…¶ç‰¹å®šçš„ç­–ç•¥æ¡ä»¶å€¼ï¼ˆ<semantics><mrow><msup><mi>V</mi><mi>Ï€</mi></msup><mo>,</mo><msup><mi>A</mi><mi>Ï€</mi></msup><mo>,</mo><msup><mi>Q</mi><mi>Ï€</mi></msup></mrow><annotation
    encoding="application/x-tex">V^\pi,A^\pi,Q^\pi</annotation></semantics>ï¼‰ã€‚åœ¨æœŸæœ›å€¼è®¡ç®—ä¸­ï¼Œä»æ•°æ®
    <semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics>
    ä¸­é‡‡æ ·ï¼Œå³åŸºäºç‰¹å®šç­–ç•¥çš„æ¡ä»¶æ•°æ®ï¼Œ<semantics><msub><mi>d</mi><mi>Ï€</mi></msub><annotation encoding="application/x-tex">d_\pi</annotation></semantics>ï¼ˆä¾‹å¦‚ï¼Œ<semantics><mrow><mi>s</mi><mo>âˆ¼</mo><msub><mi>d</mi><mi>Ï€</mi></msub></mrow><annotation
    encoding="application/x-tex">s \sim d_\pi</annotation></semantics> å’Œ <semantics><mrow><mi>a</mi><mo>âˆ¼</mo><mi>Ï€</mi><mo
    stretchy="false" form="prefix">(</mo><mi>â‹…</mi><mo>âˆ£</mo><mi>s</mi><mo stretchy="false"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">a \sim \pi(\cdot\mid
    s)</annotation></semantics> å½“ä¼°è®¡ <semantics><mrow><msub><mi>ğ”¼</mi><mrow><mi>s</mi><mo>âˆ¼</mo><msub><mi>d</mi><mi>Ï€</mi></msub><mo>,</mo><mi>a</mi><mo>âˆ¼</mo><mi>Ï€</mi><mo
    stretchy="false" form="prefix">(</mo><mi>â‹…</mi><mo>âˆ£</mo><mi>s</mi><mo stretchy="false"
    form="postfix">)</mo></mrow></msub><mo stretchy="false" form="prefix">[</mo><msup><mi>A</mi><mi>Ï€</mi></msup><mo
    stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false"
    form="postfix">)</mo><mo stretchy="false" form="postfix">]</mo></mrow><annotation
    encoding="application/x-tex">\mathbb{E}_{s\sim d_\pi,\,a\sim\pi(\cdot\mid s)}[A^\pi(s,a)]</annotation></semantics>ï¼‰ã€‚'
- en: '**Expectation of Reward Optimization**: The primary goal in RL, which involves
    maximizing the expected cumulative reward:'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¥–åŠ±æœŸæœ›ä¼˜åŒ–**ï¼šåœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­ï¼Œä¸»è¦ç›®æ ‡æ˜¯æœ€å¤§åŒ–æœŸæœ›ç´¯ç§¯å¥–åŠ±ï¼š'
- en: <semantics><mrow><munder><mi mathvariant="normal">max</mi><mi>Î¸</mi></munder><msub><mi>ğ”¼</mi><mrow><mi>s</mi><mo>âˆ¼</mo><msub><mi>Ï</mi><mi>Ï€</mi></msub><mo>,</mo><mi>a</mi><mo>âˆ¼</mo><msub><mi>Ï€</mi><mi>Î¸</mi></msub></mrow></msub><mo
    stretchy="false" form="prefix">[</mo><munderover><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mo
    accent="false">âˆ</mo></munderover><msup><mi>Î³</mi><mi>t</mi></msup><msub><mi>r</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">]</mo><mrow><mo stretchy="false" form="prefix">(</mo><mn>4</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\max_{\theta}
    \mathbb{E}_{s \sim \rho_\pi, a \sim \pi_\theta}[\sum_{t=0}^{\infty} \gamma^t r_t]\qquad{(4)}</annotation></semantics>
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <semantics><mrow><munder><mi mathvariant="normal">max</mi><mi>Î¸</mi></munder><msub><mi>ğ”¼</mi><mrow><mi>s</mi><mo>âˆ¼</mo><msub><mi>Ï</mi><mi>Ï€</mi></msub><mo>,</mo><mi>a</mi><mo>âˆ¼</mo><msub><mi>Ï€</mi><mi>Î¸</mi></msub></mrow></msub><mo
    stretchy="false" form="prefix">[</mo><munderover><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mo
    accent="false">âˆ</mo></munderover><msup><mi>Î³</mi><mi>t</mi></msup><msub><mi>r</mi><mi>t</mi></msub><mo
    stretchy="false" form="postfix">]</mo><mrow><mo stretchy="false" form="prefix">(</mo><mn>4</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\max_{\theta}
    \mathbb{E}_{s \sim \rho_\pi, a \sim \pi_\theta}[\sum_{t=0}^{\infty} \gamma^t r_t]\qquad{(4)}</annotation></semantics>
- en: where <semantics><msub><mi>Ï</mi><mi>Ï€</mi></msub><annotation encoding="application/x-tex">\rho_\pi</annotation></semantics>
    is the state distribution under policy <semantics><mi>Ï€</mi><annotation encoding="application/x-tex">\pi</annotation></semantics>,
    and <semantics><mi>Î³</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics>
    is the discount factor.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼Œ<semantics><msub><mi>Ï</mi><mi>Ï€</mi></msub><annotation encoding="application/x-tex">\rho_\pi</annotation></semantics>
    æ˜¯ç­–ç•¥ <semantics><mi>Ï€</mi><annotation encoding="application/x-tex">\pi</annotation></semantics>
    ä¸‹çš„çŠ¶æ€åˆ†å¸ƒï¼Œè€Œ <semantics><mi>Î³</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics>
    æ˜¯æŠ˜æ‰£å› å­ã€‚
- en: '**Finite Horizon Reward (<semantics><mrow><mi>J</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">J(\pi_\theta)</annotation></semantics>)**:
    The expected finite-horizon discounted return of the policy <semantics><msub><mi>Ï€</mi><mi>Î¸</mi></msub><annotation
    encoding="application/x-tex">\pi_\theta</annotation></semantics>, parameterized
    by <semantics><mi>Î¸</mi><annotation encoding="application/x-tex">\theta</annotation></semantics>
    is defined as:'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æœ‰é™æ—¶é—´èŒƒå›´å¥–åŠ± (<semantics><mrow><mi>J</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">J(\pi_\theta)</annotation></semantics>)**:
    ç”±å‚æ•° <semantics><mi>Î¸</mi><annotation encoding="application/x-tex">\theta</annotation></semantics>
    ç¡®å®šçš„ç­–ç•¥ <semantics><msub><mi>Ï€</mi><mi>Î¸</mi></msub><annotation encoding="application/x-tex">\pi_\theta</annotation></semantics>
    çš„æœŸæœ›æœ‰é™æ—¶é—´èŒƒå›´æŠ˜æ‰£å›æŠ¥å®šä¹‰ä¸ºï¼š'
- en: <semantics><mrow><mi>J</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ğ”¼</mi><mrow><mi>Ï„</mi><mo>âˆ¼</mo><msub><mi>Ï€</mi><mi>Î¸</mi></msub></mrow></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><munderover><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>T</mi></munderover><msup><mi>Î³</mi><mi>t</mi></msup><msub><mi>r</mi><mi>t</mi></msub><mo
    stretchy="true" form="postfix">]</mo></mrow><mrow><mo stretchy="false" form="prefix">(</mo><mn>5</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">J(\pi_\theta)
    = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \gamma^t r_t \right]\qquad{(5)}</annotation></semantics>
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <semantics><mrow><mi>J</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>Ï€</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>ğ”¼</mi><mrow><mi>Ï„</mi><mo>âˆ¼</mo><msub><mi>Ï€</mi><mi>Î¸</mi></msub></mrow></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><munderover><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>T</mi></munderover><msup><mi>Î³</mi><mi>t</mi></msup><msub><mi>r</mi><mi>t</mi></msub><mo
    stretchy="true" form="postfix">]</mo></mrow><mrow><mo stretchy="false" form="prefix">(</mo><mn>5</mn><mo
    stretchy="false" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">J(\pi_\theta)
    = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \gamma^t r_t \right]\qquad{(5)}</annotation></semantics>
- en: where <semantics><mrow><mi>Ï„</mi><mo>âˆ¼</mo><msub><mi>Ï€</mi><mi>Î¸</mi></msub></mrow><annotation
    encoding="application/x-tex">\tau \sim \pi_\theta</annotation></semantics> denotes
    trajectories sampled by following policy <semantics><msub><mi>Ï€</mi><mi>Î¸</mi></msub><annotation
    encoding="application/x-tex">\pi_\theta</annotation></semantics> and <semantics><mi>T</mi><annotation
    encoding="application/x-tex">T</annotation></semantics> is the finite horizon.
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼Œ<semantics><mrow><mi>Ï„</mi><mo>âˆ¼</mo><msub><mi>Ï€</mi><mi>Î¸</mi></msub></mrow><annotation
    encoding="application/x-tex">\tau \sim \pi_\theta</annotation></semantics> è¡¨ç¤ºé€šè¿‡éµå¾ªç­–ç•¥
    <semantics><msub><mi>Ï€</mi><mi>Î¸</mi></msub><annotation encoding="application/x-tex">\pi_\theta</annotation></semantics>
    é‡‡æ ·çš„è½¨è¿¹ï¼Œè€Œ <semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics>
    æ˜¯æœ‰é™çš„æ—¶é—´èŒƒå›´ã€‚
- en: '**On-policy**: In RLHF, particularly in the debate between RL and Direct Alignment
    Algorithms, the discussion of **on-policy** data is common. In the RL literature,
    on-policy means that the data is generated *exactly* by the current form of the
    agent, but in the general preference-tuning literature, on-policy is expanded
    to mean generations from that edition of model â€“ e.g.Â a instruction-tuned checkpoint
    before running any preference fine-tuning. In this context, off-policy could be
    data generated by any other language model being used in post-training.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æŒ‰ç­–ç•¥**ï¼šåœ¨ RLHF ä¸­ï¼Œå°¤å…¶æ˜¯åœ¨ RL å’Œç›´æ¥å¯¹é½ç®—æ³•ä¹‹é—´çš„è¾©è®ºä¸­ï¼Œå…³äº **æŒ‰ç­–ç•¥** æ•°æ®çš„è®¨è®ºå¾ˆå¸¸è§ã€‚åœ¨ RL æ–‡çŒ®ä¸­ï¼ŒæŒ‰ç­–ç•¥æ„å‘³ç€æ•°æ®æ˜¯ç”±å½“å‰å½¢å¼çš„ä»£ç†â€œå®Œå…¨â€ç”Ÿæˆçš„ï¼Œä½†åœ¨ä¸€èˆ¬åå¥½è°ƒæ•´æ–‡çŒ®ä¸­ï¼ŒæŒ‰ç­–ç•¥è¢«æ‰©å±•ä¸ºæŒ‡ä»è¯¥ç‰ˆæ¨¡å‹ç”Ÿæˆçš„æ•°æ®â€”â€”ä¾‹å¦‚ï¼Œåœ¨è¿è¡Œä»»ä½•åå¥½å¾®è°ƒä¹‹å‰çš„ä¸€ä¸ªæŒ‡ä»¤è°ƒæ•´æ£€æŸ¥ç‚¹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç¦»ç­–ç•¥å¯èƒ½æ˜¯ä»»ä½•å…¶ä»–åœ¨è®­ç»ƒåä½¿ç”¨çš„è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„æ•°æ®ã€‚'
- en: RLHF Only Definitions
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RLHF ä»…å®šä¹‰
- en: '**Reference Model (<semantics><msub><mi>Ï€</mi><mtext mathvariant="normal">ref</mtext></msub><annotation
    encoding="application/x-tex">\pi_{\text{ref}}</annotation></semantics>)**: This
    is a saved set of parameters used in RLHF where outputs of it are used to regularize
    the optimization.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å‚è€ƒæ¨¡å‹ (<semantics><msub><mi>Ï€</mi><mtext mathvariant="normal">ref</mtext></msub><annotation
    encoding="application/x-tex">\pi_{\text{ref}}</annotation></semantics>)**ï¼šè¿™æ˜¯åœ¨
    RLHF ä¸­ä½¿ç”¨çš„ä¿å­˜å‚æ•°é›†ï¼Œå…¶è¾“å‡ºè¢«ç”¨äºæ­£åˆ™åŒ–ä¼˜åŒ–ã€‚'
- en: Extended Glossary
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ‰©å±•è¯æ±‡è¡¨
- en: '**Synthetic Data**: This is any training data for an AI model that is the output
    from another AI system. This could be anything from text generated from an open-ended
    prompt of a model to a model rewriting existing content.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**åˆæˆæ•°æ®**ï¼šè¿™æ˜¯ä¸º AI æ¨¡å‹æä¾›çš„ä»»ä½•è®­ç»ƒæ•°æ®ï¼Œå®ƒæ˜¯å¦ä¸€ä¸ª AI ç³»ç»Ÿçš„è¾“å‡ºã€‚è¿™å¯èƒ½åŒ…æ‹¬ä»æ¨¡å‹çš„å¼€æ”¾å¼æç¤ºç”Ÿæˆçš„æ–‡æœ¬ï¼Œåˆ°æ¨¡å‹é‡å†™ç°æœ‰å†…å®¹çš„ä¸€åˆ‡ã€‚'
- en: '**Distillation**: Distillation is a general set of practices in training AI
    models where a model is trained on the outputs of a stronger model. This is a
    type of synthetic data known to make strong, smaller models. Most models make
    the rules around distillation clear through either the license, for open weight
    models, or the terms of service, for models accessible only via API. The term
    distillation is now overloaded with a specific technical definition from the ML
    literature.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è’¸é¦**ï¼šè’¸é¦æ˜¯è®­ç»ƒ AI æ¨¡å‹çš„ä¸€ç»„é€šç”¨å®è·µï¼Œå…¶ä¸­æ¨¡å‹åœ¨æ›´å¼ºæ¨¡å‹çš„è¾“å‡ºä¸Šè¿›è¡Œè®­ç»ƒã€‚è¿™æ˜¯ä¸€ç§å·²çŸ¥å¯ä»¥åˆ¶ä½œå¼ºå¤§ã€æ›´å°æ¨¡å‹çš„åˆæˆæ•°æ®ã€‚å¤§å¤šæ•°æ¨¡å‹é€šè¿‡è®¸å¯è¯ï¼ˆé’ˆå¯¹å…¬å¼€æƒé‡æ¨¡å‹ï¼‰æˆ–æœåŠ¡æ¡æ¬¾ï¼ˆé’ˆå¯¹ä»…å¯é€šè¿‡
    API è®¿é—®çš„æ¨¡å‹ï¼‰æ˜ç¡®å…¶è’¸é¦è§„åˆ™ã€‚ç°åœ¨ï¼Œè’¸é¦ä¸€è¯å·²ç»è¶…è½½ï¼Œå…·æœ‰æ¥è‡ªæœºå™¨å­¦ä¹ æ–‡çŒ®çš„ç‰¹å®šæŠ€æœ¯å®šä¹‰ã€‚'
- en: '**(Teacher-student) Knowledge Distillation**: Knowledge distillation from a
    specific teacher to a student model is a specific type of distillation above and
    where the term originated. It is a specific deep learning method where a neural
    network loss is modified to learn from the log-probabilities of the teacher model
    over multiple potential tokens/logits, instead of learning directly from a chosen
    output [[51]](ch021.xhtml#ref-hinton2015distilling). An example of a modern series
    of models trained with Knowledge Distillation is Gemma 2 [[52]](ch021.xhtml#ref-team2024gemma)
    or Gemma 3\. For a language modeling setup, the next-token loss function can be
    modified as follows [[53]](ch021.xhtml#ref-agarwal2024policy), where the student
    model <semantics><msub><mi>P</mi><mi>Î¸</mi></msub><annotation encoding="application/x-tex">P_\theta</annotation></semantics>
    learns from the teacher distribution <semantics><msub><mi>P</mi><mi>Ï•</mi></msub><annotation
    encoding="application/x-tex">P_\phi</annotation></semantics>:'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ï¼ˆæ•™å¸ˆ-å­¦ç”Ÿï¼‰çŸ¥è¯†è’¸é¦**ï¼šä»ç‰¹å®šæ•™å¸ˆåˆ°å­¦ç”Ÿæ¨¡å‹çš„çŸ¥è¯†çš„è’¸é¦æ˜¯ä¸Šè¿°è’¸é¦çš„ä¸€ç§ç‰¹å®šç±»å‹ï¼Œä¹Ÿæ˜¯è¯¥æœ¯è¯­çš„èµ·æºã€‚è¿™æ˜¯ä¸€ç§ç‰¹å®šçš„æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œå…¶ä¸­ç¥ç»ç½‘ç»œæŸå¤±è¢«ä¿®æ”¹ä¸ºä»æ•™å¸ˆæ¨¡å‹åœ¨å¤šä¸ªæ½œåœ¨æ ‡è®°/å¯¹æ•°ä¼¼ç„¶ä¸Šçš„å¯¹æ•°æ¦‚ç‡ä¸­å­¦ä¹ ï¼Œè€Œä¸æ˜¯ç›´æ¥ä»é€‰å®šçš„è¾“å‡ºä¸­å­¦ä¹ 
    [[51]](ch021.xhtml#ref-hinton2015distilling)ã€‚ä½¿ç”¨çŸ¥è¯†è’¸é¦è®­ç»ƒçš„ç°ä»£æ¨¡å‹ç³»åˆ—çš„ä¸€ä¸ªä¾‹å­æ˜¯ Gemma 2 [[52]](ch021.xhtml#ref-team2024gemma)
    æˆ– Gemma 3ã€‚å¯¹äºè¯­è¨€æ¨¡å‹è®¾ç½®ï¼Œä¸‹ä¸€ä¸ªæ ‡è®°çš„æŸå¤±å‡½æ•°å¯ä»¥ä¿®æ”¹å¦‚ä¸‹ [[53]](ch021.xhtml#ref-agarwal2024policy)ï¼Œå…¶ä¸­å­¦ç”Ÿæ¨¡å‹
    <semantics><msub><mi>P</mi><mi>Î¸</mi></msub><annotation encoding="application/x-tex">P_\theta</annotation></semantics>
    ä»æ•™å¸ˆåˆ†å¸ƒ <semantics><msub><mi>P</mi><mi>Ï•</mi></msub><annotation encoding="application/x-tex">P_\phi</annotation></semantics>
    ä¸­å­¦ä¹ ï¼š'
- en: <semantics><mrow><msub><mi>â„’</mi><mtext mathvariant="normal">KD</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>âˆ’</mi><msub><mi>ğ”¼</mi><mrow><mi>x</mi><mo>âˆ¼</mo><mi>ğ’Ÿ</mi></mrow></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><munderover><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><msub><mi>P</mi><mi>Ï•</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>âˆ£</mo><msub><mi>x</mi><mrow><mo><</mo><mi>t</mi></mrow></msub><mo
    stretchy="false" form="postfix">)</mo><mi mathvariant="normal">log</mi><msub><mi>P</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>âˆ£</mo><msub><mi>x</mi><mrow><mo><</mo><mi>t</mi></mrow></msub><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow><mi>.</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>6</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">\mathcal{L}_{\text{KD}}(\theta) = -\,\mathbb{E}_{x
    \sim \mathcal{D}}\left[\sum_{t=1}^{T} P_{\phi}(x_t \mid x_{<t}) \log P_{\theta}(x_t
    \mid x_{<t})\right]. \qquad{(6)}</annotation></semantics>
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><msub><mi>â„’</mi><mtext mathvariant="normal">KD</mtext></msub><mo
    stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>âˆ’</mi><msub><mi>ğ”¼</mi><mrow><mi>x</mi><mo>âˆ¼</mo><mi>ğ’Ÿ</mi></mrow></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><munderover><mo>âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><msub><mi>P</mi><mi>Ï•</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>âˆ£</mo><msub><mi>x</mi><mrow><mo><</mo><mi>t</mi></mrow></msub><mo
    stretchy="false" form="postfix">)</mo><mi mathvariant="normal">log</mi><msub><mi>P</mi><mi>Î¸</mi></msub><mo
    stretchy="false" form="prefix">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>âˆ£</mo><msub><mi>x</mi><mrow><mo><</mo><mi>t</mi></mrow></msub><mo
    stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow><mi>.</mi><mrow><mo
    stretchy="false" form="prefix">(</mo><mn>6</mn><mo stretchy="false" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">\mathcal{L}_{\text{KD}}(\theta) = -\,\mathbb{E}_{x
    \sim \mathcal{D}}\left[\sum_{t=1}^{T} P_{\phi}(x_t \mid x_{<t}) \log P_{\theta}(x_t
    \mid x_{<t})\right]. \qquad{(6)}</annotation></semantics>
- en: '**In-context Learning (ICL)**: In-context here refers to any information within
    the context window of the language model. Usually, this is information added to
    the prompt. The simplest form of in-context learning is adding examples of a similar
    form before the prompt. Advanced versions can learn which information to include
    for a specific use-case.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰**ï¼šè¿™é‡Œçš„â€œä¸Šä¸‹æ–‡â€æŒ‡çš„æ˜¯è¯­è¨€æ¨¡å‹ä¸Šä¸‹æ–‡çª—å£å†…çš„ä»»ä½•ä¿¡æ¯ã€‚é€šå¸¸ï¼Œè¿™æ˜¯æ·»åŠ åˆ°æç¤ºä¸­çš„ä¿¡æ¯ã€‚ä¸Šä¸‹æ–‡å­¦ä¹ çš„æœ€ç®€å•å½¢å¼æ˜¯åœ¨æç¤ºä¹‹å‰æ·»åŠ ç±»ä¼¼å½¢å¼çš„ç¤ºä¾‹ã€‚é«˜çº§ç‰ˆæœ¬å¯ä»¥å­¦ä¹ ä¸ºç‰¹å®šç”¨ä¾‹åŒ…å«å“ªäº›ä¿¡æ¯ã€‚'
- en: '**Chain of Thought (CoT)**: Chain of thought is a specific behavior of language
    models where they are steered towards a behavior that breaks down a problem in
    a step-by-step form. The original version of this was through the prompt â€œLetâ€™s
    think step-by-stepâ€ [[54]](ch021.xhtml#ref-wei2022chain).'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ€ç»´é“¾ï¼ˆCoTï¼‰**ï¼šæ€ç»´é“¾æ˜¯è¯­è¨€æ¨¡å‹çš„ä¸€ç§ç‰¹å®šè¡Œä¸ºï¼Œå®ƒä»¬è¢«å¼•å¯¼ä»¥é€æ­¥åˆ†è§£é—®é¢˜çš„å½¢å¼ã€‚è¿™ç§è¡Œä¸ºçš„åŸå§‹ç‰ˆæœ¬æ˜¯é€šè¿‡æç¤ºâ€œè®©æˆ‘ä»¬ä¸€æ­¥æ­¥æ€è€ƒâ€ [[54]](ch021.xhtml#ref-wei2022chain)ã€‚'
