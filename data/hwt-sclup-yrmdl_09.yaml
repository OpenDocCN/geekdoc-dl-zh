- en: Serving LLaMA 3-70B on TPUs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 TPUs 上部署 LLaMA 3-70B
- en: 原文：[https://jax-ml.github.io/scaling-book/applied-inference](https://jax-ml.github.io/scaling-book/applied-inference)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://jax-ml.github.io/scaling-book/applied-inference](https://jax-ml.github.io/scaling-book/applied-inference)
- en: '<d-title>Part 8 of [How To Scale Your Model](/scaling-book) ([Part 7: Inference](../inference)
    | [Part 9: Profiling](../profiling))'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: <d-title>《如何扩展你的模型》第 8 部分 [How To Scale Your Model](/scaling-book) ([第 7 部分：推理](../inference)
    | [第 9 部分：分析](../profiling))
- en: Let's take a close look at how we'd serve LLaMA 3-70B models on TPU v5e. How
    expensive are different models to serve at roofline? How large are their KV caches?
    What batch sizes should we use? How are the parameters and activations sharded
    during inference? Let's work through some back-of-the-envelope estimates for latency
    and throughput in production.</d-title>  <d-byline><d-article><d-contents>###
    Contents
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们仔细看看如何在 TPU v5e 上部署 LLaMA 3-70B 模型。部署不同模型在屋顶线上的成本是多少？它们的 KV 缓存有多大？我们应该使用多大的批量大小？推理过程中参数和激活是如何分片的？让我们来估算一下生产环境中的延迟和吞吐量。</d-title>  <d-byline><d-article><d-contents>###
    内容
- en: '[What''s the LLaMA Serving Story?](#what-s-the-llama-serving-story)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[LLaMA 部署的故事是什么？](#what-s-the-llama-serving-story)'
- en: '[Thinking about throughput](#thinking-about-throughput)'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[思考吞吐量](#thinking-about-throughput)'
- en: '[What about prefill?](#what-about-prefill)'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[关于预填充的思考](#what-about-prefill)'
- en: '[Visualizing the Latency Throughput Tradeoff](#visualizing-the-latency-throughput-tradeoff)[Worked
    Problems](#worked-problems)</d-contents>'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[可视化延迟吞吐量权衡](#visualizing-the-latency-throughput-tradeoff)[练习题](#worked-problems)</d-contents>'
- en: '*This section will look at what it takes to serve LLaMA-3 and how efficiently
    it can be done. As in the previous “applied” section, try to work out the answers
    on your own with a pen and paper before looking them up!*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*本节将探讨如何部署 LLaMA-3 以及如何高效地完成部署。与之前的“应用”部分一样，在查阅答案之前，请尝试用笔和纸自己解决问题！*'
- en: What’s the LLaMA Serving Story?
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLaMA 部署的故事
- en: 'Let’s remind ourselves what LLaMA 3-70B looks like (see [Section 6](../applied-training)
    for reference):'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下 LLaMA 3-70B 的样子（参见 [第 6 部分](../applied-training) 以获取参考）：
- en: '| **hyperparam** | **value** |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| **超参数** | **值** |'
- en: '| --- | --- |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| \(n_\text{layers}\) (L) | 80 |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| \(n_\text{layers}\) (L) | 80 |'
- en: '| \(d_\text{model}\) (D) | 8,192 |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| \(d_\text{model}\) (D) | 8,192 |'
- en: '| \(d_{ff}\) (F) | 28,672 |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| \(d_{ff}\) (F) | 28,672 |'
- en: '| \(n_\text{heads}\) (N) | 64 |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| \(n_\text{heads}\) (N) | 64 |'
- en: '| \(n_\text{kv heads}\) (K) | 8 |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| \(n_\text{kv heads}\) (K) | 8 |'
- en: '| \(d_\text{qkv}\) (H) | 128 |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| \(d_\text{qkv}\) (H) | 128 |'
- en: '| \(n_\text{embeddings}\) (V) | 128,256 |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| \(n_\text{embeddings}\) (V) | 128,256 |'
- en: 'Let’s start with a simple question: **what hardware should we serve on?** The
    answer is basically, whichever is cheapest in FLOPs / dollar.<d-footnote>This
    isn''t always true, sometimes more HBM or ICI bandwidth is critical rather than
    FLOPs, but this is a good heuristic.</d-footnote> For this reason, we typically
    want to serve on TPU v5e, our current dedicated inference chip (cost comes from
    [Google Cloud pricing](https://cloud.google.com/tpu/pricing) as of February 2025):'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个问题开始：**我们应该在什么硬件上部署？** 答案基本上是，在 FLOPs/美元上最便宜的那个。 <d-footnote>这并不总是正确的，有时更多的
    HBM 或 ICI 带宽比 FLOPs 更关键，但这是一个好的经验法则。</d-footnote> 因此，我们通常希望部署在 TPU v5e 上，我们当前的专用推理芯片（截至
    2025 年 2 月的价格来自 [Google Cloud 定价](https://cloud.google.com/tpu/pricing)）：
- en: '| **TPU type** | **bfloat16 FLOPs/s** | **Google Cloud USD / hour** | **FLOPs
    / $** |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| **TPU 类型** | **bfloat16 FLOPs/s** | **Google Cloud 美元/小时** | **FLOPs/$**
    |'
- en: '| --- | --- | --- | --- |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| H100 | 9.9e14 | $10.8 | 3.3e17 |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| H100 | 9.9e14 | $10.8 | 3.3e17 |'
- en: '| v5p | 4.59e14 | $4.2 | 3.9e17 |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| v5p | 4.59e14 | $4.2 | 3.9e17 |'
- en: '| v5e | 1.97e14 | $1.2 | **5.8e17** |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| v5e | 1.97e14 | $1.2 | **5.8e17** |'
- en: 'Each TPU v5e has 16GB of HBM which will require us to shard our model fairly
    aggressively. Let’s start by thinking about some basic quantities that might matter
    for us:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 TPU v5e 都有 16GB 的 HBM，这将要求我们非常积极地分片我们的模型。让我们先思考一些可能对我们很重要的基本量：
- en: '**Question:** How large are LLaMA 3-70B’s KV caches per token? *You can assume
    we store them in int8\. This determines how large our batch size can be on a given
    topology.*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题：LLaMA 3-70B 每个标记的 KV 缓存有多大？*你可以假设我们以 int8 格式存储它们。这决定了在给定拓扑结构上我们的批量大小可以有多大。*'
- en: <details><summary>Click here once you’ve thought it through!</summary>
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>在你思考过后点击这里！</summary>
- en: LLaMA 3-70B has 8 KV heads, so the size per token is `2 * K * H * L = 2 * 8
    * 128 * 80 = 160kB`.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA 3-70B 有 8 个 KV 头，因此每个标记的大小为 `2 * K * H * L = 2 * 8 * 128 * 80 = 160kB`。
- en: '**Note just how big this is!** If we have a sequence length of 32k tokens (as
    is common), this uses `162e3 * 32,768 = 5.3GB / sequence`. For BS=240, this is
    1.3TB! Since TPU v5e only have 16GB a piece, we would need about `(70e9 + 1.3e12)
    / 16e9 = 86` TPU v5e chips to even fit this much memory. Also note how large this
    is compared to the 70GB of model parameters.</details>'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意这有多大！** 如果我们有一个32k个token的序列长度（这是常见的），这将使用 `162e3 * 32,768 = 5.3GB / sequence`。对于BS=240，这将达到1.3TB！由于TPU
    v5e每个只有16GB，我们需要大约 `(70e9 + 1.3e12) / 16e9 = 86` 个TPU v5e芯片才能容纳这么多内存。也请注意，这与70GB的模型参数相比有多大。</details>'
- en: '**Question:** Let’s say we want to serve L3 70B at batch size 32 and 8192 sequence
    length with everything (params and KVs) in int8\. How much total memory will this
    use? What’s the smallest slice we could serve this on?'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题：** 假设我们希望在批处理大小为32和8192序列长度的情况下服务L3 70B，并且所有内容（参数和KV）都在int8中。这将使用多少总内存？我们能在最小的切片上服务这个吗？'
- en: <details><summary>Answer</summary>
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>答案</summary>
- en: Since our KVs are `160e3` bytes in int8, our total KV memory is `160e3 * 8192
    * 32 = 41.9e9` bytes. Our parameters are `70e9` bytes, since we have 1 byte per
    parameter. Thus, our total memory usage is `41.9e9 + 70e9 = 112GB`.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的KV在int8中是 `160e3` 字节，我们的总KV内存是 `160e3 * 8192 * 32 = 41.9e9` 字节。我们的参数是 `70e9`
    字节，因为我们每个参数有1个字节。因此，我们的总内存使用量是 `41.9e9 + 70e9 = 112GB`。
- en: The smallest slice we could use would have `112e9 / 16e9 = 7` TPUs, or (rounding
    to an even size), TPU v5e `4x2`. This will be a tight fit and we might not be
    able to quite fit this accounting for other overhead, so we might need a `4x4`
    at minimum (or to drop the batch size).</details>
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能使用的最小切片将包含 `112e9 / 16e9 = 7` 个TPU，或者（四舍五入到偶数大小），TPU v5e `4x2`。这将非常紧凑，我们可能无法完全考虑到其他开销，因此我们可能至少需要
    `4x4`（或者减少批处理大小）。</details>
- en: '**Question:** At this batch size and quantization on a TPU v5e `4x2`, roughly
    what latency would we expect per decode step? What throughput (tokens / sec /
    chip). What about a `4x4`? *Assume we perform our FLOPs in bfloat16 and everything
    is fully sharded.*'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题：** 在这个批处理大小和量化在TPU v5e `4x2`上，我们大致可以期望每个解码步骤的多少延迟？吞吐量（tokens / sec / chip）是多少？对于
    `4x4` 呢？**假设我们在bfloat16中执行FLOPs，并且所有内容都是完全分片的。**'
- en: <details><summary>Answer</summary>
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>答案</summary>
- en: We can invoke the formula from the previous section that
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以调用前一个部分中的公式
- en: \[\begin{align*} \tiny \text{Theoretical Step Time (General)} = \underbrace{\frac{\text{Batch
    Size} \times \text{KV Cache Size}}{\tiny \text{Total Memory Bandwidth}}}_{\text{Attention
    (always bandwidth-bound)}} + \underbrace{\max\left(\frac{2 \times \text{Batch
    Size} \times \text{Parameter Count}}{\text{Total FLOPs/s}}, \frac{\text{Parameter
    Size}}{\text{Total Memory Bandwidth}}\right)}_{\tiny \text{MLP (can be compute-bound)}}
    \end{align*}\]
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \tiny \text{理论步骤时间（通用）} = \underbrace{\frac{\text{批处理大小} \times
    \text{KV缓存大小}}{\tiny \text{总内存带宽}}}_{\text{注意力（总是带宽限制的）}} + \underbrace{\max\left(\frac{2
    \times \text{批处理大小} \times \text{参数数量}}{\text{总FLOPs/s}}, \frac{\text{参数大小}}{\text{总内存带宽}}\right)}_{\tiny
    \text{MLP（可能是计算限制的）}} \end{align*}\]
- en: Here our critical batch size will be about 120 since our parameters are in int8
    but our FLOPs are in bfloat16\. We could also manually calculate the RHS maximum,
    but that’s basically a calculation we’ve already done several times. **So we’re
    well into the memory-bound regime for both our matmul and our FLOPs.**
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们的关键批处理大小将是大约120，因为我们的参数在int8中，但我们的FLOPs在bfloat16中。我们也可以手动计算RHS的最大值，但这基本上是我们已经做过多次的计算。**因此，我们的矩阵乘法和FLOPs都已经进入了内存限制阶段。**
- en: Strictly looking at memory bandwidth then, our step time is basically `(KV size
    + param size) / (8 * HBM bandwidth) = 112e9 / (8 * 8.1e11) = 17ms`. **So theoretically
    our step time is about 17ms.** Our throughput would be `32 / .017 = 1882 tokens
    / sec`, or `1882 / 8 = 235 tokens / sec / chip`.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 严格地看内存带宽，我们的步骤时间是 `(KV大小 + 参数大小) / (8 * HBM带宽) = 112e9 / (8 * 8.1e11) = 17ms`。**因此，从理论上讲，我们的步骤时间大约是17ms。**我们的吞吐量将是
    `32 / .017 = 1882 tokens / sec`，或者 `1882 / 8 = 235 tokens / sec / chip`。
- en: There’s one caveat here which is to check if we might be ICI bound on our matmuls.
    We could dedicate 2 axes to it here, so we’re ICI bound in theory when $Y > 2
    * F / 2200 = 2 * 28672 / 2200 = 26$, so we’re golden!
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个需要注意的地方，就是检查我们是否可能在矩阵乘法上受到ICI的限制。我们可以在这里为其分配2个轴，因此当 $Y > 2 * F / 2200 =
    2 * 28672 / 2200 = 26$ 时，我们在理论上受到ICI的限制，所以没问题！
- en: If we were to run on a `4x4`, we’d still be fine ICI-wise, so our latency would
    drop to `17 / 2 = 8.5ms`, but our throughput per-chip would remain the same.</details>
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在 `4x4` 上运行，我们仍然在ICI方面没有问题，所以我们的延迟将降低到 `17 / 2 = 8.5ms`，但每片的吞吐量将保持不变。</details>
- en: Thinking about throughput
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 考虑吞吐量
- en: Let’s spend a little time thinking purely about throughput. When we optimize
    for throughput, we want to be compute bound, meaning we come close to utilizing
    all the TPU MXU capacity. Typically that means we want the batch size to be as
    large as possible, so we are doing as much work as possible.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花点时间纯粹地思考吞吐量。当我们优化吞吐量时，我们希望达到计算限制，这意味着我们接近利用所有TPU MXU容量。通常这意味着我们希望批量大小尽可能大，这样我们就能做尽可能多的工作。
- en: '**Question:** On TPU v5e, using bfloat16 weights and activations, how large
    do our batch sizes need to be for us to be compute-bound in our matmuls? What
    if we do int8 weights but perform our FLOPs in bfloat16? What about int8 weights
    with int8 FLOPs?'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题：** 在TPU v5e上，使用bfloat16权重和激活，我们的批量大小需要多大才能在matmuls中达到计算限制？如果我们使用int8权重但在bfloat16中执行FLOPs会怎样？关于int8权重和int8
    FLOPs又会怎样？'
- en: <details><summary>Answer</summary>
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>回答</summary>
- en: As discussed in Section 7, for any bfloat16 matmul for which $B \ll D, F$ we
    have
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如第7节所述，对于任何 $B \ll D, F$ 的bfloat16 matmul，我们有
- en: \[\begin{equation*} T_\text{math} > T_\text{comms} \leftrightarrow \frac{2BDF}{2DF}
    \geq \frac{\text{TPU bfloat16 FLOPs/s}}{\text{HBM bandwidth}} = 240 \end{equation*}\]
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation*} T_\text{math} > T_\text{comms} \leftrightarrow \frac{2BDF}{2DF}
    \geq \frac{\text{TPU bfloat16 FLOPs/s}}{\text{HBM bandwidth}} = 240 \end{equation*}\]
- en: When our weights are in int8, we lose a factor of 2 in the denominator, so we
    have $2BDF / DF = 2B > 240$, or equally $B > 120$, half the critical batch size
    from before. That’s really helpful for us! When we do int8 weights and int8 FLOPs,
    we have to use the int8 value for TPU FLOPs/s, which goes from 1.97e14 for bfloat16
    to 3.94e14, nearly double. That means we’re back where we started at about $B
    > 240$.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的权重在int8时，分母中会损失一个因子，所以我们有 $2BDF / DF = 2B > 240$，或者同样 $B > 120$，是之前临界批量大小的一半。这对我们来说非常有帮助！当我们使用int8权重和int8
    FLOPs时，我们必须使用TPU FLOPs/s的int8值，这从bfloat16的1.97e14增加到3.94e14，几乎翻倍。这意味着我们回到了大约 $B
    > 240$ 的起点。
- en: The case of int8 weights and bfloat16 FLOPs is quite common, since quantizing
    parameters losslessly is often easier than doing low-precision arithmetic.</details>
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: int8权重和bfloat16 FLOPs的情况相当常见，因为无损量化参数通常比进行低精度算术更容易。
- en: '**Question:** What is the smallest TPU v5e topology we could serve LLaMA 3-70B
    on using bfloat16, int8, and int4 (both KVs and parameters) with 8k context? *You
    can think of KV caches as negligibly small for this one.*'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题：** 使用bfloat16、int8和int4（包括KVs和参数）以及8k上下文，我们能在TPU v5e上为LLaMA 3-70B提供最小的拓扑吗？*你可以将KV缓存视为在这个例子中可以忽略不计的小。*'
- en: <details><summary>Answer</summary>
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>回答</summary>
- en: 'This is easy! If we’re OK with a tiny batch size then the only limit is fitting
    parameter memory in HBM, i.e. it is just `ceil(num_params * sizeof(dtype) / HBM
    per TPU`, or `ceil(70e9 * sizeof(dtype) / 16e9)` rounded to the nearest reasonable
    topology (some multiple of 2):'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这很简单！如果我们对很小的批量大小没有问题，那么唯一的限制就是将参数内存拟合到HBM中，即它只是 `ceil(num_params * sizeof(dtype)
    / HBM per TPU)`，或者 `ceil(70e9 * sizeof(dtype) / 16e9)` 四舍五入到最接近的合理拓扑（2的某个倍数）：
- en: '| dtype | param size | KV size / token (bytes) | min TPU v5es | actual min
    slice | remaining HBM for KV caches | num KV caches @ 8k |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| dtype | param size | KV size / token (bytes) | min TPU v5es | actual min
    slice | remaining HBM for KV caches | num KV caches @ 8k |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| bf16 | 140GB | 324kB | 8.75 | 4x4 = 16 chips | 116 | 43 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| bf16 | 140GB | 324kB | 8.75 | 4x4 = 16 chips | 116 | 43 |'
- en: '| int8 | 70GB | 162kB | 4.38 | 4x2 = 8 chips | 58 | 43 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| int8 | 70GB | 162kB | 4.38 | 4x2 = 8 chips | 58 | 43 |'
- en: '| int4 | 35GB | 81kB | 2.81 | 2x2 = 4 chips | 29 | 43 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| int4 | 35GB | 81kB | 2.81 | 2x2 = 4 chips | 29 | 43 |'
- en: That’s pretty cool! It tells us we could fit LLaMA 70B on a TPU v5e 2x2 if we
    wanted to. Except you’ll notice the number of KV caches is very small. That’s
    our batch size! That means we’ll be getting terrible FLOPs utilization. We’d be
    very happy to use a larger topology in order to push our batch size up to 240.</details>
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常酷！它告诉我们，如果我们想的话，我们可以在TPU v5e 2x2上拟合LLaMA 70B。但你会注意到KV缓存的数目非常小。那就是我们的批量大小！这意味着我们将得到非常差的FLOPs利用率。我们会非常高兴使用更大的拓扑来将我们的批量大小提高到240。
- en: '**Question:** Assume we use the largest batch size that fits on these topologies,
    what latency we could expect for each generate step?'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题：** 假设我们使用适合这些拓扑的最大批量大小，我们能为每个生成步骤预期多少延迟？'
- en: <details><summary>Answer</summary>
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>回答</summary>
- en: This is also easy, since we’re picking our batch size to fill up all our HBM!
    This is just a question of how long it takes to load a full TPU v5e’s worth of
    bytes into the MXU. This is just `v5e HBM / v5e HBM memory bandwidth = 16GB /
    8.2e11 = 19ms`, so this is **19ms / step**. Assuming our generations have a median
    length of 512 tokens, that is about 9s for each decode. Note that we could get
    marginally better latency with a smaller batch size, for instance if we only looked
    at model parameters in int4 our minimum latency is about 10ms / step, since HBM
    is no longer full.</details>
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这也很简单，因为我们选择批处理大小来填满所有的HBM！这只是一个问题，即加载一个完整的TPU v5e字节数需要多长时间进入MXU。这仅仅是`v5e HBM
    / v5e HBM内存带宽 = 16GB / 8.2e11 = 19ms`，所以这是**每步19ms**。假设我们的生成文本的中位数长度为512个标记，那么每次解码大约需要9秒。注意，如果我们使用更小的批处理大小，例如如果我们只查看int4模型参数，我们的最小延迟大约是每步10ms，因为HBM不再满。</details>
- en: '**Takeaway**: we can always lower bound decode latency by asking how long it
    takes to load all the model’s parameters from HBM into the MXU. When our KV caches
    are small, you can think about each layer as just loading the weights chunk-by-chunk
    and then discarding them. Unless we’re using large batch sizes or lots of inter-device
    comms, this is often a reasonable bound (within 1.5x). When our batch size is
    bigger, we need to model the KV cache loading as well, since that dominates the
    parameters.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结**：我们可以通过询问从HBM加载所有模型参数到MXU需要多长时间来降低解码延迟的下限。当我们的KV缓存较小时，你可以将每一层视为逐块加载权重然后丢弃它们。除非我们使用大批处理大小或大量的跨设备通信，这通常是一个合理的界限（在1.5倍以内）。当我们的批处理大小更大时，我们需要将KV缓存加载也建模，因为这将主导参数。'
- en: Likewise, in the FLOPs-bound regime (e.g. training or big-batch inference),
    we can use the \(\text{Total FLOPs} / (N \cdot C) = 2 \cdot \text{param count}
    \cdot B / (N \cdot C)\) lower bound, which assumes no communication.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，在FLOPs限制区域（例如训练或大批推理），我们可以使用\(\text{Total FLOPs} / (N \cdot C) = 2 \cdot
    \text{param count} \cdot B / (N \cdot C)\)的下限，这假设没有通信。
- en: '**Question:** For each of these, what throughput per chip does this give us
    (in terms of queries / chip)? *You can assume our median decode length is 512
    tokens.*'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：对于这些，每芯片的吞吐量是多少（以查询/芯片计）？*你可以假设我们的中值解码长度为512个标记。*'
- en: <details><summary>Answer</summary>
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>答案</summary>
- en: This is an important question because it’s exactly correlated with cost / token.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个重要的问题，因为它与成本/标记直接相关。
- en: 'With our assumption about median decode length, our throughput is just \(B
    / (\text{per-step latency} \cdot \text{median steps} \cdot N) \approxeq 43 / (0.019
    * 512 * N)\). This gives us roughly \((4.42 / N)\) QPS, so plugging in \(N\) we
    get:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们对中值解码长度的假设，我们的吞吐量仅仅是\(B / (\text{每步延迟} \cdot \text{中值步数} \cdot N) \approx
    43 / (0.019 * 512 * N)\)。这给我们大约\((4.42 / N)\) QPS，所以将\(N\)代入我们得到：
- en: '| dtype | QPS / chip |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 数据类型 | 每芯片QPS |'
- en: '| --- | --- |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| bfloat16 | 0.27 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| bfloat16 | 0.27 |'
- en: '| int8 | 0.55 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| int8 | 0.55 |'
- en: '| int4 | 1.11 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| int4 | 1.11 |'
- en: Note that this is rather optimistic since it totally ignores the working memory
    of the forward pass (memory allocated to activations and attention). This is not
    ridiculous with Flash Attention, but it is also not realistic. The real numbers
    are likely maybe 1/2 of this. For absolutely maximum throughput we would probably
    want to more than double the number of chips and increase the batch size significantly
    as well.</details>
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这相当乐观，因为它完全忽略了前向传递的工作内存（分配给激活和注意力的内存）。这对于Flash Attention来说并不荒谬，但也不是现实的。实际的数字可能大约是这个数字的一半。为了获得绝对的最大吞吐量，我们可能需要将芯片数量加倍以上，并且显著增加批处理大小。</details>
- en: '**Question:** How would our peak throughput change if we doubled our topology
    for each of the above examples?'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：如果我们加倍上述每个示例中的拓扑结构，我们的峰值吞吐量会如何变化？'
- en: <details><summary>Answer</summary>
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>答案</summary>
- en: If we used a 4x8 slice in bfloat16, we would have 372GB remaining for KV caches,
    which would let us up our batch size to 140\. Then since our step time would remaining
    the same, we would have a throughput of `14.39 / num_chips`, or
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用4x8切片在bfloat16中，我们将剩下372GB用于KV缓存，这将使我们能够将批处理大小提高到140。然后，由于我们的步长时间保持不变，我们将有`14.39
    / num_chips`的吞吐量，
- en: '| dtype | QPS / chip |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 数据类型 | 每芯片QPS |'
- en: '| --- | --- |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| bfloat16 (on 4x8) | 0.44 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| bfloat16（在4x8上） | 0.44 |'
- en: '| int8 (on 4x4) | 0.90 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| int8（在4x4上） | 0.90 |'
- en: '| int4 (on 2x4) | 1.80 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| int4（在2x4上） | 1.80 |'
- en: A further increase would give an even bigger win! The big takeaway is that **the
    smallest topology is not the most performance topology** in all cases, if we’re
    limited by KV cache size.</details>
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步增加将带来更大的收益！重要的结论是，**在所有情况下，最小的拓扑结构并不一定是性能最佳的拓扑结构**，如果我们受KV缓存大小的限制。
- en: '**Question:** Now let’s dig into the question of sharding. Let’s say we wanted
    to serve in bfloat16 on a TPU v5e 4x8\. What sharding would we use for our model
    on a TPU v5e 4x8 during generation? Can we avoid being communication bound?'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题：** 现在我们来深入探讨分片的问题。假设我们想在TPU v5e 4x8上以bfloat16格式提供服务。在生成过程中，我们为TPU v5e
    4x8上的模型使用什么分片方式？我们能避免成为通信瓶颈吗？'
- en: <details><summary>Answer</summary>
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>答案</summary>
- en: 'As discussed in the previous section, we only really have one option for sharding
    during generation: model parallelism. How much can we do before we become communication
    bound? As we’ve discussed in the previous section, our models become communication
    bound roughly when'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如前节所述，我们在生成过程中的分片选项实际上只有一个：模型并行。在我们成为通信瓶颈之前我们能做多少？正如前节所述，我们的模型在大约以下情况下成为通信瓶颈：
- en: \[Y > \frac{F \cdot M_Y}{2200}\]
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: \[Y > \frac{F \cdot M_Y}{2200}\]
- en: For LLaMA 3-70B we have `F = 28,672`, so if we do 2 axes of model sharding this
    gives us roughly \(Y = 28672 \cdot 2 / 2200 = 26\), so in general we could scale
    up to about 16 chips without being communication bound, which lets us use a `4x4`
    but not a `4x8`. Generally, since we do not perfectly overlap computation, even
    this estimate is overly optimistic.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 对于LLaMA 3-70B，我们有`F = 28,672`，所以如果我们进行2轴模型分片，这将给我们大约 \(Y = 28672 \cdot 2 / 2200
    = 26\)，因此，一般来说，我们可以扩展到大约16个芯片而不会成为通信瓶颈，这让我们可以使用`4x4`而不是`4x8`。一般来说，由于我们并没有完美地重叠计算，即使这个估计也过于乐观。
- en: '**Takeaway: we cannot actually serve on a 4x8 with pure model parallelism.**
    The best we can do here is a 4x2 or *maybe* a 4x4.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**要点：** 实际上，我们无法在4x8上仅使用纯模型并行来提供服务。我们能做到的最好情况是4x2或者*可能*是4x4。'
- en: 'However, as we’ve discussed, when our batch size is small we can often do more
    model parallelism without significantly hurting throughput, since our model is
    memory-bandwidth-bound and not FLOPs bound. We said before that this value is
    roughly $Y=F / (8\cdot B)$, so if we did batch size 64, we could in theory go
    up to `Y = 28,672 / (8 * 64) = 56` way model parallelism before we become ICI-bound.
    To sanity check this, we can look at $T_\text{ici comms}$, $T_\text{hbm comms}$,
    and $T_\text{math}$ for a single matmul. We clearly have:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正如我们之前讨论的，当我们的批量大小较小时，我们通常可以进行更多的模型并行而不会显著影响吞吐量，因为我们的模型是内存带宽瓶颈而不是FLOPs瓶颈。我们之前说过这个值大约是
    $Y=F / (8\cdot B)$，所以如果我们使用批量大小64，理论上我们可以进行到`Y = 28,672 / (8 * 64) = 56`个模型并行，在我们成为ICI瓶颈之前。为了验证这一点，我们可以查看单个矩阵乘法的$T_\text{ici
    comms}$，$T_\text{hbm comms}$和$T_\text{math}$。我们明显有：
- en: \[\begin{align*}T_\text{ici comms} = \frac{2BD}{W_\text{ici}} && T_\text{hbm
    comms} = \frac{2DF}{Y \cdot W_\text{hbm}} && T_\text{math} = \frac{2BDF}{Y \cdot
    C}\end{align*}\]
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*}T_\text{ici comms} = \frac{2BD}{W_\text{ici}} && T_\text{hbm
    comms} = \frac{2DF}{Y \cdot W_\text{hbm}} && T_\text{math} = \frac{2BDF}{Y \cdot
    C}\end{align*}\]
- en: For a `4x8`, this would give us $T_\text{ici comms}$ = `(2 * 64 * 8192) / 9e10
    = 11us`, $T_\text{hbm comms}$ = `(2 * 8192 * 28,672) / (32 * 8.1e11) = 18us`,
    and $T_\text{math}$ = `(2 * 64 * 8192 * 28,672) / (32 * 1.97e14) = 4us`, so in
    theory we’re still HBM bandwidth bound, which is great! *Note that scaling up
    from a `4x4` to a `4x8` probably isn’t helpful from a throughput standpoint, but
    it’ll reduce our latency!
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`4x8`，这将给我们 $T_\text{ici comms}$ = `(2 * 64 * 8192) / 9e10 = 11us`，$T_\text{hbm
    comms}$ = `(2 * 8192 * 28,672) / (32 * 8.1e11) = 18us`，和 $T_\text{math}$ = `(2
    * 64 * 8192 * 28,672) / (32 * 1.97e14) = 4us`，所以在理论上我们仍然是HBM带宽瓶颈，这是很好的！*注意，从`4x4`扩展到`4x8`可能从吞吐量角度来看并不有帮助，但它会减少我们的延迟！*
- en: 'If we look at the int8 and int4 configs, we *can* do those with pure model
    parallelism. So we’ve hit a point at which quantization actually gives us a meaningful
    advantage beyond faster FLOPs: it lets us use a larger batch size before we become
    comms-bound. **So the end of this story is that we can’t achieve peak throughput
    on a 4x8, but for the int8 and int4 configs we could do pure model parallelism*.</details>'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看int8和int4配置，我们可以使用纯模型并行来完成这些。所以，我们已经达到了一个点，量化实际上给我们带来了超越更快FLOPs的有意义的优势：它让我们在使用通信瓶颈之前可以使用更大的批量大小。**所以这个故事的结果是，我们无法在4x8上实现峰值吞吐量，但对于int8和int4配置，我们可以进行纯模型并行**。</details>
- en: '**Tip**: the maximum amount of useful model parallelism depends on \(d_{ff}\)
    and the number of axes over which you’re sharding your model. The maximum value
    usually ranges between 8 and 32 depending on the model size. You can scale beyond
    this limit to improve latency at some throughput cost.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示**：最大有用的模型并行度取决于 \(d_{ff}\) 和你在哪个轴上对模型进行分片。最大值通常在 8 到 32 之间，具体取决于模型大小。你可以超过这个限制以在吞吐量有一定成本的情况下提高延迟。'
- en: What about prefill?
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预填充怎么样？
- en: We’ve mostly ignored prefill here because it’s much simpler. Let’s put a couple
    of concepts together and think about the end-to-end picture.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里主要忽略了预填充，因为它要简单得多。让我们将几个概念结合起来，思考端到端的情况。
- en: '**Question:** Assume we achieve a 40% FLOPs utilization during prefill. How
    long will a prefill of length 8192 take on 16 TPU v5e chips?'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：假设我们在预填充期间达到 40% 的 FLOPs 利用率。长度为 8192 的预填充在 16 个 TPU v5e 芯片上需要多长时间？'
- en: <details><summary>Answer</summary>
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>答案</summary>
- en: At 8k tokens, we are solidly compute bound, so we just need to reason about
    FLOPs. We know our model has `70e9` parameters so each forward pass uses `2 *
    70e9 * B` FLOPs. Assuming 40% MFU (FLOPs utilization), this gives us a runtime
    of about `2 * 70e9 * 8192 / (16 * 1.97e14 * 0.4) = 0.91s`. Compared to the numbers
    we’ve been looking at before, that’s actually quite a lot!</details>
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在 8k 标记时，我们完全受计算限制，所以我们只需要考虑 FLOPs。我们知道我们的模型有 `70e9` 个参数，所以每次前向传递使用 `2 * 70e9
    * B` FLOPs。假设 40% MFU（FLOPs 利用率），这给我们大约 `2 * 70e9 * 8192 / (16 * 1.97e14 * 0.4)
    = 0.91s` 的运行时间。与之前我们看到的数字相比，这实际上相当多！
- en: '**Question:** Assume we have a median prefill length of 8192 tokens and a median
    decode length of 4096 tokens. Say we have a generate batch size of 32\. On average
    how many sequences finish decoding per step? On average how many tokens are evicted
    from our KV cache each step?'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：假设我们有平均预填充长度为 8192 个标记和平均解码长度为 4096 个标记。如果我们有一个生成批大小为 32。平均每步有多少序列完成解码？平均每步有多少标记从我们的
    KV 缓存中移除？'
- en: <details><summary>Answer</summary>
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>答案</summary>
- en: This is kind of straightforward. Since we have a median decode length of 4096
    tokens, a sequence will finish roughly every 1 / 4096 tokens. Given a batch size
    of 32, this means we have `32 / 4096` sequences evicted per step. Since our KV
    cache length is roughly `8192 + 4096`, this is `32 * (8192 + 4096) / 4096 = 96`
    tokens evicted per step. The general formula is $B * (P + G) / G$ where $P$ and
    $G$ are the prefill and generate lengths.</details>
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这相当直接。由于我们有平均解码长度为 4096 个标记，一个序列大约每 1 / 4096 个标记完成一次。给定批大小为 32，这意味着每步我们有 `32
    / 4096` 个序列被移除。由于我们的 KV 缓存长度大约为 `8192 + 4096`，这意味着每步移除 `32 * (8192 + 4096) / 4096
    = 96` 个标记。一般公式是 $B * (P + G) / G$，其中 $P$ 和 $G$ 是预填充和生成长度。
- en: '**Question:** Assume we do disaggregated serving with a median prefill length
    of 8192 and a median decode length of 512\. Assume the prefill and generate latencies
    calculated above in bfloat16\. What ratio of prefill:generate servers will you
    need to keep both fully saturated.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：假设我们使用平均预填充长度为 8192 和平均解码长度为 512 的解耦服务。假设预填充和生成延迟如上所述在 bfloat16 下计算。你需要多少预填充：生成服务器的比例才能保持两者都完全饱和？'
- en: <details><summary>Answer</summary>
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>答案</summary>
- en: This is kind of a fun question. Let $P$ be the number of prefill servers and
    $G$ be the number of generate servers. So generally speaking, this is a pipeline
    problem where we feed sequences in at a rate of `P / prefill_latency` and consume
    them at a rate of `B * G / (generate_latency * median_decode_length)`. We had
    calculated `910ms` per prefill step and `19ms` per decode step at batch size 43
    (let’s call that 32). Therefore we need `P / 0.91 = 32 * G / (0.019 * 512)` or
    `P = 3G`, i.e. we need about 3 times more prefill servers than generation servers!</details>
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个很有趣的问题。设 $P$ 为预填充服务器的数量，$G$ 为生成服务器的数量。所以一般来说，这是一个管道问题，我们以 `P / prefill_latency`
    的速率输入序列，以 `B * G / (generate_latency * median_decode_length)` 的速率消费它们。我们在批大小为
    43（让我们称之为 32）的情况下计算了每个预填充步骤 `910ms` 和每个解码步骤 `19ms`。因此，我们需要 `P / 0.91 = 32 * G
    / (0.019 * 512)` 或 `P = 3G`，也就是说，我们需要比生成服务器多大约 3 倍的预填充服务器！
- en: Visualizing the Latency Throughput Tradeoff
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化延迟吞吐量权衡
- en: Sticking with LLaMA 70B for a second, let’s actually look at the latency and
    throughput for different batch sizes during generation. As we showed in the previous
    section for PaLM models, this gives us a Pareto frontier for throughput/latency.
    Let’s assume 16-way tensor parallelism since that’s a reasonable bound on what
    we can use while staying compute-bound in the MLP blocks. We’ll use a TPU v5e
    4x4 topology here. **The slider controls the sequence length so you can see the
    effect of larger KV caches.**
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续关注 LLaMA 70B，实际查看生成过程中不同批量大小的延迟和吞吐量。正如我们在上一节中为 PaLM 模型所展示的，这为我们提供了吞吐量/延迟的帕累托前沿。让我们假设
    16 方张量并行性，因为这是我们在 MLP 块中保持计算受限的合理界限。这里我们将使用 TPU v5e 4x4 体系结构。**滑块控制序列长度，你可以看到更大
    KV 缓存的影响。**
- en: '**See how dramatic the tradeoff is between cost and latency.** At the cost
    of doubling per-token latency, we can achieve a roughly 100x reduction in per-token
    cost. Also, our latency can range anywhere from 5.5ms with low batch size to 20
    ms with very large batches.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**看看成本和延迟之间的权衡是多么的显著。** 以每-token 延迟加倍为代价，我们可以实现每-token 成本的约 100 倍减少。此外，我们的延迟可以从低批量大小的
    5.5ms 到非常大的批量大小的 20ms 不等。'
- en: Note how at 2k context the throughput effectively plateaus at around 1 token
    / ms / chip when it hits the BS 120 roofline (120 here because we do int8 weights
    but bf16 FLOPs). As the sequence length increases, however, we can no longer fit
    this batch size in memory, so we never hit the point of full saturation.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意，在 2k 上下文中，当达到 BS 120 屋顶线（这里因为使用 int8 权重但 bf16 FLOPs，所以是 120）时，吞吐量实际上在 1 token
    / ms / chip 左右达到平台期。然而，随着序列长度的增加，我们无法将这个批大小放入内存中，所以我们永远不会达到完全饱和的点。
- en: Note how much higher the latency is at large batch sizes for the same throughput,
    since KV loading becomes dominant (instead of parameter loading).
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意，在相同吞吐量下，大批量大小的延迟要高得多，因为 KV 加载变得占主导地位（而不是参数加载）。
- en: We can understand this better by breaking down the sources of cost and latency
    into param loading time, KV loading time, and FLOPs time. The red sector is the
    region in which we expect to be compute-bound in our MLP blocks.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将成本和延迟的来源分解为参数加载时间、KV 加载时间和 FLOPs 时间来更好地理解这一点。红色区域是我们预期在 MLP 块中计算受限的区域。
- en: This tells quite a story. You can see that initially, parameter loading represents
    the vast majority of the latency, until the batch size becomes large enough that
    FLOPs and KV loading become more significant. Notably, at all sequence lengths
    greater than 2048, we spend more time on KV cache loading than we do on FLOPs!
    **So while we can improve our hardware utilization by increasing batch size, at
    long context lengths KV loading always dominates the total step time.**
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这讲述了一个相当有趣的故事。你可以看到，最初，参数加载代表了延迟的绝大部分，直到批量大到足以使 FLOPs 和 KV 加载变得更重要。值得注意的是，在所有大于
    2048 的序列长度中，我们在 KV 缓存加载上花费的时间比在 FLOPs 上还要多！**因此，虽然我们可以通过增加批大小来提高硬件利用率，但在长上下文长度中，KV
    加载总是主导总步骤时间。**
- en: '**Takeaway:** for LLaMA 3-70B, we are strongly KV cache memory bandwidth-bound
    (and HBM-bound) in almost all of these configurations, highlighting just how important
    reducing KV cache size is for generation throughput. Also note just how dramatic
    the latency/throughput tradeoff remains here.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结：** 对于 LLaMA 3-70B，我们在几乎所有这些配置中都是强 KV 缓存内存带宽限制（以及 HBM 限制）的（并且是 HBM 限制），这突出了减少
    KV 缓存大小对于生成吞吐量是多么的重要。也请注意，这里的延迟/吞吐量权衡仍然非常显著。'
- en: <details><summary>The code for this is quite simple.</summary>
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>此代码相当简单。</summary>
- en: 'Here’s the code for computing these rooflines:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是计算这些屋顶线的代码：
- en: '[PRE0]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Note how we very explicitly break out latency into two sources: KV loading
    and param loading, and how the latency is either bound by FLOPs or comms, whichever
    is bigger.</details>'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们如何非常明确地将延迟分为两个来源：KV 加载和参数加载，以及延迟是由 FLOPs 还是通信限制，哪个更大。
- en: Worked Problems
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 已解决的问题
- en: Here are a few worked problems. Some of these repeat things that are worked
    above, but might be pedagogically useful.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些已解决的问题。其中一些重复了上面已经解决的问题，但可能在教学上是有用的。
- en: '**Question 1:** How many FLOPs does each forward pass for LLaMA 3-405B use
    per-token? Assuming we’re FLOPs bound, what is a lower bound on a single forward
    pass on N chips on TPU v5e? What if we’re comms bound? *Ignore the fact that the
    model does not fit on a single chip.*'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 1：** LLaMA 3-405B 每个前向传递每-token 使用多少 FLOPs？假设我们是 FLOPs 限制的，那么在 N 个芯片上
    TPU v5e 上单个前向传递的下限是多少？如果我们是通信限制的呢？*忽略模型无法适应单个芯片的事实。*'
- en: '**Question 2:** Assume we want to serve LLaMA 3-8B with BS240 using int8 weights
    and int8 KV caches. How many bytes are used by (a) model parameters (b) KV caches
    and (c) peak working activations (roughly)? What’s the smallest topology we can
    run this on?'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 2：** 假设我们想使用 BS240 和 int8 权重以及 int8 KV 缓存来服务 LLaMA 3-8B。以下内容分别使用了多少字节：（a）模型参数（b）KV
    缓存和（c）峰值工作激活（大致数量）。我们可以在最小的拓扑结构上运行这个模型吗？'
- en: '**Question 3:** How would you serve LLaMA 3-405B on TPU v5e? Assume int8 weights
    and bfloat16 FLOPs. Let’s say we have a firm limit of 15ms / token, what’s the
    highest throughput configuration we could achieve? What is the theoretical minimum
    step time?'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 3：** 你会如何在 TPU v5e 上服务 LLaMA 3-405B？假设使用 int8 权重和 bfloat16 FLOPs。如果我们有一个严格的
    15ms / token 限制，我们能够达到的最高吞吐量配置是什么？理论上的最小步时间是多少？'
- en: That’s all for Part 8! For Part 9, with a deep dive into XLA and TPU profiling,
    click [here](../profiling).</d-article>  <d-appendix><d-footnote-list><d-citation-list>###
    Miscellaneous
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第 8 部分（Part 8）的内容到此结束！要深入了解 XLA 和 TPU 性能分析（profiling），请点击[这里](../profiling)。</d-article>  <d-appendix><d-footnote-list><d-citation-list>###
    杂项
- en: ^*Work done at Google DeepMind, now at MatX.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ^*在 Google DeepMind 完成的工作，现在在 MatX。
- en: Citation
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 引用
- en: 'For attribution in academic contexts, please cite this work as:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在学术环境中进行归属时，请引用以下工作：
- en: '[PRE1]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'or as a BibTeX entry:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 或者作为一个 BibTeX 条目：
- en: '[PRE2]</d-citation-list></d-footnote-list></d-appendix> <d-bibliography src="/scaling-book/assets/bibliography/"></d-bibliography></d-byline>'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE2]</d-citation-list></d-footnote-list></d-appendix> <d-bibliography src="/scaling-book/assets/bibliography/"></d-bibliography></d-byline>'
