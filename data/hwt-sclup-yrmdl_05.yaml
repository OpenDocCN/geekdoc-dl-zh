- en: All the Transformer Math You Need to Know
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你需要了解的所有 Transformer 数学知识
- en: 原文：[https://jax-ml.github.io/scaling-book/transformers](https://jax-ml.github.io/scaling-book/transformers)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://jax-ml.github.io/scaling-book/transformers](https://jax-ml.github.io/scaling-book/transformers)
- en: '<d-title>Part 4 of [How To Scale Your Model](/scaling-book) ([Part 3: Sharding](../sharding)
    | [Part 5: Training](../training))'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: <d-title>《如何扩展你的模型》第 4 部分 [How To Scale Your Model](/scaling-book) ([第 3 部分：分片](../sharding)
    | [第 5 部分：训练](../training))
- en: Here we'll do a quick review of the Transformer architecture, specifically how
    to calculate FLOPs, bytes, and other quantities of interest.</d-title>  <d-byline><d-article><d-contents>###
    Contents
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将快速回顾 Transformer 架构，特别是如何计算 FLOPs、bytes 以及其他感兴趣的量。</d-title>  <d-byline><d-article><d-contents>###
    内容
- en: '[Counting Dots](#counting-dots)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[点积计数](#counting-dots)'
- en: '[Forward and reverse FLOPs](#forward-and-reverse-flops)'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[正向和反向 FLOPs](#forward-and-reverse-flops)'
- en: '[Transformer Accounting](#transformer-accounting)[Global FLOPs and Params Calculation](#global-flops-and-params-calculation)[Miscellaneous
    Math](#miscellaneous-math)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[Transformer 计费](#transformer-accounting)[全局 FLOPs 和参数计算](#global-flops-and-params-calculation)[其他数学知识](#miscellaneous-math)'
- en: '[Sparsity and Mixture-of-Experts](#sparsity-and-mixture-of-experts)'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[稀疏性和专家混合](#sparsity-and-mixture-of-experts)'
- en: '[Gradient checkpointing](#gradient-checkpointing)'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[梯度检查点](#gradient-checkpointing)'
- en: '[Key-Value (KV) caching](#key-value-kv-caching)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[键值（KV）缓存](#key-value-kv-caching)'
- en: '[What Should You Take Away from this Section?](#what-should-you-take-away-from-this-section)[A
    Few Problems to Work](#a-few-problems-to-work)[Appendix](#appendix)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[从这个部分你能学到什么？](#what-should-you-take-away-from-this-section)[一些问题供你练习](#a-few-problems-to-work)[附录](#appendix)'
- en: '[Appendix A: How does Flash Attention work?](#appendix-a-how-does-flash-attention-work)</d-contents>'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[附录 A：闪速注意力是如何工作的？](#appendix-a-how-does-flash-attention-work)</d-contents>'
- en: Counting Dots
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 点积计数
- en: 'Let’s start with vectors \(x\),\(y\) and matrices \(A\),\(B\) of the following
    shapes:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从以下形状的向量 \(x\)、\(y\) 和矩阵 \(A\)、\(B\) 开始：
- en: \[\def \red#1{\textcolor{red}{#1}} \def \green#1{\textcolor{green}{#1}} \def
    \blue#1{\textcolor{blue}{#1}} \def \purple#1{\textcolor{purple}{#1}} \def \orange#1{\textcolor{orange}{#1}}
    \def \gray#1{\textcolor{gray}{#1}} \begin{array}{cc} \textrm{array} & \textrm{shape}
    \\ \hline x & \textrm{[P]} \\ y & \textrm{[P]} \\ A & \textrm{[N P]} \\ B & \textrm{[P
    M]} \\ \hline \end {array}\]
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: \[\def \red#1{\textcolor{red}{#1}} \def \green#1{\textcolor{green}{#1}} \def
    \blue#1{\textcolor{blue}{#1}} \def \purple#1{\textcolor{purple}{#1}} \def \orange#1{\textcolor{orange}{#1}}
    \def \gray#1{\textcolor{gray}{#1}} \begin{array}{cc} \textrm{array} & \textrm{shape}
    \\ \hline x & \textrm{[P]} \\ y & \textrm{[P]} \\ A & \textrm{[N P]} \\ B & \textrm{[P
    M]} \\ \hline \end {array}\]
- en: A dot product of \(x \cdot y\) requires \(P\) *adds* and *multiplies*, or \(2P\)
    floating-point operations total.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(x \cdot y\) 的点积需要 \(P\) 个加法和乘法，总共 \(2P\) 个浮点运算。
- en: A matrix-vector product \(Ax\) does \(N\) dot-products along the rows of \(A\),
    for \(2NP\) FLOPs.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵-向量乘积 \(Ax\) 在 \(A\) 的行上执行 \(N\) 个点积，需要 \(2NP\) FLOPs。
- en: A matrix-matrix product \(AB\) does a matrix-vector product for each of the
    \(M\) columns of \(B\), for \(2NPM\) FLOPs total.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵-矩阵乘积 \(AB\) 对 \(B\) 的每一列执行矩阵-向量乘法，总共需要 \(2NPM\) FLOPs。
- en: In general, if we have two higher dimensional arrays \(C\) and \(D\), where
    some dimensions are CONTRACTING and some are BATCHING. (e.g. \(C[\blue{GH}IJ\red{KL}],
    D[\blue{GH}MN\red{KL}]\)) then the FLOPs cost of this contraction is two times
    the product of all of the \(C\) and \(D\) dimensions where the batch and contraction
    dimensions are only counted once, (e.g. \(2\blue{GH}IJMN\red{KL}\)). Note that
    a dimension is only batching if it occurs in both multiplicands. (Note also that
    the factor of 2 won’t apply if there are no contracting dimensions and this is
    just an elementwise product.)
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常，如果我们有两个高维数组 \(C\) 和 \(D\)，其中一些维度是收缩的，而一些是批处理的。（例如 \(C[\blue{GH}IJ\red{KL}],
    D[\blue{GH}MN\red{KL}]\)）那么这种收缩的 FLOPs 成本是所有 \(C\) 和 \(D\) 维度的乘积的两倍，其中批处理和收缩维度只计算一次，（例如
    \(2\blue{GH}IJMN\red{KL}\)）。注意，一个维度只有在乘数中同时出现时才进行批处理。（注意，如果没有收缩维度，这只是一个逐元素乘积，2
    的因子不适用。）
- en: \[\begin{array}{ccc} \textrm{Operation} & \textrm{FLOPs} & \textrm{Data} \\
    \hline x \cdot y & 2P & 2P \\ A x & 2NP & NP + P \\ AB & 2NPM & NP + PM \\ [c_0,...,c_N]
    \cdot [d_0,...,d_N] & 2 \prod c_i \times \prod_{\substack{d_j \notin \blue{BATCH}
    \\ d_j \notin \red{CONTRACT}}} d_j & \prod c_i + \prod d_j \\ \hline \end {array}\]
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{array}{ccc} \textrm{Operation} & \textrm{FLOPs} & \textrm{Data} \\
    \hline x \cdot y & 2P & 2P \\ A x & 2NP & NP + P \\ AB & 2NPM & NP + PM \\ [c_0,...,c_N]
    \cdot [d_0,...,d_N] & 2 \prod c_i \times \prod_{\substack{d_j \notin \blue{BATCH}
    \\ d_j \notin \red{CONTRACT}}} d_j & \prod c_i + \prod d_j \\ \hline \end {array}\]
- en: Make note of the fact that for a matrix-matrix multiply, the *compute* scales
    cubically \(O(N^3)\) while the data transfer only scales quadratically \(O(N^2)\)
    - this means that as we scale up our matmul size, it becomes *easier* to hit the
    compute-saturated limit. This is extremely unusual, and explains in large part
    why we use architectures dominated by matrix multiplication - they’re amenable
    to being scaled!
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，对于一个矩阵-矩阵乘法，*计算*的规模是立方级 \(O(N^3)\)，而数据传输的规模是二次级 \(O(N^2)\) —— 这意味着随着我们扩大矩阵乘法的大小，达到计算饱和极限会变得*更容易*。这种情况非常不寻常，这也是我们为什么使用以矩阵乘法为主导的架构的主要原因之一——它们易于扩展！
- en: <picture>![](../Images/65062640ca5583ac283453f9f1298420.png)</picture>
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: <picture>![](../Images/65062640ca5583ac283453f9f1298420.png)</picture>
- en: Forward and reverse FLOPs
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 正向和反向FLOPs
- en: During training, we don’t particularly care about the result of a given matrix
    multiply; we really care about its derivative. That means we do significantly
    more FLOPs during backpropagation.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们并不特别关心给定矩阵乘法的结果；我们真正关心的是它的导数。这意味着在反向传播过程中，我们执行了显著更多的浮点运算（FLOPs）。
- en: 'If we imagine **B** is just one matrix in a larger network and **A** are our
    input activations with **C = A B**, the derivative of the loss **L** with respect
    to **B** is given by the chain rule:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想象 **B** 是一个更大网络中的一个矩阵，而 **A** 是我们的输入激活，**C = A B**，则损失 **L** 对 **B** 的导数由链式法则给出：
- en: \[\frac{\partial L}{\partial B} = \frac{\partial L}{\partial C}\frac{\partial
    C}{\partial B} = A^T \left(\frac{\partial L}{\partial C}\right)\]
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: \[\frac{\partial L}{\partial B} = \frac{\partial L}{\partial C}\frac{\partial
    C}{\partial B} = A^T \left(\frac{\partial L}{\partial C}\right)\]
- en: which is an outer product and requires $2NPM$ FLOPs to compute (since it contracts
    over the $N$ dimension). Likewise, the derivative of the loss with respect to
    **A** is
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个外积，需要 \(2NPM\) FLOPs 来计算（因为它在 \(N\) 维度上收缩）。同样，损失对 **A** 的导数是
- en: \[\frac{\partial L}{\partial A} = \frac{\partial L}{\partial C}\frac{\partial
    C}{\partial A} = \left(\frac{\partial L}{\partial C}\right) B^T\]
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: \[\frac{\partial L}{\partial A} = \frac{\partial L}{\partial C}\frac{\partial
    C}{\partial A} = \left(\frac{\partial L}{\partial C}\right) B^T\]
- en: is again $2NPM$ FLOPs since **dL/dC** is a (co-)vector of size \([N, M]\). While
    this quantity isn’t the derivative wrt. a parameter, it’s used to compute derivatives
    for previous layers of the network (e.g. just as dL/dC is used to compute dL/dB
    above).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 \(\frac{\partial L}{\partial C}\) 是一个大小为 \([N, M]\) 的（共）向量，因此它又是 \(2NPM\)
    FLOPs，用于计算（例如，正如 \(\frac{\partial L}{\partial C}\) 用于计算 \(\frac{\partial L}{\partial
    B}\) 上面的导数一样）。
- en: 'Adding these up, we see that **during training, we have a total of 6NPM FLOPs**,
    compared to 2NPM during inference: 2NPM in the forward pass, 4NPM in the backward
    pass. Since PM is the number of parameters in the matrix, this is the simplest
    form of the famous \(6 * \text{num parameters} * \text{num tokens}\) approximation
    of Transformer FLOPs during training: each token requires \(6 * \text{num parameters}\)
    FLOPs. We’ll show a more correct derivation below.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些加起来，我们看到在训练过程中，我们总共有 \(6NPM\) FLOPs，而在推理过程中是 \(2NPM\)：正向传递中 \(2NPM\)，反向传递中
    \(4NPM\)。由于 PM 是矩阵中的参数数量，这是著名的 \(6 * \text{num parameters} * \text{num tokens}\)
    近似公式在训练期间变换器 FLOPs 的最简单形式：每个标记需要 \(6 * \text{num parameters}\) FLOPs。我们将在下面展示一个更准确的推导。
- en: Transformer Accounting
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变换器会计
- en: Transformers are the future. Well, they’re the present at least. Maybe a few
    years ago, they were one of many architectures. But today, it’s worth knowing
    pretty much every detail of the architecture. We won’t reintroduce the architecture
    but [this blog](https://jalammar.github.io/illustrated-transformer/) and the [original
    Transformer paper](https://arxiv.org/abs/1706.03762) may be helpful references.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器是未来的。好吧，至少它们现在是。也许几年前，它们只是众多架构之一。但今天，了解架构的每一个细节都很有价值。我们不会重新介绍架构，但[这篇博客](https://jalammar.github.io/illustrated-transformer/)和[原始的变换器论文](https://arxiv.org/abs/1706.03762)可能是有用的参考资料。
- en: 'Here’s a basic diagram of the Transformer decoder architecture:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是变换器解码器架构的基本图示：
- en: <picture>![](../Images/2a272a1a4f8f6ddbe5aaa45fcb38ed57.png)</picture>
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: <picture>![](../Images/2a272a1a4f8f6ddbe5aaa45fcb38ed57.png)</picture>
- en: '**Figure:** this diagram shows one layer of a standard Transformer and flows
    from top-to-bottom. We use a single-letter convention to describe the shapes and
    layouts of arrays in a Transformer, again showing contracting dimensions in red,
    and batched dimensions in blue. In a given operation, the input shape is given
    on top-left and the parameter shape is given on the top-right, with the resulting
    shape below, e.g. BTD is the input shape for the gating einsum and DF is the weight
    shape.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**图示**：此图显示了标准 Transformer 的一层，从上到下流动。我们使用单字母约定来描述 Transformer 中数组的形状和布局，再次用红色显示收缩维度，用蓝色显示批量维度。在给定操作中，输入形状位于左上角，参数形状位于右上角，结果形状位于下方，例如，BTD
    是门控 einsum 的输入形状，DF 是权重形状。'
- en: '**Note [gating einsum]**: The diagram above uses a “[gating einsums](https://arxiv.org/abs/2002.05202)”
    <d-cite key="glu">where we split the up-projection matrix into two matrices ($W_\text{In1}$
    and $W_\text{In2}$ above) whose outputs are elementwise multiplied as a kind of
    “gating function”. Not all LLMs use this, so you will sometimes see a single $W_\text{In}$
    matrix and a total MLP parameter count of 2DF instead of 3DF. Typically in this
    case, D and F will be scaled up to keep the parameter count the same as the 3
    matrix case. With that said, some form of gating einsum is used by LLAMA, DeepSeek,
    and many other models.</d-cite>'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意 [门控 einsum]**：上面的图使用了一个“[门控 einsums](https://arxiv.org/abs/2002.05202)”
    <d-cite key="glu">，其中我们将上投影矩阵分成两个矩阵（上面的 $W_\text{In1}$ 和 $W_\text{In2}$），它们的输出作为“门控函数”逐元素相乘。并非所有大型语言模型都使用这种方法，因此有时你会看到一个单一的
    $W_\text{In}$ 矩阵，总 MLP 参数计数为 2DF 而不是 3DF。通常在这种情况下，D 和 F 将按比例放大以保持参数计数与 3 个矩阵的情况相同。话虽如此，LLAMA、DeepSeek
    和许多其他模型都使用某种形式的门控 einsum。</d-cite>'
- en: '**Note 2 [MHA attention]**: With self-attention, T and S are the same but for
    cross-attention they may be different. With vanilla Multi-Head Attention (MHA),
    N and K are the same while for [Multi-Query Attention](https://arxiv.org/abs/1911.02150)
    (MQA) <d-cite key="mqa">K=1 and for [Grouped MQA](https://arxiv.org/abs/2305.13245)
    (GMQA) <d-cite key="gmqa">K merely has to divide N.</d-cite></d-cite>'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意 2 [MHA 注意力]**：在自注意力中，T 和 S 是相同的，但对于交叉注意力，它们可能不同。在标准的多头注意力（MHA）中，N 和 K
    是相同的，而对于[多查询注意力](https://arxiv.org/abs/1911.02150)（MQA），K=1，对于[分组 MQA](https://arxiv.org/abs/2305.13245)（GMQA），K
    只需将 N 分割。</d-cite></d-cite>'
- en: Global FLOPs and Params Calculation
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 全局 FLOPs 和 Params 计算
- en: For the below we’re going to compute per-layer FLOPs to avoid having to stick
    factors of **L** everywhere.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对于以下内容，我们将计算每层的 FLOPs 以避免在所有地方都粘附 **L** 的因子。
- en: MLPs
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MLPs
- en: 'The MLPs of a Transformer typically consist of 2 input matmuls that are element-wise
    combined and a single output matmul:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 的 MLP 通常由 2 个输入矩阵乘法组成，它们是逐元素组合的，以及一个单独的输出矩阵乘法：
- en: \[\begin{array}{ccc} \textrm{operation} & \textrm{train FLOPs} & \textrm{params}
    \\ \hline \\ A[B,T,\red{D}] \cdot W_{in1}[\red{D}, F] & 6BTDF & DF \\[10pt] A[B,T,\red{D}]
    \cdot W_{in2}[\red{D}, F] & 6BTDF & DF \\[10pt] \sigma\left(A_{in1}\right)[B,T,
    F] * A_{in2}[B,T, F] & \gray{O(BTF)} \\[10pt] A[B,T,\red{F}] \cdot W_{out}[\red{F},
    D] & 6BTDF & DF \\[10pt] \hline \\ & \approx 18BTDF & 3DF \end{array}\]
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{array}{ccc} \textrm{操作} & \textrm{训练 FLOPs} & \textrm{参数} \\ \hline
    \\ A[B,T,\red{D}] \cdot W_{in1}[\red{D}, F] & 6BTDF & DF \\[10pt] A[B,T,\red{D}]
    \cdot W_{in2}[\red{D}, F] & 6BTDF & DF \\[10pt] \sigma\left(A_{in1}\right)[B,T,
    F] * A_{in2}[B,T, F] & \gray{O(BTF)} \\[10pt] A[B,T,\red{F}] \cdot W_{out}[\red{F},
    D] & 6BTDF & DF \\[10pt] \hline \\ & \approx 18BTDF & 3DF \end{array}\]
- en: Attention
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意力
- en: 'For the generic grouped-query attention case with different **Q** and **KV**
    head numbers, let us assume equal head dimension H for **Q**,**K**,**V** projections,
    and estimate the cost of the **QKVO** matmuls:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有不同 **Q** 和 **KV** 头数的通用分组查询注意力情况，我们假设 **Q**、**K**、**V** 投影的头部维度 H 相等，并估计
    **QKVO** 矩阵乘法的成本：
- en: \[\begin{array}{ccc} \textrm{operation} & \textrm{train FLOPs} & \textrm{params}
    \\ \hline \\ A[B,T,\red{D}] \cdot W_{Q}[\red{D}, N, H] & 6BTDNH & DNH \\[10pt]
    A[B,T,\red{D}] \cdot W_{K}[\red{D}, K, H] & 6BTDKH & DKH \\[10pt] A[B,T,\red{D}]
    \cdot W_{V}[\red{D}, K, H] & 6BTDKH & DKH \\[10pt] A[B,T,\red{N}, \red{H}] \cdot
    W_{O}[\red{N}, \red{H}, D] & 6BTDNH & DNH \\[10pt] \hline \\ & 12BTD(N+K)H & 2D(N+K)H
    \end{array}\]
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{array}{ccc} \textrm{操作} & \textrm{训练 FLOPs} & \textrm{参数} \\ \hline
    \\ A[B,T,\red{D}] \cdot W_{Q}[\red{D}, N, H] & 6BTDNH & DNH \\[10pt] A[B,T,\red{D}]
    \cdot W_{K}[\red{D}, K, H] & 6BTDKH & DKH \\[10pt] A[B,T,\red{D}] \cdot W_{V}[\red{D},
    K, H] & 6BTDKH & DKH \\[10pt] A[B,T,\red{N}, \red{H}] \cdot W_{O}[\red{N}, \red{H},
    D] & 6BTDNH & DNH \\[10pt] \hline \\ & 12BTD(N+K)H & 2D(N+K)H \end{array}\]
- en: 'The dot-product attention operation is more subtle, effectively being a \(TH
    \cdot HS\) matmul batched over the \(B\), \(K\) dimensions, a softmax, and a \(TS
    \cdot SH\) matmul again batched over the \(B\), \(K\) dimensions. We highlight
    the batched dims in blue:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 点积注意力操作更为微妙，实际上是一个\(TH \cdot HS\)矩阵乘法，在\(B\)、\(K\)维度上批处理，然后是softmax，再次是\(TS
    \cdot SH\)矩阵乘法，在\(B\)、\(K\)维度上批处理。我们用蓝色突出显示批处理维度：
- en: \[\begin{array}{cc} \textrm{operation} & \textrm{train FLOPs} \\ \hline \\[3pt]
    Q[\blue{B}, T, \blue{K}, G, \red{H}] \cdot K[\blue{B}, S, \blue{K}, \red{H}] &
    6BTSKGH = 6BTSNH \\[3pt] \textrm{softmax}_S \;\; L[B, T, S, K, G] & \gray{O(BTSKG)
    = O(BTSN)} \\[3pt] S[\blue{B}, T, \red{S}, \blue{K}, G] \cdot V[\blue{B}, \red{S},
    \blue{K}, H] & 6BTSKGH = 6BTSNH \\[3pt] \hline \\ & \approx 12BTSNH = 12BT^2NH
    \\ \end{array}\]
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{array}{cc} \textrm{operation} & \textrm{train FLOPs} \\ \hline \\[3pt]
    Q[\blue{B}, T, \blue{K}, G, \red{H}] \cdot K[\blue{B}, S, \blue{K}, \red{H}] &
    6BTSKGH = 6BTSNH \\[3pt] \textrm{softmax}_S \;\; L[B, T, S, K, G] & \gray{O(BTSKG)
    = O(BTSN)} \\[3pt] S[\blue{B}, T, \red{S}, \blue{K}, G] \cdot V[\blue{B}, \red{S},
    \blue{K}, H] & 6BTSKGH = 6BTSNH \\[3pt] \hline \\ & \approx 12BTSNH = 12BT^2NH
    \\ \end{array}\]
- en: '**Note [causal masking]**: Most recent transformers use a causal mask as opposed
    to full bidirectional attention. In this case the useful FLOPs of the dot product
    operations are reduced by a factor of 1/2\. To achieve this reduction in practice
    we need to make use of an attention kernel, rather than a naive einsum.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意[因果掩码]**：大多数最近的Transformer使用因果掩码而不是全双向注意力。在这种情况下，点积操作的有效FLOPs减少了1/2倍。为了在实际中实现这种减少，我们需要使用一个注意力内核，而不是简单的einstein求和。'
- en: Other Operations
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 其他操作
- en: There are several other operations happening in a Transformer. Layernorms are
    comparatively cheap and can be ignored for first-order cost estimates. There is
    also the final enormous (though not per-layer) unembedding matrix multiply.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer中还有其他几个操作。层归一化相对便宜，在第一阶成本估计中可以忽略。还有一个巨大的（尽管不是每层的）反嵌入矩阵乘法。
- en: \[\begin{array}{ccc} \textsf{operation} & \textsf{train FLOPs} & \textsf{params}
    \\ \hline \\ \textrm{layernorm}_D \;\; A[B,T,\red{D}] & \gray{O\left(BTD\right)}
    & \gray{D} \\[10pt] A[B,T,\red{D}] \cdot W_{unembed}[\red{D}, V] & 6BTDV & DV
    \\ \end{array}\]
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{array}{ccc} \textsf{operation} & \textsf{train FLOPs} & \textsf{params}
    \\ \hline \\ \textrm{layernorm}_D \;\; A[B,T,\red{D}] & \gray{O\left(BTD\right)}
    & \gray{D} \\[10pt] A[B,T,\red{D}] \cdot W_{unembed}[\red{D}, V] & 6BTDV & DV
    \\ \end{array}\]
- en: General rule of thumb for Transformer FLOPs
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Transformer FLOPs的一般经验法则
- en: If we neglect the cost of dot-product attention for shorter-context training,
    then the total FLOPs across all layers is
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们忽略短上下文训练中点积注意力的成本，那么所有层的总FLOPs为
- en: \[\begin{align*} (18BTDF + 12BTD(N+K)H)L = 6 *BT * (3DF + 2D(N+K)H)L \\ = 6
    * \textrm{num tokens} * \textrm{parameter count} \end{align*}\]
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} (18BTDF + 12BTD(N+K)H)L = 6 *BT * (3DF + 2D(N+K)H)L \\ = 6
    * \textrm{num tokens} * \textrm{parameter count} \end{align*}\]
- en: Leading to a famous rule of thumb for estimating dense Transformer FLOP count,
    ignoring the attention FLOPs. (Unembedding is another simple matmul with $6BSDV$
    FLOPs and $DV$ params, and follows the same rule of thumb.)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 导致估算密集Transformer FLOP计数的著名经验法则，忽略注意力FLOPs。（反嵌入是另一个简单的矩阵乘法，具有$6BSDV$ FLOPs和$DV$参数，并遵循相同的经验法则。）
- en: Fractional cost of attention with context length
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意力与上下文长度成比例的成本
- en: 'If we do account for dot-product attention above and assume \(F=4D\), \(D=NH\)
    (as is typical) and \(N=K\):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们考虑上述点积注意力并假设 \(F=4D\), \(D=NH\)（这是典型的），且 \(N=K\)：
- en: \[\small{\frac{\textrm{attention FLOPs}}{\textrm{matmul FLOPs}} = \frac{12BT^2NH}{18BTDF
    + 24BTDNH} = \frac{12BT^2D}{4*18 BTD^2 + 24 BTD^2} = \frac{12BT^2D}{96 BTD^2}
    = \frac{T}{8D}}\]
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: \[\small{\frac{\textrm{attention FLOPs}}{\textrm{matmul FLOPs}} = \frac{12BT^2NH}{18BTDF
    + 24BTDNH} = \frac{12BT^2D}{4*18 BTD^2 + 24 BTD^2} = \frac{12BT^2D}{96 BTD^2}
    = \frac{T}{8D}}\]
- en: So the takeaway is that **dot-product attention FLOPs only become dominant during
    training once T>8D**. For D ~ 8k, this would be ~64K tokens. This makes some sense,
    since it means as the MLP size increases, the attention FLOPs become less critical.
    For large models, the quadratic cost of attention is not actually a huge obstacle
    to longer context training. However, for smaller models, even e.g. Gemma-27B,
    D=4608 which means attention becomes dominant around 32k sequence lengths. Flash
    Attention also helps alleviate the cost of long-context, which we discuss briefly
    [in Appendix A](#appendix-a-how-does-flash-attention-work).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，总结来说，**点积注意力FLOPs仅在T>8D的训练过程中才变得占主导地位**。对于D ~ 8k，这将大约是64K个标记。这有些道理，因为这意味着随着MLP大小的增加，注意力FLOPs变得不那么关键。对于大型模型，注意力的二次成本实际上并不是长期上下文训练的巨大障碍。然而，对于较小的模型，即使是例如Gemma-27B，D=4608，这意味着注意力在约32k序列长度时变得占主导地位。Flash
    Attention也有助于减轻长上下文成本，我们将在附录A中简要讨论其工作原理。[在附录A中讨论Flash Attention的工作原理](#appendix-a-how-does-flash-attention-work)。
- en: Miscellaneous Math
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 杂项数学
- en: Sparsity and Mixture-of-Experts
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 稀疏性和混合专家
- en: We’d be remiss not to briefly discuss Mixture of Experts (MoE) models<d-cite
    key="moe">, which replace the single dense MLP blocks in a standard Transformer
    with a set of independent MLPs that can be dynamically routed between. To a first
    approximation, **an MoE is just a normal dense model with E MLP blocks per layer**,
    instead of just one. Each token activates $k$ of these experts, typically $k=2$.
    This increases the parameter count by $O(E)$, while multiplying the total number
    of activated parameters per token by $k$, compared with the dense version.</d-cite>
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能不简要讨论混合专家（MoE）模型[d-cite key="moe"]，它用一组独立的MLP替换了标准Transformer中的单个密集MLP块，这些MLP可以在之间动态路由。首先近似地，**MoE只是一个具有每层E个MLP块的普通密集模型**，而不是只有一个。每个标记激活k个这样的专家，通常k=2。这增加了参数计数为O(E)，同时将每个标记激活的总参数数乘以k，与密集版本相比。</d-cite>
- en: <picture>![](../Images/e2e267195a5dc6f4e5a6a3592fac60dd.png)</picture>
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: <picture>![](../Images/e2e267195a5dc6f4e5a6a3592fac60dd.png)</picture>
- en: '**Figure:** an example MoE layer with $n$ experts. The gating expert routes
    each token to $k$ of them, and the output of those $k$ MLPs get summed. Our parameter
    count is $n$ times the size of each expert, but only $k$ are used for each token.
    [Source](https://deepgram.com/learn/mixture-of-experts-ml-model-guide).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**图**：一个具有n个专家的MoE层的示例。门控专家将每个标记路由到k个中，这些k个MLP的输出被求和。我们的参数计数是每个专家大小的n倍，但每个标记只使用k个。
    [来源](https://deepgram.com/learn/mixture-of-experts-ml-model-guide)。'
- en: Compared to a dense model, an MoE introduces new comms, primarily two AllToAlls
    (one before and one after the MoE block) that route tokens to the correct expert
    and bring them back to their home device.<d-footnote>Technically, this only happens
    if we are data or sequence sharded along the same axis as our experts.</d-footnote>
    However as we saw in the previous section, the cost of each AllToAll is only 1/4
    that of a comparable AllGather along a single axis (for a bidirectional ring).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 与密集模型相比，MoE引入了新的通信，主要是两个AllToAll（一个在MoE块之前，一个在之后），将标记路由到正确的专家，并将它们带回其设备。<d-footnote>技术上，这只有在我们的数据或序列在专家相同的轴上分片时才会发生。</d-footnote>然而，正如我们在上一节中看到的，每个AllToAll的成本仅为沿单个轴可比AllGather的四分之一（对于双向环形）。
- en: Gradient checkpointing
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度检查点
- en: 'Backpropagation as an algorithm trades memory for compute. Instead of a backward
    pass requiring \(O(n_\text{layers}^2)\) FLOPs, **it requires \(O(n_\text{layers})\)
    memory**, saving all intermediate activations generated during the forward pass.
    While this is better than quadratic compute, it’s incredibly expensive memory-wise:
    a model with \(B * T=4M\) (4M total tokens per batch), L=64, and D=8192 that avoids
    all unnecessary backward pass compute would have to save roughly \(2 * 20 * B
    * T * D * L = 84TB\) of activations in bfloat16\. 20 comes from (roughly) counting
    every intermediate node in the Transformer diagram above, since e.g.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播作为一种算法，以内存换取计算。而不是反向传递需要\(O(n_\text{layers}^2)\) FLOPs，**它需要\(O(n_\text{layers})\)内存**，保存了正向传递过程中生成的所有中间激活。虽然这比二次计算要好，但在内存方面却非常昂贵：一个\(B
    * T=4M\)（每个批次4M个总标记），L=64，D=8192且避免所有不必要的反向传递计算的模型，将不得不保存大约\(2 * 20 * B * T *
    D * L = 84TB\)的激活值。20来自（大致上）计算Transformer图中每个中间节点，例如。
- en: \[f(x) = \exp(g(x))\] \[\frac{df}{dx} = \exp(g(x)) \cdot \frac{dg}{dx}\]
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: \[f(x) = \exp(g(x))\] \[\frac{df}{dx} = \exp(g(x)) \cdot \frac{dg}{dx}\]
- en: so to avoid recomputing we need to save \(g(x)\) and \(\exp(g(x))\) from the
    forward pass. To avoid saving this much memory, we can choose to only save some
    fraction of the intermediate activations. Here are a few strategies we use.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免重新计算，我们需要保存前向传递中的 \(g(x)\) 和 \(\exp(g(x))\)。为了避免保存这么多内存，我们可以选择只保存一些中间激活的分数。以下是我们使用的一些策略。
- en: '**Block remat**: only save the input to each layer. This is the most aggressive
    method we use and only saves 1 checkpoint per layer, meaning we’d only save 4.2TB
    in the example above. This forces us to repeat essentially all forward pass FLOPs
    in the backward pass, meaning we increase our FLOPs from \(6ND\) to roughly \(8ND\).'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**块remat**：只保存每个层的输入。这是我们使用最激进的策略，每个层只保存一个检查点，这意味着在上面的例子中我们只会保存4.2TB。这迫使我们在反向传递中重复所有前向传递的FLOPs，这意味着我们将FLOPs从
    \(6ND\) 增加到大约 \(8ND\)。'
- en: '**Big matmuls only:** another simple policy is to only save the outputs of
    large matmuls. This lets us avoid recomputing any large matmuls during the backward
    pass, but still makes us recompute other activation functions and parts of attention.
    This reduces 20 per layer to closer to 7 per layer.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**仅大矩阵乘法**：另一个简单的策略是只保存大矩阵乘法的输出。这让我们在反向传递中避免重新计算任何大矩阵乘法，但仍然需要重新计算其他激活函数和注意力的一部分。这将每层的20个FLOPs减少到更接近7个。'
- en: This by no means comprehensive. When using JAX, these are typically controlled
    by `jax.remat`/`jax.checkpoint` (you can read more [here](https://jax.readthedocs.io/en/latest/_autosummary/jax.checkpoint.html)).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这绝不是全面的。当使用 JAX 时，这些通常由 `jax.remat`/`jax.checkpoint` 控制（你可以在这里阅读更多[信息](https://jax.readthedocs.io/en/latest/_autosummary/jax.checkpoint.html)）。
- en: Key-Value (KV) caching
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 键值（KV）缓存
- en: As we’ll see in [Section 7](../inference), LLM inference has two key parts,
    prefill and generation.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将在[第7节](../inference)中看到的那样，LLM推理有两个关键部分：预填充和生成。
- en: '**Prefill** processes a long prompt and saves its attention activations in
    a Key-Value Cache (KV Cache) for use in generation, specifically the key-value
    projections in the attention block.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预填充**处理一个长提示并将其注意力激活保存在键值缓存（KV缓存）中，用于生成，特别是注意力块中的键值投影。'
- en: '**Generation** batches several of these KV caches together and samples tokens
    from each of them.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成**批处理几个这些 KV 缓存，并从中采样标记。'
- en: Each KV cache is then effectively an array of size $[2, S, L, K, H]$ where the
    2 accounts for the keys and values. This is quite large! The total size of the
    Key-Value cache in int8 is $2SLKH$. For a moderately-sized model with 8k context
    length, 64 layers, and $KH = NH = D = 8192$, this is $2 \cdot 8192 \cdot 64 \cdot
    8192 = 8\text{GiB}$. You can see why we would want to use GMQA with $K \ll N$.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 每个KV缓存实际上是一个大小为 \([2, S, L, K, H]\) 的数组，其中2代表键和值。这相当大！在int8中，键值缓存的总量为 \(2SLKH\)。对于一个中等大小的模型，8k上下文长度，64层，\(KH
    = NH = D = 8192\)，这是 \(2 \cdot 8192 \cdot 64 \cdot 8192 = 8\text{GiB}\)。你可以看到为什么我们想要使用
    \(K \ll N\) 的 GMQA。
- en: What Should You Take Away from this Section?
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你应该从这一节中学到什么？
- en: 'The overall parameters and FLOPs of a Transformer are fairly easy to calculate,
    and are summarized here, assuming MHA (with batch size B, vocab size V, a sequence
    of length T, D=d[model], and F=d[ff]):'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer的整体参数和FLOPs计算相对简单，这里总结了，假设MHA（具有批大小B、词汇大小V、长度为T的序列、D=d[model]、F=d[ff]）：
- en: '| Component | Params per layer | Training FLOPs per layer |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 组件 | 每层的参数 | 每层的训练FLOPs |'
- en: '| --- | --- | --- |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **MLP** | 3DF | 18BTDF |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| **MLP** | 3DF | 18BTDF |'
- en: '| **Attention** | 4DNH | 24BTDNH + 12BT²NH |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| **Attention** | 4DNH | 24BTDNH + 12BT²NH |'
- en: '| **Other** | D | BTD |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| **其他** | D | BTD |'
- en: '| **Vocab** | DV (total, not per-layer) | 12BTDV |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| **Vocab** | DV (total, not per-layer) | 12BTDV |'
- en: The parameter count of the MLP block dominates the total parameter count and
    the MLP block also dominates the FLOPs budget as long as the sequence length $T
    < 8D$.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLP块的参数计数主导了总参数计数，只要序列长度 \(T < 8D\)，MLP块也主导了FLOPs预算。
- en: The total FLOPs budget during training is well approximated by \(6 \cdot \text{num_params}
    \cdot \text{num_tokens}\) for reasonable context lengths.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练过程中的总FLOPs预算可以很好地近似为 \(6 \cdot \text{num_params} \cdot \text{num_tokens}\)，对于合理的上下文长度。
- en: During inference, our KV caches are roughly \(2 \cdot S \cdot L \cdot N \cdot
    H\) per cache, although architectural modifications can often reduce this.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在推理过程中，我们的 KV 缓存大约是每个缓存 \(2 \cdot S \cdot L \cdot N \cdot H\)，尽管架构修改通常可以减少这个数值。
- en: A Few Problems to Work
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一些需要解决的问题
- en: '**Question 1:** How many parameters does a model with $D=4096$, $F=4 \cdot
    D$, $V=32,000$, and $L=64$ have? What fraction of these are attention parameters?
    How large are our KV caches per token? *You can assume $N\cdot H=D$ and multi-head
    attention with int8 KVs.*'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 1:** 一个具有 $D=4096$, $F=4 \cdot D$, $V=32,000$, 和 $L=64$ 的模型有多少参数？其中有多少比例是注意力参数？每个标记的
    KV 缓存有多大？*你可以假设 $N\cdot H=D$ 并且使用 int8 KVs 的多头注意力。*'
- en: <details><summary>Click here for the answer.</summary>
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>点击此处查看答案。</summary>
- en: The total parameters is roughly \(L \cdot (3DF + 4DNH + D) + 2DV\). For the
    given numbers, this is \(64 \cdot (3 \cdot 4e3 \cdot 16e3 + 4 \cdot 4e3 \cdot
    4e3 + 4e3) + 2 \cdot 4e3 \cdot 32e3 = 16e9\), or 16B parameters.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 总参数量大约是 \(L \cdot (3DF + 4DNH + D) + 2DV\)。对于给定的数字，这是 \(64 \cdot (3 \cdot 4e3
    \cdot 16e3 + 4 \cdot 4e3 \cdot 4e3 + 4e3) + 2 \cdot 4e3 \cdot 32e3 = 16e9\)，或
    16B 参数。
- en: The ratio of attention parameters to total parameters in general is \(4DNH /
    (4DNH + 3DF) = 4D^2 / (4D^2 + 12D^2) = 1/4\). This gives us roughly 1/4 of parameters
    are used in attention.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一般情况下，注意力参数与总参数的比例是 \(4DNH / (4DNH + 3DF) = 4D^2 / (4D^2 + 12D^2) = 1/4\)。这意味着大约有
    1/4 的参数用于注意力。
- en: Per token, our KV caches are \(2 \cdot L \cdot N \cdot H = 2 \cdot 64 \cdot
    4096\) in int8, which is `512kB / token`.</details>
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个标记，我们的 KV 缓存是 \(2 \cdot L \cdot N \cdot H = 2 \cdot 64 \cdot 4096\) 在 int8，这相当于
    `512kB / token`。</details>
- en: '**Question 2:** How many total FLOPs are required to perform A[B[X], D[Y]]
    *[D] W[D[Y], F] on `{‘X'': 4, ‘Y'': 8, ‘Z'': 4}`. How many FLOPs are performed
    by each TPU?'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 2:** 在 `{‘X'': 4, ‘Y'': 8, ‘Z'': 4}` 上执行 A[B[X], D[Y]] *[D] W[D[Y], F]
    需要多少总的 FLOPs？每个 TPU 执行了多少 FLOPs？'
- en: <details><summary>Click here for the answer.</summary>
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>点击此处查看答案。</summary>
- en: The total “theoretical” FLOPs of the operation is \(2 \cdot B \cdot D \cdot
    F\). However, because the computation isn’t sharded across the Z dimension, we’re
    actually doing Z extra FLOPs, meaning \(2 \cdot B \cdot D \cdot F \cdot Z\) total
    FLOPs. Since the computation is sharded across the other dimensions, the total
    per-device is roughly \(2 \cdot B \cdot D \cdot F / (X \cdot Y)\).</details>
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 该操作的“理论”总 FLOPs 是 \(2 \cdot B \cdot D \cdot F\)。然而，因为计算没有在 Z 维度上分片，我们实际上做了 Z
    个额外的 FLOPs，这意味着总共有 \(2 \cdot B \cdot D \cdot F \cdot Z\) 个 FLOPs。由于计算在其他维度上进行了分片，因此每个设备的总计算量大约是
    \(2 \cdot B \cdot D \cdot F / (X \cdot Y)\)。</details>
- en: '**Question 3:** How many FLOPs are involved in performing $A[I,J,K,L] * B[I,J,M,N,O]
    \rightarrow C[K,L,M,N,O]$?'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 3:** 执行 $A[I,J,K,L] * B[I,J,M,N,O] \rightarrow C[K,L,M,N,O]$ 操作涉及多少 FLOPs？'
- en: <details><summary>Click here for the answer.</summary>
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>点击此处查看答案。</summary>
- en: Following the rule above, we have I and J as contracting dimensions and K, L,
    M, N, and O as non-contracting dimensions. We have no “batching dimensions”, so
    this is just \(2 \cdot I \cdot J \cdot K \cdot L \cdot M \cdot N \cdot O\), the
    sum of all the axes. If we had a shared axis, it would only be counted once.</details>
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 依照上述规则，我们有 I 和 J 作为收缩维度，K、L、M、N 和 O 作为非收缩维度。我们没有“批处理维度”，所以这仅仅是 \(2 \cdot I \cdot
    J \cdot K \cdot L \cdot M \cdot N \cdot O\)，所有轴的总和。如果我们有一个共享的轴，它只会被计算一次。</details>
- en: '**Question 4:** What is the arithmetic intensity of self-attention (ignoring
    the Q/K/V/O projections)? *Give the answer as a function of the Q and KV lengths
    T and S.* At what context length is attention FLOPs-bound? Given the HBM bandwidth
    of our TPUs, plot the effective relative cost of attention to the FFW block as
    the context length grows.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 4:** 自注意力（忽略 Q/K/V/O 投影）的算术强度是多少？*将答案作为 Q 和 KV 长度 T 和 S 的函数给出。在什么上下文长度下注意力
    FLOPs 受限？给定我们 TPUs 的 HBM 带宽，随着上下文长度的增加，绘制注意力相对于 FFW 块的有效相对成本图。'
- en: <details><summary>Click here for the answer.</summary>
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>点击此处查看答案。</summary>
- en: Self-attention requires loading the \(Q\), \(K\), and \(V\) activations, then
    computing \(\text{softmax}(Q \cdot K) \cdot V\), then writing the result back
    to HBM. This will be done with Flash Attention so there are some caveats to this
    math, but basically in bf16 self-attention performs
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力需要加载 \(Q\)、\(K\) 和 \(V\) 激活，然后计算 \(\text{softmax}(Q \cdot K) \cdot V\)，然后将结果写回
    HBM。这将是使用 Flash Attention 完成的，因此这个数学有一些注意事项，但基本上在 bf16 自注意力执行
- en: \[\text{Q[B,T,N,H]} \rightarrow_\text{reshape} \text{Q[B, T, K, G, H]} \cdot
    \text{K[B, S, K, H]} \rightarrow \text{O[B, T, S, K, G]}\] \[U=\text{softmax}_S(\text{O[B,
    T, S, K, G]})\] \[\text{U[B, T, S, K, G]} \cdot \text{V[B, S, K, H]} \rightarrow
    \text{X[B, T, K, G, H]}\]
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: \[\text{Q[B,T,N,H]} \rightarrow_\text{reshape} \text{Q[B, T, K, G, H]} \cdot
    \text{K[B, S, K, H]} \rightarrow \text{O[B, T, S, K, G]}\] \[U=\text{softmax}_S(\text{O[B,
    T, S, K, G]})\] \[\text{U[B, T, S, K, G]} \cdot \text{V[B, S, K, H]} \rightarrow
    \text{X[B, T, K, G, H]}\]
- en: So our total bytes is \(2 * \text{sizeof}(Q) + 2 * \text{sizeof(K or V)} = 4BTNH
    + 4BSKH = 4BHK * (TG + S)\), total FLOPs is \(4BTSNH + O(BTSN)\) and the arithmetic
    intensity is \(4BTSKGH / (4BHK * (TG + S))\).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的总字节数是 \(2 * \text{sizeof}(Q) + 2 * \text{sizeof}(K 或 V)} = 4BTNH + 4BSKH
    = 4BHK * (TG + S)\)，总 FLOPs 是 \(4BTSNH + O(BTSN)\)，算术强度是 \(4BTSKGH / (4BHK * (TG
    + S))\)。
- en: So basically, during prefill we have \(S=T\) so we have an arithmetic intensity
    of \(4BT^2KGH / 4BHKT \cdot (G+1) = TG/(G + 1) = O(T)\). During generation, \(T=1\)
    so we have \(4BSKGH / (4BHK \cdot (G + S)) = SG / (G + S) \rightarrow G\) assuming
    \(S\) is very large. Depending on how you interpret the question, during prefill
    or training self-attention is compute bound at S=240 assuming no sequence sharding.
    During generation, we are never compute bound because \(G\) is small. Nonetheless,
    however, you can see that increasing \(G\) leads to us being closer to compute
    bound.</details>
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 所以基本上，在预填充过程中，我们有 \(S=T\)，因此我们的算术强度为 \(4BT^2KGH / 4BHKT \cdot (G+1) = TG/(G
    + 1) = O(T)\)。在生成过程中，\(T=1\)，所以我们有 \(4BSKGH / (4BHK \cdot (G + S)) = SG / (G +
    S) \rightarrow G\) 假设 \(S\) 非常大。根据你对问题的理解，在预填充或训练自注意力时，假设没有序列分片，在 \(S=240\) 时我们从未达到计算限制。在生成过程中，我们从未达到计算限制，因为
    \(G\) 很小。然而，你可以看到，增加 \(G\) 会使我们更接近计算限制。</details>
- en: '**Question 5:** At what sequence length are self-attention FLOPs equal to the
    QKVO projection FLOPs?'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 5：** 在什么序列长度下，自注意力 FLOPs 等于 QKVO 投影 FLOPs？'
- en: <details><summary>Click here for the answer.</summary>
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>点击此处查看答案。</summary>
- en: This is purely a question of when \(24BTDNH == 12BT^2NH\). Simplifying we get
    \(2D = T\), so e.g. for \(D=4096\), this is \(8192\). This tells us that for most
    reasonable context lengths, matmul FLOPs are greater.</details>
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这纯粹是一个关于何时 \(24BTDNH == 12BT^2NH\) 的问题。简化后我们得到 \(2D = T\)，例如，对于 \(D=4096\)，这将是
    \(8192\)。这告诉我们，对于大多数合理的上下文长度，矩阵乘法浮点运算次数更多。
- en: '**Question 6:** Say we only save the output of each of the 7 main matmuls in
    a Transformer layer during our forward pass (Q, K, V, O + the three FFW matrices).
    How many extra FLOPs do we need to “rematerialize” during the backwards pass?'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 6：** 假设我们在正向传播过程中只保存每个 7 个主要矩阵乘法的输出（Q，K，V，O + 三个 FFW 矩阵）。在反向传播过程中我们需要多少额外的
    FLOPs 来“重新计算”？'
- en: <details><summary>Click here for the answer.</summary>
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>点击此处查看答案。</summary>
- en: Saving only the seven matmul outputs (Q, K, V, O, W₁, W₂, W₃) means the backward
    pass must recompute the two attention matmuls
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 仅保存七个矩阵乘法输出（Q，K，V，O，W₁，W₂，W₃）意味着反向传播必须重新计算两个注意力矩阵乘法
- en: \[QK^{\top} \quad\text{and}\quad \operatorname{softmax}(QK^{\top})V.\]
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: \[QK^{\top} \quad\text{和}\quad \operatorname{softmax}(QK^{\top})V.\]
- en: Each is a $T \times T$ matmul batched over $B$ sequences and $N$ heads, so the
    additional FLOPs are
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 每个都是 $T \times T$ 的矩阵乘法，批处理在 $B$ 个序列和 $N$ 个头之间，所以额外的 FLOPs 是
- en: \[4 \; B \, T^{2} \, N \, H.\]
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: \[4 \; B \, T^{2} \, N \, H.\]
- en: All other recomputed operations are only $O(BTD)$.</details>
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 所有其他重新计算的操作都是 $O(BTD)$。</details>
- en: '**Question 7:** DeepSeek v3 says it was trained for 2.79M H800 hours on 14.8T
    tokens ([source](https://arxiv.org/pdf/2412.19437v1)). Given that it has 37B activated
    parameters, roughly what hardware utilization did they achieve? *Hint: note that
    they used FP8 FLOPs without structured sparsity.*'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 7：** DeepSeek v3 表示它在 14.8T 个标记上，使用 2.79M H800 小时进行了训练 ([来源](https://arxiv.org/pdf/2412.19437v1))。鉴于它有
    37B 个激活参数，他们大约实现了多少硬件利用率？*提示：注意他们使用了无结构稀疏性的 FP8 FLOPs。*'
- en: <details><summary>Click here for the answer.</summary>
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>点击此处查看答案。</summary>
- en: From the spec sheet [here](https://lenovopress.lenovo.com/lp1814.pdf), we find
    3,026 TFLOPs/s of FP8 performance with sparsity, or typically half this (`1.513e15`
    FLOPs/s) without sparsity. 2.79M H800 hours means `2.79e6 * 1.513e15 * 60 * 60
    = 1.52e25` total FLOPs. Given the activated parameter count of 37B, this training
    run should have used about `6 * 37e9 * 14.8e12 = 3.3e24` FLOPs. That means the
    FLOPs utilization is about `3.3e24 / 1.52e25 = 21.7%`.</details>
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 从规格说明 [这里](https://lenovopress.lenovo.com/lp1814.pdf)，我们找到了具有稀疏性的 3,026 TFLOPs/s
    的 FP8 性能，或者通常是没有稀疏性的一半（`1.513e15` FLOPs/s）。2.79M H800 小时意味着 `2.79e6 * 1.513e15
    * 60 * 60 = 1.52e25` 总 FLOPs。鉴于 37B 的激活参数数量，这次训练运行应该使用了大约 `6 * 37e9 * 14.8e12
    = 3.3e24` FLOPs。这意味着 FLOPs 利用率大约是 `3.3e24 / 1.52e25 = 21.7%`。</details>
- en: '**Question 8:** Mixture of Experts (MoE) models have $E$ copies of a standard
    dense MLP block, and each token activates $k$ of these experts. What batch size
    in tokens is required to be compute-bound for an MoE with weights in int8 on TPU
    v5e? For DeepSeek, which has 256 (routed) experts and $k=8$, what is this number?'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 8:** 专家混合（MoE）模型有 $E$ 个标准密集 MLP 块的副本，每个标记激活 $k$ 个这些专家。对于在 TPU v5e 上使用
    int8 权重的 MoE，需要多少个标记的批大小才能达到计算限制？对于具有 256 个（路由）专家和 $k=8$ 的 DeepSeek，这个数字是多少？'
- en: <details><summary>Click here for the answer.</summary>
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>点击这里查看答案。</summary>
- en: Because we have $E$ copies of each expert, in int8, we need to load $E \cdot
    D \cdot F$ bytes. Because each token activates $k$ experts, we have $2\cdot k
    \cdot B \cdot D \cdot F$ FLOPs. To be compute-bound with bfloat16 FLOPs, we need
    an arithmetic intensity over 240 which happens when $(2\cdot k \cdot BDF) / EDF
    > 240$ or $k \cdot B / E > 120$.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 因为每个专家有 $E$ 个副本，在 int8 中，我们需要加载 $E \cdot D \cdot F$ 字节。因为每个标记激活 $k$ 个专家，所以我们有
    $2\cdot k \cdot B \cdot D \cdot F$ FLOPs。为了使用 bfloat16 FLOPs 达到计算限制，我们需要超过 240
    的算术强度，这发生在 $(2\cdot k \cdot BDF) / EDF > 240$ 或 $k \cdot B / E > 120$ 时。
- en: Therefore, we need $B > 120 \cdot E / k$ to be compute bound. For DeepSeek,
    this gives us $B > 120 \cdot 256 / 8 = 3840$. This is a remarkably large batch
    size at generation time.</details>
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要 $B > 120 \cdot E / k$ 来达到计算限制。对于 DeepSeek，这给我们 $B > 120 \cdot 256 /
    8 = 3840$。这在生成时是一个非常大的批大小。</details>
- en: That’s it for Part 4! For Part 5 (about scaling Transformer training), [click
    here](../training)!
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第四部分到此结束！对于第五部分（关于 Transformer 训练的扩展），[点击这里](../training)！
- en: Appendix
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: 'Appendix A: How does Flash Attention work?'
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 A：Flash Attention 是如何工作的？
- en: 'The traditional objection to scaling Transformers to very long context is that
    the attention FLOPs and memory usage scale quadratically with context length.
    While it’s true that the attention QK product has shape $[B, S, T, N]$ where B
    is the batch size, S and T are the Q and K sequence dims, and N is the number
    of heads, this claim comes with some serious caveats:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 将 Transformers 扩展到非常长的上下文的传统反对意见是，注意力的 FLOPs 和内存使用量与上下文长度成二次关系。虽然注意力 QK 乘积的形状是
    $[B, S, T, N]$，其中 B 是批大小，S 和 T 是 Q 和 K 序列维度，N 是头数，但这个说法有一些严重的警告：
- en: As we noted in Section 4, even though this is quadratic, the attention FLOPs
    only dominated when \(S > 8 \cdot D\), and especially during training the memory
    of a single attention matrix is small compared to all of the weights and activation
    checkpoints living in memory, especially when sharded.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正如我们在第 4 节中提到的，尽管这是二次的，但注意力 FLOPs 只有在 \(S > 8 \cdot D\) 时才占主导地位，尤其是在训练期间，单个注意力矩阵的内存与所有生活在内存中的权重和激活检查点相比非常小，尤其是在分片的情况下。
- en: We don’t need to materialize the full attention matrix in order to compute attention!
    We can compute local sums and maxes and avoid ever materializing more than a small
    chunk of the array. While the total FLOPs is still quadratic, we drastically reduce
    memory pressure.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们不需要实现完整的注意力矩阵来计算注意力！我们可以计算局部总和和最大值，并避免实现超过数组一小部分的内容。尽管总的 FLOPs 仍然是二次的，但我们极大地减少了内存压力。
- en: This second observation was first made by [Rabe et al. 2021](https://arxiv.org/abs/2112.05682)
    and later in the [Flash Attention paper](https://arxiv.org/abs/2205.14135) (Dao
    et al. 2022). The basic idea is to compute the attention in chunks of K/V, where
    we compute the local softmax and some auxiliary statistics, then pass them onto
    the next chunk which combines them with its local chunk. Specifically, we compute
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个观察结果首先由 [Rabe 等人 2021](https://arxiv.org/abs/2112.05682) 提出，后来在 [Flash Attention
    论文](https://arxiv.org/abs/2205.14135)（Dao 等人 2022）中提出。基本思想是分块计算注意力，其中我们在 K/V 块中计算局部
    softmax 和一些辅助统计信息，然后将它们传递到下一个块，该块将它们与其局部块结合。具体来说，我们计算
- en: '**M:** The running max of \(q \cdot k\) over the sequence dimension'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**M:** 在序列维度上 \(q \cdot k\) 的运行最大值'
- en: '**O:** The running full attention softmax over the sequence dimension'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**O:** 在序列维度上的运行全注意力 softmax'
- en: '**L:** The running denominator \(\sum_i (q \cdot k_i - \text{running max})\)'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**L:** 运行分母 \(\sum_i (q \cdot k_i - \text{running max})\)'
- en: 'With these, we can compute the new max, the new running sum, and the new output
    with only a constant amount of memory. To give a sketchy description of how this
    works, attention is roughly this operation:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些信息，我们只需常数内存量就能计算新的最大值、新的运行总和和新的输出。为了简要描述这是如何工作的，注意力大致是这样的操作：
- en: \[\text{Attn}(Q, K, V) = \sum_i \frac{\exp(Q \cdot K_i - \max_j Q \cdot K_j)
    V_i}{\sum_l \exp(Q \cdot K_l - \max_j Q \cdot K_j)}\]
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: \[\text{Attn}(Q, K, V) = \sum_i \frac{\exp(Q \cdot K_i - \max_j Q \cdot K_j)
    V_i}{\sum_l \exp(Q \cdot K_l - \max_j Q \cdot K_j)}\]
- en: The max is subtracted for numerical stability and can be added without affecting
    the outcome since \(\sum_i \exp(a_i + b) = \exp(b) \sum \exp(a)\). Looking just
    at the denominator above, if we imagine having two contiguous chunks of key vectors,
    \(K^1\) and \(K^2\) and we compute the local softmax sums \(L^1\) and \(L^2\)
    for each
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 为了数值稳定性，减去最大值，并且可以添加而不影响结果，因为 \(\sum_i \exp(a_i + b) = \exp(b) \sum \exp(a)\)。仅看上面的分母，如果我们想象有两个连续的键向量块，\(K^1\)和\(K^2\)，并且我们为每个块计算局部softmax总和\(L^1\)和\(L^2\)，
- en: \[L^1 = \sum_i \exp(Q \cdot K_i^1 - \max_j Q \cdot K_j^1)\] \[L^2 = \sum_i \exp(Q
    \cdot K_i^2 - \max_j Q \cdot K_j^2)\]
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: \[L^1 = \sum_i \exp(Q \cdot K_i^1 - \max_j Q \cdot K_j^1)\] \[L^2 = \sum_i \exp(Q
    \cdot K_i^2 - \max_j Q \cdot K_j^2)\]
- en: Then we can combine these into the full softmax sum for these two chunks together
    by using
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以通过使用以下方法将这两个块合并为完整的softmax总和
- en: \[L^\text{combined} = \exp(M^1 - \max(M^1, M^2)) \cdot L^1 + \exp(M^2 - \max(M^1,
    M^2)) \cdot L^2\]
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: \[L^\text{combined} = \exp(M^1 - \max(M^1, M^2)) \cdot L^1 + \exp(M^2 - \max(M^1,
    M^2)) \cdot L^2\]
- en: where
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: \[M^1 = \max_j Q \cdot K_j^1 \text{ and } M^2 = \max_j Q \cdot K_j^2\]
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: \[M^1 = \max_j Q \cdot K_j^1 \text{ 和 } M^2 = \max_j Q \cdot K_j^2\]
- en: This can be done for the full softmax as well, giving us a way of accumulating
    arbitrarily large softmax sums. Here’s the full algorithm from the Flash Attention
    paper.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这也可以用于完整的softmax，为我们提供累积任意大的softmax总和的方法。以下是Flash Attention论文中的完整算法。
- en: <picture>![](../Images/3343722c46d28c8a9ea7cab270302e49.png)</picture>
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: <picture>![](../Images/3343722c46d28c8a9ea7cab270302e49.png)</picture>
- en: From a hardware standpoint, this lets us fit our chunk of Q into VMEM (what
    the algorithm above calls on-chip SRAM) so we only have to load the KV chunks
    on each iteration, reducing the arithmetic intensity. We can also keep the running
    statistics in VMEM.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 从硬件角度来看，这使得我们可以将我们的Q块放入VMEM（上述算法中称为片上SRAM）中，因此我们只需在每次迭代中加载KV块，从而降低算术强度。我们还可以在VMEM中保留运行统计信息。
- en: 'One last subtle point worth emphasizing is an attention softmax property that’s
    used to make the Flash VJP (reverse mode derivative) calculation practical for
    training. If we define an intermediate softmax array as:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个值得强调的微妙之处是用于使Flash VJP（逆模式导数）计算对训练实用的注意力softmax属性。如果我们定义一个中间softmax数组为：
- en: \[S_{ij} = \frac{e^{\tau q_i \cdot k_j}}{\sum_k e^{\tau q_i \cdot k_j}}\]
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: \[S_{ij} = \frac{e^{\tau q_i \cdot k_j}}{\sum_k e^{\tau q_i \cdot k_j}}\]
- en: 'In attention, we obtain *dS* from reverse-mode *dO* and *V* arrays:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在注意力中，我们从逆模式的*dO*和*V*数组中获得*dS*：
- en: \[dS_{ij} = dO_{id} \cdot_d V_{jd} = \sum_d dO_{id} V_{jd}\]
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: \[dS_{ij} = dO_{id} \cdot_d V_{jd} = \sum_d dO_{id} V_{jd}\]
- en: During the backpropagation of this gradient to Q and K
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在将此梯度反向传播到Q和K的过程中
- en: \[d(q_i \cdot k_j) = (dS_{ij} - S_{ij} \cdot_j dS_{ij}) S_{ij}\]
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: \[d(q_i \cdot k_j) = (dS_{ij} - S_{ij} \cdot_j dS_{ij}) S_{ij}\]
- en: We exploit an identity that allows us to exchange a contraction along the large
    key **length** dimension with a local contraction along the feature **depth**
    dimension.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用一个恒等式，允许我们将沿着大键**长度**维度的收缩与沿着特征**深度**维度的局部收缩进行交换。
- en: \[\begin{align*} S_{ij} \cdot_j dS_{ij} &= \sum_j \frac{e^{\tau q_i \cdot k_j}}{\sum_k
    e^{\tau q_i \cdot k_k}} \sum_d dO_{id} V_{jd} \\ &= \sum_d dO_{id} \sum_j \frac{e^{\tau
    q_i \cdot k_j}}{\sum_k e^{\tau q_i \cdot k_k}} V_{jd} \\ &= \sum_d dO_{id} O_{id}
    \\ &= dO_{id} \cdot_d O_{id} \end{align*}\]
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} S_{ij} \cdot_j dS_{ij} &= \sum_j \frac{e^{\tau q_i \cdot k_j}}{\sum_k
    e^{\tau q_i \cdot k_k}} \sum_d dO_{id} V_{jd} \\ &= \sum_d dO_{id} \sum_j \frac{e^{\tau
    q_i \cdot k_j}}{\sum_k e^{\tau q_i \cdot k_k}} V_{jd} \\ &= \sum_d dO_{id} O_{id}
    \\ &= dO_{id} \cdot_d O_{id} \end{align*}\]
- en: This replacement is crucial for being able to implement a sequence-block *local*
    calculation for the VJP, and enables further clever sharding schemes like ring
    attention.</d-article>  <d-appendix><d-footnote-list><d-citation-list>### Miscellaneous
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这种替换对于能够实现VJP的序列块**局部**计算至关重要，并使像环注意力这样的进一步巧妙分片方案成为可能。</d-article>  <d-appendix><d-footnote-list><d-citation-list>###
    杂项
- en: ^*Work done at Google DeepMind, now at MatX.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ^*在Google DeepMind完成的工作，现在在MatX。
- en: Citation
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 引用
- en: 'For attribution in academic contexts, please cite this work as:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在学术环境中进行归属时，请引用此工作：
- en: '[PRE0]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'or as a BibTeX entry:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 或者作为一个BibTeX条目：
- en: '[PRE1]</d-citation-list></d-footnote-list></d-appendix> <d-bibliography src="/scaling-book/assets/bibliography/main.bib"></d-bibliography></d-byline>'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE1]</d-citation-list></d-footnote-list></d-appendix> <d-bibliography src="/scaling-book/assets/bibliography/main.bib"></d-bibliography></d-byline>'
